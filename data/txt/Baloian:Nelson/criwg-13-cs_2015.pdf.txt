Factors Influencing the Decision to Crowdsource 

Nguyen Hoang Thuan1, Pedro Antunes1, David Johnstone1  

1 School of Information Management, VUW, Rutherford House  

23 Lambton Quay, Pipitea Campus, New Zealand 

{Thuan.Nguyen, Pedro.Antunes, David.Johnstone}@vuw.ac.nz 

Abstract.  In  order  to  integrate  a  crowdsourcing  strategy  to  an  organization’s 
business processes, managers need to decide whether or not crowdsourcing is 
suitable  for  the  organizational  context.  This  study  conducted  a  structured 
literature  review  to  identify  factors  related  to  this  decision.  These  identified 
factors have been synthesized into a framework for supporting the decision to 
crowdsource. Based on this framework, recommendations for managers, which 
were summarized in the decision tables, have been proposed. 

Keywords: Crowdsourcing, crowdsourcing decision, business process, 
literature review, socio-technical system 

1 

Introduction 

Since its introduction, the term “crowdsourcing” was firstly introduced by Howe [1] 
to refer to a model that relies on the crowd, a large undefined group of individuals, to 
achieve  specific  tasks.  Pioneering  studies  have  suggested  that  this  model  can  bring 
multiple  competitive  advantages  for  organizations,  such  as  more  flexibility  and 
responsiveness  to  business  strategy,  cost  savings  [2],  and  harvesting  expertise, 
information, skills, and labour [3, 4]. Some organizations that successfully utilize this 
model  for  their  business  strategies  are  Wikipedia  for  writing  and  editing  articles, 
Threadless  for  T-shirt  design,  and  Starbucks,  i.e.  MyStarbucksIdea  project,  for 
collecting customers’ ideas.  

Given that crowdsourcing can benefit organizations, it is reasonable to expect that 
crowdsourcing should be potentially integrated with existing organizational business 
processes. However, this does not seem to have happened. A recent survey [5] reports 
that  only  10%  of  surveyed  organizations  have  actually  deployed  a  crowdsourcing 
strategy.  If  crowdsourcing  is  such  a  promising  strategy,  then  why  has  it  not  been 
widely  adopted  by  organizations?  One  of  the  possible  answers  to  this  question  has 
been  suggested  by  Malone  et  al.  [6],  who  state  that  “[organizations]  do  not  know 
how”  to  utilize  crowdsourcing  and  advocate  more  investigation  into  the  “how  to” 
question. In the same vein, Vukovic and Bartolini [4] and Khazankin et al. [7] also 
suggest  further  research  on  this  question,  especially  focusing  on  how  to  integrate 
crowdsourcing with existing organizations’ business processes.  

The  literature  addressing  this  problem  shows  that  integration  can  be  addressed 
from two different angles: the manager’s view, which is responsible for coordinating 

the  tasks;  and  the  designer’s  view,  which  is  responsible  for  implementing  and 
configuring the crowdsourcing strategy on a particular platform. While many studies 
[8, 9] have focused on the design issues, currently there is little research focusing on 
the  manager’s  perspective,  including  analysis  of  the  multiple  issues  that  managers 
have to consider when adopting a crowdsourcing strategy [10]. This paper focuses on 
one of the management issues, which is the “decision to crowdsource or not”. This 
decision requires managers to determine whether crowdsourcing is a suitable strategy 
for a particular organizational context, rather than with the actual implementation of 
this  crowdsourcing  strategy.  The  “decision  to  crowdsource  or  not”  is  challenging 
because  multiple  factors  need  to  be  considered  and  evaluated  in  order  to  make  an 
informed decision [11]. This leads to the research question, what factors influence an 
organizations’ decision to crowdsource? 

To  address  the  question,  this  study  conducted  a  structured  literature  review  to 
analyse the factors influencing the decision to crowdsource. Since crowdsourcing can 
be seen as a socio-technical system [10], these identified factors will be synthesized to 
a  decision  framework  including  different  layers  of  a  socio-technical  system.  The 
study  contributes  to  current  knowledge  by  answering  the  question  raised  in  the 
literature,  “to  crowdsource  or  not  to  crowdsource”  [12].  From  the  practitioner’s 
perspective,  it  provides  practical  recommendations  for  making  the  crowdsourcing 
decision in an organizational context. The recommendations will be presented using 
decision tables.  

2  Literature Review 

2.1  Concepts and Terminology  

Since crowdsourcing is an emerging research area, different terms were used for this 
concept, including crowdsourcing, collective intelligence, human computation, mass 
collaboration  and  peer  production  [13,  14].  As  a  result,  researchers  have  proposed 
different definitions for crowdsourcing. Some researchers, such as Doan et al. [14],  
define  crowdsourcing  as  a  system,  in  which  the  problem  owner  asks  the  crowd  to 
solve a problem. Others, such as Howe [1] and Schenk and Guittard [15], have seen 
crowdsourcing  as  a  form  of  outsourcing,  in  which  tasks  traditionally  performed  by 
organizational employees or other companies were sent to the members of the crowd. 
In some cases a single researcher, such as Brabham [3, 16] and Vukovic [17, 18] may 
provide more than one definition. In order to conceptualize a definition that captures 
“any  given  crowdsourcing  activity”  [19],  Estellés-Arolas  and  González-Ladrón-deGuevara 
 [19]  recently  analysed  the  existing  definitions  extracted  from  literature.  A 
selection  of  209  articles  was  examined  and  40  of  them,  which  present  original 
definitions  of  crowdsourcing,  were  analysed.  As  a  result,  eight  common 
characteristics of crowdsourcing have been identified: clearly defined crowd, a task 
with  a  clear  goal,  a  clear  recompense  for  the  crowd,  the  identified  crowdsourcer, 
defined  compensation  for  the  crowdsourcer,  online  process,  open  call,  and  internet 
usage.  The  authors  [19] 
into  a  single 
comprehensive definition.  

these  characteristics 

then 

integrate 

Although the definition proposed in [19] is comprehensive, it is wordy [20]. Thus 
the current study simplifies and adapts it for an organizational context. As a result, 
crowdsourcing  is  defined  as  an online strategy, in which an organization proposes 
defined task(s) to the members of the crowd via a flexible open call. By undertaking 
the  task(s),  the  members  contribute  their  work,  knowledge,  skills  and/or  experience 
and  receive  reward,  including  economic  reward,  social  recognition,  self-esteem,  or 
the development of individual skills. The organization will obtain these contributions 
and utilize the results for the defined goals.  In  the  following  part,  two  examples  to 
clarify the definition are introduced.  

First,  Amazon  Mechanical  Turk  (AMT)  is  a  profit  platform  [21]  that  allows 
organizations to crowdsource their simple tasks. After defining tasks and deciding to 
choose  crowdsourcing,  an  organization  creates  and  publishes  these  tasks  on  the 
platform using the predefined templates. Members (or workers) on the AMT platform 
browse information of available tasks, including requirements and payments, and may 
decide to perform these tasks. These tasks are usually performed individually and the 
results are submitted back to the organization. If these results’ are sufficient quality, 
the  organization  will  pay  the  compensation  to the  members  who  perform  the  tasks. 
Second,  different  from  AMT,  Brabham  [22]  introduced  a  non-profit  crowdsourcing 
competition in the case of NextStopDesign, where the members participate to solve a 
design task without any concrete award. In this project, the task is published on its 
own website where anyone who has design skill can submit their design solution. The 
design solution then was evaluated based on the crowd members’ vote. As a result, 
the three designs, which receive the highest vote, win the competition.  

Although  these  examples  show  that  crowdsourcing  activities  can  be  different, 
ranging  from  micro  tasks  to  problem  solving,  from  individual  to  competition,  from 
profit to non-profit projects, the typical process of crowdsourcing can be presented in 
the following way.  

When  an  organization  has  tasks  to  be  accomplished,  the  first  step  is  to  decide 
whether  to  use  crowdsourcing  to  perform  these  tasks  [23].  Then,  if  the  decision  to 
choose crowdsourcing is made, the organization creates an open call and releases the 
tasks to the crowd. This step can be done through a platform developed either by the 
organization  (e.g.  NextStopDesign)  or  by  a  third  party  (e.g.  AMT).  Through  the 
platform,  the  organization  can  approach  members  of  the  crowd.  Depending  on  the 
organization’s requirements, the members can be specific to a particular community, 
such  as  designers  in  NextStopDesign,  or  anyone  willing  to  perform  the  task. 
Accomplishing these tasks individually or collaboratively, the members then submit 
the  results  back  to  the  organization  which  assesses  the  quality  of  the  results.  The 
payment  or  other  incentives  will  be  given  to  the  members  if  the  organization  is 
satisfied with the results  [2, 10]. In practice, this process can vary. For example, a big 
task  can  be  divided  into  many  smaller  tasks  with  a  defined  workflow  before 
delivering  to  the  crowd,  and  thus  the  results  need  to  be  aggregated  to  achieve  the 
original task [24].  

Currently, this process has been used in varied contexts with different applications. 
Because of this broad area of applications, terminology is not always consistent. For 
example, the term “task” can prefer to a problem, human intelligence task, micro task, 
or crowd work while the crowd member is called a solver, worker, labourer, user, or 
participant depending on the applications. This paper uses “task” and “member” since 

these  terms  can  be  used  in  a  broad  sense  and  are  more  consistent  with  the  above 
described process. 

2.2  Types of Crowdsourcing 

Existing  literature  has  introduced  several  ways  to  categorize  crowdsourcing.  Some 
researchers  choose  one  dimension  to  classify  crowdsourcing  activities,  while  others 
suggest multi-dimensional classification. In the former approach, Whitla [2] classified 
crowdsourcing  applied  to  marketing  into  three  areas  based  on  the  purpose  of  the 
activity,  including  product  development,  advertising  and  promotion,  and  marketing 
research.  Similarly,  Brabham  [25]  proposed  a  crowdsourcing  typology  for  problem 
solving  based  on  four  functions:  knowledge  discovery  and  management,  broadcast 
search, peer-vetted creative production, and distributed human intelligence tasking.  

In the latter approach, Rouse [11] presented her taxonomy of crowdsourcing with 
three dimensions: nature of the task, distribution of benefits, and forms of motivation. 
Geiger  el  al.  [26]  identified  four  dimensions:  preselection  of  contributions, 
accessibility of peer contributions, aggregation of contributions, and remuneration for 
contributions. Malone et al. [6] based their classification around four basic questions: 
what is being crowdsource, who is performing the task, why people do this, and how 
the task is being done. 

According  to  Nickerson  et  al.  [27],  a  taxonomy  and  its  dimensions  should  be 
evaluated according to its “usefulness”. In this study, the main purpose is to support 
managers making crowdsourcing decision. Zhao and Zhu [10] suggest the complexity 
of tasks should be clarified before making this decision, and we believe that the nature 
to  achieve  tasks  individually  or  competitively  can  also  influence  this  decision. 
Consequently, this study employs two dimensions proposed by Schenk and Guittard’s 
[15]:  task  complexity  and  the  difference  between  integration  and  selection  based 
crowdsourcing for categorizing crowdsourcing.  

Table 1. Examples of crowdsourcing task types 

                      Participation mode 
Complexity 

Simple 

Skilled 

Individual 
(Integrative) 
Market place 
- AMT 
- Taskcn 
Collective intelligence 
- Wikipedia 
- Writing academic 
papers [28] 

Competitive 
(Selective) 
Simple contest 
- Yahoo Answers 
- Ask Ville by Amazon 
Problem solving contest 
- NextStopDesign  
- Innocentive 
- Threadless 
- IStockPhoto 

By  examining  the  characteristics  of  crowdsourcing  in  practice,  Schenk  and 
Guittard’s  [15]  stressed 
important  dimension. 
Crowdsourcing tasks can be classified as simple, complex or creative. Simple tasks 
are jobs that can be accomplished with generic skills. Complex tasks require expertise 

task  complexity  as 

the  first 

and problem solving skills. Creative tasks relate to individual creativity such as logo 
design. It is worth to note that most of the complex tasks also require certain level of 
creativity while creative tasks’ purposes are normally to find solutions for problems. 
Consequently, the difference between complex tasks and creative tasks is not large, 
and  we  combined  them  to  “skilled”  tasks  in  this  study.  Secondly,  the  authors  [15] 
suggest the difference between the integrative and selective nature of the process as 
another  dimension,  which  we  named  here  as  the  participation  mode  that  represents 
how tasks can be performed individually or competitively. Table 1 presents examples 
of different types of crowdsourcing, based on task properties. 

2.3  Decision to Crowdsource 

The  decision  to  crowdsource  has  to  be  made  before  an  organization  chooses  a 
crowdsourcing strategy. According to Rouse [11], this decision is significant for the 
organization  since  a  failed  crowdsourcing  project  can  waste  the  organization’s 
resources. With this in mind, researchers have started to examine closely the factors 
related to this decision. 

Ranade  and  Varshney  [12]  propose  the  question  “to  crowdsource  or  not  to 
crowdsource?”, but their study was confined to crowdsourcing contests, also known 
as  problem  solving  contests.  Also  focusing  on  a  particular  type  of  crowdsourcing, 
Buecheler et al. [29] examined collective intelligence in scientific method. Using the 
“three constituents principle” from Artificial Intelligence, they suggested a framework 
of  three  factors  (environment,  agent,  and  task)  to  determine  the  viability  of 
crowdsourcing. Although each constituent principle has detailed variables, the authors 
did  not  specify  how  these  variables  influence  the  crowdsourcing  decision.  More 
importantly, the framework cannot be fully validated as the authors themselves stated 
“the data collection was not thorough enough to analyse all the variables mentioned in 
our framework”. 

Also  focused  on  problem  solving  contests,  Afuah  and  Tucci  [30]  recently 
suggested  circumstances  where  crowdsourcing  could  be  used.  They  evaluated  the 
likelihood of crowdsourcing by comparing three alternative ways to solve a problem: 
internal  sourcing,  outsourcing  and  crowdsourcing.  Based  on  behavioural  and 
evolutionary theories of organizations, they identified four organizational factors and 
one  environmental  factor  that  need  to  be  considered  before  the  decision  to 
crowdsource  can  be  made.  Four  organizational  factors  that  positively  influence  the 
probability of crowdsourcing are: characteristics of the problem (ease of delineation 
and transmission, and modularizability), characteristics of knowledge required for the 
solution  (effective  distance,  and  tacitness  and  complexity),  characteristics  of  the 
crowd  (pervasiveness  of  problem  solving  know-how,  and  motivation),  and 
characteristics  of  solutions  to  be  evaluated  and  of  evaluators  (experience-good 
orientation, and number of solution evaluators required). The external factor includes 
the  pervasiveness  and  low  cost  of  IT,  which  positively  moderate  the  relationship 
between aforementioned variables and the probability of crowdsourcing. 

Adopting  a  broader  perspective,  Sharma  [31]  provided  a  framework  of  several 
success  factors  associated  with  crowdsourcing  initiatives,  which  are  necessarily 
involved in the decision to crowdsource. In this framework, motive alignment of the 

crowd is the central factor influencing crowdsourcing success since it is “aligned to 
long term objectives of the crowdsourcing initiative” [31]. This factor is affected by 
five peripheral factors: vision and strategy, human capital, infrastructure, linkages and 
trust, and the external environment. However, many factors in this framework need to 
be  detailed  [10]  before  the  framework  can  be  used  to  support  managers  to  make 
informed decision.   

In summary, making an informed decision whether to crowdsource or not requires 
a  comprehensive  analysis  in  which  multiple  factors  should  be  examined  in  a 
systematic way [10, 11]. Although studies highlighted the importance of the decision 
to crowdsource, most of them have focused on a particular type of task. Therefore, the 
overall picture of the crowdsourcing decision is still missing. Moreover, these studies 
offer different lists of factors that should be considered in this decision, and none of 
them proposes a comprehensive framework to support the decision to crowdsource. 
Taking  that  in  consideration,  this  study  addresses  this  gap  by  synthesizing  the 
accumulated knowledge in the literature to clarify the factors related to crowdsourcing 
decision for general types of task.  

3  Method 

Selecting Articles. A structured literature review was chosen as the research method 
for this study. Following the approach introduced by Webster and Watson [32], this 
review is concept-centric without being limited by selected journals. In addition, since 
crowdsourcing is an emerging research field  [10], many findings were presented  in 
conference  papers  which  are  also  included  in  this  study.  Consequently,  six  online 
bibliographic databases were selected: ACM, IEEE, Science Direct, SAGE, Springer 
Link and Emerald (as identified by Estellés-Arolas and González-Ladrón-de-Guevara 
[19]). These databases were searched, using ‘crowdsourcing’ as the keyword, between 
February  and  March  2013.  Only  English  publications  available  in  full  text  were 
selected. The results are shown in Table 2.  

Table 2. Search results 

Document types  ACM  IEEE  Science  

Conference paper  274 
Journal 
Total 

 
274 

110 
33 
143 

Direct 
 
33 
33 

Sage 

Emerald 

 
16 
16 

 
8 
8 

Springer 
Link 
 
137 
137 

Total 

384 
227 
611 

After  removing  duplicates,  editorial  introductions,  conference  posters,  letters, 
tutorials,  and  publications  that  contain  the  searching  keyword  but  focus  on  other 
issues, the total of 500 papers were left in the initial pool. 

Filtering Articles. In an effort to filter the papers which are not related to the focus of 
this  study  (the  decision  to  crowdsource),  we  first  eliminated  the  articles  related  to 
crowdsourcing  design  issues  based  on  the  paper’  title  and  their  keywords.  This 
elimination  is  performed  based  on  the  work  of  Kittur  et  al.  [13],  who  suggests  key 
topics in designing complex crowdsourcing processes, such as workflow design, task 
assignment, designing real-time crowdsourcing, collaboration and quality control. 112 
articles, which have the titles and keywords related to these topics, were mapped to 
the  design  theme.  This  step  also  filtered  out  articles  focused  on  crowdfunding  (3 
articles) and legal discussion (1 article). As a result, the pool reduced to 384 articles.   

Classifying  Articles.  Since,  in  our  knowledge,  there  is  currently  no  classification 
frame  or  keyword  schema  that  can  distinguish  the  papers  related  to  crowdsourcing 
decision from the unrelated ones, a classifying procedure is needed. Consequently, we 
defined the following iterative procedure for classifying the remaining 384 papers.  

First, some papers, whose titles are clearly related to the decision to crowdsource, 
were  classified  to  the  crowdsourcing  decision  group  of  papers.  Examples  of  these 
articles  are  “to  crowdsource  or  not  to  crowdsource?”  [12]  and  “crowdsourcing 
critical success factor model: strategies to harness the collective intelligence of the 
crowd”  [31].  Second,  by  reading  these  classified  articles,  a  list  of  important  terms 
which relate to the decision to crowdsource was identified. Third, unclassified papers 
were examined, focusing on the papers’ abstracts, introductions and conclusions. If a 
paper has term(s) in the list (or phases that have the equivalent meaning with terms in 
the list), it was added to the crowdsourcing decision group of chosen papers. Fourth, 
by examining the new added paper, new term(s) may be added to the list. Steps three 
and four were performed iteratively until no new term could be found. As a result, the 
list  includes  the  following  key  terms:  crowdsource  or  not  to  crowdsource, 
crowdsourcing circumstances, crowdsourcing success factors, crowdsourcing success, 
crowdsourcing  decision,  feasibility  of  using  crowdsourcing,  crowdsource  ability, 
crowdsourcing  viability,  crowdsourcing  alternatives,  probability  of  crowdsourcing, 
crowdsourcing 
risks  of 
crowdsourcing. In the final step, we engaged in detailed reading of the unclassified 
papers’  abstracts,  introductions  and  conclusions,  and  classified  them  based  on  the 
terms list related to crowdsourcing decision.  

framework,  crowdsourcing 

factors,  and  potential 

As  a  result,  38  articles  related  to  the  decision  to  crowdsource  were  identified. 
Although  this  number  is  relatively  small,  it  is  consistent  with  a  recent  literature 
review  [10],  which  also  reported  limited  publications  on  adopting  crowdsourcing. 
Following  the  forward  and  backward  searching  proposed  by  Webster  and  Watson 
[32], additional 10 articles were identified, resulting in 48 papers overall.  

4  A  Theoretical  Framework  to  Support  the  Decision  to 
Crowdsource 

By  analysing  the  chosen  articles,  the  factors  related  to  the  crowdsourcing  decision 
were  identified.  From  a  system’s  perspective,  crowdsourcing  is  a  socio-technical 
system  [10,  33],  which  involves  interaction  and  connectivity  between  humans  and 

technology.  Adopting  this  perspective,  the  study  adapted  the  various  layers  of  a 
complex sociotechnical system from Vicente’s work [34] and classified the identified 
factors to these layers (Figure 1). There are four layers in this framework: the task that 
an  organization  wants  to  crowdsource,  the  people  who  perform  the  task,  the 
management which plans how the task can be coordinated, and the environment. A 
discussion of each layer in the framework follows. 

-­‐	  Platform:	  Internal	  (build)	  vs.	  External	  (available)

Environment

Management

-­‐	  Budget:	  Small	  vs.	  Large
-­‐	  Crowdsourcing	  expert	  and	  experience:	  Available	  vs.	  Not	  available	  
-­‐	  Acceptance	  level	  of	  low	  quality	  result	  risk:	  High	  vs.	  Low

People

-­‐	  Employee	  for	  task:	  Few	  vs.	  Large
-­‐	  The	  crowd	  for	  task:	  Available	  vs.	  Must	  build

Task
-­‐	  Internet	  vs.	  Physical
-­‐	  Interactive	  vs.	  Independent
-­‐	  Sensitive	  information	  vs.	  Non-­‐sensitive
-­‐	  Partitioned	  vs.	  Non-­‐partitioned

 

Fig. 1. A Theoretical Framework to support the decision to crowdsource (Adapted from [34]) 

Task  Properties.  Existing  evidence  has  highlighted  the  nature  of  tasks  as  an 
important factor in the decision to crowdsource [12]. According to Kazman and Chen 
[35], the crowd can be good for certain tasks, but not for all kinds of tasks. Four task 
properties were highlighted. The first property is whether a task and its corresponding 
result  can  be  delivered  and  collected  through  the  internet.  Most  of  the  existing 
literature suggests crowdsourcing should only be used for internet activities, and some 
of them go further by adding this property to the crowdsourcing definition [15, 19, 
36].  Only  one  exception  [37],  based  on  the  deployment  of  tasks  through  physical 
kiosks,  was  identified  in  the  searching  papers.  However,  in  this  case,  the  problem 
solving task could easily be transferred to an online platform.  

The  second  property  is  the  interaction  property,  focusing  on  the  nature  of  the 
relationship  between  the  organization  and  the  members  during  the  crowdsourcing 
activities.  Burger-Helmchen  and  Pénin  [38],  for  example,  suggest  crowdsourcing 

contests  are  not  suitable  for  tasks  that  require  large  interaction  between  the 
organization  and  the  members  (solvers).  This  suggestion  is  logical  since  the  crowd 
members are usually anonymous to the organization and consequently, it is quite hard 
to establish the interaction between them. This argument can also be applied to other 
types of crowdsourcing tasks such as tasks published on AMT and Taskcn [39]. 

Third, since tasks in crowdsourcing are sent to anonymous members in the crowd, 
Muntés-Mulero  et  al.  [40]  claim  that  tasks  with  sensitive  information,  including 
privacy,  security,  and  intellectual  property,  are  not  suitable  for  crowdsourcing. 
However, other believe that with additional actions in defining tasks, these tasks can 
still  be  crowdsourced.  An  action  handling  sensitive  information  in  crowdsourcing 
tasks is introduced by Feller et al. [41], who advise organizations to decompose a task 
into  a  number  of  small  tasks  that  conceal  the  overall  picture,  thus  increasing  the 
ability to protect privacy or intellectual property. Roy et al. [42] present another case 
of crowdsourcing sensitive-information tasks on digitizing data from scanned images 
of  insurance  forms.  In  this  case,  the  authors  [42]  describe  a  sequence  of  actions 
“overcoming the security challenges”.  

Finally, the ease with which a task can be partitioned into smaller pieces of work 
also  affects  the  crowdsourcing  decision.  Malone  et  al.  [6],  when  discussing 
crowdsourcing in terms of collective intelligence, suggest the crowd should be used 
for tasks that can be subdivided. Afuah and Tucci [30] noted that “modular problems 
are  particularly  conducive  to  collaboration-based  crowdsourcing”.  This  has  been 
supported by other studies [24, 43].  

People. An organization should consider who performs tasks in term of its available 
employees and the crowd members. Malone et al. [6] suggest choosing crowdsourcing 
when  an  organization  does  not  have  enough  employees  to  deploy  the  tasks.  With 
tasks,  such  as  transcriptions  and  image  labelling,  requiring  significant  human 
resources  that  often  exceed  an  organization’s  capability,  organizations  should 
consider  crowdsourcing  as  an  option.  For  example,  a  recent  project  that  aimed  to 
transcribe  41  diaries  written  over  21,000  days  and  thousands  of  prints  found  that 
“[they]  can’t  do  the  project  with  existing  human  resources”  and  consequently, 
crowdsourcing was a good (if not the only) possibility [44]. Afuah and  Tucci [30] 
agreed  with  this  argument,  but  extended  the  boundary  of  the  organization’s  human 
resources  to  include  outsourcing  contractors.  Consequently,  they  recommend  using 
crowdsourcing if “the knowledge required to solve the problem falls outside the focal 
agent’s knowledge neighbourhood”. 

As key actors in the crowdsourcing system, the nature of the target members will 
influence crowdsourcing decisions [45]. Since some tasks, such as designing T-Shirts 
or writing academic papers [28], require the crowd members to have a certain level of 
skill,  crowd  member  availability  will  influence  the  decision  to  crowdsource.  Both 
Afuah  and  Tucci  [30],  examining  crowdsourcing  contests,  and  Malone  et  al.  [6], 
studying  collective  intelligence,  identify  the  positive  influence  of  the  available 
members,  who  know  how  to  perform  the  tasks,  on  the  crowdsourcing  probability. 
Sharma [31] supports this argument by presenting the skills and abilities of the crowd 
as human capital in her crowdsourcing critical success factor model. 

Management.  Considering  crowdsourcing  as  a  type  of  outsourcing  project,  Rouse 
[11] advises the decision to crowdsource should “only be made” after examining four 
factors.  Besides  the  production  factor,  which  was  discussed  in  the  task  section,  the 
other three factors are: costs, coordination and risks. Cost saving is one main reason 
to choose crowdsourcing [10, 46, 47]. Consequently, the budget of the crowdsourcing 
project  influences  this  decision.  Crowdsourcing  has  been  suggested  when  a  project 
does  not  have  enough  money  to  hire  employees  or  other  companies  to  perform  the 
task  [6].  In  other  words,  project  with  limited  budget  should  be  crowdsourced,  and 
Wikipedia  is  a  typical  example  of  crowdsourcing  a  huge  amount  of  writing  tasks 
within a limited budget. 

However,  crowdsourcing  activities  can  only  succeed  if  organizations  allocate 
appropriate  expertise  and  experience  to  handle  the  coordination  in  these  activities. 
Rouse [11] states that poor coordination can lead the project to the drain of resources 
and substantial delays, while other studies have stressed the importance of expertise 
and management in different parts of the crowdsourcing process, such as workflow 
management [48], members management [49], and agreement management [50].  

Risk  and  risk  management,  as  with  any  project,  should  be  considered  in 
crowdsourcing  activities  [11,  45].  Since  members  of  the  crowd  perform  the  tasks 
voluntarily,  organizations  will  not  have  the  same  level  of  control  over  member 
behaviours as they would have over their own employees [10], and this could lead to 
poor  member  contributions  to  the  project.  Consequently,  the  risk  of  low  quality 
results should be considered. 

Environment. The choice between internal or external platforms plays a role in the 
crowdsourcing  decision.  In  terms  of  cost,  which  is  one  of  the  reasons  to  choose 
crowdsourcing [2, 10, 47], the availability of a crowdsourcing platform can decrease 
the  development  cost,  which  makes  the  decision  to  crowdsource  become  more 
attractive. In addition, since different platforms include different pools of members, 
which relates to the probability of the decision to crowdsource, the availability of the 
platform that is suitable for the defined task is valuable in term of the availability of 
its  members.  For  example,  Amazon  Mechanical  Turk  has  approximately  100,000 
members [51] who can be utilized to address tasks that organizations would otherwise 
struggle with. 

5  Discussion and Suggestions 

Based on the framework, the following implications can be applied for crowdsourcing 
activities.  In  order  to  present  these  implications  in  a  precise  and  compact  way,  the 
chosen presenting technique in this study is decision table. According to Huysmans et 
al.  [52],  decision  table  is  the  best  presenting  technique  in  term  of  interpretability 
compared  to  decision  tree,  propositional  rule,  and  oblique  rule.  The  authors  [52] 
conducted  an  experiment  measuring  the  accuracy,  response  time,  and  answer 
confidence when the participants using the aforementioned presenting techniques for 
problem solving tasks. The results from the experiment show that decision tables help 
the participants “answer the questions faster, more accurately and more confidently”. 

Consequently, recommendations for crowdsource decision-making are presented as a 
series  of  decision  tables.  Each  layer  of  the  framework  is  summarised  as  a  decision 
table, except for the Environment layer, which has only one factor.  

Table 3. Decision table for layer 1: Task Properties 

Condition: Task properties 

Internet 
Interactive 
Sensitive information 
Partitioned 
Action 
Not to crowdsource 
Should crowdsource 
Crowdsource  with  additional  action:  defining 
tasks aiming to hide the sensitive information 
Crowdsource  with  additional  action:  only 
crowdsource as a contest  

 
N 
- 
- 
- 
 
X 
 
 

 

 
Y 
Y 
- 
- 
 
X 
 
 

 

 

Y  Y 
N  N 
N  Y 
N  Y 
 
 
 
 
 
 
 
X 

X 

 

Y 
N 
Y 
N 
 
 
 
X 

X 

Y 
N 
N 
Y 
 
 
X 
 

 

Since task is an important factor in crowdsourcing activities, task properties related 
to  crowdsourcing  decision  were  presented  in  Table  3.  On  the  one  hand,  managers 
should only choose to crowdsource tasks that can be performed through the internet 
[15,  19,  36].  On  the  other  hand,  tasks  which  require  a  significant  level  of 
communication  should  not  be  crowdsourced  [38].  In  addition,  if  tasks  include 
sensitive information or intellectual property, additional actions to hide the sensitive 
information  are  necessary  [41].  Examples  of  these  actions  can  be  found  in  [42]. 
Finally, crowdsourcing is more suitable for tasks, which can be partitioned into small 
pieces  of  work  [6].  One  can  argue  that  many  big  contest  tasks,  which  are  not 
necessarily divisible, can still be crowdsourced using platforms such as Innocentive. 
However, if these tasks can be modularized, “it may be easier for the focal agent to 
articulate  a  module”  [30].  In  other  words,  the  probability  to  accomplish  divided 
contest tasks is higher compared to the same non-divided tasks.  

Table 4. Decision table for layer 2: People 

Condition: People 

The crowd for task: Available (A) vs. Not available (N) 
Employee for task: Few (F) vs. Large (L) 
Action 
Not to crowdsource 
Should crowdsource 
Crowdsource with additional action: consider other factors 

 
A 
F 
 
 
X 
 

N 
- 
 
X 
 
 

A 
L 
 
 
 
X 

Table 4 shows the influence of human resources on the decision to crowdsource. 
Crowdsourcing  tasks  can  only  be  performed  if  the  organization  can  approach  mass 
and suitable members. For simple tasks, the number of crowd members is important, 
while for skilled tasks, the ability of the members is significant. In short, “the constant 

availability of sufficient quantity and quality, of on-line workers” is a requirement for 
crowdsourcing [53]. From the organizational context, when an organization does not 
have enough appropriately skilled labours that are currently possessing by the crowd, 
crowdsourcing is a good option [6]. Finally, if both employees in the organization and 
the crowd members have the ability to perform the tasks, other factors, such as task 
properties, and management factors should be considered. 

The  factors  in  the  Management  layer  were  summarized  in  Table  5.  Some 
organizations, such as Wikipedia, and non-profit organizations  [54], show that they 
can  employ  crowdsourcing  with  little  or  no  money.  Consequently,  crowdsourcing 
should  be  chosen  when  the  fund  allocated  for  tasks  is  not  enough  to  perform  these 
tasks in the traditional way [6]. However, it is worth noting that crowdsourcing also 
needs  good  expertise  and  experience  in  order  to  organize  the  activities  [11].  As  a 
result,  if  a  project  has  limited  budget,  and  limited  or  no  crowdsourcing  expert,  it 
should not be crowdsourced.  

Lack  of  commitment  between  the  organization  and  the  crowd  members  creates 
risks for crowdsourcing activities, including low quality results. In  order to address 
the risk of low quality outcomes, organizations should crowdsource tasks where the 
results  are  easy  to  be  evaluated  [30].  In  addition,  different  mechanisms  that  can  be 
used for control quality have been suggested, including checking results by experts, 
using  members  of  the  crowd  for  evaluating,  and  evaluating  by  a  third  party 
organization [10]. 

Table 5. Decision table for layer 3: Management 

Condition: Management 

Budget: Small (S) vs. Large (L) 
Crowdsourcing  expert:  Available  (A)  vs.  Not 
available (N)  
Acceptance level of low quality result risk: High 
(H) vs. Low (L) 
Action 
Not to crowdsource 
Should crowdsource 
Crowdsource with additional action: hire outside 
experts (due to large budget) 
Crowdsource  with  additional  action:  implement 
mechanisms for quality control 

 
S 
A 

H 

 
 
X 
 

 

 
S 

 
S 

 
L 

 
 
S 
L 
A  N  N  A  A  N  N 

 
L 

 
L 

L  H 

L  H 

L 

H 

L 

 
 
 
 

 
 
X  X 
 
 
 
 

 
 
X 
 

 
 
 
 

 
 
 
 
 
 
X  X 

X 

 

 

 

X 

 

 

Finally,  as  the  lone  environmental  factor,  platform  availability  should  also  be 
evaluated.  Although  many  crowdsourcing  initiatives  can  be  done  by  building  their 
own  platforms,  the  availability  of  a  platform  is  an  important  factor  when 
organizations  decide  to  crowdsource,  especially  for  small  and  medium-sized 
enterprises which have fewer financial resources and lower technical expertise. The 
availability of platforms, in some cases, has a relationship with the availability of the 
crowd members, which is the crucial factor in crowdsourcing decision [6, 30].  

6  Conclusion and Limitations 

Some studies highlighted the importance of factors that need to be considered when 
making a decision to crowdsource. Since most of these studies chose a particular type 
of  crowdsourcing  to  explore  the  factors,  a  broader  view  which  can  be  used  for 
different types of crowdsourcing activities is necessary. Using a structured literature 
review method, this study developed a framework of identified factors related to the 
crowdsourcing  decision,  and  proposed  decision  tables  suggesting  actions  for 
managers when they make the decision. 

There  are  some  potential  improvements  that  can  be  applied  for  this  study.  First, 
since crowdsourcing is a practical decision, discussion related to it can also be found 
from  organizational  presentations,  reports,  website  and  news  media,  such  as  the 
discussion in [44] and [55]. Consequently, future research should extend the scope in 
term of searching sources and keywords. Second, the current study foresees the ability 
to use these factors, not only in the decision to crowdsource, but also to design and 
implement  crowdsourcing.  By  doing  so,  more  factors  related  to  each  phase  in 
crowdsourcing  process  should  be  explored.  The  results  will  enable  a  more 
comprehensive framework to be built, and provide a tool supporting the organization 
to decide on, design and implement crowdsourcing activities. 

Reference  

1.  Howe, J., The rise of crowdsourcing, in Wired magazine2006, Dorsey Press. p. 1-4. 
2.  Whitla,  P.,  Crowdsourcing  and  its  application  in  marketing  activities.  Contemporary 

Management Research, 2009. 5(1). 

3.  Brabham,  D.C.,  Crowdsourcing  as  a  Model  for  Problem  Solving:  An  Introduction  and 
Cases. Convergence: The International Journal of Research into New Media Technologies, 
2008. 14(1): p. 75-90. 

4.  Vukovic, M. and C. Bartolini, Towards a research agenda for enterprise crowdsourcing. 
Leveraging applications of formal methods, verification, and validation, 2010: p. 425-434. 
5.  Andriole,  S.J.,  Business impact of Web 2.0 technologies.  Communications  of  the  ACM, 

2010. 53(12): p. 67-79. 

6.  Malone, T.W., R. Laubacher, and C. Dellarocas, The collective intelligence genome. IEEE 

Engineering Management Review, 2010. 38(3): p. 38. 

7.  Khazankin, R., B. Satzger, and S. Dustdar. Optimized execution of business processes on 
crowdsourcing  platforms.  in  8th  International  Conference  on  Collaborative  Computing: 
Networking, Applications and Worksharing (CollaborateCom’12), IEEE. 2012. Pittsburgh, 
PA  

8.  Zheng,  H.,  D.  Li,  and  W.  Hou,  Task  Design,  Motivation,  and  Participation  in 
Crowdsourcing Contests.  International  Journal  of  Electronic  Commerce,  2011.  15(4):  p. 
57-88. 

9.  Khasraghi,  H.J.  and  M.J.  Tarokh,  Efficient  Business  Process  Reengineering  with 

Crowdsourcing. International Journal of Applied Information Systems, 2012. 2(7). 

10.  Zhao,  Y.  and  Q.  Zhu,  Evaluation on crowdsourcing research: Current status and future 

direction. Information Systems Frontiers, 2012: p. 1-18. 

11.  Rouse,  A.C.  A  preliminary  taxonomy  of  crowdsourcing.  in  ACIS  2010:  Information 
Systems:  Defining  and  Establishing  a  High  Impact  Discipline:  Proceedings  of  the  21st 
Australasian Conference on Information Systems. 2010. ACIS. 

12.  Ranade, G. and L.R. Varshney. To Crowdsource or not to Crowdsource? in Workshops at 

the Twenty-Sixth AAAI Conference on Artificial Intelligence. 2012. 

13.  Kittur,  A.,  et  al.  The Future of Crowd Work.  in  Proceedings of the 2013 conference on 

Computer supported cooperative work. 2013. San Antonio, TX, USA. 

14.  Doan, A., R. Ramakrishnan, and A.Y. Halevy, Crowdsourcing systems on the world-wide 

web. Communications of the ACM, 2011. 54(4): p. 86-96. 

15.  Schenk,  E.  and  C.  Guittard,  Towards  a  characterization  of  crowdsourcing  practices. 

Journal of Innovation Economics, 2011(1): p. 93-107. 

16.  Brabham, D.C., Moving the crowd at threadless. Information, Communication & Society, 

2010. 13(8): p. 1122-1145. 

2009. Los Angeles, CA: IEEE. 

17.  Vukovic,  M.  Crowdsourcing  for  enterprises.  in  Services-I,  2009  World  Conference  on. 

18.  Vukovic, M., M. Lopez, and J. Laredo. Peoplecloud for the globally integrated enterprise. 

in Service-Oriented Computing. ICSOC/ServiceWave 2009 Workshops. 2010. Springer. 

19.  Estellés-Arolas,  E.  and  F.  González-Ladrón-de-Guevara,  Towards  an 

integrated 

crowdsourcing definition. Journal of Information science, 2012. 38(2): p. 189-200. 

20.  Brabham, D.C., Crowdsourcing2013: The MIT Press. 
21.  Buhrmester, M., T. Kwang, and S.D. Gosling, Amazon's Mechanical Turk A New Source 
of  Inexpensive,  Yet  High-Quality,  Data?  Perspectives  on  Psychological  Science,  2011. 
6(1): p. 3-5. 

22.  Brabham, D.C., Motivations for Participation in a Crowdsourcing Application to Improve 
Public  Engagement  in  Transit  Planning.  Journal  of  Applied  Communication  Research, 
2012. 40(3): p. 307-328. 

23.  Wexler,  M.N.,  Reconfiguring  the  sociology  of  the  crowd:  exploring  crowdsourcing. 

International Journal of Sociology and Social Policy, 2011. 31(1/2): p. 6-20. 

24.  Kulkarni,  A.,  M.  Can,  and  B.  Hartmann,  Collaboratively crowdsourcing workflows with 
turkomatic,  in  Proceedings  of  the  ACM  2012  conference  on  Computer  Supported 
Cooperative Work2012, ACM: Seattle, Washington, USA. p. 1003-1012. 

25.  Brabham,  D.C.,  A Model for Leveraging Online Communities.  The  participatory  cultures 

handbook, 2012: p. 120. 

26.  Geiger, D., et al. Managing the crowd: towards a taxonomy of crowdsourcing processes. 
in  Proceedings  of  the  Seventeenth  Americas  Conference  on  Information  Systems.  2011. 
Detroit, Michigan. 

27.  Nickerson, R.C., U. Varshney, and J. Muntermann, A method for taxonomy development 
and  its  application  in  information  systems.  European  Journal  of  Information  Systems, 
2012. 

28.  Tomlinson,  B.,  et  al.,  Massively  distributed  authorship  of  academic  papers,  in  CHI  '12 
Extended Abstracts on Human Factors in Computing Systems2012, ACM: Austin, Texas, 
USA. p. 11-20. 

29.  Buecheler,  T.,  et  al.  Crowdsourcing,  open  innovation  and  collective  intelligence  in  the 
scientific  method:  a  research  agenda  and  operational  framework.  in  Artificial  life  XII. 
Proceedings  of  the  twelfth  international  conference  on  the  synthesis  and  simulation  of 
living systems, Odense, Denmark. 2010. 

30.  Afuah,  A.  and  C.L.  Tucci,  Crowdsourcing as a solution to distant search.  Academy  of 

Management Review, 2012. 37(3): p. 355-375. 

31.  Sharma,  A.,  Crowdsourcing  Critical  Success  Factor  Model:  Strategies  to  Harness  the 

Collective Intelligence of the Crowd. Working paper, 2010. 

32.  Webster,  J.  and  R.T.  Watson,  Analyzing  the  past  to  prepare  for  the  future:  writing  a 

literature review. MIS quarterly, 2002. 26(2): p. xiii-xxiii. 

33.  Geiger, D., et al. Crowdsourcing Information SystemsDefinition,
 Typology, and Design. 
in  Proceedings  of  the  33rd  International  Conference  on  Information  Systems.  2012. 
Association for Information Systems/AIS Electronic Library (AISeL). 

34.  Vicente,  K.J.,  Cognitive work analysis: Toward safe, productive, and healthy computerbased 
work1999: CRC PressI Llc. 

35.  Kazman,  R.  and  H.M.  Chen,  The  metropolis  model  a  new  logic  for  development  of 

crowdsourced systems. Communications of the ACM, 2009. 52(7): p. 76-84. 

36.  Saxton, G.D., O. Oh, and R. Kishore, Rules of crowdsourcing: Models, issues, and systems 

of control. Information Systems Management, 2013. 30(1): p. 2-20. 

37.  Heimerl, K., et al. CommunitySourcing: engaging local crowds to perform expert work via 
physical kiosks. in Proceedings of the 2012 ACM annual conference on Human Factors in 
Computing Systems. 2012. Austin, Texas, USA: ACM. 

38.  Burger-Helmchen, T. and J. Pénin. The limits of crowdsourcing inventive activities: What 
do transaction cost theory and the evolutionary theories of the firm teach us. in Workshop 
on Open Source Innovation, Strasbourg, France. 2010. 

39.  Dow, S., et al. Shepherding the crowd yields better work. in Proceedings of the ACM 2012 
conference on Computer Supported Cooperative Work.  2012.  Seattle,  Washington,  USA: 
ACM. 

40.  Muntés-Mulero,  V.,  et  al.,  Crowdsourcing for industrial problems,  in  Citizen in Sensor 

Networks2013, Springer. p. 6-18. 

41.  Feller,  J.,  et  al.,  ‘Orchestrating’sustainable crowdsourcing: A characterisation of solver 

brokerages. The Journal of Strategic Information Systems, 2012. 

42.  Roy,  S.,  C.  Balamurugan,  and  S.  Gujar.  Sustainable  employment  in  India  by 
crowdsourcing enterprise tasks. in Proceedings of the 3rd ACM Symposium on Computing 
for Development. 2013. ACM. 

43.  Kittur,  A.,  et  al.  Crowdforge: Crowdsourcing complex work.  in  Proceedings of the 24th 

annual ACM symposium on User interface software and technology. 2011. ACM. 

44.  Kingston, A., “Choir attempted that beautiful anthem “Oh, Radiant Morn” – made a hash 
of  it”  -  Making  a  hash  of  the  Adkin  Diary  transcriptions,  2013,  in  Workshop  on 
Crowdsourcing for the Digital Humanities and Cultural Heritage Sector. 

45.  Marjanovic, S., C. Fry, and J. Chataway, Crowdsourcing based business models: In search 

of evidence for innovation 2.0. Science and Public Policy, 2012. 39(3): p. 318-332. 

46.  Rosen,  P.A.,  Crowdsourcing  Lessons  for  Organizations.  Journal  of  Decision  Systems, 

2011. 20(3): p. 309-324. 

47.  Van  Pelt,  C.  and  A.  Sorokin.  Designing  a  scalable  crowdsourcing  platform.  in 

Proceedings of the 2012 international conference on Management of Data. 2012. ACM. 

48.  Potter,  A.,  M.  McClure,  and  K.  Sellers.  Mass  collaboration  problem  solving:  A  new 
approach  to  wicked  problems.  in  Collaborative  Technologies  and  Systems  (CTS),  2010 
International Symposium on. 2010. IEEE. 

49.  Dow,  S.,  et  al.  Shepherding  the  crowd:  managing  and  providing  feedback  to  crowd 
workers.  in  Proceedings  of  the  2011  annual  conference  extended  abstracts  on  Human 
factors in computing systems. 2011. ACM. 

50.  Psaier,  H.,  et  al.  Resource  and  agreement  management  in  dynamic  crowdcomputing 
environments.  in  Enterprise  Distributed  Object  Computing  Conference  (EDOC),  2011 
15th IEEE International. 2011. IEEE. 

51.  Mason,  W.  and  S.  Suri,  Conducting behavioral research on Amazon’s Mechanical Turk. 

Behavior research methods, 2012. 44(1): p. 1-23. 

52.  Huysmans,  J.,  et  al.,  An empirical evaluation of the comprehensibility of decision table, 
tree and rule based predictive models. Decision Support Systems, 2011. 51(1): p. 141-154. 
53.  Corney,  J.,  et  al.,  Putting  the  crowd  to  work  in  a  knowledge-based  factory.  Advanced 

2010. 
 
 

Engineering Informatics, 2010. 24(3): p. 243-250. 

54.  Brabham,  D.C.,  Crowdsourcing  the  Public  Participation  Process  for  Planning  Projects. 

Planning Theory, 2009. 8(3): p. 242-262. 

55.  Holley,  R.,  Crowdsourcing: How and Why Should Libraries Do It?,  in  D-Lib Magazine, 

