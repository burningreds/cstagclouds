SOFTWARE – PRACTICE AND EXPERIENCE
Softw. Pract. Exper. 2012; 42:1165–1192
Published online 15 September 2011 in Wiley Online Library (wileyonlinelibrary.com). DOI: 10.1002/spe.1120

Execution proﬁling blueprints

Alexandre Bergel1,*,†, Felipe Bañados1, Romain Robbes1 and Walter Binder2

1Pleiad Lab, Computer Science Department (DCC), University of Chile, Santiago, Chile

2University of Lugano, Switzerland

SUMMARY

Although traditional approaches to code proﬁling help locate performance bottlenecks, they offer only
limited support for removing these bottlenecks. The main reason is the lack of detailed visual runtime
information to identify and eliminate computation redundancy. We provide three proﬁling blueprints that
help identify and remove performance bottlenecks. The structural distribution blueprint graphically represents 
the CPU consumption share for each method and class of an application. The behavioral distribution
blueprint depicts the distribution of CPU consumption along method invocations and hints at method candidates 
for caching optimizations. The behavioral evolution blueprint compares proﬁles of different versions
of a software system and highlights performance-critical changes in the system. These three blueprints
helped us to signiﬁcantly optimize Mondrian, an open source visualization engine. Our implementation
is freely available for the Pharo development environment and has been evaluated in a number of different
scenarios. Copyright © 2011 John Wiley & Sons, Ltd.

Received 9 December 2010; Revised 11 August 2011; Accepted 14 August 2011

KEY WORDS: Proﬁling; Visualization; Pharo

1. INTRODUCTION

Improving system performance is a permanent focus in research and in industrial software development.
 Although the applicability of static program analysis to identify performance bottlenecks
and hotspots is limited—particularly for software written in dynamic object-oriented languages—
dynamic program analysis allows gathering detailed execution statistics for individual program
executions. Proﬁling techniques are widely used for collecting dynamic metrics to locate hotspots
that are the target for subsequent program optimizations.

However, whereas the proﬁles produced by many prevailing proﬁlers offer detailed execution
time statistics for individual methods, they often fail to explain the reason why a certain method is
slow, for example, which particular method arguments cause an expensive computation.For instance,
gprof, which appeared in 1982, offers a number of textual reports focused on ‘how much time was
spent executing directly in each function’ and on call graphs.‡ Almost 30 years later, it is interesting
to notice that this output has not evolved much. Consider JProﬁler, an award winning proﬁler for
Java that is actively developed. JProﬁler essentially produces the same output, using a graphical rendering 
instead of a textual one.§ Whereas JProﬁler uses tremendously more sophisticated proﬁling
techniques than its predecessors, it still considers the output of its proﬁling activity as a mere tree
widget to indicate the CPU consumption. It appears that a large part of the research conducted in

*Correspondence to: Alexandre Bergel, Pleiad Lab, Computer Science Department (DCC), University of Chile, Santiago,
Chile.
†E-mail: abergel@dcc.uchile.cl
‡http://sourceware.org/binutils/docs/gprof/Output.html#Output
§http://www.ej-technologies.com/products/jproﬁler/screenshots.html

Copyright © 2011 John Wiley & Sons, Ltd.

1166

A. BERGEL ET AL.

the ﬁeld of code proﬁling focuses on reducing the overhead triggered by the code instrumentation
and observation [1–3].

The set of dynamic metrics and visualizations used to proﬁle object-oriented applications are very
similar to those used to proﬁle procedural applications. When we retrospectively look at the history
of execution proﬁlers, we see that tool usability and proﬁling overhead reduction have steadily
improved, but the offered visualizations have not changed much. Prevailing proﬁle visualizations
do not properly address the speciﬁc structure of object-oriented software. Dynamic metrics are presented 
in a similar form when proﬁling an application written in Java or in C. In both cases, the call
frame is the main element of the generated proﬁles.

Tracing application performance across different versions of an evolving application is another
weak point in state of the art execution proﬁlers. Proﬁles are usually gathered for a given snapshot
of a program. Questions such as ‘Which method of a particular software version is faster than in
the current version?’ can hardly be answered with current proﬁlers, without resorting to manual
screening and differentiation of the proﬁles. Most proﬁlers do not offer any dedicated support for
comparing proﬁles (e.g., hprof, ¶ JProbe||).

In this article, we apply some visualizations that have been previously used in static software
analysis to display dynamic metrics for proﬁling purposes. We describe visualizations for rendering 
dynamic information that effectively enables comparison of different metrics related to a
program execution. Structural distribution blueprint and behavioral distribution blueprint are two
visualizations intended to identify bottlenecks and to give hints on how to remove them. The ﬁrst
blueprint represents the distribution of the CPU effort along the program structure. The second
blueprint directs the distribution along method invocations and identiﬁes methods prone to one class
of optimization, namely caching.

This article builds on our previously published work on blueprint proﬁling [4]. Since then, we
have constantly used our blueprints to address performance issues in our software development
activities. Our experience made us realize that exploiting the history of a software plays an important 
role in understanding performance of a particular software version. Identifying where the
speedup gained from a particular optimization in the past got lost in subsequent software versions
requires considering the evolution of the source code. We address this issue with the new behavioral
evolution blueprint, which is the original scientiﬁc contribution of this article.

The work presented in this article aims at complementing existing proﬁlers with new visualizations 
that help speciﬁc optimization tasks. We obtained the results presented in this article
using Pharo,** an open-source Smalltalk-dialect programming language. We apply our proﬁling
techniques to the visualization framework Mondrian†† [5], our running example.

Intuitively, the blueprint proﬁling techniques presented in this article may be applied to applications 
written in any other object-oriented programming language as well. However, the strategy we
used to draw our conclusion is characterized as idiographic [6], meaning that we focus only on the
phenomenon in the context of Pharo and the applications in which we found performance issues.
The points we ﬁx in our work are Pharo, its scheduling strategy and the size of the applications we
considered for our experiment (topping at 200 classes and 2000 methods). Even though proﬁling
long-running applications, including large scale servers, is indeed a major challenge [7], our work
focuses on medium-sized applications ((cid:2) 20,000 lines of code) with rather short proﬁling execution
time (less than 1 min).

The proﬁler providing the visualizations presented in this article is publicly available in Pharo‡‡

under the MIT license.

Note that the structural distribution blueprint is grayscale only; the behavioral distribution
blueprint and behavioral evolution blueprint use colors, although grayscale equivalents are provided
in this paper.

¶http://java.sun.com/developer/technicalArticles/Programming/HPROF.html
||http://www.quest.com/jprobe
**http://www.pharo-project.org/home
††http://www.moosetechnology.org/tools/mondrian
‡‡http://www.squeaksource.com/Spy.html

Copyright © 2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2012; 42:1165–1192
DOI: 10.1002/spe

EXECUTION PROFILING BLUEPRINTS

1167

This article is structured as follows. We ﬁrst describe the structural and behavioral distribution
blueprints (Section 2). We then identify and implement opportunities for optimization in Mondrian
(Section 3). Sections 2 and 3 are taken from our previous publication [4]. Next, we investigate factors
impacting the performance evolution of two systems, Mondrian and GitFS, along their development
history (Section 4) and discuss the experience we gained (Section 5); this part constitutes the novel
contribution of this article. We then review related work (Section 6) and conclude (Section 7).

2. PROFILING BLUEPRINTS

2.1. Proﬁling blueprint in a nutshell

Time proﬁling blueprints are graphical representations meant to help programmers (i) assess the
time distribution and (ii) identify bottlenecks, giving hints on how to remove them for a given program 
execution. The essence of proﬁling blueprints is to enable a better comparison of elements
constituting the program structure and its dynamic behavior. To render information, these blueprints
use a graph metaphor, composed of nodes and edges.

The size of a node hints at its importance in the execution. In the case that nodes represent methods,
 a large node may say that the program execution spends ‘a lot of time’ in this method. The
expression ‘a lot of time’ is then quantiﬁed by visually comparing the height and/or the width of the
node against other nodes.

Color is used to either transmit a property (e.g., a yellow node represents a method that always
returns the same value) or a metric (e.g., a color gradient is mapped to the number of times a method
has been invoked).

We propose two blueprints that help identify opportunities for code optimization. They provide
hints to programmers to refactor their program along the following two principles: (i) make oftenused 
methods faster and (ii) call slow methods less often. The metrics we adopted in this article help
ﬁnding methods that are either unlikely to perform a side effect or return always the same result,
good candidates for simple caching optimizations.

2.2. Polymetric views

The blueprints we propose are graphically rendered as polymetric views [8]. A polymetric view is a
lightweight software visualization enriched with software metrics. It has been successfully used to
provide ‘software maps’ intended to help software comprehension and reverse engineering. Figure 1
depicts the general aspect of a polymetric view.

Given two-dimensional nodes representing entities, we can map up to ﬁve metrics on the node

characteristics: position (X and Y ), size (width and height), and color.

(cid:3) Position. The X and Y coordinates of the position of a node may reﬂect two measurements.
(cid:3) Size. The width and height of a node can render two measurements. We follow the intuitive

notion that the wider and the higher the node, the larger the associated metric.

width property

color

property

height
property

edge width and 
color properties

X property

Y

property

Figure 1. Principle of polymetric view.

Copyright © 2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2012; 42:1165–1192
DOI: 10.1002/spe

1168

A. BERGEL ET AL.

(cid:3) Color. The color interval between white and black may render one measurement. The convention 
that is usually adopted [9] is that the higher the measurement, the darker the node. Thus,
light gray represents a smaller measurement than dark gray.

Edges may also render properties along a number of dimensions (width, color, direction, etc.).

However, for the purpose of this work, all edges are identical.

2.3. Structural distribution blueprint

The execution of an object-oriented program yields a large amount of information [10] (e.g., number
of objects created at runtime, total execution time of a method). Unfortunately, all these dimensions
cannot be visually rendered in a meaningful fashion. The structural distribution blueprint displays
a selected number of metrics indicating the distribution of the execution time along the static structure 
of a program (i.e., classes, methods and class hierarchy). Table I gives the speciﬁcation of
the structural distribution blueprint. The blueprint renders a program in terms of classes, methods
and inheritance relations. Each method representation exhibits its corresponding CPU time proﬁling
information along three metrics:

on. Because of implementation limitations, this is at the moment a lower bound estimate.

(cid:3) Number of different receivers: amount of different object receivers the method has been invoked
(cid:3) Total execution time of a method: time for which a call frame corresponding to the method
is present on the stack at runtime. The precision depends on the underlining proﬁler used to
collect runtime information.
(cid:3) Number of executions: number of times the method has been executed, independently of the

object receiver.

Actual metric values, and additional information, are accessible through a contextual popup

window.

Example. Throughout this article, we use the graph visualization framework Mondrian as a case
study. The blueprints described in this article are also rendered using Mondrian. An example of the
structural distribution blueprint is given in Figure 2. Four classes are represented: MOGraphElement,
 MOViewRenderer, MONode, and MORoot. This ﬁgure is a small part of a bigger picture
obtained by evaluating the following code snippet, which renders a simple visualization of 100
nodes, each containing 100 nodes:

Table I. Speciﬁcation of the structural distribution blueprint.

Structural distribution blueprint

Scope
Edge
Layout

Full system execution time
Class inheritance (upper is superclass of below)
Tree layout for outer nodes and grid layout for inner nodes
(inner nodes are ordered by increasing height)
Linear (except for node width, which is logarithmic)
Outer node is a class, an inner node is a method
Number of different receivers

Metric scale
Node
Inner node color
Inner node height Total execution time of a method
Inner node width Number of executions (logarithmic scale)
Example

Figure 2

Copyright © 2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2012; 42:1165–1192
DOI: 10.1002/spe

EXECUTION PROFILING BLUEPRINTS

1169

shapeBoundsAt:

ifPresent:

bounds

translateTo:

legend for methods

# executions

execution 

time

(color)

#different 
receiver

Figure 2. Example of a structural blueprint.

The code being proﬁled is indicated using a bold font in the example source code. As the last line
shows, this particular proﬁling is instantiated on one package, Mondrian in our case, which contains 
the entire application; it is possible to be more—or less—selective if needed. MOGraphElement 
inherits from MONode, MORoot from MOGraphElement, and MOViewRenderer
from Object. Because Object does not belong to Mondrian (but to the Kernel package), it is
not rendered in the blueprint.

The height of a method node is proportional to the total execution time taken by the method (e.g.,
53% of the code execution is spent in the method #applyLayout and 38% in #bounds). The
width is proportional to the number of times the method has been executed. A logarithmic scale
is used. The method node color represents the number of different objects this method has been
executed on (more than 3732). The scope of the blueprint is global, which means that the darkest
method corresponds to the method that has been executed on the greatest number of object receivers,
system-wide.

The view is interactive; moving the mouse over a method node pops up additional contextual
information. In the example, the contextual window says that the method #applyLayout deﬁned
in the class MOGraphElement has been executed 10,100 times and has been executed on more
than 3732 distinct receiver objects (i.e., instances of MOGraphElement or one of its subclasses).
It also indicates that this method always returns the same value for a given object receiver. Although
the blueprint emphasizes the three metrics indicated previously (execution time, number of invocations,
 number of receivers), the contextual information provides useful data when one wants to
know more about a particular method.

Within a class, methods are ordered along their height; this helps to quickly spot the costlier
methods. For example, it is clear that among MOGraphElement’s methods, three are dominating
with respect to execution time.

Interpretation. Classes represented in Figure 2 illustrate part of a scenario that totals 11 classes.
Among the 111 classes that deﬁne Mondrian, these 11 classes are the only classes involved in
the code snippet execution given previously. Only classes that are covered by the execution, even
partially, are depicted in the blueprint.

Copyright © 2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2012; 42:1165–1192
DOI: 10.1002/spe

1170

A. BERGEL ET AL.

MOGraphElement contains ‘many large and dark’ methods. This indicates that this class is
central to the code snippet execution: these large and black methods consume much CPU time and
are invoked on many different instances. Almost all of MOGraphElement’s methods are executed
a large number of times: in the visualization, they are quite wide compared with methods in other
classes. For most of them, this is not a problem because they are thin and horizontal: even if these
methods are executed many times, they do not consume CPU time. On the left of #applyLayout
stands the #bounds method. This method takes 38% of the CPU time and is invoked 70,201 times
on more than 3732 object receivers. The third costliest method on MOGraphElement, #shape-
BoundsAt:ifPresent:, takes 33% of the CPU time. MONode contains a black and relatively
large method: MONode» translateTo: consumes 22% of the total CPU time. The method has
been invoked 10,100 times on at least 3732 receivers.

Comparing with MOGraphElement and MONode, we ﬁnd that other classes are not involved
in the computation as much. The representation of MOViewRenderer quickly says that its methods 
are invoked a few times without consuming much CPU. Moreover, methods are white, which
tells that they are invoked on few instances only. The contextual information obtained by moving
the mouse over the methods reveals that these methods are executed on a unique receiver. This is not
surprising because only one instance of MOViewRenderer is created in the code example given
previously.

MORoot also does not seem to be the cause of a bottleneck at runtime. The few methods of
this class are not frequently executed because they are relatively narrow. MORoot also deﬁnes a
method #applyLayout. This method is the tall, thin and white method. The contextual information
reveals that this method is executed once and on one object only. It consumes 97% of the execution
time; this means that the CPU spent 97% of the time in the method #applyLayout or in one of the
methods recursively called by #applyLayout. We can safely conclude that this method is the entry
point of the computation. The method MORoot» applyLayout invokes MOGraphElement»
applyLayout on each of the nodes. The relation between these two #applyLayout methods is
indicated by a ﬂy-by-highlighting (not represented in the picture) and the behavioral distribution
blueprint, described in the following.

All in all, a large piece of the total CPU time is distributed over four methods: MON-
ode»translateTo: (24%), MOGraphElement»bounds (32%), MOGraphElement» shape-
BoundsAt:ifPresent: (33%), and MOGraphElement»applyLayout (53%). Note that at this
stage, we cannot say that the CPU time share of these three methods is the sum of their individual
share. We have 24C 32C 33C 53 D 142. This indicates that some of these methods call each other
because their sum cannot exceed 100% otherwise.

2.4. Behavioral distribution blueprint

In a pure object-oriented setting, computation is essentially performed through message sending
between objects. The CPU time consumption is distributed along method executions. Assessing the
runtime distribution along method invocations complements the structural assessment described in
the previous section. To reﬂect this proﬁling along method invocations, we provide the behavioral
distribution blueprint. Table II gives the speciﬁcation of the ﬁgure.

The goal of this blueprint is to assess runtime information alongside method call invocations.
It is intended to ﬁnd optimization opportunities that may be tackled with caching. In addition to
the metrics such as the number of calls and execution time, we also show whether a given method
returns constant values and whether it is likely to perform a side effect or not. As shown later, this
information is helpful to identify a class of bottlenecks related to caching opportunities.

Classes do not appear on this blueprint; methods are represented by nodes and invocations by
directed edges. The behavioral blueprint uses the two metrics described in the structural blueprint
for the width and height of a method. In addition to the shape, node color indicates a property:

(cid:3) The gray color indicates methods that return self, the default return value. When no return
value is speciﬁed in Pharo, the object receiver is returned. This corresponds to void methods
in a statically typed language. No result is expected from the method, strongly suggesting that
the method operates via side effects.

Copyright © 2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2012; 42:1165–1192
DOI: 10.1002/spe

EXECUTION PROFILING BLUEPRINTS

1171

Table II. Speciﬁcation of the behavioral distribution blueprint.

Scope

Edge

Layout
Metric scale
Nodes

Node color

Node height
Node width
Example

Behavioral distribution blueprint

All methods directly or indirectly invoked for a given
starting method
Method invocation (upper methods invoke lower
ones)
Tree layout
Linear (except for node width)
Methods

Gray, return always self; yellow, same return value
per object receiver; white, remaining methods
Total execution time
Number of executions (logarithmic scale)
Figure 3

(cid:3) The yellow color (which appears as light gray on a black and white printout) indicates methods
(cid:3) Other methods are white.

that are constant on their return value, this value being different from self.

Keeping track of the returned value in such a way is one of the characteristics of this visualization

that supports us in our goal of identifying a class of bottlenecks.

A tree layout is used to order methods, with upper methods calling lower methods. We illustrate 
this blueprint on the MOGraphElement» bounds method that we previously identiﬁed as
a candidate for optimization.

Example. In the structural blueprint (Figure 2), right-clicking on the method MORoot» applyLayout 
opens a behavioral distribution blueprint for this method. The complete picture is given in
Figure 3; it should be read in a top-down fashion. Methods in this blueprint have the same dimensions 
as in the behavioral blueprint. We recognize the tall and thin MORoot» applyLayout at the
top. All methods in Figure 3 are therefore invoked directly or indirectly by MORoot » applyLayout.
 MORoot» applyLayout invokes three methods, including MOGraphElement» applyLayout 
(labelled in the ﬁgure). MOGraphElement» applyLayout calls MOAbstractLayout»
applyOn:, and both of these are called by MORoot» applyLayout.

Interpretation. As the ﬁrst blueprint revealed, #bounds, #applyLayout, and #shape-
BoundsAt:ifPresent:, #translateTo: are expensive in terms of CPU time consumption. The
behavioral blueprint highlights this fact from a different point of view, along method invocations.
In the following, we will optimize #bounds by identifying the reason of its high cost and provide
a solution to ﬁx it. Our experience with Mondrian tells us that this method has a surprisingly high
cost. Where to start a refactoring among all potential candidates remains the programmer’s task. Our
blueprint only says ‘how it is’ and not ‘how it should be’; however, it is a rich source of indication
of the events occurring at runtime.

The return value of MOGraphElement» bounds is constant over time; hence, it is painted
in yellow. This method is involved in a rich invocation graph (presented in Figure 3). In general,
understanding the interaction of a single method is likely to be difﬁcult when a complete call graph
is used; the contextual menu obtained by right-clicking on a method offers a ﬁltered view on the
entity of interest.

Figure 4 shows a detailed view of the previous behavioral blueprint, focused on MOGraphEle-
ment» bounds. This method is at the center of the picture: located above are the methods calling
#bounds, and located below is the unique method that is called by #bounds. Among the ﬁve
methods that call #bounds, three always return the same value when executed. The method called
by #bounds also remains constant on its return value. Figure 4 renders #bounds and #shape-
BoundsAt:ifPresent: with the same width. It is therefore likely that these two methods are invoked
the same number of times. The contextual window indicates that each of these two methods is indeed
invoked 70,201 times. We can deduce the following:

Copyright © 2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2012; 42:1165–1192
DOI: 10.1002/spe

1172

A. BERGEL ET AL.

MORoot>>applyLayout

MOGraphElement>>

applyLayout

MOAbstractLayout>>applyOn:

legend for methods

# executions

execution 

time

gray = 
return 

self

yellow = 
constant 
on return 

value

m1

m2

m3
m1 

invokes 

m2 and m3

MONode>>
translateTo:

bounds
shapeBoundsAt:

ifPresent:

Figure 3. Example of a behavioral blueprint.

computeExtentHavingChildrenFor:

Calling  #bounds

MOGraphElement>>

origin

bounds

Called by  #bounds

shapeBoundsAt:ifPresent:

Figure 4. Detailed view of MOGraphElement» bounds.

value. This is indicated in the upper part of Figure 4.

(cid:3) #bounds belongs to several execution paths in which each method is constant on its return
(cid:3) #bounds calls #shapeBoundsAt:ifPresent:, which is constant on return value.
(cid:3) #bounds and #shapeBoundsAt:ifPresent: are invoked the same number of times.

Copyright © 2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2012; 42:1165–1192
DOI: 10.1002/spe

EXECUTION PROFILING BLUEPRINTS

1173

This set of characteristics—large amount of invocations in multiple execution paths, constant per
receiver return value, and calling a constant-returning method—strongly suggests a caching opportunity.
 The following section addresses this bottleneck by adding a cache in #bounds and unveils
another bottleneck in Mondrian.

3. OPTIMIZING MONDRIAN

The combination of the structural and behavioral blueprints helped us to identify a number of bottlenecks 
in Mondrian. In this section, we address some of these bottlenecks by using memoization,§§
that is, we cache values to avoid redundant computations.

3.1. Bottleneck MOGraphElement»bounds
As we saw earlier, the behavioral blueprint on the method MOGraphElement» bounds reveals
a number of facts about the program’s execution. These facts are good hints that #bounds will
beneﬁt from a caching mechanism because it always returns the same value and calls a method that
is also constant. We inspect its source code:

The code source conﬁrms that #shapeBoundsAt:ifPresent: is invoked once each time
#bounds is invoked. The method #shape is also invoked at each invocation of #bounds. The
contextual window obtained in the structural blueprint reveals that the return value of #shape
is constant: it is a simple variable accessor (‘getter’ method), so it is fast. #bounds calls #com-
puteBoundsFor: and #shapeBoundsAt:put: in addition to #shapeBoundsAt:ifPresent: and
#shape. However, they do not appear in Figures 3 and 4. This means that #bounds exits before
reaching #computeBoundsFor:. The block [ :b | ˆ b], which has the effect of exiting the method,
is therefore always executed in the considered example.

We ﬁrst thought that the last three lines of #bounds may be removed because they are not executed 
in our scenario. However, when subsequently validating our change, the large number of tests
in Mondrian indicated that these lines are indeed important in other scenarios, although not in our
particular example.

We elected to upgrade #bounds with a simple cache mechanism. Differences with the original
version are indicated using a bold font. The class MOGraphElement is extended with a new
instance variable, #boundsCache. In addition, the cache variable has to be reset in ﬁve methods
related to graphical bounds manipulation of nodes, such as translating and resizing.

§§http://www.tfeb.org/lisp/hax.html#MEMOIZE

Copyright © 2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2012; 42:1165–1192
DOI: 10.1002/spe

1174

A. BERGEL ET AL.

There is no risk of concurrent accesses of #boundsCache because this variable is set when the
layout is being computed. This occurs before the display of the visualization, which is carried out
in a different thread.

Result. Adding a statement boundsCache ifNotNil: [ ˆ boundsCache] signiﬁcantly reduces
the execution time of the code given in Section 2.3. Before adding this simple cache mechanism,
the code took 430 ms to execute (on a MacBook Pro, 2 Gb of RAM (1067 MHz DDR3), 2.26 GHz
Intel Core 2 Duo, Squeak VM 4.2.1beta1U). With the cache, the same execution takes 242 ms only,
which represents a speedup of approximately 43%.

This gain is reﬂected on the overall distribution of the computational effort. Figure 5 provides
two structural blueprints of the code snippet given in Section 2.3. The comparison between the two

Upgrading 

MOGraphElement>>bounds

A

B

C

Figure 5. Upgrading #bounds has a global structural impact.

Copyright © 2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2012; 42:1165–1192
DOI: 10.1002/spe

EXECUTION PROFILING BLUEPRINTS

1175

A

Upgrading 

MOGraphElement>>bounds

B

Figure 6. Upgrading #bounds has a global behavioral impact.

versions is carried out ‘manually’; in Section 4, we will introduce a new visualization meant to support 
such comparison systematically. The upper blueprint has been produced before upgrading the
method MOGraphElement» bounds. Figure 2 is a part of it. The lower one has been produced
after upgrading #bounds as described previously. Many places are impacted. We annotated the
ﬁgure with the most signiﬁcant changes:

(cid:3) The size of the #bounds method and the methods invoked by it (C) have seen their
height signiﬁcantly reduced. Before the optimization, #bounds used 38% of the total CPU
consumption. After the optimization, its CPU use fell to 5%.
(cid:3) The ﬁve methods denoted by the circle A and B have seen their height increased and their color
darkened. The height increase illustrates the augmentation in relative CPU consumption these
methods are subject to, now that #bounds has been improved.

The evolution of the behavioral blueprint is presented in Figure 6. We can clearly see the
reduced size of #bounds and #shapeBoundsAt:ifPresent: (Circle B) and the increase of the
#applyLayout method (Circle A).

3.2. Bottleneck in MONode»displayOn:

We ﬁxed an important bottleneck when computing bounds in Mondrian. We push our analysis of
bounds computing a step further. We inspect the user interface (UI) thread of Mondrian. Most applications 
with a graphical UI run in at least two threads: one for the program logic and another in
charge of receiving user events (e.g., keystrokes, mouse events) and virtual machine/OS events (e.g.,
window refreshes); Mondrian is no exception. The blueprints presented earlier focused on proﬁling
the application logic.

Step 1. Figure 7 shows the structural proﬁling of the UI thread for the Mondrian script given in
Section 2.3. The blueprint contains many large methods, indicating methods that received a significant 
CPU share. Among these, our knowledge of Mondrian leads us to #absoluteBounds. This
method is very similar to #bounds that we previously saw. It returns the bounds of a node using
absolute coordinates (instead of relative ones). The UI thread spends most of the time in MONode»
displayOn: because it is the root of the thread’s computation.

Figure 8 shows the behavioral blueprint opened on MONode» displayOn:. The blueprint
reveals that #absoluteBounds and #absoluteBoundsFor: call each other. Return values of

Copyright © 2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2012; 42:1165–1192
DOI: 10.1002/spe

1176

A. BERGEL ET AL.

absoluteBounds

shapeBoundsAt:

ifPresent:

displayOn:

display:on:

absoluteBoundsFor:

legend for methods

# executions

execution 

time

(color)

#different 
receiver

Figure 7. Proﬁling of the user interface thread in Mondrian.

these two methods are constant as indicated by their yellow color. They are therefore good
candidates for caching:

Result. Without the cache in #absoluteBounds, the scenario takes 356 ms to run. With
the cache, it takes 231 ms. We therefore decreased running time by 35% when displaying the
visualization.

Step 2. By adding the cache in #absoluteBounds, we signiﬁcantly reduced the cost of this
method. We can still do better. As shown in Figure 8, there is another caller of #absoluteBounds.
MORectangleShape» display:on: is 85 lines long and begins with the following:

We saw in Step 1 that #absoluteBounds calls the expensive and uncached #absolute-
BoundsFor:. Replacing the call to #absoluteBoundsFor: by #absoluteBounds improves
performance further:

Copyright © 2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2012; 42:1165–1192
DOI: 10.1002/spe

EXECUTION PROFILING BLUEPRINTS

1177

MONode>>displayOn:

MORectangleShape>>

display:on:

MOShape>>

absoluteBoundsFor:

MOGraphElement>>

absoluteBounds

legend for methods

# executions

execution

time

gray = 
return
self

yellow = 
constant
on return 

value

m1

m2

m3
m1

invokes

m2 and m3

Figure 8. Proﬁling of the user interface thread in Mondrian.

B

A

C

D

A'

C'

B'

C'

Figure 9. Proﬁling of the user interface thread in Mondrian.

Result. The execution time of the code snippet has been reduced to 198 ms. A speedup of 14%

from Step 1 and of 45% overall.

Blueprint evolution. Figure 9 summarizes the two evolution steps described previously. Differences 
with a previous step are denoted using a circle. The effect of caching #absoluteBounds 
considerably diminished the execution time of this method. This is illustrated by

Copyright © 2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2012; 42:1165–1192
DOI: 10.1002/spe

1178

A. BERGEL ET AL.

Circle C. It has also the effect of reducing the size of MOShape’s methods and increasing—
relatively—MORectangleShape»display:on:. The share of the CPU consumption increased for
this method. Step 2 reduced the size of several of MOShape’s methods. Their execution times
became so small that they do not appear in the behavioral blueprint (because we use a samplingbased 
proﬁler to obtain the runtime information, methods having less than 1% of the CPU do not
appear in this blueprint).

3.3. Summary
The cache value of MOGraphElement» bounds (Section 3.1) is implemented and has been
ﬁnalized in version 341 of Mondrian.¶¶ The improvement of #absoluteBounds and #display:on:
may be found in version 352 of Mondrian. The complete experiment led to a 43% improvement in
creating the layout of a view and of 45% in displaying the same view.

We identify and remove a number of bottlenecks. From this experience, it is tempting to identify
and look after some general patterns that would easily expose ﬁxable execution bottleneck. Unfortunately,
 we have not seen the opportunity to deduce some general rules. The visualization we provide
clearly identiﬁes costly methods and classes, potentially being candidates for optimization. Whether
the optimization can be easily realized or not depends heavily on a wide range of parameters (e.g.,
algorithm, architecture, data structure), best assessed by the developer.

4. BEHAVIORAL EVOLUTION BLUEPRINT

We have presented proﬁling blueprints as a graphical representation of a software execution for a
particular version of the software. Although useful to identify execution bottlenecks, comparing two
or more versions becomes hard as it requires an exhaustive manual check for every software entity.
Focusing on differences between two versions instead of the actual values is the topic of this section.
To compare the proﬁles over two software versions, we propose the behavioral evolution
blueprint. This additional behavioral blueprint depicts the method call graph in which nodes are
methods and edges invocations. The height of a node is the share of the total execution time taken
by the represented method. The width shows the number of times the method has been invoked, on
the basis of a logarithmic scale. The differences between methods are usually so great that a linear
scale is not effective. Colors—or grayscale equivalents in this version—convey the difference of a
metric over different software versions, and border thickness refers to changes in the source code.
This blueprint can be used to graphically validate when a source change improves the performance
of the code. The speciﬁcation of this visualization is given in Table III.

Figure 10 exempliﬁes the behavioral evolution blueprint on the method MORoot » applyLayout 
for two versions of Mondrian, versions 416 and 425. The proﬁling is realized on version 425,
and colors show the result of a comparison with a similar proﬁling based on a previous version, 416
in this case. Each called method is colored as so:

execution time has decreased.

the previous software version and its total execution time decreased.

(cid:3) Green (light gray, thick borders in grayscale): the method source code has been modiﬁed from
(cid:3) Light green (light gray, thin borders): the method’s source code is unchanged, but its total
(cid:3) Yellow (black): the method was not present in the previous software version but is present in
(cid:3) Red (dark gray, thick borders): the method source code has been modiﬁed, and its total
(cid:3) Light red (dark gray, thin borders): the method is unchanged, but its execution time increased.
(cid:3) White: the execution time remains constant.
The red and light red colors quickly identify the methods that are slower in the proﬁled version
than they previously were. Green and light green colors identify methods that are faster in the new

the current one.

execution time has increased.

¶¶The source code is available at http://www.squeaksource.com/Mondrian.html

Copyright © 2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2012; 42:1165–1192
DOI: 10.1002/spe

EXECUTION PROFILING BLUEPRINTS

1179

Table III. Speciﬁcation of the behavioral evolution blueprint.

Behavioral evolution blueprint

Scope

Edge
Layout
Metric scale
Nodes
Node color

Node height
Node width
Node border thickness

Example

All methods directly or indirectly invoked for a given
starting method
Method invocation (upper methods invoke lower ones)
Tree layout
Linear (except for node width)
Methods
Green (light gray in B&W), method execution time is
less than previous version; red (dark gray in B&W),
method execution time is greater than previous version;
yellow (black in B&W), method was not implemented
in previous version; white, method execution time is
identical.
Total execution time
Number of executions (logarithmic scale)
Thin, source code identical; thick, source code is different 
than previous version
Figure 10

version. However, the performance difference might not be directly related to the particular method,
as it might be a side effect of changes in other methods being called. Therefore, we need to distinguish 
methods whose source code and performance has changed, which are the candidates to cause
the overall performance difference. The use of strong and light colors—complemented by thick and
thin borders—helps highlighting these particularly interesting methods. A strong color is assigned
when the source code is different from its previous version. Light colors mean the source code is
the same. In the case of yellow, as the method is not part of the call graph of the previous version,
there is no need for a light color property. In the remainder of the paper, we refer only to the black
and white equivalents.

Figure 10. Behavioral evolution blueprint for versions 416 and 425 of Mondrian.

Copyright © 2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2012; 42:1165–1192
DOI: 10.1002/spe

1180

A. BERGEL ET AL.

The depicted blueprint (Figure 10) is obtained from the following steps:

1. Proﬁle a piece of code for version 416 of Mondrian (we used the code snippet given in
Section 2.3 for the ﬁgure) and store source code information for every method reached by
the proﬁle.

2. Repeat step (1) for a second proﬁle of the same code snippet for version 425 of Mondrian.
3. For each method covered in version 425, we subtract its total execution time with the metric

obtained when proﬁling version 416.

4. For every method in step (3) we compare source code versions for changes.
5. We generate the blueprint using the call graph obtained from the execution of the piece of code

in version 425.

Only the methods that are deﬁned in version 425 are represented. Methods that are present in
version 425 but not in version 416 are indicated in yellow. Methods not present in version 425 are
not used anymore and thus simply not represented.

As the source code of MORoot » applyLayout does not change between both versions but its
performance in terms of execution time does, we want to use the behavioral evolution blueprint in
Figure 10 to trace the origin of this change.

Searching for the origin of the change. The coloring in the blueprint is helpful to ﬁnd the main
source of the change in performance. As seen in Figure 10, because the root node border is thin, the
source code of MORoot » applyLayout is the same in version 425 than in version 416. However,
its execution time has increased, as its light red color shows.

The ﬁrst thick bordered node in the second branch of the children of the original method corresponds 
to the method MOGraphElement » translateBy:bounded:. The version 425 of this
method is faster than in version 416, as the green color indicates. The method name—along with
the current displayed version and metric values—is available in a contextual popup window when
the mouse hovers upon the method. Upon browsing the source code of the method, we see that the
slowdown is due to moving the call to #resetCacheInEdges above in the method:

Revisiting the Mondrian optimizations. We used the behavioral evolution blueprint to compare 
the performance of Mondrian before and after the cache implementation for the bottlenecks
discussed in Sections 3.1 and 3.2. We can use the visualization to track the performance improvements 
directly to the cache implementation changes. From a global overview, we found that the
performance in MOEdge» displayOn: improved by almost 20% between versions 300 and 352,
while its source code did not change. As seen in Figure 11, that improvement is trackable to an
over 300% improvement in MOGraphElement» absoluteBounds, which coincides with the
implementation of Section 3.2.

The optimization described in the previous sections are part of a major effort to optimize
Mondrian. This effort started after version 200. To assess our optimizations on the long term, we

Copyright © 2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2012; 42:1165–1192
DOI: 10.1002/spe

EXECUTION PROFILING BLUEPRINTS

1181

Figure 11. Behavioral evolution blueprint of the user interface thread over versions 300 and 352, rooted in

MOEdge» displayOn:.

use version 200 as the referential. When compared with version 352, we identify an optimization
implemented somewhere between version 200 and 352: method MORoot» applyLayout is faster
in version 352, and its source code has slightly changed (some methods have been renamed), changing 
from the version on the left hand side to the one on the right hand side (bold font indicates
differences):

With the behavioral evolution blueprint in Figure 12, only three methods remain candidates to the
source of the improvement. First, MOAbstractLayout » initializeConnectionPositions, but
its execution time is too small to cause the global improvement. Second, MORectangleShape
» computingExtentHavingChildrenFor and third MONode» translateTo. The third method
decreases from 64 to 1 ms. Its source code was changed to include memoization, hence calling less

Copyright © 2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2012; 42:1165–1192
DOI: 10.1002/spe

1182

A. BERGEL ET AL.

Figure 12. Behavioral evolution blueprint over versions 200 and 352 rooted in MORoot» applyLayout.

times MOGraphElement» bounds—a method that later added caching as seen in 3.2. Comparing 
the proﬁling obtained with version 353 against the one obtained with version 200 retrospectively
assesses the performance of Mondrian after 7 months (version 200 was released on 14 May 14 2009
and version 352 on 25 January 2010).

Navigating through history. The behavioral evolution blueprint easily compares two execution
proﬁles, each proﬁle supposedly obtained from a particular software version. The proﬁle that is compared 
with the anterior proﬁle may be chosen by means of a contextual menu. This menu allows for
moving through the history in a frame-by-frame basis. Clicking the ‘next’ comparison button (not
shown in the ﬁgures) for a particular method renders a new blueprint where the second proﬁle of
the previous blueprint becomes the ﬁrst and is compared with the proﬁle associated with the next
version available. A ‘previous’ button is also available to navigate in the opposite direction. Note
that the analysis data for the previous versions are stored and reused; navigating through the history
of the proﬁle is therefore without noticeable lag.

To navigate the proﬁling history, we require a set of proﬁles taken over the execution of a particular 
code snippet throughout several versions of a software system. After selecting a particular set
of versions where proﬁles are to be taken, we sort them from the oldest to the newest. The set of
proﬁles begins with the ﬁrst version’s proﬁle. Steps (2) to (4) are then repeated for each subsequent
version of the system, loading, proﬁling, and comparing it with the last available. The proﬁling
information is then stored in a similar approach to the one proposed in [11] for source code history.
Experience on Mondrian. We applied the previous algorithm to obtain a behavioral evolution
blueprint analyzing the method MORoot » applyLayout through a set of 22 versions, ranging
from 400 to 570.

Navigation through the set of blueprints obtained made visible variations in the execution time
associated with the cache implementation. Between versions 450 and 460, the call of a method
MONode » resetCacheInEdges in method MONode » translateBy:bounded: was introduced 
as an effort to homogenize the various cache mechanisms Mondrian provides. This new
method produced an increase of the execution time for MORoot » applyLayout.

The time increase of MORoot » applyLayout is, in contrast, reduced in version 589 compared
with version 510. This is caused by changes in the method MOGraphElement » applyLayout.
Version 589 of this method contains an extra initialization of the shape, which signiﬁcantly reduces
the cost of layouting nodes:

Copyright © 2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2012; 42:1165–1192
DOI: 10.1002/spe

EXECUTION PROFILING BLUEPRINTS

1183

FSGitFilesystem>>commit:

FSGitFilesystem>>basicCommit:to:

Figure 13. Behavioral evolution blueprint of version 80 compared with version 70 of GitFS.

Complementary case study. Beside Mondrian, we have used the behavioral evolution blueprint
on a number of different Pharo projects. For example, GitFS|||| is an implementation of Git in Pharo.
At the time this article is being written, 91 different versions of GitFS are published. Git is a distributed 
revision control system with an emphasis on speed.*** As a representative benchmark for a
revision control system, we measured a sequence of 500 commits made on a ﬁle present in memory
(no system primitives are therefore involved).

The method FSGitFilesystem» commit: is responsible to commit a change into a new ﬁle.
This method is the entry point to realize a Git commit operation. For version 70, our benchmark
made the #commit: method takes 2541 ms to execute. For version 80, #commit: took 4255 ms,
which represents a slowdown of 67%.†††

Figure 13 visually represents the cause of the slowdown. The source code of #commit: has not
changed from version 70 to version 80. However, #commit: invokes #basicCommit:to:, and the
source code of this method has changed. It went from

||||http://www.squeaksource.com/GitFS.html
***http://git-scm.com
†††D .4255(cid:2) 2541/(cid:3) 100=2541.

Copyright © 2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2012; 42:1165–1192
DOI: 10.1002/spe

1184

A. BERGEL ET AL.

Of the two modiﬁcations, the addition of the signature when committing is responsible of the
slowdown. To conﬁrm our ﬁndings, we contacted the author of GitFS to determine whether he
agreed on our ﬁndings; he answered positively.

Summary. We propose the behavioral evolution blueprint as an effective and intuitive visualization 
that complements structural distribution blueprint and behavioral distribution blueprint, two
visualizations of a unique proﬁle snapshot. This third blueprint focuses on differencing proﬁles of
two or more snapshots. A visualization always differentiates two proﬁles of the same application
and benchmark but for different software versions. Behavioral evolution blueprint helps tracking
down the cause of a performance increase or decrease.

The effectiveness of behavioral evolution blueprint depends on the structural difference between
the proﬁled version. Versions need to be ‘close enough’ to give a useful meaning to the comparison.
This point is discussed further in the following section (Section 5).

5. DISCUSSION

We discuss several crucial points about the design of our blueprints.

Proﬁler. Proﬁling blueprints were ﬁrst implemented for Kai. Kai is a proﬁler for the Pharo
Smalltalk programming language. The graphical engine that renders proﬁling blueprint is not tied
to Kai and Pharo. Once serialization and deserialization of a proﬁling has been agreed upon, then
the blueprints may be generated for different proﬁlers.

A number of requirements must be met by a proﬁler to produce proﬁling blueprints. First, it has to
compute for each method a number of metrics: number of different receivers per method, total execution 
time of a method and number of executions for each method (cf. Section 2.3). Then whether
their return value is constant over multiple invocations (Section 2.4). Kai gives wall-clock time, thus
mixing the time spent by the interpreter and the time taken to execute virtual machine primitives.

Gathering these information is not free. Kai executes twice the application to proﬁle. It ﬁrst gathers 
the call graph and the execution time for each method, using an execution sampling technique,
and then computes the method properties in a second pass extracting more information. Without
being optimized, Kai produced satisfactory results for the situation in which it has been employed.
The advances in the ﬁeld of bytecode instrumentations [12] and aspect-oriented techniques‡‡‡ are
likely to be useful to reduce the cost of proﬁling.

Heuristic for behavior preservation. The yellow color used in the behavioral blueprint indicates
possible locations for a memoization cache insertion. The cache is inserted by modifying the class
deﬁnition, the method to be optimized and adding a method for clearing the cache, if necessary.
There is no guaranty that such a transformation will not produce ripple effects, which may result in
completely different application behavior.

In practice, we use a heuristic based on the unit tests: the right semantics of all our code transformations 
are validated using an extended set of unit tests. Before doing the transformation, we make

‡‡‡http://www.eclipse.org/aspectj

Copyright © 2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2012; 42:1165–1192
DOI: 10.1002/spe

EXECUTION PROFILING BLUEPRINTS

1185

sure that our tests are green. After the transformation, tests must remain green. Using unit testing
as a reference for the right application behavior is not ideal because having a test coverage for all
the execution is challenging [13]. However, having a robust set of unit tests has so many advantages
(e.g., maintainability, documentation) in addition to conﬁrming that semantics are preserved by our
code transformations, that it is a recommended software engineering practice.

Because of the extended reﬂective feature of Pharo, one cannot guarantee that introducing a cache
does not modify the application behavior. It is indeed trivial to make an algorithm depend on the
length of some compile methods or on the number of static calls found in a method deﬁnition. We
however have not experienced such extreme situation. As future work, we plan to add new heuristics
based on the mutation of receivers and the arguments to identify redundant computation.

Difﬁculty of proﬁling different versions. A smooth integration of the behavioral evolution
blueprint (Section 4) in the Pharo programming environment is obtained by performing all the
proﬁling on the same virtual machine. However, this has a number of drawbacks: usage of sampling 
proﬁling may produce different execution times when repeating a proﬁle over the same source
code. As this problem is repeated over each version, comparing proﬁles gets more challenging.
This problem is not unique to our approach: Mytkowicz et al. have shown that state-of-the-practice
Java proﬁlers such as xprof, hprof, jproﬁle, and yourkit exhibit the same issues [14]. The usual
workaround is to repeat the sampling process several times and average the results of these executions;
 this could be easily supported by Kai. However, as future work, we are considering different
metrics that could be used instead of time execution, such as message counting or bytecode counting 
[3, 12]; these measures may prove more stable over different runs, compared with the time
information obtained by sampling.

Comparing distant versions. Behavioral evolution blueprint highlights differences between different 
proﬁles. This comparison makes sense only for proﬁles that are structurally comparable. By
structurally, we refer to the application static structure, in terms of classes and methods. To illustrate
this point, we obtained the evolution blueprint for the MetacelloBrowser application§§§ with three
different versions (0.0, 1.1, and 1.41). Version 0.0 is a version that does not deﬁne any method or
class. We use 0.0 as the extreme case for comparison.

Figure 14 shows that version 1.41 deﬁnes the method #infos and ﬁve additional methods that are
directly and indirectly called by #infos. These six methods are new in 1.41 because they are painted
in yellow. In addition, two methods were redeﬁned in 1.41 and are indirectly called by #infos (these
are the small red methods). The call graph of #infos is almost made of yellow nodes.

In version 1.1, the call graph rooted in MetacelloBrowser» initialize is entirely painted in yellow 
because these methods do not exist in version 0.0. In version 1.41, #initialize is red, meaning
that the method is slower and its source code has changed. The method #conﬁgurationSelection:
is present in both versions. The cause of the slow behavior is due to the new version of #initialize.
The situation given in the ﬁgure clearly shows one limitation of the behavioral evolution blueprint.
It is ineffective when the versions to compare are structurally ‘very’ different. Whereas one can still
refer to the cause of a slowdown in the evolution of the #initialize method, the call graph of #infos
is pretty useless to determine what is the cause of its high execution time (#infos takes about 62%
of the execution time).

From our personal experience, identifying what causes a slowdown in a call graph that has more
than 30% of new methods is difﬁcult at best. Naturally, these phenomena is highly dependent on
the software analyzed. Note that the root method of the call graph has to be present in the previous
version. In practice, the cause of a slowdown is easier to ﬁnd with a small structural difference only.
Keeping track of returned values. The optimizations we realized stem from keeping track of
values returned when invoking methods. The intuition that we exploited is that if a method always
returns the same computed value, then it may be worth adding a memoization mechanism to avoid
redundancy. Our blueprint indicates such methods with the yellow color.

We analyzed the presence and relevance of these candidate methods. We took ﬁve representative
applications of the Pharo ecosystem. These applications are available from the Pharo SqueakSource

§§§http://metacellobrowser.dcc.uchile.cl

Copyright © 2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2012; 42:1165–1192
DOI: 10.1002/spe

1186

A. BERGEL ET AL.

Version 1.1

Version 1.41

MetacelloBrowser>>initialize

MetacelloBrowser>>infos

MetacelloBrowser>>
gurationSelection:

Figure 14. Comparing distant versions.

forge. For each of these applications, we run their associated unit tests and report our monitoring.
Unit tests often describe typical execution scenarios [15]; it therefore looks is reasonable to base our
analysis on the unit test execution.

Application Number of total methods Number of constant methods

Mondrian
Pier
Grease
Magritte
XMLParser

2098
3238
1046
2056
1125

39
106
12
66
18

The ﬁrst column gives the total amount of methods deﬁned in the applications. The second column
gives the amount of constant methods that return a value other than the receiver itself. We ﬁltered
out methods that are invoked less than two times and that perform less than two calls to avoid methods 
that simply do a delegation or return a primitive value. The amount of constant methods in these
applications ranges from 1.1% to 3.2%.

We now focus on the case of Mondrian. We see that there are 39 constant methods that are
potential candidates for a memoization mechanism. Each of them has been manually reviewed. We
can classify each of these 39 methods along three sets: caching, lazy initialization, and candidate
methods.

Caching. Keeping satisfactory performances in Mondrian is a major concern for its development.
 Mondrian contains a number of sophisticated mechanisms to cache metric values. The method
#colorFor: and #computeColorFor: is a typical example:

Among the 39 methods, three groups of two methods follow the pattern #metricFor: and #com-
puteMetricFor:. These six methods are therefore false positives because they already implement a
memoization mechanism.

Copyright © 2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2012; 42:1165–1192
DOI: 10.1002/spe

EXECUTION PROFILING BLUEPRINTS

1187

Lazy initialization. Delaying the object creation is often employed in Mondrian. The method

#fontCache and #default are representative:

Eleven methods use lazy initialization. These methods are considered as false positive.

Candidate methods. From the 39 methods, 22 methods return the same value at each invocation
and do not already use a cache or lazy initialization. Four out of these 22 are due to incomplete
testing. For example, the method #selectBoxBounds is deﬁned as follows:

This method is executed twice on the same receiver. This method is an example of a poor testing.
After a scrutinization, 18 out of the 39 methods (nearly 50%) are considered candidates for implementing 
the memoization. These 18 methods represent less than 1% of the total amount of methods
in Mondrian. None of these methods is currently responsible for a slowdown, as we corrected the
slowdowns while we applied the techniques described here.

From numerical values to visual patterns. Outputs of traditional proﬁlers make heavy use of
numerical values to describe the execution of the proﬁled software. The execution is expressed in
terms of percentage of the global execution time for each individual method. Proﬁling blueprints are
a graphical representation of these metrics. By moving from a textual to a graphical representation
of proﬁling, we gained the ability to compare many different metrics at the same time. Numerical
comparison has been traded off for visual pattern identiﬁcation. Instead of seeing that #applyLayout 
takes 53% of the total time execution and #bounds 40%, we see that the square representing
#applyLayout is ‘slightly’ larger than the one representing #bounds. Even though a visualization 
conveys less accurate values because we cannot precisely quantify distances and color intensity,
it allows one to easily perform comparisons on a large amount of entities at once, allowing us to
pinpoint the cause of the performance issues faster.

Nature of the information displayed. Our blueprints use information gathered by the Kai proﬁler;
 other proﬁlers may record additional execution information that is beyond what a proﬁler such
as Kai can offer. The nature of the information we obtain is somewhat orthogonal to our main contribution;
 our visualizations mostly concerns how that information is displayed, more than what
information is displayed. Code proﬁlers typically use a tree widget to indicate the nesting level of
methods and additional information—which in most cases is similar to the one Kai offers. We argue
that a visual and interactive representation of this information is more adequate to ﬁnd and resolve
execution bottlenecks as it allows to process a larger amount of information at a time, before delving 
for focused analyses. Needless to say, a code proﬁler that would record additional information
of interest could be used as a data source to support an improved version of our visualization. In
particular, proﬁling information collected at the statement level—that Kai does not support at the
moment—could be displayed by embedding a statement-level visualization such as Microprints [16]
in the blueprints we deﬁned previously.

Scalability. The experiments we used for this work were realized on medium-sized applications
(Mondrian totals 2000 methods and 20,000 lines of codes approximately). The visualizations were
produced on a screen with a resolution of 1440(cid:4)900 pixels. The largest execution was three screens

Copyright © 2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2012; 42:1165–1192
DOI: 10.1002/spe

1188

A. BERGEL ET AL.

large. Zooming out is hardly a practical solution: classes and methods need to be signiﬁcantly
reduced to make more than 200 classes ﬁt in one screen. Bertin [17] assessed that a good practice 
is to offer a visualization that can be grasped at one glance, without the need to scroll or move
around. The layout we have adopted are well-suited for medium-sized applications. We have no
evidence that our approach does not scale because we have not encountered an application in Pharo
that is sufﬁciently large to invalid our assumptions. We can however discuss several strategies to
deal with the potential scalability issues of our approach.

For the scalability of our visualizations to be increased, several possibilities exist. All are based
on the notion of ﬁltering to reduce the amount of data displayed on screen. We think such a general 
strategy is viable, as ultimately proﬁling for optimization is performed in concert with program
comprehension. Because even a medium-sized program is too large to be understood completely
before modiﬁcation, programmers routinely employ strategies to reduce the amount of source code
they have to understand [18]. We envision two strategies to reduce the quantity of data displayed at
any given point in time: dynamic slicing and intermediate coarse-grained visualizations.

Dynamic program slicing [19] allows one to reduce the size of the program under analysis by
only keeping the part of the trace that directly affects a given statement; if used carefully, this would
reduce the size of the dataset to display signiﬁcantly, focusing on the parts of the program that
directly affect a given statement. As it stands, our proﬁling frameworks supports a primitive version 
of this, by allowing the user to instrument a speciﬁc set of program entities and not the entire
program.

Another solution would be to introduce coarser-grained visualizations as a starting point. For
larger-scale applications, an initial visualization would display packages and classes, instead of
classes and methods, and would support navigation to the ﬁner-grained visualization; in the same
fashion, we support now navigation between the structural and the behavioral blueprint. This would
allow programmers to ﬁrst gain an overall understanding of the dynamic properties of the program
at a high level, before delving into details to get a ﬁner understanding of a subsystem, which is displayable 
onscreen. This strategy allows one to handle larger-scale programs, at the cost of a program
inspection workﬂow that implies navigating between three visualizations, instead of two.

Note that, strictly speaking, our blueprints are not the only visualization that suffer from scalability.
 YourKit and JProﬁler output their results using a textual table or simple graphics, two
visualizations that do not scale well.

Multi-threading. Our focus is on single-threaded applications, as many Pharo workloads are single 
threaded. For multi-threaded applications, the user has the option to (i) visualize the data only
for a selected thread—as we exempliﬁed in the paper, by analyzing the two threads of Mondrian
separately—or (ii) to ﬁrst integrate the proﬁling data of multiple threads before visualizing the
integrated proﬁle.

6. RELATED WORK

Our related work review discusses ﬁrst the various dynamic information visualizations that have
been proposed, before presenting the various approaches that have been proposed to compare
executions of two different versions of a program.

6.1. Visualization of dynamic information

Proﬁling capabilities have been integrated in integrated development environments such as the NetBeans 
Proﬁler¶¶¶ and Eclipse’s Tracing and Proﬁling Project.|||||| The NetBeans Proﬁler uses JFluid
[20], which offers a calling context tree (CCT) [21] augmented with the accumulated execution
time for individual methods. The CCT is visualized as an expandable tree, where calling contexts
are sorted by their execution time and can be expanded (respectively collapsed) to show (or hide)

¶¶¶http://proﬁler.netbeans.org/
||||||http://www.eclipse.org/tptp/performance/

Copyright © 2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2012; 42:1165–1192
DOI: 10.1002/spe

EXECUTION PROFILING BLUEPRINTS

1189

callees. However, as CCTs for real-world applications are often large, comprising up to some million 
nodes, an expandable tree representation makes it difﬁcult to detect hotspots in deep calling
contexts.

The calling context ring chart (CCRC) [22, 23] is a CCT visualization that eases the exploration
of large trees. Like the Sunburst visualization [24], CCRC uses a circular layout. Callee methods are
represented in ring segments surrounding the caller’s ring segment. For hot calling contexts to be
revealed, the ring segments can be sized according to a chosen dynamic metric. Recently, CCRC has
been integrated into the Senseo plugin for Eclipse [25], which enriches Eclipse’s static source views
with several dynamic metrics. Our blueprints have a different focus because global information is
shown instead of providing a line-of-code granularity.

Execution traces may be used to analyze dynamic program behavior. Execution traces are logged
events, such as method entry and exit, or object allocation. However, the resulting amount of data
can be excessive. In Deelen et al. [26], execution traces are visualized with nodes representing
classes and edges representing method calls. Node size and edge thickness are mapped to properties
(e.g., number of method invocations). A time range can be selected to limit the data to be visualized.
 Another approach to visualizing execution traces has been introduced in Holten et al. [27]. It
uses the concept of hierarchical edge bundles [28], where similar edges are put together to improve
the visualization of larger traces. Execution traces allow keeping calls in sequences and selecting a
precise time interval to be visualized, which helps understanding a particular phase in the execution
of a program. Blueprint proﬁling offers a global map of the complete execution without focusing on
sequentiality in time. However, they offer hints about the behavior of individual methods that help
to solve a class of optimization problem, namely introducing caches.

Tree-maps [29] visualize hierarchical structures. Nodes are represented as rectangular areas sized
proportionally to a metric. Tree-maps have been used to visualize proﬁling data. For instance, in
[30], the authors present KCacheGrind, a front end to a simulator-based cache proﬁling tool, using a
combination of tree-maps and call graphs to visualize the data. Our blueprint uses polymetric view
to render data. A tree-map solves a problem in a different way as a polymetric view would solve it. A
polymetric view enables one to compare several different metrics, whereas a tree-map is dedicated
to showing a single metric (besides color) in a compact space.

Greevy et al. enhanced polymetric views with a third dimension to visualize the runtime behavior 
of systems [31]. They focus on the execution of individual features, using the third dimension
to overlay dynamic information about instantiation and message sending over a polymetric view.
Their system allows one to replay each of the steps of the trace and is geared towards program comprehension,
 whereas our approach focuses on performance proﬁling and provides a comprehensive
view of performance metrics.

Sevitsky et al. present an information visualization tool specialized in the performance analysis of
Java programs, Jinsight EX [32]. JInsight EX uses execution slices to narrow down the data to analyze 
to a more manageable size and proposes visualizations centered on the slices. Previous work by
De Pauw et al. [33, 34] also abstracts dynamic information to the instance, method and class levels,
proposing visualizations such as instance-level histograms, and functions-instances or inter-classes
and intra-classes call matrices. We however abstract away more information as we show metrics
instead of the more execution-event-based approach employed in these works.

Reiss and Eddon adopt an approach radically different as ours, as they visualize extremely ﬁnegrained 
information of programs as they are executing [35]. JIVE visualizes for instance the state
of the Java heap, ﬁle input/output operations, or information on which class is used in which thread
of the program. Its successor, JOVE, further maps the information to source code locations that are
being executed at any given moment [36].

6.2. Comparison of versions

Several approaches exist that compare the execution of two versions of a program.

Zhang and Gupta [37] present an approach to match two programs that behave similarly, although
they appear to be statically different. The approach matches the execution at the level of binary
instructions, and is geared at scenarios such as debugging—for instance matching of an optimized
and an unoptimized version of a program—or piracy detection—detecting similar behavior despite

Copyright © 2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2012; 42:1165–1192
DOI: 10.1002/spe

1190

A. BERGEL ET AL.

obfuscation—and not explicitly performance. The authors focus on the performance and accuracy
of the matching algorithm. Nagajaran et al. later extended the approach to support more aggressive
transformations by working at the control ﬂow level rather than the instruction level [38].

Zhuang et al. presented a framework for differencing execution proﬁles named PerfDiff [39]. The
approach is based on matching CCTs. The algorithm identiﬁes variations in the layout of the trees
or the weight—the weight being a performance metric in their case—of the nodes. The difference
algorithm is applied to selected benchmarks of programs, running under different platforms. The
authors report on the accuracy of the algorithm. Mostafa and Krintz propose a similar approach,
aimed this time at tracking performance across different revisions of the same program [40]. They
also employ CCTs and perform an empirical evaluation of their matching algorithm.

We see that most approaches aiming at matching different executions of a program have primarily
been evaluated in terms of their matching accuracy. Our focus is more on how to exploit this infor-
mation: we presented visualizations summing up the differences between two versions of a given
program. Because the dynamic information we collect is higher-level than CCTs (as it is aggregated
at the level of methods and classes), we argue that the accuracy of the matching algorithm is not as
critical, hence our focus in exploiting the information.

7. CONCLUSION

In this article, we presented three visualizations—the structural blueprint, the behavioral blueprint,
and the evolutionary blueprint—helping developers to identify and remove performance bottlenecks.
 Providing visualizations that are intuitive and easy to use is our primary goal. Our graphical 
blueprints follow simple principles such as ‘big nodes are slow methods’, ‘gray nodes are
methods likely to have side-effects’, and ‘yellow nodes remain constant on return values’. Our
visualizations helped us to signiﬁcantly improve Mondrian, a visualization engine. We described
a number of optimizations we realized. The last version of Mondrian contains an improved version
of the #applyLayout method, thus mitigating the bottleneck caused by this method, and other
optimizations.

We also introduced a visualization aimed at the retrospective evaluation of the evolution of the performance 
of a system over time. The behavioral evolution blueprint allowed us to pinpoint changes
to the source code that caused performance degradation in two instances, for the Mondrian and
GitFS projects; the developer of GitFS reviewed and validated our ﬁndings.

A number of conclusions may be drawn from the experiment described in this article. First, bottleneck 
identiﬁcation and removal are signiﬁcantly easier when side-effects and constant return values
are localized. Second, an extensive set of unit tests remains essential to assess whether a candidate
optimization can be applied without changing the behavior of the system. Third, one must pay attention 
that further changes to a program may render previous optimizations less useful than they used
to be and be watchful over this phenomenon over time.

For future work, we plan to focus on architectural views by adopting coarser grain as methods 
and classes. We also plan to further our work on the evolution of performance by proposing
visualizations that handle more versions at once.

We gratefully thank Max Leske for his feedback on our ﬁnding related to GitFS. This work has been partially
funded by Program U-INICIA 11/06 VID 2011, grant U-INICIA 11/06, University of Chile.

ACKNOWLEDGEMENTS

REFERENCES

1. Tuduce I, Majo Z, Gauch A, Chen B, Gross TR. Asymmetries in multi-core systems—or why we need better performance 
measurement units. Proceedings of the Exascale Evaluation and Research Techniques Workshop (EXERT),
Pittsburg, USA, 2010.

2. Cornea B, Bourgeois J. Performance prediction of distributed applications using block benchmarking methods. In
PDP’11, 19-th International Euromicro Conference on Parallel, Distributed and Network-Based Processing. IEEE
Computer Society Press: Ayia Napa, Cyprus, 2011.

Copyright © 2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2012; 42:1165–1192
DOI: 10.1002/spe

EXECUTION PROFILING BLUEPRINTS

1191

3. Binder W. Portable and accurate sampling proﬁling for Java. Software: Practice and Experience 2006; 36(6):615–650.
4. Bergel A, Robbes R, Binder W. Visualizing dynamic metrics with proﬁling blueprints. In Objects, Models, Components,
 Patterns, Lecture Notes in Computer Science, Vol. 6141, Vitek J (ed.). Springer Berlin / Heidelberg: Berlin,
Germany, 2010; 291–309, DOI: 10.1007/978-3-642-13953-6_16.

5. Meyer M, Gîrba T, Lungu M. Mondrian: an agile visualization framework. In ACM Symposium on Software
Visualization (SoftVis’06). ACM Press: New York, NY, USA, 2006; 135–144. Available at: http://scg.unibe.ch/
archive/papers/Meye06aMondrian.pdf [Accessed 09 December 2010].

6. Benbasat I, Goldstein DK, Mead M. The case research strategy in studies of information systems. MIS Quarterly 
1987; 11:369–386. DOI: 10.2307/248684. Available at: http://portal.acm.org/citation.cfm?id=35194.35201
[Accessed 09 December 2010].

7. Reiss S. Visualizing the java heap to detect memory problems. Visualizing Software for Understanding and
Analysis, 2009. VISSOFT 2009. 5th IEEE International Workshop, Edmonton, Canada, 2009; 73–80, DOI:
10.1109/VISSOF.2009.5336418.

8. Lanza M, Ducasse S. Polymetric views—a lightweight visual approach to reverse engineering. Transactions 
on Software Engineering (TSE) 2003; 29(9):782–795. DOI: 10.1109/TSE.2003.1232284. Available at:
http://scg.unibe.ch/archive/papers/Lanz03dTSEPolymetric.pdf [Accessed 09 December 2010].

9. Gîrba T, Lanza M. Visualizing and characterizing the evolution of class hierarchies. WOOR 2004 (5th ECOOP Workshop 
on Object-Oriented Reengineering), 2004. Available at: http://scg.unibe.ch/archive/papers/Girb04aHierarchies
Evolution.pdf [Accessed 09 December 2010].

10. Ducasse S, Lanza M, Bertuli R. High-level polymetric views of condensed run-time information. In Proceedings 
of 8th European Conference on Software Maintenance and Reengineering (CSMR’04). IEEE Computer 
Society Press: Los Alamitos CA, 2004; 309–318, DOI: 10.1109/CSMR.2004.1281433. Available at:
http://scg.unibe.ch/archive/papers/Duca04aRuntimePolymetricViews.pdf [Accessed 09 December 2010].

11. Gîrba T, Ducasse S. Modeling history to analyze software evolution. Journal of Software Maintenance 
and Evolution: Research and Practice 2006; 18(3):207–236. DOI: 10.1002/smr.325. Available at:
http://doi.wiley.com/10.1002/smr.325 [Accessed 09 December 2010].

12. Binder W, Hulaas J, Moret P, Villazón A. Platform-independent proﬁling in a virtual execution environment.

Software: Practice and Experience 2009; 39(1):47–79.

13. Yang Q, Li JJ, Weiss DM. A survey of coverage-based testing tools. The Computer Journal 2009; 52(5):589–597.
DOI: 10.1093/comjnl/bxm021. Available at: http://comjnl.oxfordjournals.org/content/52/5/589.abstract [Accessed
09 December 2010].

14. Mytkowicz T, Diwan A, Hauswirth M, Sweeney P. Evaluating the accuracy of Java proﬁlers. In Proceedings of the
31st conference on Programming language design and implementation. PLDI ’10, ACM: New York, NY, USA, 2010;
187–197, DOI: 10.1145/1806596.1806618. Available at: http://doi.acm.org/10.1145/1806596.1806618 [Accessed
09 December 2010].

15. Martin RC. Agile Software Development. Principles, Patterns, and Practices. Prentice-Hall: New Jersey, 2002.
16. Ducasse S, Lanza M, Robbes R. Multi-level method understanding using microprints. VISSOFT’05: Proceedings of

the 3rd International Workshop on Visualizing Software for Understanding and Analysis, 2005; 33–38.

17. Bertin J. Graphische Semiologie. Walter de Gruyter: Berlin Germany, 1974.
18. Erdös K, Sneed HM. Partial comprehension of complex programs (enough to perform maintenance). IWPC’98:

Proceedings of the 6th International Workshop on Program Comprehension, Ischia, Italy, 1998; 98.

19. Korel B, Rilling J. Dynamic program slicing methods. Information & Software Technology 1998; 40(11-12):647–659.
20. Dmitriev M. Proﬁling Java applications using code hotswapping and dynamic call graph revelation. In WOSP 2004:
Proceedings of the Fourth International Workshop on Software and Performance. ACM Press: New York, NY, USA,
2004; 139–150.

21. Ammons G, Ball T, Larus JR. Exploiting hardware performance counters with ﬂow and context sensitive proﬁling.
 In PLDI 1997: Proceedings of the ACM SIGPLAN 1997 conference on Programming language design and
implementation. ACM Press, 1997; 85–96, DOI: http://doi.acm.org/10.1145/258915.258924.

22. Moret P, Binder W, Ansaloni D, Villazón A. Visualizing calling context proﬁles with ring charts. In VISSOFT 2009:
Proceedings of the 5th IEEE International Workshop on Visualizing Software for Understanding and Analysis. IEEE
Computer Society: Edmonton, Alberta, Canada, 2009; 33–36.

23. Moret P, Binder W, Villazón A, Ansaloni D, Heydarnoori A. Visualizing and exploring proﬁles with calling context

ring charts. Software: Practice and Experience 2010; 40:825–847.

24. Stasko J. An evaluation of space-ﬁlling information visualizations for depicting hierarchical structures. International

Journal of Human Computer Studies 2000; 53(5):663–694. DOI: http://dx.doi.org/10.1006/ijhc.2000.0420.

25. Röthlisberger D, Härry M, Villazón A, Ansaloni D, Binder W, Nierstrasz O, Moret P. Augmenting static source
views in ides with dynamic metrics. In ICSM 2009: Proceedings of the 25th IEEE International Conference on
Software Maintenance. IEEE Computer Society: Edmonton, Alberta, Canada, 2009; 253–262.

26. Deelen P, van Ham F, Huizing C, van de Watering H. Visualization of dynamic program aspects. VISSOFT 2007: Proceedings 
of the 4th IEEE International Workshop on Visualizing Software for Understanding and Analysis, Alberta,
Canada, 2007; 39–46.

27. Holten D, Cornelissen B, van Wijk JJ. Trace visualization using hierarchical edge bundles and massive sequence
views. VISSOFT 2007: Proceedings of the 4th IEEE International Workshop on Visualizing Software for Understanding 
and Analysis, 2007; 47–54.

Copyright © 2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2012; 42:1165–1192
DOI: 10.1002/spe

1192

A. BERGEL ET AL.

28. Holten D. Hierarchical edge bundles: visualization of adjacency relations in hierarchical data. IEEE Transactions on

Visualization and Computer Graphics 2006; 12(5):741–748.

29. Johnson B, Shneiderman B. Tree-maps: a space-ﬁlling approach to the visualization of hierarchical information
structures. In VIS 1991: Proceedings of the 2nd conference on Visualization. IEEE Computer Society Press: Los
Alamitos, CA, 1991; 284–291.

30. Weidendorfer J, Kowarschik M, Trinitis C. A tool suite for simulation based analysis of memory access behavior. In
ICCS 2004: Proceedings of the 4th International Conference on Computational Science, LNCS, Vol. 3038. Springer:
Berlin, Germany, 2004; 440–447.

31. Greevy O, Lanza M, Wysseier C. Visualizing live software systems in 3D. Proceedings of SoftVis 2006 (ACM
Symposium on Software Visualization), Brighton, UK, 2006, DOI: 10.1145/1148493.1148501. Available at:
http://scg.unibe.ch/archive/papers/Gree06aTraceCrawlerSoftVis 2006.pdf [Accessed 09 December 2010].

32. Sevitsky G, Pauw WD, Konuru R. An information exploration tool for performance analysis of Java programs.
TOOLS Europe ’01: Proceedings of the 38th International Conference on Technology of Object-Oriented Languages
and Systems, Components for Mobile Computing, 2001; 85–101, DOI: 10.1109/TOOLS.2001.911758.

33. De Pauw W, Helm R, Kimelman D, Vlissides J. Visualizing the behavior of object-oriented systems. Proceedings of
International Conference on Object-Oriented Programming Systems, Languages, and Applications (OOPSLA’93),
Washington, D.C., USA, 1993; 326–337, DOI: 10.1145/165854.165919.

34. Pauw WD, Vlissides JM. Visualizing object-oriented programs with jinsight. In Workshop ion on Object-Oriented
Technology, ECOOP ’98. Springer-Verlag: London, UK, 1998; 541–542. Available at: http://portal.acm.org/
citation.cfm?id=646778.704665 [Accessed 09 December 2010].

35. Reiss SP. JOVE: Java as it happens. Proceedings of SoftVis 2005(ACM Symposium on Software Visualization), 2005;

115–124.

36. Reiss SP, Renieris M. Jove: Java as it happens. In SoftVis ’05: Proceedings of the 2005 ACM symposium on Software

visualization. ACM: New York, NY, USA, 2005; 115–124, DOI: 10.1145/1056018.1056034.

37. Zhang X, Gupta R. Matching execution histories of program versions. In ESEC/FSE-13: Proceedings of the 10th
European Software Engineering Conference held jointly with 13th ACM SIGSOFT International Symposium on
Foundations of Software Engineering. ACM: New York, NY, USA, 2005; 197–206, DOI: 10.1145/1081706.1081738.
38. Nagarajan V, Gupta R, Zhang X, Madou M, De Sutter B. Matching control ﬂow of program versions. Proceedings
of the 25th IEEE International Conference on Software Maintenance (ICSM’07), Paris, France, 2007; 84–93, DOI:
10.1109/ICSM.2007.4362621.

39. Zhuang X, Kim S, Mi S, Choi JD. Perfdiff: a framework for performance difference analysis in a virtual machine
environment. In CGO ’08: Proceedings of the 6th annual IEEE/ACM international symposium on Code generation
and optimization. ACM: New York, NY, USA, 2008; 4–13, DOI: 10.1145/1356058.1356060.

40. Mostafa N, Krintz C, Tracking performance across software revisions. In PPPJ ’09: Proceedings of the 7th International 
Conference on Principles and Practice of Programming in Java. ACM: New York, NY, USA, 2009; 162–171,
DOI: 10.1145/1596655.1596682.

Copyright © 2011 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2012; 42:1165–1192
DOI: 10.1002/spe

