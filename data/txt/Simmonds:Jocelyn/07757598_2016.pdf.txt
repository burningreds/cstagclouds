“Teaching is Learning”: Pedagogical material

created and evaluated by students

Jérémy Barbay, Jocelyn Simmonds

Computer Science Department

University of Chile, Santiago, Chile

Adriana Keiko Nishida, Monael Pinheiro Ribeiro

Center of Mathematics, Computer and Cognition
Federal University of ABC, Santo André, Brazil

Email: jeremy@barbay.cl, jsimmond@dcc.uchile.cl

Email: keiko.nishida@ufabc.edu.br, monael@ufabc.edu.br

Abstract—The action of teaching reinforces one’s learning
but requires some external quality control when done by nonprofessionals 
(e.g., a professor supervising teaching assistants).
This quality control is costly and has limited the adoption of
peer teaching in schools. Our solution to this problem is to ask
students to create pedagogical material that will then be evaluated
by the students themselves, where they evaluate a mix of new
and already rated materials. One advantage of this technique is
that it allows students to develop critical thinking skills, since
they must judge if the presented material adequately covers a
speciﬁc topic. We describe the “Teaching is Learning” project,
that implements these ideas. We ﬁrst discuss two pilot studies: 1)
since 2009, engineering students from the University of Chile have
been creating and validating pedagogical 3D animations; and 2)
during 2015, seventh graders from the Blest Gana secondary
school created and evaluated pedagogical videos using their cell
phones, editing the videos during their Technology class. We
then give an overview of existing work on measuring student
learning, and discuss how we can evaluate the effectiveness of our
technique. We conclude by describing our plans for implementing
the “Teaching is Learning” project at a larger scale during 2016,
both in Chile and in Brazil.

I. INTRODUCTION

A common intuition among instructors is that one deepens
one’s understanding of a topic when teaching it. There are
at least two aspects of the teaching experience which can
deepen this understanding: 1) preparing teaching material,
and 2) reviewing and correcting students’ answers. When
preparing material, the instructor breaks down knowledge into
smaller fragments and organizes it, which helps improve the
memorization process. On the other hand, when reviewing
answers, especially wrong ones, an instructor is pushed to
explore the same knowledge from different points of view.

Given these beneﬁts, it is tempting to generalize this practice
from the small population of instructors to the large population
of students. For example, if second year engineering students
teach ﬁrst year students, they can deepen their own understanding 
of the material while at the same time lightening
the instructors’ teaching load. The caveat, of course, is that
generalizing from trusted instructors – who fact-check and
prepare new material – to a general population of students
– who may commit errors or mischievously introduce some
– requires external quality control. This is more expensive
than the traditional setting where the instructor simply teaches,

since each interaction between the instructors and students, as
well as between students, must be supervised and assessed.

Student

teaching activities can be included on a larger
scale if we can systematize their assessment. Inspired by
collaborative quality control techniques introduced by systems
such as Re-Captcha [1], Calibrated Peer Review, Amazon’s
Mechanical Turk [2] and others, we propose the “Teaching is
Learning” (TiL) methodology. Here, students must: 1) create
pedagogical material aimed at other students (as opposed to
teaching those students directly), and 2) evaluate the pedagogical 
material produced by other students, according to some
speciﬁc criteria deﬁned by the instructor. In this step, students
assess a random selection of both new pedagogical material
and content that has already been validated by instructors. In
addition to other more traditional objectives, we believe that
this will help students in developing critical thinking skills,
since they cannot assume a priori that all material is correct
and/or complete.

We ﬁrst describe two separate pilot studies that we carried
out during 2009 - 2015. We then give an overview of how
to measure student learning. We ﬁnally describe the studies
that we will carry out during 2016, involving both secondary
and tertiary students, and how we plan to evaluate the TiL
methodology.

II. PILOT STUDIES (2009 - 2015)

A. “EI2001: Taller de Proyecto” course (2009 - 2015)

The “EI2001: Taller de Proyecto” course, taken by second
year engineering students at the University of Chile (Santiago,
Chile), aims to bridge the gap between theory and practice.
Several sections are available, each one with a different
theme, and students must register for one of these sections.
For example, one section has students programming LEGO
Mindstorm robots, while learning: 1) how to collaborate in
small groups, 2) how to plan work under time restrictions,
and 3) how to present the outcomes of their projects.

Another section of this course, called “Pedagogical Anima-
tions”, created by J. Barbay in 2009, seeks to address the same
pedagogical objectives via the creation of pedagogical videos
using the Alice software, which is developed and distributed
by Carnegie Mellon University. During the semester, students
– in groups of 2 to 3 – must create ﬁve pedagogical animations.
Each student is expected to spend 4.5 hours per week on these

978-1-5090-1790-4/16/$31.00 ©2016 IEEEvideos (1.5 hours in class and 3 hours at home), and have 3
weeks to work on each video. Students are randomly assigned
to a different group for each video. The general theme of these
videos is common to all groups (e.g., Copyrights, Sorting algorithms,
 Comparing 2 sorting algorithms) and each group picks
or is assigned a speciﬁc subtheme (e.g. creative commons,
Google Image, sources for music “free of rights”). Students
can also propose new themes, which must be approved by the
instructor. The pedagogical videos are presented in class and
evaluated collaboratively using a paper form. This section is
taught every year, with cohorts ranging from 42 to 63 students.
The main observation from this long-running pilot is that
students are more motivated: the students, both in class and
in the teaching evaluations, express their appreciation for the
possibility to express themselves creatively; and many students
work more hours that they are supposed to. A second observation 
is that, while most students are reluctant to criticize their
peers’ work at the beginning of the semester, most are willing
to do this by the end of the semester and have learned to do
so in a constructive manner.

Along the years, we also administered questionnaires. Here
we see that students show an increase in conﬁdence about their
ability to work in groups: at the beginning of the semester,
asking them about their ideal group size yields an average
of 1.2, while by the end of the semester, asking the same
yields 1.8. We also attempted to measure how the creation of
pedagogical videos reinforced knowledge, but failed because
the question was poorly calibrated. We hope to perform a
better measurement in 2016.

B. “Video Classes” project (2015)

The “Video Classes” project1 adapted the “Pedagogical
Animations” experience (described in the previous subsection)
in order to apply it at secondary schools. We carried out a pilot
study at the Blest Gana secondary school (Santiago, Chile),
where 82 eighth graders (ages ranging from 13 to 16) were
asked to create two pedagogical videos during the semester,
targeting students at the same level. These videos were created
using the students’ cell phones and basic desktop video editing
software. Students spent 4 weeks working on each pedagogical
videos in groups of 2-3, where they were randomly assigned
to a different group for each video. Again, the general theme
for these videos is the same for all groups (one Mathematics
video, one History video), and the subtheme was assigned to or
chosen by each group (e.g., Geometry, War of the Paciﬁc, etc.).
Students uploaded their videos to YouTube, submitting the
video URL as well as the corresponding pedagogical question
(e.g., “After seeing this video, what is the value of the inscribed
angle of the circle?”) to the TiL system, software developed
speciﬁcally for this purpose. Students then used the TiL system
to collaboratively evaluate three (video, question) pairs from
other students.

The students at the Blest Gana school are divided into 3
sections, where two of these created pedagogical videos, and

1“Video Clases” in Spanish

the remaining section served as a control group. In order to
measure how this experience improved the students’ learning
experience, we analyzed the math test scores for all 3 sections.
These tests evaluated more topics than those included in
the “Video Classes” project, and were created by the math
professor, who did not participate in this project. The 18
students that created math videos improved their marks by
1.8 points while the control group improved their marks by
0.6 points (out of 7.0).

This is a promising result, supporting our intuition that
students learn more by teaching. However, overall there was
a low level of participation in this study: only 3 (15) out
of 42 (40) students created all the videos in section 1 (2).
One possible explanation is that Blest Gana is a school for
underserved students, who in Chile are historically harder to
engage in educative experiences and drop out at higher levels
than the rest of the population [3]. This is evidence by the large
age range of the students. We need to address this issue in the
next version of this project, as we believe that underserved
students are the ones that can beneﬁt the most from TiL.

III. MEASURING STUDENT LEARNING

Some questions are quite common among educators and
those who propose methods and ways of teaching, such as:
are students learning what they are being taught? is what they
learn proportional to the time and resources invested? is this
effort sustainable? It is hard to answer these questions in a
quantitative manner.

Different attempts have been made to quantify how much
has been learned. They involve, among other points of learning,
 motivation [4], preparation and use of concepts [5], and
the development of critical thinking skills [6]. These studies
present possible ways of assessing student learning in relation
to training, the use of concepts as well as critical thinking,
and could be considered in future assessments of the “Video
Classes” project, in addition to graded tests and activities that
are customarily applied in formal education.

A. Evaluation

Evaluation is not just the act of diagnosis, but also includes
deciding what to do once a diagnosis has been made, and
therefore consists of two inseparable processes: diagnosis and
decision. The diagnosis is related to the pedagogical theory
used in class [7], and translates into different educational
practices. This means that in order to measure student learning,
the data we collect must be consistent with the type of learning
outcomes being assessed. For example, are we assessing
student behaviors or skills? What course topics are being
evaluated? What level of expressiveness and clarity are we
expecting from our students? In other words, assessment tools
need to be carefully thought out, as well as the actions need
to be carried out given a result.

B. Evaluating concepts

Pelizzari et al. [5] and Ausubel [6] discuss the signiﬁcance
of learning in the evaluation of concepts, reporting that this

is more efﬁcient when students’ knowledge, personal and past
experiences are taken into account when presenting them with
new knowledge. This can only happen when students are
willing to learn, and it means something to her/him, both
logically and psychologically. This means that teachers must
carefully select evaluation tools, as well as the actions that
they will apply for a given result. Typically, two classmates
share more background and experiences than a student and
an older professor. Peer teaching lets these students use this
common experience to better communicate new ideas to their
classmates. We also plan in later iterations to evaluate if
guiding the students to include personal experiences in their
videos leads to increased student learning.

C. Development and analysis of critical thinking

Critical Thinking can be understood as an internal cognitive
process, reshaped by the experiences and by analysis carried
out by the individual, who questions and judges information
that she/he receives [8]. It is hard to “measure” a person’s
critical thinking skills: not only are these skills constantly
evolving, but these also change the person in the act. Based on
Bloom’s taxonomy of educational objectives [9], critical thinking 
is an open-minded process of discovery and understanding,
analysis and application, and synthesis and evaluation. In
TiL, students must ﬁrst study the assigned topic and ﬁgure
out what subtopic they want to focus on. They then need
to work out an example for the video, ﬁguring out how
to present this information in smaller units so that viewers
can follow the individual steps. They must also identify a
pedagogical objective for this video, expressed as a multiple
choice question. When evaluating the videos, viewers must be
critical, as not all videos will be “good” videos. They are asked
to directly evaluate the quality of the video, and to indirectly
evaluate the contents of the video by answering the associated
multiple-choice question.

In this section, we discuss two methodologies that help
understand a student’s level of critical
the ﬁrst
establishes a series of criteria that can be used to understand a
student’s level of critical thinking, but does not necessarily involve 
the use of quantitative approaches; the second proposed
a mixed assessment, with both quantitative and qualitative
aspects.

thinking:

1) Criteria for evaluating critical thinking in general edu-
cation: Northeastern Illinois University established 10 criteria
rubric for evaluating critical thinking [10] during oral and
project presentations, or via self-reﬂections essays. During
evaluation, the instructor must assess the student’s level of
proﬁciency for each criteria (no/limited, some, adequate, high).
The criteria ask if the student can:
identify and explain
issues; distinguish types of claims; recognize stakeholders
and contexts; frame personal responses; acknowledge other
perspectives; reconstruct arguments; interpret content; evaluate
assumptions; evaluate evidence; and evaluate implications,
conclusions and consequences. This rubric must be applied
by the instructor or a domain expert, so as to obtain a fair
assessment of the student’s proﬁciency levels for these criteria.

2) Mixed Methodology Approach: Garisson [11] argues that
learning is validated by sharing personal interpretations with
others. This inspired Rubino et al. [8] to propose the “Mixed
Methodology Approach”, where critical thinking skills are
quantiﬁed by measuring certain interaction aspects between
activity participants. They present an experience where they
applied this methodology, opening chat windows during a
video conference in order to measure and analyze the interactions 
that happened between participants, along with the
participation rates (as measured by the number of posts in
the chat). All this data was entered into an evaluation matrix,
which includes participation rates per person,
the number
of participants,
the types of comments (phrase, sentence,
word combination, punctuation, symbol) and the types of
participation (management, technical, social, content, combi-
nation/social). Rubino et al. linked these interactions with key
indicators from the literature [11], ﬁnding that the following
activities were related to critical thinking: the identiﬁcation of
problems during the presentation, as well as possible solutions,
and the presentation of strategies for problem solving, operation 
and integration. After this check, the data was quantiﬁed
and analyzed by category, and subjected to a cross-tabulation
statistical analysis.

IV. CURRENT IMPLEMENTATION (2016)

A. University of Chile

The experiment at the University of Chile described in
Section II-A in the context of the course “EI2001: Taller de
Proyecto” is continuing in 2016 with only minor changes. The
biggest improvement for 2016 is that students will test and
use the TiL software to submit their pedagogical material, thus
systematizing the calibrated peer reviewing of the pedagogical
videos by the student, in parallel with the more traditional,
paper-based and in-class evaluations of videos by students.
Other, smaller changes are: 1) we switched the order of the
general themes of the videos; 2) we removed several lectures
that focused on teaching the students to use Alice: as it was
designed for use by children rather than university students,
this time can be better spent on group work during class.

B. Chilean Schools

During 2016, we plan to apply the TiL methodology at 4 to
5 Chilean schools. At each school, we will be working with
2 - 3 classes per grade, with an estimated participation of 50
- 90 students and 2 - 3 instructors per school (in total: 250 -
450 students and 10 - 15 instructors).

On July 27th, we will kickoff the current implementation of
this initiative with a 1 day hands-on workshop for the instructors 
from the participating schools. There we will explain the
concept of pedagogical videos/questions and iterative design.
Instructors will then create in groups a video/question about a
predeﬁned topic, submitting them to the TiL system. They will
then evaluate 3 videos from other groups using the TiL system,
and we will wrap-up the workshop with a group discussion
about techniques that instructors can use to improve the quality
of pedagogical videos being created by their students.

Between August and October, students at participating
schools will be asked to create 3 pedagogical videos, in groups
of two to three students. As before, we have deﬁned a general
theme for each of the three videos: half of the students at each
school will work on videos explaining mathematical concepts,
the other half will focus on scientiﬁc concepts (students are
randomly assigned to a theme). Students will have 4 weeks
to create and evaluate each video. During the ﬁrst 3 weeks,
groups select a speciﬁc topic within the assigned theme, create
a pedagogical video and deﬁne a corresponding pedagogical
question, uploading them to the TiL system. In the remaining
week, students will individually evaluate 3 videos on distinct
subtopics created by other groups at the same school. We will
measure student learning before and after each video by asking
students to take multiple choice tests (half the questions will
be about mathematics, the rest about science). We expect to
see an improvement in learning based on the assigned theme.
In order to improve the low participation levels experienced
during 2015, we will conclude this year’s experience with an
inter-school event during November, which will be held at
the University of Chile. Students can participate in this event
by submitting a new pedagogical video, this time about one
of the event’s challenge topics (e.g., sustainability, privacy
and security). During the event, students will get a chance
to watch and evaluate the submitted videos using the TiL
system, as well as to attend talks on the challenge topics.
The submitted videos will also be evaluated according to
three objective criteria (format, duration, relevance) and three
subjective criteria (clarity, pedagogy, originality) by a panel,
and prizes will be awarded to the best videos per challenge
topic. We are expecting 210 participants (40 students and 2
instructors per school).

C. Brazilian College

In Brazil, we plan to apply the “Video Classes” project at the
Federal University of ABC (UFABC). Students of the Bachelor
of Science and Technology program can take the “BC0005
Bases Computacionais da Ciência” course, an introductory
course about computational science. This course gives an
overview of how computing can be used to understand and
solve complex problems, and its application to the production
of scientiﬁc knowledge. Some of the topics covered in this
course are: the graphical representation of functions; statistical
concepts like correlation and regression; databases; logic programming 
and computational modeling. This course is offered
3 times a year, and has approximately 30 students per offering.
During 2016, two of the three offering of this course will
follow the methodology proposed by the “Video Classes”
project, and the third offering will follow the traditional
methodology, thus serving as a control group for the experience.
 Note that complete student and class proﬁles will be
taken into account when comparing the groups and analyzing
differences in results between groups. This proﬁle includes
the year in which students are enrolled in the course, course
evaluations, failure rates, problems that
the students may
have and any other information that may help characterize

the students. We will also apply questionnaires during the
implementation of this project, in order to evaluate if students
learned the concepts being taught, measure any changes in
their critical thinking skills and determine if there are any
educational gains from implementing this project in this sort
of class.

V. DISCUSSION

Instructors deepen their understanding of a subject by
preparing material and assessing student answers. In the
“Teaching is Learning” project, students replicate part of this
experience through the creation and evaluation of pedagogical 
material, but at the same time avoiding the supervision
overhead that would be required in a traditional setting. The
“EI2001: Taller de Proyecto” course has shown that
this
methodology has various positive effects: instructors are better
equipped to detect misconceptions; students are more active
and motivated, as they are in charge of their own education;
both students and instructors are better equipped to detect and
ﬁll gaps in the curriculum; and students develop their critical
thinking skills by reviewing material created by other students.
the increase in
motivation might be merely due to the novelty of the technique,
 an effect that is destined to decrease over time if this
methodology is adopted in a more general way. The most
interesting effect might prove to be the development of critical
thinking in students: in a society that is increasingly connected
and promotes continuous learning, with so many sources of
information, the need for such skills is stronger than ever. The
techniques described in this paper are still quite rough, but
form a ﬁrst step towards a systematic development of critical
thinking skills in secondary and tertiary level students.

Further study is required: for instance,

DATA AND MATERIAL AVAILABILITY

The sources of the software used in this experiment are publicaly 
available and welcoming contributions at https://github.
com/Videoclases/videoclases. The formal source of information 
on the project is available at http://teachingislearning.cl,
while a more informal wiki
is maintained at http://wiki.
teachingislearning.cl/.

ACKNOWLEDGEMENT

This work has been partly funded by the Millennium Nucleus 
RC130003 “Information and Coordination in Networks”.
The authors would like to thank Oscar Jelves and Rodrigo
Prenafeta for their active participation in the design and
implementation of the “Video Classes” project; and Sergio
Celis and Anita Diaz for their advice about the design of the
pedagogical experiments.

REFERENCES

[1] L. von Ahn, M. Blum, and J. Langford, “Telling humans and computers
apart automatically,” Commun. ACM, vol. 47, no. 2, pp. 56–60, 2004.
[Online]. Available: http://doi.acm.org/10.1145/966389.966390

turk,” in Proceedings of

[2] A. Kittur, E. H. Chi, and B. Suh, “Crowdsourcing user studies
the 2008 Conference
with mechanical
on Human Factors
in Computing Systems, CHI 2008, 2008,
Florence, Italy, April 5-10, 2008, M. Czerwinski, A. M. Lund, and
D. S. Tan, Eds. ACM, 2008, pp. 453–456. [Online]. Available:
http://doi.acm.org/10.1145/1357054.1357127

[3] O. Espinoza, D. Castillo, L. Gonzalez, and J. Loyola, “Estudiantes
vulnerables y sus itinerarios educativos en el sistema escolar municipal
en Chile,” Revista Iberoamericana de Educación, vol. 60, 12 2012.

[4] C. Moares and S. Varela, “Student motivation during the teachinglearning 
process,” Transl. Brazil. [Revista Eletrónica da Educação],
vol. 1, no. 1, pp. 1–15, 2007, http://web.uniﬁl.br/docs/revista_eletronica/
educacao/Artigo_06.pdf.

[5] A. Pelizzari, M. Kriegl, M. Baron, N. Finck, and S. Dorocinski, “Theory
of meaningful learning according to Ausubel,” Transl. Brazil. [Revista
PEC], vol. 2, no. 1, pp. 37–42, 2002, http://portaldoprofessor.mec.gov.
br/storage/materiais/0000012381.pdf.

[6] D. Ausubel, “The acquisition and retention of knowledge,” Dordrecht:

Kluwer Academic Publisher, 2000.

[7] C. Luckesi, “Maneiras de avaliar a aprendizagem,” Transl. Brazil [Revista 
Pátio], vol. 3, no. 12, pp. 7–11, 2000.

[8] I. Rubino, K. Nowak, A. Martinez, H. Hipp, and H. G. Monaco, “O
pensamento crítico pode ser fomentado por museus através do uso de
redes sociais? e isso pode ser mensurado?” Revista do programa de
pós-gradua cão em ciência da informa cão da univesidade de Brazília,
vol. 3, pp. 165–186, 2015.

[9] B. S. Bloom, M. B. Engelhart, E. J. Furst, W. H. Hill, and D. R.
Krathwohl, Taxonomy of educational objectives. The classiﬁcation of
educational goals. Handbook 1: Cognitive domain. New York: Longmans 
Green, 1956.

[10] Northern Illinois University, “Measuring critical thinking,” Spring 2006,

the Nuts and Bolts Newsletter from Assessment Services.

[11] D. Garrison, “Critical thinking and adult education: a conceptual model
for developing critical thinking in adult learner,” International Journal
of Lifelong Education, vol. 10, pp. 287–303, 1991.

