Possibilities of Recursive GPU Mapping for Discrete Orthogonal Simplices

Crist´obal A. Navarro
Institute of Informatics,

Benjam´ın Bustos

Department of Computer Science (DCC)

Nancy Hitscheld

Department of Computer Science (DCC)

Universidad Austral de Chile

University of Chile, Santiago, Chile

University of Chile, Santiago, Chile

Valdivia, Chile

Email: cnavarro@inf.uach.cl

Email: bbustos@dcc.uchile.cl

Email: nancy@dcc.uchile.cl

6
1
0
2

 
t
c
O
4
2

 

 
 
]

C
D
.
s
c
[
 
 

1
v
4
9
3
7
0

.

0
1
6
1
:
v
i
X
r
a

Abstract—The problem of parallel thread mapping is studied
for the case of discrete orthogonal m-simplices. The possibility
of a O(1) time recursive block-space map λ : Zm (cid:55)→ Zm is
analyzed from the point of view of parallel space efﬁciency
and potential performance improvement. The 2-simplex and
3-simplex are analyzed as special cases, where constant time
maps are found, providing a potential improvement of up to
2× and 6× more efﬁcient than a bounding-box approach,
respectively. For the general case it is shown that ﬁnding an
efﬁcient recursive parallel space for an m-simplex depends of
the choice of two parameters, for which some insights are
provided which can lead to a volume that matches the msimplex 
for n > n0, making parallel space approximately m!
times more efﬁcient than a bounding-box.

Keywords-GPU computing; thread mapping; discrete orthogonal 
simplices;

I. INTRODUCTION

The ﬁeld of GPU computing has become a well established 
research area in the last ten years [20], [18], [17]
thanks to the high performance of programmable graphics
hardware and the release of a generic GPU programming
model, being CUDA [19] and OpenCL [10] the most known
implementations. In the GPU programming model there are
three constructs1 that allow the execution of highly parallel
algorithms; (1) thread, (2) block and (3) grid. Threads are
the smallest elements and they are in charge of executing the
instructions of the GPU kernel. A block is an intermediate
structure that contains a set of threads organized as an
Euclidean box. Blocks provide fast shared memory access as
well as local synchronization for all of its threads. The grid
is the largest construct of all three and it keeps all blocks
together spatially organized for the execution of a GPU
kernel. These three constructs play an important role when
mapping the execution resources to the problem domain.

For every GPU computation there is a stage where threads
are mapped from parallel to data space. A map, deﬁned
as f : Zk → Zm, transforms each k-dimensional point
x = (x1, x2, ..., xk) in parallel space into a unique mdimensional 
point f (x) = (y1, y2,··· , ym) in data space.
GPU parallel spaces are deﬁned as orthotopes Πm
n in m =

1This work follows the naming scheme by Nvidia CUDA. OpenCL
chooses different names for these constructs; (1) work-element, (2) workgroup 
and (3) work-space, respectively.

1, 2, 3 dimensions2. A known approach for mapping threads
is to build a bounding-box (BB) type of orthotope, sufﬁciently 
large to cover the data space and map threads using
the identity f (x) = x. Such map is highly convenient and
efﬁcient for the class of problems where data space is also
deﬁned by an orthotope; such as vectors, tables, matrices
and box-shaped volumes. But there is a different class of
problems where data space follows a discrete orthogonal
m-simplex organization (see Figure 1).

Figure 1: Discrete orthogonal m-simplices up to m = 3 dimensions.

Problems such as the Euclidean distance matrix (EDM)
[13], [12], [14], collision detection [1], adjacency matrices
[9], cellular automata simulation on triangular/tetrahedral
spatial domains [4], matrix inversion [21], LU/Cholesky
decomposition [5] and the n-body problem [23], [2], [7],
among others, follow the shape of a discrete orthogonal 2-
n) = n(n + 1)/2 ∈
simplex, ∆2
O(n2). The default bounding-box (BB) approach turns out
to be inefﬁcient because the volume of its parallel space,
n), produces n(n − 1)/2 ∈ O(n2) unnecessary threads
V (Π2
(see Figure 2).

n, with a volume of V (∆2

Figure 2: For m = 2, the bounding-box strategy generates a parallel space
P2 that approaches ∼ 2× the required volume.

2Higher dimensional orthotopes can be still be represented by linearizing

to a three-dimensional one.

Problems such as the triple-interaction n-body problem
[11] and triple correlation analysis [6] are represented with
a discrete orthogonal 3-simplex. In the 3-simplex class, data
n) = n(n + 1)(n + 2)/6 ∈ O(n3)
space has a size of V (∆3
elements, organized in a tetrahedral way. Once again, the
default bounding-box (BB) approach is inefﬁcient as it
n) with O(n3) unnecessary
generates a parallel volume V (Π3
threads (see Figure 3).

Figure 3: Bounding-box approach mapped to a discrete orthogonal tetrahedron.


In general, an orthogonal m-simplex is by deﬁnition an
m-dimensional polytope where its facets deﬁne a convex
hull, with one vertex having all of its adjacent facets
orthogonal one to each other. A discrete orthogonal mn 
, is the analog of the continuous one,
simplex, denoted as ∆m
but volumetric and composed of a ﬁnite number of discrete
elements (cid:126)x = {x1, x2, ..., xm} that can be characterized as
+|0 ≤ xi ≤ n ∧ x1 + x2 + ...xm ≤ n}. (1)
n ≡ {(cid:126)x ∈ Zm
∆m
which establishes an upper bound for the absolute Manhattan
distance from any element (cid:126)x to the orthogonal corner of the
m-simplex. The expression for the volume of an m-simplex
is well deﬁned by the Simplicial polytopic numbers

(cid:18)n + m − 1

(cid:19)

=

m

V (∆m

n ) =

be computable by arithmetic and elementary functions, its
complexity increases with m as it requires the solution of
an m-th order equation. Furthermore, the method is limited
to m ≤ 4 as no analytical solutions exist for polynomials of
m ≥ 5. It is of interest then to ﬁnd a different kind of map,
free of such problems.

The limitations of the enumeration principle can be overcome,
 in great part, by taking advantage of the dimensionality 
available in the parallel space. Although parallel spaces
in GPU cannot have a geometry different from an orthotope,
they can be m-dimensional which makes them topologically
equivalent to an m-simplex. Finding an homeomorphism of
the form λ : Zm (cid:55)→ Zm would produce zero dimensional
distance between parallel and data spaces which would free
it from the computation of m-th roots.

This work presents a study of the possibilities of recursive 
GPU mapping of thread-blocks onto m-simplices.
A dedicated analysis is devoted to the special cases of 2simplex 
and 3-simplex, where O(1) time maps are found
and described, offering a space improvement of 2× and 6×,
respectively, that results in a potential performance improvement 
given that no m-roots are required. For general m it is
shown that building an efﬁcient set of recursive orthotopes
requires ﬁnding optimal values for the reduction factor r and
the arity b. Values for both parameters are analyzed, giving
the possibility to build highly tight recursive volumes for
n ≥ n0, making an improvement of m! in parallel space
efﬁciency with respect to the bounding-box approach.

The rest of the manuscript presents related work (Section
II), a formal deﬁnition and analysis of λ((cid:126)ω) (Section III) for
the different cases and ﬁnally the main results are discussed
including future work (Section IV).

n(n + 1)...(n + m − 1)

m!

(2)

II. RELATED WORK

n(cid:88)

which can be proved using an induction [3] on the fact that
is the sum of the volumes of n stacked
the volume of ∆m+1
m-simplexes of lengths {1, 2, 3, ..., n}, i.e.,

n

V (∆m+1

n

) =

V (∆m
i )

(3)

i=1

which when combined with the properties of sums of
binomial coefﬁcients, leads to expression (2). When using
a bounding-box approach, the fraction of extra volume of
V (Πm

n ) that lies outside of the m-simplex approaches to

n→∞ α(Π, ∆)m
lim

n =

V (Πm
n )
V (∆m
n )

− 1 = m! − 1

(4)

making it an inefﬁcient approach for large n as m increases.
A natural enumeration approach can be used by expanding
expression (3) and indexing the elements in a linear way.
Such approach allows to formulate a map of the form g :
Z1 (cid:55)→ Zm with V (Πm
n ). Although g((cid:126)x) may

n ) = V (∆m

Ying et. al. have proposed a GPU implementation for
parallel computation of DNA sequence distances [22] which
is based on the Euclidean distance maps (EDM), a problem
in the 2-simplex class. The authors mention that the problem
domain is indeed symmetric and they do realize that only
the upper or lower triangular part of the interaction matrix
is sufﬁcient. Li et. al. [12] have also worked on GPU-based
EDMs on large data and have also identiﬁed the symmetry
involved in the computation.

Jung et. al. [8] proposed packed data structures for representing 
triangular and symmetric matrices with applications
to LU and Cholesky decomposition [5]. The strategy is
based on building a rectangular box strategy (RB) for
accessing and storing a triangular matrix (upper or lower).
Data structures become practically half the size with respect
to classical methods based on the full matrix. The strategy
was originally intended to modify the data space (i.e., the
matrix), however one can apply the same concept to the
parallel space.

Ries et. al. contributed with a parallel GPU method for
the triangular matrix inversion [21]. The authors identiﬁed
that the parallel space indeed can be improved by using
a recursive partition (REC) of the grid, based on a divide
and conquer strategy. The approach takes O(log2(n)) time
by doing a balanced partition of the structure, from the
orthogonal point to the diagonal.

Q. Avril et. al. proposed a GPU mapping function for
collision detection based on the properties of the uppertriangular 
map [1]. The map is a thread-space function
u(x) → (a, b), where x is the linear index of a thread tx
and the pair (a, b) is a unique two-dimensional coordinate in
the upper triangular matrix. Since the map works in thread
space, the map is accurate only in the range n ∈ [0, 3000]
of linear problem size.

Navarro, Hitschfeld and Bustos have proposed a blockspace 
map function for 2-simplices and 3-simplices [16],
[15], based on the solution of an m order equation that
is formulated from the linear enumeration of the discrete
elements. The authors report performance improvement for
2-simplices. For the 3-simplex case, the mapping technique
is extended to discrete orthogonal tetrahedron, where the
parallel space usage can be 6× more efﬁcient. However the
authors clarify that it is difﬁcult to translate such space
improvement into performance improvement, as the map
requires the computation of several square and cubic roots
that
introduce a signiﬁcant amount of overhead to the
process. From the point of view of data-reorganization, a
succinct blocked approach can be combined along with the
block-space thread map, producing additional performance
beneﬁts with a sacriﬁce of o(n3) extra memory.

The present work proposes a new type of map λ((cid:126)ω) that
uses a recursive organization of blocks but does not require
multiple passes to map threads onto the data space. Instead,
it maps all blocks directly to the data space by using a
ﬂat expression of lower computational cost than the nonlinear 
maps proposed in the past, which were based on the
enumeration principle [1], [16], [15].

III. FORMULATION OF λ((cid:126)ω)

Note: for practical purposes, a discrete orthogonal msimplex 
will be just referred as an m-simplex.

The formulation of λ((cid:126)ω) begins by considering the special
cases m = 2, 3, where the mapping is graphically represented.
 Case m = 1 is not considered as both orthotopes
and simplices match in geometry.
A. Mapping to 2-Simplices

For a 2-simplex the volume of ∆2

n is given by the

triangular numbers

V (∆2

n) =

n(n + 1)

2

(5)

An orthotope Π2
n of selfsimilar 
sub-orthotopes with a recursive structure, giving a

n can be subdivided by a set S2

with a boundary condition of V (S2
k ∈ Z+. Its expanded form produces the sum

2 ) = 1 and n = 2k with

2

n/2)

+ 2V (S2

(cid:17)2
(cid:16) n
+ ··· + 2log2 n(cid:16) n

(cid:17)2

2log2 n

volume of

V (Π2

n) = V (S2

n) =

V (S2

+ 21(cid:16) n
(cid:17)2
n) = 20(cid:16) n
(cid:17)
(cid:16) 1
log2 n(cid:88)

21

22

n2
2

(6)

(cid:17)2

(7)

(8)

i=0 ai =

(cid:33)

(9)

(10)

(11)

(12)

=

where its reduction via the geometric series (cid:80)k

i=1

2i

.

ak+1−1
a−1 , results in

log2 n(cid:88)

(cid:16) 1

(cid:17)i(cid:33)

2

i=0

(1/2)log2 n+1 − 1

1/2 − 1

V (S2

n) =

n2
2

− 1 +

− 1 +

n2
2
n(n − 1)

=

=

= ∆m

n−1.

(cid:32)
(cid:32)

2

The result from expression (11) is equivalent to

V (S2

n) + n = V (S2

n+1) = V (∆2

n).

which means that set S2
n as well as a 2-simplex ∆2
orthotope Π2
block-space homeomorphism λ : Z2
n onto ∆2
Π2
efﬁciency, as shown in Figure 4.

n+1 can be organized both as an
n. Therefore, a proper
+ (cid:55)→ Z2
+ could map
n and provide an improvement in parallel space

Figure 4: Both Π2

n and ∆2

n can be deﬁned by a recursive set S2

n+1.

Let (cid:126)ω = (wx, wy) be a block of threads in parallel space
n (each block is illustrated as a gray lined square in Figure
Π2
4) located at (x, y), with the origin at the top-left corner. The
value wy of a block can be used to obtain the recursion level,
i.e., (cid:98)log2 y(cid:99), which is used to deﬁne the starting height value
b = 2(cid:98)log2 y(cid:99) for the type of orthotope whom wx,y belongs
to. The value q = (cid:98)(wx/b)(cid:99) provides a way to know which
of the sub-orthotopes of the same level wx,y belongs to. The

combination of these parameters allows the formulation of
the homeomorphism

λ((cid:126)ω) = (wx + qb, wy + 2qb)

(13)

that maps in O(1) time, which is a considerable improvement 
over the recursive triangular map that requires
O(log2(n)) recursive steps [21], even when it is still based
on a recursive organization of elements. Additionally, since
blocks have a constant size of ρ2 (cid:28) n, with ρ the number
of threads in each dimension3, the extra number of threads
is no greater than nρ2 ∈ o(n2).

The computation of λ((cid:126)ω) requires a small number of
arithmetic operations and only two elementary functions.
The function (cid:98)log2(y)(cid:99) can be computed using the binary
relation

(cid:98)log2(y)(cid:99) = b − clz(y)

(14)

where b is the number of bits of the word and clz(x)
counts the number of leading zero bits of y. The exponential
2(cid:98)log2(y)(cid:99) can be computed using

2(cid:98)log2(y)(cid:99) = 2 << (b − clz(y))

(15)

Considering that the two elementary functions can be computed 
using bit-level operations, and that the parameters are
re-used by registers, it is expected that the parallel space
improvement from O(n2) to O(n) unnecessary threads (i.e.,
the number of unnecessary threads over the diagonal is no
greater than ρ2n ∈ O(n)) can indeed result in a signiﬁcant
performance improvement, which for the case of triangles is
in the range of 0 ≤ I ≤ 2 [16]. Moreover, since no square
roots are required, λ((cid:126)ω) has the potential to be faster than
previous mapping techniques based on the analytic solution
of a quadratic equation [1], [16].

The analysis of λ(vecω) has assumed problems with sizes
of the form n = 2k. For any value of n, one can use any of
the following approaches:

1) Approach n from above: build a single orthotope Π2
n(cid:48),
where n(cid:48) = 2(cid:100)log2(n)(cid:101) and ﬁlter out the threads outside
the domain. This approach keeps simplicity at the cost
of adding extra threads.

2) Approach n from below: apply a set of orthotopes

(cid:16)

n −(cid:80)i−1

(cid:17)

, Π2
ni

, Π2
n2

, ..., where ni = log2

Π2
k=1 nk
n1
for i ≥ 2, n1 = (cid:98)log2(n)(cid:99), plus a set of more
simpler mappings for the sub-orthotopes that remain
un-mapped at each level. This other approach does not
add extra threads but adds complexity.

Choosing one or the other can depend on the particular
type of problem. Nevertheless, it is important to mention that
in many cases, such as in physical simulations, it is possible
to adapt the problem size to n = 2k, making it possible to
use λ((cid:126)ω) in its intended form.

3For simplicity, equal block dimensions have been chosen, although the

results are not limited to this assumption.

B. Mapping to 3-Simplices

For a 3-simplex of size n per dimension, denoted as ∆3
n,

its volume is given by the tetrahedral numbers

V (∆3

n) =

n(n + 1)(n + 2)

6

.

(16)

It is important to identify that there are multiple ways of extending 
the two-dimensional approach to three dimensions.
One way to formulate Sm=3
is to extend the binary approach
used in 2-simplices, now to half-cubes with an arity of β = 3
for the recursion, as the illustration of Figure 5.

n

Figure 5: Two different views of how the orthotope set (red) maps to the
tetrahedron (white cells) using an arbitrary number of recursions.

From the illustration, the red sub-volumes that form a
structure similar to the Sierpinski gasket correspond to the
parallel space that
is
relevant to know what is the volume of this fractal structure,
relative to the tetrahedron volume.

lies outside of the tetrahedron. It

The recursive orthotope set has the volume expression

(cid:16) n

(cid:17)3

2

log2(n)(cid:88)

(cid:16) 3

(cid:17)i

23

n3
3

V (S3

n) =

+ 3V (S3

n/2) =

i=1
where its reduction via geometric series is
− 3log2(n).

V (S3

n) =

n3
5

(17)

(18)

In the inﬁnite limit of n, the extra volume approaches to

n→∞ α(S, ∆)m
lim

n = lim
n→∞

n3

5 − 3 log2(n)

n(n+1)(n+2)

6

− 1 =

1
5

(19)

Considering that the extra volume constitutes no more than
20% of the volume of the tetrahedron, one can consider
that this recursive strategy does not suffer from signiﬁcant
extra volume problems in m = 3. However, organizing
n of dimensions
the set S3
n into a single major orthotope Π3
(n − 1) × n/2 × (n + 1)/3 to match ∆3
n is not trivial as
the largest sub-orthotope is already greater than (n + 1)/3
and each recursion adds three sub-structures, leaving a gap
when trying to close Π3
n. Forcing the sub-orthotopes to ﬁt
through deformation is neither an efﬁcient approach, as it
would introduce greater complexity to the map λ((cid:126)ω). For

this reason mapping in O(log2(n)) recursive levels is reconsidered 
for 3-simplices, as it is a practical approach that
allows to keep the arithmetic computations simple. The map
λm=3 can be formulated as

λ((cid:126)ω, (cid:126)c, n)m=3 = ϕ((cid:126)ω, (cid:126)c)n/2 + λ((cid:126)ω, (cid:126)c + (

n
2
λ((cid:126)ω, (cid:126)c + (0,

, 0, 0))n/2+
n
2
λ((cid:126)ω, (cid:126)c + (0, 0,

))n/2+

n
2

)+

, 0),
n
2

where (cid:126)c is the relative center and ϕ((cid:126)ω, (cid:126)c)n = w + (cid:126)c for a
cube of n3 blocks. The map begins mapping the major cube
of (n/2)3 blocks to the initial origin (cid:126)c = (0, 0, 0), then it
recursively calls three more maps with the corresponding
new relative origins which are located at the top and the
sides of the cube. This process is repeated until the smallest
sized block is reached. In the end, the total number of map
calls must be at least

3i ≥ 2log2(n)+1 − 1

3 − 1

=

n − 1

2

= O(n).

(20)

log2(n)(cid:88)

i=1

Although the number of map calls is at least linear, in
practice it turns out to be an excessive number of parallel
calls for the GPU computing model, which at the present
time can handle up to 32 concurrent kernels. A more efﬁcient
map free of O(n) recursive calls can be formulated by doing
a small modiﬁcation to the strategy.
C. Alternative Map for 3-Simplices

Although the previous map works in O(log2(n)) time,
its main disadvantage is the number of recursive calls to the
map, making it unlikely to work efﬁcient when implemented
on a GPU.

It is possible to modify the strategy and improve the efﬁciency 
of the parallel space, as well as the map, by realizing
n can actually match the volume
that the recursive set S3
n by taking out one of the recursion
of the tetrahedron ∆3
branches initially established. By doing this, the red subtetrahedrons 
lying in the empty spaces of the Sierpinski
gasket volume can correspond to a unique uncovered subtetrahedron 
of data-space lying inside ∆3
n. The process is
done recursively, making it an effective optimization. The
modiﬁed strategy is illustrated in Figure 6.

With the new approach, the volume of the redeﬁned set
n becomes
S3

(cid:16) n

(cid:17)3

2

V (S3

n) =

+ 2V (S3

n/2) =

n3
2

log2(n)(cid:88)

(cid:16) 1

(cid:17)i

4

i=1

(21)

which can be reduced using the geometric series

V (S3

n) =

= V (∆3

n−1).

(22)

n3 − n

6

Figure 6: Two different views of how the orthotope set (red) maps to the
tetrahedron (white cells) with only two recursion branches.

n). With this new organization it is now possible to
V (∆3
build a O(1) time map free of the problems found in the
original formulation and free of square roots. The mapping
works with a main orthotope of dimensions V (Π3
n) =
for x, y, z, respectively. Figure 7

(cid:17) × 3(n−1)

(cid:17) ×(cid:16) n

(cid:16) n

2

2

4

illustrates the new map.

Figure 7: Two different views of how the orthotope set (red) maps to the
tetrahedron (white cells) with only two recursion branches.

Function λ((cid:126)ω) assumes the origin of Π3

n at the bottomn 
at
right corner from Figure 7 and the the origin of ∆3
the bottom right corner too, with the axes aligned to its
orthogonal sides. The mapping begins by moving the main
sub-orthotope of (n/2)3 directly onto the center of the
tetrahedron with a simple map of the type

n
2

h((cid:126)ω) = (cid:126)ω + (0,

, 0).

At the same time the rest of the sub-orthotopes map as

(ωx + qb, ωy + 2qb, ωz − n/2), inside

(b(1 + 2q) − ωx, 2b(1 + q) − ωy, 2b − ωz + n
2 ),
diagonal ∨ outside

λ((cid:126)ω) =

(23)

Parameters q, b have the same deﬁnitions as in the 2simplex 
map and the total cost is constant in time, i,e,,
T (h(ω)) + T (λ((cid:126)ω)) = O(1) even if done in sequence. The
extra volume introduced by this approach is

V (Πm=3
V (∆m=3

n

n

−1 =

)
)

3n2(n−1)

16

(n−1)n(n+1)

6

−1 =

2
16

(24)

As in the 2-simplex map, the diagonal plane is not considered,
 therefore the relation of data coverage is V (S3
n+1) =

α(Π, ∆)3

n =

n

only 12.5% larger than ∆3

making Πm=3
n. Such amount of
extra volume constitutes a small fraction of the boundingbox 
that surrounds the tetrahedron, which is practically
600% the volume of ∆m
n for large n. For this reason, there is
a potential performance improvement that can be exploited
by GPUs when using the optimized version of λ((cid:126)ω) on 3simplices.


D. Considerations for m-Simplices.

The maps proposed for the 2-simplex and 3-simplex followed 
speciﬁc designs for their corresponding dimensions.
Although the maps take constant time for both cases, it is
important to note that for the 3-simplex it was necessary
to introduce 12% of extra parallel volume in order to ﬁt
the set Sm
n on both Πn and ∆n and produce a single-pass
map. When generalizing the approach to m-simplices, it is
important to ﬁrst verify if V (Sm
n ) satisﬁes as
well as to ﬁnd out how much extra space is introduced.

n ) ≥ V (∆m

The task is to build a set of recursively organized orthon 
, (cid:126)x ∈ Sm
n .
topes Sm
For general m, the volume of a set of recursive orthotopes
n is deﬁned as
Sm

n , where the following satisﬁes ∀(cid:126)x ∈ ∆m
log1/r(n)−1(cid:88)

n ) = (rn)m + βV (Sm

rn) = (rn)m

(βrm)i

V (Sm

(25)
where r is the scaling factor and β the arity of the recursion.
Applying the geometric series, the expression becomes

i=0

(cid:33)

(cid:32)

V (Sm

n ) = (nr)m

(βrm)log1/r(n) − 1

βrm − 1

(26)

(27)

nm − βlog1/r(n)

1/rm − β
=
n ) ≥ V (∆m

.

n−1) must hold. For m = 2,
where at least V (Sm
it is possible verify that setting r = 1/2 and β = 2 leads
to equations (11) and (22) for m = 2, 3, respectively. For
m = 4 the total volume is

n4 − n

(n − 1)n(n + 1)(n + 2)

V (Sm

n ) =

>

14

24

, n ≥ 2

(28)
and for large n the extra volume introduced approaches to
5/7 of ∆m
n . For large n in higher dimensions, the recursive
strategy of using r = 1/2, β = 2 produces a fraction of
extra volume of

(cid:0)(n−1)+m−1

nm−n
2m−2

(cid:1) − 1 =

m

m!

2m − 2

n = lim
n→∞

n→∞ α(S, ∆)m
lim

− 1
(29)
which makes it inefﬁcient for higher dimensions, i.e., for
m = 5 and m = 7 it produces 3× and 39× the volume of
n .
∆m
A more efﬁcient set Sm
n can be found by searching the
right values for r and β in order to satisfy 1/(rm)− β = m!

or at least approach it from below. The restriction however
is that the term βlog1/r(n) needs to be positive and should
not grow too fast as it has an impact on the efﬁciency of
the parallel space.
For example, a value of r = 1/(m−1/m) produces the
required m!, making β a free parameter to be adjusted, with
β ∈ Z+ and β ≥ 2. Choosing β = 2 provides a set Sm
n from a certain n ≥ n0, where n0 is a value
that covers ∆m
that increases with m. It is possible to bring n0 closer to the
origin by increasing β, however the extra volume increases
as well. What is interesting is that from n ≥ n0, the parallel
space is practically m! times more efﬁcient than a bounding
box approach, presenting a great potential for transforming
this space improvement into a performance one. Studying
how parameters r and β can be set and relate to each other
is indeed an interesting open question, since ﬁnding the
best set becomes an optimization problem where the the
difference (1/(rm)− β)− m! and the term βlog1/r(n) are to
be minimized.

n

IV. DISCUSSION

The results from the analysis on recursive GPU mapping
for discrete orthogonal m-simplices can serve as a guide
for implementing efﬁcient GPU computations for interaction
and simulation problems which are often parallelized using
a bounding-box approach due to its simplicity in implementation.
 The 2-simplex and 3-simplex were studied as special
cases, re-deﬁning them as a set of recursive orthotopes. From
the analysis it was possible to formulate new O(1) time maps
with a potential improvement of 2× and 6× respectively.
The generalization to m-simplices presents are greater
challenge, as it has been shown that obtaining an optimal
n of orthotopes with minimal extra volume becomes
set Sm
an optimization problem where the scaling and arity parameters,
 r, β respectively, have to be chosen carefully in order to
ﬁnd a small value n0 from which the mapping can take place
and obtain a volume function that introduces a moderate
amount of extra volume. Knowing what parameters are the
optimal for building a recursive set of orthotopes for any
m-simplex, as well as provide a rule for the shape of the
orthotope container of Sm
n in any dimension, are indeed
interesting questions that require further study in order to
be answered.

ACKNOWLEDGMENT

This project was supported by the research project
FONDECYT No 3160182 from CONICYT, as well as by
the Nvidia CUDA Research Center at the Department of
Computer Science (DCC) from University of Chile.

REFERENCES

[1] Quentin Avril, Val´erie Gouranton, and Bruno Arnaldi. Fast
collision culling in large-scale environments using gpu mapping 
function. In EGPGV, pages 71–80, 2012.

[15] Crist´obal A. Navarro, Benjam´ın Bustos,

and Nancy
Hitschfeld. Potential beneﬁts of a block-space GPU approach
In CLEI-2016, XLII Confor 
discrete tetrahedral domains.
ferencia Latinoamericana de Inform´atica, Valparaiso, Chile,
October 10-14, 2016, 2016.

[16] Cristobal A. Navarro and Nancy Hitschfeld. GPU maps for
the space of computation in triangular domain problems. In
2014 IEEE International Conference on High Performance
Computing and Communications, 6th IEEE International
Symposium on Cyberspace Safety and Security, 11th IEEE International 
Conference on Embedded Software and Systems,
HPCC/CSS/ICESS 2014, Paris, France, August 20-22, 2014,
pages 375–382, 2014.

[17] Cristobal A. Navarro, Nancy Hitschfeld-Kahler, and Luis
Mateu. A survey on parallel computing and its applications
in data-parallel problems using GPU architectures. Commun.
Comput. Phys., 15:285–329, 2014.

[18] John Nickolls and William J. Dally. The gpu computing era.

IEEE Micro, 30(2):56–69, March 2010.

[19] Nvidia-Corporation. Nvidia CUDA C Programming Guide,

2016.

[20] J.D. Owens, M. Houston, D. Luebke, S. Green, J.E. Stone,
and J.C. Phillips. Gpu computing. Proceedings of the IEEE,
96(5):879–899, May 2008.

[21] Florian Ries, Tommaso De Marco, Matteo Zivieri, and
Roberto Guerrieri. Triangular matrix inversion on graphics
processing unit. In Proceedings of the Conference on High
Performance Computing Networking, Storage and Analysis,
SC ’09, pages 9:1–9:10, New York, NY, USA, 2009. ACM.

[22] Zhi Ying, Xinhua Lin, Simon Chong-Wee See, and Minglu
Li. Gpu-accelerated dna distance matrix computation.
In
Proceedings of the 2011 Sixth Annual ChinaGrid Conference,
CHINAGRID ’11, pages 42–47, Washington, DC, USA,
2011. IEEE Computer Society.

[23] Rio Yokota and Lorena A. Barba. Fast n-body simulations

on GPUs. CoRR, abs/1108.5815, 2011.

[2] Jeroen B´edorf, Evghenii Gaburov,

and Simon Portegies 
Zwart. A sparse octree gravitational n-body code that
J. Comput. Phys.,
runs entirely on the GPU processor.
231(7):2825–2839, April 2012.

[3] J. Costello. On the number of points in regular discrete
simplex (corresp.). IEEE Transactions on Information Theory,
17(2):211–212, Mar 1971.

[4] M. Gardner. The fantastic combinations of John Conway’s
new solitaire game “life”. Scientiﬁc American, 223:120–123,
October 1970.

[5] Fred Gustavson. New generalized data structures for matrices
lead to a variety of high performance algorithms. In Roman
Wyrzykowski, Jack Dongarra, Marcin Paprzycki, and Jerzy
Wasniewski, editors, Parallel Processing and Applied Mathematics,
 volume 2328 of Lecture Notes in Computer Science,
pages 418–436. Springer Berlin / Heidelberg, 2006.

[6] Dale A. Huckaby and Lesser Blum. Effect of triplet correlations 
on the adsorption of a dense ﬂuid onto a crystalline
surface. The Journal of Chemical Physics, 97(8):5773–5776,
1992.

[7] Lubomir Ivanov. The n-body problem throughout the computer 
science curriculum. J. Comput. Sci. Coll., 22(6):43–52,
June 2007.

[8] Jin Hyuk Jung and Dianne P. OLeary. Exploiting structure of
symmetric or triangular matrices on a gpu. Technical report,
University of Maryland, 2008.

[9] J. Kepner and J. Gilbert. Graph Algorithms in the Language
of Linear Algebra. Software, Environments, Tools. Society
for Industrial and Applied Mathematics, 2011.

[10] Khronos OpenCL Working Group. The OpenCL Speciﬁcation,
 version 1.0.29, 8 December 2008.

[11] Penporn Koanantakool and Katherine Yelick. A computationand 
communication-optimal parallel direct 3-body algorithm.
In Proceedings of
the International Conference for High
Performance Computing, Networking, Storage and Analysis,
SC ’14, pages 363–374, Piscataway, NJ, USA, 2014. IEEE
Press.

[12] Qi Li, Vojislav Kecman, and Raied Salman. A chunking
method for euclidean distance matrix calculation on large
In Proceedings of the 2010 Ninth
dataset using multi-gpu.
International Conference on Machine Learning and Applications,
 ICMLA ’10, pages 208–213, Washington, DC, USA,
2010. IEEE Computer Society.

[13] D. Man, K. Uda, H. Ueyama, Y. Ito, and K. Nakano.

Implementations 
of parallel computation of euclidean distance
In Networking and
map in multicore processors and gpus.
Computing (ICNC), 2010 First International Conference on,
pages 120–127, 2010.

[14] Duhu Man, Kenji Uda, Yasuaki Ito, and Koji Nakano. A gpu
implementation of computing euclidean distance map with
efﬁcient memory access. In Proceedings of the 2011 Second
International Conference on Networking and Computing,
ICNC ’11, pages 68–76, Washington, DC, USA, 2011. IEEE
Computer Society.

