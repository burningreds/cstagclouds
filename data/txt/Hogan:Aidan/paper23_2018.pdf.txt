A Data-Driven Graph Schema

Larry Gonz´alez1 and Aidan Hogan2

1 Center for Advancing Electronics Dresden (cfaed) TU Dresden, Germany.

2 Millennium Institute for Foundational Research on Data, DCC, University of Chile.

larry.gonzalez@tu-dresden.de

ahogan@dcc.uchile.cl

Abstract. In this paper, we summarise our results on Modelling Dynamics 
in Semantic Web Knowledge Graphs published at WWW 2018
where we proposed a novel data-driven schema for graphs and apply it
for the use-case of predicting high-level changes in Wikidata.

1

Introduction

Graph-based data models [1] have become increasingly common in data management 
scenarios that require ﬂexibility beyond what is oﬀered by traditional
relational databases. Such ﬂexibility is particularly important in Web scenarios,
where potentially many users may be involved (either directly or indirectly) in
the creation, management, and curation of data. An example of such a scenario
is the Wikidata knowledge graph [2] where users can add new properties and
types that can be used to deﬁne further data.

The ﬂip-side of ﬂexibility is higher levels of heterogeneity. Conceptually understanding 
the current state of a knowledge graph – in terms of what data it
contains, what it is missing, how it can be eﬀectively queried, what has changed
recently, etc. – is thus a major challenge: it is unclear how to distil an adequate,
high-level description that captures an actionable overview of knowledge graphs.
We thus need well-founded methodologies to make sense of knowledge graphs,
where an obvious approach is to deﬁne some notion of schema for such graphs.
The traditional approach in the Semantic Web has been what Pham and Boncz [3]
call the schema ﬁrst approach, which deﬁnes the schema that the data should
follow. The most established language for specifying such schemas is RDFS. An
alternative to the schema ﬁrst approach is the schema last approach [3], which
foregoes an upfront schema and rather lets the data evolve naturally; thereafter,
the goal is to understand what the legacy graph data contain by extracting highlevel 
summaries that characterise the graph, resulting in a data-driven schema.
In this paper, we summarise recently published results [4] on a novel approach
to compute a data-driven schema from knowledge graphs. We believe that such
schemas are useful for understanding what a knowledge graph contains, and how
it can be queried, among several other use-cases. Nevertheless, in this work we
focus on the use-case of predicting how the knowledge graph will evolve in future
versions, which could be used for measuring the time-to-live of cached SPARQL
results, identifying missing properties for entities, etc.

2 Preliminaries

RDF is a graph-structured model based on three disjoint sets of terms: IRIs (I),
literals (L) and blank nodes (B). Claims involving these terms can be organised
into RDF triples (s, p, o) ∈ (I∪ B)× I× (I∪ B∪ L), where s is called subject, p is
called predicate, and o is called object. An RDF graph G is then a ﬁnite set of RDF
triples; each such triple (s, p, o) ∈ G can be viewed as a directed labelled edge
p−→ o. The terms used in the predicate position are referred to as
of the form s
properties. We use the term entity to refer to the real-world objects identiﬁed by
the subjects of the graph. Given an RDF graph G, for • ∈ {s, p, o}, we denote by
π•(G) the projection of the set of terms appearing in a particular triple position
in G; e.g., πs(G) := {s | ∃p, o : (s, p, o) ∈ G}. We also use this notation for more
than one triple position, for example, πs,p(G) := {(s, p) | ∃o : (s, p, o) ∈ G}.

3 A Data-Driven Schema for (RDF) Graphs

the (non-empty) set of all subjects S with the (non-empty) characteristic set P .

such that(cid:83)
To deﬁne our data-driven schema proposal, let(cid:74)G(cid:75) ⊆ 2πs(G)×2πp(G) denote a set
(S,P )∈(cid:74)G(cid:75) S × P = πs,p(G), and where for all (S, P ) ∈(cid:74)G(cid:75), it holds
that S (cid:54)= ∅, P (cid:54)= ∅, and there does not exist (S(cid:48), P (cid:48)) ∈ (cid:74)G(cid:75), (S, P ) (cid:54)= (S(cid:48), P (cid:48))
such that S∩S(cid:48) (cid:54)= ∅ or P = P (cid:48). Intuitively, letting(cid:74)s(cid:75)G := {p | ∃o : (s, p, o) ∈ G}
denote the characteristic set of s in G [5], then each pair (S, P ) ∈(cid:74)G(cid:75) represents
We further deﬁne(cid:74)P(cid:75)G = S such that (S, P ) ∈(cid:74)G(cid:75) (or(cid:74)P(cid:75)G = ∅ if no such S
exists), and(cid:74)S(cid:75)G = P such that (S, P ) ∈(cid:74)G(cid:75) (or(cid:74)S(cid:75)G = ∅ if no such P exists).
(cid:74)G(cid:75)∗ :=(cid:74)G(cid:75) ∪ {(∅,∅), ((cid:74)πp(G)(cid:75)G, πp(G))}, adding a bottom and top concept (if
schema proposal is then given by the lattice L = ((cid:74)G(cid:75)∗,⊆).
the full set of subjects. In other words, given a lattice L = ((cid:74)G(cid:75)∗,⊆), we deﬁne
(cid:74)G(cid:75)# := {(n, P ) | ∃S : (S, P ) ∈(cid:74)G(cid:75)∗, n = |S|} with lattice L# := ((cid:74)G(cid:75)#,⊆). We
G = |(cid:74)P(cid:75)G| the number of subjects that P has. Figure 1 provides
denote by(cid:74)P(cid:75)#

Abusing notation, we say that (S, P ) ⊆ (S(cid:48), P (cid:48)) iﬀ P ⊆ P (cid:48). We then also deﬁne
needed) respectively in the ⊆ order. Finally, for a graph G, our data-driven

Given that large-scale knowledge graphs may often have orders of magnitude 
more subjects than predicates, we can greatly reduce the overall (e.g.,
in-memory) size of the lattice by encoding the number of subjects rather than

an example RDF graph and the Hasse diagram for its corresponding L#.

4 Lattice Diﬀ-Algebra

Though we believe that the lattices deﬁned previously may satisfy a number
of applications, we currently focus on the use-case of modelling and predicting
changes in a graph. More speciﬁcally, if we have the lattices for two versions of
a knowledge graph, we can apply a diﬀ to see high-level changes between both
versions. Furthermore, given such a diﬀ, we could further consider adding that
diﬀ to the most recent version to try predict future changes.

:UT :name "U Thurman" ; :star :Gattaca .
:GO :name "G Orwell" ; :writer :1984 .
:AK :name "A Kurosawa" ; :director :Ikiru , :Ran .
:PD :name "PK Dick" ; :writer :Ubik , :Valis .
:CE :name "C Eastwood" ; :director :Sully ;

:star :Unforgiven , :Tightrope .

(0,{d, n, s, w})

(1,{d, n, s})

(1,{d, n})

(1,{n, s})

(2,{n, w})

(0,∅)

Fig. 1: Example RDF graph and corresponding data-driven schema where characteristic 
sets are annotated with the number of subjects (#-lattice). Properties
are abbreviated by their initial: d/:director, n/:name, s/:star, w/:writer

for two versions (i and j) of an RDF graph G. We deﬁne the diﬀ between these

Deﬁning lattice diﬀs Let Li = ((cid:74)Gi(cid:75)∗,⊆) and Lj = ((cid:74)Gj(cid:75)∗,⊆) be the lattices
two lattices as ∆j,i := {((cid:74)s(cid:75)Gj , s,(cid:74)s(cid:75)Gi) | s ∈ πs(Gi ∪ Gj)}; note that(cid:74)s(cid:75)Gj = ∅
for deleted subjects and(cid:74)s(cid:75)Gi = ∅ for new subjects. Given ∆j,i, we also deﬁne a
j,i := {(P (cid:48), n, P ) : n = |{s : (P (cid:48), s, P ) ∈ ∆j,i}|}, where
j,i or 0 if no such n exists.

cardinality-only version ∆#
by ∆#

j,i(P (cid:48), P ) we denote n such that (P (cid:48), n, P ) ∈ ∆#
j,i, and L#

Predicting future #-lattices Given ∆#
k (for k a third version of the
graph), we can “add” the changes between the ith and jth versions to the kth
version to predict the (k+j−i)th version (where typically i < j ≤ k). We will thus
deﬁne the operation L#
[k+j−i].
To apply this operation we consider the ratio of subjects moving from a source to
a target characteristic set. Formally, we deﬁne the ratio of subjects of P (where

j,i as producing a #-lattice L#

k,j,i predicting L#

k +∆#

(cid:54)= 0, P (cid:54)= ∅) moving to P (cid:48) (where P (cid:48) (cid:54)= ∅) as ρj,i(P (cid:48), P ) :=

; in
= 0, we deﬁne ρj,i(P (cid:48), P ) = 1 iﬀ P (cid:48) = P , or 0 otherwise.

j,i(P (cid:48),P )
∆#
(cid:74)P(cid:75)#
k,j,i := ({(σ(P ), P ) | P (cid:54)= ∅ and σ(P ) (cid:54)= 0},⊆), where:

Gi

Gi

Gi

(cid:74)P(cid:75)#
the case that (cid:74)P(cid:75)#


We then deﬁne L−

σ(P ) := round

(cid:88)

Pk∈{P|∃S:(S,P )∈(cid:74)Gk(cid:75)}

 + ∆#

j,i(P,∅) .

ρj,i(P, Pk) ×(cid:74)Pk(cid:75)#

Gk

j,i(P,∅) adds the number of fresh subjects (not appearing in
The summand ∆#
version i) added to P in version j. Finally, we add top and bottom concepts (as
before) to L−

k,j,i to generate the predicted #-lattice L#

k,j,i.

5 Evaluation

We consider 11 weeks of “truthy” RDF dumps of Wikidata from 2017-04-18 to
2017-06-27; the ﬁrst version has 1,102,242,331 triples, 54,236,592 unique subjects
and 3,276 unique properties, while the last version has 1,293,099,057 triples

(+17%), 57,197,406 unique subjects (+5%) and 3,492 unique properties (+6%).
From the last version, with a MapReduce implementation, we extract 2,118,109
characteristic sets in approximately 2.5 hours; computing the lattice by the ⊆
relation then took almost 8 hours on a single machine [4].

To test the quality of the future #-lattices we predict – speciﬁcally the number 
of subjects per characteristic set in the future unseen version – we run
experiments where we consider from 2–5 previous weekly versions to predict the
next version of the #-lattice. As a baseline, for each characteristic set, we apply
linear regression over the number of subjects in that characteristic set for the
previous weeks to predict the number of subjects for the next week; we compare
this baseline with our diﬀ algebra (∆), computing the error with respect to the
real lattice of the predicted week. The results are available in [4], where we show
that our diﬀ algebra outperforms the linear regression baseline method in all
cases; we believe that this is because our diﬀ algebra considers the number of
subjects remaining in source characteristic sets for its predictions whereas the
baseline does not consider where predicted new subjects will come from.

6 Conclusion

We have proposed a form of data-driven schema for large-scale knowledge graphs
and shown it to be feasible to compute. As a concrete use-case, we presented
an algebraic method by which these schemas can be used to predict high-level
changes in the dataset. Our evaluation over 11 weeks of Wikidata demonstrates
that such predictions are feasible to compute; furthermore, we validated the quality 
of predictions made by our algebraic approach against a linear-model baseline.
We refer the interested reader to the full paper [4] for additional details on the
proposed schema, examples of diﬀs, algorithms to compute the lattices, statistics
on the lattices produced, details of the experiments, and further discussion.

Acknowledgements: This work was supported by the Millenium Scientiﬁc Initiative,
 by Fondecyt Grant No. 1181896 and by the German Research Foundation
(DFG) in CRC 912 (HAEC) and in Emmy Noether grant KR 4381/1-1.

References

1. Angles, R., Arenas, M., Barcel´o, P., Hogan, A., Reutter, J., Vrgoˇc, D.: Foundations
of modern query languages for graph databases. ACM Comp. Surveys 50(5) (2017)
2. Vrandecic, D., Kr¨otzsch, M.: Wikidata: a free collaborative knowledgebase. Commun.
 ACM 57(10) (2014) 78–85

3. Pham, M., Boncz, P.A.: Exploiting Emergent Schemas to Make RDF Systems More
Eﬃcient. In: ISWC. Lecture Notes in Computer Science, Springer (2016) 463–479
4. Gonz´alez, L., Hogan, A.: Modelling dynamics in semantic web knowledge graphs

with formal concept analysis. In: WWW. (2018)

5. Neumann, T., Moerkotte, G.: Characteristic sets: Accurate cardinality estimation
for RDF queries with multiple joins. In: ICDE, IEEE Comp. Society (2011) 984–994

