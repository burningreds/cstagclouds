LSQ: The Linked SPARQL Queries Dataset

Muhammad Saleem1(B), Muhammad Intizar Ali2, Aidan Hogan3,

Qaiser Mehmood2, and Axel-Cyrille Ngonga Ngomo1

1 Universit¨at Leipzig, IFI/AKSW, PO 100920, 04009 Leipzig, Germany

2 Insight Center for Data Analytics, National University of Ireland, Galway, Ireland

3 Department of Computer Science, Universidad de Chile, Santiago, Chile

saleem@informatik.uni-leipzig.de

Abstract. We present LSQ: a Linked Dataset describing SPARQL
queries extracted from the logs of public SPARQL endpoints. We argue
that LSQ has a variety of uses for the SPARQL research community, be
it for example to generate custom benchmarks or conduct analyses of
SPARQL adoption. We introduce the LSQ data model used to describe
SPARQL query executions as RDF. We then provide details on the four
SPARQL endpoint logs that we have RDFised thus far. The resulting
dataset contains 73 million triples describing 5.7 million query executions.

1 Introduction

Although there are now hundreds of public SPARQL endpoints available on the
Web – collectively exposing billions of facts and receiving millions of queries per
month – current works suggest that in terms of SPARQL technology, there is
still considerable room for improvement [1,2,9]. Many of these endpoints suﬀer
from availability and performance issues [2]. In addition, the recent recommendation 
of SPARQL 1.1 [6] brings new challenges. Tackling these challenges could
beneﬁt from more data about how users are currently interacting with SPARQL
endpoints and which queries they are sending. Such knowledge may help to focus
research on optimising those queries or query features that are most often used.
Although query logs are available for public SPARQL endpoints through
initiatives like USEWOD [4], the datasets are only accessible after having signed
legal agreements, which limits re-use. Likewise, the format of the raw logs is adhoc 
in nature, depending on their source. We thus introduce the Linked SPARQL
Queries Dataset (LSQ): a public, openly accessible dataset of SPARQL queries
extracted from endpoint logs.1 The current version that we describe in this
paper consists of 73.2 million triples collected from four query logs, which we
have gathered from the maintainers of public endpoints and for which we have
gotten permission to make the logs public. We foresee a number of potential use
cases for such a dataset:

UC1 Custom Benchmarks. The LSQ dataset can be used to generate realistic 
benchmarks by selecting queries matching ad-hoc desiderata [12].

1 The LSQ dataset is available from http://aksw.github.io/LSQ/.
c(cid:2) Springer International Publishing Switzerland 2015
M. Arenas et al. (Eds.): ISWC 2015, Part II, LNCS 9367, pp. 261–269, 2015.
DOI: 10.1007/978-3-319-25010-6 15

262

M. Saleem et al.

UC2 SPARQL Adoption. The data can be used by researchers to conduct

analyses of features used in real-world SPARQL queries [3,10,11].

UC3 Caching. Works on caching [8,14] could beneﬁt from a dataset of realworld 
queries by, e.g., analysing real-world sequences of queries.

UC4 Usability. Analysis of user behaviour – e.g., errors made, how they reﬁne

queries, etc. – could guide the design of better interfaces.

UC5 Meta-Querying. One could ﬁnd out what are the queries that people

are asking about a resource of interest, be it a product, person, city, etc.

These use cases not only require details about queries, but also query executions,
 agents, result sizes, etc. We now describe the LSQ data model, whose goal
is to comprehensively capture all such aspects of query logs.

2 RDF Data Model

Our goal is to create a Linked Dataset describing the SPARQL queries issued to
various public SPARQL endpoints. In Figure 1, we provide an overview of the
core of the schema for the LSQ data-model. Listing 1 provides a comprehensive
example output for a query. The main aspects of the dataset are now presented.2
Queries in the data are typed as a subclass of sq:Query (e.g., lsqv:Select,
lsqv:Ask, etc.). We create query instances for each log whereby a query is
linked to a single endpoint from whose log it was extracted. Hence, if the same
query with the same syntax is issued to the same endpoint multiple times, it is
represented with a single instance of sq:Query, linked to multiple instances
of lsqv:Execution for each time the query was run. Each such execution
instance provides a time (dct:issued) and a unique agent IRI computed from
a cryptographically-hashed and salted I.P. address (lsqv:agent).

To help make the dataset as general as possible, we attach a complete SPIN
representation of the query to each query instance [7]. Given that the SPIN representation 
may involve an arbitrary level of nesting using a variety of predicates,
to make querying LSQ more convenient and eﬃcient, we provide shortcut triples
to indicate the SPARQL query features used in the query. These triples link query
instances (with the predicate lsqv:usesFeature) to instances of sd:Feature.
We enumerate a comprehensive list of such feature instances in our vocabulary,
including lsqv:Filter, lsqv:Optional, lsqv:SubQuery, etc. We also provide
shortcuts to the IRIs and literals mentioned in a query so consumers can easily
ﬁnd all queries about a given resource.

In addition to the query structure, we also provide generic structural statistics 
[1] about the static query including the number of Basic Graph Patterns
(lsqv:bgps) and the number of triple patterns (lsqv:triplePatterns). We
also provide data-driven statistics [1] (incl. the number of results returned and
the query runtime) about the execution of the query. Since such data are not
typically provided by the logs, we generate these statistics by running the query

2 More details are available in the technical report at http://goo.gl/LZehl1.

LSQ: Linked SPARQL Queries Dataset

263

Fig. 1. LSQ data model (dashed lines indicate sub-classes)

locally against an oﬄine copy of the corresponding version of the dataset in question.
 Of course, the resulting statistics may diﬀer to those that occurred during
the original execution logged by the public endpoint. Likewise, these statistics
are computed for a static version of the dataset using Virtuoso 7.1 (16 GB RAM,
6-Core i7 3.40 GHz CPU), where results may vary in other environments. These
data are intended as a guide to query performance/result-size that is provided
“as is” and which a consumer can choose to use or not use as they see ﬁt.

Regarding Linked Data compatibility, we ensure that all query instances and
executions are identiﬁed with dereferenceable IRIs. Our data model also re-uses
class and property terms from established external vocabularies, including SPIN,
DC Terms and SPARQL Service Descriptions. LSQ provides external links to
every resource mentioned in a query. A SPARQL endpoint is also provided.

3 LSQ Dataset Statistics

We applied our extraction process over four SPARQL query logs: DBpedia (logs
from 30/04/2010–20/07/2010; a dataset with 232 million triples), Linked Geo
Data (LGD) (24/11/2010–06/07/2011; with 1 billion triples), Semantic Web
Dog Food (SWDF) (16/05/2014–12/11/2014; with 300 thousand triples) and the
British Museum (BM) (08/11/2014–01/12/2014; with 1.4 million triples). Given
that the logs were in diﬀerent formats (Virtuoso, Sesame and OWLIM), we wrote
scripts to extract and normalise data from each source, mapping them to the
target schema outlined in Section 2. In this section, we give some insights about
the types of queries (and executions) that the resulting LSQ dataset describes.

Query Analysis: Table 1 provides high-level analysis of the queries appearing 
in the four logs. While the majority of queries are SELECT (91.6% overall),
SWDF contains a large number of DESCRIBE queries (31.1%). The BM query

264

M. Saleem et al.

Listing 1. An example LSQ representation of an SWDF query

@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix lsqr: <http://lsq.aksw.org/res/> .
@prefix lsqrd: <http://lsq.aksw.org/res/SWDF-> .
@prefix lsqv: <http://lsq.aksw.org/vocab#> .
@prefix sp: <http://spinrdf.org/sp#> .
@prefix dct: <http://purl.org/dc/terms/> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .

# QUERY INSTANCE META-DATA
lsqrd:q483 lsqv:endpoint <http://data.semanticweb.org/sparql> ;

sp:text """SELECT DISTINCT ?prop
WHERE {

?obj rdf:type swdf:SessionEvent .
?obj ?prop ?targetObj .
FILTER (isLiteral(?targetObj)) }
LIMIT 150""" .

# STRUCTURAL META-DATA
lsqrd:q483 lsqv:bgps 1 ; lsqv:triplePatterns 2 ; lsqv:joinVertices 1 ;

lsqv:meanJoinVerticesDegree 2.0 ;
lsqv:usesFeature lsqv:Filter , lsqv:Distinct , lsqv:Limit ;
lsqv:mentionsSubject "?obj" ;
lsqv:mentionsPredicate "?prop" , rdf:type ;
lsqv:mentionsObject "?targetObj" , swdf:SessionEvent ;
lsqv:joinVertex lsqr:q483-obj .

lsqr:q483-obj lsqv:joinVertexDegree 2 ; rdf:type lsqv:Star .

# DATA-SENSITIVE META-DATA
lsqrd:q483 lsqv:resultSize 16 ; lsqv:runTimeMs 6 ;

lsqv:meanTriplePatternSelectivity 0.5007155695730322 ;

# QUERY EXECUTION META-DATA
lsqrd:q483 lsqv:execution lsqrd:q483-e1 , lsqrd:q483-e2 , lsqrd:q483-e3 , lsqrd:q483-e4 .
lsqrd:q483-e1 lsqv:agent lsqr:A-WlxKE0QQRlhCUBdGRx1QGVRbQRNsN2YUWF5W ;

dct:issued "2014-05-22T17:08:17+01:00"^^xsd:dateTimeStamp .

lsqrd:q483-e2 lsqv:agent lsqr:A-WlxKE0QQRlhCUBdGRx1QGVRdRBNsN2YUW1pS ;

dct:issued "2014-05-20T14:34:35+01:00"^^xsd:dateTimeStamp .

lsqrd:q483-e3 lsqv:agent lsqr:A-WlxKE0QQRlhCUBdGRx1QGVRdRBNsN2YUW1pS ;

dct:issued "2014-05-20T14:28:37+01:00"^^xsd:dateTimeStamp .

lsqrd:q483-e4 lsqv:agent lsqr:A-WlxKE0QQRlhCUBdGRx1QGVRdRBNsN2YUW1pS ;

dct:issued "2014-05-20T14:24:13+01:00"^^xsd:dateTimeStamp .

# SPIN REPRESENTATION
lsqrd:q483 a sp:Select ;

sp:distinct true ; sp:limit "150"^^xsd:long ;
sp:resultVariables ( [ sp:varName "prop"^^xsd:string ] ) ;
sp:where (

[ sp:subject [ sp:varName "obj"^^xsd:string ] ;

sp:predicate rdf:type ;
sp:object <http://data.semanticweb.org/ns/swc/ontology#SessionEvent>

]
[ sp:subject

[ sp:varName "obj"^^xsd:string ] ;

sp:predicate [ sp:varName "prop"^^xsd:string ] ;
sp:object [ sp:varName "targetObj"^^xsd:string ]

]
[ a sp:Filter ;

sp:expression [ a sp:isLiteral ; sp:arg1 [ sp:varName "targetObj"^^xsd:string ] ]

]

) .

LSQ: Linked SPARQL Queries Dataset

265

Table 1. High-level analysis of the queries and query executions in the LSQ dataset
for each log (QE = Query Executions, UQ = Unique Queries, PE = Parse Errors, RE
= Runtime Error, ZR = Zero Results, SEL = SELECT, CON = CONSTRUCT, DES
= DESCRIBE; percentages are with respect to UQ)

Dataset

QE
№

UQ
№

PE
№

RE
№

ZR SEL CON DES ASK
%

№

%

%

%

DBpedia 1,728,041 1,208,789 426,425
LGD
13,546
13,645
SWDF
BM
129,989 100,916

1,656,254
1,411,483
879,426

311,126
99,165

69,523 176,257 94.6
50,059 143,574 89.3
25,674 68.8
29,073
100

475
0

0.1
0.9
0.0
2.3
0.0 31.1
0.0
0.0

Overall

5,675,204 1,749,069 554,532 120,057 374,578 91.6

1.2

2.3

4.4
8.4
0.1
0.0

4.9

Table 2. Percentage of unique queries containing diﬀerent types of joins (a query may
contain multiple join types).

Dataset

DBpedia
LGD
SWDF
BM

Overall

Star

Path

Hybrid

Sink

No Join

%

%

38.58
28.18
10.70
0.00

33.05

8.60
9.46
11.25
0.00

8.79

%

6.79
7.57
4.01
0.00

6.62

%

6.31
1.24
0.93
0.00

4.51

%

61.23
72.00
84.25
100.00

66.51

log contains a noticeably high ratio of parse errors (77.63%), compared with
DBpedia (35.27%), SWDF (13.75%), or LGD (4.35%).3 Conversely, while LGD
is the lowest in terms of parse errors, it generates the highest ratio of runtime
errors (16.08%), followed by DBpedia (5.54%), SWDF (0.05%), and BM (0%).
Often these are timeouts, which will, in practice, occur more frequently for larger
datasets.

Table 2 shows the popularity of join types as deﬁned previously in [13]. The
idea is to count individual join variables within a BGP as individual joins and
type them depending on how they connect triple patterns. We say that a join
vertex has an “outgoing link” if it appears as a subject of a triple pattern, and
that it has an “incoming link” if it appears as predicate or object. Star has
multiple outgoing links but no incoming links. Path has precisely one incoming
and one outgoing link. Hybrid has at least one incoming and outgoing link and
three or more links. Sink has multiple incoming links but no outgoing links.
From Table 2, we see that most queries are Star (33.1%) or contain no join
(66.5%); again we see the uniformity of BM queries suggesting the inﬂuence of
one agent.

3 We suspect that for BM, one automated agent is asking a high volume of simple
potentially “invalid” queries to the endpoint; unfortunately the BM log did not
include agent data, so we can neither conﬁrm nor refute this possibility.

266

M. Saleem et al.

Table 3. Comparison of the mean values of diﬀerent query features across all query
logs (RS = Result Size, TPs = Triple Patterns, JVs = Join Vertices, MJVD = Mean
Join Vertex Degree, MTPS = Mean Triple Pattern Selectivity)

Dataset

RS BGPs TPs

JVs MJVD MTPS Runtime (ms)

DBpedia
LGD
SWDF
BM

87.57
161.90
19.65
0.00

1.81
1.75
2.57
1.00

2.22
2.16
2.94
1.00

0.40
0.37
0.26
0.00

Overall

122.45

1.74

2.04

0.24

0.78
0.75
0.35
0.00

0.45

0.002
0.030
0.025
0.000

0.013

20.26
32.28
11.98
6.78

26.40

Table 4. Percentage of queries using various speciﬁc SPARQL features

Dataset UNION OPTIONAL DISTINCT FILTER REGEX SERVICE Sub-Query

DBpedia
LGD
SWDF
BM

Overall

4.42
9.65
32.71
0.00

7.64

36.20
25.10
25.32
0.00

31.78

18.44
22.25
45.40
100.00

23.47
31.10
0.95
0.00

23.30

23.19

2.90
1.25
0.06
0.00

2.22

0.0005
0.0000
0.0012
0.0000

0.0004

0.00
0.01
0.02
0.00

0.01

Table 3 shows the mean values for various query features across all query
logs. These features are often considered, e.g., when designing SPARQL benchmarks 
[1,5]. The SWDF queries are generally more complex, on average, in terms
of the number of BGPs and total number of triple patterns. However, they contain 
fewer joins among triple patterns and the join vertex degree is also quite low
(e.g., 0.35 for SWDF vs. 0.78 for DBpedia). We also see that slower runtimes
correspond with larger dataset sizes. We again see that the BM queries often
return zero results, suggesting again a high volume of simple, synthetic queries.
Tables 4 and 5 show the percentage use of (groups of) diﬀerent SPARQL
features [3]; a query is counted in a group if it uses one such feature. We found
that the SPARQL 1.1 features are rarely used; however, in the case of DBpedia
and LGD, this may be due to the age of the logs. The most widely used feature 
is OPTIONAL (31.78%), followed by DISTINCT (23.3%) and FILTER (23.19%).
Solution modiﬁers (i.e., LIMIT, OFFSET, ORDER BY) are also quite often used
(18.11%).

Execution and Agent Analysis: Thus far we have analysed unique queries.
We now look at (a) whether the same queries tend to be executed many times
and (b) how many agents are responsible for how many executions.

With respect to the number of times a given query is executed, if we take
the total number of query executions (5,675,204) and the total number of unique
queries (1,749,069) from Table 1, we can see that a given (syntactically identical)
query is executed on average about 3.2 times in the scope of the logs deﬁned.

LSQ: Linked SPARQL Queries Dataset

267

Table 5. Percentage of queries using various classes of features

Dataset Solution Mod. Aggregates

(¬)Exists Binding Graph

DBpedia
LGD
SWDF
BM

Overall

1.036
60.443
33.265
0.000

18.117

s
n
o
i
t
u
c
e
x
e

y
r
e
u
q

f
o

o
i
t
a
r

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

equality
SWDF
BRM

DBpedia

LGD

0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

1

ratio of unique queries

0.001
0.007
2.405
0.000

0.174

s
n
o
i
t
u
c
e
x
e

y
r
e
u
q

f
o

o
i
t
a
r

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

0.001
0.000
0.001
0.000

0.001

0.000
0.000
0.008
0.000

0.001

0.002
0.000
0.001
0.000

0.001

equality
SWDF

DBpedia

LGD

0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

1

ratio of agents

Fig. 2. Lorenz curve for distribution of
executions per unique query

Fig. 3. Lorenz curve for distribution of
executions per unique agent

To compare this distribution for the four logs, Figure 2 provides a Lorenz curve,
which shows what (maximal) ratio of unique queries account for what (minimal)
ratio of query executions. For example, we see that for SWDF, 80% of the unique
queries account for about 10% of the overall executions, or equivalently that the
top 20% most frequently executed queries account for 90% of all executions. On
the other hand, the executions for DBpedia are much more evenly spread. For
LGD, the sharp ascent of the curve suggests that a handful of unique queries
are run a great many times and form the overall majority of executions.

Regarding unique agents, DBpedia had 3,041, LGD had 725 and SWDF had
274; we did not have agent data for BM. Figure 3 presents the Lorenz curve
of how executions are distributed amongst agents, in which we can see a heavy
skew; for example, 90% of the agents with fewest executions are cumulatively
responsible for fewer than 3% of the total executions (2.7% for DBpedia, 0.7%
for LGD, and 0.2% for SWDF). From this curve, we posit that most queries
encountered in these logs are from a few high-volume, automated agents; this
should potentially be taken into account by users of the LSQ dataset (again, our
goal is to provide the queries from real-world logs “as is”).

268

M. Saleem et al.

4 Conclusions and Future Directions

In this paper we presented LSQ, which is (to the best of our knowledge) the
ﬁrst public Linked Dataset describing SPARQL queries issued to endpoints. We
introduced various use cases for LSQ, detailed our data model, and analysed
the results of RDFising logs from four endpoints. The current version of LSQ
contains 73 million triples describing 5.7 million query executions.

We are currently collecting logs from other SPARQL endpoints (e.g., Bioportal,
 Strabon) that will be added into LSQ. We likewise hope to update and
extend logs from current endpoints (esp. DBpedia). We will also link the dataset
with the benchmark generation framework FEASIBLE to ease the development
of benchmarks customised towards speciﬁc software applications or algorithms.
The Linked Dataset, a SPARQL endpoint, and complete dumps are all available
on the LSQ homepage – http://aksw.github.io/LSQ/ – along with pointers to
code, a VoID description, example LSQ queries, and various other dataset assets.

Acknowledgments. This work was supported in part by a research grant from
the German Ministry for Finances and Energy under the SAKE project (Grant No.
01MD15006E), by Science Foundation Ireland (SFI) under Grant No. SFI/12/RC/2289,
by the Millennium Nucleus Center for Semantic Web Research under Grant No.
NC120004 and by Fondecyt Grant No. 11140900.

References

1. Alu¸c, G., Hartig, O., ¨Ozsu, M.T., Daudjee, K.: Diversiﬁed stress testing of RDF
data management systems. In: Mika, P., Tudorache, T., Bernstein, A., Welty, C.,
Knoblock, C., Vrandeˇci´c, D., Groth, P., Noy, N., Janowicz, K., Goble, C. (eds.)
ISWC 2014, Part I. LNCS, vol. 8796, pp. 197–212. Springer, Heidelberg (2014)

2. Buil-Aranda, C., Hogan, A., Umbrich, J., Vandenbussche, P.-Y.: SPARQL webquerying 
infrastructure: ready for action? In: Alani, H., Kagal, L., Fokoue, A.,
Groth, P., Biemann, C., Parreira, J.X., Aroyo, L., Noy, N., Welty, C., Janowicz, K.
(eds.) ISWC 2013, Part II. LNCS, vol. 8219, pp. 277–293. Springer, Heidelberg (2013)
3. Arias, M., Fern´andez, J.D., Mart´ınez-Prieto, M.A., de la Fuente, P.: An empirical

study of real-world SPARQL queries. CoRR (2011)

4. Berendt, B., Hollink, L., Hollink, V., Luczak-R¨osch, M., M¨oller, K., Vallet, D.: Usage

analysis and the web of data. SIGIR Forum 45(1), 63–69 (2011)

5. G¨orlitz, O., Thimm, M., Staab, S.: SPLODGE: systematic generation of SPARQL
benchmark queries for linked open data. In: Cudr´e-Mauroux, P., Heﬂin, J., Sirin, E.,
Tudorache, T., Euzenat, J., Hauswirth, M., Parreira, J.X., Hendler, J., Schreiber, G.,
Bernstein, A., Blomqvist, E. (eds.) ISWC 2012, Part I. LNCS, vol. 7649, pp. 116–132.
Springer, Heidelberg (2012)

6. Harris, S., Seaborne, A., Prud’hommeaux, E. (eds.): SPARQL 1.1 Query Language.

W3C Recommendation, March 21, 2013

7. Knublauch, H., Hendler, J.A., Idehen, K. (eds.): SPIN - Overview and Motivation.

W3C Member Submission, February 22, 2011

LSQ: Linked SPARQL Queries Dataset

269

8. Lampo, T., Vidal, M.-E., Danilow, J., Ruckhaus, E.: To cache or not to cache:
the eﬀects of warming cache in complex SPARQL queries. In: Meersman, R.,
Dillon, T., Herrero, P., Kumar, A., Reichert, M., Qing, L., Ooi, B.-C., Damiani, E.,
Schmidt, D.C., White, J., Hauswirth, M., Hitzler, P., Mohania, M. (eds.) OTM 2011,
Part II. LNCS, vol. 7045, pp. 716–733. Springer, Heidelberg (2011)

9. Morsey, M., Lehmann, J., Auer, S., Ngonga Ngomo, A.-C.: DBpedia SPARQL
benchmark – performance assessment with real queries on real data. In: Aroyo, L.,
Welty, C., Alani, H., Taylor, J., Bernstein, A., Kagal, L., Noy, N., Blomqvist, E. (eds.)
ISWC 2011, Part I. LNCS, vol. 7031, pp. 454–469. Springer, Heidelberg (2011)

10. Picalausa, F., Vansummeren, S.: What are real SPARQL queries like?. In: SWIM

(2011)

11. Rietveld, L., Hoekstra, R.: Man vs. machine: diﬀerences in SPARQL queries. In: USEWOD 
(2014)

12. Saleem, M., Mehmood, Q., Ngomo, A.-C.N.: FEASIBLE: a featured-based SPARQL

benchmark generation framework. In: ISWC (2015) (to appear)

13. Saleem, M., Ngonga Ngomo, A.-C.: HiBISCuS: hypergraph-based source selection for
SPARQL endpoint federation. In: Presutti, V., d’Amato, C., Gandon, F., d’Aquin,
M., Staab, S., Tordai, A. (eds.) ESWC 2014. LNCS, vol. 8465, pp. 176–191. Springer,
Heidelberg (2014)

14. Williams, G.T., Weaver, J.: Enabling ﬁne-grained HTTP caching of SPARQL query
results. In: Aroyo, L., Welty, C., Alani, H., Taylor, J., Bernstein, A., Kagal, L.,
Noy, N., Blomqvist, E. (eds.) ISWC 2011, Part I. LNCS, vol. 7031, pp. 762–777.
Springer, Heidelberg (2011)

