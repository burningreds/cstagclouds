An Empirical Evaluation of Intrinsic

Dimension Estimators

Cristian Bustos1, Gonzalo Navarro2, Nora Reyes1, and Rodrigo Paredes3(B)

1 Departamento de Inform´atica, Universidad Nacional de San Luis,

2 Department of Computer Science, Center of Biotechnology and Bioengineering,

San Luis, Argentina

{cjbustos,nreyes}@unsl.edu.ar

University of Chile, Santiago, Chile

3 Departamento de Ciencias de la Computaci´on, Universidad de Talca, Curic´o, Chile

gnavarro@dcc.uchile.cl

raparede@utalca.cl

Abstract. We study the practical behavior of diﬀerent algorithms that
aim to estimate the intrinsic dimension (ID) in metric spaces. Some of
these algorithms were speciﬁcally developed to evaluate the complexity
of searching in metric spaces, based on diﬀerent theories related to the
distribution of distances between objects on such spaces. Others were
originally designed for vector spaces only, and have been extended to
general metric spaces. To empirically evaluate the ﬁtness of various ID
estimations with the actual diﬃculty of searching in metric spaces, we
compare one representative of each of the broadest families of metric
indices: those based on pivots and those based on compact partitions.
Our preliminary conclusions are that Fastmap and the measure called
Intrinsic Dimensionality ﬁt best their purpose.

1 Introduction

Similarity search in metric spaces has received much attention due to its applications 
in many ﬁelds, ranging from multimedia information retrieval to machine
learning, classiﬁcation, and searching the Web. While a wealth of practical algorithms 
exist to handle this problem, it has been often noted that some datasets
are intrinsically harder to search than others, no matter which search algorithms
are used. An intuitive concept of “curse of dimensionality” has been coined to
denote this intrinsic diﬃculty, but a clear method to measure it, and thus to
predict the performance of similarity searching in a space, has been elusive.
The similarity between a set of objects U is modeled using a distance function
(or metric) d : U × U (cid:2)→ R+ ∪ {0} that satisﬁes the properties of triangle
inequality, strict positivity, reﬂexivity, and symmetry. In this case, the pair (U, d)
is called a metric space [6,21,24].

Partially funded by basal funds FB0001, Conicyt, Chile and Fondecyt grant 1131044,
Chile.

c(cid:2) Springer International Publishing Switzerland 2015
G. Amato et al. (Eds.): SISAP 2015, LNCS 9371, pp. 125–137, 2015.
DOI: 10.1007/978-3-319-25087-8 12

126

C. Bustos et al.

In some applications, the metric spaces are of a particular kind called “vector
spaces”, where the elements consist of D coordinates of real numbers and the
distance is some Minkowski metric. Many works exploit the geometric properties
of vector spaces, but they usually cannot be extended to general metric spaces,
where the only available information is the distance between objects. Since in
most cases the distance is very expensive to compute, the main goal when searching 
in metric spaces is to reduce the number of distance evaluations. In contrast,
vector space operations tend to be cheaper and the primary goal when searching
them is to reduce the CPU cost or the number of I/O operations carried out.
Similarity queries are usually of two types. For a given database S ⊆ U with
size |S| = n, q ∈ U and r ∈ R+, the range query (q, r)d returns all the objects
of S at distance at most r from q, whereas the nearest neighbor query kNNd(q)
retrieves the k elements of S that are closest to q.

A na¨ıve way to answer similarity queries is to compare all the database
elements with the query q and return those that are close enough to it. This
brute force approach is too expensive for real applications. Research has then
focused on ways to reduce the number of distance computations performed to
answer similarity queries. There has been signiﬁcant progress around the idea of
building an index, that is, a data structure that allows discarding some database
elements without explicitly comparing them to q.

In uniform vector spaces, the curse of dimensionality describes the wellknown 
exponential increase of the cost of all existing search algorithms as the
dimension grows. Non-uniform vector spaces may be easier to search than uniform 
ones, despite having the same explicit dimensionality. The phenomenon
also extends to general metric spaces despite their absence of coordinates: some
spaces are intrinsically harder to search than others. This has lead to the concept
of intrinsic dimensionality (ID) of a metric space, as a measure of the diﬃculty
of searching it. A reliable measure of ID has been elusive, despite the existence
of several formulas.

Computing the ID of a metric space is useful, for example, to determine
whether it is amenable to indexing at all. If the ID is too high, then we must
just resort to brute-force solutions or to approximate search algorithms (which
do not guarantee to ﬁnd the exact answers). Even when exact indexing is possible,
 the ID helps decide which kind of index to use. For example, pivot-based
methods work better on lower dimensions, whereas compact partiioning methods
outperform them in higher dimensions [6].

In this work we aim to empirically study the ﬁtness of various ID measures
to predict the search diﬃculty of metric space searching. Some measures were
speciﬁcally developed for metric spaces, based on diﬀerent theories related to
the distribution of distances between objects. Others were originally designed
for vector spaces and have then been adapted to general metric spaces. We
chose various synthetic and real-life metric spaces and two indexing methods
that are representatives of the major families of indices: one based on pivots
one and another based on compact partitions. Our comparison between real and

An Empirical Evaluation of Intrinsic Dimension Estimators

127

estimated search diﬃculty yield, as preliminary conclusions, that Fastmap [10]
and the formula by Ch´avez et al. [6] are currently the best predictors in practice.

2 Intrinsic Dimension Estimators for Vector Spaces

There are several interesting applications where the data are represented as
D. For instance, in pattern recognition applications,
D-dimensional vectors in R
D,
objects are usually represented as vectors [14]. So, data are embedded in R
even though this does not imply that its intrinsic dimension is D.

There are many deﬁnitions of ID. For instance, the ID of a given dataset is
the minimum number of free variables needed to represent the data without loss
D has ID M ≤ D, if its
of information [2]. In general terms, a dataset X ⊆ R
D [12]. Another
elements fall completely within an M-dimensional subspace of R
intuitive notion is the logarithm of the search cost, as in many cases this cost
grows exponentially with the dimension.

M

Even in vector spaces, there are many reasons to estimate the ID of a dataset.
Using more dimensions (more coordinates in the vectors) than necessary can
bring several problems. For example, the space to store the data may be an
D with |X| = n requires to store n × D real coordinates.
issue. A dataset X ⊆ R
Instead, if we know that the ID of X is M ≤ D, we can map the points to R
and just store n × M real coordinates. The CPU cost to compute a distance is
also reduced. This can in addition help identify the important dimensions in the
original data.

There are two approximations to estimate the ID of a vector space [2,14],
namely, local and global methods. The local ones make the estimation by using
the information contained in sample neighborhoods, avoiding the data projection
over spaces of lower dimensionality. The global ones deploy the dataset over an
M-dimensional space using all the dataset information.

In this work we focus on global ID estimators. That is, we consider all the
dataset information to estimate the ID as accurately as possible. Global methods
can be split into three families: projection techniques, multidimensional scaling
methods, and fractal based methods. The last two are more suitable to extend
to metric spaces, so we have selected and adapted some representatives of these
groups.

3 Intrinsic Dimension Estimators for Metric Spaces

In this section we analyze various methods to estimate the ID of vector spaces
and others to general metric spaces. We discuss how to adapt the former to the
case of general metric spaces. Note that, since multidimensional spaces are a
particular case of metric spaces, our estimators can also be applied to obtain the
ID of D-dimensional vector spaces.

128

C. Bustos et al.

3.1 Fractal Based Methods

Unlike other families, fractal based methods can estimate non-integer ID values.
The most popular techniques of this family are Box Counting [17], which is a
simpliﬁed version of the Haussdorﬀ dimension [9,18], and Correlation [3].

The dimension estimation by Box Counting DB of a set Ω ⊆ R

D is deﬁned
as follows: if v(r) is the number of boxes of size r needed to cover Ω, then
DB = limr→0

ln(v(r))
ln( 1

r ) .

In this method, the boxes are multidimensional regions of side r on each
dimension (that is, they are hypercubes of side r). Regrettably, even though
eﬃcient algorithms have been proposed, the Box Counting dimension can be
computed only for low dimension datasets, because its algorithmic complexity
grows exponentially with the dimension.
Estimating the dimension by Correlation is an alternative to Box Counting.
 It is deﬁned as follows. Let Ω = {x1, x2, . . . , xn} ⊂ R
D and the correla-
1≤i<j≤n I(||xj − xi|| < r), where I(·) is
tion integral Cm(r) = limn→∞ 2
the indicator function. Intuitively, Cm(r) is the fraction of object pairs whose
distance is lower than r. So, the dimension estimation by Correlation DC is
DC = limr→0

The most popular method to estimate the dimension by Correlation and
Box Counting is the log-log plot. It consists in plotting ln(Cm(r)) versus ln(r).
The dimension by Correlation is the slope of the linear section of the curve. To
estimate the dimension by Box Counting we replace ln(Cm(r)) by ln(v(r)).

n(n−1)

ln(Cm(r))

(cid:2)

ln r

.

In the general case of metric spaces, we do not have coordinates in general.
Thus, to adapt the Box Counting method, we consider balls of radius r, that is,
the set of objects within a distance r from a reference object o. We randomly
pick the reference objects from the dataset, and count the number B(r) of balls
of radius r needed to cover the dataset. To do so, we use the List of Clusters
(LC) index [5], whose code is available from SISAP [11], with the variant of ﬁxed
radius and centers chosen at random. Then the ID is just the length of the LC.
To estimate the dimension by Box Counting, which in this case is Ball Countin 
log-log and
ing, we replace ln(v(r)) by ln(B(r)), plot ln(B(r)) versus ln
obtain the slope of the linear section of the curve by using linear regression with
least squares over the experimental data

ln(B(r)), ln

(cid:4)(cid:4)

1
r

(cid:3)

(cid:4)

(cid:3)

(cid:3)

1
r

.

3.2 Distance Exponent

Traina et al. [22] discuss the problem of the selectivity estimation for range
queries in real-world metric spaces,
including spatial or multidimensional
datasets as special cases. Their main ﬁnding is that several datasets follow the
so-called Power Law. They call Distance Exponent the exponent of the power
law, and show how to use it to derive formulas for estimating the selectivity
of range queries. For instance, the number of objects relevant to the query, the
number of I/Os to answer the query when the data is stored on disk, the amount
of time needed to answer the query, and so on.

An Empirical Evaluation of Intrinsic Dimension Estimators

129

To ﬁnd a formula that estimates the number of neighbors of objects within
a distance r in a n-objects dataset, they introduce the following notions: (i) the
Distance Plot of a metric set is the number of object pairs at distance at most
r versus the distance r, and both axes are drawn in logarithmic scale; and (ii)
the Distance Exponent is the slope of the line that better ﬁts the distance plot
in case it is linear for a range of scales. Using these two notions, they deﬁne the
Distance Law.
Distance Law - Given a dataset of n objects from a metric space with distance
function d(x, y), the average number of distances lower than a radius r follows
a power law; that is, the average number of neighbors nb(r) within a distance r
is proportional to rD. Formally, N · Φ(r) = nb(r) ∝ rD.

If a dataset has a metric to evaluate the distance between every object pair,
then this plot can always be drawn. They show that the distance plot has an
almost linear behavior for many databases, both real and synthetic. Building the
distance plot requires O(n2) distance computations. To reduce this cost, nb(r) is
estimated using an index [22], in particular the M-tree [7]. Since in this work we
are only interested in comparing the diﬀerent ID measures, indexing the space
is not necessary and we compute nb(r) directly, considering a reference object
chosen at random from the dataset. We only determine the number of elements
at distance r from that object. The result is averaged over various choices for
the object.

3.3 Fastmap

This method arises from the proposal [13] of a fast algorithm to map objects of
any metric space onto points of a k-dimensional space (k being deﬁned by the
user), so that the dissimilarities are preserved. Its goal is to speed up searches
in traditional or multimedia databases.

To do so, the objects are mapped onto the k-dimensional space using k feature
extraction functions, provided by domain experts [13]. The main issue is how to
deﬁne such feature extraction functions. For example, in the metric case of strings
with the edit distance [16], it is not clear which features can be considered.

For a domain expert, it is generally easier to provide a distance function to
compare objects than to provide feature extraction functions. Fastmap [10] is a
generalization of the original method [13], where the objects are mapped using
only a distance function.

Fastmap ﬁnds, given a dataset of n objects from a metric space (U, d), n
image points in a k-dimensional target space, such that the distances between
the objects in the original space are preserved as much as possible in the target
space.
For evaluating the dissimilarity preservation in the target space, a stress function 
is deﬁned as follows, stress2 = ((cid:2)
, where dij is the dissimilarity
measure (the distance of the original space) between objects oi and oj, and ˆdij
is the Euclidean distance between their respective images pi and pj. The stress

i,j ( ˆdij−dij )
((cid:2)
ij)

i,j d2

2)

130

C. Bustos et al.

function gives the relative error that the distances in the target space suﬀer
on average after the transformation. Fastmap begins with an estimation that is
iteratively improved, until no additional improvement is possible.
In the metric case, we can assume that we have the n × n matrix of distances 
between all the dataset objects, and Fastmap must ﬁnd n points in the
k-dimensional space whose Euclidean distances are close to the original matrix
of n × n distances. The crux is to assume that objects are points in some mdimensional 
space, with unknown m, and to project these points over k mutually
orthogonal directions. The challenge is to compute all these projections using
only the distance matrix. Fastmap projects the objects over carefully selected
lines. It chooses two objects oa and ob, and considers the “line” passing through
them in the original space. The projections of the objects over this line are
obtained using the cosine law:

Theorem 1 (Cosine Law). Any triangle

(cid:6)

oaoiob satisﬁes:

d(ob, oi)2 = d(oa, oi)2 + d(oa, ob)2 − 2x(cid:7)

id(oa, ob).

(1)

Eq. 1 can be solved for x(cid:7)

formula x(cid:7)

i = d(oa,oi)

2

i to compute the projection of object oi with the
2−d(ob,oi)

2

+d(oa,ob)
2d(oa,ob)

.

Thus, the input of Fastmap is a set S of size n and, in each iteration, it
computes the coordinates of all the n objects over the new axis. So, after k
iterations, it produces a k-dimensional target space S(cid:7) where each object oi ∈ S
is mapped to a k-coordinate vector pi = (x(cid:7)
, x(cid:7)
i,j is
i,2
the j-th projection of the image pi of the object oi.

i,k) ∈ S(cid:7), where x(cid:7)

, . . . , x(cid:7)

i,1

In our case, we want to estimate the number of projections needed so that the
target space reaches a mapping with a small enough stress, that is, preserving
the distances suﬃciently well. Thus, we modify the Fastmap algorithm so that
it computes the stress of the target space after each new dimension is added. If
the diﬀerence between the current and the previous stress values is signiﬁcative,
we compute another projection (thus increasing the dimensionality of the target
space). Otherwise, the current dimension of the target space is reported as the
estimation of the ID of the original metric space.

3.4

Intrinsic Search Diﬃculty

Ch´avez et al. [6] introduced a measure of the intrinsic complexity of searching
in general metric spaces. It is easy to estimate and independent of the search
algorithm.

Several authors [1,4,8] have proposed to use the distance histogram to characterize 
the hardness of searching in arbitrary metric spaces, yet the cost was
tailored to a speciﬁc index. Instead, this measure [6] depends only on the histogram 
and not on any assumption on the indexing method.

The intuition behind this measure is that, in random vectors in D dimensions,
the histogram has a larger mean μ and a smaller variance σ2 as D increases. In
fact, it holds D = c · μ2/σ2 for some constant c [23]. Thus, the same formula

An Empirical Evaluation of Intrinsic Dimension Estimators

131

could be used to estimate a dimension D from the mean and variance of the
histogram of distances in a general metric space. We do not have the histogram
of the whole universe U, but we can approximate it using the histogram of the
dataset S ⊂ U.
Deﬁnition: Let μ be the mean and σ2 be the variance of the histogram of
distances of a metric space. Then, its intrinsic search diﬃculty is ρ = μ2
2σ2 .

An obvious advantage of this measure, which has contributed to its popularity,
 is that it is easy to compute from a reasonable sampling of pairs in S. Other
techniques require more complex and expensive computations.

Pestov [19] presents a system of three axioms an intrinsic dimension function
must satisfy. He proves that the intrinsic dimension measure ρ satisﬁes a weak
version of these axioms. Later [20], he introduces a set of goals an intrinsic
dimension function should fulﬁll, and ρ achieves many of them.
As the measure ρ has been designed for general metric spaces, we use it as is.
We consider the dataset S and we compute all the distances d(x, y),∀ x, y ∈ S.
d(x, y) and the variance σ2 =
Then we compute the average μ = 1
n2

(cid:2)
x,y∈S

(d(x, y) − μ)2. Finally, we obtain the value of ρ = μ2

2σ2 and report it

(cid:2)
x,y∈S

1
n2
as the ID of the metric space.

4 Experimental Results

We evaluate experimentally the four ID estimators described, on general metric
spaces. We consider two kinds of metric spaces, depending on the data source:

Synthetic: these are spaces generated artiﬁcially so that they present some
interesting characteristic to be evaluated. For instance, uniformly distributed
vectors in R

D with known dimension.

Real world: these are metric spaces obtained from real-world applications. For
instance, a feature vectors space of images obtained from a NASA image set.

4.1 Synthetic Metric Spaces

These are vector spaces with Euclidean distance. They are treated as metric
spaces, as we do not consider the coordinate information. A ﬁrst set is formed
by vectors with uniform distribution, so that the representational dimension
matches the ID. Here we can test the estimators in a case where the ID is
known. A second set is formed by vectors with Gaussian distribution, so that
the representational dimension is greater than the ID (the more clustered is the
space, the lower is the ID). The distance is also Euclidean. Here we aim to check
whether the estimators give lower values as the ID decreases.

132

C. Bustos et al.

Uniformly Distributed Vectors with Euclidean Distance. We generate
four datasets of 100,000 vectors uniformly distributed in the unitary cube [0, 1)D,
with D = 5, 10, 15 and 20. The spaces are called C5, C10, C15 and C20,
respectively.

Fig. 1(a) depicts the estimations for these four metric spaces. As it can be
seen, the Fractal estimator (Ball counting) is insensitive to the correct dimension.
The Distance Exponent increases with D, but not proportionally. The other two
estimations grow at the same rate of D, with Fastmap matching it almost perfectly 
and Intrinsic Search Diﬃculty showing a consistent factor multiplying D.

Search degradation as ID grows. To verify that the dataset ID is responsible
of the search degradation, we pick C5 and extend its vectors with zeroes to
produce spaces with 10, 15 and 20 representational dimensions, and study the
search performance over it.

We perform 10 executions of the algorithms, building the index with 90%
of the database elements, and reserving the remaining 10% (chosen at random)
for the queries. So, the query objects do not belong to the index. We average
the results over the 10 executions. In each execution, the objects in the metric
space are permuted at random. Therefore, each of the 10 indices uses a diﬀerent
dataset S, and the query objects are also diﬀerent.

We use a pivot index and a compact partition index. For the pivot index
family, we use the generic pivot algorithm. We choose at random a set of pivots
P = {P1, P2, . . . , Pk} ⊂ S of size |P| = k. We store the kn distances between
pivots and objects, and use them to ﬁlter out candidates using the triangle
inequality. For each space, we experimentally determine the number of pivots
that obtains the best search performance. Thus, the results shown for each case
correspond to the best possible ones for this method, in the corresponding metric
space.

For the case of compact partition based algorithms, we consider the LC,
which is one of the best indexes for medium and high dimensions [5]. We use
the LC variant that has a maximum size for each cluster. For each metric space
considered, we experimentally determine the cluster size whose perfomance is
the best, and this is the result shown in the plots.

In Fig. 2, we show the cost of range queries retrieving 0.01%, 0.1% and 1%
of the vector dataset per query, using the generic pivot index (Fig. 2(a)) and
the LC (Fig. 2(b)). These results are compared with the ones for searching C10,
C15 and C20. Both plots show that the four spaces of ID 5 overlay each other
(independently of the representational dimension of the space), while the curves
for C10, C15 and C20 show the usual degradation.

Gaussian Distributed Vectors with Euclidean Distance. We generate
D with mean μ = 1 and variance σ2 = 0.1, for D = 5, 10, 15
100,000 vectors in R
and 20. In these spaces, there are no, a priori, clusters of elements. These spaces
are called G5, G10, G15 and G20.

An Empirical Evaluation of Intrinsic Dimension Estimators

133

Estimations of dimensionality for uniform spaces

Estimations of dimensionality for gaussian spaces

Fractal
Exponent
Fastmap
Intrinsic

 25

y
t
i
l

 20

Fractal
Exponent
Fastmap
Intrinsic

i

i

a
n
o
s
n
e
m
d
 
f
o
 
n
o
i
t
a
m

i
t
s
E

C5

C10

C15

C20

Metric space

(a) Uniform spaces.

 15

 10

 5

 0

G101

G5

G10

G15

G20

Metric space

(b) Gaussian spaces.

 30

 25

 20

 15

 10

 5

 0

Fig. 1. Comparison of dimensionality estimations for synthetic metric spaces.

Search cost per element with Pivots, n = 100,000 uniform vectors

Search cost per element with LC, n = 100,000 uniform vectors

Dim. 5, Rep. 5
Dim. 5, Rep. 10
Dim. 5, Rep. 15
Dim. 5, Rep. 20
Dim. 10, Rep. 10
Dim. 15, Rep. 15
Dim. 20, Rep. 20

 90000

 80000

 70000

 60000

 50000

 40000

 30000

 20000

 10000

Dim. 5, Rep. 5
Dim. 5, Rep. 10
Dim. 5, Rep. 15
Dim. 5, Rep. 20
Dim. 10, Rep. 10
Dim. 15, Rep. 15
Dim. 20, Rep. 20

 90000

 80000

 70000

 60000

 50000

 40000

 30000

 20000

 10000

s
n
o

i
t

l

a
u
a
v
e
e
c
n
a

 

t
s
D

i

y
t
i
l

i

i

a
n
o
s
n
e
m
d
 
f
o
 
n
o
i
t
a
m

i
t
s
E

s
n
o

i
t

l

a
u
a
v
e
e
c
n
a

 

t
s
D

i

 0
 0.01

 0.1

Percentage retrieved

 1

 0
 0.01

 0.1

Percentage retrieved

 1

(a) Generic pivot index.

(b) List of clusters.

Fig. 2. The searching eﬀort does not vary when the ID of the space does not change.

We also generate 100,000 vectors in R101 with mean μ = 1 and variance
σ2 = 0.1 with 200 clusters (the cluster centers are uniformly distributed in the
space). This space is called G101.

Fig. 1(b) shows the estimations obtained with Fractal, Distance Exponent,
Fastmap, and Intrinsic Search Diﬃculty, for these metric spaces. Again, the
Fractal estimation fails in these spaces, being insensitive to the dimension, and
the Distance Exponent grows very slowly. The other two measures grow proportionally 
to D as they should, although Fractal is less sensitive to the fact that
the distribution is not uniform. Instead, the Intrinsic Search Diﬃculty gives
markedly lower values than in the uniform case.

4.2 Real Metric Spaces

We pick four spaces from the Metric Library [11] 1 in order to estimate their IDs
with the four ID estimators. The selected spaces are varied:

Dictionary: it is a dictionary of 69,069 English words. In this space, we use a

discrete function, the Edit Distance or Levenshtein Distance [16].

1 Available at http://www.sisap.org/library/dbs/.

134

C. Bustos et al.

NASA: this is a set of 40,700 images from NASA, represented as feature vectors
of 20 real coordinates per vector, under the Euclidean distance. They were
generated from images downloaded from the NASA site.

Histograms: this is a dataset of 112,682 histograms of medical images, each one
composed by 8-D color histograms of 112 real components. As any quadratic
form function can be used as the distance in this case, we also have chosen
the Euclidean distance, as it is the simplest alternative.

Documents: this space has 1,265 documents, represented as vectors according 
to the vectorial model of documents used in the Information Retrieval
ﬁeld. To compare documents we use the cosine distance. Each vector has
a coordinate for each vocabulary term in the colection, and documents can
be seen as points in a vector space of high representational dimension. The
documents are ﬁles obtained form the trec-3 collection.

To measure the intrinsic hardness of the searching, we consider the same two

indices as before, using range queries:

Dictionary: As the metric is discrete, we use radii 1, 2, 3, and 4, retrieving on

average about 0.003%, 0.037%, 0.326%, and 1.757% of the database.

NASA: In this continuous metric we use radii 0.012, 0.285, and 0.53, retrieving

on average approximately 0.01%, 0.1%, and 1% of the dataset.

Histograms: This metric is also continuous. To retrieve on average approximately 
0.01%, 0.1%, and 1% of the dataset, we use query radii 0.051768,
0.082514, and 0.131163.

Documents: The distance is also continuous. We use query radii 0.14, 0.15,
and 0.195, which retrieve on average 0.01%, 0.1%, and 1% of the database.

Figs. 3 and 4 show the correlation between the search cost with the Pivot
index and the List of Clusters, respectively, and the estimation reported for each
considered ID estimator. For lack of space, we only show the results of the search
that retrieve 0.01% and 0.1% of the database.

We plot the ratio between the logarithm of the search cost and the estimations
of the ID. This measures how close is the logarithm of the predicted ID to the

Evaluations of estimators with Pivots for real metric spaces, 0.01% retr.

Evaluations of estimators with Pivots for real metric spaces, 0.1% retr.

s
n
o

i
t

a
m

i
t
s
E

/
)
y
t
l

u
c
i
f
f
i

 

D
h
c
r
a
e
S
(
g
o
L

 5.5

 5

 4.5

 4

 3.5

 3

 2.5

 2

 1.5

 1

 0.5

 0

Fractal
Exponent
Fastmap
Intrinsic

ENG

NASA

HIS

DOCS

Metric space

(a) Retrieving 0.01%.

s
n
o

i
t

a
m

i
t
s
E

/
)
y
t
l

u
c
i
f
f
i

 

D
h
c
r
a
e
S
(
g
o
L

 7

 6

 5

 4

 3

 2

 1

 0

Fractal
Exponent
Fastmap
Intrinsic

ENG

NASA

HIS

DOCS

Metric space

(b) Retrieving 0.1%.

Fig. 3. Comparison of ID estimators for real metric spaces, using Pivots.

An Empirical Evaluation of Intrinsic Dimension Estimators

135

Evaluations of estimators with LC for real metric spaces, 0.01% retr.

Evaluations of estimators with LC for real metric spaces, 0.1% retr.

s
n
o

i
t

a
m

i
t
s
E

/
)
y
t
l

u
c
i
f
f
i

 

D
h
c
r
a
e
S
(
g
o
L

 9

 8

 7

 6

 5

 4

 3

 2

 1

 0

Fractal
Exponent
Fastmap
Intrinsic

ENG

NASA

HIS

DOCS

Metric space

(a) Retrieving 0.01%.

s
n
o

i
t

a
m

i
t
s
E

/
)
y
t
l

u
c
i
f
f
i

 

D
h
c
r
a
e
S
(
g
o
L

 10

 9

 8

 7

 6

 5

 4

 3

 2

 1

 0

Fractal
Exponent
Fastmap
Intrinsic

ENG

NASA

HIS

DOCS

Metric space

(b) Retrieving 0.1%.

Fig. 4. Comparison of ID estimators for real metric spaces, using List of Clusters.

actual search costs: if the search cost is consistently s = cd, where d is the
predicted ID and c is a constant, then the plots should always be close to log c.
Thus the best methods are those that give roughly the same value regardless of
the query radius and index used.

As on the synthetic spaces, Fastmap and the Intrinsic Search Diﬃculty turn
out to be the best predictors for both types of indices. The Distance Exponent
performs generally well, except for the NASA dataset.

5 Conclusions

The intrinsic dimension (ID) of metric spaces measures their search diﬃculty,
independently of the type of index used. Computing the ID is useful to determine
whether a metric space can be indexed at all (or we must resort to sequential
scanning or approximate methods), which kind of index would perform better,
and what search performance to expect.

In this paper we have analyzed several ID estimators in a practical perspective.
 Some were deﬁned for D-dimensional coordinate spaces, and we have
adapted them to the more general metric spaces. We compared their predictions
with the actual search cost using various synthetic and real-life metric spaces,
so as to verify which are better at predicting the search diﬃculty.

Although our results are preliminary, they suggest that the best performing 
measures in practice are Fastmap [10] and the simple measure proposed by
Ch´avez et al. [6]. Instead, the Distance Exponent [22] and our generalization of
Box Counting [17] did not perform so well.

The reason for the failure of Box Counting may be that it needs an
extremely large number of objects to correctly estimate D. An estimation [2]
N, which in our case implies that the method could have worked
is D < 2 log10
well up to D = 10 only. However, in our experiments the adapted method failed
even in this case. It may be that our adaptation to computing it using the List
of Clusters [5] is too crude (as other clustering methods may cover the dataset
with fewer balls). In any case, this shows that the method is not easy to apply,

136

C. Bustos et al.

but we plan to further study this issue with more points and other clustering
methods. The reason for the failure of the distance exponent, which does not
present issues to be adapted, is also unclear and deserves further research.

We also plan to analyze other estimators. For instance, we can study the correlation 
dimension [3], the concentration dimension [19], or the classical Principal
Component Analysis (PCA) method [15] (which is deﬁned on vector spaces).

References

1. Brin, S.: Near neighbor search in large metric spaces. In: Proc. 21st Conf. on Very

Large Databases (VLDB 1995), pp. 574–584 (1995)

2. Camastra, F.: Data dimensionality estimation methods: a survey. Pattern

Recognition 36(12), 2945–2954 (2003)

3. Camastra, F., Vinciarelli, A.: Estimating the intrinsic dimension of data with a

fractal-based method. IEEE TPAMI 24(10), 1404–1407 (2002)

4. Ch´avez, E., Marroqu´ın, J.: Proximity queries in metric spaces. In: Proc. 4th
South American Workshop on String Processing (WSP 1997), pp. 21–36. Carleton
University Press (1997)

5. Ch´avez, E., Navarro, G.: A compact space decomposition for eﬀective metric indexing.
 Pattern Recognition Letters 26(9), 1363–1376 (2005)

6. Ch´avez, E., Navarro, G., Baeza-Yates, R., Marroqu´ın, J.: Searching in metric

spaces. ACM Computing Surveys 33(3), 273–321 (2001)

7. Ciaccia, P., Patella, M., Zezula, P.: M-tree: an eﬃcient access method for similarity

search in metric spaces. In: Proc. 23rd VLDB, pp. 426–435 (1997)

8. Ciaccia, P., Patella, M., Zezula, P.: A cost model for similarity queries in metric

spaces. In: PODS, pp. 59–68 (1998)

9. Eckmann, J.P., Ruelle, D.: Ergodic theory of chaos and strange attractors. Rev.

Mod. Phys. 57, 617 (1985)

10. Faloutsos, C., Lin, K.-I.: Fastmap: a fast algorithm for indexing, data-mining and
visualization of traditional and multimedia datasets. In: Proc. 1995 ACM SIGMOD
Intl. Conf. on Management of Data, pp. 163–174. ACM Press (1995)

11. Figueroa, K., Navarro, G., Ch´avez, E.: Metric spaces library (2007). http://www.

sisap.org/Metric Space Library.html

12. Fukunaga, K.: Introduction to Statistical Pattern Recognition, 2nd edn. Academic

Press Professional Inc, San Diego (1990)

13. Jagadish, H.V.: A retrieval technique for similar shapes. In: SIGMOD Conference,

pp. 208–217. ACM Press (1991)

14. Jain, A.K., Dubes, R.C.: Algorithms for Clustering Data. Prentice-Hall Inc, Upper

Saddle River (1988)

15. Jolliﬀe, I.T.: Principal Component Analysis, 2nd edn. Springer Series in Statistics.

Springer (2002)

16. Levenshtein, V.I.: Binary codes capable of correcting deletions, insertions, and

reversals. Soviet Physics Doklady 10(8), 707–710 (1966)

17. Mandelbrot, B.: Fractals: Form, Chance and Dimension. W. H. Freeman, San

Francisco (1977)

18. Ott, E.: Chaos in Dynamical Systems. Cambridge University Press, Cambridge

(1993)

19. Pestov, V.: Intrinsic dimension of a dataset: what properties does one expect? In:

Intl. Joint Conf. on Neural Networks (IJCNN), pp. 2959–2964 (2007)

An Empirical Evaluation of Intrinsic Dimension Estimators

137

20. Pestov, V.: An axiomatic approach to intrinsic dimension of a dataset. Neural
Networks 21(23), 204–213 (2008). Advances in Neural Networks Research: 2007
International Joint Conference on Neural Networks (IJCNN)

21. Samet, H.: Foundations of Multidimensional and Metric Data Structures (The
Morgan Kaufmann Series in Computer Graphics and Geometric Modeling). Morgan 
Kaufmann Publishers Inc., San Francisco (2005)

22. Traina Jr., C., Traina, A.J.M., Faloutsos, C.: Distance exponent: a new concept for
selectivity estimation in metric trees. Research Paper 99–110, School of Computer
Science, Carnegie Mellon University, 03/1999 (1999)

23. Yianilos, P.: Excluded middle vantage point forests for nearest neighbor search. In:

DIMACS Implementation Challenge, ALENEX 1999, Baltimore, MD (1999)

24. Zezula, P., Amato, G., Dohnal, V., Batko, M.: Similarity Search: The Metric Space

Approach. Advances in Database Systems, vol. 32. Springer (2006)

