Speeding Up Pattern Matching by Text Sampling

Francisco Claude1,(cid:2), Gonzalo Navarro1,∗, Hannu Peltola2,

Leena Salmela2, and Jorma Tarhio2

1 Department of Computer Science, University of Chile

{fclaude,gnavarro}@dcc.uchile.cl

2 Department of Computer Science and Engineering

Helsinki University of Technology

{hpeltola,lsalmela,tarhio}@cs.hut.fi

Abstract. We introduce a novel alphabet sampling technique for speeding 
up both online and indexed string matching. We choose a subset of
the alphabet and select the corresponding subsequence of the text. Online 
or indexed searching is then carried out on that subsequence, and
candidate matches are veriﬁed in the full text. We show that this speeds
up online searching, especially for moderate to long patterns, by a factor
of up to 5. For indexed searching we achieve indexes that are as fast
as the classical suﬃx array, yet occupy space less than 0.5 times the
text size (instead of 4) plus text. Our experiments show no competitive
alternatives in a wide space/time range.

1 Introduction

The string matching problem is to ﬁnd all the occurrences of a given pattern
P = p0p1 . . . pm−1 in a large text T = t0t1 . . . tn−1, both being sequences of
characters drawn from an alphabet Σ of size σ.

One approach to string matching is online searching, which means the text is
not preprocessed. Thus these algorithms need to scan the text when searching
and their time cost is of the form O(n · f(m)). The worst-case complexity of the
problem is Θ(n), ﬁrst achieved by the Knuth-Morris-Pratt algorithm [9]. The
average complexity of the problem is Θ(n logσ m/m), achieved for example by
the BDM algorithm [3]. Other non-optimal algorithms such as the Boyer-MooreHorspool 
(BMH) algorithm [7] are very competitive in practice.
The second approach, indexed searching, tries to speed up searching by preprocessing 
the text and building a data structure that allows searching in O(m·
g(n) + occ · h(n)) time, where occ is the number of occurrences of the pattern
in the text. Popular solutions to this approach are suﬃx trees and suﬃx arrays
[10]. The ﬁrst gives an O(m + occ) time solution, while the suﬃx array gives an
O(m log n + occ) time complexity which can be improved to O(m + occ) using
extra space [1]. The problem of these approaches is that the space needed is too
large for many practical situations (4–20 times the text size). Recently, a lot

(cid:2) Partially funded by Millennium Nucleus Center for Web Research, Grant P04-067-F,

Mideplan, Chile.

A. Amir, A. Turpin, and A. Moﬀat (Eds.): SPIRE 2008, LNCS 5280, pp. 87–98, 2008.
c(cid:3) Springer-Verlag Berlin Heidelberg 2008

88

F. Claude et al.

of eﬀort has been spent to compress these indexes [13] obtaining a signiﬁcant
reduction in space, but requiring considerable implementation eﬀort [5].

In this work we explore sampling the text by removing a set of characters from
the alphabet. We ﬁrst apply an online algorithm to this sampled text, obtaining
an approach in between online searching and indexed searching. We call this kind
of structure a semi-index. This is a data structure built on top of a text, which
permits searching faster than any online algorithm, yet its search complexity
is still of the form O(n · f(m)). To be interesting, a semi-index should be easy
to implement and require little extra space. Several other semi-indexes exist in
the literature, even without using that name. For example, q-gram indexes [12],
directly searchable compression formats [11], and other sampling approaches.

We also consider indexing the sampled text. We build a suﬃx array indexing
the sampled positions of the text, and get a sampled suﬃx array. This approach
is similar to the sparse suﬃx array [8] as both index a subset of the suﬃxes, but
the diﬀerent sampling properties induce rather diﬀerent search algorithms.

A challenge in our method is how to choose the best alphabet subset to
sample. We present analytical results, supported by experiments, that simplify
this process by drastically reducing the number of combinations to try. We show
that it is suﬃcient in practice to sample the least frequent characters up to some
limit. In both cases, online and indexed, our sampling technique signiﬁcantly
improves upon the state of the art, especially for relatively long search patterns.
For example, online searching is speeded up by a factor of up to 5 on English
text. For indexed searching we achieve indexes that are as fast as the classical
suﬃx array, yet occupy less than 0.5 times the text size (instead of 4) plus text.

2 Text Sampling

The main idea of our online approach is to choose a subset of the alphabet
to be the sampled alphabet and then to build a subsequence of the text by
omitting all characters not in the sampled alphabet. At regular intervals we
map the positions of the sampled text to their corresponding positions in the
original text. When searching, we build the sampled pattern from the pattern
by omitting all characters not in the sampled alphabet and then search for this
sampled pattern in the sampled text. For each candidate returned by this search
we verify a short range of the original text with the help of the position mapping.
Let T = t0t1 . . . tn−1 be the text over the alphabet Σ and ˜Σ ⊂ Σ the sampled

alphabet. The proposed semi-index is composed of the following items:
– Sampled text ˜T : Let ˜T = ti0 ti1 . . . ti˜n−1 be the sequence of the ti’s that
– The position mapping M: A table of size (cid:3)˜n/q(cid:4) where M[i] maps the q · i’th

belong to the sampled alphabet ˜Σ. The length of the sampled text is ˜n.
character of ˜T to its corresponding position in T so ˜T [q · i] = T [M[i]].

Given a pattern P = p0p1 . . . pm−1, search on this semi-index is carried out as
follows. Let ˜P = pj0 pj1 . . . pj ˜m−1 be the subsequence of pi’s that belong to the
sampled alphabet ˜Σ. The length of the sampled pattern is thus ˜m. The sampled

Speeding Up Pattern Matching by Text Sampling

89

a

b

a

a c a b d

a a

Text

a

ac

b

Pattern

b

1

Omitting a’s

Omitting a’s

c

b d

Sampled Text

c

b

Sampled Pattern

6

Mapping

Fig. 1. Example of preprocessing

search ( ˜T = ˜t0˜t1 . . . ˜t˜n−1, ˜P = ˜p0 ˜p1 . . . ˜p ˜m−1, T = t0t1 . . . tn−1,

P = p0p1 . . . pm−1, j0, q, M[0 . . . ˜n/q])

for (i ← 0 to σ − 1) d[i] ← ˜m
for (i ← 0 to ˜m − 2) d[˜pi] ← ˜m − 1 − i
pos ← 0

j ← ˜m − 1
while (j ≥ 0 and ˜tpos+j = ˜pj) j ← j − 1
if (j = −1)

1.
2.
3.
4. while (pos < ˜n − ˜m)
5.
6.
7.
8.
9.
10.

Check for occurrence from M[pos/q] + (pos mod q) − j0
to M[pos/q + 1] − (q − pos mod q) − j0

pos ← pos + d[˜tpos+ ˜m−1]

Fig. 2. Searching the sampled text for a sampled pattern with the BMH algorithm

text ˜T is then searched for ˜P , and for every occurrence, the positions to check in
the original text are delimited by the position mapping M. If the sampled pattern
is found in position ir in ˜T , the area T [M[ir/q] + (ir mod q) − j0 . . . M[ir/q +
1] − (q − ir mod q) − j0] is checked for possible startings of real occurrences.
For example, if the text is T = abaacabdaa, the sampled text built omitting
the a’s ( ˜Σ = {b, c, d}) is ˜T = t1t4t6t7 = bcbd. If we map every other position in
the sampled text, the position mapping M is {1, 6}. For searching the pattern
acab we omit the a’s and get ˜P = p1p3 = cb. We search for ˜P = cb in ˜T = bcbd,
ﬁnding an occurrence at position 1. The previous mapped position is M[0] = 1, so
˜t0 corresponds to t1, and the next mapped position is M[1] = 6, so ˜t2 corresponds
to t6. Because the ﬁrst sampled character in P is in position 1, we verify the
area 1 . . . 4 in the original text ﬁnding the match at position 3. Preprocessing for
the text and pattern of the previous example is shown in Fig. 1.

Because the sampled patterns tend to be quite short, we implemented the
search phase with the BMH algorithm [7], which has been found to be fast in
such settings [14]. Figure 2 shows the algorithm for this basic method.

Although the above scheme works well for most of the patterns, it is obvious 
that there are some bad patterns which would be searched faster in the
original text. The average complexity of the BMH algorithm is O(n(1/m +
1/σ)) = O(n/ min(m, σ)) assuming a uniform and independent distribution of

90

F. Claude et al.

the characters of the alphabet [2]. If the distribution is not uniform, a better approximation 
is to replace σ by the the eﬀective alphabet size ¯σ, which is deﬁned
as the inverse of the probability of two random characters matching, i.e. 1/¯σ =
(cid:2)
c∈Σ p2
c, where pc is the empirical probability of occurrence of the character c.
To determine if it would be faster to just search the pattern in the original
text we tried calculating the ratios n/ min(m, ¯σ) and n · (1/m + 1/¯σ) both for
the sampled text and pattern and for the original text and pattern. If the ratio
is lower for the original text and pattern, we search only in the original text.
The results were better using the ratio n/ min(m, ¯σ).

3 Optimal Sampling for Online Search

A question arises from the previous description of our sampling method: How
to form the sampled alphabet ˜Σ? We will ﬁrst analyze how the average running
time of the BMH algorithm changes when we sample the text and then, based on
this, we will develop a method to ﬁnd the optimal sampled alphabet. Throughout
this section we assume that the characters are independent and we analyze the
approach for a general pattern not known when preprocessing the text.
c where A ⊂ Σ. Now the length
of the sampled text will be b ˜Σn, the average length of the sampled pattern b ˜Σm
(assuming it distributes similarly to the text) and the probability of two random
characters matching in the sampled text a ˜Σ/b2
. Given the average complexity
of the BMH algorithm, O(n(1/m+1/¯σ)), the average search cost in the sampled
˜Σ
text is

Let us deﬁne bA =

c∈A pc and aA =

c∈A p2

(cid:4)(cid:4)

(cid:2)

(cid:2)

(cid:3)

(cid:3)

O

b ˜Σn

1

b ˜Σm

+ a ˜Σ
b2
˜Σ

(cid:5)

(cid:5)

= O

n

(cid:6)(cid:6)

.

1
m

+ a ˜Σ
b ˜Σ

When considering the veriﬁcation cost we assume for simplicity that the mapping 
M contains the position of each sampled character in the original text, i.e.
q = 1. The probability that a position has to be veriﬁed is then
+ 1 − b ˜Σ

˜Σ(1 − b ˜Σ)m−i
bi

pver =

(cid:6)m

m(cid:7)

(cid:4)i

=

(cid:3)

(cid:5)

(cid:5)

(cid:6)

.

a ˜Σ
b2
˜Σ

a ˜Σ
b ˜Σ

m
i

i=0

If we assume that each veriﬁcation costs O(m) then the cost of veriﬁcation is

n · pver · O(m) = n ·

+ 1 − b ˜Σ
The total cost of searching in our scheme is thus

a ˜Σ
b ˜Σ

(cid:5)

(cid:5)

(cid:6)m · O(m) .

(cid:5)

(cid:5)
n ·

O

1
m

+ a ˜Σ
b ˜Σ

+

+ 1 − b ˜Σ

a ˜Σ
b ˜Σ

(cid:6)(cid:6)

(cid:6)m · m

and hence the optimal sampled alphabet ˜Σ minimizes the cost per text character

(cid:5)

E( ˜Σ) =

1
m

+ a ˜Σ
b ˜Σ

+

+ 1 − b ˜Σ

a ˜Σ
b ˜Σ

(cid:6)m · m

Speeding Up Pattern Matching by Text Sampling

91

which can be divided into the search cost in the sampled text 1/m + a ˜Σ/b ˜Σ and
the veriﬁcation cost (a ˜Σ/b ˜Σ + 1 − b ˜Σ)m · m.
The veriﬁcation cost always increases when a character is removed from the
alphabet so the search cost in the sampled text must decrease for the combined
cost to decrease. If R = Σ\ ˜Σ is the set of removed characters, the function

hR(p) =

+ aΣ − aR − p2
1 − bR − p

1
m

gives the search cost in the sampled text, per text character, if an additional
character with probability p is removed. The derivative of hR(p) is

(cid:5)

R(p) = 1 − (1 − bR)2 − (aΣ − aR)

(1 − bR − p)2

h

which has exactly one zero pz = (1−bR)−(cid:8)
(1 − bR)2 − (aΣ − aR) in the interval
[0, 1−bR]. We can see that the function hR(p) is increasing until pz and decreasing
after that. Solving the equation hR(pR) = hR(0) we get pR = (aΣ−aR)/(1−bR).
So removing a single additional character decreases the search cost in the sampled
text only if the probability of occurrence for that character is larger than pR.
Otherwise both the search cost in the sampled text and the veriﬁcation cost will
increase and thus removing the character is not beneﬁcial.

Suppose now that we have already ﬁxed whether we are going to keep or
remove each character with probability of occurrence higher than pc and now we
need to decide if we should remove the character c. If pc > pR, we will need to
explore both options as removing the character will decrease search cost in the
sampled text and increase veriﬁcation cost. However, if pc < pR we know that if
we added only c to R the searching time in the sampled text would also increase
and therefore we should not remove c. But could it be beneﬁcial to remove c
together with a set of other characters with probabilities of occurrence less than
pR? In fact it cannot be. Suppose that we remove a character c with probability
(cid:5) = R∪{c} so we get aR(cid:2) = aR + p2
pc < pR. Now the new removed set will be R
c
and bR(cid:2) = bR + pc. Now the new critical probability will be

pR(cid:2) = aΣ − aR(cid:2)

1 − bR(cid:2) = aΣ − aR − p2
1 − bR − pc

c

.

We know that hR(pc) > hR(pR) = hR(0) because pc < pR. Therefore

and so

c

1
m

+ aΣ − aR − p2
1 − bR − pc
pR(cid:2) = aΣ − aR − p2
1 − bR − pc

c

+ aΣ − aR
1 − bR

1
m

>

aΣ − aR
1 − bR

>

= pR .

Thus even now it is not good to remove a character with probability less than
the critical value pR for the previous set and this will again hold if another character 
with a small probability is removed. Therefore we do not need to consider

92

F. Claude et al.

Ropt = {}
sort characters of Σ in descending order
ﬁnd_opt(0, {})
return Ropt

else

if (i = σ)

ﬁnd_opt(i, R)
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.

else

if (E(Σ\R) < E(Σ\Ropt))

Ropt = R
pR = aΣ−aR
1−bR
if (pi > pR)
ﬁnd_opt(i + 1, R ∪ {i})
ﬁnd_opt(i + 1, R)

ﬁnd_opt(σ, R)

Fig. 3. Pseudo code for searching for the optimal set of removed characters

removing characters with probabilities less than pR. Note however that removing
a character with a higher probability will decrease the critical probability pR and
after this it can be beneﬁcial to remove a previously unbeneﬁcial character. In
fact, if the sampled alphabet contains two characters with diﬀerent probabilities
of occurrence, the probability of occurrence for the most frequent character in
the sampled alphabet is always larger than pR. Thus it is always beneﬁcial for
searching in the sampled text to remove the most frequent character.

The above can be applied to prune the exhaustive search for the optimal
set of removed characters. First we sort the characters of the alphabet in the
decreasing order of frequency. We then ﬁgure out if it is beneﬁcial for searching
in the sampled text to remove the most frequent character not considered yet.
If it is, we try both removing and not removing that character and proceed
recursively for both cases. If it is not, we prune the search here because none of
the remaining characters should be removed. Figure 3 gives the pseudo code.

In practice when using this pruning technique the number of examined sets
drops drastically as compared to the exhaustive search, although the worst case
is still exponential. For example, the number of examined sets drops from 261 to
2,810 when considering the King James Bible as the text.

Table 1. Predicted and observed optimal number of removed characters for the King
James Bible. The predicted optima are computed with the algorithm suggested by the
analysis, which in our experiments always returned a set of most frequent characters.

10 20 30 40 50 60 70 80 90 100
Predicted optimal number of removed characters 3 7 9 11 12 13 14 15 16 16
Observed optimal number of removed characters 3 7 11 13 14 15 17 17 16 18

m

Speeding Up Pattern Matching by Text Sampling

93

In our experiments, the optimal set of removed characters always contained
the most frequent characters up to some limit depending on the length of the
pattern, as shown in Table 1. Therefore a simpler heuristic is to remove the k
most frequent characters for varying k and choose the set that predicts the best
overall time. However, if the veriﬁcation cost is very high for some reason (e.g.
going to disk to retrieve the text, or uncompressing part of it) it is possible that
the optimal set of removed characters is not a set of most frequent characters.

4 Sampled Suﬃx Array

To turn the sampling approach into an index, we use a suﬃx array to index the
sampled positions of the text. When constructing the suﬃx array, only suﬃxes
starting with a sampled character will be considered, but the sorting will still be
done considering the full suﬃxes. The resulting sampled suﬃx array is like the
suﬃx array of the original text where suﬃxes starting with unsampled characters
have been omitted. The construction of the sampled suﬃx array can be done in
O(n) time using O(˜n) words of space if we apply the construction technique of
the word suﬃx array [4]. The sampled suﬃx array for the text T = abaacabdaa
is shown in Fig. 4, where the sampled alphabet is ˜Σ = {b, c, d}.

Search on the sampled suﬃx array is carried out as follows. Given a pattern
P = p0p1 . . . pm−1 we ﬁrst ﬁnd the ﬁrst sampled character of the pattern. Let this
be at index j. The pattern is now divided into the unsampled preﬁx p0 . . . pj−1
and the suﬃx starting with the ﬁrst sampled character pj . . . pm−1. We search
the sampled suﬃx array for this suﬃx of the pattern like in an ordinary suﬃx
array. Each candidate match returned by this search will then be veriﬁed by
comparing the unsampled preﬁx against the text.

We could also construct the suﬃx array directly for the sampled text but this
would entail more veriﬁcations as the unsampled characters of the pattern suﬃx
would not be required to match. We would also need to store the sampled text,
or to skip the unsampled characters in the original text each time we read a
suﬃx.

The sampled suﬃx array resembles a sparse suﬃx array [8], which indexes
regularly sampled text positions. However, we only need to make one search on

T =  a b a a c a b d a a
0 1 2 3 4 5 6 7 8 9

Sampled SA

1 baacabdaa
6
4
7

bdaa
cabdaa
daa

Fig. 4. The sampled suﬃx array for the text T = abaacabdaa with the sampled alphabet 
˜Σ = {b, c, d}. The sorted suﬃxes are only shown for convenience. They are not
part of the structure.

94

F. Claude et al.

the sampled suﬃx array, while using a sparse suﬃx array one would need to
make q searches if the sparse suﬃx array indexes every q’th position. On the
other hand, the sampled suﬃx array can only be used for patterns that contain
at least one sampled character whereas the sparse suﬃx array can be used if
the pattern length is at least q. The variance of the search time when using the
sampled suﬃx array is also larger than when using a sparse suﬃx array because
in the sampled suﬃx array we have much less control over the length of the
string that is used in the suﬃx array search.

5 Optimal Sampling for Suﬃx Array
Suppose that we have enough space to create the sampled suﬃx array for b · n
suﬃxes where 0 < b < 1. How should we now choose the sampled alphabet
˜Σ so that the search time would be optimal? Obviously b ˜Σ = b but we still
have a number of possible sampled alphabets to choose from. The search on the
suﬃx array will compare the suﬃx of the pattern starting with the ﬁrst sampled
character against a text string O(log n) times. The comparison time is minimized
when the probability of matching for the ﬁrst sampled character is minimized.
Thus the sampled alphabet ˜Σ should be a set of least frequent characters.
Let us then consider the veriﬁcation. The probability that two random characters 
are unsampled and match is aR = aΣ − a ˜Σ where R is the set of removed
characters. Thus the average cost of a single veriﬁcation is 1/(1 − aΣ + a ˜Σ).
The probability that the suﬃx of the pattern starting with the ﬁrst sampled
character matches a random string of equal length is

b ˜Σ

a ˜Σ
b2
˜Σ

(aΣ)ms−1 = a ˜Σ
b ˜Σ

(aΣ)ms−1

where ms is the length of the suﬃx starting with the ﬁrst sampled character.
This is also the probability of veriﬁcation per character in the original text. The
average cost of veriﬁcation per text character is then

(aΣ)ms−1 ·

a ˜Σ
b ˜Σ

1

1 − aΣ + a ˜Σ

=

a ˜Σ

1 − aΣ + a ˜Σ

· (aΣ)ms−1

b ˜Σ

.

Because we attempt to determine the optimal sampled alphabet such that b ˜Σ =
b, b ˜Σ and the distribution of ms do not depend on which characters we remove.
Thus we should minimize f(a ˜Σ) = a ˜Σ/(1− aΣ + a ˜Σ). The derivative of f(a ˜Σ) is

(cid:5)

f

(a ˜Σ) =

1 − aΣ

(1 − aΣ + a ˜Σ)2 > 0

so the veriﬁcation cost increases when a ˜Σ increases. To minimize a ˜Σ the sampled
alphabet ˜Σ should be a set of least frequent characters. This also minimizes the
total cost because also the suﬃx array search cost is minimized by this choice.
Interestingly, this corresponds to the simpliﬁed heuristic we proposed in Sect. 3.

Speeding Up Pattern Matching by Text Sampling

95

6 Experiments

6.1 Semi-index

To determine the sampled alphabet, we ran the exact algorithm of Sect. 3 for different 
pattern lengths to choose the sampled alphabet that produces the smallest
estimated cost E( ˜Σ). For all pattern lengths the algorithm recommended removing 
a set of most frequent characters. To see how well these results correspond
to practice, we tested the semi-index approach by removing the k most frequent
characters from the text for varying k. We used a 2 MB preﬁx of the King James
Bible as the text, and the patterns are random substrings of the text. For each
pattern length 500 patterns were generated, and the reported running times are
averages over 200 runs with each of the patterns. The most frequent characters
in the decreasing order of frequency were “␣ethaonsirdlfum,wycgbp” where ␣ is

Mean

Distribution

m=10
m=20
m=30
m=50
m=70
m=100

)
s
m

(
 
e
m

i
t
 

n
u
R

 3

 2.5

 2

 1.5

 1

 0.5

 0

 0

)
s
m

(
 
e
m

i
t
 

n
u
R

 5

 4

 3

 2

 1

 0

 20

m=20
m=50
m=100

 0

 5

 10

 15

 20

Number of different characters removed

 5
Number of different characters removed

 10

 15

Fig. 5. The running time for various pattern lengths for the basic method. The left
ﬁgure shows the mean running time; the right shows the median, minimum, maximum,
and 25% and 75% quartiles.

Mean

Distribution

m=10
m=20
m=30
m=50
m=70
m=100

)
s
m

(
 
e
m

i
t
 

n
u
R

 3

 2.5

 2

 1.5

 1

 0.5

 0

 0

)
s
m

(
 
e
m

i
t
 

n
u
R

 5

 4

 3

 2

 1

 0

 20

m=20
m=50
m=100

 0

 5

 10

 15

 20

Number of different characters removed

 5
Number of different characters removed

 10

 15

Fig. 6. The running time for various pattern lengths for the tuned version where searching 
in the sampled text is skipped if it looks like searching in the original text is faster.
The left ﬁgure shows the mean running time; the right ﬁgure shows the median, minimum,
 maximum, and 25% and 75% quartiles.

96

F. Claude et al.

the space character. The tests were run on a 1.0 GHz AMD Athlon dual core
processor with 2 GB of memory, 64 kB L1 cache and 512 kB L2 cache, running
Linux 2.6.23. The code is in C and compiled with gcc using -O3 optimization.
Figure 5 shows the results of these experiments with the basic method mapping 
every 64’th sampled character to its position in the original text. If we make
the mapping sparser the running time will start to increase a little earlier, but
the eﬀect is quite mild. The results for zero removed characters correspond to the
original BMH algorithm. As we can see, the semi-index is up to 5 times faster,
especially when the patterns are long. Figure 5 also shows that, for each pattern
length, there is an optimal number of characters to remove. A comparison of
these optima and those given by the analysis is shown in Table 1. As we can see,
the analysis gives reasonably good results although it recommends removing too
few characters with long patterns, because we estimated the veriﬁcation time
quite pessimistically. When more characters are removed it is unlikely that we
would need to scan m characters for each veriﬁed position.

The results for the tuned method, where we search the original text if the
ratio n/ min(m, ¯σ) looks unfavorable for searching the sampled text, is shown
in Fig. 6. Again we are mapping every 64’th sampled character to its position
in the original text. As we can see, the optimal number of removed characters
is closer to being the same for all pattern lengths than in the basic approach.
For example by choosing to remove the 13 most frequent characters, we would
do reasonably well for all pattern lengths using just 0.18 times the original text
size to store the sampled text. Comparing Figs. 5 and 6 we see that the median
running times are almost the same, but the maximum and the 75% quartile are
lower for the tuned method. This is also reﬂected in the average values.

6.2 Sampled Suﬃx Array

Figure 7 shows the results obtained by comparing our sampled suﬃx array
against our implementation of the sparse suﬃx array [8] and the locally compressed 
suﬃx array (LCSA) [6], an index that compresses the diﬀerential suﬃx
array using Re-Pair. Note that when the space usage of the sampled or sparse
suﬃx array is maximal (3.25 times the text) both of them index all suﬃxes and
behave exactly like a normal suﬃx array. The experiments were run on a Pentium
IV 2.0 GHz processor with 2 GB of RAM running SuSE Linux with kernel 2.4.31.
The code was compiled using gcc version 3.3.6 with -O9 optimization. We used
50 MB texts from the PizzaChili site, http://pizzachili.dcc.uchile.cl.

Our approach performs very well for moderate to long patterns. Already for
m = 50 it starts to dominate the other alternatives. For m = 100 the sampled
suﬃx array behaves almost like a suﬃx array (and much faster than the other
methods), even when using less than 0.5 times the text size (plus text). The
novel compressed self-indexes [5,13] are designed to use much less space (e.g.
0.8 times the text size including the text) but take much more time, and thus
are inappropriate for this comparison. We chose the LCSA as an alternative that
compresses less but is much faster than the other self-indexes [6]. Its compression
performance varies widely with the text type, and is not particularly good on

Speeding Up Pattern Matching by Text Sampling

97

)
y
r
e
u
q

 
r
e
p

 
s
c
e
s

i
l
l
i

m

(
 

e
m

i
t

)
y
r
e
u
q

 
r
e
p
 
s
c
e
s

i
l
l
i

m

(
 
e
m

i
t

 3

 2.5

 2

 1.5

 1

 0.5

 0

 1

 0.1

 0.08

 0.06

 0.04

 0.02

 0

 1

m=20

Sparse SA XML
Sampled SA XML

 1.5

 2

 2.5

 3

 3.5

 4

 4.5

space (fraction of the text)

m=50

Sparse SA XML
Sparse SA English
Sparse SA Proteins
Sampled SA XML
Sampled SA English
Sampled SA Proteins
LCSA English
LCSA Proteins

 1.5

 2

 2.5

 3

 3.5

 4

 4.5

space (fraction of the text)

)
y
r
e
u
q

 
r
e
p

 
s
c
e
s

i
l
l
i

m

(
 

e
m

i
t

)
y
r
e
u
q

 
r
e
p
 
s
c
e
s

i
l
l
i

m

(
 
e
m

i
t

 0.1

 0.08

 0.06

 0.04

 0.02

 0

 1

 0.1

 0.08

 0.06

 0.04

 0.02

 0

 1

m=20

Sparse SA English
Sparse SA Proteins
Sampled SA English
Sampled SA Proteins
LCSA English
LCSA Proteins

 1.5

 2

 2.5

 3

 3.5

 4

 4.5

space (fraction of the text)

m=100

Sparse SA XML
Sparse SA English
Sparse SA Proteins
Sampled SA XML
Sampled SA English
Sampled SA Proteins
LCSA English
LCSA Proteins

 1.5

 2

 2.5

 3

 3.5

 4

 4.5

space (fraction of the text)

Fig. 7. Search times for the sampled and sparse suﬃx arrays and LCSA for XML,
English and protein data. LCSA uses little space for XML data but it is much slower
than the other approaches, so these results are not shown. The top ﬁgures show results
for pattern length 20 and the bottom ﬁgures show the results for pattern lengths 50
and 100. The space fraction includes that of the text, so it is of the form 1+ index size
text size .

English and Proteins. On XML it requires extra space equal to the size of the
text, yet its times are much higher and fall well outside the plot (and this is still
much faster than the other self-indexes!). The LCSA, on the other hand, would
perform better on shorter patterns, where our index is not competitive.

7 Conclusions and Further Work

We have presented two sampling approaches to speed up string matching with
long patterns. The sampled semi-index proﬁts from nonuniform character distribution 
to gain a speedup over online searching, while the sampled suﬃx array
works also with a uniform distribution. It is also worth noting that in the semiindex 
approach the sampled text is an internal structure of the semi-index so
any transform, like compression or code splitting [15], could be applied to it.

The current approach is not applicable to small alphabets. To extend the
approach to smaller alphabets we could use q-grams. In the semi-index approach
we would then deﬁne a sampled alphabet for each (q − 1)-long context and the
sampled text would contain those characters that are sampled in the context
where they occur. When searching for a pattern, we must always discard the

F. Claude et al.

98
ﬁrst q− 1 characters of the pattern as their context is not known. Using q-grams
with the sampled suﬃx array is simpler. The sampled suﬃx array would just
index all suﬃxes starting with a sampled q-gram.

Another interesting direction to minimize the extra space of the semi-index is
to replace the original text by the subsequence of the non-sampled characters,
and use a bitmap to indicate the subset each symbol of T belongs to. With
rank/select capabilities [13] this bitmap replaces the current position mapping for
veriﬁcation and permits searching on the sampled or the unsampled characters.

References

1. Abouelhoda, M., Kurtz, S., Ohlebusch, E.: Replacing suﬃx trees with enchanced

suﬃx arrays. Journal of Discrete Algorithms 2(1), 53–86 (2004)

2. Baeza-Yates, R.: String searching algorithms revisited. In: Dehne, F., Sack, J.R.,
Santoro, N. (eds.) WADS 1989. LNCS, vol. 382, pp. 75–96. Springer, Heidelberg
(1989)

3. Crochemore, M., Czumaj, A., Gąsieniec, L., Jarominek, S., Lecroq, T., Plandowski,
W., Rytter, W.: Speeding up two string-matching algorithms. Algorithmica 12,
247–267 (1994)

4. Ferragina, P., Fischer, J.: Suﬃx arrays on words. In: Ma, B., Zhang, K. (eds.) CPM

2007. LNCS, vol. 4580, pp. 328–339. Springer, Heidelberg (2007)

5. Ferragina, P., González, R., Navarro, G., Venturini, R.: Compressed text indexes:

From theory to practice (manuscript 2007), http://pizzachili.dcc.uchile.cl

6. González, R., Navarro, G.: Compressed text indexes with fast locate. In: Ma, B.,
Zhang, K. (eds.) CPM 2007. LNCS, vol. 4580, pp. 216–227. Springer, Heidelberg
(2007)

7. Horspool, R.N.: Practical fast searching in strings. Software – Practise & Experience 
10, 501–506 (1980)

8. Kärkkäinen, J., Ukkonen, E.: Sparse suﬃx trees. In: Cai, J., Wong, C.K. (eds.)

COCOON 1996. LNCS, vol. 1090, pp. 219–230. Springer, Heidelberg (1996)

9. Knuth, D.E., Morris, J.H., Pratt, V.R.: Fast pattern matching in strings. SIAM

Journal on Computing 6, 323–350 (1977)

10. Manber, U., Myers, G.: Suﬃx arrays: A new method for online string searches.

SIAM Journal on Computing 22(5), 935–948 (1993)

11. Moura, E., Navarro, G., Ziviani, N., Baeza-Yates, R.: Fast and ﬂexible word searching 
on compressed text. ACM Trans. on Information Systems 18(2), 113–139 (2000)
12. Navarro, G., Baeza-Yates, R., Sutinen, E., Tarhio, J.: Indexing methods for approximate 
string matching. IEEE Data Engineering Bulletin 24(4), 19–27 (2001)
13. Navarro, G., Mäkinen, V.: Compressed full-text indexes. ACM Computing Surveys 
39(1), 1–61 (2007)

14. Navarro, G., Raﬃnot, M.: Flexible Pattern Matching in Strings – Practical on-line
search algorithms for texts and biological sequences. Cambridge University Press,
Cambridge (2002)

15. Rautio, J., Tanninen, J., Tarhio, J.: String matching with stopper encoding and
code splitting. In: Apostolico, A., Takeda, M. (eds.) CPM 2002. LNCS, vol. 2373,
pp. 45–52. Springer, Heidelberg (2002)

