SIAM J. COMPUT.
Vol. 43, No. 5, pp. 1781–1806

c(cid:2) 2014 Society for Industrial and Applied Mathematics

∗
OPTIMAL DYNAMIC SEQUENCE REPRESENTATIONS

GONZALO NAVARRO

†

AND YAKOV NEKRICH

‡

Abstract. We describe a data structure that supports access, rank, and select queries, as well
as symbol insertions and deletions, on a string S[1, n] over alphabet [1..σ] in time O(log n/ log log n),
which is optimal even on binary sequences and in the amortized sense. Our time is worst case for
the queries and amortized for the updates. This complexity is better than the best previous ones by
a Θ(1 + log σ/ log log n) factor. We also design a variant where times are worst case, yet rank and
updates take O(log n) time. Our structure uses nH0(S) + o(n log σ) + O(σ log n) bits, where H0(S)
is the zero-order entropy of S. Finally, we pursue various extensions and applications of the result.

Key words. succinct data structures, strings, rank and select

AMS subject classiﬁcations. 68P05, 68P10, 68P20, 68P30

DOI. 10.1137/130908245

1. Introduction. String representations supporting rank and select queries are
fundamental in many data structures, including full-text indexes [26, 20, 23], permutations 
[23, 2], inverted indexes [11, 2], graphs [18], document retrieval indexes [55],
labeled trees [23, 5], XML indexes [28, 19], binary relations [5], and many more. The
problem is to encode a string S[1, n] over an alphabet Σ = [1..σ] so as to support the
following queries:

ranka(S, i) = number of occurrences of a ∈ Σ in S[1, i], for 1 ≤ i ≤ n;
selecta(S, i) = position in S of the ith occurrence of a ∈ Σ for 1 ≤ i ≤ ranka(S, n);
access(S, i) = S[i].

There exist various static representations of S (i.e., S cannot change) that support
these operations [26, 23, 20, 2, 7]. The most recent work [7] shows a lower bound of
Ω(lg lg σ
lg w ) time (where lg means log2 when it matters, and unless another base is
speciﬁed) for operation rank on a RAM machine with w-bit words, using any space
of the form O(n lgO(1) n). It also provides a matching upper bound that in addition
achieves almost constant time for select and access, using compressed space. Thus the
problem for static representations is essentially closed.

However, various applications need dynamism, that is, the ability to update S

via insertions and deletions of symbols. Formally,

inserts a ∈ Σ between S[i − 1] and S[i] for 1 ≤ i ≤ n;

inserta(S, i) :
delete(S, i) : deletes S[i] from S for 1 ≤ i ≤ n.

A lower bound for this case, in order to just support operations rank, insert, and
delete, even for bit vectors (σ = 2) and in the amortized sense, is Ω(lg n/ lg lg n) [22].

∗

Received by the editors February 1, 2013; accepted for publication (in revised form) August 19,
2014; published electronically October 14, 2014. Partially funded by Millennium Nucleus Information
and Coordination in Networks ICM/FIC P10-024F, Chile. An early partial version of this paper
appeared in Proceedings of the 23rd Annual ACM-SIAM Symposium on Discrete Algorithms, ACM,
New York, 2013, pp. 865–876.

†
‡

http://www.siam.org/journals/sicomp/43-5/90824.html
Department of Computer Science, University of Chile, Santiago, Chile (gnavarro@dcc.uchile.cl).
David R. Cheriton School of Computer Science, University of Waterloo, Lawrence, KS 66045

(yakov.nekrich@googlemail.com).

1781

                                                                     Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php1782

GONZALO NAVARRO AND YAKOV NEKRICH

On the other hand the best known upper bound [29, 49] is O((1 + lg σ/lg lg n)
lg n/lg lg n), that is, a factor Θ(lg σ/lg lg n) away from the lower bound for alphabets
larger than polylogarithmic. Their space is nH0(S) + o(n lg σ) bits, where H0(S) =
(cid:2)

a∈Σ(na/n) lg(n/na) ≤ lg σ is the zero-order entropy of S, na being the number of

occurrences of a in S.

In this paper we close this gap by providing an optimal-time dynamic representation 
of sequences. Our representation takes O(lg n/ lg lg n) time for all the operations,
worst case for the three queries, and amortized for updates. We present a second variant 
achieving worst-case bounds for all the operations, O(lg n/ lg lg n) for select and
access, and O(lg n) for rank, insert, and delete. The space is also nH0(S) + o(n lg σ)
bits. Time O(lg n) is still faster than previous work for lg σ = Ω((lg lg n)2). This gets
much closer to closing this problem under the dynamic scenario as well.

∗

We then show how to handle general alphabets, such as Σ = R, or Σ = Γ

for a
symbol alphabet Γ, in optimal time. For example, in the comparison model for Σ = R,
the time is O(lg σ+lg n/ lg lg n), where σ is the number of distinct symbols that appear
for general Γ, the time is O(|a| + lg γ + lg n/ lg lg n), where |a|
in S; in the case Σ = Γ
is the length of the involved symbol (a string) and γ the number of distinct symbols
of Γ that appear in the elements of S. Previous dynamic solutions have assumed
∗
that the alphabet [1..σ] was static. An exception for the case Σ = Γ
[27] obtains
O(|a| lg γ lg n) time for all the operations.

∗

At the end, we describe several applications where our result oﬀers improved
time/space trade-oﬀs. These include compressed indexes for dynamic text collections,
 construction of the Burrows–Wheeler transform [12], and construction of static
compressed text indexes within a compressed space, among others.

We start with an overview of the state of the art, putting our solution in context,
in section 2. We review the wavelet tree data structure [26], which is fundamental in
our solution (and in most previous ones) in section 3. In section 4 we describe the core
of our amortized solution, deferring to section 5 the management of deletions and its
relation with a split-ﬁnd data structure needed for rank and insert. Section 6 encapsulates 
a technical part related to the structure of blocks that handle subsequences
of polylogarithmic size. Section 7 deals with the changes in lg n and how we obtain
times independent of σ, and concludes with Theorem 7.1, our result on uncompressed
sequences. Then section 8 shows how to improve the data encoding to obtain compressed 
space in Theorem 8.1, and section 9 shows how to obtain worst-case times,
Theorem 9.1. Finally, section 10 describes some extensions and applications of our
results. We conclude in section 11.

2. Related work. With one exception [28], all the previous work on dynamic
sequences built on the wavelet tree structure [26]. The wavelet tree decomposes S
hierarchically. In a ﬁrst level, it separates larger from smaller symbols, marking in a
bit vector which symbols of S are larger and which are smaller. The two subsequences
of S are recursively separated. The lg σ levels of bit vectors describe S, and access,
rank, and select operations on S are carried out via lg σrank and select operations on
the bit vectors (see section 3 for more details).

In the static case, rank and select operations on bit vectors take constant time,
and therefore access, rank, and select on S takes O(lg σ) time [26]. This can be reduced
to O(1 + lg σ/ lg lg n) by using multiary wavelet trees [20]. These separate the symbols
into ρ = Θ(lgε n) ranges for any constant 0 < ε < 1 and; instead of bit vectors, store
sequences over an alphabet of size ρ. On an alphabet of that size, rank and select can
still be solved in constant time, and thus the time on the wavelet tree operations is
reduced to O((cid:4)lg σ/ lg ρ(cid:5)).

                                                                     Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpOPTIMAL DYNAMIC SEQUENCE REPRESENTATIONS

1783

Insertions and deletions in S can also be carried out by inserting and deleting
bits from lg σ bit vectors. However, the operations on dynamic bit vectors are bound
to be slower. Fredman and Saks [22] show that Ω(lg n/ lg lg n) time is necessary, even
in the amortized sense, to support rank, insert, and delete operations on a bit vector.
By using dynamic bit vector solutions [30, 14, 8, 13, 32] on the wavelet tree levels,
one immediately obtains a dynamic sequence representation, where the space and the
time of the dynamic bit vector solution is multiplied by lg σ (the sum of the zero-order
entropies of the bit vectors adds up to nH0(S) [26]). With this combination one can
obtain times as good as O(lg σ lg n/ lg lg n) (using n lg σ + o(n lg σ) bits) [13], or spaces
as good as O(nH0(S)) bits (with O(lg σ lg n) time) [8].1

M¨akinen and Navarro [38, 39] made the above combination explicit, and obtained
O(lg σ lg n) time for all the sequence operations coupled with the best compressed
space until then, nH0(S) + o(n lg σ) bits. They also obtained O((1 + lg σ/ lg lg n) lg n)
query time, but with an update time of O(lg σ lg1+ε n) for any constant 0 < ε < 1.
This was achieved by replacing binary with multiary wavelet trees, and obtaining
O(lg n) query time for the operations on sequences over a small alphabet of size
o(lg n).

Lee and Park [36, 37] pursued this path further, obtaining O((1+lg σ/ lg lg n) lg n)
time for queries and update operations, yet the space was not compressed, n lg σ +
o(n lg σ) bits, and update times were amortized. Shortly after, Gonz´alez and Navarro
[24, 25] obtained the best of both worlds, making all the times worst case and compressing 
the space again to nH0(S) + o(n lg σ) bits. Both solutions managed to solve
all query and update operations in O(lg n) time on sequences over small alphabets of
size o(lg n).

Finally, almost simultaneously, He and Munro [29] and Navarro and Sadakane [49]
obtained the currently best result, O((1 + lg σ/ lg lg n) lg n/ lg lg n) time, still within
the same compressed space. They did so by improving the times of the dynamic
sequences on small alphabets to O(lg n/ lg lg n), which as said is optimal even on bit
vectors and in the amortized sense.

As mentioned, the solution by Gupta et al. [28] deviates from this path and is
a general framework for using any static data structure and periodically rebuilding
it. By using it over a given representation [23], it achieves O(lg lg n) query time
and O(nε) amortized update time.
It would probably achieve compressed space if
combined with more recent static data structures [2]. This shows that query times
can be signiﬁcantly smaller if one allows for much higher update times. In this paper,
however, we focus on achieving similar times for all the operations. Table 1 gives
more details on previous results and our new results.
Wavelet trees can also be used to model n×n grids of points, in which case σ = n.
Bose et al. [10] used a wavelet-tree-like structure to solve range counting in optimal
static time O(lg n/ lg lg n), using operations slightly more complex than rank on the
wavelet tree levels. It is conceivable that this can be turned into an O((lg n/ lg lg n)2)
time algorithm using dynamic sequences on the wavelet tree levels. On the other hand,
Ω((lg n/ lg lg n)2) is a lower bound for dynamic range counting in two dimensions [52].
This suggests that it is unlikely to obtain better results for dynamic wavelet trees,
and thus that the current path of progress on dynamic wavelet trees has reached its
optimum in O((1 + lg σ/ lg lg n) lg n/ lg lg n) time.

1For simplicity, we are omitting an O(σ lg n) additive term present in the space complexities,

which is o(n lg σ) as long as σ = o(n). We do write it explicitly in the theorems.

                                                                     Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php1784

GONZALO NAVARRO AND YAKOV NEKRICH

O
u
r
s

O
u
r
s

[
4
9
]

[
2
9
]

[
2
4
,

[
3
6
,

[
2
8
]

[
1
3
]

2
5
]

3
7
]

[
3
8
,

3
9
]

[
8
]

[
1
4
]

[
3
0
,

[
2
2
]

3
2
]

S
o
u
r
c
e

n
H
0
(
S
)

+
O
(
n
H
0
(
S
)
/
l
g
l
g
n
)

+
O
(
n

l
g
σ
/
l
g

n
H
0
(
S
)

+
O
(
n

l
g
σ
/
l
g

1
−
ε
n
)

n

l
g
σ
+
O
(
n

l
g
σ
/

n

l
g
σ
+
O
(
n

O
(
n

l
g
σ
)

n
H
0
(
S
)

+
O
(
n

l
g
σ
/
(
ε
l
g

n
H
0
(
S
)

n
H
0
(
S
)

+
O
(
n

+
O
(
n

l
g
σ
/

1
l
g
−
n
ε
)
n
)
)

l
g
√
σ
/
l
g
l
g
σ
)

l
g
σ
/
√
l
g
n
)

l
g
n
√
)
+
O
(
n
)

n
H
0
(
S
)

+
O
(
n

l
g
σ
/
l
g

n
H
0
(
S
)

+
O
(
n

l
g
σ
/

1
/
2
−
ε
n
)

l
g
n
)

O
(
n
H
0
(
S
)

+

O
(
n

l
g
σ
)

l
g
n
√
)

n

l
g
σ
+
O
(
n

l
g
σ
(
l
g
l
g
n
)

2
/
l
g
n
)

1
−
ε
n
)

O
(

ε
2

1

O
(

ε
2

1

l
g
n
/
l
g
l
g
n
)

l
g
n
/
l
g
l
g
n
)
,

O
(
1ε

l
g
n
)

f
o
r

r
a
n
k

O
(

ε
2

1

O
(
l
g
n
)

l
g
n
/
l
g
l
g
n
)

O
(
(
1
+
1ε

l
g
σ
/
l
g
l
g
n
)
l
g
n
/
l
g
l
g
n
)

O
(
(
1
+
1ε

l
g
σ
/
l
g
l
g
n
)
l
g
n
/
l
g
l
g
n
)

O
(
(
1
+

l
g
σ
/
l
g
l
g
n
)
l
g
n
/
l
g
l
g
n
)

O
(
(
1
+

l
g
σ
/
l
g
l
g
n
)
l
g
n
/
l
g
l
g
n
)

O
(
(
1
+

O
(
(
1
+

l
g
σ
/
l
g
l
g
n
)
l
g
n
)

l
g
σ
/
l
g
l
g
n
)
l
g
n
)

O
(
(
1
+

O
(
(
1
+

l
g
σ
/
l
g
l
g
n
)
l
g
n
)

l
g
σ
/
l
g
l
g
n
)
l
g
n
)

O
(
1ε

l
g
l
g
n
+

l
g
l
g
σ
)

O
(
l
g
σ

l
g
n
/
l
g
l
g
n
)

O
(
(
1
+
1ε

l
g
σ
/
l
g
l
g
n
)
l
g
n
)

O
(
l
g
σ

O
(
l
g
σ

O
(
l
g
σ

l
g
n
)

l
g
n
)

l
g
n
)

O
(
l
g
σ

l
g
n
/
l
g
l
g
n
)

O
(
1ε
n
ε

)

O
(
l
g
σ

l
g
n
/
l
g
l
g
n
)

O
(
1ε

l
g
σ

l
g

1
+
ε
n
)

O
(
l
g
σ

O
(
l
g
σ

O
(
l
g
σ

l
g
n
)

l
g
n
)

l
g
n
)

O
(
l
g
σ
(
l
g
n
/
l
g
l
g
n
)

2

)

W

A

W

W

W

A

A

W

W

W

W

W

A

A

(

W
)
o
r
s
t
-
c
a
s
e

o
r

(
A
)
m
o
r
t
i
z
e
d
.

o
n
l
y

f
o
r

b
i
n
a
r
y

s
e
q
u
e
n
c
e
s

a
n
d

t
h
e

r
e
s
u
l
t

w
e

g
i
v
e

i
s

o
b
t
a
i
n
e
d

b
y

u
s
i
n
g

t
h
e
m

i
n

c
o
m
b
i
n
a
t
i
o
n

w

i
t
h

w
a
v
e
l
e
t

t
r
e
e
s
.

l

C
o
u
m
n
W
A

t
e
l
l
s

w
h
e
t
h
e
r

t
h
e

u
p
d
a
t
e

t
i

m
e
s

a
r
e

H
i
s
t
o
r
y

o
f

r
e
s
u
l
t
s

o
n
m
a
n
a
g
i
n
g

d
y
n
a
m
i
c

s
e
q
u
e
n
c
e
s

S

[
1
,

n

]

o
v
e
r

l

a
p
h
a
b
e
t

[
1
.
.

σ

]
,

a
s
s
u
m
i
n
g

σ
=
o
(
n
/
l
g
n
)

t
o

s
i

m
p
l
i
f
y
.

S
o
m
e

r
e
s
u
l
t
s

[
3
0
,

8
,

1
4
]

w
e
r
e

p
r
e
s
e
n
t
e
d

T
a
b
l
e

1

S
p
a
c
e

(
b

i
t
s
)

Q
u
e
r
y

t
i

m
e

U
p
d
a
t
e

t
i

m
e

W
A

Ω
(
l
g
n
/
l
g
l
g
n
)

f
o
r

r
a
n
k
+

i

n
s
e
r
t

+
d
e
l
e
t
e

                                                                     Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpOPTIMAL DYNAMIC SEQUENCE REPRESENTATIONS

1785

In this paper we show that this dead end can be broken by abandoning the implicit
assumption that, to provide access, rank, and select on S, we must provide rank and
select on the bit vectors (or sequences over [1..ρ]). We show that all that is needed is
to track positions of S downwards and upwards along the wavelet tree. It turns out
that this tracking can be done in constant time per level, breaking the Θ(lg n/ lg lg n)
per-level barrier.

As a result, we obtain the optimal time complexity O(lg n/ lg lg n) for all the
queries (worst case) and update operations (amortized), independently of the alphabet 
size. This is Θ(1 + lg σ/ lg lg n) times faster than what was believed to be the
“ultimate” solution. Our space is nH0(S) + o(n lg σ) bits, similar to previous solutions.
 We develop, alternatively, a data structure achieving worst-case time for all the
operations, yet this raises to O(lg n) for rank, insert, and delete.

Among the many applications of this result, it is worth mentioning that any
dynamic sequence representation supporting rank and insert in O(t(n)) amortized
time can be used to compute the Burrows–Wheeler transform (BWT) [12] of a sequence 
S[1, n] in worst-case time O(n t(n)). Thus our results allow us to build the
BWT in O(n lg n/ lg lg n) time and compressed space. The best existing space-time
trade-oﬀs are by Okanohara and Sadakane [51], who achieve optimal O(n) time
within O(n lg σ lg lgσ n) bits, Hon, Sadakane, and Sung [31], who achieve O(n lg lg σ)
√
time with O(n lg σ) bits, and K¨arkk¨ainen [34], who obtains O(n lg n + nv) time and
O(n lg n/
v) extra bits for a parameter v. Using less space allows us to improve
BWT-based compressors (like Bzip2) by allowing them to cut the sequence into larger
blocks, given a ﬁxed amount of main memory for the compressor. Several other results
will be mentioned in section 10.

∗

3. Basic notation and wavelet trees. Let S be a string over alphabet Σ =
[1..σ].
In this paper we will use a model for strings that, although it would be
more complex than necessary for typical purposes, is adequate for our descriptions.
A string S will be a set of distinct elements S = {s1, s2, . . . , sn}. Each element
s ∈ S will have two components, s.key ∈ N and s.chr ∈ Σ. We will write S[i] to
denote the element s ∈ S with the ith smallest s.key value, thus S[i].chr is what
is usually regarded as the ith character (or symbol) of S. We denote S[i].str =
S[1].chr ◦ S[2].chr ◦ ··· ◦ S[|S|].chr, what is usually regarded as the string itself (an
), where ◦ is the concatenation operator. Sometimes we refer to S as
element of Σ
a sequence of its elements s ∈ S, by taking them in increasing s.key order. Note
that a subset of S corresponds to what is usually called a subsequence. When clear
from context, we will write S instead of S.str. Note that we will not represent any
string S as a set of elements, but rather will use this conceptual model to describe
our algorithms and data structures. Thus a string S can be physically represented
using |S| lg σ bits by writing the symbols of S.str.
We use this model also for binary strings B over alphabet Σ = {0, 1}. To avoid
confusion, elements of binary strings will be called binary elements, or belements for
short. Thus, if b is a belement, b.chr ∈ {0, 1} is a bit. We will reserve the term “bit
vector” to describe physical sequences of bits. Thus B.str can be represented as a bit
vector, using |B| bits of space.
Now we describe the wavelet tree data structure [26]. We associate each a ∈ Σ
with a leaf va of a balanced binary tree T . Each node of T is associated with a subset
Σv ⊆ Σ, where Σv = {a ∈ Σ, va descends from v}. In particular, Σva = {a} for any
a ∈ Σ, and Σvr = Σ for the root vr of T . The essential idea of the wavelet tree is the
representation of a string S by binary strings stored in the nodes of T . We associate a

                                                                     Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php1786

GONZALO NAVARRO AND YAKOV NEKRICH

string S(v) ⊆ S with every node v of T , S(v) = {s ∈ S, s.chr ∈ Σv}. The wavelet tree
does not store strings S(v) explicitly, but just binary strings B(v) at internal nodes v.
We set B(v)[i].chr = t if S(v)[i] ∈ S(vt), where vt is the tth child of v (the left child
corresponds to t = 0 and the right to t = 1). This data structure (i.e., T and the bit
vectors B(v).str) is called the wavelet tree of S. Note that no bit vectors are stored
for the leaf nodes va, as the conceptual strings S(va) do not need to be represented.
Since T has O(σ) nodes and (cid:4)lg σ(cid:5) levels, and the bit vectors at each level add up to
length n, the wavelet tree requires n(cid:4)lg σ(cid:5) + O(σ lg n) bits of space. If the bit vectors
B(v).str are compressed to |B(v)|H0(B(v)) + o(|B(v)|) bits, the total size adds up to
nH0(S) + o(n lg σ) + O(σ lg n) bits [26] (we write H0(S) for H0(S.str)). There are
various surveys on wavelet trees [48, 40, 47].
For any element S[i] and every internal node v such that S[i].chr ∈ Σv, there is
exactly one element S(v)[j] = S[i]. Then the belement bv = B(v)[j] indicates in which
child of v is the leaf vS(v)[j].chr = vS[i].chr stored. We will say that such bv encodes
S[i] in B(v), and will write bv.enc = S[i] = S(v)[j]. We will also say that belement
bv ∈ B(v) corresponds to a belement bu ∈ B(u) if bv.enc = bu.enc in two nodes v and
u on a path of T . Identifying the belements that encode the same element plays a
crucial role in wavelet trees. Other, more complex, operations rely on the ability to
navigate in the tree and keep track of belements that encode the same element.

The wavelet tree encodes S, in the sense that it allows us to extract any S[i].chr.
To implement access(S, i) we traverse a path from the root vr to the leaf vS[i].chr.
In each visited node we read the belement bv that encodes S[i] and proceed to the
corresponding belement in the bv.chr-th child of v. Upon arriving at a leaf va we
answer access(S, i) = a.

The wavelet tree also implements operations rank and select. To compute
selecta(S, i), we start at the (conceptual) element S(va)[i] and identify the corresponding 
belement bv = B(v)[j] in the parent v of va, that is, bv.enc = B(v)[j].enc =
S(v)[j] = S(va)[i]. We continue this process towards the root until reaching a belement 
B(vr)[j] such that B(vr)[j].enc = S(vr)[j] = S[j] = S(va)[i]. Then the answer
is selecta(S, i) = j. Finally, to compute ranka(S, i), we traverse the wavelet tree from
element S(vr)[i] and bv = B(vr)[i] to some element in the leaf va. At each node v
in the path, we identify the child vt of v such that a ∈ Σvt , and then ﬁnd the bele-
.key ≤ bv.key. Then we move to the belement
in vt. Upon arriving at element S(va)[j], the answer

.chr = t and highest b

with b

ment b
bt = B(vt)[j] corresponding to b
is rank(S, i) = j.

(cid:5)

(cid:5)

(cid:5)

(cid:5)

The standard method used in wavelet trees for identifying corresponding belements 
is to maintain rank/select data structures on the bit vectors B(v).str. Let
B(v)[i].chr = t, then we can ﬁnd the oﬀset j of the corresponding belement B(vt)[j]
in the child vt of v as j = rankt(B(v).str, i). Conversely, we can ﬁnd the oﬀset j
of the belement B(v)[j] corresponding to B(vt)[i] as j = selectt(B(v).str, i). Finally,
needed for ranka(S, i) is easily solved
the more complicated process of ﬁnding the b
using rank on B(v).str: If bv = B(v)[i] and vt is the tth child of v, then without
we know that its corresponding belement in vt is B(vt)[j], for
the need to ﬁnd b
j = rankt(B(v).str, i). This approach leads to O(lg σ) query times in the static case
because rank/select queries on a bit vector B(v).str can be answered in constant time
and |B(v)|+o(|B(v)|) bits of space [46, 17], and even using |B(v)|H0(B(v))+o(|B(v)|)
bits [53]. However, we need Ω(lg n/ lg lg n) time to support rank/select and updates
on a bit vector [22], which multiplies the operation times in the dynamic case.

(cid:5)

(cid:5)

An improvement (for both static and dynamic wavelet trees) can be achieved by
increasing the fan-out of the wavelet tree to ρ = Θ(lgε n) for a constant 0 < ε < 1:

                                                                     Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpOPTIMAL DYNAMIC SEQUENCE REPRESENTATIONS

1787

the strings B(v) are not binary anymore but all the deﬁnitions and procedures remain
verbatim. This enables us to reduce the height of the wavelet trees and the query
time by a Θ(lg lg n) factor, because the rank/select times over alphabet [1..ρ] are
still constant in the static case [20] and O(lg n/ lg lg n) in the dynamic case [29, 49].
However, it seems that further improvements that are based on dynamic rank/select
queries in every node are not possible.

In this paper we use a diﬀerent approach to identifying the corresponding elements.
 We partition sequences B(v) into blocks, which are stored in compact list
structures L(v). Pointers from selected elements in L(v) to the structure L(vt) in
children nodes vt (and vice versa) enable us to navigate between nodes of the wavelet
tree in constant time. We extend the idea to multiary wavelet trees.

Our ideas are related to the general concept of fractional cascading [15, 16], and in
particular to its dynamic variant [43]. Similar techniques have also been used recently
in some geometric data structures [9, 50]. However, applying them on compressed data
structures where the bit budget is severely limited is much more challenging.

4. Basic structure. We start by describing the main components of our
modiﬁed wavelet tree. Then, we show how our structure supports access(S, i) and
selecta(S, i). In the third part of this section we describe additional structures that
enable us to answer ranka(S, i). Finally, we show how to support updates. During
this and the following sections we will obtain time O((lg σ + lg n)/ lg lg n) for all the
operations; in section 7 we will obtain times fully independent of σ.

B(v)[e], as no confusion will arise.

In the rest of the paper, we will use the term “elements” both for S(v)[e] and
4.1. Structure. We assume that the wavelet tree T has node degree ρ = Θ(lgε n).
We divide the sequences B(v) into g(v) blocks G1(v), G2(v), . . . , Gg(v)(v), and store
those blocks in a doubly linked list L(v). Each block Gj(v) contains |Gj(v)| =
Θ(lg3 n/ lg ρ) consecutive elements from B(v), except the last, which can be smaller.
For each Gj (v) we maintain a data structure Rj(v) that supports access, rank, and
select queries on Gj(v).str. Since a block contains a polylogarithmic number of elements 
over an alphabet of size ρ, we can answer those queries in O(1) time (this will
be described later, in section 6, because the details are rather technical).

The location of an element B(v)[e] consists of two parts: (1) a unique identiﬁer of
the block Gj(v) that contains the oﬀset e, and (2) the local index of e within Gj(v).
Such a pair gives constant-time access to the element. A pointer to an element will
indicate, precisely, its location.

We maintain pointers between selected corresponding elements in L(v) and the

lists of its parent and children.

(cid:5)

(cid:5)

].chr for all e

• If an element B(v)[e] is stored in a block Gj (v) and B(v)[e].chr = t (cid:9)=
B(v)[e
< e in Gj(v) (i.e., e is the ﬁrst oﬀset where symbol
t occurs in Gj(v).str), then we store a pointer from B(v)[e] to the corresponding 
element B(vt)[et] in L(vt). Pointers are bidirectional, that is, we
also store a pointer from B(vt)[et] to B(v)[e].
• In addition, if e is the ﬁrst oﬀset in its block Gj (v) (i.e., the location of
B(v)[e] is (Gj(v), 1)) and B(u)[e
v, then we store a pointer from B(v)[e] to B(u)[e
from B(u)[e

] corresponds to B(v)[e] in the parent u of
] and, by bidirectionality,

(cid:5)

] to B(v)[e].

(cid:5)

(cid:5)

All these pointers will be called internode pointers. We describe how they are implemented 
later in this section. Figure 1 shows an example (disregard for now the
dashed arrows).

                                                                     Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php1788

GONZALO NAVARRO AND YAKOV NEKRICH

v

20

1

G 2
1 2 2 2 1 0 0 0 1 21

v0

G 1

v

1

v

2

Fig. 1. An example of our data structure with ρ = 3. Circles represent wavelet tree nodes.
The parent node v has children v0, v1, and v2. We emphasize the relation between v and v2; the
rest are grayed. Each node shows (part of ) its list L, focusing on the relation between G2(v) and its
children. Each block Gj is a rectangle. Bidirectional pointers are shown with solid arrows, with ﬁlled
arrowheads marking their original direction. Thus one pointer leaves G2(v) from the ﬁrst occurrence
of each t towards its corresponding element in L(vt). Also, the ﬁrst element of each block in L(vt)
points upward. The dashed arrows illustrate the process of tracking downwards G2(v)[9], the third 1.
The ﬁrst step ﬁnds the closest previous 1 that has a downward pointer. It is G2(v)[1]. The second
step is to follow the pointer towards G1(v1). The third step is to advance by 2 in G1(v1), because
rank1(G2(v).str, 9) − rank1(G2(v).str, 1) = 2.

(cid:2)

It is easy to see that the number of internode pointers from L(v) to L(vt), for
any ﬁxed t, is O(g(v)). Hence, the total number of pointers that point downwards
from a node v is O(g(v)ρ). Additionally, there are O(g(v)) upward pointers to the
parent of v. Thus, the total number of internode pointers in the wavelet tree equals
v∈T g(v)ρ) = O(n lg σ/ lg3−ε n + σ lgε n), where the term σ lgε n accounts for the
O(
v nodes that have just one block, G1(v). Since the children vt of those nodes must also
have just one block, G1(vt), we avoid storing their pointers, as we know that all point
to the same block G1(vt), and their index inside G1(vt) can be found with constanttime 
rank/select operations inside G1(v). Similarly, if the parent u of v also has only
one block, G1(u), we avoid storing explicit pointers between G1(v) and G1(u), as they
can be computed in constant time with rank/select inside G1(u). If, instead, u has
more than one block, then we explicitly represent the pointers between L(v) and L(u)
and charge their space to u. This yields the cleaner expression O(n lg σ/ lg3−ε n) for
the number of pointers.

The pointers leaving from a block Gj(v) are stored in a data structure Fj(v).
Using Fj(v) we can ﬁnd, for any oﬀset e in Gj(v) and any 0 ≤ t < ρ, the last oﬀset
(cid:5) ≤ e in Gj(v) such that there is a pointer from B(v)[e
(cid:5)
e
t] in
L(vt). We describe in section 6 how Fj (v) implements the queries and updates in
constant time.

] to an element B(vt)[e

(cid:5)

A dynamic partial-sums data structure maintains m nonnegative integers
j
x1, . . . , xm, and supports two queries:
i=1 xi and search(v) =
max{j, sum(j) ≤ v}, as well as updates to elements xj by ±O(lg m), and inserting 
and deleting elements xj = 0. Furthermore, we can associate satellite data yj to
each xj , so that search(v) can return yj instead of simply j. The following lemma is
useful.

sum(j) =

(cid:2)

Lemma 4.1 ([49, Lem. 1], adapted). A dynamic partial-sums data structure over
m elements with satellite data supports operations sum, search, updates by O(lg m),
and insertions/deletions of zero values in O(lg m/ lg lg m) worst-case time per operation,
 using O(m lg m) bits of space.

                                                                     Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpOPTIMAL DYNAMIC SEQUENCE REPRESENTATIONS

1789

Structures inside any node v of the wavelet tree T , or only in the root node vr and the leaves va.

Table 2

The third column gives the extra space in bits, on top of the data, for the whole structure.

Structure Meaning
L(v)
Gj(v)
Rj(v)
Fj(v)
Hj(v)
Pt(v)
K(v)
Dj(v)
DEL

List of blocks storing B(v)
jth block of list L(v)
Supports rank/select/access inside Gj(v)
Pointers leaving from Gj (v)
Pointers arriving at Gj (v)
Predecessor in L(v) containing symbol t
Partial sums on block lengths for vr and va
Deleted elements in Gj (v), for vr and va
Global list of deleted elements in S

Extra space in bits

O(n lg σ(lg lg n)2/ lg n + σ lg n)

O(n lg σ/ lg2 n + σ lg n)
O(n lg σ lg lg n/ lg1−ε n)
O(n lg σ lg lg n/ lg1−ε n)

O(n lg σ/ lg2 n)
O(n lg σ/ lg2−ε n)
O(n lg lg n/ lg2 n)
O(n(lg lg n)2/ lg n)

O(n/ lg n + n lg σ/ lg2 n)

In addition to the pointers, we store, for the root node vr, a dynamic searchable
partial-sums data structure K(vr) on the blocks of L(vr): the values xj are the sizes
|Gj(vr)|, the satellite data yj is a reference to block Gj (vr), and lg m = Θ(lg n). Using
K(vr), query search(e) returns the block Gj(vr) that contains the element S(vr)[e],
and query sum(j − 1) returns the sizes of all the blocks that precede Gj(vr). The
same data structures K(va) are also stored in the leaves va of T . Since g(vr) =
O(n lg ρ/ lg3 n), and also
a∈Σ g(va) = O(n lg ρ/ lg3 n), we store O(n lg lg n/ lg3 n)
elements in the partial sums K(vr) and K(va), for an overall size of O(n lg lg n/ lg2 n)
bits.

(cid:2)

We recall that we do not store a sequence B(va) in a leaf node va, only in internal
nodes. Nevertheless, we divide the (implicit) sequence B(va) into blocks and store
their sizes in K(va); we maintain K(va) only if L(va) consists of more than one block.
Moreover we store internode pointers from the parent of va to va and vice versa.
Pointers in a leaf are maintained using the same rules of any other node.

For future reference, we provide the list of secondary data structures in Table 2.

They will be described in detail in the next sections.

(cid:5)
t] in L(vt). Due to our construction, such an e

(cid:5) ≤ e in Gj (v) such that there is a pointer from B(v)[e

4.2. Access and select queries. Assume the location (Gj(v), iv) of an element
B(v)[e] in L(v) is known, and B(v)[e].chr = t. Then, the location of the corresponding
element B(vt)[et] in L(vt) is computed as follows. Using Fj(v), we ﬁnd the local index
(cid:5)
i
v of the largest oﬀset e
] to some
B(vt)[e
must exist (it may be e itself),
(cid:5)
(cid:5)
since the ﬁrst such e
t) be the location
(cid:5)
of B(vt)[e
t] in L(vt). Due to our rules to deﬁne pointers, B(vt)[et] also belongs to
G(cid:4)(vt), since if it belonged to another block Gm(vt), the upward pointer from the ﬁrst
and e, and since pointers are bidirectional,
oﬀset of Gm(vt) would point between e
. Furthermore, let rv = rankt(Gj(v).str, iv)
this would contradict the deﬁnition of e
(cid:5)
and r
v). Thus we can
ﬁnd the location of B(vt)[et] in O(1) time if the location of B(v)[e] is known. The
dashed arrows in Figure 1 show an example.

(cid:5)
v). Then the local index of et is i

in each block has a pointer. Let (G(cid:4)(vt), i

(cid:5)
v = rankt(Gj(v).str, i

t + (rv − r
(cid:5)

(cid:5)

(cid:5)

(cid:5)

(cid:5)

Analogously, assume we know the location (Gj (vt), it) of B(vt)[et] and want to
ﬁnd the location of the corresponding element B(v)[e] in its parent node v. Using
≤ et in Gj(vt) such that there is an upward pointer
Fj(vt) we ﬁnd the last oﬀset e
(cid:5)
from B(vt)[e
t exists by construction (it can be the upward pointer from
the ﬁrst index in Gj (vt) or the reverse of some pointer from L(v) to Gj (vt)). Let
(cid:5)
B(vt)[e
v). Then, by our construction,
B(v)[e] is also in G(cid:4)(v), since if it belonged to a diﬀerent block Gm(v), then there

(cid:5)
t] point to B(v)[e

], with location (G(cid:4)(v), i

(cid:5)
t]. Oﬀset e

(cid:5)
t

(cid:5)

                                                                     Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php1790

GONZALO NAVARRO AND YAKOV NEKRICH

(cid:5)
t be the local index of e

(cid:5)
t and
(cid:5)
t. Furthermore,
(cid:5)
t in Gj (vt). Then the local index of B(v)[e] is iv =

would be a pointer from the ﬁrst occurrence of t in Gm(v).str pointing between e
et, and its bidirectional version would contradict the deﬁnition of e
let i
selectt(G(cid:4)(v).str, rankt(G(cid:4)(v).str, i
To solve access(S, i), we visit the nodes v0 = vr, v1, . . . , vh = va, where h = lgρ σ is
the height of T , vk is the tkth child of vk−1, and B(vk−1)[ek−1].chr = tk encodes S[i].
We do not compute the oﬀsets e1, . . . , eh, but just their locations. The location of
B(vr)[e0 = i] is found in O(lg n/ lg lg n) time using the partial-sums structure K(vr).
If the location of B(vk−1)[ek−1] is known, we can ﬁnd that of B(vk)[ek] in O(1) time,
as explained. When a leaf node vh = va is reached, we have S[i] = a.

v) + (it − i
(cid:5)

(cid:5)
t)).

To solve selecta(S, i), we set eh = i and identify the location of B(va)[eh] in the
list L(va) of the leaf va, using structure K(va). Then we traverse the path vh =
va, vh−1, . . . , v0 = vr, where vk−1 is the parent of vk, until the root node is reached.
In every node vk, we ﬁnd the location of B(vk−1)[ek−1] in L(vk−1) that corresponds
to B(vk)[ek], as explained above. Finally, we compute the number of elements that
precede e0 in L(vr) using structure K(vr).

Thus access and select require O(lgρ σ + lg n/ lg lg n) = O((lg σ + lg n)/ lg lg n)

worst-case time.

4.3. Rank queries. We need some additional data structures for the eﬃcient
support of rank queries. In every node v such that L(v) consists of more than one
block, we store a data structure P (v). Using P (v) we can ﬁnd, for any 0 ≤ t < ρ
and for any block Gj (v), the last block G(cid:4)(v) such that (cid:6) ≤ j and G(cid:4)(v) contains an
Pt(v) for 0 ≤ t < ρ. We describe in section 5 a way to support these predecessor

element B(v)[e] with B(v)[e].chr = t. P (v) consists of ρ predecessor data structures

queries in constant time in our scenario.

Let the location of B(v)[e] be (Gj (v), i). Structure P (v) enables us to ﬁnd the
(cid:5) ≤ e such that B(v)[e
(cid:5)
last oﬀset e
].chr = t. First, we use Rj(v) to compute r =
(cid:5)
rankt(Gj (v).str, i). If r > 0, then e
belongs to the same block Gj(v), and its local
index is selectt(Gj (v).str, r). Otherwise, we use Pt(v) to ﬁnd the last block G(cid:4)(v)
that precedes Gj(v) and G(cid:4)(v).str contains an occurrence of t. Then we ﬁnd the local
index of the last such occurrence in G(cid:4)(v).str using R(cid:4)(v).
Now we are ready to describe the procedure to answer ranka(S, i). The symbol a
is represented as a concatenation of symbols t0◦ t1◦···◦ th, where each tk ∈ [1..ρ]. We
traverse the path from the root vr = v0 to the leaf va = vh. We ﬁnd the location of
e0 = i in vr using the data structure K(vr). In each node vk, 0 ≤ k < h, we identify
≤ ek and B(vk)[e
(cid:5)
k].chr = tk,
the location of the last element B(vk)[e
(cid:5)
using Ptk (vk) as explained. From the location of B(vk)[e
k] we ﬁnd the location of the
corresponding element B(vk+1)[ek+1] in L(vk+1), just as done for access.

(cid:5)
k] such that e

(cid:5)
k

When our procedure reaches the leaf node vh = va, the (virtual) element B(vh)[eh]
encodes the last occurrence of a in S[1, i]. Note that we know the location (G(cid:4)(vh), ih)
of B(vh)[eh], not the oﬀset eh itself. Then we ﬁnd the number r of elements in all the
blocks that precede G(cid:4)(vh) using K(va). Finally, ranka(S, i) = r + ih.

rank is also O(lgρ σ + lg n/ lg lg n) =

Therefore,

the

total

time

for

O((lg σ + lg n)/ lg lg n).

4.4. Insertions. Now we describe how internode pointers are implemented. We
say that an element is pointed if there is a pointer to it. We cannot directly store the
local index of a pointed element in the pointer: when a new element is inserted into
a block, the indexes of all the elements that follow it are incremented by 1. Since

                                                                     Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpOPTIMAL DYNAMIC SEQUENCE REPRESENTATIONS

1791

a block can contain Θ(lg3 n/ lg ρ) pointed elements, we would have to update that
many pointers after each insertion and deletion.

Therefore we resort to the following two-level scheme. Each pointed element in a
block is assigned a unique identiﬁer. When a new element is inserted, we assign it the
identiﬁer max−id + 1, where max−id is the maximum identiﬁer value used so far. We
also maintain a data structure Hj(v) for each block Gj(v), which enables us to ﬁnd
the local index of a pointed element given its identiﬁer in Gj(v). The implementation
of Hj(v) is based on standard word RAM techniques and a table that contains the
identiﬁers of the pointed elements; the details are given in section 6.

Now we describe how to insert a new symbol a at position i in S. Let e0, e1, . . . , eh
be the oﬀsets of the elements that will encode a = t0◦···◦th in vr = v0, v1, . . . , vh = va.
We can ﬁnd the location of B(vr)[e0 = i] in L(vr) in O(lg n/ lg lg n) time using
K(vr), and then insert a new element bvr with bvr .chr = t0 at that location, so
that now B(vr)[e0] = bvr . Now, given the location of B(vk)[ek] in L(vk), where
(cid:5)
B(vk)[ek].chr = tk has just been inserted, we ﬁnd the location of B(vk)[e
k] for the
(cid:5)
k].chr = tk, in the same way as for rank
largest oﬀset e
(cid:5)
queries.2 From the location of B(vk)[e
k], we ﬁnd the location of the corresponding
(cid:5)(cid:5)
element, B(vk+1)[e
k+1], in L(vk+1). The symbol tk+1 must then be inserted into
L(vk+1) immediately after B(vk+1)[e

(cid:5)
k < ek such that B(vk)[e

(cid:5)(cid:5)
k+1], that is, at oﬀset ek+1 = e

(cid:5)(cid:5)
k+1 + 1.

The insertion of a new element bvk into a block Gj(vk) is handled by structure
Rj(vk) and the memory manager of the block. We must also update structures Fj(vk)
and Hj(vk) to keep the correct alignments, and possibly to create and destroy a
constant number of internode pointers to maintain our invariants. Also, since pointers
are bidirectional, a constant number of internode pointers in the parent and children
of node vk may be updated. All those changes can be done in O(1) time; see section 6
for the details. Insertions may also require updating structures Pt(vk), which takes
O(1) amortized time; see section 5. Finally, if vk is the root node or a leaf, we also
update K(vk). This update is only by ±1, so we recall it requires just O(lg n/ lg lg n)
If |Gj (vk)| exceeds 2 lg3 n/ lg ρ, we split Gj(vk) evenly into two blocks, Gj1 (vk) and
Gj2 (vk). Then, we rebuild the data structures R, F , and H for the two new blocks.
Note that there are internode pointers to Gj(vk) that now could become dangling
pointers, but all those can be known from Fj(vk), since pointers are bidirectional,
and updated to point to the right locations in Gj1 (vk) or Gj2 (vk). Finally, if vk is the
root or a leaf, then K(vk) is updated.

time.

The total cost of splitting a block is dominated by that of building the new data
structures R, F , and H. These are easily built in O(lg3 n/ lg ρ) time. Since we split
a block Gj (v) at most once per sequence of Θ(lg3 n/ lg ρ) insertions in Gj(v), the
amortized cost incurred by splitting a block is O(1). Therefore the total amortized
cost of an insertion in L(v) is O(1). The insertion of a new symbol leads to O(lgρ σ)
insertions into lists L(vk).

To update K(vk), upon an overﬂow in Gj(vk), we add a new element before
or after xj . Lemma 4.1 does not support updates with large values.
Inserting
a new value xj+1 = 0, then increasing it up to xj+1 = |Gj2 (vk)|, and then decreasing 
the value of xj = |Gj (vk)| to make it xj = |Gj1 (vk)|, can be done in
O(lg3 n/(lg ρ lg lg n)) time by adding/subtracting O(lg n) units at a time. Each such
increment/decrement and insertion takes O(lg n/ lg lg n) time, and we carry it out

2We cannot descend using ek itself, as for access, because the internode pointer invariants have

not yet been restored for the new elements.

                                                                     Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php1792

GONZALO NAVARRO AND YAKOV NEKRICH

O(|Gj (vk)|/ lg n) = O(lg2 n/ lg ρ) times. Still, this total cost amortizes to o(1) per

operation.

Hence, the total amortized cost of an insertion is O(lgρ σ + lg n/ lg lg n) =

O((lg σ + lg n)/ lg lg n).

Next we describe how deletions are handled, and we also describe the data structure 
P (v).

5. Lazy deletions and data structure P (v). We do not process deletions
immediately, but in lazy form: we do not maintain exactly S but a supersequence S
of it. When S[i] is deleted from S, we retain it in S but mark S[i] as deleted. When
the number of elements marked as deleted exceeds a certain threshold, we expunge
them all from the data structure. We deﬁne B(v) and the list L(v) for the sequence
S in the same way as B(v) and L(v) are deﬁned for S.

We will make use of split-ﬁnd data structures. This structure maintains a sequence
of objects, some of which are marked. Operation split(x) marks object x. Operation
ﬁnd(x) gives the rightmost marked object that is not after x in the sequence. A
new object can be inserted immediately before or after an object x in the sequence.
A split-ﬁnd data structure [33] can implement operation ﬁnd in constant time, and
operation split and insertions in amortized constant time.

Since elements of L(v) are never removed, we can implement P (v) using splitﬁnd 
data structures. For each t, 0 ≤ t < ρ, Pt(v) will be a split-ﬁnd data structure
containing one object per block Gj (v) in L(v). The marked objects in Pt(v) are the
blocks Gj(v) that contain an occurrence of t, so ﬁnd(Gj (v)) gives the block G(cid:4)(v) for
the maximum (cid:6) ≤ j such that G(cid:4)(v).str contains a t, in constant time as desired.

The insertion of a symbol t in Gj (v) may induce a new split in Pt(v), if Gj(v)
was not already marked. Furthermore, overﬂows in Gj(v), which convert it into two
blocks Gj1 (v) and Gj2 (v), induce insertions in Pt(v). Note that an overﬂow in Gj(v)
triggers ρ insertions in the Pt(v) structures, but the resulting O(ρ) time amortizes to
o(1) because overﬂows occur every Θ(lg3 n/ lg ρ) insertions.

Structures Pt(v) do not support “unsplitting” or removals. The replacement of
Gj(v) by Gj1 (v) and Gj2 (v) is implemented by leaving in Pt(v) the object corresponding 
to Gj(v) and inserting one corresponding to either Gj1 (v) or Gj2 (v).
If
Gj(v).str contained t, then at least one of Gj1 (v).str and Gj2 (v).str must contain t,
and the other can be inserted as a new element (possibly followed by a split, if it also
contains t).

We will maintain partial-sums structures K(v), storing the number of nondeleted
elements in each block of L(v), for the root node v = vr and each leaf node, v = va.
Moreover, we maintain a new data structure Dj(v) for every block Gj(v), where v
is either the root or a leaf node. Dj(v) returns, in constant time, the number of
nondeleted elements in Gj (v)[1, i], for any local index i, as well the local index in
Gj(v) of the ith nondeleted element. The implementation of Dj(v) is described in
section 6. We use K(v) and Dj(v) to ﬁnd the oﬀset e in L(v) of the eth nondeleted
element, and to count the number of nondeleted elements that occur up to oﬀset e in
L(v).

We also store a global list DEL that contains, in any order, all the elements
marked as deleted that have not yet been expunged from the wavelet tree. For any
element S[i] in DEL we store a pointer to B(vr)[i] in L(vr). These are implemented
in the same way as internode pointers.

5.1. Queries. Queries are answered very similarly to section 4. The main idea
is that we can essentially ignore deleted elements except at the root and at the leaves.

                                                                     Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpOPTIMAL DYNAMIC SEQUENCE REPRESENTATIONS

1793

access(S, i). We do exactly as in section 3, except that e0 refers to the ith nondeleted 
element in L(vr), and is found using K(vr) and Dj(vr).

selecta(S, i). We ﬁnd the location of the ith nondeleted element in L(vh), where
vh = va, and the oﬀset eh is found using K(va) and some Dj(va). Then we move
up in the tree exactly as in section 4. When the root node v0 = vr is reached, we
count the number of nondeleted elements that precede oﬀset e0 using K(vr) and some
Dj(vr).

ranka(S, i). We ﬁnd the oﬀset e0 of the ith nondeleted element in L(vr). Let vk, tk
≤ ek such that
be deﬁned as in section 4. In every node vk, we ﬁnd the last oﬀset e
(cid:5)
B(vk)[e
k].chr = tk. Note that this element may be a deleted one, but it still drives us
to the correct location in L(vk+1). We proceed exactly as in section 4 until we arrive
at a leaf vh = va. Finally, we count the number of nondeleted elements preceding
oﬀset eh using K(va) and some Dj(va).

(cid:5)
k

5.2. Updates. Insertions are performed just as in section 4, except that we
also update the data structure Dj(vk) when an element B(vk)[ek] that encodes the
inserted symbol a is added to a block Gj(vk). When S[i] is deleted, we append it
to the list DEL. Then we visit each block Gj(vk) containing the element B(vk)[ek]
that encodes S[i] and update the data structures Dj(vk). Finally, K(vr) and K(va)
are also updated. This takes in total O(lgρ σ + lg n/ lg lg n) = O((lg σ + lg n)/ lg lg n)
time.

When the number of symbols in the list DEL reaches n/ lg2 n, we perform a
procedure to eﬀectively delete all of its elements. Therefore DEL never requires
more than O(n/ lg n) bits, and the space overhead due to storing deleted symbols
is O(n lg σ/ lg2 n) bits.

Let B(vk)[ek], 0 ≤ k ≤ h, be the elements that encode a symbol S[i] ∈ DEL. The
method for tracking the elements B(vk)[ek], removing them from their blocks Gj(vk),
and updating the block structures, is symmetric to the insertion procedure described
in section 4. In this case we do not need the predecessor queries to track the elements
to delete, as the procedure is similar to that for accessing S[i]. When the size of a
block Gj(vk) falls below (lg3 n)/(2 lg ρ) and it is not the last block of L(vk), we merge
it with Gj+1(vk), and then split the result if its size exceeds 2 lg3 n/ lg ρ. This retains
O(1) amortized time per deletion in any node vk, including the updates to K(vk)
structures, and adds up to O((lg σ + lg n)/ lg lg n) amortized time per deleted symbol.
Once all the pointers in DEL are processed, we rebuild from scratch the structures
P (v) for all nodes v. The total size of all the P (v) structures is O(ρn lg σ/ lg3 n).
Since a data structure for incremental split-ﬁnd is constructed in linear time [33], all
the P (v) structures are rebuilt in O(n lg σ/ lg3−ε n) time. Hence the amortized time
to rebuild the P (v)s is O(lg σ/ lg1−ε n), which does not aﬀect the amortized time
O((lg σ + lg n)/ lg lg n) to carry out the eﬀective deletions.

6. Data structures for handling blocks. We describe the way the data are
stored in blocks Gj(v), as well as the way the various structures inside blocks operate.
All the data structures are based on the same idea: we maintain a tree with node
degree lgδ n and leaves that contain O(lg n) bits. Since elements within a block can be
addressed with O(lg lg n) bits, each internal node and each leaf ﬁts into O(1) machine
words. As a result, we can support searching and basic operations in each node in
constant time.

6.1. Data organization. The block data are physically stored as a sequence
lgρ n to 2 lgρ n symbols, using Θ(lg n) bits. Thus there are

of miniblocks of

                                                                     Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php1794

GONZALO NAVARRO AND YAKOV NEKRICH

O(|Gj (v)|/ lgρ n) = O(lg2 n) miniblocks in a block. These miniblocks will be the
leaves of a B-tree T with internal nodes of arity τ to 2τ for τ = Θ(lgδ n) and some
constant 0 < δ < 1. The height of this tree is constant, O(1/δ). Each node of T stores
Θ(τ ) counters telling the number of symbols stored at the leaves that descend from
each child. This requires just O(τ lg lg n) = o(lg n) bits. To access any position of
Gj(v), we descend in T , using the counters to determine the correct child. When we
arrive at a leaf, we know the local oﬀset of the desired symbol within the leaf, and can
access it directly. Since the counters ﬁt in less than a machine word, a small global
precomputed table gives the correct child in constant time, for any possible sequence

of up to 2τ counters and any possible desired oﬀset. The table has 22τ lg(2 lgρ n)· 2 lgρ n
entries, which is o(nα) for any constant α > 0. Therefore, we have O(1) time access
to any symbol.

Upon updates in Gj (v), we arrive at the correct leaf of its B-tree T , insert or delete
the symbol (in constant time because the leaf contains O(lg n) bits), and update the
counters in the path from the root (in constant time as they have o(lg n) bits, with the
help of another global precomputed table). Splits/merges upon overﬂows/underﬂows
of leaves are handled as usual, and can be solved in O(1/δ) constant-time operations
(again, with the help of global precomputed tables to update the counters upon splits
and merges).

The space overhead of the internal nodes of T is O(|Gj (v)| lg ρ lg lg n/ lg n) bits,
as there are O((|Gj (v)|/ lgρ n)/τ ) internal nodes and each one uses O(τ lg lg n) bits

for its counters.

We consider now the space used by the data itself, that is, the string Gj (v).str.
In order not to waste space, the miniblock leaves are stored using a simple memory
management structure.

Lemma 6.1 (see [45]). A memory area storing s bits in total, handling chunks of
sizes up to b bits, can be managed so that chunks can be allocated, freed, and accessed
in O(b/ lg n) worst-case time, within a total space of s + O((s/b) lg s + b2 + b lg s) bits.
This space forms a contiguous area that grows or shrinks by multiples of some ﬁxed
value in Θ(b).

For our case, where s = |Gj (v)| lg ρ = O(lg3 n) and b = Θ(lg n) is a memory 
area of polylogarithmic size, the lemma allows us to allocate, free, and access 
miniblocks in constant time, while using pointers of O(lg lg n) bits and wasting

O(|Gj (v)| lg ρ lg lg n/ lg2 n + lg2 n + lg n lg lg n) bits. Added up over all the wavelet
tree, the ﬁrst term adds up to O(n lg σ lg lg n/ lg2 n) bits, whereas the rest adds up to
O(n lg σ/ lg n + σ lg2 n) bits. In order to reduce the last term to O(σ lg n), the ﬁrst
blocks G1(v) of all the σ wavelet tree lists L(v) are grouped into sets of size Θ(lg n)
and their memory areas are managed together. Their combined address spaces are of
size O(lg4 n), which does not aﬀect the result. This grouping is ﬁxed and does not
change upon updates.

The allocation structure of Lemma 6.1 uses a memory area of ﬁxed-size cells that
grows or shrinks at the end as miniblocks are created or destroyed. Such a structure is
called an extendible array (EA) [54], and the problem is how to handle a collection of
EAs. In our case, we must handle a set of O(n lg σ/ lg3 n + σ/ lg n) EAs. A collection
of EAs must support accessing any cell of any EA, letting any EA grow or shrink by
one cell, and create and destroy EAs. The following lemma, which assumes words of
lg n bits, is useful.

√
Lemma 6.2 ([54, Lem. 1], simpliﬁed). A collection of a EAs of total size s bits
sa lg n) bits of space, so that the operations of

can be represented using s + O(a lg n +

                                                                     Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpOPTIMAL DYNAMIC SEQUENCE REPRESENTATIONS

1795

(cid:5)

(cid:5)

bits can be destroyed in time O(s

creation of an empty EA and access take constant worst-case time, whereas grow/shrink
take constant amortized time. An EA of s

/ lg n).
√
In our case a = O(n lg σ/ lg3 n + σ/ lg n) and s = O(n lg σ), so the space overhead
posed by the EAs is O(n lg σ/ lg2 n+σ+n lg σ/ lg n+
nσ lg σ) = O(n lg σ/ lg n+σ lg n).
When we store the miniblocks in compressed form, in section 8, they could use
as little as O(lgε n lg lg n) bits, and thus we could store up to Θ(lg1−ε n/ lg lg n)
miniblocks in a single leaf of T . This can still can be handled in constant time using
(more complicated) global precomputed tables [39], and the counters and pointers of
O(lg lg n) bits are still large enough.
total
O(n lg σ(lg lg n)2/ lg n + σ lg n) bits.

Summing

overhead

of

space

the

structure

is

up,

the

6.2. Structure Rj(v). To support rank and select we enrich T with further
information per node. We store ρ counters with the number of occurrences of each
in the subtree of each child. The node size becomes O(τ ρ lg lg n) =
symbol
O(lgε+δ n lg lg n) = o(lg n) as
This adds up to
O(|Gj (v)|ρ lg ρ lg lg n/ lg n) bits because the leaves of T handle Θ(τ ) miniblocks. Added
over the whole wavelet tree, this is O(n lg σ lg lg n/ lg1−ε n) bits.

long as ε + δ < 1.

With this information on the nodes we can easily solve rank and select in constant
time, by descending on T and determining the correct child (and accumulating data
about the leftward children) in O(1) time using global precomputed tables. Nodes can
also be updated in constant time even upon splits and merges, since all the counters
can be recomputed in O(1) time with the help, again, of global precomputed tables.
6.3. Structure Fj(v). This structure stores all the internode pointers leaving

from block Gj (v), to its parent and to any of the ρ children of node v.

The structure is a tree Tf very similar in spirit to T . The pointers are stored at
the leaves of Tf , in increasing order of their source index inside Gj(v). The pointers
stored are internode, and thus require Θ(lg n) bits. Thus we store a constant number
of pointers per leaf of Tf . For each pointer we store the local index in Gj (v) holding
the pointer, and the target location, formed by a system-wide pointer to a block G(cid:4)(u)
plus an identiﬁer of the local index within G(cid:4)(u) (see section 6.4). The internal nodes
x, of arity Θ(τ ), maintain information on the number of indexes of Gj(v) covered by
each child of x, and the number of pointers of each kind stored in the subtree of each
child of x (1 + ρ counters, for the parent of v and for the tth wavelet tree child of v
for each 0 ≤ t < ρ). This requires O(τ ρ lg lg n) = o(lg n) bits, as before. To ﬁnd the
last local index up to i holding a pointer of a certain kind, we traverse Tf from the
root looking for index i. At each node x, it might be that the child y where we have
to enter holds pointers of that kind, or not. If it does, then we ﬁrst enter into child
y. If we return with an answer, we recursively return it. If we return with no answer,
or there are no pointers of the desired kind below y, we enter into the last sibling to
the left of y that holds a pointer of the desired kind, and switch to a diﬀerent mode
where we simply go down the tree looking for the rightmost child with a pointer of the
desired kind. It is not hard to see that this procedure visits O(1/δ) nodes, and thus it
is constant time because all the computations inside nodes can be done in O(1) time
with global precomputed tables. When we arrive at the leaf, we scan for the desired
pointer in constant time.

The tree Tf must be updated when a symbol t is inserted before any other occurrence 
of t in Gj(v), when a symbol is inserted at the ﬁrst position of Gj(v), and, similarly,
 when symbols are deleted from Gj(v). The needed queries are easily answered
with tree T (enriched for rank/select queries). Moreover, due to the bidirectionality,

                                                                     Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php1796

GONZALO NAVARRO AND YAKOV NEKRICH

we must also update Tf when pointers to Gj(v) are created from the parent or a child
of v, or when they are deleted. Those updates work just like on the tree T . Tf is also
updated upon insertions and deletions of symbols, even if they do not change pointers,
to maintain the positions up to date. In this case we traverse Tf looking for the position 
of the update, change the local indexes stored at the leaf, and update the subtree
sizes stored at the internal nodes of Tf . The total space overhead is as of Rj(v).

6.4. Structure Hj(v). This structure manages the internode pointers that
point inside Gj(v). As explained in section 4.4, we give a handle to the outside
nodes, that does not change over time, and Hj(v) translates handles to local indexes
in Gj(v).

We store a tree Th that is just like Tf , where the incoming pointers are stored.
Th is simpler, however, because at each node we only need to store the number of
indexes covered by the subtree of each child. It must also be possible to traverse Th
from a leaf to the root.

In addition, we manage a table T bl so that T bl[id] points to the leaf of Th where
the pointer corresponding to handle id is stored. T bl is also managed as a tree similar
to Tf , with pointers sorted by id, where a constant number of identiﬁers id are stored
at the leaves together with their pointers to the leaves of Th (note that there are
O(lg3 n/ lg ρ) identiﬁers at most, so we need O(lg lg n) bits for the identiﬁers and
their pointers to Th). Each internal node in T bl maintains the maximum identiﬁer,
and the number of identiﬁers, stored at its leaves. Thus one can, in constant time,
ﬁnd the pointer to Th corresponding to a given id, and also ﬁnd the smallest unused
identifer when a fresh one is needed (by looking for the ﬁrst leaf of T bl where the
maximum identiﬁer is larger than the number of identiﬁers up to that leaf).

At the leaves of Th we store, for each pointer, a back pointer to the corresponding
leaf of T bl and the local index in Gj (v). Given a handle id, we ﬁnd using T bl the
corresponding place in the leaf of Th, and move upwards up to the root of Th, adding
to the leaf index the number of indexes covered by the leftward children of each node.
At the end we obtain the local index of id.

When pointers to Gj(v) are created or destroyed, we insert or remove pointers
in Th. Insertion requires traversing Th top-down to ﬁnd the leaf covering the desired
index, and then creating a fresh entry T bl[id] pointing to it (so that id will be the
external handle associated with the inserted pointer). Deletion of id requires going
to the leaf of Th given by T bl[id], removing the pointer from the leaf, and freeing
id from T bl. Splits and merges of leaves of Th require moving a constant number of
pointers, updating their pointers from T bl (which are found using the back pointers),
and updating the counters towards the root of Th. Similarly, the splits and merges in
T bl require updating the back pointers from Th, which are found using the pointers
from T bl to Th. We must also update Th upon symbol insertions and deletions in
Gj(v), to maintain the indexes up to date.

T bl and Th may contain up to Θ(lg3 n/ lg ρ) pointers of O(lg lg n) bits, which can
be signiﬁcant for some blocks. However, across the whole structure there can be only
O(ρn lg σ/ lg3 n) pointers, adding up to s = O(ρn lg σ lg lg n/ lg3 n) bits, spread across
a = O(n lg σ/ lg3 n) structures T bl and Th. Using again Lemma 6.2, a collection of
EAs poses an overhead of O(n lg σ/ lg2 n) bits. This includes the space overhead of
the values stored at internal nodes.

6.5. Structure Dj(v) and the ﬁnal result. Structure Dj(v) is implemented
as a tree Td analogous to T , storing at each node the number of elements and the
number of nondeleted elements below each child. It takes O(|Gj (v)| lg ρ lg lg n/ lg n)

                                                                     Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpOPTIMAL DYNAMIC SEQUENCE REPRESENTATIONS

1797

bits. Since these are stored only for the root vr and the leaves va of T , its space adds
up to O(n lg ρ lg lg n/ lg n) = O(n(lg lg n)2/ lg n) bits.

Let us now consider the total space for all the data structures. While the
raw data add up to n lg σ bits, the extra space is dominated by structures Rj(v)
and Fj(v), adding up to O(n lg σ lg lg n/ lg1−δ−ε n) bits, plus O(σ lg n) for the memory 
management overhead. We can use, say, δ = ε and then have O(1/ε) time
and O(n lg σ lg lg n/ lg1−ε n + σ lg n) bits for any 0 < ε < 1 (renaming 2ε as ε). By
further
space overhead as
O(n lg σ/lg1−ε n+σ lg n) bits for any 0 < ε < 1.

increasing ε inﬁnitesimally, we

can write

the

7. Changes in lg n and alphabet independence. Note that our structures
depend on the value of lg n, so they should be rebuilt when (cid:4)lg n(cid:5) changes. We use w =
(cid:4)lg n(cid:5) as a ﬁxed value and rebuild the structure from scratch when n reaches another
power of two (more precisely, we use words of w = (cid:4)lg n(cid:5) bits until (cid:4)lg n(cid:5) increases by 1
or decreases by 2, and only then update w and rebuild). These reconstructions do not
aﬀect the amortized complexities, and the slightly larger words waste an O(1/ lg n)
extra space factor in the redundancy.
We take advantage of using a ﬁxed w value to get rid of the alphabet dependence.
If lg σ ≤ w, our time complexities are the optimal O(lg n/ lg lg n). However, if σ
is larger, this means that not all the alphabet symbols can appear in the current
sequence (which contains at most n ≤ 2w < σ distinct symbols). Therefore, in this
case we create the wavelet tree for an alphabet of size s = 2w, not σ (this wavelet tree
is created when w changes). We also set up a mapping array SN [1, σ] that will tell to
which value in [1..s] a symbol is mapped, and a reverse mapping N S[1, s] that tells
to which original symbol in [1..σ] does a mapped symbol correspond. Both SN and
N S are initialized in constant time [42, section III.8.1] and require O(σ lg n + n lg σ)
bits of space. Since this is used only when σ > n, the space is O(σ lg n).

Upon operations ranka(S, i) and selecta(S, j), the symbol a is mapped using SN
(the answer is obvious if a does not appear in SN ) in constant time. The answer of
operation access(S, i) is mapped using N S in constant time as well. Upon insertion of
a, we also map a using SN . If not present in SN , we ﬁnd a free slot N S[i] (we maintain
a list of free slots) and assign N S[i] = a and SN [a] = i. When the last occurrence of
a symbol a is deleted (we maintain global counters to determine this in constant time)
we return its slot to the free list and uninitialize its entry in SN . In this way, when
lg σ > lg n, we can support all the operations in time O(lg s/ lg lg s) = O(lg n/ lg lg n).
We are ready to state a ﬁrst version of our result, not yet compressing the
In section 6 it was shown that the time for the operations is O(1/ε).
sequence.
Since the height of the wavelet tree is lgρ min(σ, s) = O( 1
ε lg n/ lg lg n), then we have
O( 1
ε2 lg n/ lg lg n) time for all the operations. Considering the total space overhead
obtained in section 6, which also dominates previous ones like the space for the structures 
L(v), K(v), P (v), and DEL (see Table 2), we obtain the following result. (Note
that when σ > n we use an alphabet of size O(n), but then still we need the SN
mapping, that takes O(σ lg n) bits.)
Theorem 7.1. A dynamic string S[1, n] over alphabet [1..σ] can be stored in
a structure using n lg σ + O(n lg σ/ lg1−ε n + σ lg n) bits, for any 0 < ε < 1, and
supporting queries access, rank, and select in time O( 1
ε2 lg n/ lg lg n). Insertions and
deletions of symbols are supported in O( 1

ε2 lg n/ lg lg n) amortized time.

8. Compressed space. Now we compress the space of the data structure to
zero-order entropy (nH0(S) plus redundancy). We show how a diﬀerent encoding of

                                                                     Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php1798

GONZALO NAVARRO AND YAKOV NEKRICH

the bits within the blocks reduces the n lg σ to nH0(S) in the space without aﬀecting
the time complexities.

Raman, Raman, and Rao [53] describe an encoding for a bit vector B[1, n] that
occupies nH0(B) + O(n lg lg n/ lg n) bits of space. It consists of cutting the bit vector
into chunks of length b = (lg n)/2 and encoding each chunk i as a pair (ci, oi): ci is
the class, which indicates how many 1’s there are in the chunk, and oi is the oﬀset,
which is the index of this particular chunk within its class. The ci components add
up to O(n lg lg n/ lg n) bits, whereas the oi components add up to nH0(B). Navarro
and Sadakane [49, sect. 8] describe a technique to maintain a dynamic bit vector in
this format. They allow the chunk length b to vary, so they encode triples (bi, ci, oi)
maintaining the invariant that bi + bi+1 > b for any i. They show that this retains
the same space, and that each update aﬀects O(1) chunks.

i , . . . , cρ

We extend this encoding to handle an alphabet [1..ρ] [20], so that b = (lgρ n)/2
symbols, and each chunk is encoded as a tuple (bi, c1
i counts the
occurrences of t in the block. The classes (bi, c1
i ) use O(ρn lg lg n/ lg n) bits,
and the oﬀsets still add up to nH0(B). Blocks are encoded/decoded in O(1) time,
as the class takes O(ρ lg lg n) = o(lg n) bits and the block encoding requires at most
O(lg n) bits. At the end of section 6.1 we show that the plain representation of the
data can be changed to a compressed one without aﬀecting the operations inside
i , oi) uses at least Θ(lgε n lg lg n)
blocks, thanks to the fact that a tuple (bi, c1
bits, so the number of miniblocks that ﬁt in a single leaf of T is still polylogarithmic,
and then the counters in internal nodes of T still require O(lg lg n) bits.

i , oi), where ct

i , . . . , cρ

i , . . . , cρ

The sum of the local entropies of the chunks, across the whole L(v), adds up
to nH0(Bv), and these add up to nH0(S) [26]. The redundancy over the entropy is
O(ρ lg lg n) bits per miniblock, adding up to O(nH0(S) lg lg n/ lg1−ε n) bits. The fact
that we store S instead of S, with up to O(n/ lg2 n) spurious symbols, can increase
nH0(S) up to nH0(S) ≤ nH0(S) + O(n/ lg n) bits. Adjusting ε inﬁnitesimally, all the
space overhead on top of nH0(S) is dominated by the O(n lg σ/ lg1−ε n + σ lg n) bits
of space overhead obtained in section 6.3 Thus we get the following result, for any
desired 0 < ε < 1.

entropy H0(S)

Theorem 8.1. A dynamic string S[1, n] over alphabet [1..σ] and with zero-order
stored in a structure using nH0(S) +
empirical
O(n lg σ/ lg1−ε n + σ lg n) bits, for any 0 < ε < 1, and supporting queries access,
rank, and select in time O( 1
ε2 lg n/ lg lg n). Insertions and deletions of symbols are
supported in O( 1

ε2 lg n/ lg lg n) amortized time.

can be

9. Worst-case complexities. While in previous sections we have obtained optimal 
time and compressed space, the time for the update operations is amortized.
In this section we derive worst-case time complexities, at the price of losing the time
optimality, which will now become logarithmic for some operations. In the rest of the
section we remove the various sources of amortization in our solution.

9.1. Block splits and merges. Our amortized solution splits overﬂowing blocks
and rebuilds the two new blocks from scratch (section 4.4). Similarly, it merges underﬂowing 
blocks (as a part of the cleaning of the global DEL list in section 5.2). This
gives good amortized times but in the worst case the cost is Ω(lg3 n/ lg lg n).

3Actually, some of this space overhead is also reduced to a function of H0(S) instead of lg σ,
but there are, anyway, other additive terms that are insensitive to the compression, which make this
path less attractive to pursue.

                                                                     Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpOPTIMAL DYNAMIC SEQUENCE REPRESENTATIONS

1799

We use a technique [25] that avoids global rebuildings. A block is called dense
if it contains at least lg3 n bits, and sparse otherwise. While sparse blocks of any
size (larger than zero) are allowed, we maintain the invariant that no two consecutive
sparse blocks may exist. This retains the fact that there are O(n lg σ/ lg3 n+σ) blocks
in the data structure. The maximum size of a block will be 2 lg3 n bits. When a block
overﬂows due to an insertion, we move its last element to the beginning of the next
block. If the next block would also overﬂow, then we are entitled to create a new
sparse block between both dense blocks, containing only that element. Analogously,
when a deletion converts a dense block into sparse (i.e., it falls below length lg3 n
bits), we check if both neighbors are dense. If they are, the current block can become
sparse too. If, instead, there is a sparse neighbor, we move its ﬁrst/last element into
the current block to avoid it becoming sparse.
If this makes that sparse neighbor
block become of size zero, we remove it.

Therefore, we only create and destroy empty blocks, and move a constant number
of elements to neighboring blocks. This can be done in constant worst-case time. It
also simpliﬁes the operations on the partial-sum data structures K(v), since now only
updates by ±1 and insertions/deletions of elements with value zero are necessary, and
these are carried out in O(lg n/ lg lg n) worst-case time (Lemma 4.1). Recall that lg n
is ﬁxed in each instance of our data structure, so the deﬁnition of sparse and dense is
static.

9.2. Split-ﬁnd data structure and lazy deletions. The split-ﬁnd data structure 
[33] we used in section 5 to implement the Pt structures has constant amortized
insertion time. We replace it by another one [44, Thm 4.1] achieving O(lg lg n) worstcase 
time. Their structure handles a list of colored elements (list nodes), where each
element can have O(1) colors (each color is a positive integer bounded by O(lgε n) for
a constant 0 < ε < 1). We will only use list nodes with 0 or 1 color. The operations
of interest to us are creating a new list node without colors, assigning or removing a
color to/from a list node, and ﬁnding the last list node preceding a given node and
having some given color. Node deletions are not supported. The number of list nodes
must be smaller than a certain upper bound n
).
In our case, since lg n is ﬁxed, we can use n

= 2w = O(n) as the upper bound.

, and the operations cost O(lg lg n

(cid:5)

(cid:5)

(cid:5)

We use ρ colors, one per symbol in the sequences. Each time we create a block,
we add a new uncolored node to the list, with a bidirectional pointer to the block.
Each time we insert a symbol t ∈ [1..ρ] for the ﬁrst time in a block, we add a new
node colored t to the list, right after the uncolored element that represents the block,
and also set a bidirectional pointer between this node and the block.

We cannot use the lazy deletions mechanism of section 5, as it gives only good
amortized complexity. We carry out the deletions immediately in the blocks, as said
in section 9.1. Each time the last occurrence of a symbol t ∈ [1..ρ] is deleted from a
block, we remove the color from the corresponding list node (if the symbol reappears
later, we reuse the same node and color it, instead of creating a new one).

Therefore, ﬁnding the last block where a symbol t appears, as needed by the
rank query and for insertions, corresponds to ﬁnding the last list node colored t and
preceding the uncolored node that represents the current block.

Since list nodes cannot be deleted, when a block disappears its (uncolored) list
nodes are left without an associated block. This does not alter the result of queries,
but there is the risk of maintaining too many useless nodes. We permanently run an
incremental list “copying” process, traversing the current list of blocks and inserting

                                                                     Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php1800

GONZALO NAVARRO AND YAKOV NEKRICH

the corresponding nodes into a new list. This new list is also updated, together with
the current list, on operations concerning the blocks already copied. When the new
list is ready it becomes the current list and the previous list is incrementally deleted.
In O(nρ lg ρ/ lg3 n) steps we have copied the current list; by this time the number of
useless nodes is at most O(nρ lg ρ/ lg3 n) and just poses O(n lg lg n/ lg2−ε n) bits of
space overhead.

Note that blocks must manage the sets of up to ρ pointers to their colored nodes.
This is easily handled in constant time with the same techniques used for structure
Fj(v) in section 6.

Since the colored list data structure requires O(lg lg n) time, operations rank and
ε lg n), whereas access, select, and delete still stay in

insert take worst-case time O( 1
O( 1

ε2 lg n/ lg lg n).
9.3. Changes in lg n. As an alternative to reconstructing the whole structure
when n doubles or halves, M¨akinen and Navarro [39] describe a way to handle this
problem without aﬀecting the space nor the time complexities, in a worst-case scenario.
 The sequence is cut into a preﬁx, a middle part, and a suﬃx. The middle
part uses a ﬁxed value (cid:4)lg n(cid:5), the preﬁx uses (cid:4)lg n(cid:5)− 1, and the suﬃx uses (cid:4)lg n(cid:5) + 1.
Insertions and deletions trigger slight expansions and contractions in this separation,
so that when n doubles all the sequence is in the suﬃx part, and when n halves all
the sequence is in the preﬁx part, and we smoothly move to a new value of lg n. This
means that the value of (cid:4)lg n(cid:5) is ﬁxed for any instance of our data structure. Operations 
access, rank, and select, as well as insert and delete, are easily adapted to handle
this split string.

Actually, to have suﬃcient time to build global precomputed tables of size O(nα)
for 0 < α < 1, the solution [39] maintains the sequence split into ﬁve, not three, parts.
This also gives suﬃcient time to build any global precomputed table we need to handle
block operations in constant time, as well as to build the wavelet tree structures of
the new partitions.

9.4. Memory management inside blocks. The EAs of Lemma 6.2 (section 6)
have amortized times to grow and shrink. Converting those to worst-case time requires
a constant space overhead factor. While this is acceptable for the EAs of structures T bl
and Th in section 6, they raise the overall space to O(nH0(S)) bits if used to maintain
the main data. Instead, we get completely rid of the EA mechanism to maintain the
data, and use a single large memory area for all the miniblocks of section 6, using
Lemma 6.1.

The problem of using a single memory area is that the pointers to the miniblocks
require Θ(lg n) bits, which is excessive because miniblocks are also of Θ(lg n) bits.
Instead, we use slightly larger miniblocks, of Θ(lg n lg lg n) bits. This makes the
overhead due to pointers to miniblocks O(|Gj (v)|/ lg lg n), adding up to an additional
O(nH0(S)/ lg lg n + n lg σ/ lg1−ε n) = o(n lg σ) bits.

The price of using larger miniblocks is that now the operations on blocks are not
constant time anymore because they need to traverse a miniblock, which takes time
O(lg lg n). We can still retain constant time for the query operations, by considering
logical miniblocks of Θ(lg n) bits, which are stored in physical areas of Θ(lg lg n)
miniblocks. However, update operations like insert and delete must shift all the data
in the miniblock area and possibly relocate it in the memory manager, plus updating
pointers to all the logical miniblocks displaced or relocated. This costs O(lg lg n) time
per insertion, and deletion. This completes our result.

                                                                     Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpOPTIMAL DYNAMIC SEQUENCE REPRESENTATIONS

1801

Theorem 9.1. A dynamic string S[1, n] over alphabet [1..σ] can be stored in
a structure using nH0(S) + O(nH0(S)/ lg lg n + n lg σ/ lg1−ε n + σ lg n) = nH0(S) +
o(n lg σ)+O(σ lg n) bits, for any constant 0 < ε < 1, and supporting queries access and
ε2 lg n/ lg lg n), and query rank, insertions, and deletions
select in worst-case time O( 1
in worst-case time O( 1
ε lg n).

10. Extensions and applications. We ﬁrst describe two direct applications
of our result in important open problems. Then we extend our results to handling
general alphabets, and describe various applications of this extension.

10.1. Dynamic sequence collections. A landmark application of dynamic
sequences, stressed out in several papers along time [14, 38, 13, 38, 36, 39, 24, 37, 25,
29, 49], is to maintain a collection C of texts, where one can carry out indexed pattern
matching, as well as inserting and deleting texts from the collection. Plugging in our
new representation we can signiﬁcantly improve the time and space of previous work,
with an amortized and with a worst-case update time, respectively.
Theorem 10.1. There exists a data structure for handling a collection C of
texts over an alphabet [1..σ] within size nHh(C) + o(n lg σ) + O(σh+1 lg n + m lg n)
bits, simultaneously for all h. Here n is the length of the concatenation of m texts,
C = T1 ◦ T2 ··· ◦ Tm, and we assume that the alphabet size is σ = o(n). The structure
supports counting of the occurrences of a pattern P in O(|P| lg n/ lg lg n) time. After
counting, any occurrence can be located in time O(lgσ n lg n). Any substring of length
(cid:6) from any T in the collection can be displayed in time O(((cid:6)/ lg lg n + lgσ n) lg n).
Inserting or deleting a text T takes O(lg n + |T| lg n/ lg lg n) amortized time. For
0 ≤ h ≤ (α lgσ n) − 1, for any constant 0 < α < 1, the space simpliﬁes to nHh(C) +
Theorem 10.2. There exists a data structure for handling a collection C of
texts over an alphabet [1..σ] within size nHh(C) + o(n lg σ) + O(σh+1 lg n + m lg n)
bits, simultaneously for all h. Here n is the length of the concatenation of m texts,
C = T1 ◦ T2 ··· ◦ Tm, and we assume that the alphabet size is σ = o(n). The structure
supports counting of the occurrences of a pattern P in O(|P| lg n) time. After counting,
any occurrence can be located in time O(lgσ n lg n lg lg n). Any substring of length
(cid:6) from any T in the collection can be displayed in time O(((cid:6) + lgσ n lg lg n) lg n).
Inserting or deleting a text T takes O(|T| lg n) time. For 0 ≤ h ≤ (α lgσ n) − 1, for
any constant 0 < α < 1, the space simpliﬁes to nHh(C) + o(n lg σ) + O(m lg n) bits.
The theorems refer to Hh(C), the hth order empirical entropy of sequence C

o(n lg σ) + O(m lg n) bits.

[41]. This is a lower bound to any semistatic statistical compressor that encodes
each symbol as a function of the h preceding symbols in the sequence, and it holds
Hh(C) ≤ Hh−1(C) ≤ H0(C) ≤ lg σ for any h > 0. To oﬀer search capabilities, the
BWT [12] of C, Cbwt, is represented, not C; then access and rank operations on Cbwt
are used to support pattern searches and text extractions. K¨arkk¨ainen and Puglisi
[35] showed that, if Cbwt is split into superblocks of size Θ(σ lg2 n), and a zero-order
compressed representation is used for each superblock, the total number of bits is
nHh(C) + o(n).

We use their partitioning, and Theorems 8.1 or 9.1 to represent each superblock.
For Theorem 10.1, the superblock sizes are easily maintained upon insertions and
deletions of symbols, by splitting and merging superblocks and rebuilding the structures 
involved, without aﬀecting the amortized time per operation. They [35] also
needed to manage a table storing the rank of each symbol up to the beginning of
each superblock. This is arranged, in the dynamic scenario, with σ partial sum
data structures containing O(n/(σ lg2 n)) elements each, plus another one storing the

                                                                     Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php1802

GONZALO NAVARRO AND YAKOV NEKRICH

superblock lengths. This adds O(n/ lg n) bits and O(lg n/ lg lg n) time per operation
(Lemma 4.1). Upon block splits and merges, we use the same techniques used for K
structures described in section 4.4.

For Theorem 10.2 we use the smooth block size management algorithm described
in section 9.1 for the superblocks, which guarantees worst-case times and the same
space redundancy. Then partial-sums data structures are used without problems.

Finally, the locating and displaying of overheads are obtained by marking one
element out of lgσ n lg lg n, so that the space overhead of o(n lg σ) is maintained.
Other simpler data structures used in previous work [39], such as mappings from
document identiﬁers to their position in Cbwt and the samplings of the suﬃx array,
can easily be replaced by O(lg n/ lg lg n) time partial-sums data structures and simpler
structures to maintain dictionaries of values.

10.2. Burrows–Wheeler transform. Another application of dynamic sequences 
is to build the BWT of a text T , T bwt, within compressed space, by starting from
an empty sequence and inserting each new character, T [n], T [n − 1], . . . , T [1], at the
proper positions. Equivalently, this corresponds to initializing an empty collection and
then inserting a single text T using Theorem 10.1. The result is also stated as the
compressed construction of a static FM-index [20], a compressed index that consists
essentially of a (static) wavelet tree of T bwt. Our new representation improves upon
the best previous result on compressed space [49].

Theorem 10.3. The alphabet-friendly FM-index [20], as well as the BWT [12],
of a text T [1, n] over an alphabet of size σ, can be built using nHh(T ) + o(n lg σ) bits,
simultaneously for all 1 ≤ h ≤ (α lgσ n) − 1 and any constant 0 < α < 1, in time
O(n lg n/ lg lg n). It can also be built within the same time and nH0(T ) + o(n lg σ) +
O(σ lg n) bits, for any alphabet size σ.

We are using Theorem 10.1 for the case h > 0, and Theorem 8.1 to obtain a
less alphabet-restrictive result for h = 0 (in this case, we do not split the text into
superblocks of O(σ lg2 n) symbols, but just use a single sequence). Note that, although
insertion times are amortized in those theorems, this result is worst case because we
compute the sum of all the insertion times.

This is the ﬁrst time that o(n lg n) time complexity is obtained within compressed
space. Other space-conscious results that achieve better time complexity (but more
space) are Okanohara and Sadakane [51], who achieved optimal O(n) time within
O(n lg σ lg lgσ n) bits, and Hon, Sadakane, and Sung [31], who achieved O(n lg lg σ)
√
time and O(n lg σ) bits. On the other hand, K¨arkk¨ainen [34] may obtain less space
v) bits on top of the raw data,
but more time: O(n log n + nv) time and O(n log n/
for any parameter v.

10.3. Handling general alphabets. Our time results do not depend on the
alphabet size σ, yet our space does, in a way that ensures that σ does not interfere
with the results as long as σ = o(n) (so σ lg n = o(n lg σ)).

Let us now consider the case where the alphabet Σ is much larger than the eﬀective
alphabet of the string, that is, the set of symbols that actually appear in S at a given
point in time. Let us now use s ≤ n to denote the eﬀective alphabet size. Our aim is
to maintain the space within nH0(S)+o(n lg s)+O(s lg n) bits, even when the symbols
come from a large universe Σ = [1..|Σ|], or even from a general ordered universe such
as Σ = R or Σ = Γ

(i.e., Σ are words over another alphabet Γ).

∗

Our mappings SN and N S of section 7 give a simple way to handle a sequence
over an unbounded ordered alphabet. By changing SN to a custom structure to
search Σ, and storing elements of Σ in array N S, we obtain the following results,
using, respectively, Theorems 8.1 and 9.1.

                                                                     Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpOPTIMAL DYNAMIC SEQUENCE REPRESENTATIONS

1803

Theorem 10.4. A dynamic string S[1, n] over a general alphabet Σ can be stored
in a structure using nH0(S) + o(n lg s) + O(s lg n) + S(s) bits and supporting queries
access, rank, and select in time O(T (s) + lg n/ lg lg n). Insertions and deletions of
symbols are supported in O(U(s) + lg n/ lg lg n) amortized time. Here s ≤ n is the
number of distinct symbols of Σ occurring in S, S(s) is the number of bits used by a
dynamic data structure to search over s elements in Σ plus to refer to s elements in
Σ, T (s) is the worst-case time to search for an element among s of them in Σ, and
U(s) is the amortized time to insert/delete symbols of Σ in the structure.
Theorem 10.5. A dynamic string S[1, n] over a general alphabet Σ can be stored
in a structure using nH0(S) + o(n lg s) + O(s lg n) + S(s) bits and supporting queries
access and select in time O(T (s) + lg n/ lg lg n) and rank in time O(T (s) + lg n).
Insertions and deletions of symbols are supported in O(U(s) + lg n) time. Here s ≤ n
is the number of distinct symbols of Σ occurring in S, S(s) is the number of bits used
by a dynamic data structure to search over s elements in Σ plus to refer to s elements
in Σ, T (s) is the time to search for an element among s of them in Σ, and U(s) is
the time to insert/delete symbols of Σ in the structure. All times are worst case

For example, a sequence of arbitrary real numbers can be handled with a balanced
search tree for the alphabet data structure that adds O(lg s) time to the operations.
A large integer range Σ = [1..|Σ|], instead, can be handled with a predecessor data
structure that adds O((lg lg |Σ|)2) to the times, and O(s lg |Σ|) further bits. When
the identity of the symbols is not important, one can handle a contiguous alphabet
[1..s], and only insert new symbols s + 1. In this case there is no penalty for letting
the alphabet grow dynamically.

∗

The only case where a previous solution with dynamic alphabet exists [27] is for
on an alphabet Γ. Here we can store the eﬀective set of strings in a
the case Σ = Γ
data structure by Franceschini and Grossi [21], so that operations involving a string a
take time O(|a|+ lg γ + lg n/ lg lg n), where γ is the number of symbols of Γ actually in
use. With Theorem 10.5 we obtain worst-case time O(|a| + lg γ + lg n). The previous
dynamic data structure [27] requires time O(|a| lg γ lg n) (although its space could be
lower than ours).

Handling general alphabets impacts on various dynamic representations that build
on access, rank, and select operations on strings, where alphabet dynamism is essential.
For example, it allows inserting/deleting both objects and labels in binary relations
[4], inserting/deleting nodes in graphs [18], inserting/deleting both rows and columns
in discrete grids [10], and inserting new words and deleting words from text collections
in positional [6] and nonpositional [3] inverted indexes [1].

11. Conclusions and further challenges. We have obtained O(lg n/ lg lg n)
time for all the operations that handle a dynamic sequence on an arbitrary alphabet
[1..σ], matching lower bounds that apply to binary alphabets [22], and using zeroorder 
compressed space. Our structure is faster than the best previous work [29, 49]
by a factor of Θ(lg σ/ lg lg n) when the alphabet is larger than polylogarithmic. The
query times are worst case, yet the update times are amortized. We also show that
it is possible to obtain worst case for all the operations, although times for rank and
updates rise to O(lg n). Finally, we show how to handle general and inﬁnite alphabets.
Our result can be applied to a number of problems and improve previous upper bounds
on them; we have described several cases.

The lower bounds [22] are valid also for amortized times, so our amortized-time
solution is optimal, yet our worst-case solution might not be. The main remaining
challenge is to determine whether it is possible to attain the optimal O(lg n/ lg lg n)
worst-case time for all the operations.

                                                                     Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php1804

GONZALO NAVARRO AND YAKOV NEKRICH

Another interesting challenge is to support a stronger set of update operations,
such as block edits, concatenations, and splits in the sequences. Navarro and Sadakane
[49] support those operations within time O(σ lg1+ε n). While it seems feasible to
achieve, in our structure, O(σ lg n) time by using blocks of Θ(lg2 n) bits, the main
hurdle is the diﬃculty of mimicking the same splits and concatenations on the list
maintenance data structures we use [33, 44].

REFERENCES

[1] R. Baeza-Yates and B. Ribeiro, Modern Information Retrieval, 2nd ed., Addison-Wesley,

New York, 2011.

[2] J. Barbay, F. Claude, T. Gagie, G. Navarro, and Y. Nekrich, Eﬃcient fully-compressed

sequence representations, Algorithmica, 69 (2014), pp. 232–268.

[3] J. Barbay, F. Claude, and G. Navarro, Compact binary relation representations with rich

functionality, Inform. and Comput., 232 (2013), pp. 19–37.

[4] J. Barbay, A. Golynski, I. Munro, and S. S. Rao, Adaptive searching in succinctly encoded 
binary relations and tree-structured documents, Theoret. Comput. Sci., 387 (2007),
pp. 284–297.

[5] J. Barbay, M. He, I. Munro, and S. S. Rao, Succinct indexes for strings, binary relations

and multi-labeled trees, ACM Trans. Algorithms, 7 (2011), 52.

[6] J. Barbay and G. Navarro, Compressed representations of permutations, and applications,
in Proceedings of the 26th International Symposium on Theoretical Aspects of Computer
Science (STACS), IBFI, Schloss Dagstuhl, Germany, 2009, pp. 111–122.

[7] D. Belazzougui and G. Navarro, New lower and upper bounds for representing sequences,
in Proceedings of the 20th Annual European Symposium on Algorithms (ESA), Lecture
Notes in Comput. Sci. 7501, Springer, Berlin, 2012, pp. 181–192.

[8] D. Blandford and G. Blelloch, Compact representations of ordered sets, in Proceedings
of the 15th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), SIAM,
Philadelphia, 2004, pp. 11–19.

[9] G. Blelloch, Space-eﬃcient dynamic orthogonal point location, segment intersection, and
range reporting, in Proceedings of the 19th Annual ACM-SIAM Symposium on Discrete
Algorithms, (SODA), SIAM, Philadelphia, 2008, pp. 894–903.

[10] P. Bose, M. He, A. Maheshwari, and P. Morin, Succinct orthogonal range search structures
on a grid with applications to text indexing, in Proceedings of the 11th International Symposium 
on Algorithms and Data Structures (WADS), Springer, Berlin, 2009, pp. 98–109.
[11] N. Brisaboa, A. Fari˜na, S. Ladra, and G. Navarro, Implicit indexing of natural language

text by reorganizing bytecodes, Inform. Retrieval, 15 (2012), pp. 527–557.

[12] M. Burrows and D. Wheeler, A block sorting lossless data compression algorithm, Technical

report 124, Digital Equipment Corporation, Maynard, MA, 1994.

[13] H. Chan, W.-K. Hon, T.-H. Lam, and K. Sadakane, Compressed indexes for dynamic text

collections, ACM Trans. Algorithms, 3 (2007), 21.

[14] H. Chan, W.-K. Hon, and T.-W. Lam, Compressed index for a dynamic collection of texts, in
Proceedings of the 15th Annual Symposium on Combinatorial Pattern Matching (CPM),
Lecture Notes in Comput. Sci. 3109, Springer, Berlin, 2004, pp. 445–456.

[15] B. Chazelle and L. Guibas, Fractional cascading: I. A data structuring technique, Algorithmica 
J. IFAC, 1 (1986), pp. 133–162.

[16] B. Chazelle and L. Guibas, Fractional cascading: II. Applications, Algorithmica J. IFAC,

1 (1986), pp. 163–191.

[17] D. Clark, Compact Pat Trees, PhD thesis, University of Waterloo, Waterloo, Canada, 1996.
[18] F. Claude and G. Navarro, Extended compact Web graph representations, in Algorithms and
Applications (Ukkonen Festschrift), Lecture Notes in Comput. Sci. 6060, Springer, Berlin,
2010, pp. 77–91.

[19] P. Ferragina, F. Luccio, G. Manzini, and S. Muthukrishnan, Compressing and indexing

labeled trees, with applications, J. ACM, 57 (2009), 4.

[20] P. Ferragina, G. Manzini, V. M¨akinen, and G. Navarro, Compressed representations of

sequences and full-text indexes, ACM Trans. Algorithms, 3 (2007), 20.

                                                                     Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpOPTIMAL DYNAMIC SEQUENCE REPRESENTATIONS

1805

[21] G. Franceschini and R. Grossi, A general technique for managing strings in comparisondriven 
data structures, in Proceedings of the 31st International Colloquium on Automata,
Languages and Programming (ICALP), Lecture Notes in Comput. Sci. 3142, Springer,
Berlin, 2004, pp. 606–617.

[22] M. Fredman and M. Saks, The cell probe complexity of dynamic data structures, in Proceedings 
of the 21st Annual ACM Symposium on Theory of Computing (STOC), ACM,
New York, 1989, pp. 345–354.

[23] A. Golynski, I. Munro, and S. S. Rao, Rank/select operations on large alphabets: A tool
for text indexing, in Proceedings of the 17th Annual ACM-SIAM Symposium on Discrete
Algorithms (SODA), SIAM, Philadelphia, 2006, pp. 368–373.

[24] R. Gonz´alez and G. Navarro, Improved dynamic rank-select entropy-bound structures, in
Proc. 8th Latin American Symposium on Theoretical Informatics (LATIN), Lecture Notes
in Comput. Sci. 4957, Springer, Berlin, 2008, pp. 374–386.

[25] R. Gonz´alez and G. Navarro, Rank/select on dynamic compressed sequences and applications,
 Theoret. Comput. Sci., 410 (2009), pp. 4414–4422.

[26] R. Grossi, A. Gupta, and J. Vitter, High-order entropy-compressed text indexes, in Proceedings 
of the 14th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), 2003,
SIAM, Philadelphia, pp. 841–850.

[27] R. Grossi and G. Ottaviano, The wavelet trie: maintaining an indexed sequence of strings in
compressed space, in Proceedings of the 31st ACM Symposium on Principles of Database
Systems (PODS), ACM, New York, 2012, pp. 203–214.

[28] A. Gupta, W.-K. Hon, R. Shah, and J. Vitter, A framework for dynamizing succinct data
structures, in Proceedings of the 34th International Colloquium on Automata, Languages
and Programming (ICALP), Lecture Notes in Comput. Sci. 4596, Springer, Berlin, 2007,
pp. 521–532.

[29] M. He and I. Munro, Succinct representations of dynamic strings, in Proceedings of the 17th
International Symposium on String Processing and Information Retrieval (SPIRE), Lecture
Notes in Comput. Sci. 6393, Springer, Berlin, 2010, pp. 334–346.

[30] W. -K. Hon, K. Sadakane, and W.-K. Sung, Succinct data structures for searchable partial
sums, in Proceedings of 14th Annual International Symposium on Algorithms and Computation 
(ISAAC), Lecture Notes in Comput. Sci. 2906, Springer, Berlin, 2003, pp. 505–516.
[31] W. -K. Hon, K. Sadakane, and W.-K. Sung, Breaking a time-and-space barrier in constructing 
full-text indices, SIAM Journal of Computing, 38 (2009), pp. 2162–2178.

[32] W. -K. Hon, K. Sadakane, and W.-K. Sung, Succinct data structures for searchable partial
sums with optimal worst-case performance, Theoret. Comput. Sci., 412 (2011), pp. 5176–
5186.

[33] H. Imai and T. Asano, Dynamic segment intersection search with applications, in Proceedings
of the 25th Symposium on Foundations of Computer Science (FOCS), IEEE, Los Angeles,
CA, 1984, pp. 393–402.

[34] J. K¨arkk¨ainen, Fast BWT in small space by blockwise suﬃx sorting, Theoret. Comput. Sci.,

387 (2007), pp. 249–257.

[35] J. K¨arkk¨ainen and S. J. Puglisi, Fixed block compression boosting in FM-indexes, in Proceedings 
of the 18th International Symposium on String Processing and Information Retrieval
(SPIRE), Lecture Notes in Comput. Sci. 7024, Springer, Berlin, 2011, pp. 174–184.

[36] S. Lee and K. Park, Dynamic rank-select structures with applications to run-length encoded
texts, in Proceedings of the 18th Annual Symposium on Combinatorial Pattern Matching
(CPM), Lecture Notes in Comput. Sci. 4580, Springer, Berlin, 2007, pp. 95–106.

[37] S. Lee and K. Park, Dynamic rank/select structures with applications to run-length encoded

texts, Theoret. Comput. Sci., 410 (2009), pp. 4402–4413.

[38] V. M¨akinen and G. Navarro, Dynamic entropy-compressed sequences and full-text indexes,
in Proceedings of the 17th Annual Symposium on Combinatorial Pattern Matching (CPM),
Lecture Notes in Comput. Sci. 4009, Springer, Berlin, 2006, pp. 307–318.

[39] V. M¨akinen and G. Navarro, Dynamic entropy-compressed sequences and full-text indexes,

ACM Trans. Algorithms, 4 (2008), 32.

[40] C. Makris, Wavelet trees: A survey, Comput. Sci. Inform. Systems, 9 (2012), pp. 585–625.
[41] G. Manzini, An analysis of the Burrows-Wheeler transform, J. ACM, 48 (2001), pp. 407–430.
[42] K. Mehlhorn, Data Structures and Algorithms: Sorting and Searching, Monogr. EATCS

Ser. 1, Theoret. Comput. Sci. Springer-Verlag, Berlin, 1984.

[43] K. Mehlhorn and S. N¨aher, Dynamic fractional cascading, Algorithmica J. IFAC, 5 (1990),

pp. 215–241.

                                                                     Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php1806

GONZALO NAVARRO AND YAKOV NEKRICH

[44] C. Mortensen, Fully-dynamic two dimensional orthogonal range and line segment intersection
reporting in logarithmic time, in Proceedings of the 14th Annual ACM-SIAM Symposium
on Discrete Algorithms (SODA), SIAM, Philadelphia, 2003, pp. 618–627.

[45] I. Munro, An implicit data structure supporting insertion, deletion, and search in O(log n)

time, J. Comput. System Sci., 33 (1986), pp. 66–74.

[46] I. Munro, Tables, in Proceedings of the 16th Conference on Foundations of Software Technology 
and Theoretical Computer Science (FSTTCS), Lecture Notes in Comput. Sci. 1180,
Springer, Berlin, 1996, pp. 37–42.

[47] G. Navarro, Wavelet trees for all, J. Discrete Algorithms, 25 (2014), pp. 2–20.
[48] G. Navarro and V. M¨akinen, Compressed full-text indexes, ACM Comput. Surveys, 39

(2007), 2.

[49] G. Navarro and K. Sadakane, Fully-functional static and dynamic succinct trees, ACM

Trans. Algorithms, 10 (2014), 16.

[50] Y. Nekrich, A dynamic stabbing-max data structure with sub-logarithmic query time, in Proceedings 
of the 22nd International Symposium on Algorithms and Computation, (ISAAC),
Lecture Notes in Comput. Sci. 7074, Springer, Berlin, 2011, pp. 170–179.

[51] D. Okanohara and K. Sadakane, A linear-time Burrows-Wheeler transform using induced
sorting, in Proceedings of the 16th International Symposium on String Processing and
Information Retrieval (SPIRE), Lecture Notes in Comput. Sci. 5721, Springer, Berlin,
2009, pp. 90–101.

[52] M. Patrascu, Lower bounds for 2-dimensional range counting, in Proceedings of the 39th
Annual ACM Symposium on Theory of Computing (STOC), ACM, New York, 2007,
pp. 40–46.

[53] R. Raman, V. Raman, and S. S. Rao, Succinct indexable dictionaries with applications to

encoding k-ary trees, preﬁx sums and multisets, ACM Trans. Algorithms, 3 (2007), 8.

[54] R. Raman and S. S. Rao, Succinct dynamic dictionaries and trees, in Proceedings of the 30th
International Colloquium on Automata, Languages and Computation (ICALP), Lecture
Notes in Comput. Sci. 2719, Springer, Berlin, 2003, pp. 357–368.

[55] N. V¨alim¨aki and V. M¨akinen, Space-eﬃcient algorithms for document retrieval, in Proceedings 
of the 18th Annual Symposium on Combinatorial Pattern Matching (CPM), Lecture
Notes in Comput. Sci. 4580, Springer, Berlin, 2007, pp. 205–215.

                                                                     Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php