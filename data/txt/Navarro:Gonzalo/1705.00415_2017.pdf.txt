7
1
0
2

 

y
a
M
1

 

 
 
]
S
D
.
s
c
[
 
 

1
v
5
1
4
0
0

.

5
0
7
1
:
v
i
X
r
a

Parallel Construction of Compact Planar
Embeddings∗
Leo Ferres1, José Fuentes-Sepúlveda2, Travis Gagie3, Meng He4,
and Gonzalo Navarro2

1

3

3

Faculty of Engineering, Universidad del Desarrollo, Chile
lferres@udd.cl

2 Department of Computer Science, University of Chile, Chile

jfuentess@dcc.uchile.cl, gnavarro@dcc.uchile.cl
School of Computer Science and Telecommunications, Diego Portales
University, Chile
travis.gagie@mail.udp.cl
Faculty of Computer Science, Dalhousie University, Canada
mhe@cs.dal.ca

Abstract

The sheer sizes of modern datasets are forcing data-structure designers to consider seriously
both parallel construction and compactness. To achieve those goals we need to design a parallel 
algorithm with good scalability and with low memory consumption. An algorithm with
good scalability improves its performance when the number of available cores increases, and an
algorithm with low memory consumption uses memory proportional to the space used by the
dataset in uncompact form. In this work, we discuss the engineering of a parallel algorithm with
linear work and logarithmic span for the construction of the compact representation of planar
embeddings. We also provide an experimental study of our implementation and prove experimentally 
that it has good scalability and low memory consumption. Additionally, we describe
and test experimentally queries supported by the compact representation.

1998 ACM Subject Classiﬁcation E.4 Coding and Information Theory

Keywords and phrases planar graph, multicore algorithm, compact data structure

Digital Object Identiﬁer 10.4230/LIPIcs.CVIT.2016.23

Introduction

1
Planar embeddings are present in several applications that need an underlying representation
of topological information, such as in the mesh representation in ﬁnite-element simulations,
road networks and Geographical Information Systems (GIS) in general. Because of their very
nature, the size of such planar embeddings is large and dynamic, meaning it is constantly
growing. For example, the underlying planar embedding to represent OpenStreetMap, has

∗ The second and ﬁfth authors received travel funding from EU grant H2020-MSCA-RISE-2015 BIRDS
GA No. 690941. The second author received funding from Conicyt Fondecyt 3170534. The third and
ﬁfth authors received funding Basal Funds FB0001, Conicyt, Chile. The third author received funding
from Academy of Finland grant 268324. Early parts of this work were done while the third author was
at the University of Helsinki and while the third and ﬁfth authors were visiting the University of A
Coruña.

© Leo Ferres, José Fuentes-Sepúlveda, Travis Gagie, Meng He and Gonzalo Navarro;
licensed under Creative Commons License CC-BY

42nd Conference on Very Important Topics (CVIT 2016).
Editors: John Q. Open and Joan R. Acces; Article No. 23; pp. 23:1–23:13

Leibniz International Proceedings in Informatics
Schloss Dagstuhl – Leibniz-Zentrum für Informatik, Dagstuhl Publishing, Germany

23:2

Parallel Construction of Compact Planar Embeddings

more than 3 billion nodes1. Manipulating those large planar embeddings, storing and
updating them eﬃciently is of practical importance. Having a data structure with a small
memory footprint to represent this class of graphs will help us manipulate huge graphs in
small devices with little (on-board, fast) memory.

Since the 1990s, compact data structures have become a viable alternative to represent
large data in a small space, storing data in space close to its information-theoretic lower bound
while still supporting queries eﬃciently. Their usefulness has been practically demonstrated
in several libraries, such as, LibCDS [4], SDSL [11], Sux [21], Dynamic [17] and Succinct
[16]. However, there has been no attempt to implement and test the practical eﬃciency of
compact data structures for planar graphs with planar embeddings.

The construction stage of compact data structures is of particular interest, since fast
construction algorithm can simulate real-time updates, eﬀectively making it behave as
a dynamic version of it.
In this work, we take advantage of multicore architectures to
provide a fast implementation of a algorithm to construct compact data structures for planar
embeddings.

Recently, Ferres et al. [9] extended the compact representation of planar embeddings
of Turán [20]. The extension supports navigation queries, without greatly increasing the
complexity introduced in the original. In the same work, Ferres et al. also introduced a workoptimal 
parallel algorithm with logarithmic depth for the extended compact representation.
In our work, we provide the algorithm engineering of the parallel algorithm of Ferres et al.
We discuss the implementation of each used parallel algorithm, such us, Euler Tour and
spanning tree, and discuss some practical trade-oﬀs. We provide a set of experiments to
prove the scalabitily and good space-usage of our implementation, using a small portion
of the original input. Finally, we also provide implementations of useful queries that also
behave eﬃciently.

The layout of the rest of the paper is as follows: in Section 2 we summarize the compact
representation (for more details, we refer the reader to [9]); in Section 3 we describe the
parallel algorithm of [9] and discuss the details of its implementation; in Section 4 we describe
our experiments for construction and query, and present their results; ﬁnally, in Secion 5, we
present our conclusions and future work.

Representation

2
In [9], we introduced an extension of the compact representation of Turán [20] for connected
planar multi-graph with n vertices and m edges is introduced. The extended representation
works as follows: First, we chose an arbitrary spanning tree T of the planar graph, rooted at
a vertex on the outer face. Second, by traversing the spanning tree in DFS order, we create
three binary sequences A, B and B∗. The ﬁrst time we traverse an edge of T, we write a 0 in
the sequence B, otherwise we write a 1 in B. Similarly, the ﬁrst time that we reach an edge
that does not belong to T, we write a 0 in B∗, otherwise we write a 1 in B∗. Finally, each
time that we traverse an edge of T, we write a 1 in A, otherwise we write a 0. The sequence
B has length 2n − 2 and corresponds to the balanced-parentheses representation of T. The
sequence B∗ has length 2(m − n + 1) and corresponds to reversed balanced-parentheses
representation of the complementary spanning tree of the dual of the graph. The sequence A

1 In OpenStreetMap, a node is deﬁned as a speciﬁc point on the earth’s surface deﬁned by its latitude
and longitude. Updated statistics are available at http://www.openstreetmap.org/stats/data_stats.
html (last access: April 06, 2017)

J. Q. Open and J. R. Access

23:3

Figure 1 Left: A planar embedding of a planar graph G, with a spanning tree T of G shown in red
and the complementary spanning tree T ∗ of the dual of G shown in blue. Right: The two spanning
trees, with T rooted at the vertex 1 on the outer face and T ∗ rooted at the vertex A corresponding
to the outer face. We can represent this embedding of G with the three bitvectors A[1..28] =
0110110101110010110100010100, B[1..14] = 00101100110011 and B∗[1..14] = 01001001110101.

has length 2m and indicates how the sequences B and B∗ are interleaved during the traversal.
See Figure 1 as an example of the construction of A, B and B∗.

We can add sublinear number of bits to each balanced-parentheses representation, we
can support fast navigation in the trees, and by storing the sequence A as a bitvector,
we can support fast navigation in the graph. In particular, for the balanced-parentheses
representation we are interested on the constant time queries match and parent, where
match(i) returns the position of the parenthesis matching the ith parenthesis and parent(v)
returns the parent of v, given as its pre-order rank in the traversal, or 0 if v is the root of
its tree. For the binary sequence A we are interested in the constant time queries rank and
select, where rankb(i) returns the number of bits set to b in the preﬁx of length ‘ of the A
and selectb(j) returns the position of the jth bit set to b. For more details on bitvectors and
balanced-parentheses representations, we refer the reader to Navarro’s text [15]. Based on
the previous queries, we deﬁned the following basic queries:
ﬁrst(v): return i such that the ﬁrst edge we process while visiting v is the ith we process

during our traversal;

next(i): return j such that if we are visiting v when we process the ith edge during our
traversal, then the next edge incident to v in counterclockwise order is the one we process
jth;

mate(i): return j such that we process the same edge ith and jth during our traversal;
vertex(i): return the vertex v such that we are visiting v when we process the ith edge

The compact representation support these basic queries in constant time as follows:

during our traversal.

0

ﬁrst(v) =

next(i) =

(cid:26) A.select1(B.select0(v − 1)) + 1 if m ≥ 1
 i + 1

mate(i) + 1 if A[i] = 1 and B[A.rank1(i)] = 0
0

if A[i] = 0 and i < 2m

otherwise

otherwise

CVIT 2016

145678ABCDEF23GHHGCBEDFA3462517823:4

Parallel Construction of Compact Planar Embeddings

mate(i) =

vertex(i) =



(cid:26) A.select0(B∗.match(A.rank0(i))) if A[i] = 0

A.select1(B.match(A.rank1(i)))

otherwise

B.rank0(A.rank1(i)) + 1

if A[i] = 0 and B[A.rank1(i)] = 0

B.parent(B.rank0(B.match(A.rank1(i)))) + 1

if A[i] = 0 and B[A.rank1(i)] = 1
B.parent(B.rank0(A.rank1(i))) + 1
if A[i] = 1 and B[A.rank1(i)] = 0
B.rank0(B.match(A.rank1(i))) + 1

otherwise.

With those basic queries, it is possible to deﬁne more complex queries. As an example,
we present three queries based on the basic ones: counting(v), the number of neighbors of
vertex v; listing(v), the list of neighbors of vertex v, in counterclockwise order; face(e), the
list of vertices, in clockwise order, of the face where the edge e belongs. The implementation
of the three queries follows. For queries listing(v) and face(e), we report each vertex by using
the function print.

Function counting
Input : node v

1 d = 0
2 nxt = first(v)
3 while nxt < 2m do
nxt = next(nxt)
4
d = d + 1
5

Function listing
Input : node v
1 nxt = first(v)
2 while nxt < 2m do
mt = mate(nxt)
3
print(vertex(mt))
4
nxt = next(nxt)
5

Function face
Input : edge e

1 nxt = e, f lag = true
2 while nxt 6= e or f lag do
3
4
5
6

f lag = f alse
mt = mate(nxt)
print(vertex(mt))
nxt = next(nxt)

In the rest of this paper, we explain how to construct A, B and B∗ eﬃciently in parallel,

and demostrate experimentally that our representation is practical.

Parallel algorithm for compact planar embeddings

3
In this section we discuss the parallel construction of the compact representation of planar
embeddings. Since the compact representation is based on spanning trees and tree traversals,
we can borrow ideas of well-known parallel algorithms, such as Euler Tour traversal in parallel
or parallel computation of spanning trees.

For the analysis of our algorithm, we use the Dynamic Multithreading (DyM) Model [7].
In this model, a multithreaded computation is modelled as a directed acyclic graph (dag)
where vertices are instructions and an edge (u, v) represents precedence among instruction
u and v. The model is based on two parameters of the multithreaded computation:
its
work T1 and its span T∞. The work is the running time on a single thread, that is, the
number of nodes (i.e., instructions) in the dag, assuming each instruction takes constant
time. The span is the length of the longest path in the dag; the intrinsically sequential
part of the computation. The time Tp needed to execute the computation on p threads has
complexity Θ(T1/p + T∞)), which can be reached with a greedy scheduler. The improvement
of a multithreaded computation using p threads is called speedup, T1/Tp. The upper bound
on the achievable speedup, T1/T∞, is called parallelism. Finally, the eﬃciency is deﬁned as

J. Q. Open and J. R. Access

23:5

T1/pTp and can be interpreted as the percentage of improvenment achieved by using p cores
or how close we are to the linear speedup. In the DyM model, the workload of the threads is
balanced by using the work-stealing algorithm [3].

To describe parallel algorithms in the DyM model, we augment sequential pseudocode
with three keywords. The spawn keyword, followed by a procedure call, indicates that the
procedure should run in its own thread and may thus be executed in parallel to the thread
that spawned it. The sync keyword indicates that the current thread must wait for the
termination of all threads it has spawned. Finally, parfor is “syntactic sugar” for spawning
one thread per iteration in a for loop, thereby allowing these iterations to run in parallel,
followed by a sync operation that waits for all iterations to complete. In practice, the parfor
keyword is implemented by halving the range of loop iterations, spawning one half and using
the current procedure to process the other half recursively until reaching one iteration per
range. After that, the iterations are executed in parallel. Therefore, this implementation
adds an overhead bounded above the logarithm of the number of loop iterations. In our
algorithm, we include such overhead in our complexities.

For the rest of this section, each tree T is represented with an adjacency list representation.
Such representation consists of an array of nodes of size n, VT , and an array of edges of
size m, ET . Each node v ∈ VT stores two indices in ET , v.f irst and v.last, indicating the
adjacency list of v, sorted counterclockwise around v and starting with v’s parent edge
(except the root). Notice that the number of children of v is (v.last − v.f irst). Each edge
e ∈ ET has three ﬁelds, e.src, which is a pointer to the source vertex, e.tgt, which is a pointer
to the target vertex and e.cmp, which is the position in ET of the complement edge of e,
.tgt = e.src. For x ∈ {e.src, e.tgt}, we use next(x) and
0, where the e
e
ﬁrst(x) to denote the indices in EG of e’s successor and of the ﬁrst element (parent edge)
in x’s adjacency list, respectively. Both are easily computed in constant time by following
pointers. Notice that |ET| = 2(|VT| − 1). The representation of graphs is similar, with the
exception that the concept of parent of a vertex is not valid in graphs, therefore the ﬁrst
edge in the adjacency list of a vertex v cannot be interpreted as the v’s parent edge.

.src = e.tgt and e

0

0

3.1 Parallel construction of compact planar embeddings
Given a planar embedding of a connected planar graph G = (VG, EG), for the moment we
assume a spanning tree of T = (VT , ET ) of G and an array C that stores the number of
edges of G \ T between two consecutive edges in T, in counterclockwise order, are given as
part of input. Later, in Section 3.2, we explain how to obtain the spanning tree T and the
array C in parallel. With the spanning tree, we construct the bitvectors A, B and B∗ by
performing an Euler Tour over T. During tour, by writing a 0 for each forward (parent to
child) edge and a 1 for each backward (child to parent) edge, we obtain the bitvector B,
by counting the number of edges of G \ T between two consecutive edges of T (array C),
representing them by 0’s and the edges of T by 1’s, we obtain the bitvector A, and by using
the previous Euler Tour and the array C we can obtain the bitvector B∗. Algorithm 1 shows
this idea in more details. The algorithm works in six steps: In the ﬁrst step, the algorithm
creates an auxiliar array LE (line 4) that is used to store the traversal of the tree following
the Euler Tour. Each entry of LE represents one traversed edge of T and stores four ﬁelds:
value is 0 or 1 depending on whether the edge is a forward or a backward edge, respectively;
succ is the index in LE of the next edge in the Euler tour; rankA is the rank of the edge in
A; and rankB is the rank of the edge in B.
In the second step, the algorithm traverses T (lines 6–22). For each edge ej ∈ ET , rankA
is set as C[ET [j].cmp] + 1 and rankB as 1 (lines 9–10). Those ranks will be used later to

CVIT 2016

23:6

Parallel Construction of Compact Planar Embeddings

Algorithm 1: Parallel compact planar embedding algorithm (par-spe).
Input
Output : Bitvectors A, B and B∗ induced by G and T .

: A planar embedding of a planar graph G = (VG, EG), a spanning tree T = (VT , ET ) of G,
an array C of size |ET|, the starting vertex init and the number of threads, threads.

j = t ∗ chk + i
LE[j].rankA = C[ET [j].cmp] + 1
LE[j].rankB = 1
if ET [j].src == init OR ﬁrst(ET [j].src) 6= j then // forward edge

for i = 0 to chk − 1 do

1 A = a bitvector of length |EG|
2 B = a bitvector of length |ET| − 2
3 B∗ = a bitvector of length |EG| − |ET| + 2
4 LE = an array of length |ET|
5 chk = |ET|/threads
6 parfor t = 0 to threads − 1 do
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22

LE[j].succ = ﬁrst(ET [j].tgt)
LE[j].succ = next(ET [j].tgt)

LE[j].value = 0 // opening parenthesis
if ET [j].tgt is a leaf then
LE[j].succ = ET [j].cmp
else
LE[j].succ = ﬁrst(ET [j].tgt) + 1

else // backward edge

LE[j].value = 1 // closing parenthesis
if ET [j] is the last edge in the adjacency list of ET [j].src then
else

j = t ∗ chk + i
A[LE[j].rankA] = 1 // By default, all elements of A are 0’s
B[LE[j].rankB] = LE[j].value

for i = 0 to chk − 1 do

j = t ∗ chk + i
pos = LE[j].rankA − LE[j].rankB
lim = ref (ET [j].cmp) + C[ET [j].cmp]
for k = ref (ET [j].cmp) + 1 to lim do

23 parallelListRanking(LE)
24 parfor t = 0 to threads − 1 do
for i = 0 to chk − 1 do
25
26
27
28
29 Dpos, Dedge = two arrays of length |EG| − |ET| + 2
30 parfor t = 0 to threads − 1 do
31
32
33
34
35
36
37
38
39 chk = (|EG| − |ET| + 2)/threads
40 parfor t = 0 to threads − 1 do
41
42
43
44
45
46 createRankSelect(A), createBP(B), createBP(B∗)

j = t ∗ chk + i
cmp = ET [Dedge[j]].cmp
if j > Dpos[cmp] then

Dpos[k] = pos
Dedge[pos] = k
pos = pos + 1

for i = 0 to chk − 1 do

B∗[j + 1] = 1 // By default, all elements of B∗ are 0’s

J. Q. Open and J. R. Access

23:7

compute the ﬁnal position of the edges in A, B and B∗. For each forward edge, a 0 is written
in the corresponding value ﬁeld and the succ ﬁeld is connected to the next edge in the Euler
Tour. For backward edges is similar. Considering the adjacency list representation of T, all
the edges in the adjacency list of a node (except the root) of T are forward edges, except the
ﬁrst one (parent edge). For the root, all the edges of its adjacency list are forward edges.

In the third step, the algorithm computes the ﬁnal ranks in A and B using a parallel list
ranking algorithm (line 23). We use the algorithm introduced in [12] over the rankA and
rankB ﬁelds of LE to obtain the ﬁnal position of each edge in A and B, respectively.

In the fourth step, bitvectors A and B are written. If initially all the elements of A are
0’s, it is enough to set to 1 all the elements given by the ﬁelds rankA’s. For B, the algorithm
copies the content of ﬁeld value at position rankB, for all the elements in LE.

In the ﬁfth step, the algorithm computes the position of each edge of G \ T in B∗. That
information is implicit in the ﬁelds rankA and rankB of LE (line 33), after the list ranking
of the third step. For each edge ej ∈ ET , the algorithm computes the positions, in B∗, of
the edges in G \ T that follow, in counterclockwise order, the complement edge of ej (lines
34–38). The algorithm uses two auxiliar arrays, Dpos and Dedge. The entry Dpos[j] stores
the position of the edge ej of G \ T in B∗. The array Dedge is the inverse of Dpos. It stores
the position of the j-th edge of B∗ in G \ T. Thus, Dpos[i] = j iﬀ Dedge[j] = i. In this step,
the function ref (ET [j]) returns the position of the edge ej of ET in EG.

In the sixth step, the algorithm computes if the edges stores in Dpos are forward or
backward edges. For each edge e in G \ T, it is done by comparing the position in B∗ of e
and its complement. If the position of e is greater than the position of its complement, then
e is a backward edge and, therefore, represented by a 1. By default, we assume that all the
elements of B∗ are 0’s.

Finally, the structures to support operations rank, select, match and parent are constructed.
 For the bitvector A, Algorithm 1 uses the parallel algorithms of Labeit et al.
[13]
(createRankSelect). In the case of B and B∗, the algorithm uses the parallel algorithm of
Ferres et al. [10] (createBP) for balanced parenthesis sequences.

Now we present the analysis of our algorithm. In the ﬁrst step there is not computation
involved, therefore, we do not include it in the complexity. In the second step, the algorithm
traverse the edges of T, performing an independent computation in each edge, therefore, with
the overhead of the parfor loop, we obtain T1 = O(n) and T∞ = O(lg n) time. The third step
uses the algorithm of Helman and JáJá for the parallel list ranking problem over m elements,
with complexities T1 = O(n) and T∞ = O(lg n) time. In the fourth step, the assignation of
the values to A and B can be done independently for each entry of the bitvectors. With the
overhead of the parallel loop, we have T1 = O(n) and T∞ = O(lg n) time. In the ﬁfth step,
the algorithm traverses all the edges in G \ T. Observe that the range of the loop in line 35
can be processed in parallel using a domain decomposition technique. With that, we obtain
T1 = O(m− n) and T∞ = O(lg(m− n)) time. We decide to implement it as it appears in line
35, because we obtained good practical results. If we need to process the line 35 in parallel,
we can use the same domain decomposition technique of lines 30–32. Similar to the fourth
step, in the sixth step the algorithm sets the entries of the bitvector B∗, which can be done
independently for each entry. Therefore, T1 = O(m − n) and T∞ = O(lg(m − n)) time.

The rank/select structures can be constructed in T1 = O(m) and T∞ = O(lg m) time
by using the results of [13]. A structure that supports match and parent operations over a
balanced parentheses sequence can be constructed in T1 = O(m) and T∞ = O(lg m) time
with the results of [10].

In addition to the size of the compact data structure, the memory consumption of

CVIT 2016

23:8

Parallel Construction of Compact Planar Embeddings

our algorithm depends on the size of arrays LE, Dpos and Dedge. The array LE uses
O(n lg n) bits, and arrays Dpos and Dedge uses O((m − n) lg n) bits. Thus, the total memory
consumption of our algorithm is O(m lg n) bits plus the output data structure. Notice that
the memory comsumption is independent of the number of threads.

In summary, we have the following theorem.

(cid:73) Theorem 1. Given a planar embedding of a connected planar graph G = (VG, EG) with
m edges and a spanning tree of G, we can compute in parallel compact representation of G,
using 4m + o(m) bits and supporting navegational operations, in O(m) work, O(lg m) span,
O(m/p + lg m) time, using O(m lg n) bits of additional memory, where p is the number of
available threads.

3.2 Parallel computation of spanning trees
In this section we discuss the parallel computation of the spanning tree T = (VT , ET ) and
the array C used in Section 3.1.

The work of Bader and Cong [2] can be used to compute a spanning tree of a planar graph.
Their algorithm works as follows: Given a starting vertex of the graph G with n vertices
and m edges, the algorithm computes sequentially a spanning tree of size O(p), called stub
spanning tree, where p is the number of available threads. Then, an evenly number of leaves
of the stub spanning tree are assigned to the p threads as starting vertices. Each thread
traverses G, using its starting vertices, constructing spanning trees with a DFS traversal
using a stack. For each vertex, a reference to its parent is assigned. Since a vertex can be
visited by several threads, the assigment of the parent of the vertex may genarate a race
condition. However, since the parent assigned by any thread already belongs to a spanning
tree, any assignation will generate a correct tree. Thus, the race condition is a benign race
condition. Once a thread has no more vertices on its stack, the thread tries to steal vertices
from the stack of other thread by using the work stealing algorithm. Since the spanning
trees generated by all the threads are connected to the stub spanning tree, the union of
all the spanning tree generates a spanning tree of G. Thus, the algorithm gives an array
of parent references for each vertex. With such array of references, we can construct the
corresponding adjacency list representation of the spanning tree. To do that, we mark with
a 1 each edge EG that belongs to ET and with 0 the rest of the edges. Using a parallel
preﬁx sum algorithm over EG, we compute the position of all the marked edges of EG in
ET . The ﬁrst and last ﬁelds of each node in the spanning tree are computed similarly. As a
byproduct of the computation of ET , we can compute an array C which stores the number
of edges of G \ T between two consecutive edges in T, in counterclockwise order. It can be
done by using the marks in the edges, counting the number of 0’s between two consecutive
1’s. Notice the starting vertex for the stub spanning tree must be in the outer face of G, to
meet the description of the compact data structure for planar embeddings.

The complexities of the spanning tree algorithm depends on both the random traversal
of the threads and the diameter of G: T1 = O(m + n) work, T∞ = O(m + n) span, since the
stub spanning tree is computed sequentially and its size is proportional to the number of
threads, and Tp = O((m + n)/p) expected time for general random graphs, for p (cid:28) n. This
algorithm has a worst case when G has diameter of O(m) and low connectivity. In that case,
the expected time is Tp = (m + n). Despite its span and worst case, the algorithm of Bader
and Cong has a good practical behavior and its implementation is simple. The adjancency
list representation of the spanning tree T and the array C are computed by using a parallel
preﬁx sum algorithm, which is well-known to have T1 = O(m) and T∞ = O(lg m) time.

J. Q. Open and J. R. Access

23:9

Since the spanning tree of G is part of the input of our algorithm for planar embeddings,
we decide to explore diﬀerent parallel models to overcome the worst case of Bader and
Cong’s algorithm, and thus improve the overall complexity of our algorithm. The algorithm
introduced in this manuscript is simple enough to be adapted to other parallel models,
because it is based on Euler Tour and list ranking algorithm, and parallel ﬁlling of arrays,
which are well-known algorithms in most of the parallel models. In particular, we can adapt
our parallel algorithm to the CRCW PRAM model, since we can use the CRCW PRAM
Euler Tour algorithm of [6], with O(lg n) parallel time using O(m) cores, and the CRCW
PRAM list ranking algorithm of [5], with O(lg n) parallel time using O(n/ lg n) cores. For
spanning trees, we have the algorithms of [1] and [18] in the CRCW PRAM model. The
algorithm in [1] takes O(lg n) parallel time using O(m) cores, and the algorithm in [18] takes
O(lg n) parallel time using n + 2m cores.

Thus, in the CRCW PRAM model, our algorithm for compact planar embeddings reaches
logarithmic parallel time, including the computation of the spanning tree. The array C can
be constructed using the strategy explained before, based on parallel preﬁx sum.

Despite its the worst case, we used the spanning tree algorithm of Bader and Cong in our

implementation and experiments, since it showed good practical results.

Experiments

4
We implemented the par-spe algorithm in C and compiled it using GCC 5.4 with Cilk Plus
extension, an implementation of the DyM model.2 In our implementation of the parallel
spanning tree algorithm of Bader and Cong, to reduce the worst case, we included a treshold
of O(n/p) elements in the stack size of each thread. Each time that a thread has more
nodes that the threshold, that thread create a new parallel task with the half of its stack.
Additionally, we also return for each node the reference to its parent. Returning the references
to parents gives better performance than forcing the ﬁrst edge of each node to be the reference
to its parent. However, in the description of the par-spe algorithm we assume that the ﬁrst
edge of each node is the reference to its parent to make it more readable. Additionally, we
implemented a sequential algorithm called seq, which corresponds to the serialization of
the par-spe algorithm and a sequential DFS algorithm to compute the spanning tree. To
serialize a parallel algorithm in the DyM model, we replaced each parfor keyword for the
for keyword and deleted the spawn and sync keywords.

The experimental trials consisted in running the implementation on artiﬁcial datasets of
diﬀerent number of nodes and threads. The datasets are shown in Table 1. Each dataset
was generated in three stages: In the ﬁrst stage, we used the function rnorm of R to generate
random coordinates (x, y)3. The only exception was the dataset wc, which corresponds to the
coordinates of 2, 243, 467 uniques cities in the world.4 In the second stage, we generated the
Delaunay Triangulation of the coordinates generated in the ﬁrst stage. The triangulations

~jfuentess/pemb/.

2 The code and data needed to replicate our results are available at http://www.dcc.uchile.cl/

3 The rnorm function generates random numbers for the normal distribution given a mean and a standard
deviation. In our case, the x and y components was generated using mean 0 and standard deviation
10000. For more information about the rnorm function, please visit https://stat.ethz.ch/R-manual/
R-devel/library/stats/html/Normal.html

4 The dataset containing the coordinates was created by MaxMind, available from https://www.maxmind.
com/en/free-world-cities-database. The original dataset contains 3, 173, 959 cities, but some of
them have the same coordinates. We selected the 2, 243, 467 cities with unique coordinates to build our
dataset worldcities.

CVIT 2016

23:10

Parallel Construction of Compact Planar Embeddings

Dataset Vertices (n) Edges (m)
6,730,395
14,999,983
29,999,979
44,999,983
59,999,975
74,999,979

1 wc
2
3
4
5
6
Table 1 Datasets used in the experiments of

2,243,467
5,000,000
10,000,000
15,000,000
20,000,000
25,000,000

pe5M
pe10M
pe15M
pe20M
pe25M

the par-spe algorithm.

wc pe5M pe10M pe15M pe20M pe25M
p
seq 5.56 13.55 27.86 42.34 57.04 71.76
7.08 17.14 35.63 54.21 73.11 92.17
1
4.06 9.71 20.26 30.67 41.37 52.16
2
2.12 4.98 10.31 15.75 21.17 26.70
4
8
1.13 2.65
8.25 10.81 13.61
9.46
5.61
.78 1.84
12
7.28
4.32
.60 1.40
16
20
.49 1.15
3.57
6.02
5.10
3.15
.41
24
.97
5.36
3.04
.42 1.01
28
4.80
2.82
.90
.38
32
36
.34
.83
2.57
4.38
4.04
2.41
.76
.33
40
3.72
2.23
.71
.31
44
3.56
2.09
.68
.30
48
52
.29
.65
2.01
3.36
3.35
1.90
.61
.31
56

5.36
3.74
2.84
2.33
2.01
2.05
1.85
1.72
1.58
1.47
1.37
1.33
1.24

7.50
5.74
4.65
4.11
3.88
3.77
3.48
3.22
3.01
2.76
2.67
2.54

Figure 2 Speedup of the par-spe algorithm.

Table 2 Running times of par-spe algorithm

in seconds.

were generated using Triangle, a piece of software dedicated to the generation of meshes
and triangulations5. In the ﬁnal stage, we generated planar embeddings of the Delaunay
triangulations computed in the second stage. The planar embedding was generated with the
The Edge Addition Planarity Suite6. The minimum and maximum degree of the dataset wc
was 3 and 36, respectively. For the rest of the datasets, the minimum degree was 3 and the
maximum degree was 16. We repeated each trial ﬁve times and recorded the median time.
The experiments were carried out on a NUMA machine with two NUMA nodes. Each
NUMA node includes a 14-core Intel® Xeon® CPU (E5-2695) processor clocked at 2.3GHz.
The machine runs Linux 2.6.32-642.el6.x86_64, in 64-bit mode. The machine has per-core L1
and L2 caches of sizes 64KB and 256KB, respectively and a per-processor shared L3 cache of
35MB, with a 768GB DDR3 RAM memory (384GB per NUMA node), clocked at 1867MHz.
Hyperthreading was enabled, giving a total of 28 logical cores per NUMA node.

Table 2 shows the running times obtained in our experiments, and Figure 2 shows the
speedups compared with the seq algorithm. On average, the seq algorithm took about 76%
of the time obtained by the par-spe algorithm running with 1 thread. With p ≥ 2, the
par-spe algorithm shows better times than the seq algorithm. We observe an almost linear
speedup up to p = 24, with an eﬃciency of at least 56%, considering all the datasets. With
p = 28 the speedup has a slowdown, due to the topology of our machine. Up to 24 cores,
all the threads were running in the same NUMA node. With p ≥ 28, both NUMA nodes

5 The software is available at http://www.cs.cmu.edu/~quake/triangle.html. Our triangulations were

generated using the options -cezCBVPNE.

6 The suite is available at https://github.com/graph-algorithms/edge-addition-planarity-suite.

Our embeddings were generated using the options -s -q -p.

Number of threadsSpeedup1481216202428323640444852560246810121416182022llwcpe5Mpe10Mpe15Mpe20Mpe25MllllllllllllllllllllllllllllllllJ. Q. Open and J. R. Access

23:11

Figure 3 Memory consumption of the parspe 
algorithm.

Figure 4 Median times of counting, listing

and face queries.

are used which implies higher communication costs. The communication costs intra NUMA
nodes are lower than the communication costs inter NUMA nodes [8]. In particular, the case
of p = 28 also uses both NUMA nodes, since at least one core on our machine was available
to OS processes. For p = 56, the wc dataset exhibits an eﬃciency of only 32% due to it is
the smaller dataset. For the rest of datasets, the lower eﬃciency was 38%.

Figure 3 shows the memory consumption of our algorithm. Speciﬁcally, the ﬁgure shows
the space used by each dataset in adjacency list representation (inputGraph), the peak of
consumption of our implementation (peakMem) and the size of the compact representation
of each dataset (compGraph). Compared with the space consumption of the adjacency list
representation, our implementation uses 36% more space and the compact representation
uses about 4.6% of it. The consumption per edge was 5 bits, which matches the Theorem 1.
Finally, we tested the three queries introduced in Section 2: counting, listing and
face. Observe that, given the adjacency list representation described in Section 3, to answer
counting and listing queries is straightforward. In our experiments, we tested counting
and listing 10 times for each vertex, and face 10 times per edge. Figure 4 shows the median
time per query, both for the adjacency list (al-counting, al-listing and al-face) and
compact representation (comp-counting, comp-listing and comp-face). The adjacency list
representation allows to answer counting and listing queries 100 and 80 times faster than
the compact representation. This result was expected, since the adjacency list representation
we assumed already has the list of neighbors in counterclockwise order. For the face query,
the adjacency list representation is only 14 times faster.

In summary, our parallel algorithm has good scalability to construct the compact representation 
of planar embeddings of [9]. In particular, using only one NUMA node, our
algorithm scales linearly. To answer queries, compact representations of data structures are
slower than their uncompacted counterparts. However, such compact representation use less
memory, allowing to ﬁt data structures close to fast memories, such as main memory and
caches, speeduping up the overall computation for large datasets. In the particular case of
the face query, a query closer to what we expect when solving more realistic problems, our
compact implementation is 14 slower and uses 20 times less memory than the uncompacted
representation.

CVIT 2016

inputGraphpeakMemcompGraphDatasetsMemory consumption(MB)wcpe5Mpe10Mpe15Mpe20Mpe25M020050080011001400DatasetsTime log scale (microsecs)wcpe5Mpe10Mpe15Mpe20Mpe25M0.010.1110100100llal−countingcomp−countingal−listingcomp−listingal−facecomp−facellllllllllll23:12

Parallel Construction of Compact Planar Embeddings

Conclusions

5
In this paper, we presented the algorithm engineering of the parallel algorithm for the
construction of compact representations of planar embeddings introduced in [9]. We also
show empirically that our proposed implementation has good scalability in shared-memory
architectures. Finally, we tested three diﬀerent queries supported by our implementation and
show that they have good execution-time behavior, making them of practical importance.
Notice, interestingly, that the compact representation can be extended to unconnected
planar graphs. To do this, we ﬁrst need to ﬁnd all the connected components of the graph.
Then, we compute an arbitrary spanning tree for each connected component. Then, we
construct the binary sequences: the sequence B will represent the forest of the spanning
trees, concatenating all the balanced-parentheses representations; the sequence B∗ will
represent complementary spanning tree of the dual of the graph. Since the outer face of
all the connected components is the same, B∗ represents a tree and not a forest. Finally,
sequence A indicates the interleaving of the sequences B and B∗. For that, we arbitrarily
choose a connected component to start the traversal. For the computation of the connected
components, we can use the work of Shun et al. [19] which has good theoretical and practical
results.

As future work, we will compare our implementation of the Bader and Cong algorithm
with the algorithm for connected components of Shun et al. [19]. The algorithm of Shun et
al. computes the connected components of a graph by calling the partitioning algorithm of
Miller et al. [14]. During the partitioning step, the algorithm performs multiple BFS’s over
the graph. All the vertices in a partition are contracted to generate a new graph with less
edges. The process is recursively repeated until the contracted graph has not edges. We can
modify this algorithm in order to obtain the spanning tree by returning the edges traversed
in each BFS.

References

1 B. Awerbuch and Y. Shiloach. New connectivity and msf algorithms for shuﬄe-exchange
network and pram. IEEE Transactions on Computers, C-36(10):1258–1263, Oct 1987. doi:
10.1109/TC.1987.1676869.

2 David A. Bader and Guojing Cong. A fast, parallel spanning tree algorithm for symmetric
multiprocessors (SMPs). Journal of Parallel and Distributed Computing, 65(9):994–1006,
2005.

3 Robert D. Blumofe and Charles E. Leiserson. Scheduling multithreaded computations by

work stealing. Journal of the ACM, 46(5):720–748, 1999.
Francisco Claude. A compressed data structure library. https://github.com/fclaude/
libcds. Last accessed: April 07, 2017.

4

5 Richard Cole and Uzi Vishkin.

Faster optimal parallel preﬁx sums and list
ranking.
URL: http:
//www.sciencedirect.com/science/article/pii/0890540189900369, doi:http://dx.
doi.org/10.1016/0890-5401(89)90036-9.

Information and Computation, 81(3):334 – 352, 1989.

6 Guojin Cong and D. A. Bader. The euler tour technique and parallel rooted spanning tree.
In International Conference on Parallel Processing, 2004. ICPP 2004., pages 448–457 vol.1,
Aug 2004. doi:10.1109/ICPP.2004.1327954.

7 T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein. Multithreaded algorithms. In

Introduction to Algorithms, pages 772–812. The MIT Press, 3rd edition, 2009.

8 Ulrich Drepper. What every programmer should know about memory, 2007. URL: http:

//people.redhat.com/drepper/cpumemory.pdf.

J. Q. Open and J. R. Access

23:13

9

10

11

Leo Ferres, José Fuentes, Travis Gagie, Meng He, and Gonzalo Navarro. Fast and compact
planar embeddings. In Workshop on Algorithms and Data Structures (WADS), 2017. To
appear.
Leo Ferres, José Fuentes-Sepúlveda, Meng He, and Norbert Zeh. Parallel construction of
succinct trees. In Proceedings of the 14th Symposium on Experimental Algorithms (SEA),
pages 3–14, 2015.
Simon Gog.
sdsl-lite. Last accessed: April 07, 2017.

Succinct data structure library 2.0.

https://github.com/simongog/

13

12 David R. Helman and Joseph JáJá. Preﬁx computations on symmetric multiprocessors.

Journal of Parallel and Distributed Computing, 61(2):265–278, 2001.
Julian Labeit, Julian Shun, and Guy E. Blelloch. Parallel lightweight wavelet tree, suﬃx
array and FM-index construction.
In Proceedings of the Data Compression Conference
(DCC), pages 33–42, 2016.

14 Gary L. Miller, Richard Peng, and Shen Chen Xu. Parallel graph decompositions using
random shifts. In Proceedings of the Twenty-ﬁfth Annual ACM Symposium on Parallelism
in Algorithms and Architectures, SPAA ’13, pages 196–203, New York, NY, USA, 2013.
ACM. URL: http://doi.acm.org/10.1145/2486159.2486180, doi:10.1145/2486159.
2486180.

15 Gonzalo Navarro. Compact Data Structures: A Practical Approach. Cambridge University

Press, 2016.

07, 2017.

16 Giuseppe Ottaviano. Succinct. https://github.com/ot/succinct. Last accessed: April

17 Nicola Prezza. Dynamic: a succinct and compressed dynamic data structures library. https:

//github.com/nicolaprezza/DYNAMIC. Last accessed: April 07, 2017.

19

18 Yossi Shiloach and Uzi Vishkin. An o(logn) parallel connectivity algorithm. Journal of
Algorithms, 3(1):57 – 67, 1982. URL: http://www.sciencedirect.com/science/article/
pii/0196677482900086, doi:http://dx.doi.org/10.1016/0196-6774(82)90008-6.
Julian Shun, Laxman Dhulipala, and Guy Blelloch. A simple and practical linear-work parallel 
algorithm for connectivity. In Proceedings of the 26th ACM Symposium on Parallelism
in Algorithms and Architectures, SPAA ’14, pages 143–153, New York, NY, USA, 2014.
ACM. URL: http://doi.acm.org/10.1145/2612669.2612692, doi:10.1145/2612669.
2612692.

20 György Turán. On the succinct representation of graphs. Discrete Applied Mathematics,

8(3):289–294, 1984.
Sebastiano Vigna. Sux: Implementing succinct data structures. http://sux.di.unimi.
it/. Last accessed: April 07, 2017.

21

CVIT 2016

