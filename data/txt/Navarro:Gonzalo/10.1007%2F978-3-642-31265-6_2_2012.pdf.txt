Wavelet Trees for All(cid:2)

Gonzalo Navarro

Dept. of Computer Science, University of Chile

gnavarro@dcc.uchile.cl

Abstract. The wavelet tree is a versatile data structure that serves
a number of purposes, from string processing to geometry. It can be
regarded as a device that represents a sequence, a reordering, or a grid
of points. In addition, its space adapts to various entropy measures of the
data it encodes, enabling compressed representations. New competitive
solutions to a number of problems, based on wavelet trees, are appearing
every year. In this survey we give an overview of wavelet trees and the
surprising number of applications in which we have found them useful:
basic and weighted point grids, sets of rectangles, strings, permutations,
binary relations, graphs, inverted indexes, document retrieval indexes,
full-text indexes, XML indexes, and general numeric sequences.

1

Introduction

The wavelet tree was invented in 2003 by Grossi, Gupta, and Vitter [54], as a data
structure to represent a sequence and answer some queries on it. Curiously, a
data structure that has turned out to have a myriad of applications was buried in
a paper full of other eye-catching results. The ﬁrst mention to the name “wavelet
tree” appears on page 8 of 10 [54, Sec. 4.2]. The last mention is also on page 8,
save for a ﬁgure caption on page 9. Yet, the wavelet tree was a key tool to obtain
the main result of the paper, a milestone in compressed full-text indexing.

It is interesting that, after some thought, one can see that the wavelet tree
is a slight generalization of an old (1988) data structure by Chazelle [25], heavily 
used in Computational Geometry. This data structure represents a set of
points on a two-dimensional grid: it describes a successive reshuﬄing process
where the points start sorted by one coordinate and end up sorted by the other.
K¨arkk¨ainen, in 1999 [66], was the ﬁrst to put this structure in use in the completely 
diﬀerent context of text indexing. Still, the concept and usage were totally
diﬀerent from the one Grossi et al. would propose four years later.

We have already mentioned three ways in which wavelet trees can be regarded:
(i) as a representation of a sequence; (ii) as a representation of a reordering of
elements; (iii) as a representation of a grid of points. Since 2003, these views
of wavelet trees, and their interactions, have been fruitful in a surprisingly wide
range of problems, extending well beyond the areas of text indexing and computational 
geometry where the structure was conceived.

(cid:2) Partially funded by Millennium Nucleus Information and Coordination in Networks

ICM/FIC P10-024F, Chile.

J. K¨arkk¨ainen and J. Stoye (Eds.): CPM 2012, LNCS 7354, pp. 2–26, 2012.
c(cid:2) Springer-Verlag Berlin Heidelberg 2012

Wavelet Trees for All

3

  
raaba
0 0 0 0 0 00
1

l
1

al_a_

1

_
0 0 0

raaba
1 1

l
0
1

0 0

da
0

_,a,b

d,l,r

aaba
0

_a_a_aabaa
0 1 0

0 0 0 00

0 0

0 1

0

b 

bb

_,a

a_a_a_aa

aa
aa
0 0 0 0 0 0 0 0 0

1 1 1

_

__ _

a

aaaaaa
aa
 

a

rl
llrd
0 0 0 01
1

r

rr
 

d,l

lll d
1 1 01

d

d

l

ll l

Fig. 1. A wavelet tree on string S = "alabar a la alabarda”. We draw the spaces as
underscores. The subsequences of S and the subsets of Σ labeling the edges are drawn
for illustration purposes; the tree stores only the topology and the bitmaps.

Our goal in this article is to give an overview of this marvellous data structure
and its many applications. We aim to introduce, to an audience with a general
algorithmic background, the basic data organization used by wavelet trees, the
information they can model, and the wide range of problems they can solve.
We will also mention the most technical results and give the references to be
followed by the more knowledgeable readers, advising the rest what to skip.

Being ourselves big fans of wavelet trees, and having squeezed them out for
several years, it is inevitable that there will be many references to our own work
in this survey. We apologize in advance for this, as well as for oversights of others’
results, which are likely to occur despite our eﬀorts.

x).

2 Data Structure
Let S[1, n] = s1s2 . . . sn be a sequence of symbols si ∈ Σ, where Σ = [1..σ] is
called the alphabet. Then S can be represented in plain form using n(cid:3)lg σ(cid:4) =
n lg σ + O(n) bits (we use lg x = log2
Structure. A wavelet tree [54] for sequence S[1, n] over alphabet [1..σ] can be
described recursively, over a sub-alphabet range [a..b] ⊆ [1..σ]. A wavelet tree
over alphabet [a..b] is a binary balanced tree with b − a + 1 leaves. If a = b,
the tree is just a leaf labeled a. Else it has an internal root node, vroot, that
represents S[1, n]. This root stores a bitmap Bvroot [1, n] deﬁned as follows: if
S[i] ≤ (a + b)/2 then Bvroot [i] = 0, else Bvroot [i] = 1. We deﬁne S0[1, n0] as the
subsequence of S[1, n] formed by the symbols c ≤ (a + b)/2, and S1[1, n1] as the
subsequence of S[1, n] formed by the symbols c > (a + b)/2. Then, the left child
of vroot is a wavelet tree for S0[1, n0] over alphabet [a..(cid:7)(a + b)/2(cid:8)] and the right
child of vroot is a wavelet tree for S1[1, n1] over alphabet [1 + (cid:7)(a + b)/2(cid:8)..b].
Here for legibility we are using Σ = {’ ’, a, b, d, l, r}, so n = 19 and σ = 6.

Fig. 1 displays a wavelet tree for the sequence S = "alabar a la alabarda".
Note that this wavelet tree has height (cid:3)lg σ(cid:4), and it has σ leaves and σ − 1
internal nodes. If we regard it level by level, it is not hard to see that it stores

4

G. Navarro

exactly n bits at each level, and at most n bits in the last one. Thus, n(cid:3)lg σ(cid:4)
is an upper bound to the total number of bits it stores. Storing the topology of
the tree requires O(σ lg n) further bits, if we are careful enough to use O(lg n)
bits for the pointers. This extra space may be a problem on large alphabets. We
show in the paragraph “Removing redundancy” how to save it.

Tracking Symbols. This wavelet tree represents S, in the sense that one can
recover S from it. More than that, it is a succinct data structure for S, in the
sense that it takes space asymptotically equal to a plain representation of S, and
it permits accessing any S[i] in time O(lg σ), as follows.
To extract S[i], we ﬁrst examine Bvroot [i]. If it is a 0, we know that S[i] ≤ (σ +
1)/2, otherwise S[i] > (σ+1)/2. In the ﬁrst case, we must continue recursively on
the left child; in the second case, on the right child. The problem is to determine
where has position i been mapped to on the left (or right) child. In the case of
the left child, where Bvroot [i] = 0, i has been mapped to position i0, which is the
number of 0s in Bvroot up to position i. For the right child, where Bvroot [i] = 1,
this corresponds to position i1, the number of 1s in Bvroot up to position i. The
number of 0s (resp. 1s) up to position i in a bitmap B is called rank0(B, i) (resp.
rank1(B, i)). We continue this process recursively until we arrive at a leaf. The
label of this leaf is S[i]. Note that we do not store the leaf labels; those are
deduced as we successively restrict the subrange [a..b] of [1..σ] as we descend.

Operation rank was already considered by Chazelle [25], who gave a simple
data structure using O(n) bits for a bitmap B[1, n], that computed rank in
constant time (note that we only have to solve rank1(B, i), since rank0(B, i) =
i − rank1(B, i)). Jacobson [63] improved the space to n + O(n lg lg n/ lg n) =
n + o(n) bits, and Golynski [48,49] proved this space is optimal as long as we
maintain B in plain form and build extra data structures on it. The solution
is, essentially, storing rank answers every s = lg2 n bits of B (using lg n bits
per sample), then storing rank answers relative to the last sample every (lg n)/2
bits (using lg s = 2 lg lg n bits per sub-sample), and using a universal table to
complete the answer to a rank query within a sub-sample. We will use in this
survey the notation rankb(B, i, j) = rankb(B, j) − rankb(B, i − 1).

Above, we have tracked a position from the root to a leaf, and as a consequence
we have discovered the symbol represented at the root position. It is also useful to
carry out the inverse process: given a position at a leaf, we can track it upwards
and ﬁnd out where it is on the root bitmap. This is done as follows.
Assume we start at a given leaf, at position i. If the leaf is the left child of
its parent v, then the position i(cid:3)
corresponding to i at v is the i-th occurrence
of a 0 in its bitmap Bv. If the leaf is the right child of its parent v, then i(cid:3)
is the position of the i-th occurrence of a 1 in Bv. This procedure is repeated
from v until we reach the root, where we ﬁnd the ﬁnal position. The operation
of ﬁnding the i-th 0 (resp. 1) in a bitmap B[1, n] is called select0(B, i) (resp.
select1(B, i)), and it can also be solved in constant time using the n bits of B
plus o(n) bits [27,79]. Thus the time to track a position upwards is also O(lg σ).
The constant-time solution for select [27,79] is analogous to that of rank.
The bitmap is cut into blocks with s 1s. Those that are long enough to store

Wavelet Trees for All

5

all their answers within sublinear space are handled in this way. The others are
O(1) n)) and thus encoding positions inside them require
not too long (i.e., O(lg
fewer bits (i.e., O(lg lg n)). This permits repeating the idea recursively a second
time. The third time, the remaining blocks are so short that can be handled
in constant time using universal tables. Golynski [48,49] reduced the o(n) extra
space to O(n lg lg n/ lg n) and proved this is optimal if B is stored in plain form.
With the support for rank and select, the space required by the basic binary
balanced wavelet tree reaches n(cid:3)lg σ(cid:4) + o(n) lg σ + O(σ lg n) bits. This completes
a basic description of wavelet trees; the rest of the section is more technical.
Reducing Redundancy. As mentioned, the O(σ lg n) term can be removed if
necessary [72,74]. We slightly alter the balanced wavelet tree shape, so that all
the leaves are grouped to the left (for this sake we divide the interval [a..b] of
(cid:4)lg(b−a+1)(cid:5)..b]). Then, all the bitmaps
[1..σ] into [a..a + 2
at all the levels belong to consecutive nodes, and they can all be concatenated
into a large bitmap B[1, n(cid:3)lg σ(cid:4)]. We know the bitmap of level (cid:4) starts at position
1 + n((cid:4) − 1). Moreover, if we have determined that the bitmap of a wavelet tree
node corresponds to B[l, r], then the bitmap of its left child is at B[n + l, n + l +
rank0(B, l, r)− 1], and that of the right child is at B[n + l + rank0(B, l, r), n + r].

(cid:4)lg(b−a+1)(cid:5)− 1] and [a + 2

Moving to the parent of a node is more complicated, but upward traversals can
always be handled by ﬁrst going down from the root to the desired leaf, so as to
discover all the ranges in B of the nodes in the path, and then doing the upward
processing as one returns from the recursion.
Using just one bitmap, we do not need pointers for the topology, and the
overall space becomes n(cid:3)lg σ(cid:4) + o(n) lg σ bits. The time complexities do not
change (albeit in practice the operations are slowed down a bit due to the extra
rank operations needed to navigate [28]).

The redundancy can be further reduced by representing the bitmaps using
a structure by Golynski et al. [50], which uses n + O(n lg lg n/ lg2 n) bits and
supports constant-time rank and select (this representation does not leave the
bitmap in plain form, and thus it can break the lower bound [49]). Added over
all the wavelet tree bitmaps, the space becomes n lg σ + O(n lg σ lg lg n/ lg2 n) =
n lg σ + o(n) bits.1 This structure has not been implemented as far as we know.

Speeding Up Traversals. Increasing the arity of wavelet trees reduces their
height, which dictates the complexity of the downward and upward traversals.
If the wavelet tree is d-ary, then its height is (cid:3)lgd σ(cid:4). However, the wavelet tree
1 We assume lg σ = O(lg n) here; otherwise there are many symbols that do not appear
in S. If this turns out to be the case, one should use a mapping from Σ to the range
], where σ(cid:2) ≤ n is the number of symbols actually appearing in S. Such a
[1..σ(cid:2)
mapping takes constant time and σ(cid:2)
) + O(lg lg σ) bits of space using
the “indexable dictionaries” of Raman et al. [93]. Added to the n lg σ(cid:2)
+ o(n) bits of
the wavelet tree, we are within n lg σ + o(n) + O(lg lg σ) bits. This is n lg σ + o(n)
unless n = O(lg lg σ), in which case a plain representation of S using n(cid:3)lg σ(cid:4) bits
solves all the operations in O(lg lg σ) time. To simplify, a recent analysis [45] claims
n lg σ + O(n) bits under similar assumptions. We will ignore the issue from now, and
assume for simplicity that all symbols in [1..σ] do appear in S.

) + o(σ(cid:2)

lg(σ/σ(cid:2)

6

G. Navarro

does not store bitmaps anymore, but rather sequences Bv over alphabet [1..d],
so that the symbol at Sv[i] is stored at the child numbered Bv[i] of node v.

In order to obtain time complexities O(1 + lgd σ) for the operations, we need
to handle rank and select on sequences over alphabet [1..d], in constant time.
Ferragina et al. [40] showed that this is indeed possible, while maintaining the
overall space within n lg σ + o(n) lg σ, for d = o(lg n/ lg lg n). Using, for example,
d = lg1− n for any constant 0 <  < 1, the overall space is n lg σ+O(n lg σ/ lg
 n)
bits. Golynski et al. [50] reduced the space to n lg σ + o(n) bits.
To support symbol rank and select on a sequence R[1, n] over alphabet
[1..d], we assume we have d bitmaps Bc[1, n], for c ∈ [1..d], where Bc[i] = 1 iﬀ
R[i] = c. Then rankc(R, i) and selectc(R, i) are reduced to rank1(Bc, i) and
select1(Bc, i). We cannot aﬀord to store those Bc, but we can store their extra
o(n) data for binary rank and select. Each time we need access to Bc, we access
instead R and use a universal table to simulate the bitmap’s content. Such table
gives constant-time access to chunks of length lgd(n)/2 instead of lg(n)/2, so
the overall space using Golynski et al.’s bitmap index representation [48,49] is
O(dn lg lg n/ lgd n), which added over the lgd σ levels of the wavelet tree gives
O(n lg σ·d lg d lg lg n/ lg n). This is o(n lg σ) for any d = lg1− n. Further reducing
the redundancy to o(n) bits requires more sophisticated techniques [50].

Thus, the O(lg σ) upward/downward traversal times become O(lg σ/ lg lg n)
with multiary wavelet trees. Although theoretically attractive, it is not easy to
translate their advantages to practice (see, e.g., a recent work studying interesting 
practical alternatives [17]). An exception, for a particular application, is
described in the paragraph “Positional inverted indexes” of Section 5).

The upward traversal can be speeded up further, using techniques known in
computational geometry [25]. Imagine we are at a leaf u representing a sequence
S[1, nu] and want to directly track position i to an ancestor v at distance t, which
represents sequence S[1, nv]. We can store at the leaf u a bitmap Bu[1, nv], so
that the nu positions corresponding to leaf u are marked as 1s in Bu. This bitmap
is sparse, so it is stored in compressed form as an “indexable dictionary” [93],
which uses nu lg(nv/nu)+o(nu)+O(lg lg nv) bits and can answer select1(Bu, i)
queries in O(1) time. Thus we track position i upwards for t levels in O(1) time.
The space required for all the bitmaps that point to node v is the sum, over
at most 2t leaves u, of those nu lg(nv/nu) + o(nu) + O(lg lg nv) bits. This is
maximized when nu = nv/2t for all those u, where the space becomes t · nv +
o(nv) + O(2t lg lg nv). Added over all the wavelet tree nodes with height multiple
of t, we get n lg σ + o(n lg σ) + O(σ lg lg n) = n lg σ + o(n lg σ). This is in addition
to those n lg σ + o(n) bits already used by the wavelet tree.

If we want to track only from the leaves to the root, we may just use t = lg σ
and do the tracking in constant time. In many cases, however, one wishes to
track from arbitrary to arbitrary nodes. In this case we can use 1/ values of
 σ) upward steps with one
t = lg
value of t before reaching the next one. This gives a total complexity for upward
traversals of O((1/) lg

i σ, for i ∈ [1..1/ − 1], so as to carry out O(lg

 σ) using O((1/)n lg σ) bits of space.

Wavelet Trees for All

7

Construction. It is easy to build a wavelet tree in O(n lg σ) time, by a lineartime 
processing at each node. It is less obvious how to do it in little extra space,
which may be important for succinct data structures. Two recent results [31,96]
oﬀer various relevant space-time tradeoﬀs, building the wavelet tree within the
time given, or close, and asymptotically negligible extra space.

3 Compression

The wavelet tree adapts elegantly to the compressibility of the data in many
ways. Two key techniques to achieve this are using speciﬁc encodings on bitmaps,
and altering the tree shape. This whole section is technical, yet nonexpert readers
may ﬁnd inspiring the beginning of the paragraph “Entropy coding”, and the
paragraph “Changing shape”.
Entropy Coding. Consider again Fig. 1. The fact that the ’a’ is much more
frequent than the other symbols translates into unbalanced 0/1 frequencies in
various bitmaps. Dissimilarities in symbol frequencies are an important source
of compressibility. The amount of compression that can be reached is measured
by the so-called empirical zero-order entropy of a sequence S[1, n]:

H0(S) =

(nc/n) lg(n/nc) ≤ lg σ

(cid:2)

c∈Σ

where nc is the number of occurrences of c in S and the sum considers only the
symbols that do appear in S. Then nH0(S) is the least number of bits into which
S can be compressed by always encoding the same symbol in the same way.2

Grossi et al. [54] already showed that, if the bitmaps of the wavelet tree are
compressed to their zero-order entropy, then their overall space is nH0(S). Let
Bvroot contain n0 0s and n1 1s. Then zero-order compressing it yields space
n0 lg(n/n0) + n1 lg(n/n1). Now consider its left child vl. Its bitmap, Bvl , is of
length n0, and say it contains n00 0s and n01 1s. Similarly, the right child is of
length n1 and contains n10 0s and n11 1s. Adding up the zero-order compressed
space of both children yields n00 lg(n0/n00) + n01 lg(n0/n01) + n10 lg(n1/n10) +
n11 lg(n1/n11). Now adding the space of the root bitmap yields n00 lg(n/n00) +
n01 lg(n/n01) + n10 lg(n/n10) + n11 lg(n/n11). This would already be nH0(S) if
σ = 4. It is easy to see that, by splitting the spaces of the internal nodes until
reaching the wavelet tree leaves, we arrive at

c∈Σ nc lg(n/nc) = nH0(S).

(cid:3)

This enables using any zero-order entropy coding for the bitmaps that supports 
constant-time rank and select. One is the “fully-indexable dictionary” of
Raman et al. [93], which for a bitmap B[1, n] requires nH0(B)+ O(n lg lg n/ lg n)
bits. A theoretically better one is that of Golynski et al. [50], which we have already 
mentioned without yet telling that it actually compresses the bitmap, to
nH0(B) + O(n lg lg n/ lg2 n). P˘atra¸scu [91] showed this can be squeezed up to
2 In classical information theory [32], H0 is the least number of bits per symbol achievable 
by any compressor on an inﬁnite source that emits symbols independently and
randomly with probabilities nc/n.

8

G. Navarro

nH0(B)+O(n/ lg
c, and that this is essentially optimal [92].

c n), answering rank and select in time O(c), for any constant

Using the second or third encoding, the wavelet tree represents S within
nH0(S) + o(n) bits, still supporting the traversals in time O(lg σ). Ferragina
et al. [40] showed that the zero-order compression can be extended to multiary
wavelet trees, reaching nH0(S) + o(n lg σ) bits and time O(1 + lg σ/ lg lg n) for
the operations, and Golynski et al. [50] reduced the space to nH0(S) + o(n) bits.
Recently, Belazzougui and Navarro [12] showed that the times can be reduced to
O(1 + lg σ/ lg w), where w = Ω(lg n) is the size of the machine word. Basically
they replace the universal tables with bit-parallel operations. Their space grows
to nH0(S) + o(n(H0(S) + 1)). (They also prove and match the lower bound time
complexity Θ(1 + lg(lg σ/ lg w)) using techniques that are beyond wavelet trees
and this survey, but that do build on wavelet trees [7,4].)

It should not be hard to see at this point that the sums of nu lg(nv/nu) spaces

used for fast upward traversals in Section 2 also add up to (1/)nH0(S).

Changing Shape. The algorithms for traversing the wavelet tree work independently 
of its balanced shape. Furthermore, our previous analysis of the entropy
coding of the bitmap also shows that the resulting space, at least with respect to
the entropy part, is independent of the shape of the tree. This was already noted
by Grossi et al. [55], who proposed using the shape to optimize average query
time: If we know the relative frequencies fc with which each leaf c is sought,
we can create a wavelet tree with the shape of the Huﬀman tree [62] of those
c∈Σ fc lg(1/fc) ≤ lg σ.
frequencies, thus reducing the average access time to

(cid:3)

M¨akinen and Navarro [70, Sec. 3.3], instead, proposed giving the wavelet tree
the Huﬀman shape of the frequencies with which the symbols appear in S.
This has interesting consequences. First, it is easy to see that the total number
of bits stored in the wavelet tree is exactly the number of bits output by a
Huﬀman compressor that takes the symbol frequencies in S, which is upper
bounded by n(H0(S) + 1). Therefore, even using plain bitmap representations
taking n + o(n) bits of space, the total space becomes at most n(H0(S) + 1) +
o(n(H0(S) + 1)) + O(σ lg n), that is, we compress not only the data, but also
the redundancy space. This may seem irrelevant compared to the nH0(S) + o(n)
bits that can be obtained using Golynski et al. [50] over a balanced wavelet
tree. However, it is unclear whether that approach is practical; only that of
Raman et al. [93] has successful implementations [89,28,84], and this one leads
to total space nH0(S) + o(n lg σ). Furthermore, plain bitmap representations are
signiﬁcantly faster than compressed ones, and thus compressing the wavelet tree
by giving it a Huﬀman shape leads to a much faster implementation in practice.
Another consequence of using Huﬀman shape, implied by Grossi et al. [55],
is that if the accesses to the leaves are done with frequency proportional to
their number of occurrences in S (which occurs, for example, if we access at
random positions in S), then the average access time is O(1 + H0(S)), better
than the O(lg σ) of balanced wavelet trees. A problem is that the worst case
could be as bad as O(lg n) if a very infrequent symbol is sought [70]. However,
one can balance wavelet subtrees after some depth, so that the average depth is

Wavelet Trees for All

9

O(1 + H0(S)), the maximum depth is O(lg σ), and the total number of bits is at
most n(H0(S) + 2) [70].

Recently, Barbay and Navarro [10] showed that Huﬀman shapes can be combined 
with multiary wavelet trees and entropy compression of the bitmaps, to
achieve space nH0(S)+o(n) bits, worst-case time O(1+lg σ/ lg lg n), and average
case time O(1 + H0(S)/ lg lg n).

An interesting extension of Huﬀman shaped wavelet trees that has not been
emphasized much is to use them a mechanism to give direct access on any
variable-length preﬁx-free coding. Let S = s1, s2, . . . , sn be a sequence of symbols,
 which are encoded in some way into a bit-stream C = c(s1)c(s2) . . . c(sn).
For example, S may be a numeric sequence and c can be a δ-code, to favor
small numbers [13], or c can be a Huﬀman or another preﬁx-free encoding. Any
preﬁx-free encoding ensures that we can retrieve S from C, but if we want to
maintain the compressed form C and access arbitrary positions of S, we need
tricks like sampling S at regular intervals and store pointers to C.

Instead, a wavelet tree representation of S, where for each si we rather encode
c(si), uses the same number of bits of C and gives direct access to any S[i] in
time O(|c(si)|). More precisely, at the bitmap root position Bvroot [i] we write a
0 if c(si) starts with a 0, and 1 otherwise. In the ﬁrst case we continue by the
left child and in the second case we continue by the right child, from the second
bit of c(si), until the code is exhausted. Gagie et al. [43] combined this idea with
multiary wavelet trees to obtain a faster decoding.

Very recently, Grossi and Ottaviano [56] also took advantage of speciﬁc shapes,
to give the wavelet tree the form of a trie of a set of strings. The goal was to
handle a sequence of strings and extend operations like access and rank to such
strings. The idea extends a previous, more limited, approach [72,74].

High-Order Entropy Coding. High-order compression extends zero-order
compression by encoding each symbol according to a context of length k that
precedes or follows it. The k-th order empirical entropy of S [77] is deﬁned as
A∈Σk (|SA|/n) H0(SA) ≤ Hk−1(S), where SA is the string of symbols
Hk(S) =
preceding context A in S. Any statistical compressor assigning ﬁxed codes that
depend on a context of length k outputs at least nHk(S) bits to encode S.

(cid:3)

The Burrows-Wheeler transform [22] is a useful tool to achieve high-order
entropy. It is a reversible transformation that permutes the symbols of a string
S[1, n] as follows. First sort all the suﬃxes S[i, n] lexicographically, and then list
the symbols that precede each suﬃx (where S[n] precedes S[1, n]). The result,
Sbwt[1, n], is the concatenation of the strings SA for all the contexts A. By
deﬁnition, if we compress each substring SA of Sbwt to its zero-order entropy,
the total space is the k-th order entropy of S, for k = |A|.

The ﬁrst [54] and second [39] reported use of wavelet trees used a similar partitioning 
to represent each range of Sbwt with a zero-order compressed wavelet
tree, so as to reach nHk(S) + o(n lg σ) bits of space, for any k ≤ α lgσ n and any
constant 0 < α < 1. In the second case [39], the use of Sbwt was explicit. The partitioning 
was not with a ﬁxed context length, but instead an optimal partitioning
was used [36]. This way, they obtained the given space simultaneously for any k in

10

G. Navarro

the range. In the ﬁrst case [54], they made no reference to the Burrows-Wheeler
transform, but also compressed the sequences SA of the k-th order entropy formula,
 for a ﬁxed k. We give more details on the reasons behind the use of Sbwt in
Section 5.

Already in 2004, Grossi et al. [55] realized that the careful partitioning into
many small wavelet trees, one per context, was not really necessary to achieve
k-th order compression. By using a proper encoding on its bitmaps, a wavelet
tree on the whole Sbwt could reach k-th order entropy compression of a string S.
They obtained 2nHk(S) bits, plus redundancy, by using γ-codes [13] on the runs
of 0s and 1s in the wavelet tree bitmaps. M¨akinen and Navarro [73] observed the
same fact when encoding the bitmaps using Raman et al. [93] fully indexable
dictionaries. They reached nHk(S) + o(n lg σ) bits of space, simultaneously for
any k ≤ α lgσ n and any constant 0 < α < 1, using just one wavelet tree for the
whole string. This yielded simpler and faster indexes in practice [28].

The key property is that some entropy-compression methods are local, that
is, their space is the sum of the zero-order entropies of short substrings of Sbwt.
This can be shown to be upper-bounded by the entropy of the whole string, but
also by the sum of the entropies of the substrings SA. Even more surprisingly,
K¨arkk¨ainen and Puglisi [67] recently showed that the k-th order entropy is still
reached if one cuts Sbwt into equally-spaced regions of appropriate length, and
thus simpliﬁed these indexes further by using the faster and more practical
Huﬀman-shaped wavelet trees on each region.

There are also more recent and systematic studies [35,59] of the compressibility 
properties of wavelet trees, and how they relate to gap and run-length
encodings of the bitmaps, as well to the balancing and the arity.

Exploiting Repetitions. Another relevant source of compressibility is repetitiveness,
 that is, that S[1, n] can be decomposed into a few substrings that
have appeared earlier in S, or alternatively, that there is a small context-free
grammar that generates S. Many compressors build on these principles [13],
but supporting wavelet tree functionality on such compressed representations is
harder.

M¨akinen and Navarro [71] studied the eﬀect of repetitions in the BurrowsWheeler 
transform of S. They showed that Sbwt could be partitioned into at most
nHk(S)+σk runs of equal letters in Sbwt, for any k. It is not hard to see that those
runs are inherited by the wavelet tree bitmaps, where run-length compression
would take proper advantage of them. M¨akinen and Navarro followed a diﬀerent
path: they built a wavelet tree on the run heads and used a couple of bitmaps
to simulate the operations on the original strings. The compressibility of those
two bitmaps has been further studied by M¨akinen et al. [95,75] in the context
of highly repetitive sequence collections, and also by Simon Gog [47, Sec. 3.6.1].
In some cases, however, we need the wavelet tree of the very same string S
that contains the repetition, not its Burrows-Wheeler transform. We describe
such an application in the paragraph “Document retrieval indexes” of Section 6.
Recently, Navarro et al. [86] proposed a grammar-compressed wavelet tree
for this problem. The key point is that repetitions in S[1, n] induce repetitions

Wavelet Trees for All

11

in Bvroot [1, n]. They used Re-Pair [69], a grammar-based compressor, on the
bitmaps, and enhanced a Re-Pair-based compressed sequence representation [53]
to support binary rank (they only needed downward traversals). This time, the
wavelet tree partitioning into left and right children cuts each repetition into
two, so quickly after a few levels such regularities are destroyed and another
type of bitmap compression (or none) is preferred. While the theoretical space
analysis is too weak to be useful, the result is good in practice and leaves open
the challenge of achieving stronger theoretical and practical results.

We will ﬁnd even more speciﬁc wavelet tree compression problems later.

4 Sequences, Reorderings, or Point Grids?

Now that we have established the basic structure, operations, and encodings of
wavelet trees, let us take a view with more perspective. Various applications we
have mentioned display diﬀerent ways to regard a wavelet tree representation.

As a Sequence of Values. This is the most basic one. The wavelet tree on a
sequence S = s1, . . . , sn represents the values si. The most important operations
that the wavelet tree must oﬀer to support this view are, apart from accessing
any S[i] (that we already explained in Section 2), rank and select on S. For
example, the second main usage of wavelet trees [39,40] used access and rank
on the wavelet tree built on sequence Sbwt in order to support searches on S.

The process to support rankc(S, i) is similar to that for access, with a subtle
diﬀerence. We start at position i in Bvroot , and decide whether to go left or
right depending on where is the leaf corresponding to c (and not depending on
Bvroot [i]). If we go left, we rewrite i ← rank0(Bvroot , i), else we rewrite i ←
rank1(Bvroot , i). When we arrive at the leaf c, the value of i is the ﬁnal answer.
The time complexity for this operation is that of a downward traversal towards
the leaf labeled c. To support selectc(S, i) we just apply the upward tracking,
as described in Section 2, starting at the i-th position of the leaf labeled c.

As a Reordering. Less obviously, the wavelet tree structure describes a stable
ordering of the symbols in S, so that if one traverses the leaves one ﬁnds ﬁrst
all the occurrences of the smaller symbols, and within the same symbol (i.e., the
same leaf), they are ordered by original position. As it will be clear in Section 5,
one can argue that this is the usage of wavelet trees made by their creators [54].
In this case, tracking a position downwards in the wavelet tree tells where
it goes after sorting, and tracking a position upwards tells where each symbol
is placed in the sequence. An obvious application is to encode a permutation π
over [1..n]. Our best wavelet tree takes n lg n + o(n) bits and can compute any
π(i) and π−1(i) in time O(lg n/ lg lg n) by carrying out, respectively, downward
and upward tracking of position i. We will see improvements on this idea later.

As a Grid of Points. The slightly less general structure of Chazelle [25] can
be taken as the representation of a set of points supported by wavelet trees. It is
generally assumed that we have an n×n grid with n points so that no two points
share the same row or column (i.e., a permutation). A general set of n points is

12

G. Navarro

mapped to such a discrete grid by storing the real coordinates somewhere else
and breaking ties somehow (arbitrarily is ﬁne in most cases).

Take the set of points (xi, yi), in x-coordinate order (i.e., xi < xi+1). Now deﬁne 
string S[1, n] = y1, y2, . . . , yn. Then we can ﬁnd the i-th point in x-coordinate
order by accessing S[i]. Moreover, since the wavelet tree is representing the reordering 
of the points according to y-coordinate, one can ﬁnd the i-th point in
y-coordinate order by tracking upwards the i-th point in the leaves.
Unlike permutations, here the emphasis is in counting and reporting the points
that lie within a rectangle [xmin, xmax] × [ymin, ymax]. This is solved through a
more complicated tracking mechanism, well-known in computational geometry
and also described explicitly on wavelet trees [72]. We start at the root bitmap
range Bvroot [xl, xr], where xl = xmin and xr = xmax. Now we map the interval
to the left and to the right, using xl ← rank0/1(Bvroot , xl − 1) + 1 and xr ←
rank0/1(Bvroot , xr), and continue recursively. At any node along the recursion,
we may stop if (i) the interval [xl, xr] becomes empty (thus there are no points
to report); (ii) the interval of leaves (i.e., y-coordinate values) represented by the
node has no intersection with [ymin, ymax]; (iii) the interval of leaves is contained
in [ymin, ymax]. In case (iii) we can count the number of points falling in this
sub-rectangle as xr−xl +1. As it is well known that we visit only O(lg n) wavelet
tree nodes before stopping all the recursive calls (see, e.g., a recent detailed proof,
among other more sophisticated wavelet tree properties [45]), the counting time
is O(lg n). Each of the xr − xl + 1 points found in each node can be tracked
up and down to ﬁnd their xand 
y-coordinates, in O(lg n) time per reported
occurrence. There are more eﬃcient variants of this technique that we will cover
in Section 7, but they build on this basic idea.

5 Applications as Sequences

Full-Text Indexes. A full-text index built a string S[1, n] is able to count and
locate the occurrences of arbitrary patterns P [1, m] in S. A classical index is the
suﬃx array [52,76], A[1, n], which lists the starting positions of all the suﬃxes
of S, S[A[i], n], in lexicographic order, using n(cid:3)lg n(cid:4) bits. The starting positions
of the occurrences of P in S appear in a contiguous range in A, which can be
binary searched in time O(m lg n), or O(m+ lg n) by doubling the space. A suﬃx
tree [98,78,1] is a more space-consuming structure (yet still O(n lg n) bits) that
can ﬁnd the range in time O(m). After ﬁnding the range, each occurrence is
reported in constant time, both in suﬃx trees and arrays.
The suﬃx array of S is closely related to its Burrows-Wheeler transform:
Sbwt[i] = S[A[i]− 1] (taking S[0] = S[n]). Ferragina and Manzini [37,38] showed
how, using at most 2m access and rank operations on Sbwt, one could count
the number of occurrences in S of a pattern P [1, m]. Using multiary wavelet
trees [40,50] this gives a counting time of O(m) on polylog-sized alphabets, and
O(m lg σ/ lg lg n) in general. Each such occurrence can then be located in time
O(lg1+ n lg σ/ lg lg n) for any  > 0, at the price of O(n/ lg
 n) = o(n) further
bits of space. This result has been superseded very recently [7,12,11,4], in some

Wavelet Trees for All

13

cases using wavelet trees as a part of the solution, and in all cases with some
extra price in terms of redundancy, such as o(nHk(S)) and O(n) further bits.

Grossi et al. [57,58,54] used wavelet trees to obtain a similar result via a
quite diﬀerent strategy. They represented A by means of a permutation Ψ (i) =
A−1[A[i] + 1], that is, the cell in A pointing to A[i] + 1. Ψ turns out to be formed
by σ contiguous ascending runs. The suﬃx array search can be simulated in
O(m lg n) accesses to Ψ . They encode Ψ separately for the range of each context
SA (recall paragraph “High-order entropy coding” in Section 3). As all the Ψ
pointers coming from each run are increasing, a wavelet tree is used to describe
how the σ ascending sequences of pointers coming from each run are intermingled
in the range of SA. This turns out to be, precisely, the wavelet tree of SA. This
is why both Ferragina et al. and Grossi et al. obtain basically the same space,
nHk(S) + o(n lg σ) bits. Due to the diﬀerent search strategy, the counting time
of Grossi et al. is higher. On the other hand, the representation of Ψ allows them
to locate patterns in sublogarithmic time, still using O(nHk(S)) + o(n lg σ) bits.
This is the best known usage of wavelet trees as sequences, and it is well
covered in earlier surveys [82]. New extensions of these basic concepts, supporting
more sophisticated search problems, appear every year (e.g., [94,14]). We cover
next other completely diﬀerent applications.

Positional Inverted Indexes. Consider a natural language text collection. A
positional inverted index is a data structure that stores, for each word, the list
of the positions where it appears in the collection [3]. In compressed form [99] it
takes space close to the zero-order entropy of the text seen as a sequence of words
[82]. This entropy yields very competitive compression in natural language texts.
Yet, we need to store both the text (usually zero-order compressed, so that direct
access is possible) and the inverted index, adding up to at least 2nH0(S), where
S is the text regarded as a sequence of word identiﬁers. Inverted indexes are by
far the most popular data structures to index natural language text collections,
so reducing their space requirements is of high relevance.

By representing the sequence of word identiﬁers using a wavelet tree, we obtain 
a single representation for both the text and the inverted index, all within
nH0(S) + o(n) bits [28]. In order to access any text word, we just compute S[i].
In order to access the i-th element of the inverted list of any word c, we compute
selectc(S, i). Furthermore, operation rankc(S, i) is useful to implement some
list intersection algorithms [8], as it ﬁnds the position i in the inverted list of
word c more eﬃciently than with a binary or exponential search.

Arroyuelo et al. [2] extended this functionality to document retrieval: retrieve
the distinct documents where a word appears. They use a special symbol “$”
to mark document boundaries. Then, given the ﬁrst occurrence of a word c,
p = selectc(S, 1), the document where this occurrence lies is j = rank$(S, p)+1,
document j ends at position p(cid:3)
= select$(S, j), it contains o = rankc(S, p, p(cid:3)
)
occurrences of the word c, and the search for further relevant documents can
continue from query selectc(S, o + 1).

An improvement over the basic idea is to use multiary wavelet trees, more
precisely of arity up to 256, and using the property that wavelet trees give direct

14

G. Navarro

access to any variable-length code. Brisaboa et al. [19] started with a byteoriented 
encoding of the text words (using either Huﬀman with 256 target symbols,
 or other practical encoding methods [20]) and then organized the sequence
of codes into a wavelet tree, as described in the paragraph “Changing shape” of
Section 3. A naive byte-based rank and select implementation on the wavelet
tree levels gives good results in this application, with the bytes represented in
plain form. The resulting structure is indeed competitive with positional inverted
indexes in many cases. A variant specialized on XML text collections, where the
codes are also used to distinguish structural elements (tags, content, attributes,
etc.) in order to support some XPath queries, is also being developed [18].

Graphs. Another simple application of this idea is the representation of directed
graphs [28]. Let G be a graph with n nodes and e edges. An adjacency list,
using n lg e + e lg n bits (the n pointers to the lists plus the e target nodes)
gives direct access to the neighbors of any node v. If we want also to perform
reverse nagivation, that is, to know which nodes point to v, we must spend other
n lg e + e lg n bits to represent the transposed graph.

Once again, representing with a wavelet tree the sequence S[1, e] concatenating 
all the adjacency lists, plus a compressed bitmap B[1, e] marking the
beginnings of the lists, gives access to both types of neighbors within space
n lg(e/n) + e lg n + O(n) + o(e), which is close to the space of the plain representation 
(actually, possibly less). To retrieve the i-th neighbor of a node v,
we compute the starting point of the list of v, l ← select1(B, v), and then
access S[l + i− 1]. To retrieve the i-th reverse neighbor of a node v, we compute
p ← selectv(S, i) to ﬁnd the i-th time that v is mentioned in an adjacency
list, and then compute with rank1(B, p) the owner of the list where v is mentioned.
 Both operations take time O(lg n/ lg lg n). This is also useful to represent
undirected graphs, where adjacency lists must usually represent each edge twice.
With a wavelet tree we can choose any direction for an edge, and at query time
we join direct and reverse neighbors of nodes to build their list.

Note, ﬁnally, that the wavelet tree can compress S to its zero-order entropy,
which corresponds to the distribution of in-degrees of the nodes. A more sophisticated 
variant of this idea, combined with Re-Pair compression [69], was shown
to be competitive with current Web graph compression methods [29].

6 Applications as Reorderings

Apart from its ﬁrst usage [54], that can be regarded as encoding a reordering,
wavelet trees oﬀer various interesting applications when seen in this way.

Permutations. As explained in Section 4, one can easily encode a permutation
with a wavelet tree. It is more interesting that the encoding can take less space
when the permutation is, in a sense, compressible. Barbay and Navarro [9,10]
considered permutations π of [1..n] that can be decomposed into ρ contiguous
ascending runs, of lengths r1, r2, . . . , rρ. They deﬁne the entropy of such a permutation 
as H(π) =
i=1(ri/n) lg(n/ri), and show that it is possible to sort
an array with such ascending runs in time O(n(H(π) + 1)). This is obtained by

(cid:3)ρ

Wavelet Trees for All

15

building a Huﬀman tree on the run lengths (seen as frequencies) and running a
mergesort-like algorithm that follows the Huﬀman tree shape.

They note that, if we encode with 0 or 1 the results of the comparisons of the
mergesort algorithm at each node of the merging tree, the resulting structure
contains at most n(H(π) + 1) bits, and it represents the permutation. Starting
at position i in the top bitmap Bvroot one can track down the position exactly as
done with wavelet trees, so as to arrive at position j of the t-th leaf (i.e., run). By
storing, in O(ρ lg n) bits, the starting position of each run in π, we can convert
the leaf position into a position in π. Therefore the downward traversal solves
operation π−1(i), because it starts from value i (i.e., position i after sorting π),
and gives the position in π from where it started before the merging took place.
The corresponding upward traversal, consequently, solves π(i). Other types of
runs, more and less general, are also studied [9,10].
Some thought reveals that this structure is indeed the wavelet tree of a sequence 
formed by replacing, in π−1, each symbol belonging to the i-th run, by
the run identiﬁer i. Then the fact that a downward traversal yields π−1(i) and
that the upward traversal yields π(i) are natural consequences. This relation is
made more explicit in a later article [7,4].

Generic Numeric Sequences. There are several basic problems on sequences
of numbers that can be solved in nontrivial ways using wavelet trees. We mention
a few that have received attention in the literature.

One such problem is the range quantile query: Preprocess a sequence of numbers 
S[1, n] on the domain [1..σ] so that later, given a range [l, r] and a value i,
we can compute the i-th smallest element in S[l, r].

Classical solutions to this problem have used nearly quadratic space and constant 
time. Only a very recent solution [65] reaches O(n lg n) bits of space (apart
from storing S) and O(lg n/ lg lg n) time. We show that, by representing S with
a wavelet tree, we can solve the problem in O(lg σ) time and just o(n) extra
bits [46,45]. This is close to O(lg n/ lg lg n) (in this problem, we can always make
σ ≤ n hold), and it can be even better if σ is small compared to n.

Starting from the range S[l, r], we compute rank0(Bvroot , l, r). If this is i or
more, then the i-th value in this range is stored in the left subtree, so we go to
the left child and remap the interval [l, r] as done for counting points in a range
(see Section 4). Otherwise we go right, subtracting rank0(Bvroot , l, r) from i and
remapping [l, r] in the same way. When we arrive at a leaf, its label is the i-th
smallest element in S[l, r].

Another fundamental problem is called range next value: Preprocess a sequence 
of numbers S[1, n] on the domain [1..σ] so that later, given a range [l, r]
and a value x, we return the smallest value in S[l, r] that is larger than x.

The state of the art also includes superlinear-space and constant-time solutions,
 as well as one using O(n lg n) bits of space and O(lg n/ lg lg n) time [100].
Once again, we achieve o(n) extra bits and O(lg σ) time using wavelet trees [45]
(we improve this time in the paragraph “Binary relations” of Section 7).

Starting at the root from the range S[l, r], we see if value x labels a leaf
descending from the left or from the right child. If x descends from the right

16

G. Navarro

child, then no value on the left child can be useful, so we recursively descend
to the right child and remap the interval [l, r] as done for counting points in a
range. Else, there may be values > x on both children, but we prefer those on
the left, if any. So we ﬁrst descend to the left child looking for an answer (there
may be no answer if, at some node, the interval [l, r] becomes empty). If the left
child returns an answer, this is what we seek and we return it. If, however, there
is no value > x on the left child, we seek the smallest value on the right child.
We then enter into another mode where we see if there is any 0-bit in Bv[l, r].
If there is one, we go to the left child, else we go to the right child. It can be
shown that the overall process takes O(lg σ) time.

A variant of the range next value problem is called prevLess [68]: return the
rightmost value in S[1, r] that is smaller than x. Here we start with S[1, r]. If
value x labels a leaf descending from the left, we map the interval to the left
child and continue recursively from there. If, instead, x descends from the right
child, then the answer may be on the left or the right child, and we prefer the
rightmost in [1, r]. Any 0-bit in Bv[1, r] is a value smaller than x and thus a valid
answer. We use rank and select to ﬁnd the rightmost 0 in Bv[1, r]. We also
continue recursively by the right child, and if it returns an answer, we map it to
the bitmap Bv[1, r]. Then we choose the rightmost between the answer from the
right child and the rightmost zero. The overall time is O(lg σ).

Non-positional Inverted Indexes. These indexes store only the list of distinct 
documents where each word appears, and come in two ﬂavors [99,3]. In the
ﬁrst, the documents for each word are sorted by increasing identiﬁer. This is useful 
to implement list unions and intersections for boolean, phrase and proximity
queries. In the second, a “weight” (measuring importance somehow) is assigned
to each document where a word appears. The lists of each word store those
weights and are sorted by decreasing weight. This is useful to implement ranked
bag-of-word queries, which give the documents with highest weights added over
all the query words. It would seem that, unless one stores two inverted indexes,
one must choose one order in detriment of the queries of the other type.

By representing a reordering, wavelet trees can store both orderings simultaneously 
[85,45]. Let us represent the documents where each word appears in
decreasing weight order, and concatenate all the lists into a sequence S[1, n]. A
bitmap B[1, n] marks the starting positions of the lists, and the weights are stored
separately. Then, a wavelet tree representation of S simulates, within the space
of just one list, both orderings. By accessing S[l+i−1], where l = select1(B, c),
we obtain the i-th element of the inverted list of word c, in decreasing weight
order. To access the i-th element of the inverted list of a word in increasing
document order, we also compute the end of its list, r = select1(B, c + 1) − 1,
and then run a range quantile query for the i-th smallest value in the range
[l, r]. Many other operations of interest in information retrieval can be carried
out with this representation and little auxiliary data [85,45].

Document Retrieval Indexes. An interesting extension to full-text retrieval
is document retrieval, where a collection S[1, n] of general strings (so inverted
indexes cannot be used) is to be indexed to answer diﬀerent document retrieval

Wavelet Trees for All

17

queries. The most basic one, document listing, is to output the distinct documents 
where a pattern P [1, m] appears. Muthukrishnan [80] deﬁned a so-called
document array D[1, n], where D[i] gives the document to which the i-th lexicographically 
smallest suﬃx of S belongs (i.e., where the suﬃx S[A[i], n] belongs,
where A is the suﬃx array of S). He also deﬁned an array C[1, n], where C[i]
points to the previous occurrence of D[i] in D. A suﬃx tree was used to identify
the range A[l, r] of the pattern occurrences, so that we seek to report the distinct
elements in D[l, r]. With further structures to ﬁnd minima in ranges of C [15],
Muthukrishnan gave an O(m + occ) algorithm to ﬁnd the occ distinct documents
where P appears. This is time-optimal, yet the space is impractical.

This is another case where wavelet trees proved extremely useful. M¨akinen and
V¨alim¨aki [97] showed that, if one implemented D as a wavelet tree, then array
C was not necessary, since C[i] = selectD[i](D, rankD[i](D, i − 1)). They also
used a compressed full-text index [39] to identify the range D[l, r], so the total
time turned out to be O(m lg σ + occ lg d), where d is the number of documents
in S. Moreover, for each document c output, rankc(D, l, r) gave the number of
times P appeared in c, which is important for ranked document retrieval.

Gagie et al. [46,45] showed that an application of range quantile queries
enabled the wavelet tree to solve this problem elegantly and without any range
minima structure: The ﬁrst distinct document is the smallest value in D[l, r].
If it occurs f1 times, then the second distinct document is the (1 + f1)-th
smallest value in D[l, r], and so on. They retained the complexities of M¨akinen
and V¨alim¨aki, but the solution used less space and time in practice. Later
[45] they replaced the range quantile queries by a depth-ﬁrst traversal of the
wavelet tree that reduced the time complexity, after the suﬃx array search, to
O(occ lg(d/occ)). The technique is similar to the two-dimensional range searches:
recursively enter into every wavelet tree branch where the mapped interval [l, r]
is not empty, and report the leaves found, with frequency r − l + 1.

This depth-ﬁrst search method can easily be extended to support more complex 
queries, for example t-thresholded ones: given s patterns, we want the documents 
where at least t of the terms appear. We can ﬁrst identify the s ranges
in D and then traverse the wavelet tree while maintaining the s ranges, stopping
when less than t intervals are nonempty, or when we arrive at leaves (where
we report the document). Other sophisticated traversals have been proposed for
retrieving the documents ranked by number of occurrences of the patterns [33].
An interesting problem is how to compress the wavelet tree of D eﬀectively.
The zero-order entropy of D has to do with document lengths, which is generally
uninteresting, and unrelated to the compressiblity of S. It has been shown [44,86]
that the compressibility of S shows up as repetitions in D, which has stimulated
the development of wavelet tree compression methods that take advantage of
the repetitiveness of D, as described at the end of Section 3.

7 Applications as Grids

Discrete Grids. Much work has been done in Computational Geometry over
structures very similar to wavelet trees. We only highlight some results of

18

G. Navarro

interest, generally focusing on structures that use linear space. We assume here
that we have an n× n grid with n points not sharing rows nor columns. Interestingly,
 these grids with range counting and reporting operations have been intensively 
used in compressed text indexing data structures [66,81,38,72,26,16,30,68]
Range counting can be done in time O(lg n/ lg lg n) and O(n lg n) bits [64].
O(1) n) [90], but it can be
This time cannot be improved within space O(n lg
matched with a multiary wavelet-tree like structure using just n lg n + o(n lg n)
bits [16]. Reaching this time, instead of the easy O(lg n) we have explained in
Section 4, requires a sophisticated solution to the problem of doing the range
counting among several consecutive children of a node, that are completely contained 
in the x-range of the query. They [16] also obtain a range reporting time
(for the occ points in the range) of O((1+occ) lg n/ lg lg n). This is not surprising
once counting has been solved: it is a matter of upward or downward tracking on
a multiary wavelet tree. The technique for faster upward tracking we described
in the paragraph “Speeding up traversals” of Section 2 can be used to improve
 n), using O((1/)n lg n) bits of space [24].
the reporting time to O((1 + occ) lg
Wavelet trees oﬀer relevant solutions to other geometric problems, such as
ﬁnding the dominant points in a grid, or solving visiblity queries. Those problems
can be recast as a sequence of queries of the form “ﬁnd the smallest element larger
than x in a range”, described in the paragraph “Generic numeric sequences” of
Section 6, and therefore solved in time O(lg n) per point retrieved [83]. That
paper [83,87] also studies extensions of geometric queries where the points have
weights and statistical queries on them are posed, such as ﬁnding range sums,
averages, minima, quantiles, majorities, and so on. The way those queries are
solved open interesting new avenues in the use of wavelet trees.

Some queries, such as ﬁnding the minimum value of a two-dimensional range,
are solved by enriching wavelet trees with extra information aligned to the
bitmaps. Recall that each wavelet tree node v handles a subsequence Sv of the
sequence of points S[1, n]. To each node v with bitmap Bv[1, nv] we associate a
data structure using 2nv + o(nv) bits that answers one-dimensional range minimum 
queries [41] on Sv[1, nv]. Once built, this structure does not need to access
Sv, yet it gives the position of the minimum in constant time. Since, as explained,
 a two-dimensional range is covered by O(lg n) wavelet tree nodes, only
those O(lg n) minima must be tracked upwards, where the actual weights are
stored, to obtain the ﬁnal result. Thus the query requires O(lg1+ n) time and
O((1/)n lg n) bits of space by using the fast upward tracking mechanism.

Other queries, such as ﬁnding the i-th smallest value of a two-dimensional
range, are handled with a wavelet tree on the weight values. Each wavelet tree
node stores a grid with the points whose weights are in the range handled by that
node. Then, by doing range counting queries on those grids, one can descend left
or right, looking for the rightmost leaf (i.e., value) such that the counts of the
children to the left of the path followed add up to less than i. The total time is
O(lg2 n/ lg lg n), however the space becomes superlinear, O(n lg2 n) bits.

Finally, an interesting extension to the typical point grids are grids of rectangles,
 which are used in geographic information systems as minimum bounding

Wavelet Trees for All

19

rectangles of complex objects. Then one wishes to ﬁnd the set of rectangles
that intersect a query rectangle. This is well solved with an R-tree data structure 
[60], but a wavelet tree may oﬀer interesting space reductions. Brisaboa et
al. [21] describe a technique to store n rectangles where one does not contain
another in the x-coordinate range (so the set is ﬁrst separated into maximal “x-
independent” subsets and each subset is queried separately). Two arrays with
the ascending lower and upper x-coordinates of the rectangles are stored (as the
sets are x-independent, the same position in both arrays corresponds to the same
rectangle). A wavelet tree on those x-coordinate-sorted rectangles is set up, so
that each node handles a range of y-coordinate values. This wavelet tree stores
two bitmaps per node v: one tells whether the rectangle Sv[i] extends to the yrange 
of the left child, and the other whether it extends to the right child. Both
bitmaps can store a 1 at a position i, and thus the rectangle is stored in both
subtrees. To avoid representing a large rectangle more than O(lg n) times, both
bits are set to 0 (which is otherwise impossible) when the rectangle completely
contains the y-range of the current node. The total space is O(n lg n) bits.
Given a query [xmin, xmax] × [ymin, ymax], we look for xmin in the array of
upper x-coordinates, to ﬁnd position xl, and look for xmax in the array of lower
x-coordinates, to ﬁnd position xr. This is because a query intersects a rectangle
on the x-axis if the query does not start after the rectangle ends and the query
does not end before the rectangle starts. Now the range [xl, xr] is used to traverse
the wavelet tree almost like on a typical range search, except that we map to
the left child using rank1 on one bitmap, and to the right child using rank1 on
the other bitmap. Furthermore, we report all the rectangles where both bitmaps
contain a 0-bit, and we remove duplicates by merging results at each node, as
the same rectangle can be encountered several times. The overall time to report
the occ rectangles is still O((1 + occ) lg n).

Binary Relations. A binary relation R between two sets A and B can be
thought of as a grid of size |A| × |B|, containing |R| points. Apart from strings,
permutations and our grids, that are particular cases, binary relations are good
abstractions for a large number of more applied structures. For example, a nonpositional 
inverted index is a binary relation between a set of words and a set
of documents, so that a word is related to the documents where it appears. As
another example, a graph is a binary relation between the set of nodes and itself.
The most typical operations on binary relations are determining the elements
b ∈ B that are related to some a ∈ A and vice versa, and determining whether
a pair (a, b) ∈ A × B is related in R. However, more complex queries are also
of interest. For example, counting or retrieving the documents related to any
term in a range enables on-the-ﬂy stemming and query expansion. Retrieving
the terms associated to a document permits vocabulary analyses. Accessing the
documents in a range related to a term enables searches local to subcollections.
Range counting and reporting allows regarding graphs at a larger granularity
(e.g., a Web graph can be regarded as a graph of hosts, or of pages, on the ﬂy).
Barbay et al. [5,6] studied a large number of complex queries for binary relations,
 including accessing the points in a range in various orders, as well as

20

G. Navarro

reporting rows or columns containing points in a range. They proposed two
wavelet-tree-like data structures for handling the operations. One is basically a
wavelet tree of the set of points (plus a bitmap that indicates when we move
from one column to the next). It turns out that almost all the solutions described
so far on wavelet trees ﬁnd application to solve some of the operations.

In the extended version [6] they use multiary wavelet trees to reduce the
times of most of the operations. Several nontrivial structures and algorithms
are designed in order to divide the times of various operations by lg lg n (the
only precedent we know of is that of counting the number of points in a range
[16]). For example, it is shown how to solve the range next value problem (recall
paragraph “Generic numeric sequences” of Section 6) in time O(lg n/ lg lg n).
Others, like the range quantile query, stay no better than O(lg n).

√
2)H+O(|A|+|B|+|R|) bits, where H = lg

Barbay et al. also propose a second data structure that is analogous to the
one described for rectangles in the paragraph “Discrete grids”. Two bitmaps are
stored per node, indicating whether a given column has points in the ﬁrst and
in the second range of rows. This extension of a wavelet tree is less powerful
(cid:5)
(cid:4)|A|·|B|
than the previous structure, but it is shown that its space is close to the entropy
|R|
.
of the binary relation: (1+
This is not achieved with the classical wavelet tree. A separate work [34] builds
on this to obtain a fully-compressed grid representation, within H + o(H) bits.
Colored Range Queries. A problem of interest in areas like query log and web
mining is to count the diﬀerent colors in a sequence S[1, n] over a universe of σ
colors. Inspired in the idea of Muthukrishnan [80] for document retrieval (recall
paragraph “Document retrieval indexes” in Section 6), Gagie et al. [44] showed
that this is a matter of counting how many values smaller than l are there in
C[l, r], where C[i] = max{j<i, S[j]=S[i]}. This is a range counting query for
[l, r] × [1, l−1] on C seen as a grid, that can be solved in time O(lg n) using the
wavelet tree of C. Note that this wavelet tree, unlike that of S, uses n lg n + o(n)
bits. Gagie et al. compressed it to n lg σ + O(n lg lg n) bits, by taking advantage
of the particular structure of C, which shows up in the bit-vectors. Gagie and
K¨arkk¨ainen [42] then reduced the space to nH0(S) + o(nH0(S)) + O(n) with
more advanced techniques, and also reduced the query time to O(lg(r − l + 1)).

8 Conclusions and Further Challenges

We have described the wavelet tree, a surprisingly versatile data structure that
oﬀers nontrivial solutions to a wide range of problems in areas like string processing,
 computational geometry, and many more. An important additional asset of
the wavelet tree is its simplicity to understand, teach, and program. This makes
it a good data structure to be introduced at an undegraduate level, at least in its
more basic variants. In many cases, solutions with better time complexity than
the ones oﬀered by wavelet trees are not so practical nor easy to implement.

Wavelet trees seem to be unable to reach access and rank/select times of
the form O(lg lg σ), as other structures for representing sequences do [51], close
to the lower bounds [12]. However, both have been combined to oﬀer those time

Wavelet Trees for All

21

complexities and good zero-order compression of data and redundancy [7,4,12].
Yet, the lower bounds on some geometric problems [24], matched with current
wavelet trees [16,6], suggest that this combination cannot be carried out much
further than those three operations. Still, there are some complex operations
where it is not clear that wavelet trees have matched lower bounds [45].

We have described the wavelet tree as a static data structure. However, if the
bitmaps or sequences stored at the nodes support insertions and deletions in time
indel(n), then the wavelet tree easily supports insertions and deletions in the sequence 
S[1, n] it represents, in time O(h·indel(n)), where h is its height. This has
been used to support indels in time O((1 + lg σ/ lg lg n) lg n/ lg lg n) [61,88]. The
alphabet, however, is still ﬁxed in those solutions. While such a limitation may
seem natural for sequences, it looks deﬁnitely artiﬁcial when representing grids:
one can insert and delete new x-coordinates and points, but the y-coordinate
universe cannot change. Creating or removing alphabet symbols requires changing 
the shape of the wavelet tree, and the bitmaps or sequences stored at the
nodes undergo extensive modiﬁcations upon small tree shape changes (e.g., AVL
rotations). Extending dynamism to support this type of updates, with good time
complexities at least in the amortized sense, is an important challenge for this
data structure. It is also unclear what is the dynamic lower bound on a general
alphabet; on a constant-size alphabet it is Θ(lg n/ lg lg n) [23]. Very recently [56]
a dynamic scheme for a particular case (sequences of strings) has been proposed.
A path that, in our opinion, has only started to be exploited, is to enhance the
wavelet tree with “one-dimensional” data structures at its nodes v, so that, by
eﬃciently solving some kind of query over the corresponding subsequences Sv,
we solve a more complex query on the original sequence S. In most cases along
this survey, these one-dimensional queries have been rank and select on the
bitmaps, but we have already shown some examples involving more complicated
queries [44,87,83]. This approach may prove to be very fruitful.

In terms of practice, although there are many successful and publicly available 
implementations of wavelet tree variants (see, e.g., libcds.recoded.cl and
http://www.uni-ulm.de/in/theo/research/sdsl.html), there are some challenges
ahead, such as carrying to practice the theoretical results that promise fast and
small multiary wavelet trees [40,50,17] and lower redundancies [49,91,50].

Acknowledgements. Thanks to J´er´emy Barbay, Ana Cerdeira, Travis Gagie,
Juha K¨arkk¨ainen, Susana Ladra, Simon Puglisi, and all those who took the time
to read early versions of this manuscript.

References

1. Apostolico, A.: The myriad virtues of subword trees. In: Combinatorial Algorithms 
on Words. NATO ISI Series, pp. 85–96. Springer (1985)

2. Arroyuelo, D., Gonz´alez, S., Oyarz´un, M.: Compressed Self-indices Supporting
Conjunctive Queries on Document Collections. In: Chavez, E., Lonardi, S. (eds.)
SPIRE 2010. LNCS, vol. 6393, pp. 43–54. Springer, Heidelberg (2010)

22

G. Navarro

3. Baeza-Yates, R., Ribeiro-Neto, B.: Modern Information Retrieval, 2nd edn.

Addison-Wesley (2011)

4. Barbay, J., Claude, F., Gagie, T., Navarro, G., Nekrich, Y.: Eﬃcient fullycompressed 
sequence representations. CoRR, abs/0911.4981v4 (2012)

5. Barbay, J., Claude, F., Navarro, G.: Compact Rich-Functional Binary Relation
Representations. In: L´opez-Ortiz, A. (ed.) LATIN 2010. LNCS, vol. 6034, pp.
170–183. Springer, Heidelberg (2010)

6. Barbay, J., Claude, F., Navarro, G.: Compact binary relation representations with

rich functionality. CoRR, abs/1201.3602 (2012)

7. Barbay, J., Gagie, T., Navarro, G., Nekrich, Y.: Alphabet Partitioning for Compressed 
Rank/Select and Applications. In: Cheong, O., Chwa, K.-Y., Park, K.
(eds.) ISAAC 2010, Part II. LNCS, vol. 6507, pp. 315–326. Springer, Heidelberg
(2010)

8. Barbay, J., L´opez-Ortiz, A., Lu, T., Salinger, A.: An experimental investigation

of set intersection algorithms for text searching. ACM J. Exp. Alg. 14 (2009)

9. Barbay, J., Navarro, G.: Compressed representations of permutations, and applications.
 In: Proc. 26th STACS, pp. 111–122 (2009)

10. Barbay, J., Navarro, G.: On compressing permutations and adaptive sorting.

CoRR, abs/1108.4408 (2011)

11. Belazzougui, D., Navarro, G.: Alphabet-Independent Compressed Text Indexing.
In: Demetrescu, C., Halld´orsson, M.M. (eds.) ESA 2011. LNCS, vol. 6942, pp.
748–759. Springer, Heidelberg (2011)

12. Belazzougui, D., Navarro, G.: New lower and upper bounds for representing sequences.
 CoRR, abs/1111.2621 (2011)

13. Bell, T., Cleary, J., Witten, I.: Text Compression. Prentice Hall (1990)
14. Beller, T., Gog, S., Ohlebusch, E., Schnattinger, T.: Computing the Longest Common 
Preﬁx Array Based on the Burrows-Wheeler Transform. In: Grossi, R., Sebastiani,
 F., Silvestri, F. (eds.) SPIRE 2011. LNCS, vol. 7024, pp. 197–208. Springer,
Heidelberg (2011)

15. Bender, M., Farach-Colton, M.: The LCA Problem Revisited. In: Gonnet, G.H.,
Viola, A. (eds.) LATIN 2000. LNCS, vol. 1776, pp. 88–94. Springer, Heidelberg
(2000)

16. Bose, P., He, M., Maheshwari, A., Morin, P.: Succinct Orthogonal Range Search
Structures on a Grid with Applications to Text Indexing. In: Dehne, F., Gavrilova,
M., Sack, J.-R., T´oth, C.D. (eds.) WADS 2009. LNCS, vol. 5664, pp. 98–109.
Springer, Heidelberg (2009)

17. Bowe, A.: Multiary Wavelet Trees in Practice. Honours thesis, RMIT Univ., Australia 
(2010)

18. Brisaboa, N.R., Cerdeira-Pena, A., Navarro, G.: A Compressed Self-indexed Representation 
of XML Documents. In: Agosti, M., Borbinha, J., Kapidakis, S., Papatheodorou,
 C., Tsakonas, G. (eds.) ECDL 2009. LNCS, vol. 5714, pp. 273–284.
Springer, Heidelberg (2009)

19. Brisaboa, N., Fari˜na, A., Ladra, S., Navarro, G.: Reorganizing compressed text.

In: Proc. 31st SIGIR, pp. 139–146 (2008)

20. Brisaboa, N., Fari˜na, A., Navarro, G., Param´a, J.: Lightweight natural language

text compression. Inf. Retr. 10, 1–33 (2007)

21. Brisaboa, N.R., Luaces, M.R., Navarro, G., Seco, D.: A Fun Application of Compact 
Data Structures to Indexing Geographic Data. In: Boldi, P. (ed.) FUN 2010.
LNCS, vol. 6099, pp. 77–88. Springer, Heidelberg (2010)

22. Burrows, M., Wheeler, D.: A block sorting lossless data compression algorithm.

Tech. Rep. 124, Digital Equipment Corporation (1994)

Wavelet Trees for All

23

23. Chan, H.-L., Hon, W.-K., Lam, T.-W., Sadakane, K.: Compressed indexes for

dynamic text collections. ACM Trans. Alg. 3(2), article 21 (2007)

24. Chan, T., Larsen, K.G., P˘atra¸scu, M.: Orthogonal range searching on the RAM,

revisited. In: Proc. 27th SoCG, pp. 1–10 (2011)

25. Chazelle, B.: A functional approach to data structures and its use in multidimensional 
searching. SIAM J. Comp. 17(3), 427–462 (1988)

26. Chien, Y.-F., Hon, W.-K., Shah, R., Vitter, J.: Geometric Burrows-Wheeler trans-
form: Linking range searching and text indexing. In: Proc. 18th DCC, pp. 252–261
(2008)

27. Clark, D.: Compact Pat Trees. PhD thesis, Univ. of Waterloo, Canada (1996)
28. Claude, F., Navarro, G.: Practical Rank/Select Queries over Arbitrary Sequences.
In: Amir, A., Turpin, A., Moﬀat, A. (eds.) SPIRE 2008. LNCS, vol. 5280, pp. 176–
187. Springer, Heidelberg (2008)

29. Claude, F., Navarro, G.: Extended Compact Web Graph Representations. In:
Elomaa, T., Mannila, H., Orponen, P. (eds.) Ukkonen Festschrift 2010. LNCS,
vol. 6060, pp. 77–91. Springer, Heidelberg (2010)

30. Claude, F., Navarro, G.: Self-indexed grammar-based compression. Fund.

Inf. 111(3), 313–337 (2010)

31. Claude, F., Nicholson, P.K., Seco, D.: Space Eﬃcient Wavelet Tree Construction.
In: Grossi, R., Sebastiani, F., Silvestri, F. (eds.) SPIRE 2011. LNCS, vol. 7024,
pp. 185–196. Springer, Heidelberg (2011)

32. Cover, T., Thomas, J.: Elements of Information Theory. Wiley (1991)
33. Culpepper, J.S., Navarro, G., Puglisi, S.J., Turpin, A.: Top-k Ranked Document
Search in General Text Databases. In: de Berg, M., Meyer, U. (eds.) ESA 2010,
Part II. LNCS, vol. 6347, pp. 194–205. Springer, Heidelberg (2010)

34. Farzan, A., Gagie, T., Navarro, G.: Entropy-Bounded Representation of Point
Grids. In: Cheong, O., Chwa, K.-Y., Park, K. (eds.) ISAAC 2010, Part II. LNCS,
vol. 6507, pp. 327–338. Springer, Heidelberg (2010)

35. Ferragina, P., Giancarlo, R., Manzini, G.: The myriad virtues of wavelet trees.

Inf. Comp. 207(8), 849–866 (2009)

36. Ferragina, P., Giancarlo, R., Manzini, G., Sciortino, M.: Boosting textual compression 
in optimal linear time. J. ACM 52(4), 688–713 (2005)

37. Ferragina, P., Manzini, G.: Opportunistic data structures with applications. In:

Proc. 41st FOCS, pp. 390–398 (2000)

38. Ferragina, P., Manzini, G.: Indexing compressed texts. J. ACM 52(4), 552–581

(2005)

39. Ferragina, P., Manzini, G., M¨akinen, V., Navarro, G.: An Alphabet-Friendly FMIndex.
 In: Apostolico, A., Melucci, M. (eds.) SPIRE 2004. LNCS, vol. 3246, pp.
150–160. Springer, Heidelberg (2004)

40. Ferragina, P., Manzini, G., M¨akinen, V., Navarro, G.: Compressed representations

of sequences and full-text indexes. ACM Trans. Alg. 3(2), article 20 (2007)

41. Fischer, J.: Optimal Succinctness for Range Minimum Queries. In: L´opez-Ortiz,
A. (ed.) LATIN 2010. LNCS, vol. 6034, pp. 158–169. Springer, Heidelberg (2010)
42. Gagie, T., K¨arkk¨ainen, J.: Counting Colours in Compressed Strings. In: Giancarlo,
 R., Manzini, G. (eds.) CPM 2011. LNCS, vol. 6661, pp. 197–207. Springer,
Heidelberg (2011)

43. Gagie, T., Navarro, G., Nekrich, Y.: Fast and Compact Preﬁx Codes. In: van
Leeuwen, J., Muscholl, A., Peleg, D., Pokorn´y, J., Rumpe, B. (eds.) SOFSEM
2010. LNCS, vol. 5901, pp. 419–427. Springer, Heidelberg (2010)

24

G. Navarro

44. Gagie, T., Navarro, G., Puglisi, S.J.: Colored Range Queries and Document Retrieval.
 In: Chavez, E., Lonardi, S. (eds.) SPIRE 2010. LNCS, vol. 6393, pp. 67–81.
Springer, Heidelberg (2010)

45. Gagie, T., Navarro, G., Puglisi, S.J.: New algorithms on wavelet trees and applications 
to information retrieval. Theor. Comp. Sci. 426-427, 25–41 (2012)

46. Gagie, T., Puglisi, S.J., Turpin, A.: Range Quantile Queries: Another Virtue of
Wavelet Trees. In: Karlgren, J., Tarhio, J., Hyyr¨o, H. (eds.) SPIRE 2009. LNCS,
vol. 5721, pp. 1–6. Springer, Heidelberg (2009)

47. Gog, S.: Compressed Suﬃx Trees: Design, Construction, and Applications. PhD

thesis, Univ. of Ulm, Germany (2011)

48. Golynski, A.: Optimal Lower Bounds for Rank and Select Indexes. In: Bugliesi,
M., Preneel, B., Sassone, V., Wegener, I. (eds.) ICALP 2006. LNCS, vol. 4051,
pp. 370–381. Springer, Heidelberg (2006)

49. Golynski, A.: Optimal lower bounds for rank and select indexes. Theor. Comp.

Sci. 387(3), 348–359 (2007)

50. Golynski, A., Grossi, R., Gupta, A., Raman, R., Rao, S.S.: On the Size of Succinct
Indices. In: Arge, L., Hoﬀmann, M., Welzl, E. (eds.) ESA 2007. LNCS, vol. 4698,
pp. 371–382. Springer, Heidelberg (2007)

51. Golynski, A., Munro, J.I., Rao, S.S.: Rank/select operations on large alphabets:

a tool for text indexing. In: Proc. 17th SODA, pp. 368–373 (2006)

52. Gonnet, G., Baeza-Yates, R., Snider, T.: New indices for text: Pat trees and Pat
arrays. In: Information Retrieval: Data Structures and Algorithms, ch. 3, pp. 66–
82. Prentice-Hall (1992)

53. Gonz´alez, R., Navarro, G.: Compressed Text Indexes with Fast Locate. In: Ma, B.,
Zhang, K. (eds.) CPM 2007. LNCS, vol. 4580, pp. 216–227. Springer, Heidelberg
(2007)

54. Grossi, R., Gupta, A., Vitter, J.: High-order entropy-compressed text indexes. In:

Proc. 14th SODA, pp. 841–850 (2003)

55. Grossi, R., Gupta, A., Vitter, J.: When indexing equals compression: Experiments
with compressing suﬃx arrays and applications. In: Proc. 15th SODA, pp. 636–
645 (2004)

56. Grossi, R., Ottaviano, G.: The wavelet trie: Maintaining an indexed sequence of

strings in compressed space. In: Proc. 31st PODS (to appear, 2012)

57. Grossi, R., Vitter, J.: Compressed suﬃx arrays and suﬃx trees with applications
to text indexing and string matching. In: Proc. 32nd STOC, pp. 397–406 (2000)
58. Grossi, R., Vitter, J.: Compressed suﬃx arrays and suﬃx trees with applications

to text indexing and string matching. SIAM J. Comp. 35(2), 378–407 (2006)

59. Grossi, R., Vitter, J., Xu, B.: Wavelet trees: From theory to practice. In: Proc.

1st CCP, pp. 210–221 (2011)

60. Guttman, A.: R-trees: A dynamic index structure for spatial searching. In: Proc.

10th SIGMOD, pp. 47–57 (1984)

61. He, M., Munro, J.I.: Succinct Representations of Dynamic Strings. In: Chavez,
E., Lonardi, S. (eds.) SPIRE 2010. LNCS, vol. 6393, pp. 334–346. Springer, Heidelberg 
(2010)

62. Huﬀman, D.: A method for the construction of minimum-redundancy codes. Proceedings 
of the I.R.E. 40(9), 1090–1101 (1952)

63. Jacobson, G.: Space-eﬃcient static trees and graphs. In: Proc. 30th FOCS, pp.

549–554 (1989)

64. J´aJ´a, J., Mortensen, C.W., Shi, Q.: Space-Eﬃcient and Fast Algorithms for Multidimensional 
Dominance Reporting and Counting. In: Fleischer, R., Trippen, G.
(eds.) ISAAC 2004. LNCS, vol. 3341, pp. 558–568. Springer, Heidelberg (2004)

Wavelet Trees for All

25

65. Jørgensen, A.G., Larsen, K.D.: Range selection and median: Tight cell probe lower
bounds and adaptive data structures. In: Proc. 22nd SODA, pp. 805–813 (2011)
66. K¨arkk¨ainen, J.: Repetition-Based Text Indexing. PhD thesis, Univ. of Helsinki,

Finland (1999)

67. K¨arkk¨ainen, J., Puglisi, S.J.: Fixed Block Compression Boosting in FM-Indexes.
In: Grossi, R., Sebastiani, F., Silvestri, F. (eds.) SPIRE 2011. LNCS, vol. 7024,
pp. 174–184. Springer, Heidelberg (2011)

68. Kreft, S., Navarro, G.: Self-indexing Based on LZ77. In: Giancarlo, R., Manzini,

G. (eds.) CPM 2011. LNCS, vol. 6661, pp. 41–54. Springer, Heidelberg (2011)

69. Larsson, J., Moﬀat, A.: Oﬀ-line dictionary-based compression. Proceedings of the

IEEE 88(11), 1722–1732 (2000)

70. M¨akinen, V., Navarro, G.: New search algorithms and time/space tradeoﬀs for
succinct suﬃx arrays. Tech. Rep. C-2004-20, Univ. of Helsinki, Finland (April
2004)

71. M¨akinen, V., Navarro, G.: Succinct suﬃx arrays based on run-length encoding.

Nordic J. Comp. 12(1), 40–66 (2005)

72. M¨akinen, V., Navarro, G.: Position-Restricted Substring Searching. In: Correa,
J.R., Hevia, A., Kiwi, M. (eds.) LATIN 2006. LNCS, vol. 3887, pp. 703–714.
Springer, Heidelberg (2006)

73. M¨akinen, V., Navarro, G.: Implicit Compression Boosting with Applications
to Self-indexing. In: Ziviani, N., Baeza-Yates, R. (eds.) SPIRE 2007. LNCS,
vol. 4726, pp. 229–241. Springer, Heidelberg (2007)

74. M¨akinen, V., Navarro, G.: Rank and select revisited and extended. Theor. Comp.

Sci. 387(3), 332–347 (2007)

75. M¨akinen, V., Navarro, G., Sir´en, J., V¨alim¨aki, N.: Storage and retrieval of highly

repetitive sequence collections. J. Comp. Biol. 17(3), 281–308 (2010)

76. Manber, U., Myers, G.: Suﬃx arrays: a new method for on-line string searches.

SIAM J. Comp. 22(5), 935–948 (1993)

77. Manzini, G.: An analysis of the Burrows-Wheeler transform. J. ACM 48(3), 407–

430 (2001)

78. McCreight, E.: A space-economical

suﬃx tree construction algorithm. J.

ACM 23(2), 262–272 (1976)

79. Munro, I.: Tables. In: Chandru, V., Vinay, V. (eds.) FSTTCS 1996. LNCS,

vol. 1180, pp. 37–42. Springer, Heidelberg (1996)

80. Muthukrishnan, S.: Eﬃcient algorithms for document retrieval problems. In: Proc.

13th SODA, pp. 657–666 (2002)

81. Navarro, G.: Indexing text using the Ziv-Lempel trie. J. Discr. Alg. 2(1), 87–114

(2004)

82. Navarro, G., M¨akinen, V.: Compressed full-text indexes. ACM Comp. Surv. 39(1),

article 2 (2007)

83. Navarro, G., Nekrich, Y., Russo, L.: Space-eﬃcient data-analysis queries on grids.

CoRR, abs/1106.4649v2 (2012)

84. Navarro, G., Providel, E.: Fast, small, simple rank/select on bitmaps. In: Proc.

11th SEA (to appear, 2012)

85. Navarro, G., Puglisi, S.J.: Dual-Sorted Inverted Lists. In: Chavez, E., Lonardi, S.

(eds.) SPIRE 2010. LNCS, vol. 6393, pp. 309–321. Springer, Heidelberg (2010)

86. Navarro, G., Puglisi, S.J., Valenzuela, D.: Practical Compressed Document Retrieval.
 In: Pardalos, P.M., Rebennack, S. (eds.) SEA 2011. LNCS, vol. 6630, pp.
193–205. Springer, Heidelberg (2011)

26

G. Navarro

87. Navarro, G., Russo, L.M.S.: Space-Eﬃcient Data-Analysis Queries on Grids. In:
Asano, T., Nakano, S.-i., Okamoto, Y., Watanabe, O. (eds.) ISAAC 2011. LNCS,
vol. 7074, pp. 323–332. Springer, Heidelberg (2011)

88. Navarro, G., Sadakane, K.: Fully-functional static and dynamic succinct trees.

CoRR, abs/0905.0768v5 (2010)

89. Okanohara, D., Sadakane, K.: Practical entropy-compressed rank/select dictionary.
 In: Proc. 9th ALENEX (2007)

90. P˘atra¸scu, M.: Lower bounds for 2-dimensional range counting. In: Proc. 39th

STOC, pp. 40–46 (2007)

91. P˘atra¸scu, M.: Succincter. In: Proc. 49th FOCS, pp. 305–313 (2008)
92. P˘atra¸scu, M., Viola, E.: Cell-probe lower bounds for succinct partial sums. In:

Proc. 21st SODA, pp. 117–122 (2010)

93. Raman, R., Raman, V., Rao, S.: Succinct indexable dictionaries with applications
to encoding k-ary trees and multisets. In: Proc. 13th SODA, pp. 233–242 (2002)
94. Schnattinger, T., Ohlebusch, E., Gog, S.: Bidirectional Search in a String with
Wavelet Trees. In: Amir, A., Parida, L. (eds.) CPM 2010. LNCS, vol. 6129, pp.
40–50. Springer, Heidelberg (2010)

95. Sir´en, J., V¨alim¨aki, N., M¨akinen, V., Navarro, G.: Run-Length Compressed Indexes 
Are Superior for Highly Repetitive Sequence Collections. In: Amir, A.,
Turpin, A., Moﬀat, A. (eds.) SPIRE 2008. LNCS, vol. 5280, pp. 164–175. Springer,
Heidelberg (2008)

96. Tischler, G.: On Wavelet Tree Construction. In: Giancarlo, R., Manzini, G. (eds.)

CPM 2011. LNCS, vol. 6661, pp. 208–218. Springer, Heidelberg (2011)

97. V¨alim¨aki, N., M¨akinen, V.: Space-Eﬃcient Algorithms for Document Retrieval.
In: Ma, B., Zhang, K. (eds.) CPM 2007. LNCS, vol. 4580, pp. 205–215. Springer,
Heidelberg (2007)

98. Weiner, P.: Linear pattern matching algorithm. In: Proc. 14th Annual IEEE Symposium 
on Switching and Automata Theory, pp. 1–11 (1973)

99. Witten, I., Moﬀat, A., Bell, T.: Managing Gigabytes, 2nd edn. Morgan Kaufmann

(1999)

100. Yu, C.-C., Hon, W.-K., Wang, B.-F.: Eﬃcient Data Structures for the Orthogonal
Range Successor Problem. In: Ngo, H.Q. (ed.) COCOON 2009. LNCS, vol. 5609,
pp. 96–105. Springer, Heidelberg (2009)

