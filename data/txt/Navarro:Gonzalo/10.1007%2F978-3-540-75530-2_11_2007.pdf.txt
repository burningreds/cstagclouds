A Fast and Compact Web Graph Representation(cid:2)

Francisco Claude and Gonzalo Navarro

Department of Computer Science, Universidad de Chile

{fclaude,gnavarro}@dcc.uchile.cl

Abstract. Compressed graphs representation has become an attractive
research topic because of its applications in the manipulation of huge
Web graphs in main memory. By far the best current result is the technique 
by Boldi and Vigna, which takes advantage of several particular
properties of Web graphs. In this paper we show that the same properties 
can be exploited with a diﬀerent and elegant technique, built on
Re-Pair compression, which achieves about the same space but much
faster navigation of the graph. Moreover, the technique has the potential 
of adapting well to secondary memory. In addition, we introduce
an approximate Re-Pair version that works eﬃciently with limited main
memory.

1 Introduction

A compressed data structure, besides answering the queries supported by its classical 
(uncompressed) counterpart, uses little space for its representation. Nowadays 
this kind of structures is receiving much attention because of two reasons:
(1) the enormous amounts of information digitally available, (2) the ever-growing
speed gaps in the memory hierarchy. As an example of the former, the graph of
the static indexable Web was estimated in 2005 to contain more than 11.5 billion
nodes [12] and more than 150 billion links. A plain adjacency list representation
would need around 600 GB. As an example of (2), access time to main memory
is about one million times faster than to disk. Similar phenomena arise at other
levels of memory hierarchy. Although memory sizes have been growing fast, new
applications have appeared with data management requirements that exceeded
the capacity of the faster memories. Because of this scenario, it is attractive to
design and use compressed data structures, even if they are several times slower
than their classical counterpart. They will run much faster anyway if they ﬁt in
a faster memory.

In this scenario, compressed data structures for graphs have suddenly gained
interest in recent years, because a graph is a natural model of the Web structure.
Several algorithms used by the main search engines to rank pages, discover communities,
 and so on, are run over those Web graphs. Needless to say, relevant Web
graphs are huge and maintaining them in main memory is a challenge, especially
if we wish to access them in compressed form, say for navigation purposes.

(cid:2) Partially funded by a grant from Yahoo! Research Latin America.

N. Ziviani and R. Baeza-Yates (Eds.): SPIRE 2007, LNCS 4726, pp. 118–129, 2007.
c(cid:2) Springer-Verlag Berlin Heidelberg 2007

A Fast and Compact Web Graph Representation

119

As far as we know, the best results in practice to compress Web graphs such
that they can be navigated in compressed form are those of Boldi and Vigna [6].
They exploit several well-known regularities of Web graphs, such as their skewed
inand 
out-degree distributions, repetitiveness in the sets of outgoing links, and
locality in the references. For this sake they resort to several ad-hoc mechanisms
such as node reordering, diﬀerential encoding, compact interval representations
and references to similar adjacency lists.

In this paper we present a new way to take advantage of the regularities that
appear in Web graphs. Instead of diﬀerent ad-hoc techniques, we use a uniform
and elegant technique called Re-Pair [19] to compress the adjacency lists. As the
original linear-time Re-Pair compression requires much main memory, we develop
an approximate version that adapts to the available space and can smoothly work
on secondary memory thanks to its sequential access pattern. This method can
be of independent interest. Our experimental results over diﬀerent Web crawls
show that our method achieves space comparable to that of Boldi and Vigna,
yet our navigation is several times faster.

2 Related Work

Let us consider graphs G = (V, E), where V is the set of vertices and E is
the set of edges. We call n = |V | and e = |E| in this paper. Standard graph
representations such as the incidence matrix and the adjacency list require n(n−
1)/2 and 2e log n bits, respectively, for undirected graphs. For directed graphs
the numbers are n2 and e log n, respectively1. We call the neighbors of a node
v ∈ V those u ∈ V such that (v, u) ∈ E.

The oldest work on graph compression focuses on undirected unlabeled graphs.
The ﬁrst result we know of [30] shows that planar graphs can be compressed into
O(n) bits. The constant factor was later improved [17], and ﬁnally a technique
yielding the optimal constant factor was devised [14]. Results on planar graphs
can be generalized to graphs with constant genus [20]. More generally, a graph
with genus g can be compressed into O(g + n) bits [10]. The same holds for
a graph with g pages. A page is a subgraph whose nodes can be written in a
linear layout so that its edges do not cross. Edges of a page hence form a nested
structure that can be represented as a balanced sequence of parentheses.

Some classes of planar graphs have also received special attention, for example
trees, triangulated meshes, triconnected planar graphs, and others [15,17,13,28].
For dense graphs, it is shown that little can be done to improve the space required
by the adjacency matrix [23].

The above techniques consider just the compression of the graph, not its
access in compressed form. The ﬁrst compressed data structure for graphs we
know of [16] requires O(gn) bits of space for a g-page graph. The neighbors of
a node can be retrieved in O(log n) time each (plus an extra O(g) complexity
for the whole query). The main idea is again to represent the nested edges using
parentheses, and the operations are supported using succinct data structures
1 In this paper logarithms are in base 2.

120

F. Claude and G. Navarro

that permit navigating a sequence of balanced parentheses. The retrieval was
later improved to constant time by using improved parentheses representations
[22], and also the constant term of the space complexity was improved [9]. The
representation also permits ﬁnding the degree (number of neighbors) of a node,
as well as testing whether two nodes are connected or not, in O(g) time.

All those techniques based on number of pages are unlikely to scale well to
more general graphs, in particular to Web graphs. A more powerful concept that
applies to this type of graph is that of graph separators. Although the separator
concept has been used a few times [10,14,8] (yet not supporting access to the
compressed graph), the most striking results are achieved in recent work [5,4].
Their idea is to ﬁnd graph components that can be disconnected from the rest
by removing a small number of edges. Then, the nodes within each component
can be renumbered to achieve smaller node identiﬁers, and only a few external
edges must be represented.

They [4] apply the separator technique to design a compressed data structure
that gives constant access time per delivered neighbor. They carefully implement
their techniques and experiment on several graphs. In particular, on a graph of 1
million (1M) nodes and 5M edges from the Google programming contest2, their
data structures require 13–16 bits per edge (bpe), and work faster than a plain
uncompressed representation using arrays for the adjacency lists. It is not clear
how these results would scale to larger graphs, as much of their improvement
relies on smart caching, and this eﬀect should vanish with real Web graphs.

There is also some work speciﬁcally aimed at compression of Web graphs
[7,1,29,6]. In this graph the (labeled) nodes are Web pages and the (directed)
edges are the hyperlinks. Several properties of Web graphs have been identiﬁed
and exploited to achieve compression:

Skewed distribution: The indegrees and outdegrees of the nodes distribute
according to a power law, that is, the probability that a page has i links is
1/iθ for some parameter θ > 0. Several experiments give rather consistent
values of θ = 2.1 for incoming and θ = 2.72 for outgoing links [2,7].

Locality of reference: Most of the links from a site point within the site. This
motivates in [3] the use of lexicographical URL order to list the pages, so
that outgoing links go to nodes whose position is close to that of the current
node. Gap encoding techniques are then used to encode the diﬀerences among
consecutive target node positions.

Similarity of adjacency lists: Nodes close in URL lexicographical order share
many outgoing links [18,6]. This permits compressing them by a reference to
the similar list plus a list of edits. Moreover, this translates into source nodes
pointing to a given target node forming long intervals of consecutive numbers,
which again permits easy compression.

In [29] they partition the adjacency lists considering popularity of the nodes, and
use diﬀerent coding methods for each partition. A more hierarchical view of the
nodes is exploited in [26]. In [1,27] they take explicit advantage of the similarity

2 www.google.com/programming-contest, not anymore available.

A Fast and Compact Web Graph Representation

121

property. A page with similar outgoing links is identiﬁed with some heuristic, and
then the current page is expressed as a reference to the similar page plus some
edit information to express the deletions and insertions to obtain the current
page from the referenced one. Finally, probably the best current result is from
[6], who build on previous work [1,27] and further engineer the compression to
exploit the properties above.

Experimental ﬁgures are not always easy to compare, but they give a reasonable 
idea of the practical performance. Over a graph with 115M nodes and
1.47 billion (1.47G) edges from the Internet Archive, [29] require 13.92 bpe (plus
around 50 bits per node, bpn). In [27], over a graph of 61M nodes and 1G edges,
they achieve 5.07 bpe for the graph. In [1] they achieve 8.3 bpe (no information
on bpn) over TREC-8 Web track graphs (WT2g set), yet they cannot access the
graph in compressed form. In [7] they require 80 bits per node plus 27.2 bpe
(and can answer reverse neighbor queries as well).

By far the best ﬁgures are from [6]. For example, they achieve space close to 3
bpe to compress a graph of 118M nodes and 1G link from WebBase3. This space,
however, is not suﬃcient to access the graph in compressed form. An experiment
including the extra information required for navigation is carried out on a graph
of 18.5M nodes and 292M links, where they need 6.7 bpe to achieve access times
below the microsecond. Those access times are of the same order of magnitude
of other representations [29,26,27]. For example, the latter reports times around
300 nanoseconds per delivered edge.

A recent proposal [24] advocates regarding the adjacency list representation
as a text sequence and use compressed text indexing techniques [25], so that
neighbors can be obtained via text decompression and reverse neighbors via
text searching. The concept and the results are interesting but not yet suﬃciently
competitive with those of [6].

3 Re-Pair and Our Approximate Version

Re-Pair [19] is a phrase-based compressor that permits fast and local decompression.
 It consists of repeatedly ﬁnding the most frequent pair of symbols in a
sequence of integers and replacing it with a new symbol, until no more replacements 
are convenient. This technique was recently used in [11] for compressing
suﬃx arrays. More precisely, Re-Pair over a sequence T works as follows:

1. It identiﬁes the most frequent pair ab in T
2. It adds the rule s → ab to a dictionary R, where s is a new symbol that does

not appear in T .

3. It replaces every occurrence of ab in T by s.4
4. It iterates until every pair in T appears once.

Let us call C the resulting text (i.e., T after all the replacements). It is easy
to expand any symbol c from C in time linear on the expanded data (that is,

3 www-diglib.stanford.edu/~testbed/doc2/WebBase/
4 As far as possible, e.g. one cannot replace both occurrences of aa in aaa.

c

F. Claude and G. Navarro

122
optimal): We expand c using rule c → c
(cid:3)(cid:3) in R, and continue recursively with
(cid:3)(cid:3), until we obtain the original symbols of T . In [11] they propose a
(cid:3) and c
c
new way of compressing the dictionary R which further reduces the space. This
compression makes it worthy to replace pairs that appear just twice in the text.
Despite its quadratic appearance, Re-Pair can be implemented in linear time
[19]. However, this requires several data structures to track the pairs that must
be replaced. This is usually problematic when applying it to large sequences, see
for example [31]. Indeed, it was also a problem when using it over suﬃx arrays
[11], where a couple of techniques were proposed to run with restricted memory.
A ﬁrst one was an O(n log n) time exact method (meaning that it obtained the
same result as the algorithm as described), which was in practice extremely slow.
A second one was an approximate algorithm (which does not always choose the
most frequent pair to replace), which was much faster and lost some compression,
yet it applies only to suﬃx arrays.

(cid:3)

We present now an alternative approximate method that (1) works on any
sequence, (2) uses as little memory as desired on top of T , (3) given an extra
memory to work, can trade accurateness for speed, (4) is able to work smoothly
on secondary memory due to its sequential access pattern.
We describe the method assuming we have M > |T| main memory available.
If this is not the case, we can anyway run the algorithm by maintaining T on disk
since, as explained, we will access it sequentially (performing several passes).
We place T inside the bigger array of size M, and use the remaining space
as a (closed) hash table H of size |H| = M − |T|. Table H stores unique pairs
of symbols ab occurring in T , and a counter of the number of occurrences it
has in T . The key ab is represented as a single integer by its position in T (any
occurrence works). We traverse T = t1t2 . . . sequentially and insert all the pairs
titi+1 into H. If, at some point, the table surpasses a load factor 0 < α < 1,
we do not insert new pairs anymore, yet we keep traversing of T to increase the
counters of already inserted pairs.
After the traversal is completed, we scan H and retain the k most frequent
pairs from it, for some parameter k ≥ 1. A heap of k integers is suﬃcient for this
purpose. Those pairs will be simultaneously replaced in a second pass over T .
For this sake we must consider that some replacements may invalidate others,
for example we cannot replace both ab and bc in abc. Some pairs can have so
many occurrences invalidated that they are not worthy of replacement anymore
(especially at the end, when even the most frequent pairs occur a few times).

The replacement proceeds as follows. We empty H and insert only the k pairs
to be replaced. This time we associate to each pair a ﬁeld pos, the position of its
ﬁrst occurrence in T . This value is null if we have not yet seen any occurrence
in this second pass, and proceed if we have already started replacing it. We now
scan T and use H to identify pairs that must be replaced. If pair ab is in H and
its pos value is proceed, we just replace ab by sz, where s is the new symbol for
pair ab and z is an invalid entry. If, instead, pair ab already has a ﬁrst position
recorded in pos, and we read this position in T and it still contains ab (after
possible replacements that occurred after we saw that position), then we make

A Fast and Compact Web Graph Representation

123

both replacements and set the pos value to proceed. Otherwise, we set the pos
value of pair ab to the current occurrence we are processing. This method ensures
that we create no new symbols s that will appear just once in T .
After this replacement, we compact T by deleting all the z entries, and restart
the process. As now T is smaller, we can have a larger hash table of size |H| =
M − |T|. The traversal of T , regarded as a circular array, will now start at the
point where we stopped inserting pairs in H in the previous stage, to favor a
uniform distribution of the replacements.

Assume the exact method carries out r replacements. This approximate
method can carry out r replacements (achieving hopefully similar compression)
in time O((cid:5)r/k(cid:6)(n + h log k)) average time, where h = |H| = O(n). Thus we
can trade time for accurateness by tuning k. This analysis, however, is approximate,
 as some replacements could be invalidated by others and thus we cannot
guarantee that we carry out k of them per round. Yet, the analysis is useful to
explain the space/time tradeoﬀ involved in the choice of k.

Note that even k = 1 does not guarantee that the algorithm works exactly as
Re-Pair, as we might not have space to store all the diﬀerent pairs in H. In this
respect, it is interesting that we become more accurate (thanks to a larger H)
for the later stages of the algorithm, as by that time the frequency distribution
is ﬂatter and more precision is required to identify the best pairs to replace.
As explained, the process works well on disk too. This time T is on disk and
table H occupies almost all the main memory, |H| ≈ M. In H we do not store
the position of pair ab but instead ab explicitly, to avoid random accesses to
T . The other possible random access is the one where we check pos, the ﬁrst
occurrence of a pair ab, when replacing T . Yet, we note that there are at most
k positions in T needing random access at any time, so a buﬀer of k disk pages
totally avoids the cost of those accesses. That is, we maintain in main memory
the disk blocks containing the position pos of each pair to be replaced. When pos
changes we can discard the disk block from main memory and retain the new
one instead (which is the block we are processing). It is possible, however, that
we have to write back those pages to disk, but this occurs only when we replace
the ﬁrst occurrence of a pair ab, that is, when pos changes from a position to
the value proceed. This occurs at most k times per stage.
Thus the worst-case I/O cost of this algorithm is O((cid:5)r/k(cid:6)(n/B + k)) =
O((cid:5)r/k(cid:6) n/B + r + k), where B is the disk block size (again, this is is an ap-
proximation).

4 A Compressed Graph Representation

Let G = (V, E) be the graph we wish to compress and navigate. Let V =
{v1, v2, . . . , vn} be the set of nodes in arbitrary order, and adj(vi) = {vi,1, vi,2, . . .
} the set of neighbors of node vi. Finally, let vi be an alternative identiﬁer
vi,ai
for node vi. We represent G by the following sequence:

T = T (G) = v1 v1,1 v1,2 . . . v1,a1 v2 v2,1 v2,2 . . . v2,a2 . . . vn vn,1 vn,2 . . . v1,an

F. Claude and G. Navarro

124
so that vi,j < vi,j+1 for any 1 ≤ i ≤ n, 1 ≤ j < ai. This is essentially the
concatenation of all the adjacency lists with separators that indicate the node
each list belongs to.

Applying Re-Pair to this representation T (G) has several advantages:

– Re-Pair permits fast local decompression, as it is a matter of extracting
successive symbols from C (the compressed T ) and expanding them using
the dictionary of rules R.

– This works also very well if T (G) must be anyway stored in secondary memory 
because the accesses to C are local and sequential, and moreover we access 
fewer disk blocks because it is a compressed version of T . This requires,
however, that R ﬁts in main memory, which can be forced at compression
time, at the expense of losing some compression ratio.

– As the symbols vi are unique in T , they will not be replaced by Re-Pair. This
guarantees that the beginning of the adjacency list of each vi will start at a
new symbol in C, so that we can decompress it in optimal time O(|adj(vj)|)
without decompressing unnecessary symbols.

– If there are similar adjacency lists, Re-Pair will spot repeated pairs, therefore
capturing them into shorter sequences in C. Actually, assume adj(vi) =
adj(vj). Then Re-Pair will end up creating a new symbol s which, through
several rules, will expand to adj(vi) = adj(vj). In C, the text around those
nodes will read visvi+1 . . . vjsvj+1. Even if those symbols do not appear
elsewhere in T (G), the compression method for R developed in [11] will
represent R using |adj(vi)| numbers plus 1 + |adj(vi)| bits. Therefore, in
practice we are paying almost the same as if we referenced one adjacency
list from the other. Thus we achieve, with a uniform technique, the result
achieved in [6] by explicit techniques such as looking for similar lists in an
interval of nearby nodes.

– Even when the adjacency lists are not identical, Re-Pair can take partial
advantage of their similarity. For example, if we have abcde and abde, Re-
(cid:3), respectively. Again, we obtain
Pair can transform them to scs
automatically what in [6] is done by explicitly encoding the diﬀerences using
bitmaps and other tools.

(cid:3) and ss

– The locality property is not exploited by Re-Pair, unless its translates into
similar adjacency lists. This, however, makes our technique independent of
the numbering. In [6] it is essential to be able of renumbering the nodes
according to site locality. Despite this is indeed a clever numbering for other
reasons, it is possible that renumbering is forbidden if the technique is used
inside another application. However, we show next a way to exploit locality.

The representation T (G) we have described is useful for reasoning about the
compression performance, but it does not give an eﬃcient method to know where
a list adj(vi) begins. For this sake, after compressing T (G) with Re-Pair, we
remove all the symbols vi from the compressed sequence C (as explained, those
symbols must remain unaltered in C). Using exactly the same space we have
gained with this removal, we create a table that, for each node vi, stores a

A Fast and Compact Web Graph Representation

125

pointer to the beginning of the representation of adj(vi) in C. With it, we can
obtain adj(vi) in optimal time for any vi.

If we are allowed to renumber the nodes, we can exploit the locality property
in a subtle way. We let the nodes be ordered and numbered by their URL,
and every adjacency list encoded using diﬀerential encoding. The ﬁrst value is
absolute and the rest represents the diﬀerence to the previous value. For example
the list 4 5 8 9 11 12 13 is encoded as 4 1 3 1 2 1 1.

Diﬀerential encoding is usually a previous step to represent small numbers
with fewer bits. We do not want to do this as it hampers decoding speed. Our
main idea to exploit diﬀerential encoding is that, if every node tends to have local
links, there will be many small diﬀerences we could exploit with Re-Pair, say
pairs like (1, 1), (1, 2), (2, 1), etc. We also present results for this variant and show
that the compression is improved at the expense of some extra decompression.

5 Experimental Results

The experiments were run on a Pentium IV 3.0 GHz with 4GB of RAM using
Gentoo GNU/Linux with kernel 2.6.13 and g++ with -O9 and -DNDEBUG options.
The compression ratios r we show are the compressed ﬁle size as a percentage
of the uncompressed ﬁle size.

We ﬁrst study the performance of our approximate technique described in
Section 3, as compared to the original technique. Table 1 shows the results for
diﬀerent M (amount of main memory for construction) and k parameters, over a
400 MB suﬃx array built from a 100 MB XML ﬁle (see [11]). The same ﬁle with the
method proposed in [11] achieves r = 20.08% after 7.22 hours. The approximate
version that works only over suﬃx arrays achieves 21.3% in 3 minutes.

As it can be seen, our approximate method obtains compression ratios reasonably 
close to the original one, while being practical in time and RAM for construction.
 In the following we only use the approximate method, as our graphs
are much larger and the exact method does not run.

We now study our graph compression proposal (Section 4). Fig. 1 shows the
results for four Web crawls, all downloaded from http://law.dsi.unimi.it/.
UK is a graph with 18,520,486 nodes and 298,113,762 edges, EU has 862,664 nodes
and 19,235,140 edges, Arabic has 22,744,080 nodes and 639,999,458 edges, and
Indochina has 7,414,866 nodes and 194,109,311 edges.

Table 1. Compression ratios and times for diﬀerent memory usage for construction
M, and parameter k

r % time(min)

k

M (MB)

k

10,000,000
5,000,000
1,000,000
500,000
100,000

M (MB)
1126
930
840
821
805

23.41%
23.25%
22.68%
22.44%
22.03%

22 10,000,000
13
5,000,000
1,000,000
13
500,000
18
59
100,000

r % time(min)
21
12
14
21
67

891 27.40%
763 27.86%
611 29.42%
592 28.30%
576 30.49%

126

F. Claude and G. Navarro

 0.0005

 0.0004

 0.0003

 0.0002

 0.0001

)
e
g
d
e

/

g
e
s
m

(
 

e
m

i
t

 0

 0

 2

 4

 0.0005

 0.0004

 0.0003

 0.0002

)
e
g
d
e
g
e
s
m

/

(
 
e
m

i
t

 0.0001

 0

 0

 2

 4

 0.0005

 0.0004

 0.0003

 0.0002

 0.0001

)
e
g
d
e
g
e
s
m

/

(
 
e
m

i
t

UK

Re-Pair
Re-Pair (diffs)
Plain
Compact

UK

Re-Pair
Re-Pair (diffs)
BV
BV-Memory

 0.0005

 0.0004

 0.0003

 0.0002

 0.0001

)
e
g
d
e

/

g
e
s
m

(
 

e
m

i
t

 6

 8

space (bits/edge)

 10

 12

EU

 0

 0

 5

 10

 15

 20

space (bits/edge)

 25

 30

 35

EU

Re-Pair
Re-Pair (diffs)
Plain
Compact

Re-Pair
Re-Pair (diffs)
BV
BV-Memory

 0.0005

)
e
g
d
e
g
e
s
m

/

(
 
e
m

i
t

 0.0004

 0.0003

 0.0002

 0.0001

 6

 8

 10

space (bits/edge)

 12

 14

Arabic

 0

 0

 5

 10

 15

 20

space (bits/edge)

 25

 30

 35

Arabic

Re-Pair
Re-Pair (diffs)
Plain
Compact

Re-Pair
Re-Pair (diffs)
BV
BV-Memory

 0.0005

 0.0004

 0.0003

 0.0002

 0.0001

)
e
g
d
e
g
e
s
m

/

(
 
e
m

i
t

 0

 0

 2

 4
space (bits/edge)

 6

 8

 10

 0

 0

 5

 10

 15

 20

space (bits/edge)

 25

 30

 35

Indochina

Re-Pair
Re-Pair (diffs)
Plain
Compact

Indochina

Re-Pair
Re-Pair (diffs)
BV
BV-Memory

 0.0005

 0.0004

 0.0003

 0.0002

 0.0001

)
e
g
d
e
/
g
e
s
m

(
 
e
m

i
t

 0.0005

 0.0004

 0.0003

 0.0002

 0.0001

)
e
g
d
e
/
g
e
s
m

(
 
e
m

i
t

 0

 0

 2

 4

 6

space (bits/edge)

 8

 10

 0

 0

 5

 10

 15

 20

 25

 30

 35

space (bits/edge)

Fig. 1. Space and time to ﬁnd neighbors for diﬀerent graph representations, over different 
Web crawls. BV-Memory represents the minimum heap space needed by the
process to run.

A Fast and Compact Web Graph Representation

127

We show on the left side the behavior of our Re-Pair-based method with and
without diﬀerential encoding, compared to Boldi & Vigna’s implementation [6]
run on our machine with diﬀerent space/time tradeoﬀs. The space is measured in
bits per edge (bpe), where the total space cost is divided by the number of edges.
The implementation of Boldi & Vigna gives a bpe measure that is consistent
with the sizes of the generated ﬁles. However, their process (in Java) needs
more memory to run. This could suggest that they actually need to build more
structures that are not stored on the ﬁle, but this is diﬃcult to quantify because
of other space overheads that come from Java itself and from the WebGraph
framework their code is inside. To account for this, we draw a second line that
shows the minimum amount of RAM needed for their process to run. In all cases,
however, the times we show are obtained with the garbage collector disabled and
suﬃcient RAM to let the process achieve maximum speed. Although our own
code is in C++, we found that the Java compiler achieves very competitive
results (in unrelated tests over a similar code).

On the right side we compare our method with two fast uncompressed rep-
resentations: a plain one using 32-bit integers to represent the adjacency lists,
and a compact representation using (cid:5)log2 n(cid:6) bits for every link and (cid:5)log2 m(cid:6) for
every node (to point inside the adjacency list).
The results show that our method is a very competitive alternative to Boldi &
Vigna’s technique, which is currently the best by a wide margin for Web graphs.
In all cases, our method gives a comparable amount of space. Moreover, using
the same amount of space, our method is always faster (usually twice as fast,
even considering their best line). In addition, one of our versions does not impose
any particular node numbering.

Compared to an uncompressed graph representation, our method is also a
very interesting alternative. It is 3–5 times smaller than the compact version
and 2–3 times slower than it; and it is 4–6 times smaller than the the plain
version and 3–6 times slower. In particular, a graph like Arabic needs 2.4 GB of
RAM with a plain representation, whereas our compressed version requires only
420 MB of RAM. This can be easily manipulated in a normal 1 GB machine,
whereas the plain version would have to resort to disk.

6 Conclusions

We have presented a graph compression method that takes advantage of similarity 
between adjacency lists by using Re-Pair [19], a phrase-based compressor.
The results over diﬀerent Web crawls show that our method achieves compression 
ratios similar to the best current schemes [6], while being signiﬁcantly faster
to navigate the compressed graph. Our scheme adapts well to secondary memory,
where it can take fewer accesses to disk than its uncompressed counterpart for
navigation. In passing, we developed an eﬃcient approximate version of Re-Pair,
which also works well on secondary memory. Our work opens several interesting
lines for future work:

128

F. Claude and G. Navarro

1. More thorough experiments, considering also the case where the construction

and/or the navigation must access secondary memory.

2. Thorough exploration of the performance of our approximate Re-Pair method
by itself. Apart from studying it in more general scenarios, we are considering 
tuning it in diﬀerent ways. For example we could use a varying k across
the compression stages.

3. Further study of the compression format itself and tradeoﬀs. For example
it is possible to compress sequence C with a zero-order compressor (the
zero-order entropy of our C sequences tells that its size could be reduced
to 61%-77%), although expansion of symbols in C will be slower. Another
tradeoﬀ is obtained by replacing the vector of pointers from each vj to its list
in C by a bitmap of |C| bits that mark the beginning of the lists. A select(j)
operation (which gives the position of the j-th bit set [21]) over this bitmap
would give the position of adj(vj) in C. This is slower than a direct pointer
but will usually save space. For example we would save 1.0 bits/edge in the
UK crawl (estimated without an implementation).

4. Combine the current representation with the ideas advocated in [24], so as
to have a kind of self-index which, with some overhead over the current
representation, would be able of ﬁnding reverse neighbors and answer other
queries such as indegree and outdegree of a node, presence of a speciﬁc link,
and so on5.

References

1. Adler, M., Mitzenmacher, M.: Towards compressing Web graphs. In: Proc. IEEE

DCC, pp. 203–212. IEEE Computer Society Press, Los Alamitos (2001)

2. Aiello, W., Chung, F., Lu, L.: A random graph model for massive graphs. In: Proc.

ACM STOC, pp. 171–180. ACM Press, New York (2000)

3. Bharat, K., Broder, A., Henzinger, M., Kumar, P., Venkatasubramanian, S.: The
Connectivity Server: Fast access to linkage information on the web. In: Proc.
WWW, pp. 469–477 (1998)

4. Blandford, D.: Compact data structures with fast queries. PhD thesis, School of
Computer Science, Carnegie Mellon University, Also as TR CMU-CS-05-196 (2006)
5. Blandford, D., Blelloch, G., Kash, I.: Compact representations of separable graphs.

In: Proc. SODA, pp. 579–588 (2003)

6. Boldi, P., Vigna, S.: The webgraph framework I: compression techniques. In: Proc.

WWW, pp. 595–602 (2004)

7. Broder, A., Kumar, R., Maghoul, F., Raghavan, P., Rajagopalan, S., Stata, R.,
Tomkins, A., Wiener, J.: Graph structure in the web. J. Computer Networks 33(1–
6), 309–320 (2000) Also in Proc. WWW9

8. Chakrabarti, D., Papadimitriou, S., Modha, D., Faloutsos, C.: Fully automatic

cross-associations. In: Proc. ACM SIGKDD, ACM Press, New York (2004)

5 This line of work is in cooperation with P. Ferragina and R. Venturini, University of

Pisa.

A Fast and Compact Web Graph Representation

129

9. Chuang, R., Garg, A., He, X., Kao, M.-Y., Lu, H.-I.: Compact encodings of planar 
graphs with canonical orderings and multiple parentheses. In: Larsen, K.G.,
Skyum, S., Winskel, G. (eds.) ICALP 1998. LNCS, vol. 1443, pp. 118–129. Springer,
Heidelberg (1998)

10. Deo, N., Litow, B.: A structural approach to graph compression. In: Brim, L.,
Gruska, J., Zlatuˇska, J. (eds.) MFCS 1998. LNCS, vol. 1450, pp. 91–101. Springer,
Heidelberg (1998)

11. Gonz´alez, R., Navarro, G.: Compressed text indexes with fast locate. In: Ma, B.,
Zhang, K. (eds.) CPM 2007. LNCS, vol. 4580, pp. 216–227. Springer, Heidelberg
(2007)

12. Gulli, A., Signorini, A.: The indexable web is more than 11.5 billion pages. In:

Proc. WWW (2005)

13. He, X., Kao, M.-Y., Lu, H.-I.: Linear-time succinct encodings of planar graphs via

canonical orderings. J. Discrete Mathematics 12(3), 317–325 (1999)

14. He, X., Kao, M.-Y., Lu, H.-I.: A fast general methodology for informationtheoretically 
optimal encodings of graphs. SIAM J. Comput. 30, 838–846 (2000)

15. Itai, A., Rodeh, M.: Representation of graphs. Acta Informatica 17, 215–219 (1982)
16. Jacobson, G.: Space-eﬃcient static trees and graphs. In: Proc. FOCS, pp. 549–554

(1989)

17. Keeler, K., Westbook, J.: Short encodings of planar graphs and maps. Discrete

Applied Mathematics 58, 239–252 (1995)

18. Kumar, R., Raghavan, P., Rajagopalan, S., Tomkins, A.: Extracting large scale

knowledge bases from the Web. In: Proc. VLDB (1999)

19. Larsson, J., Moﬀat, A.: Oﬀ-line dictionary-based compression. Proc. IEEE 88(11),

1722–1732 (2000)

20. Lu, H.-I.: Linear-time compression of bounded-genus graphs into informationtheoretically 
optimal number of bits. In: Proc. SODA, pp. 223–224 (2002)

21. Munro, I.: Tables. In: Chandru, V., Vinay, V. (eds.) Foundations of Software Technology 
and Theoretical Computer Science. LNCS, vol. 1180, pp. 37–42. Springer,
Heidelberg (1996)

22. Munro, I., Raman, V.: Succinct representation of balanced parentheses, static trees

and planar graphs. In: Proc. FOCS, pp. 118–126 (1997)

23. Naor, M.: Succinct representation of general unlabeled graphs. Discrete Applied

Mathematics 28, 303–307 (1990)

24. Navarro, G.: Compressing web graphs like texts. Technical Report TR/DCC-20072,
 Dept. of Computer Science, University of Chile (2007)

25. Navarro, G., M¨akinen, V.: Compressed full-text indexes. ACM Computing Surveys 
39(1) article 2 (2007)

26. Raghavan, S., Garcia-Molina, H.: Representing Web graphs. In: Proc. ICDE (2003)
27. Randall, K., Stata, R., Wickremesinghe, R., Wiener, J.: The LINK database: Fast
access to graphs of the Web. Technical Report 175, Compaq Systems Research
Center, Palo Alto, CA (2001)

28. Rossignac, J.: Edgebreaker: Connectivity compression for triangle meshes. IEEE

Transactions on Visualization 5(1), 47–61 (1999)

29. Suel, T., Yuan, J.: Compressing the graph structure of the Web. In: Proc. IEEE

DCC, pp. 213–222. IEEE Computer Society Press, Los Alamitos (2001)

30. Tur´an, G.: Succinct representations of graphs. Discrete Applied Mathematics 8,

289–294 (1984)

31. Wan, R.: Browsing and Searching Compressed Documents. PhD thesis, Dept. of

Computer Science and Software Engineering, University of Melbourne (2003)

