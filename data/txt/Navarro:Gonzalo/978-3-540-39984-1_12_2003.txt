SCM: Structural Contexts Model for Improving
Compression in Semistructured Text Databases(cid:1)

Joaqu´ın Adiego1, Gonzalo Navarro2, and Pablo de la Fuente1

1 Departamento de Inform´atica, Universidad de Valladolid, Valladolid, Espa˜na.

{jadiego, pfuente}@infor.uva.es

2 Departamento de Ciencias de la Computaci´on,

Universidad de Chile, Santiago, Chile.

gnavarro@dcc.uchile.cl

Abstract. We describe a compression model for semistructured documents,
 called Structural Contexts Model, which takes advantage of the
context information usually implicit in the structure of the text. The
idea is to use a separate semiadaptive model to compress the text that
lies inside each diﬀerent structure type (e.g., diﬀerent XML tag). The
intuition behind the idea is that the distribution of all the texts that belong 
to a given structure type should be similar, and diﬀerent from that
of other structure types. We test our idea using a word-based Huﬀman
coding, which is the standard for compressing large natural language
textual databases, and show that our compression method obtains signiﬁcant 
improvements in compression ratios. We also analyze the possibility 
that storing separate models may not pay oﬀ if the distribution of
diﬀerent structure types is not diﬀerent enough, and present a heuristic
to merge models with the aim of minimizing the total size of the compressed 
database. This technique gives an additional improvement over
the plain technique. The comparison against existing prototypes shows
that our method is a competitive choice for compressed text databases.
Finally, we show how to apply SCM over text chunks, which allows one
to adjust the diﬀerent word frequencies as they change across the text
collection.

Keywords: Text Compression, Compression Model, Semistructured Documents.


1 Introduction

Compression of large document collections not only reduces the amount of disk
space occupied by the data, but it also decreases the overall query processing
time in text retrieval systems. Improvements in processing times are achieved
thanks to the reduced disk transfer times necessary to access the text in compressed 
form. Since in the last decades processor speeds have increased much

(cid:1) This work was partially supported by CYTED VII.19 RIBIDI project (all authors)

and Fondecyt Project 1-020831 (second author).

M.A. Nascimento, E.S. de Moura, A.L. Oliveira (Eds.): SPIRE 2003, LNCS 2857, pp. 153–167, 2003.
c(cid:1) Springer-Verlag Berlin Heidelberg 2003

154

Joaqu´ın Adiego, Gonzalo Navarro, and Pablo de la Fuente

faster than disk transfer speeds, trading disk transfer times by processor decompression 
times has become a better and better choice. Moreover, recent research
on “direct” compressed text searching, i.e., searching a compressed text without
decompressing it, has led to a win-win situation where the compressed text takes
less space and is searched faster than the plain text [WMB99, ZMNBY00].

Compressed text databases pose some requirements that outrule some compression 
methods. The most deﬁnitive is the need for random access to the text
without the possibility of decompressing it from the beginning. This rules out
most adaptive compression methods such as Ziv-Lempel compression and arithmetic 
coding. On the other hand, semiadaptive models such as Huﬀman [Huf52]
yield poor compression. In the case of compressing natural language texts, it has
been shown that an excellent choice is to consider the words, not the characters,
as the source symbols [Mof89]. Thanks to the biased distribution of words, the
use of this model joined to a Huﬀman coder gives compression ratios close to
25%, much better than those usually obtained with the best adaptive methods.
These results are barely aﬀected if one switches to byte-oriented Huﬀman coding,
 where each source symbol is coded as a sequence of bytes instead of bits.
Although compression ratios raise to 30% (which is still competitive), we have
in exchange much faster decoding and searching, which are essential features for
compressed text databases. Finally, the fact that the alphabet and the vocabulary 
of the text collections coincide permits eﬃcient and highly sophisticated
searching, both in the form of sequential searching and in the form of compressed
inverted indexes over the text [WMB99, ZMNBY00, NMN+00, MNZB00].

Although the area of natural language compressed text databases has gone
a long way since the end of the eighties, it is interesting that little has been
done about considering the structure of the text in this picture. Thanks to
the widespread acceptance of SGML, HTML and XML as the standards for
storing, exchanging and presenting documents, semistructured text databases
are becoming the standard. Some techniques to exploit the text structure have
been proposed, such as XMill [LS00] and XMLPPM [Che01]. However, these are
not designed to permit searching the text. Others, like XGrind [TH02], permit
searching but do not take advantage of the structure (they just allow it).

Our goal in this paper is to explore the possibility of considering the text
structure in the context of a compressed text database. We aim at taking advantage 
of the structure, while still retaining all the desirable features of a wordbased 
Huﬀman compression over a semiadaptive model. An idea like that of
XMLPPM, where the context given by the path in the structure tree is used
to model the text in the subtree, is based on the intuition that the text under
similar structural elements (i.e., XML tags) should follow a similar distribution.
(In fact XMLPPM uses the full path, which is more powerful.) Although this
compression is adaptive and does not ﬁt our search purposes, a simpliﬁcation
where only the last element in the path is considered can be joined to a semiadaptive 
model, which is suitable for searching. The idea is then to use separate
semiadaptive models to compress the text that lies inside diﬀerent tags. For ex-

