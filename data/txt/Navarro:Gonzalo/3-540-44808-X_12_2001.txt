A Probabilistic Spell

for the Curse of Dimensionality(cid:1)

Edgar Ch´avez1 and Gonzalo Navarro2

1 Escuela de Ciencias F´ısico-Matem´aticas,

Univ. Michoacana, Morelia, Mich. M´exico. elchavez@zeus.ccu.umich.mx.

2 Dept. of Computer Science,

Univ. of Chile, Santiago, Chile. gnavarro@dcc.uchile.cl.

Abstract. Range searches in metric spaces can be very diﬃcult if the
space is “high dimensional”, i.e. when the histogram of distances has
a large mean and/or a small variance. This so-called “curse of dimen-
sionality”, well known in vector spaces, is also observed in metric spaces.
There are at least two reasons behind the curse of dimensionality: a large
search radius and/or a high intrinsic dimension of the metric space. We
present a general probabilistic framework based on stretching the triangle 
inequality, whose direct eﬀect is a reduction of the eﬀective search
radius. The technique gets more eﬀective as the dimension grows, and
the basic principle can be applied to any search algorithm. In this paper
we apply it to a particular class of indexing algorithms. We present an
analysis which helps understand the process, as well as empirical evidence 
showing dramatic improvements in the search time at the cost of
a very small error probability.

1 Introduction

The concept of “proximity” searching has applications in a vast number of ﬁelds.
Some examples are multimedia databases, data mining, machine learning and
classiﬁcation, image quantization and compression, text retrieval, computational
biology and function prediction, just to name a few.
All those applications have some common characteristics. There is a universe
X of objects, and a nonnegative distance function d : X × X −→ R
+ deﬁned
among them. This distance satisﬁes the three axioms that make the set a metric
space: strict positiveness (d(x, y) = 0 ⇔ x = y), symmetry (d(x, y) = d(y, x))
and triangle inequality (d(x, z) ≤ d(x, y) + d(y, z)). This distance is considered
expensive to compute (think, for instance, in comparing two ﬁngerprints). We
have a ﬁnite database U ⊆ X, of size n, which is a subset of the universe of
objects. The goal is to preprocess the database U to quickly answer (i.e. with
as few distance computations as possible) range queries and nearest neighbor

(cid:1) This work has been partially supported by CYTED VII.13 AMYRI Project (both
authors), CONACyT grant R-28923A (ﬁrst author) and FONDECYT Project 1000929 
(second author).

A.L. Buchsbaum and J. Snoeyink (Eds.): ALENEX 2001, LNCS 2153, pp. 147–160, 2001.
c(cid:1) Springer-Verlag Berlin Heidelberg 2001

148

E. Ch´avez and G. Navarro

queries. We are interested in this work in range queries, expressed as (q, r) (a
point in X and a tolerance radius), which should retrieve all the points at distance
r or less from q, i.e. {u ∈ U / d(u, q) ≤ r}. Other interesting queries are nearestneighbor 
ones, which retrieve the K elements of U that are closest to q.

1.1 The Curse of Dimensionality
k and the
A particular case of the problem arises when the metric space is R
distance is Minkowski’s Ls. In this case the objects have a geometric meaning
and the coordinate information can be used to guide the search.

There are eﬀective methods for vector spaces, such as kd-trees [2], R-trees [10]
or X-trees [3]. However, for random vectors on more than roughly 20 dimensions
all those structures cease to work well. There exist proven lower bounds [8] which
show that the search complexity is exponential with the dimension.

It is interesting to point out that the concept of “dimensionality” can be
translated into metric spaces as well. The typical feature of high dimensional
spaces in vector spaces is that the probability distribution of distances among
elements has a very concentrated histogram (with a larger mean as the dimension
grows). In [5,7] this is used as a deﬁnition of intrinsic dimensionality for general
metric spaces, which we adopt in this paper:
Deﬁnition 1. The intrinsic dimension of a database in a metric space is ρ =
µ2
2σ2 , where µ and σ2 are the mean and variance of its histogram of distances.

Under this deﬁnition, a database formed by random k dimensional vectors
where the coordinates are independent and identically distributed has intrinsic 
dimension Θ(k) [14]. Hence, the deﬁnition extends naturally that of vector
spaces.

Analytical lower bounds and experiments in [5,7] show that all the algorithms
degrade systematically as the intrinsic dimension ρ of the space increases. The
problem is so hard that it has received the name of “curse of dimensionality”,
and it is due to two possible reasons. On one hand, if ρ increases because the
variance is reduced, then we have that a concentrated histogram of distances
gives less information1. On the other hand, if ρ increases because the mean of
the distribution grows, then in order to retrieve a ﬁxed fraction of the database
(and also to get a constant number of nearest neighbors) we need to use a larger
search radius. In most cases both facts hold simultaneously.

An interesting question is whether a probabilistic or approximate algorithm
could break the curse of dimensionality or at least alleviate it. Approximate
and probabilistic algorithms are acceptable in most applications that search in
metric spaces, because in general the modelization as a metric space already
carries some kind of relaxation. In most cases, ﬁnding some close elements is as
good as ﬁnding all of them.

This is the focus of this paper. In the next section we review related work

and put our contribution in context.
1 In the extreme case we have a space where d(x, x) = 0 and ∀y (cid:2)= x, d(x, y) = 1,
where it is impossible to avoid any distance evaluation at search time.

