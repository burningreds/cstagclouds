Optimal Lower and Upper Bounds
for Representing Sequences

DJAMAL BELAZZOUGUI
Helsinki Institute for Information Technology HIIT and University of Helsinki
GONZALO NAVARRO
University of Chile

Sequence representations supporting queries access, select and rank are at the core of many data
structures. There is a considerable gap between the various upper bounds and the few lower
bounds known for such representations, and how they relate to the space used. In this article
we prove a strong lower bound for rank, which holds for rather permissive assumptions on the
space used, and give matching upper bounds that require only a compressed representation of the
sequence. Within this compressed space, operations access and select can be solved in constant
or almost-constant time, which is optimal for large alphabets. Our new upper bounds dominate
all of the previous work in the time/space map.

Categories and Subject Descriptors: E.1 [Data Structures]: ; E.4 [Coding and Information
Theory]: Data Compaction and Compression

General Terms: Algorithms

Additional Key Words and Phrases: String Compression, Succinct Data Structures, Text Indexing.

1.

INTRODUCTION

A large number of data structures build on sequence representations. In particular,
supporting the following three queries on a sequence S[1, n] over alphabet [1, σ] has
proved extremely useful:

—access(S, i) gives S[i];
—selecta(S, j) gives the position of the jth occurrence of a ∈ [1, σ] in S; and
—ranka(S, i) gives the number of occurrences of a ∈ [1, σ] in S[1, i].

The most basic case is that of bitmaps, when σ = 2. Obvious applications are
set representations supporting membership and predecessor search, although many
other uses, such as representing tree topologies, multisets, and partial sums [Jacobson 
1989; Raman et al. 2007] have been reported. The focus of this article is on
general alphabets, where further applications have been described. For example,
the FM-index [Ferragina and Manzini 2005], a compressed indexed representation

3
1
0
2

 

g
u
A
3
2

 

 
 
]
S
D
.
s
c
[
 
 

2
v
1
2
6
2

.

1
1
1
1
:
v
i
X
r
a

Partially funded by Fondecyt Grant 1-110066, Chile. First author also partially supported by the
French ANR-2010-COSI-004 MAPPI Project and the Academy of Finland under grant 250345
(CoECGR).
An early partial version of this work appeared in Proc. ESA’12 [Belazzougui and Navarro 2012].
Authors’ address: Djamal Belazzougui, Helsinki Institute for Information Technology HIIT, Department 
of Computer Science, University of Helsinki, Finland, djamal.belazzougui@gmail.com.
Gonzalo Navarro, Department
of Chile, Chile,
gnavarro@dcc.uchile.cl

Science, University

of Computer

ACM Transactions on Algorithms, Vol. TBD, No. TDB, Month Year, Pages 1–??.

·

2

Optimal Lower and Upper Bounds for Representing Sequences

for text collections that supports pattern searches, is most successfully implemented
over a sequence representation supporting access and rank [Ferragina et al. 2007],
and more recently select [Belazzougui and Navarro 2011]. Grossi et al. [2003] had
used earlier similar techniques for text indexing. Golynski et al. [2006] used these
operations for representing labeled trees and permutations. Further applications
of these operations to multi-labeled trees and binary relations were uncovered by
Barbay et al.
[2006], and Arroyuelo
et al. [2010a] devised new applications to XML indexing. Other applications were
described as well to representing permutations and inverted indexes [Barbay and
Navarro 2009; Barbay et al. 2012] and graphs [Claude and Navarro 2010; Hern´andez
and Navarro 2012]. V¨alim¨aki and M¨akinen [2007] and Gagie et al. [2010] applied
them to document retrieval on general texts. Finally, applications to various types
of inverted indexes on natural language text collections have been explored [Brisaboa 
et al. 2012; Arroyuelo et al. 2010b; Arroyuelo et al. 2012].

[2011]. Ferragina et al.

[2009], Gupta et al.

When representing sequences supporting the three operations, it seems reasonable 
to aim for O(n lg σ) bits of space. However, in many applications the size of
the data is huge and space usage is crucial: only sublinear space on top of the raw
data can be accepted. This is our focus.

Various timeand 
space-eﬃcient sequence representations supporting the three
operations have been proposed, and also various lower bounds have been proved. All
the representations proposed assume the RAM model with word size w = Ω(lg n).
In the case of bitmaps, Munro [1996] and Clark [1996] achieved constant-time rank
and select using o(n) extra bits on top of a plain representation of S. Golynski [2007]
proved a lower bound of Ω(n lg lg n/ lg n) extra bits for supporting either operation
in constant time if S is to be represented in plain form, and gave matching upper
bounds. This assumption is particularly inconvenient in the frequent case where
the bitmap is sparse, that is, it has only m (cid:28) n 1s, and hence can be compressed.

When S can be represented arbitrarily, P˘atra¸scu [2008] achieved lg(cid:0) n
lg(cid:0) n
Raman et al. [2007] achieved constant time and lg(cid:0) n

(cid:1)+O(n/ lgc n)
(cid:1) + O(m) bits, if superconstant time for the operations is permitted [Gupta
(cid:1) + o(m) + O(lg lg n) bits of

et al. 2007; Okanohara and Sadakane 2007], or if the operations are weakened:
When rank1(S, i) can only be applied if S[i] = 1 and only select1(S, j) is supported,

bits of space, where c is any constant. This space was shown later to be optimal
[P˘atra¸scu and Viola 2010]. However, the space can be reduced further, up to

m

m

m

tropy of S, H0(S) =(cid:80)

space. When only rank1(S, i) is supported for the positions i such that S[i] = 1,
and in addition we cannot even determine S[i], the structure is called a monotone
minimum perfect hash function (mmphf) and can be implemented in O(m lg lg n
m )
bits and answering in constant time [Belazzougui et al. 2009].

a∈[1,σ]

(cid:80)
For general sequences, a useful measure of compressibility is the zeroth-order en-
, where na is the number of occurrences of a in
A∈[1,σ]k |TA|H0(TA),
S. This can be extended to the k-th order entropy, Hk(S) = 1
n
where TA is the string of symbols following k-tuple A in S. It holds 0 ≤ Hk(S) ≤
Hk−1(S) ≤ H0(S) ≤ lg σ for any k, but the entropy measure is only meaningful for
k < lgσ n. See Manzini [2001] and Gagie [2006] for a deeper discussion.

na
n lg n
na

We say that a representation of S is succinct if it takes n lg σ + o(n lg σ) bits,
zeroth-order compressed if it takes nH0(S) + o(n lg σ) bits, and high-order comACM 
Transactions on Algorithms, Vol. TBD, No. TDB, Month Year.

(cid:16)

(cid:17)

D. Belazzougui, G. Navarro

·

3

pressed if it takes nHk(S) + o(n lg σ) bits. We may also compress the redundancy,
o(n lg σ), to use for example nH0(S) + o(nH0(S)) bits.

Upper and lower bounds for sequence representations supporting the three operations 
are far less understood over arbitrary alphabets. Grossi et al. [2003] introduced 
the wavelet tree, a zeroth-order compressed representation using nH0(S) +
o(n lg σ) bits that solves the three queries in time O(lg σ). The time was reduced

1 + lg σ
lg lg n

with multiary wavelet trees [Ferragina et al. 2007], and later the
to O
space was reduced to nH0(S)+o(n) bits [Golynski et al. 2008]. Note that the query
times are constant for lg σ = O(lg lg n), that is, σ = O(polylog n). Golynski et al.
[2006] proposed a succinct representation that is more interesting for large alphabets.
 It solves access and select in O(1) and O(lg lg σ) time, or vice versa, and rank
in O(lg lg σ) time or slightly more. This representation was made slightly faster (i.e.,
rank time is always O(lg lg σ)) and compressed to nH0(S) + o(nH0(S)) + o(n) by
Barbay et al. [2012]. Alternatively, Barbay et al. [2011] achieved high-order compression,
 nHk(S) + o(n lg σ) bits for any k = o(lgσ n), and slightly higher times,
which were again reduced by Grossi et al. [2010].

There are several curious aspects in the map of the current solutions for general
sequences. On the one hand, in various solutions for large alphabets [Golynski et al.
2006; Barbay et al. 2012; Grossi et al. 2010] the times for access and select seem to
be complementary (i.e., one is constant and the other is not), whereas that for rank
is always superconstant. On the other hand, there is no smooth transition between
the complexity of the wavelet-tree based solutions, O
, and those for

(cid:16)

(cid:17)

1 + lg σ
lg lg n

larger alphabets, O(lg lg σ).

The complementary nature of access and select is not a surprise. Golynski [2009]
proved lower bounds that relate the time performance that can be achieved for these
operations with the redundancy of any encoding of S on top of its information
content. The lower bound acts on the product of both times, that is, if t and t(cid:48)
are the time complexities for access and select, and ρ is the bit-redundancy per
symbol, then ρ · t · t(cid:48) = Ω((lg σ)2/w) holds for a wide range of values of σ. Many
upper bounds for large alphabets [Golynski et al. 2006; Barbay et al. 2012; Grossi
et al. 2010] match this lower bound when lg σ = Θ(w).

Despite operation rank seems to be harder than the others (at least no constanttime 
solution exists except for polylog-sized alphabets), no general lower bounds
on this operation have been proved. Only a result [Grossi et al. 2010] for the case
in which S must be encoded in plain form states that if one solves rank within
a = O
accesses to the sequence, then the redundancy per symbol is

(cid:16)

(cid:17)

1 + lg σ
lg lg σ

ρ = Ω((lg σ)/a). Since in the RAM model one can access up to w/ lg σ symbols in
one access, this implies a lower bound of ρ · t = Ω((lg σ)2/w), similar to the one
by Golynski [2009] for the product of access and select times and also matched
by current solutions [Golynski et al. 2006; Barbay et al. 2012; Grossi et al. 2010]
when lg σ = Θ(w).

In this article we make several contributions that help close the gap between

lower and upper bounds on sequence representation.

(1) We prove the ﬁrst general lower bound on rank, which shows that this operation 
is, in a sense, noticeably harder than the others: Any structure using

ACM Transactions on Algorithms, Vol. TBD, No. TDB, Month Year.

4

Optimal Lower and Upper Bounds for Representing Sequences

·
O(n · wO(1)) bits needs time Ω

(cid:16)

(cid:16)

(cid:17)

(cid:17)

to answer rank queries (the bound is
if σ > n; we mostly focus on the interesting case σ ≤ n). Note
only Ω
that the space includes the rather permissive O(n·polylog n). The existing lower
bound [Grossi et al. 2010] not only is restricted to plain encodings of S but only

lg lg n
lg w

lg lg σ
lg w

forbids achieving this time complexity within n lg σ + O

n lg2 σ/(w lg lg σ
lg w )

=

n lg σ + o(n lg σ) bits of space. Our lower bound uses a reduction from predecessor 
queries [P˘atra¸scu and Thorup 2008].

(cid:16)

(cid:17)

(cid:16)

(cid:17)

(cid:16)

(cid:17)

time O

1 + lg σ
lg w

(2) We give a matching upper bound for rank, using O(n lg σ) bits of space and

lg lg σ
lg w

answering queries in time O
. This is lower than any time complexity
achieved so far for this operation within O(n · wO(1)) bits, and it elegantly
uniﬁes both known upper bounds under a single and lower time complexity.
This is achieved via a reduction to a predecessor query structure that is tuned
to use slightly less space than usual.

(3) We derive succinct and compressed representations of sequences that achieve

for access, select and rank, improving upon previous results

[Ferragina et al. 2007; Golynski et al. 2008]. This yields constant-time operations 
for σ = wO(1). Succinctness is achieved by replacing universal tables
used in previous solutions [Ferragina et al. 2007; Golynski et al. 2008] with bit
manipulations in the RAM model. Compression is achieved by combining the
succinct representation with known compression boosters [Barbay et al. 2012].
(4) We derive succinct and compressed representations of sequences over larger

alphabets, which achieve the optimal time O

for rank, and almost-

(cid:16)

lg lg σ
lg w

(cid:17)

constant time for access and select (i.e., one is constant time and the other any
superconstant time, as low as desired). The result improves upon all succinct
and compressed representations proposed so far [Golynski et al. 2006; Barbay
et al. 2011; Barbay et al. 2012; Grossi et al. 2010]. This is achieved by
plugging our O(n lg σ)-bit solutions into some of those succinct and compressed
data structures.

(5) As an immediate application, we obtain the fastest text self-index [Grossi et al.
2003; Ferragina and Manzini 2005; Ferragina et al. 2007] able to provide pattern
matching on a text compressed to its kth order entropy within o(n)(Hk(S) + 1)
bits of redundancy, improving upon the best current one [Barbay et al. 2012],
and being only slightly slower than the fastest one [Belazzougui and Navarro
2011], which however poses O(n) further bits of space redundancy.

Table I compares our new upper bounds with the best current ones. It can be
seen that, combining our results, we dominate all of the best current work [Golynski
et al. 2008; Barbay et al. 2012; Grossi et al. 2010], as well as earlier ones [Golynski
et al. 2006; Ferragina et al. 2007; Barbay et al. 2011] (but our solutions build on
some of those).

Besides w = Ω(lg n), we make for simplicity the reasonable assumption that
lg w = O(lg n), that is, w = nO(1); this avoids irrelevant technical issues (otherwise,
for example, all the text ﬁts in a single machine word!). We also avoid mentioning
the need to store a constant number of systemwide pointers (O(w) bits), which

ACM Transactions on Algorithms, Vol. TBD, No. TDB, Month Year.

D. Belazzougui, G. Navarro

·

5

)
σ
g
l
g
l
(
O

)
σ
g
l
g
l
(
O

(cid:1)
(cid:1)
σ
σ
g
g
l
l

(cid:1)
(cid:1)
σ
σ
g
g
l
l

w
g
l

w
g
l

w
g
l

w
g
l

(cid:0)

(cid:0)

+
1
(cid:0)

g
l
(cid:0)
O

g
l

O

g
l

O

O

)
1
(
ω

y
n
a

(cid:1)
σ
g
l

w
g
l

)
σ
g
l
g
l
(
O

)
σ
g
l
g
l
(
O

(cid:0)

)
1
(
O

)
1
(
ω

y
n
a

)
1
(
ω

y
n
a

)
1
(
ω

y
n
a

(cid:1)
σ
g
l

w
g
l

)
1
(
O

)
1
(
O

(cid:0)

O

)
1
(
O

)
1
(
O

)
1
(
O

e
h
T

.
k
n
a
r

d
n
a

t
c
e
l
e
s

,
s
s
e
c
c
a

g
n

i
t
r
o
p
p
u
s

s
e
r
u
t
c
u
r
t
s

a
t
a
d

r
o
f

,
s
e
n
o

t
s
e
b

w
e
n

r
u
o

d
n
a

,
s
d
n
u
o
b

r
e
p
p
u

s
u
o
i
v
e
r
p

t
s
e
b

e
h
T

.
I

e
l
b
a
T

.
)
n
σ
g
l
(
o
=
k

y
n
a

r
o
f

s
d
l
o
h

)
S
(
k
H
d
n
u
o
b

e
c
a
p
s

)
σ
g
l
g
l
(
O

(cid:1)

σ
g
l

n
g
l
g
l

+
1

k
n
a
r

(cid:0)

O

(cid:1)

σ
g
l

n
g
l
g
l

+
1

)
1
(
O

t
c
e
l
e
s

(cid:0)

O

(cid:1)

σ
g
l

n
g
l
g
l

+
1

s
s
e
c
c
a

(cid:0)

O

)
s
t
i

b
(

e
c
a
p
s

e
c
r
u
o
s

)
n
(
o
+

)
S
(
0

H
n

]
4

.

m
h
T

,
8
0
0
2

.
l
a

t
e

i
k
s
n
y
l
o
G

[

)
σ
g
l
g
l
(
O

)
n
(
o
+

)
)
S
(
0

H
n
(
o
+

)
S
(
0

H
n

]
2

.

m
h
T

,
2
1
0
2

.
l
a

t
e

y
a
b
r
a
B

[

)
n
(
o
+

)
)
S
(
0

H
n
(
o
+

)
S
(
0

H
n

]
2

.

m
h
T

,
2
1
0
2

.
l
a

t
e

y
a
b
r
a
B

[

)
σ
g
l

n
(
o
+

)
S
(
k
H
n

]
2

.
r
o
C

,
0
1
0
2

.
l
a

t
e

i
s
s
o
r
G

[

+
1

O

+
1

)
n
(
o
+

)
S
(
0

H
n

)
1
(
ω

y
n
a

)
n
(
o
+

)
)
S
(
0

H
n
(
o
+

)
S
(
0

H
n

)
n
(
o
+

)
)
S
(
0

H
n
(
o
+

)
S
(
0

H
n

7
m
e
r
o
e
h
T

8
m
e
r
o
e
h
T

8
m
e
r
o
e
h
T

)
σ
g
l

n
(
o
+

)
S
(
k
H
n

)
)
w
g
l
(
ω
=
σ
g
l
(

1
1
m
e
r
o
e
h
T

)
σ
g
l

n
(
o
+

)
S
(
k
H
n

)
)
w
g
l
(
O
=
σ
g
l
(

2
1
m
e
r
o
e
h
T

ACM Transactions on Algorithms, Vol. TBD, No. TDB, Month Year.

·

6

Optimal Lower and Upper Bounds for Representing Sequences

is needed in any reasonable implementation. Finally, our results assume that, in
the RAM model, bit shifts, bitwise logical operations, and arithmetic operations
(including multiplication) are permitted. Otherwise we can simulate them with
universal tables using o(2w) extra bits of space. This space is o(n) if lg n ≥ w+O(1);
otherwise we can reduce the universal tables to use o(n) bits, but any lg w in the
upper bounds becomes lg lg n.

The next section proves our lower bound for rank. Section 3 gives a matching
upper bound within O(n lg σ) bits of space. Within this space, achieving constant
time for access and select is trivial. Section 4 shows how to retain the same
upper bound for rank within succinct space, while reaching constant or almostconstant 
time for access and select. Section 5 retains those times while reducing the
size of the representation to zeroth-order or high-order compressed space. Finally,
Section 6 gives our conclusions and future challenges.

2. LOWER BOUND FOR RANK

Our technique is to reduce from a predecessor problem and apply the density-aware
lower bounds of P˘atra¸scu and Thorup [2006]. Assume that we have n keys from a
universe of size u = nσ, then the keys are of length (cid:96) = lg u = lg n + lg σ. According
to branch 2 of P˘atra¸scu and Thorup’s result, the time for predecessor queries in
, where a = lg(s/n) + lg w and s
this setting is lower bounded by Ω

(cid:16) (cid:96)−lg n

(cid:17)(cid:17)

(cid:16)

lg

a

is the space in words of our representation (the lower bound is in the cell probe
model for word length w, so the space is always expressed in number of cells). The
lower bound holds even for a more restricted version of the predecessor problem in
which one of two colors is associated with each element and the query only needs
to return the color of the predecessor.
The reduction is as follows. We divide the universe [1, n· σ] into σ intervals, each
of size n. This division can be viewed as a binary matrix of n columns c ∈ [1, n] and
σ rows r ∈ [1, σ], where we set a 1 at row r and column c iﬀ element (r − 1) · n + c
belongs to the set. We will use four data structures.

(1) A plain bitvector L[1, n] which stores the color associated with each element.

The array is indexed by the original ranks of the elements.

(2) A partial sums structure R stores the number of elements in each row. This
is a bitmap concatenating the σ unary representations, 1nr 0, of the number
nr of 1s in each row r ∈ [1, σ]. Thus R is of length n + σ and can give in
constant time the number of 1s up to (and including) any row r, count(r) =
rank1(R, select0(R, r)) = select0(R, r) − r, in constant time and O(n + σ) bits
of space [Munro 1996; Clark 1996].

(3) A column mapping data structure C that maps the original columns into a set
of columns where (i) empty columns are eliminated, and (ii) new columns are
created when two or more 1s fall in the same column. C is a bitmap concatenating 
the n unary representations, 1nc0, of the number nc of 1s in each column
c ∈ [1, n]. So C is of length 2n. Note that the new matrix of mapped columns
also has n columns (one per element in the set) and exactly one 1 per column.
The original column c is then mapped to col(c) = rank1(C, select0(C, c)) =
select0(C, c)− c, using constant time and O(n) bits. Note that col(c) is the last
of the columns to which the original column c might have been expanded.

ACM Transactions on Algorithms, Vol. TBD, No. TDB, Month Year.

D. Belazzougui, G. Navarro

·

7

Fig. 1. Illustration of the lower bound technique, moving from a predecessor to a rank query, and
of the upper bound technique, moving from the rank query to a predecessor query.

(4) A string S[1, n] over alphabet [1, σ], so that S[c] = r iﬀ the only 1 at column c
(after column remapping) is at row r. Over this string we build a data structure
able to answer queries rankr(S, c).

Colored predecessor queries are solved in the following way. Given an element
x ∈ [1, u], we ﬁrst decompose it into a pair (r, c) where x = (r − 1) · n + c and
1 ≤ c ≤ n. In a ﬁrst step, we compute count(r − 1) in constant time. This gives
us the count of elements up to point (r − 1) · n. Next we must compute the count
of elements in the range [(r − 1) · n + 1, (r − 1) · n + c]. For doing that we ﬁrst
remap the column to c(cid:48) = col(c) in constant time, and ﬁnally compute rankr(S, c(cid:48)),
which gives the number of 1s in row r up to column c(cid:48). Note that if column c was
expanded to several ones, we are counting the 1s up to the last of the expanded
columns, so that all the original 1s at column c are counted at their respective rows.
Then the rank of the predecessor of x is p = count(r−1)+rankr(S, col(c)). Finally,
the color associated with x is given by L[p].

Example. Fig. 1 illustrates the technique on a universe of size u = n×σ = 5×3 =
15, the set {5, 6, 7, 10, 12} of n = 5 points black or white, and the query pred(9),
which must return the color of the 3rd point. The bitmap L indicates the colors
of the points. The top matrix is obtained by taking the ﬁrst, second, and third
(σ = 3) segments of length n = 5 from the universe, and identifying points with
1-bits (the omitted cells are 0-bits). Bitmaps R and C count the number of 1s in
rows and columns, respectively. Bitmap C is used to map the matrix into a new
one below it, with exactly one point per column. Then the predecessor query is
mapped to the matrix, and spans several whole rows (only 1 in this example) and
one partial row. The 1s in whole rows (1 in total) are counted using R, whereas
those in the partially covered row are counted with a rankb(S, 3) = 2 query on the

ACM Transactions on Algorithms, Vol. TBD, No. TDB, Month Year.

brank  (S,3)<1,1><2,1><2,2><2,3><3,1>X10010Lpred(9) 111111011101011111R1100011010CS = bbcabpred(8)·

8

Optimal Lower and Upper Bounds for Representing Sequences

string S = bbcab represented by the mapped matrix. Then we obtain the desired 3
(3rd point), and L[3] = 0 is the color. Ignore for now the last line in the ﬁgure. 2
Theorem 1. Given a data structure that supports rank queries on strings of
length n over alphabet [1, σ], in time t(n, σ) and using s(n, σ) bits of space, we can
solve the colored predecessor problem for n integers from universe [1, nσ] in time
t(n, σ) + O(1) using a data structure that occupies s(n, σ) + O(n + σ) bits.

By the reduction above we get that any lower bound for predecessor search for
n keys over a universe of size nσ must also apply to rank queries on sequences of
length n over alphabet of size σ. In our case, if we aim at using n · wO(1) bits of
space for the rank data structure, and allow any σ ≤ n · wO(1), this lower bound
(branch 2 [P˘atra¸scu and Thorup 2006]) is Ω

(cid:96)−lg n

(cid:16)

(cid:17)

(cid:16)

(cid:17)

= Ω

lg lg σ
lg w

.

lg

lg(s/n)+lg w

Theorem 2. Any data structure that uses n·wO(1) space to represent a sequence
to

of length n over alphabet [1, σ], for any σ ≤ n · wO(1), must use time Ω
answer rank queries.

lg lg σ
lg w

(cid:16)

(cid:17)

For larger σ, the space of our representation is dominated by the O(σ) bits of

structure R, so the lower bound becomes Ω
, which worsens (decreases)
as σ grows from n·wω(1), and becomes completely useless for σ = n1+Ω(1). However,
lg lg n
since the time for rank is monotonic in σ, we still have the lower bound Ω
lg w

lg(σ/n)

lg lg σ

(cid:17)

(cid:16)

(cid:16)

(cid:16)

(cid:17)

(cid:17)

when σ > n; thus a general lower bound is Ω
we have focused in the most interesting case.

lg lg min(σ,n)

lg w

time. For simplicity

Assume to simplify that w = Θ(lg n). The lower bound of Theorem 2 is trivial
for small lg σ = O(lg lg n) (i.e., σ = O(polylog n)), where constant-time solutions
for rank exist that require only nH0(S) + o(n) bits [Ferragina et al. 2007]. On the
other hand, if σ is suﬃciently large, lg σ = (lg lg n)1+Ω(1), the lower bound becomes
simply Ω(lg lg σ), where it is matched by known compressed solutions requiring as
little as nH0(S) + o(nH0(S)) + o(n) [Barbay et al. 2012] or nHk(S) + o(n lg σ)
[Grossi et al. 2010] bits.

The range where this lower bound has not yet been matched is ω(lg lg n) = lg σ =
(lg lg n)1+o(1). It is also unmatched when lg n = o(w). The next section presents a
new matching upper bound.

3. OPTIMAL UPPER BOUND FOR RANK

We now show a matching upper bound with optimal time and space O(n lg σ) bits.
In the next sections we make the space succinct and even compressed.

We reduce the problem to predecessor search and then use a convenient solution
for that problem. The idea is simply to represent the string S[1, n] over alphabet
[1, σ] as a matrix of σ rows and n columns, and regard each S[c] as a point (S[c], c).
Then we represent the matrix as the set of n points {(S[c] − 1) · n + c, c ∈ [1, n]}
over the one-dimensional universe [1, nσ], which is roughly the inverse of the transform 
used in the previous section. We also store in an array X[1, n] the pairs
(cid:104)r, rankr(S, c)(cid:105), where r = S[c], for the point corresponding to each column c in
the set. Those pairs are stored in row-major order in X, that is, by increasing point
value (r − 1) · n + c.

ACM Transactions on Algorithms, Vol. TBD, No. TDB, Month Year.

D. Belazzougui, G. Navarro

·

9

To query rankr(S, c) we compute the predecessor of (r− 1)· n + c, which gives us
its position p in X. If X[p] is of the form (cid:104)r, v(cid:105), for some v, this means that there
are points in row r and columns [1, c] of the matrix, and thus there are occurrences
of r in S[1, c]. Moreover, v = rankr(S, c) is the value we must return. Otherwise,
there are no points in row r and columns [1, c] (i.e., our predecessor query returned
a point from a previous row), and thus there are no occurrences of r in S[1, c]. Thus
we return zero.

Example. Fig. 1 also illustrates the upper-bound technique on string S = bbcab,
of length n = 5 over an alphabet of size σ = 3. It corresponds to the lower matrix
in the ﬁgure, which is read row-wise and the 1s are written as n = 5 points in a
universe of size nσ = 15. To each point we associate the row it comes from and
its rank in the row, in array X. Now the query rankb(S, 3) is converted into query
pred(8) = 3 (8 = 5 × 1 + 3). This yields X[3] = (cid:104)2, 2(cid:105), the ﬁrst 2 indicating that
there are bs up to position 3 in S (b is the 2nd alphabet symbol), and the second
2 indicating that there are 2 bs in the range, so the answer is 2. Instead, a query
like rankc(S, 2) would be translated into pred(12) = 4 (12 = 5× 2 + 2). This yields
X[4] = (cid:104)2, 3(cid:105). Since the ﬁrst component is not 3, there are no cs up to position 2
2
in S and the answer is zero.

This solution requires n lg σ + n lg n bits for the pairs of X, on top of the space
of the predecessor structure. If σ ≤ n we can reduce this extra space to 2n lg σ by
storing the pairs (cid:104)r, rankr(S, c)(cid:105) in a diﬀerent way. We virtually cut the string into
chunks of length σ, and store the pair as (cid:104)r, rankr(S, c)− rankr(S, c− (c mod σ))(cid:105),
that is, we only store the number of occurrences of c from the beginning of the
current chunk. Such a pair requires 2 lg σ bits. The rest of the rankr information,
that is, up to the beginning of the chunk, is obtained in constant time and O(n) bits
using the reduction to chunks of Golynski et al. [2006]: They store a bitmap A[1, 2n]
where the matrix is traversed row-wise and we append to A a 1 for each 1 found in
the matrix and a 0 each time we move to the next chunk (so we append n/σ 0s per
row). Then the remaining information for rankr(S, c) is rankr(S, c− (c mod σ)) =
select0(A, p1) − select0(A, p0) − (cid:98)c/σ(cid:99), where p0 = (r − 1) · n/σ is the number of
chunks in previous rows and p1 = p0 + (cid:98)c/σ(cid:99) is the number of chunks preceding
the current one (we have simpliﬁed the formulas by assuming σ divides n). The
select0(A,·) operations map chunk numbers to positions in A, and the ﬁnal formula
counts the number of 1s in between.

Theorem 3. Given a solution for predecessor search on a set of n keys chosen
from a universe of size u, that occupies space s(n, u) and answers in time t(n, u),
there exists a solution for rank queries on a sequence of length n over an alphabet
[1, σ] that runs in time t(n, nσ) + O(1) and occupies s(n, nσ) + O(n lg σ) bits.

In the extended version of their article, P˘atra¸scu and Thorup [2008] give an
upper bound matching the lower bound of branch 2 and using O(n lg u) bits for n
elements over a universe [1, u]. In Appendix A we show that the same time can be
achieved with space O(n lg(u/n)), which is not surprising (they have given hints,
actually) but we opt for completeness. By using this predecessor data structure,
the following result is immediate.

ACM Transactions on Algorithms, Vol. TBD, No. TDB, Month Year.

·

10

Optimal Lower and Upper Bounds for Representing Sequences

Theorem 4. A string S[1, n] over alphabet [1, σ] can be represented using O(n lg σ)

bits, so that operation rank is solved in time O

(cid:16)

(cid:17)

lg lg σ
lg w

.

Note that, within O(n lg σ) bits, operations access and select can also be solved in
constant time: we can add a plain representation of A to have constant-time access,
plus a succinct representation [Golynski et al. 2006] that supports constant-time
select, adding 2n lg σ + o(n lg σ) bits in total.

When σ > n, we can add a perfect hash function mapping [1, σ] to the (at
most) n symbols actually occurring in S, in constant time, and then S[1, n] can
be built over the mapped alphabet of size at most n. The hash function can be
implemented as an array of n lg σ bits listing the symbols that do appear in S plus
O(n lg lg(σ/n)) bits for a mmphf to map from [1, σ] to the array [Belazzougui et al.

2009]. Therefore, in this case we obtain the improved time O

(cid:16)

(cid:17)

lg lg n
lg w

.

4. USING SUCCINCT SPACE

(cid:16)

(cid:17)

We design a sequence representation using n lg σ + o(n lg σ) bits (i.e., succinct)
that answers access and select queries in almost-constant time, and rank in time
. This is done in two phases: a constant-time solution for σ = wO(1),
O
and then a solution for general alphabets.

lg lg σ
lg w

(cid:16)

(cid:17)

4.1 Succinct Representation for Small Alphabets

Using multiary wavelet trees [Ferragina et al. 2007; Golynski et al. 2008] we can

obtain succinct space and O

time for access, select and rank. This

1 + lg σ
lg lg n

is constant for lg σ = O(lg lg n). We start by extending this result to the case
lg σ = O(lg w), as a base case for handling larger alphabets thereafter. More
precisely, we prove the following result.

Theorem 5. A string S[1, n] over alphabet [1, σ], σ ≤ n, can be represented
using n lg σ + o(n) bits so that operations access, select and rank can be solved in

(cid:16)

(cid:17)

time O

1 + lg σ
lg w

.

A multiary wavelet tree for S[1, n] divides, at the root node v, the alphabet [1, σ]
into r contiguous regions of the same size. A sequence Rv[1, n] recording the region
each symbol belongs to is stored at the root node v (note Rv is a sequence over
alphabet [1, r]). This node has r children, each handling the subsequence of S
formed by the symbols belonging to a given region. The children are decomposed
recursively, thus the wavelet tree has height h = (cid:100)lgr σ(cid:101). Queries access, select
and rank on sequence S[1, n] are carried out via O(lgr σ) similar queries on the
sequences Rv stored at wavelet tree nodes [Grossi et al. 2003]. By choosing r such
that lg r = Θ(lg lg n), it turns out that the operations on the sequences Rv can be
carried out in constant time, and thus the cost of the operations on the original
[2008] show
sequence S is O

[Ferragina et al. 2007]. Golynski et al.

(cid:16)

(cid:17)

1 + lg σ
lg lg n

how to retain these time complexities within only n lg σ + o(n) bits of space.

In order to achieve time O

, we need to handle in constant time the
operations over alphabets of size r = wβ, for some 0 < β < 1, so that lg r = Θ(lg w).

1 + lg σ
lg w

(cid:16)

(cid:17)

ACM Transactions on Algorithms, Vol. TBD, No. TDB, Month Year.

D. Belazzougui, G. Navarro

·

11

This time we cannot resort to universal tables of size o(n), but rather must use bit
manipulation on the RAM model. The description of bit-parallel operations is
rather technical; readers interested only in the result (which is needed afterwards)
can skip to Section 4.2.
The sequence Rv[1, n] is stored as the concatenation of n ﬁelds of length (cid:96) = (cid:100)lg r(cid:101),
into consecutive machine words. Thus achieving constant-time access is trivial: To
access Rv[i] we simply extract the corresponding bits, from the (1 + (i − 1) · (cid:96))-th
to the (i · (cid:96))-th, from one or two consecutive machine words, using bit shifts and
masking.

Operations rank and select are more complex. We will proceed by cutting the
sequence Rv into blocks of length b = Θ(wα/(cid:96)) symbols, for some β < α < 1.
First we show how, given a block number i and a symbol a, we extract from
R[1, b] = Rv[(i − 1) · b + 1, i · b] a bitmap that marks the values R[j] = a. Then we
use this result to achieve constant-time rank queries. Next, we show how to solve
predecessor queries in constant time, for several ﬁelds of length lg w bits ﬁtting in
a machine word. Finally, we use this result to obtain constant-time select queries.
In the following, we will sometimes write bit-vector constants; in those, bits are
written right-to-left, that is, the rightmost bit is that at the bitmap position 1.

Projecting a block. Given sequence R[1, b] = Rv[1 + (i − 1) · b, i · b], which is of
bit length b · (cid:96) = Θ(wα) = o(w), and given a ∈ [1, r], we extract B[1, b · (cid:96)] such that
B[j · (cid:96)] = 1 iﬀ R[j] = a. To do so, we ﬁrst compute X = a · (0(cid:96)−11)b. This creates
b copies of a within (cid:96)-bit long ﬁelds. Second, we compute Y = R xor X, which
will have zeroed ﬁelds at the positions j where R[j] = a. To identify those ﬁelds,
we compute Z = (10(cid:96)−1)b − Y , which will have a 1 at the highest bit of the zeroed
ﬁelds in Y . Finally, B = Z and (10(cid:96)−1)b isolates those leading bits.

Constant-time rank queries. We now describe how we can do rank queries in
constant time for Rv[1, n]. Our solution follows that of Munro [1996]. We choose
w − 1)/(cid:96). For each a ∈ [1, r], we
a superblock size s = w2 and a block size b = (
store the accumulated values per superblock, ranka(Rv, i · s) for all 1 ≤ i ≤ n/s.
We also store the within-superblock accumulated values per block, ranka(Rv, i ·
b) − ranka(Rv,(cid:98)(i · b)/s(cid:99) · s), for 1 ≤ i ≤ n/b. Both arrays of counters require, over
√
all symbols, r((n/s) · w + (n/b) · lg s) = O(nwβ(lg w)2/
w) bits. Added over the
wavelet tree levels, the space required is O(n lg σ lg w/w1/2−β) bits. This
O

(cid:16) lg σ

√

(cid:17)

lg w

is o(n lg σ) for any β < 1/2.
To solve a query ranka(Rv, i), we need to add up three values: (i) the superblock
accumulator at position (cid:98)i/s(cid:99), (ii) the block accumulator at position (cid:98)i/b(cid:99), (iii),
the bits set at B[1, (i mod b) · (cid:96)], where B corresponds to the values equal to a in
Rv[(cid:98)i/b(cid:99)· b + 1,(cid:98)i/b(cid:99)· b + b]. We have just shown how to extract B[1, b· (cid:96)] from Rv,
so we count the number of bits set in C = B and 1(i mod b)·(cid:96).

√

This counting is known as a popcount operation. Given a bit block C of length
w − 1, with bits possibly set at positions multiple of (cid:96), we popcount it using

b(cid:96) =
the following steps:

(1) We ﬁrst duplicate the block b times into b ﬁelds. That is, we compute X =

C · (0b(cid:96)−11)b.

ACM Transactions on Algorithms, Vol. TBD, No. TDB, Month Year.

·

12

Optimal Lower and Upper Bounds for Representing Sequences

(2) We now isolate a diﬀerent bit in each diﬀerent ﬁeld. This is done with Y =

X and (0b(cid:96)10(cid:96)−1)b. This will isolate the ith aligned bit in ﬁeld i.

(3) We now sum up all those isolated bits using the multiplication Z = Y ·
(0b(cid:96)+(cid:96)−11)b. The result of the popcount operation lies at the bits Z[b2(cid:96), b2(cid:96) +
lg b − 1].

(4) We ﬁnally extract the result as c = (Z (cid:29) (b2(cid:96) − 1)) and (1lg b).

Constant-time select queries. We now describe how we can do select queries in
constant time for Rv[1, n]. Our solution follows that of Clark [1996]. For each
a ∈ [1, r], consider the virtual bitmap Ba[1, n] so that Ba[j] = 1 iﬀ Rv[j] = a. We
choose a superblock size s = w2 and a block size b = w1/3/(2 lg r). Superblocks
contain s 1-bits and are of variable length. They are called dense if their length
is at most w4, and sparse otherwise. We store all the positions of the 1s in sparse
superblocks, which requires O(n/w) bits of space as there are at most n/w4 sparse
superblocks. For dense superblocks we only store their starting position in Rv and
a pointer to a memory area. Both pointers require O(n/w) bits since there are at
most n/w2 superblocks.

We divide the dense superblocks into blocks of b 1s. Blocks are called dense if
their length is at most w2/3, and sparse otherwise. We store all the positions of
the 1s in sparse blocks. Since each position requires only lg(w4) as it is within
a dense superblock, and there are at most n/w2/3 sparse blocks, the total space
for sparse blocks is O((n/w2/3)b lg w) = O(n/w1/3) bits. For dense blocks we
store only their starting position within their dense superblock, which requires
O((n/b) lg w) = O(n(lg w)2/w1/3) bits.
The space, added over the r symbols, is O(rn(lg w)2/w1/3) = O(n(lg w)2/w1/3−β).
wavelet tree levels, the total space is O(n lg σ lg w/w1/3−β)

(cid:16) lg σ

Summing for O

(cid:17)

lg w

bits. This is o(n lg σ) for any β < 1/3.
In order to compute a selecta(Rv, j) query, we use the data structures for virtual
bitmap Ba[1, n]. If (cid:98)j/s(cid:99) is a sparse superblock, then the answer is readily stored.
If it is a dense superblock, we only know its starting position and the oﬀset o =
j − (j mod s) of the query within its superblock. Now, if (cid:98)o/b(cid:99) is a sparse block in
its superblock, then the answer (which must be added to the starting position of
the superblock) is readily stored. If it is a dense block, we only know its starting
position in Rv (and in Ba), but now we only have to complete the search within an
area of length b = O(w1/3/ lg w) in Ba. We have showed how to extract a chunk
B[1, b· (cid:96)] from Rv, so that B[i· (cid:96)] = Ba[i]. Now we detail how we complete a select
query within a chunk of length b· (cid:96) = O(w1/3) for the remaining j(cid:48) = j − (j mod b)
bits. This is based on doing about w1/3 parallel popcount operations on about w1/3
bit blocks. We proceed as follows:
(1) Duplicate B into b superﬁelds with X = B · (0k−11)b, where k = 2b2(cid:96) is the

superﬁeld size.

(2) Compute Y = X and (0k−b(cid:96)1b(cid:96)) . . . (0k−2(cid:96)12(cid:96))(0k−(cid:96)1(cid:96)). This operation will

keep only the ﬁrst i aligned bits in superﬁeld i.

(3) Do popcount in parallel on all superﬁelds using the algorithm described in
Section 4.1. Note that each superﬁeld will have capacity k = 2b2(cid:96), but only the

ACM Transactions on Algorithms, Vol. TBD, No. TDB, Month Year.

D. Belazzougui, G. Navarro

·

13

ﬁrst b(cid:96) bits in it are set, and the alignment is (cid:96). Thus the popcount operation
will have enough available space in each superblock to operate.

(4) Let Z contain all the partial counts for all the preﬁxes of B. We need the
position in Z of the ﬁrst count equal to j(cid:48). We use the same projecting method
described in Section 4.1 to spot the superﬁelds equal to j(cid:48) (the only diﬀerence
is that superﬁelds are much wider than lg w, namely of width (cid:96) = k, but still
all ﬁts in a machine word). This method returns a word W [1, 2b3(cid:96)] such that
W [k · i] = 1 iﬀ the ith superﬁeld of Z is equal to j(cid:48).

(5) Isolate the least signiﬁcant bit of W with V = W and (W xor (W − 1)).
(6) The ﬁnal answer to select1(B, j(cid:48)) is the position of the only 1 in V , divided
by k. This is easily computed by using mmphfs over the set {2ki, 1 ≤ i ≤
b}. Existing data structures [Belazzougui et al. 2009] take constant time and
O(b lg w) = O(w) bits. Such a data structure is universal and requires the same
space as systemwide pointers.

This is not fully satisfactory when σ is not a power of two.

Space analysis. We choose r = wβ to be a power of 2, r = 2(cid:96). This is always
possible because it is equivalent to ﬁnding an integer (cid:96) = β lg w, where we can
choose any constant 0 < β < 1/3 and any (cid:96) = Θ(lg w) (e.g., one solution is
4 (cid:99)/ lg w, (cid:96) = (cid:98)lg w/4(cid:99), and r = 2(cid:98)lg w/4(cid:99) ≈ w1/4). In this case the wavelet
β = (cid:98) lg w
tree simply stores, at level l, the bits (l − 1) · (cid:96) + 1 to l · (cid:96) of the binary descriptions
of the symbols of S. The wavelet tree has height h = (cid:100)lgr σ(cid:101) = (cid:100)(lg σ)/(cid:96)(cid:101), so it will
store sequences of symbols of (cid:96) = lg r bits in each of the h levels except in the ﬁrst,
where it will store a sequence of symbols of (cid:100)lg σ(cid:101) − (h − 1)(cid:96) ≤ (cid:96) bits. The total
adds up to n(cid:100)lg σ(cid:101) bits.
In this case we
proceed as follows. We choose an integer y = lg σ − Θ(lg lg n) as the number of bits
of the representation that will be stored integrally, just as explained. The other
x = lg σ − y bits (where x is not an integer) will be represented as symbols over
alphabet [1, σ0] = [1,(cid:100)2x(cid:101)] = [1,(cid:100)σ/2y(cid:101)]. By construction, σ0 = lgΘ(1) n, thus we
can represent the sequence of x highest bits (i.e., the numbers (cid:100)S[i]/2y(cid:101)) using the
space-eﬃcient wavelet tree of Golynski et al.
[2008]. This will take n lg σ0 + o(n)
bits and support access, select and rank in constant time, and will act as the
root level of our whole wavelet tree. For each value c ∈ [1, σ0] we will store, as a
child of that root, a separate wavelet tree handling the subsequence of positions
i such that (cid:100)S[i]/2y(cid:101) = c. These wavelet trees will handle the y lower bits of
the sequence with the technique of the previous paragraph, which will take ny
bits and solve the three queries in O
time. Adding up the spaces we
get n lg σ0 + ny + o(n) < n(lg(1 + σ/2y) + y + o(1)) = n(lg(σ + 2y) + o(1)) =
n(lg(σ(1 + 1/ lgΘ(1) n)) + o(1)) ≤ n(lg σ + 1/ lgΘ(1) n + o(1)) = n lg σ + o(n).

(cid:17)

(cid:16)

1 + y
lg w

To this space we must add the o(n lg σ) bits of the extra structures to support
rank and select on the wavelet tree levels. The special level using less than (cid:96) bits
can use the same α value of the next levels without trouble (actually the redundancy
may be lower since more symbols can be packed in the blocks).

In order to further reduce the redundancy to o(n) bits, we use the scheme we have
described only for w > lgd n, for some constant d to be deﬁned soon. For smaller
w, we directly use the scheme of Golynski et al.
[2008], which uses n lg σ + o(n)

ACM Transactions on Algorithms, Vol. TBD, No. TDB, Month Year.

·

14

Optimal Lower and Upper Bounds for Representing Sequences

(cid:16)

(cid:17)

(cid:16)

(cid:17)

. For the
bits and solves all the operations in time O
larger w case, and choosing our example β ≤ 1/4, our redundancy is of the form
O(n lg σ · (lg w/w1/3−β)) = O(n lg n · (d lg lg n/(lg n)d/12)), which is made o(n) by
choosing any d > 12 (a smaller d can be chosen if a smaller β is used).

= O

1 + lg σ
lg lg n

1 + lg σ
lg w

Finally, we have the space redundancy of the wavelet tree pointers. On binary
wavelet trees this is easily solved by concatenating all the bitmaps [M¨akinen and
Navarro 2007]. This technique can be extended to r-ary wavelet trees, but in
this case a simpler solution is as follows. As the wavelet tree has a perfect r-ary
structure, we deploy its nodes levelwise in memory. For each level, we concatenate
all the sequences of the nodes, read left-to-right, into a large sequence of at most
n symbols. Then the node position we want at each level can be algebraically
computed from that of the previous or next level, whereas its starting positions in
the concatenation of sequences can be marked in a bitmap of length n, which will
have at most rj 1s for the level j of the wavelet tree. Using the representation of
Raman et al. [2007] for this bitmap, the space is O(rj lg(n/rj)) + o(n) bits. Thus
the space is dominated by the last level, which has rj = σ/wβ ≤ n/wβ 1s, giving
overall space O(n lg w/wβ) + o(n) = o(n) bits. Then any pointer can be retrieved
with a constant-time select operation on the bitmap of its level.

4.2 Succinct Representation for Larger Alphabets

We now assume lg σ = ω(lg w) and develop fast succinct solutions for these larger
alphabets. We build on the solution of Golynski et al. [2006]. They ﬁrst cut S into
chunks of length σ. With the bitvector A[1, 2n] described in Section 3 they reduce
all the queries, in constant time, to within a chunk. For each chunk they store a
bitmap X[1, 2σ] where the number of occurrences of each symbol a ∈ [1, σ] in the
chunk, na, is concatenated in unary, X = 1n101n20 . . . 1nσ 0. Now they introduce
two complementary solutions.

Constant-time select. The ﬁrst one stores, for each consecutive symbol a ∈ [1, σ],
the chunk positions where it appears, in increasing order. Let π be the resulting
permutation, which is stored with the representation of Munro et al. [2003]. This
requires σ lg σ(1 + 1/f (n, σ)) bits and computes any π(i) in constant time and any
π−1(j) in time O(f (n, σ)), for any f (n, σ) ≥ 1. With this representation they solve,
within the chunk, selecta(i) = π(select0(X, a − 1) − (a − 1) + i) in constant time
and access(i) = 1 + rank0(select1(X, π−1(i))) in time O(f (n, σ)).
For ranka(i), they basically carry out a predecessor search within the interval of
π that corresponds to a: [select0(X, a − 1) − (a − 1) + 1, select0(X, a) − a]. They
have a sampled predecessor structure with one value out of lg σ, which takes just
O(σ) bits. With this structure they reduce the interval to size lg σ, and a binary
search completes the process, within overall time O(lg lg σ).

To achieve optimal time, we sample one value out of lg σ

lg w . We build the predecessor 
data structures of P˘atra¸scu and Thorup [2008] mentioned in Section 3. Over all

the symbols of the chunk, these structures take O

lg σ

= O(n lg w) =

(cid:16)(cid:16)

n/ lg σ
lg w

(cid:17)

(cid:17)

o(n lg σ) bits (as we assumed lg σ = ω(lg w)). The predecessor structures take time

(see Theorem 14 in Appendix A). The ﬁnal binary search time also

(cid:16)

(cid:17)

O

lg lg σ
lg w

ACM Transactions on Algorithms, Vol. TBD, No. TDB, Month Year.

(cid:16)

lg lg σ
lg w

(cid:17)

.

takes time O

D. Belazzougui, G. Navarro

·

15

Constant-time access. This time we use the structure of Munro et al. on π−1, so
we compute any π−1(j) in constant time and any π(i) in time O(f (n, σ)). Thus we
get access in constant time and select in time O(f (n, σ)).

Now the binary search of rank needs to compute values of π, which is not anymore 
constant time. This is why Golynski et al. [2006] obtained time slightly over
f (n,σ) .

lg lg σ time for rank in this case. We instead set the sampling step to

The predecessor structures on the sampled values still answer in time O

(cid:17) 1

(cid:16) lg σ
(cid:16)

lg w

(cid:17)

lg lg σ
lg w

,

(cid:17) 1

f (n,σ)

(cid:19)

(cid:19)

(cid:16) lg σ
(cid:17)

lg w

n/

lg lg σ
lg w

(cid:18)(cid:18)
(cid:16)
(cid:17)

, as desired.

but they take O

(cid:16) f (n,σ)

vided f (n, σ) = o
f (n,σ) lg lg σ

is O

lg w

lg σ

bits of space. This is o(n lg σ) pro-

. On the other hand, the time for the binary search

(cid:16)

lg lg σ
lg w

(cid:17)

The following theorem, which improves upon the result of Golynski et al. [2006]
(not only as a consequence of a higher low-order space term), summarizes our result.

Note that we do not mention the limit f (n, σ) = o

, as if a larger f (n, σ) is

desired we can always use a smaller one (and be faster). We also omit the condition
lg σ = ω(lg w) because otherwise the result also holds by Theorem 5.

Theorem 6. A string S[1, n] over alphabet [1, σ], σ ≤ n, can be represented
using n lg σ+o(n lg σ) bits, so that, given any function f (n, σ) = ω(1), (i) operations
access and select can be solved in time O(1) and O(f (n, σ)), or vice versa, and (ii)

(cid:16)

(cid:17)

rank can be solved in time O

lg lg σ
lg w

.

Note that we can partition into chunks only of σ ≤ n. If σ = o(n lg n) we can
still apply the same scheme using a single chunk, and the space overhead for having
σ > n will be O(σ) = o(n lg σ). For larger σ, however, we must use a mechanism
like the one used at the end of Section 3, mapping [1, σ] to [1, n]. However, this
adds at least n lg σ bits to the space, and thus the space is not succinct anymore,
unless σ is much larger, lg σ = ω(lg n), so that the space of the mapping array
dominates. For simplicity we will consider only the case σ ≤ n in the rest of the
article.

5. COMPRESSING THE SPACE

Now we compress the space of the succinct solutions of the previous sections. First
we achieve zeroth-order compression (of the data and the redundancy) by using
an existing compression booster [Barbay et al. 2012]. Second, we reach high-order
compression by designing an index that operates over a compressed representation 
[Ferragina and Venturini 2007] and simulates the working of a succinct data
structure of the previous section.

5.1 Zero-order Compression
Barbay et al. [2012, Thm. 2] showed how, given a sequence representation R using
n lg σ(1 + r(n, lg σ)) + o(n) bits, where r(n, lg σ) = O(1) is nonincreasing with σ,

ACM Transactions on Algorithms, Vol. TBD, No. TDB, Month Year.

·

16

Optimal Lower and Upper Bounds for Representing Sequences

its times for access, select and rank can be maintained while reducing its space to
nH0(S)(1 + r(n, Θ(lg lg n)) + o(n) bits.1 This can be done even if R works only for
σ ≥ lgc n for some constant c.

The technique separates the symbols according to their frequencies into lg2 n
classes. The sequence of classes is represented using a multiary wavelet tree [Ferragina 
et al. 2007], and the subsequences of the symbols of each class are represented 
with an instance of R if the local alphabet size is σ(cid:48) ≥ lgc n, or with a
multiary wavelet tree otherwise. Hence the global per-bit redundancy can be upper
bounded by r(n, c lg lg n) and it is shown that the total number of bits represented
is nH0(S) + O(n/ lg n).
We can use this technique to compress the space of our succinct representations.
By using Theorem 5 as our structure R, where we can use r(n, lg σ) = 0, we improve
upon Ferragina et al. [2007] and Golynski et al. [2008].

Theorem 7. A string S[1, n] over alphabet [1, σ], σ ≤ n, can be represented
using nH0(S) + o(n) bits so that operations access, select and rank can be solved
in time O

(cid:16)

(cid:17)

.

1 + lg σ
lg w

(cid:16)

(cid:17)

(cid:16)

(cid:17)

lg lg σ(cid:48)

lg w

time and O

n lg σ(cid:48) lg w
lg σ(cid:48)

To obtain better times when lg σ = ω(lg w), we use Theorem 6 as our struc-
[2012] apply R over smaller
ture R. A technical problem is that Barbay et al.
alphabets [1, σ(cid:48)], and thus in Theorem 6 we would sample one position out of lg σ(cid:48)
lg w ,
bits of space, which is o(n lg σ(cid:48))
obtaining O
only if lg σ(cid:48) = ω(lg w) (this is why we have used Theorem 6 only in that case).
To handle this problem, we will use a sampling of size lg σ
(cid:17)
the case of constant-time access), even if the alphabet of the local sequence is of
lg σ ) = o(n lg σ(cid:48)) and
size σ(cid:48). As a consequence, the redundancy will be O(n lg σ(cid:48) lg w
the time for rank will stay O
). Similarly, we always 
use sampling rate f (n, σ) instead of f (n, σ(cid:48)). Therefore our redundancy is
r(n, lg σ) = O

, which is o(1) if lg σ = ω(lg w).

(cid:16) lg w

(cid:16) lg σ

(cid:17) 1

(instead of O

lg lg σ(cid:48)

lg w

lg w (or

lg w

f (n,σ)

in

(cid:16)

(cid:17)

Still, in the ﬁrst levels where σ(cid:48) = O(1), the redundancy of Theorem 6 contains
space terms of the form O(n) that would not be o(n lg σ(cid:48)). To avoid this, we
lg w ≤ 1, where all times are constant, and the variant
will use Theorem 5 up to lg σ(cid:48)
just described for larger σ(cid:48). The result is an improvement over Barbay et al. [2012]
(again, we do not mention the condition lg σ = ω(lg w) because otherwise the result
holds anyway by Theorem 7).

lg σ + 1

f (n,σ)

(cid:17)

lg lg σ
lg w

(cid:16)

Theorem 8. A string S[1, n] over alphabet [1, σ], σ ≤ n, can be represented
using nH0(S) + o(nH0(S)) + o(n) bits, so that, given any function f (n, σ) = ω(1),
(i) operations access and select can be solved in time O(1) and O(f (n, σ)), or vice

versa, and (ii) rank can be solved in time O

(cid:16)

(cid:17)

lg lg σ
lg w

.

1They used the case r(n, lg σ) = 1/ lg lg σ, but their derivation is general.

ACM Transactions on Algorithms, Vol. TBD, No. TDB, Month Year.

D. Belazzougui, G. Navarro

·

17

5.2 Self-Indexing

Likewise, we can improve upon the result of Barbay et al. that plugs a zerothorder 
compressed sequence representation to obtain a k-th order compressed fulltext 
self-index [Barbay et al. 2012, Thm. 5]. This result is not subsumed by
that of Belazzougui and Navarro [2011] because their index, although obtaining
better times, uses O(n) extra bits of space. Ours is the best result using only
o(n)(Hk(S) + 1) bits of redundancy. We start with a version for small alphabets.

Theorem 9. Let S[1, n] be a string over alphabet [1, σ], lg σ = O(lg w). Then
we can represent S using nHk(S) + o(n) bits, for any k ≤ (δ lgσ n)− 1 and constant
0 < δ < 1, while supporting the following queries, for any function f (n) = ω(1):
(i) count the number of occurrences of a pattern P [1, m] in S, in time O(m); (ii)
locate any such occurrence in time O(f (n) lg n); (iii) extract S[l, r] in time O(r −
l + f (n) lg n).

To obtain this result, we follow the proof of Theorem 5 of Barbay et al. [2012].
Our zeroth-order compressed structure will be that of our Theorem 7, with constant
time for all the operations and space overhead O(n/ lgγ n) = o(n) bits, for some
0 < γ < 1. For operations (ii) and (iii), we sample one text position out of
O(f (n) lg n) in the suﬃx array to obtain the claimed times.

On general alphabets, we obtain the following result, where once again we only

need to prove the case lg σ = ω(lg w).

Theorem 10. Let S[1, n] be a string over alphabet [1, σ], σ ≤ n. Then we can
(cid:17)
represent S using nHk(S) + o(n)(Hk(S) + 1) bits, for any k ≤ (δ lgσ n) − 1 and
constant 0 < δ < 1, while supporting the following queries, for any f (n, σ) = ω(1):

(i) count the number of occurrences of a pattern P [1, m] in S, in time O

(cid:16)

m lg lg σ
lg w

;

(ii) locate any such occurrence in time O(f (n, σ) lg n); (iii) extract S[l, r] in time
O(r − l + f (n, σ) lg n).

, we set it to f (n, σ) = lg lg σ

Again we follow the proof of Theorem 5 of Barbay et al. [2012]. First, if f (n, σ) =
lg lg σ
lg w , to ensure that no operation will be slower
ω
lg w
than rank. Our string structure will be that of Theorem 8 with constant-time
select, O(f (n, σ)) time access, and O(nH0(S)(lg w/ lg σ + 1/f (n, lg σ)) + o(n) bits
of overhead. Barbay et al. partition the text into strings Si, which are represented
to their zeroth-order entropy. The main issue is to upper bound the sum of the
redundancies over all the strings Si in terms of the total length n. More precisely,
we need to bound the factor multiplying |Si|H0(Si), O(lg w/ lg σ + 1/f (|Si|, lg σ)),
in terms of n and not |Si|. However, we can simply use the sampling value f (n, σ)
for all the strings Si that are represented using Theorem 8, regardless of the length
|Si|. Then their Theorem 5 can be applied immediatly.

For operations (ii) and (iii), we again sample one out of O(f (n, σ) lg n) text
positions in the suﬃx array, but instead of moving backward in the text using rank
and access, we move forward using select, as in Belazzougui and Navarro [2011,
Sec. 4], which is constant-time.

ACM Transactions on Algorithms, Vol. TBD, No. TDB, Month Year.

(cid:16)

(cid:17)

·

18

Optimal Lower and Upper Bounds for Representing Sequences

5.3 High-order Compression

Ferragina and Venturini [2007] showed how a string S[1, n] over alphabet [1, σ] can
be stored within nHk(S) + o(n lg σ) bits, for any k = o(lgσ n), so that it oﬀers
constant-time access to any O(lgσ n) consecutive symbols.

We provide select and rank functionality on top of this representation by adding
extra data structures that take o(n lg σ) bits, whenever lg σ = ω(lg w). The technique 
is similar to those used by Barbay et al. [2011] and Grossi et al. [2010], and
we use the terminology of Section 4.2. We divide the text logically into chunks,
as with Golynski et al.
[2006], and for each chunk we store a mmphf fa for each
a ∈ [1, σ]. Each fa stores the positions where symbol a occurs in the chunk, so that
given the position i of an occurrence of a, fa(i) gives ranka(i) within the chunk. All
the mmphfs can be stored within O(n lg lg σ) = o(n lg σ) bits and can be queried in
constant time [Belazzougui et al. 2009]. With array X we can know, given a, how
many symbols smaller than a are there in the chunk.
Now we have suﬃcient ingredients to compute π−1 in constant time: Let a be the
ith symbol in the chunk (obtained in constant time using Ferragina and Venturini’s
structure), then π−1(i) = fa(i) + select0(X, a − 1) − (a − 1). Now we can compute
select and rank just as done in the “constant-time access” branch of Section 4.2.
The resulting theorem improves upon the results of Barbay et al. [2011] (they did
not use mmphfs).

Theorem 11. A string S[1, n] over alphabet [1, σ], for σ ≤ n and lg σ = ω(lg w),
can be represented using nHk(S) + o(n lg σ) bits for any k = o(lgσ n) so that, given
any function f (n, σ) = ω(1), (i) operation access can be solved in constant time,
(ii) operation select can be solved in time O(f (n, σ)), and (ii) operation rank can

be solved in time O

lg lg σ
lg w

.

(cid:16)

(cid:17)

To compare with the corresponding result by Grossi et al. [2010], who do use mmphfs 
to achieve nHk(S) + O(n lg σ/ lg lg σ) bits, O(1) time for access and O(lg lg σ)
time for select and rank, we can ﬁx f (n, σ) = lg lg σ to obtain the same redundancy.
Then we obtain the same time for operations access and select, and improved time
for rank. Their results, however, hold for any alphabet size, which we do not cover
for the case lg σ = O(lg w). We can, however, improve that branch too, by using
any superconstant sampling g(n, σ)
f (n,σ) , for lg g(n, σ) = ω(f (n, σ)). Then the
time for rank becomes O( f (n,σ)
f (n,σ) lg g(n, σ)). By using, say, lg g(n, σ) = f (n, σ)2, we
get the following result.

1

Theorem 12. A string S[1, n] over alphabet [1, σ], for lg σ = O(lg w), can be
represented using nHk(S) + o(n lg σ) bits for any k = o(lgσ n) so that, given any
function f (n, σ) = ω(1), (i) operation access can be solved in constant time, (ii)
operation select can be solved in time O(f (n, σ)), and (ii) operation rank can be
solved in time O(f 2(n, σ)).

This result, while improving that of Grossi et al., is not necessarily optimal,
as no lower bound prevents us from reaching constant time for all the operations.
 We can achieve time optimality and kth order compression for small alphabet 
sizes, as follows. We build on the representation of Ferragina and Venturini 
[2007]. For k = o(lgσ n), they partition the sequence S[1, n] into chunks of
ACM Transactions on Algorithms, Vol. TBD, No. TDB, Month Year.

D. Belazzougui, G. Navarro

·

19

√

2 lgσ n = ω(k) symbols, and encode the sequence of chunks S(cid:48)[1, n/s] over
s = 1
alphabet [1, σs] = [1,
n] into zeroth-order entropy. This gives kth order compression 
of S and supports constant-time access to any chunk. Now we add, for
each c ∈ [1, σ], a bitmap Bc[1, n/s] so that Bc[i] = 1 iﬀ chunk S(cid:48)[i] contains an
occurrence of symbol c. We store in addition a bitmap Cc with the number of
occurrences, in unary, of c in all the chunks i where Bc[i] = 1. That is, for each
Bc[i] = 1, we append 0m−11 to Cc, where m is the number of times c occurs in
the chunk S(cid:48)[i]. Then we can easily know the number of occurrences of any c in
S(cid:48)[1, i] using select1(Cc, rank1(Bc, i)). With a universal table on the chunks, of
√
size σs+1 lg s = O(
n polylog(n)) = o(n), we can complete the computation of any
rankc(S, i) in constant time. Similarly, we can determine in which chunk is the
jth occurrence of any c in S(cid:48), by computing select1(Bc, 1 + rank(Cc, j)), and then
we can easily complete the calculation of any selectc(S, j) with a similar universal
table, all in constant time.

Let us consider space now. The Bc bitmaps add up to σn/s bits, of which at most
[2007] we get total space
n are set. By using the representation of Raman et al.
bits, which is o(n lg σ) for any σ = O(lg1+o(1) n)
n lg σ
and σ = ω(1). On the other hand, the Cc bitmaps add up to length n and require
o(n lg σ) bits of space for any σ = ω(1).

n + (σn/s) lg lg(σn/s)

s + O

lg(σn/s)

(cid:17)

(cid:16)

For constant σ, instead, we can represent the Bc bitmaps in plain form, using
O(σn/s) = o(n) bits, and the Cc bitmaps using Raman et al., as they have only
σn/s = O(n/s) 1s, and thus their total space is O( n lg s
) + o(n) = o(n) bits. The
same time complexities are maintained.

s

Theorem 13. A string S[1, n] over alphabet [1, σ], for σ = O

, can
be represented using nHk(S) + o(n lg σ) bits for any k = o(lgσ n) so that operations
access, select and rank can be solved in constant time.

lg1+o(1) n

(cid:16)

(cid:17)

6. CONCLUSIONS

(cid:16)

(cid:17)

This work considerably reduces the gap between upper and lower bounds for sequence 
representations providing access, select and rank queries. Most notably, we
give matching lower and upper bounds Θ
for operation rank, which was
the least developed one in terms of lower bounds. The issue of the space related to
this complexity is basically solved as well: we have shown it can be achieved even
within compressed space, and it cannot be surpassed within space O(n · wO(1)).
On the other hand, operations access and select can be solved, within the same
compressed space, in almost constant time (i.e., one taking O(1) and the other as
close to O(1) as desired but not both reaching it, unless we double the space). Our
new compressed representations improve upon most of the previous work.

lg lg σ
lg w

There are still, however, some intriguing issues that remain unclear, which prevent 
us from considering this problem completely closed:

(1) The lower bounds of Golynski [2009] leave open the door to achieving constant
time for access and select simultaneously, with O(n lg σ lg σ
w ) bits of redundancy.
 That is, both could be constant time with o(n lg σ) redundancy in the
interesting case lg σ = o(w). We have achieved this when lg σ = O(lg w), but
it is open whether this is possible in the area ω(lg w) = lg σ = o(w). In our

ACM Transactions on Algorithms, Vol. TBD, No. TDB, Month Year.

20

Optimal Lower and Upper Bounds for Representing Sequences

·
solution, this would imply computing π and π−1 in constant time on a permutation 
using n lg n + o(n lg n) bits. A lower bound on the redundancy of

(cid:16)

(cid:17)

permutations in the same paper [Golynski 2009], Ω

n lg n lg n
w

bits, forbids

this for lg n = Θ(w) but not for lg n = o(w). It is an interesting open challenge
to achieve this or prove that a stronger lower bound holds.

(2) While we can achieve constant-time select and almost-constant time for access
(or vice versa), only the second combination is possible within high-order entropy 
space. Lower bounds on the indexing model [Grossi et al. 2010] show
that this must be the case (at least in the general case where lg σ = Θ(w))
as long as our solution builds on a compressed representation of S supporting
constant-time access, as it has been the norm [Barbay et al. 2011; Barbay et al.
2012; Grossi et al. 2010]. Yet, it is not clear that this is the only way to reach
high-order compression.

(3) We have achieved high-order compression with almost-constant access and
select times, and optimal rank time, but on alphabets of size superpolynomial 
in w. For smaller alphabets, although constant time seems to be possible,
we achieved it only for σ = O(lg1+o(1) n). This leaves open the interesting
band of alphabet sizes lg1+Ω(1) n = σ = wO(1), where we have achieved only
(any) superconstant time. It is also unclear whether we can obtain o(n) redundancy,
 instead of o(n lg σ), for alphabets polynomial in w, with high-order
compression.

REFERENCES
Arroyuelo, D., Claude, F., Maneth, S., M¨akinen, V., Navarro, G., Nguy˜ˆen, K., Sir´en,
J., and V¨alim¨aki, N. 2010a. Fast in-memory xpath search over compressed text and tree
indexes. In Proc. 26th IEEE International Conference on Data Engineering (ICDE) (2010),
pp. 417–428.

Arroyuelo, D., Gonz´alez, S., Mar´ın, M., Oyarz´un, M., and Suel, T. 2012. To index or not to
index: time-space trade-oﬀs in search engines with positional ranking functions. In Proc. 35th
International ACM SIGIR Conference on Research and Development in Information Retrieval
(SIGIR) (2012), pp. 255–264.

Arroyuelo, D., Gonz´alez, S., and Oyarz´un, M. 2010b. Compressed self-indices supporting
conjunctive queries on document collections. In Proc. 17th International Symposium on String
Processing and Information Retrieval (SPIRE), LNCS 6393 (2010), pp. 43–54.

Barbay, J., Claude, F., Gagie, T., Navarro, G., and Nekrich, Y. 2012.

ﬁcient
http://link.springer.com/article/10.1007/s00453-012-9726-3.

fully-compressed

Algorithmica.

sequence

representations.

To

Efappear.


Barbay, J., He, M., Munro, I., and Rao, S. S. 2011. Succinct indexes for strings, binary

relations and multilabeled trees. ACM Transactions on Algorithms 7, 4, article 52.

Barbay, J. and Navarro, G. 2009. Compressed representations of permutations, and applicaIn 
Proc. 26th International Symposium on Theoretical Aspects of Computer Science

tions.
(STACS) (2009), pp. 111–122.

Belazzougui, D., Boldi, P., Pagh, R., and Vigna, S. 2009. Monotone minimal perfect hashing:
searching a sorted table with O(1) accesses. In Proc. 20th Annual ACM-SIAM Symposium on
Discrete Algorithms (SODA) (2009), pp. 785–794.

Belazzougui, D. and Navarro, G. 2011. Alphabet-independent compressed text indexing. In
Proc. 19th Annual European Symposium on Algorithms (ESA), LNCS 6942 (2011), pp. 748–
759. Extended version to appear in ACM Trans. on Algorithms.

ACM Transactions on Algorithms, Vol. TBD, No. TDB, Month Year.

D. Belazzougui, G. Navarro

·

21

Belazzougui, D. and Navarro, G. 2012. New lower and upper bounds for representing sequences.
In Proc. 20th Annual European Symposium on Algorithms (ESA), LNCS 7501 (2012), pp. 181–
192.

Brisaboa, N., Fari˜na, A., Ladra, S., and Navarro, G. 2012.

Implicit indexing of natural

language text by reorganizing bytecodes. Information Retrieval 15, 6, 527–557.

Clark, D. 1996. Compact Pat Trees. Ph. D. thesis, University of Waterloo, Canada.
Claude, F. and Navarro, G. 2010. Extended compact Web graph representations. In Algorithms

and Applications (Ukkonen Festschrift), LNCS 6060 (2010), pp. 77–91. Springer.

Ferragina, P., Luccio, F., Manzini, G., and Muthukrishnan, S. 2009. Compressing and

indexing labeled trees, with applications. Journal of the ACM 57, 1, article 4.

Ferragina, P. and Manzini, G. 2005. Indexing compressed texts. Journal of the ACM 52, 4,

552–581.

Ferragina, P., Manzini, G., M¨akinen, V., and Navarro, G. 2007. Compressed representations

of sequences and full-text indexes. ACM Transactions on Algorithms 3, 2, article 20.

Ferragina, P. and Venturini, R. 2007. A simple storage scheme for strings achieving entropy

bounds. Theoretical Computer Science 372, 1, 115–121.

Gagie, T. 2006. Large alphabets and incompressibility. Information Processing Letters 99, 6,

246–251.

Gagie, T., Navarro, G., and Puglisi, S. 2010. Colored range queries and document retrieval. In
Proc. 17th International Symposium on String Processing and Information Retrieval (SPIRE),
LNCS 6393 (2010), pp. 67–81.

Golynski, A. 2007. Optimal lower bounds for rank and select indexes. Theoretical Computer

Science 387, 3, 348–359.

Golynski, A. 2009. Cell probe lower bounds for succinct data structures. In Proc. 20th Annual

ACM-SIAM Symposium on Discrete Algorithms (SODA) (2009), pp. 625–634.

Golynski, A., Munro, I., and Rao, S. 2006. Rank/select operations on large alphabets: a tool for
text indexing. In Proc. 17th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)
(2006), pp. 368–373.

Golynski, A., Raman, R., and Rao, S. 2008. On the redundancy of succinct data structures.
In Proc. 11th Scandinavian Workshop on Algorithm Theory (SWAT), LNCS 5124 (2008), pp.
148–159.

Grossi, R., Gupta, A., and Vitter, J. 2003. High-order entropy-compressed text indexes. In
Proc. 14th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA) (2003), pp. 841–
850.

Grossi, R., Orlandi, A., and Raman, R. 2010. Optimal trade-oﬀs for succinct string indexes.
In Proc. 37th International Colloquim on Automata, Languages and Programming (ICALP)
(2010), pp. 678–689.

Gupta, A., Hon, W.-K., Shah, R., and Vitter, J. 2006. Dynamic rank/select dictionaries with
applications to XML indexing. Technical Report CSD TR #06-014 (July), Purdue University.
Gupta, A., Hon, W.-K., Shah, R., and Vitter, J. S. 2007. Compressed data structures: Dictionaries 
and data-aware measures. Theoretical Computer Science 387, 3, 313–331.

Hern´andez, C. and Navarro, G. 2012. Compressed representation of Web and social networks
via dense subgraphs. In Proc. 19th International Symposium on String Processing and Information 
Retrieval (SPIRE), LNCS 7608 (2012), pp. 264–276.

Jacobson, G. 1989. Space-eﬃcient static trees and graphs. In Proc. 30th IEEE Symposium on

Foundations of Computer Science (FOCS) (1989), pp. 549–554.

M¨akinen, V. and Navarro, G. 2007. Rank and select revisited and extended. Theoretical

Computer Science 387, 3, 332–347.

Manzini, G. 2001. An analysis of the Burrows-Wheeler transform. Journal of the ACM 48, 3,

407–430.

Munro, I. 1996. Tables. In Proc. 16th Conference on Foundations of Software Technology and

Theoretical Computer Science (FSTTCS), LNCS 1180 (1996), pp. 37–42.

ACM Transactions on Algorithms, Vol. TBD, No. TDB, Month Year.

·

22

Optimal Lower and Upper Bounds for Representing Sequences

Munro, J. I., Raman, R., Raman, V., and Rao, S. S. 2003. Succinct representations of permutations.
 In Proc. 30th International Colloquium on Algorithms, Languages and Programming
(ICALP) (2003), pp. 345–356.

Okanohara, D. and Sadakane, K. 2007. Practical entropy-compressed rank/select dictionary.
In Proc. 9th Workshop on Algorithm Engineering and Experiments (ALENEX) (2007), pp.
60–70.

P˘atras¸cu, M. 2008. Succincter.

In Proc. 49th Annual IEEE Symposium on Foundations of

Computer Science (FOCS) (2008), pp. 305–313.

P˘atras¸cu, M. and Thorup, M. 2006. Time-space trade-oﬀs for predecessor search. In Proc. 38th

Annual ACM Symposium on Theory of Computing (STOC) (2006), pp. 232–240.

P˘atras¸cu, M. and Thorup, M. 2008.

Time-space trade-oﬀs

for predecessor

search.

CoRR cs/0603043v1. http://arxiv.org/pdf/cs/0603043v1.

P˘atras¸cu, M. and Viola, E. 2010. Cell-probe lower bounds for succinct partial sums. In Proc.

21st Annual ACM-SIAM Symposium on Discrete Algorithms (SODA) (2010), pp. 117–122.

Raman, R., Raman, V., and Rao, S. S. 2007. Succinct indexable dictionaries with applications
to encoding k-ary trees, preﬁx sums and multisets. ACM Transactions on Algorithms 3, 4,
article 43.

V¨alim¨aki, N. and M¨akinen, V. 2007. Space-eﬃcient algorithms for document retrieval. In Proc.
18th Annual Symposium on Combinatorial Pattern Matching (CPM), LNCS 4580 (2007), pp.
205–215.

A. UPPER BOUND FOR PREDECESSOR SEARCH

We describe a data structure that stores a set S of n elements from universe U =
[1, u] in O(n lg(u/n)) bits of space, while supporting predecessor queries in time
O(lg lg u−lg n
lg w ). We ﬁrst start with a solution that uses O(n lg u) bits of space. We
use a variant of the traditional recursive van Emde Boas solution [P˘atra¸scu and
Thorup 2008]. Let (cid:96) ≥ lg u be the length of the keys. We choose (cid:96) as the smallest
value of the form (cid:96) = (lg w − 1) · 2i ≥ lg u, for some integer i ≥ 0 (note (cid:96) ≤ 2 lg u).
We denote the predecessor data structure that stores a set S of keys of length (cid:96)
by D(cid:96)(S). Given an element x the predecessor data structure should return a pair
(y, r) where y is the predecessor of x in S (i.e., the maximum value ≤ x in S) and
r is the rank of y in S (i.e., the number of elements of S smaller than or equal to
y). If the key x has no predecessor in S (i.e., it is smaller than any key in S), the
query should return (0, 0).

We now describe the solution. We partition the set S according to the most
signiﬁcant (cid:96)/2 bits. We call h(x) the (cid:96)/2 most signiﬁcant bits of x, and l(x) the
(cid:96)/2 least signiﬁcant bits of x, x = 2(cid:96)/2h(x) + l(x).
Let Sp = {x ∈ S, h(x) = p} denote the set of all the elements x such that h(x) =
p, let S(cid:48)
p denote the set Sp deprived of its minimal and maximal elements, and let
ˆSp = {l(x), x ∈ S(cid:48)
p. Furthermore,
let P = {h(x), x ∈ S} denote the set of all distinct values of h(x) in S. The data
structure consists of the following components:

p} denote the set of lower parts of elements in S(cid:48)

(1) A predecessor data structure D(cid:96)/2(P ).
(2) A predecessor data structure D(cid:96)/2( ˆSp) for each p ∈ P where ˆSp is non-empty.
(3) A dictionary I(P ) (a perfect hash function with constant time and linear space)
that stores the set P . To each element p ∈ P , the dictionary associates the tuple 
(m, rm, M, rM , q) with m (respectively M ) being the smallest (respectively

ACM Transactions on Algorithms, Vol. TBD, No. TDB, Month Year.

D. Belazzougui, G. Navarro

·

23

largest) element in Sp, rm (respectively rM ) being the rank of m (respectively
M ) in S, and q a pointer to D(cid:96)/2( ˆSp).

We have described the recursive data structure. The base case is a predecessor
data structure Dlg w−1(S) for a set S of size t. Note that the set S is a subset of U =
[1, 2lg w−1] = [1, w/2]. This structure is technical and is described in Section A.1.
It encodes S using O(t lg |U|) = O(t lg w) bits and answers predecessor queries in
constant time.

We now get back to the main data structure and describe how queries are done
on it. Given a key x, we ﬁrst query I(P ) for the key p = h(x). Now, depending on
the result, we have two cases:
(1) The dictionary does not ﬁnd p. Then we query D(cid:96)/2(P ) for the key p− 1. This
returns a pair (y, r). If (y, r) = (0, 0) we return (0, 0). Else we search I(P ) for
y, which returns a tuple (m, rm, M, rM , q), and the ﬁnal answer is (M, rM ).

(2) The dictionary ﬁnds p and returns a tuple (m, rm, M, rM , q). We have the

following subcases:
(a) We have x < m. Then we proceed exactly as in case 1.
(b) We have x = m, then the answer is (m, rm).
(c) We have x ≥ M , then the answer is (M, rM ).
(d) We have m < x < M . Then we query D(cid:96)/2( ˆSp) (pointed by q) for the
key l(x). This returns a tuple (y, r). The ﬁnal answer answer is (2(cid:96)/2 p +
y, rm + r) if (y, r) (cid:54)= (0, 0) and (m, rm) otherwise.

Time analysis. We query the data structures D(cid:96)/2i

(.) for i = 0, . . . until (cid:96)/2i =
lg w − 1 (we may stop the recursion before reaching this point). For each recursive
step we spend constant time querying the dictionary. Thus the global query time
is upper bounded by O(lg (cid:96)

lg w ).

Space analysis. The space can be proved to be O(n lg u) bits by induction. Let
us ﬁrst focus on the storage of the components (m, rm, M, rM ) of the dictionaries,
which need (cid:96) bits each. For the base case (cid:96) = lg w−1 we have that t keys are encoded
using O(t lg w) bits. Now for any recursive data structure D(cid:96)(S) we notice that the
p np ≤
n. We store the dictionary I(P ), which uses O(n(cid:96)) bits, and the substructures
D(cid:96)/2( ˆSp). We denote by s((cid:96),|S(cid:48)|) the space usage of any D(cid:96)(|S|). Then the space
p s((cid:96)/2, np) + O(n(cid:96)). The

substructures D(cid:96)/2( ˆSp) are disjoint. Let us call np = | ˆSp| and n = |S|, then(cid:80)
usage of our D(cid:96)(S) follows the recurrence s((cid:96), n) = (cid:80)

solution to this recurrence is O(n(cid:96)) = O(n lg u).

In addition, the dictionaries store pointers q, whose size does not halve from one
level to the next. Yet, since each of the n elements is stored in only one structure
D(·), there are at most n such structures and pointers to them. As the rest of the
data occupies O(n(cid:96)) bits, we need n pointers of size O(lg n + lg (cid:96)) = O(lg u) bits.2
Thus the space is O(n lg u) bits.

2In the tuples we must avoid using lg u bits for null pointers. Rather, we use just a bitmap (with
one bit per tuple) to tell whether the pointer is null or not, and store the non-null pointers in a
separate memory area indexed by rank over this bitmap.

ACM Transactions on Algorithms, Vol. TBD, No. TDB, Month Year.

·

24

Optimal Lower and Upper Bounds for Representing Sequences

A.1 Predecessor Queries on Short Keys

we have a set S of t keys, each of length (cid:96) = (lg w)/2− 1. Clearly t ≤ √
We now describe the base case of the recursion for O(lg w)-bit keys. Suppose that
w/2. What
we want is to do predecessor search for any x over the set S. For that we ﬁrst sort
block B of t((cid:96) + 1) consecutive bits (this uses t(lg w)/2 ≤ √
the keys (in ascending order) obtaining an array A[1, t]. Then we pack them in a
w(lg w)/4 ≤ w bits,
which is less than one word) where each key is separated from the other by a zero
bit. That is, we store the element A[i] in the bits B[(i − 1)((cid:96) + 1) + 1, i((cid:96) + 1) − 1]
and store a zero at bit B[i((cid:96) + 1)].

We now show how to do a predecessor query for a key x on S in constant time.

This is done in the following steps:

(1) We ﬁrst duplicate the key x, t times, and set the separator bits. That is, we
compute X = (x · (0(cid:96)1)t) or (10(cid:96))t.
(2) We subtract B from X, obtaining Y = X − B. This does in parallel the
computation of x − A[i] for all 1 ≤ i ≤ t, and the result of each subtraction
(negative or nonnegative) is stored in the separator bit Y [i((cid:96) + 1)].

(3) We mask all but separator bits. That is, we compute Z = Y and (10(cid:96))t.
(4) We ﬁnally determine the rank of x. If Z = 0 then we answer (0, 0). Otherwise,
to ﬁnd the ﬁrst 1 in Z, we create a small universal mmphf storing the values
{2(cid:96)i, 1 ≤ i ≤ t}, which takes constant time and O(t lg w) = O(
w lg w) = o(w)
bits. With the position of the bit we easily compute the rank r and extract the
answer y from the corresponding ﬁeld in B, so as to answer (y, r).

√

A.2 Reducing Space Usage

We now describe how the space usage can be improved to O(n lg(u/n)). For this
we use a standard idea. We partition the set S into n(cid:48) = 2(cid:98)lg n(cid:99) partitions using
the lg n(cid:48) most signiﬁcant bits. For all the keys in a partition Sp, we have that the
lg n(cid:48) most signiﬁcant bits are equal to p. Let ˆSp denote the set that contains the
elements of Sp truncated to their lg u− lg n(cid:48) least signiﬁcant bits. We now build an
independent predecessor data structure Dlg u−lg n(cid:48)
( ˆSp). Each such data structure
occupies at most c(|Sp|(lg u−lg n(cid:48))) bits, for some constant c. We compact all those
data structures in a memory area A of cn cells of lg u − lg n(cid:48) bits.
A bitvector B[1, n + n(cid:48)] stores the size of the predecessor data structures. That
is, for each p ∈ [1, n(cid:48)] we append to B as many 1s as the number of elements inside
Sp, followed by a 0. Then, to compute the predecessor of a key x in S, we ﬁrst
compute p = h(x) (here h(x) extracts the lg n(cid:48) most signiﬁcant bits and l(x) the
lg u− lg n(cid:48) least signiﬁcant bits). Then we compute r0 = select0(B, p)− p, which is
the number of elements in Sq for all q < p. Then we query Dlg u−lg n(cid:48)
( ˆSp) (whose
data structure starts at A[c · r0(lg u − lg n(cid:48))]) for the key l(x), which returns an
answer (y, r). We now have two cases:
(1) If the returned answer is (y, r) (cid:54)= (0, 0), then the ﬁnal answer is just (pn(cid:48) +

y, r0 + r).

(2) Otherwise, the rank of the answer is precisely r0, but we must ﬁnd the set Sp(cid:48)

that contains it in order to ﬁnd its value. There are two subcases:
(a) If r0 = 0, then there is no previous element and we return (0, 0).

ACM Transactions on Algorithms, Vol. TBD, No. TDB, Month Year.

D. Belazzougui, G. Navarro

·

25

(b) Else we compute the desired index, p(cid:48) = select1(r0) − r0, and query
. This must reDlg 
u−lg n(cid:48)
turn a pair (y, r), and the ﬁnal answer is (p(cid:48)n(cid:48) + y, r0).

( ˆSp(cid:48)) for the maximum possible key, 1lg u−lg n(cid:48)

Since B has O(n) bits, it is easy to see that the data structure occupies O(n(lg u−
lg w ). We thus have proved the

lg n)) bits and it answers queries in time O(lg lg u−lg n
following theorem:

Theorem 14. Given a set S of n keys over universe [1, u], there is a data structure 
that occupies O(n(lg(u/n))) bits of space and answers predecessor queries in
time O(lg lg(u/n)

lg w ).

ACM Transactions on Algorithms, Vol. TBD, No. TDB, Month Year.

