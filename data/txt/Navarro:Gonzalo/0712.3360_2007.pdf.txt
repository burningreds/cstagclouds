7
0
0
2
 
c
e
D
0
2

 

 
 
]
S
D
.
s
c
[
 
 

1
v
0
6
3
3

.

2
1
7
0
:
v
i
X
r
a

Compressed Text Indexes:From Theory to

Practice!∗

Paolo Ferragina 1 Rodrigo Gonz´alez 2
Gonzalo Navarro 2 Rossano Venturini 1

1 Dept. of Computer Science, University of Pisa.

{ferragina,rventurini}@di.unipi.it

2 Dept. of Computer Science, University of Chile.
{rgonzale,gnavarro}@dcc.uchile.cl

February 13, 2013

Abstract

A compressed full-text self-index represents a text in a compressed
form and still answers queries eﬃciently. This technology represents a
breakthrough over the text indexing techniques of the previous decade,
whose indexes required several times the size of the text. Although it
is relatively new, this technology has matured up to a point where theoretical 
research is giving way to practical developments. Nonetheless
this requires signiﬁcant programming skills, a deep engineering eﬀort,
and a strong algorithmic background to dig into the research results.
To date only isolated implementations and focused comparisons of
compressed indexes have been reported, and they missed a common
API, which prevented their re-use or deployment within other applications.


The goal of this paper is to ﬁll this gap. First, we present the
existing implementations of compressed indexes from a practitioner’s

∗Partially supported by Yahoo! Research grants ”Data compression and indexing in
hierarchical memories” (Barcelona) and ”Compressed data structures” (Chile), Italian
MIUR grants Italy-Israel FIRB ”Pattern Discovery Algorithms in Discrete Structures, with
Applications to Bioinformatics”, PRIN ”MainStream MAssive INformation structures and
dataSTREAMs”, and Millennium Nucleus Center for Web Research, Grant P04-067-F,
Mideplan, Chile.

1

point of view. Second, we introduce the Pizza&Chili site, which oﬀers
tuned implementations and a standardized API for the most successful 
compressed full-text self-indexes, together with eﬀective testbeds
and scripts for their automatic validation and test. Third, we show
the results of our extensive experiments on these codes with the aim
of demonstrating the practical relevance of this novel and exciting
technology.

1 Introduction

A large fraction of the data we process every day consists of a sequence of
symbols over an alphabet, and hence is a text. Unformatted natural language,
XML and HTML collections, program codes, music sequences, DNA and
protein sequences, are just the typical examples that come to our mind when
thinking of text incarnations. Most of the manipulations required over those
sequences involve, sooner or later, searching those (usually long) sequences
for (usually short) pattern sequences. Not surprisingly, text searching and
processing has been a central issue in Computer Science research since its
beginnings.

Despite the increase in processing speeds, sequential text searching long
ago ceased to be a viable alternative for many applications, and indexed
text searching has become mandatory. A text index is a data structure built
over a text which signiﬁcantly speeds up searches for arbitrary patterns, at
the cost of some additional space. The inverted list structure [38] is an
extremely popular index to handle so-called “natural language” text, due to
its simplicity, low space requirements, and fast query times. An inverted
list is essentially a table recording the occurrences of every distinct text
word. Thus every word query is already precomputed, and phrase queries
are carried out essentially via list intersections. Although inverted lists are
ubiquitous in current Web search engines and IR systems, they present three
main limitations: (1) there must exist a clear concept of “word” in the text,
easy to recognize automatically; (2) those words must follow some statistical
rules, so that there are not too many diﬀerent words in a text, otherwise the
table is too large; (3) one can search only for whole words or phrases, not for
any substring.

There are applications in which these limitations are unacceptable — e.g.
bio-informatics, computational linguistics, multimedia databases, search engines 
for agglutinating and Far East languages — and thus full-text indexing
must be used. With this term we mean an index which is able to support
so-called substring searches, that is, searches not limited to word boundaries

2

on any text T . These indexes will be the focus of our paper.

Although space consumption by itself is not really a problem given the
availability of cheap massive storage, the access speed of that storage has not
improved much, while CPU speeds have been doubling every 18 months, as
well the sizes of the various (internal) memory levels. Given that nowadays
an access to the disk can be up to one million times slower than main memory,
it is often mandatory to ﬁt the index in internal memory and leave as few
data as possible onto disk. Unfortunately, full-text indexing was considered
a technique that inevitably wasted a lot of space: Data structures like suﬃx
trees and suﬃx arrays required at the very least four times the text size to
achieve reasonable eﬃciency.

This situation is drastically changed in less than a decade [34]. Starting in
the year 2000, a rapid sequence of achievements showed how to relate information 
theory with string matching concepts, in a way that index regularities
that show up when the text is compressible were discovered and exploited to
reduce index occupancy without impairing the query eﬃciency. The overall
result has been the design of full-text indexes whose size is proportional to
that of the compressed text. Moreover, those indexes are able to reproduce
any text portion without accessing the original text, and thus they replace
the text — hence the name self-indexes. This way compressed full-text selfindexes 
(compressed indexes, for short) allow one to add search and random
access functionalities to compressed data with a negligible penalty in time
and space performance. For example, it is feasible today to index the 3 GB
Human genome on a 1 GB RAM desktop PC.

Although a comprehensive survey of these theoretical developments has
recently appeared [34], the algorithmic technology underlying these compressed 
indexes requires for their implementation a signiﬁcant programming
skill, a deep engineering eﬀort, and a strong algorithmic background. To
date only isolated implementations and focused comparisons of compressed
indexes have been reported, and they missed a common API, which prevented 
their re-use or deploy within other applications. The present paper
has therefore a threefold purpose:

Algorithmic Engineering. We review the most successful compressed indexes 
that have been implemented so far, and present them in a way that
may be useful for software developers, by focusing on implementation choices
as well on their limitations. We think that this point of view complements
[34] and ﬁxes the state-of-the-art for this technology, possibly stimulating
improvements in the design of such sophisticated algorithmic tools.

In addition, we introduce two novel implementations of compressed indexes.
 These correspond to new versions of the FM-Index, one of which com3


bines the best existing theoretical guarantees with a competitive space/time
tradeoﬀ in practice.

Experimental. We experimentally compare a selected subset of implementations.
 This not only serves to help programmers in choosing the best index
for their needs, but also gives a grasp of the practical relevance of this fascinating 
technology.

Technology Transfer. We introduce the Pizza&Chili site1, which was developed 
with the aim of providing publicly available implementations of compressed 
indexes. Each implementation is well-tuned and adheres to a suitable
API of functions which should, in our intention, allow any programmer to
easily plug the provided compressed indexes within his/her own software.
The site also oﬀers a collection of texts and tools for experimenting and validating 
the proposed compressed indexes. We hope that this simple API
and the good performance of those indexes will spread their use in several
applications.

The use of compressed indexes is obviously not limited to plain text
searching. Every time one needs to store a set of strings which must be
subsequently accessed for query-driven or id-driven string retrieval, one can
use a compressed index with the goal of squeezing the dictionary space without 
slowing down the query performance. This is the subtle need that any
programmer faces when implementing hash tables, tries or other indexing
data structures. Actually, the use of compressed indexes has been successfully 
extended to handle several other more sophisticated data structures,
such as dictionary indexes [12], labeled trees [6, 7], graphs [33], etc. Dealing 
with all those applications is out of the scope of this paper, whose main
goal is to address the above three issues, and comment on the experimental
behavior of this new algorithmic technology.

This paper is organized as follows. Section 2 explains the key conceptual 
ideas underlying the most relevant compressed indexes. Section 3 describes 
how the indexes implement those basic ideas. Section 4 presents the
Pizza&Chili site, and next Section 5 comments on a large suite of experiments
aimed at comparing the most successful implementations of the compressed
indexes present in this site. Finally, Section 6 concludes and explores the
future of the area.

1Available

two
pizzachili.di.unipi.it

at

mirrors:

pizzachili.dcc.uchile.cl

and

4

2 Basic Concepts

Let us introduce some notation. We will refer to strings with S = S[1, ℓ] =
S1,ℓ = s1s2 . . . sℓ to denote a sequence of symbols over an alphabet Σ of size σ.
By S[i, j] = Si,j = sisi+1 . . . sj we will denote substrings of S, which are called
preﬁxes if i = 1 or suﬃxes if j = ℓ. The length of a string will be written
|S| = |S1,ℓ| = ℓ, and the reverse of a string will be written Sr = sℓsℓ−1 . . . s1.
The text searching problem is then stated as follows. Given a text string
T [1, n] and a pattern P [1, m], we wish to answer the following queries: (1)
count the number of occurrences (occ) of P in T ; (2) locate the occ positions
in T where P occurs. In this paper we assume that T can be preprocessed,
and an index is built on it, in order to speed up the execution of subsequent
queries. We assume that the cost of index construction is amortized over
suﬃciently many searches, as otherwise sequential searching is preferable.

In the case of self-indexes, which replace the text, a third operation of

interest is (3) extract the substring Tl,r, given positions l and r in T .

For technical convenience we will assume that the last text character is
tn = $, a special end-marker symbol that belongs to Σ but does not appear
elsewhere in T nor P , and that is lexicographically smaller than any other
symbol in Σ.

2.1 Classical Full-Text Indexes

Many diﬀerent indexing data structures have been proposed in the literature
for text searching, most notably suﬃx trees and suﬃx arrays.

The suﬃx tree [19] of a text T is a trie (or digital tree) built on all the
n suﬃxes Ti,n of T , where unary paths are compressed to ensure O(n) size.
The suﬃx tree has n leaves, each corresponding to a suﬃx of T , and each
internal suﬃx tree node corresponds to a unique substring of T that appears
more than once. The suﬃx tree can count the pattern occurrences in time
O(m), independent of n and occ, by descending in the tree according to the
successive symbols of P (each node should store the number of leaves that
descend from it). Afterwards, it can locate the occurrences in optimal O(occ)
time by traversing the subtree of the node arrived at counting. The suﬃx
tree, however, uses much more space than the text itself. In theoretical terms,
it uses Θ(n log n) bits whereas the text needs n log σ bits (logarithms are in
base 2 unless otherwise stated). In practice, a suﬃx tree requires from 10 to
20 times the text size.

The suﬃx array [26] is a compact version of the suﬃx tree. It still requires
Θ(n log n) bits, but the constant is smaller: 4 times the text size in practice.
The suﬃx array A[1, n] of a text T1,n contains all the starting positions of the

5

suﬃxes of T listed in lexicographical order, that is, TA[1],n < TA[2],n < . . . <
TA[n],n. A can be obtained by traversing the leaves of the suﬃx tree, or it
can be built directly by naive or sophisticated ad-hoc sorting methods [35].
Any substring of T is the preﬁx of a text suﬃx, thus ﬁnding all the
occurrences of P is equivalent to ﬁnding all the text suﬃxes that start with P .
Those form a lexicographical interval in A, which can be binary searched in
O(m log n) time, as each comparison in the binary search requires examining
up to m symbols of the pattern and a text suﬃx. The time can be boosted
to O(m + log n), by using an auxiliary structure that doubles the space
requirement of the suﬃx array [26], or even to O(m+ log |Σ|) by adding some
further data structures (called suﬃx trays [4]). Once the interval A[sp, ep]
containing all the text suﬃxes starting with P has been identiﬁed, counting
is solved as occ = ep−sp+1, and the occurrences are located at A[sp], A[sp+
1], . . . A[ep].

2.2 Backward Search

In the previous section we described the classical binary-search method over
suﬃx arrays. Here we review an alternative approach which has been recently
proposed in [10], hereafter named backward search. For any i = m, m −
1, . . . , 1, this search algorithm keeps the interval A[spi, epi] storing all text
suﬃxes which are preﬁxed by Pi,m. This is done via two main steps:

Initial step. We have i = m, so that it suﬃces to access a precomputed

Inductive step. Let us assume to have computed the interval A[spi+1, epi+1],

table that stores the pair hspm, epmi for all possible symbols pm ∈ Σ.
whose suﬃxes are preﬁxed by Pi+1,m. The present step determines the next
interval A[spi, epi] for Pi,m from the previous interval and the next pattern
symbol pi. The implementation is not obvious, and leads to diﬀerent realizations 
of backward searching in several compressed indexes, with various time
performances.

The backward-search algorithm is executed by decreasing i until either
an empty interval is found (i.e. spi > epi), or A[sp1, ep1] contains all pattern
occurrences.
In the former case no pattern occurrences are found; in the
latter case the algorithm has found occ = ep1 − sp1 + 1 pattern occurrences.

2.3 Rank Query

Given a string S[1, n], function rankx(S, i) returns the number of times symbol 
x appears in the preﬁx S[1, i]. Rank queries are central to compressed
indexing, so it is important to understand how they are implemented and how

6

much space/time they need. We have two cases depending on the alphabet
of S.
Rank over Binary Sequences. In this case there exist simple and practical
constant-time solutions using o(n) bits of space in addition to S [29]. We
cover only rank1 as rank0(S, i) = i − rank1(S, i). One of the most eﬃcient
solutions in practice [15] consists of partitioning S into blocks of size s, and
storing explicit answers for rank-queries done at block beginnings. Answering
rank1(S, i) then consists of summing two quantities: (1) the pre-computed
answer for the preﬁx of S which ends at the beginning of the block enclosing
S[i], plus (2) the relative rank of S[i] within its block. The latter is computed
via a byte-wise scanning of the block, using small precomputed tables. This
solution involves a space/time tradeoﬀ related to s, but nonetheless its querytime 
performance is rather satisfactory already with 5% space overhead on
top of S.
Rank over General Sequences. Given a sequence S[1, n] over an alphabet
of size σ, the wavelet tree [17, 13] is a perfect binary tree of height Θ(log σ),
built on the alphabet symbols, such that the root represents the whole alphabet 
and each leaf represents a distinct alphabet symbol.
If a node v
represents alphabet symbols in the range Σv = [i, j], then its left child vl
represents Σvl = [i, i+j
2 + 1, j].
We associate to each node v the subsequence Sv of S formed by the symbols
in Σv. Sequence Sv is not really stored at the node, but it is replaced by a
bit sequence Bv such that Bv[i] = 0 iﬀ Sv[i] is a symbol whose leaf resides
in the left subtree of v. Otherwise, Bv[i] is set to 1.

2 ] and its right child vr represents Σvr = [ i+j

The power of the wavelet tree is to reduce rank operations over general 
alphabets to rank operations over a binary alphabet, so that the rankmachinery 
above can be used in each wavelet-tree node. Precisely, let us
answer the query rankc(S, i). We start from the root v of the wavelet tree
(with associated vector Bv), and check which subtree encloses the queried
symbol c. If c descends into the right subtree, we set i ← rank1(Bv, i) and
move to the right child of v. Similarly, if c belongs to the left subtree, we
set i ← rank0(Bv, i) and go to the left child of v. We repeat this until we
reach the leaf that represents c, where the current i value is the answer to
rankc(S, i). Since any binary-rank takes O(1) time, the overall rank operation 
takes O(log σ) time.

We note that the wavelet tree can replace S as well: to obtain S[i],
If Bv[i] = 0, then we set
we start from the root v of the wavelet tree.
i ← rank0(Bv, i) and go to the left child. Similarly, if Bv[i] = 1, then we set
i ← rank1(Bv, i) and go to the right child. We repeat this until we reach a
leaf, where the symbol associated to the leaf is the answer. Again, this takes

7

O(log σ) time.

The wavelet tree requires comparable space to the original sequence, as
it requires n log σ (1 + o(1)) bits of space. A practical way to reduce the
space occupancy to the zero-order entropy of S is to replace the balanced
tree structure by the Huﬀman tree of S. Now we have to follow the binary
Huﬀman code of a symbol to ﬁnd its place in the tree. It is not hard to see
that the total number of bits required by such a tree is at most n(H0(S) +
1) + o(n log σ) and the average time taken by rank and access operations is
O(H0(S)), where H0 is the zero-th order empirical entropy of S (see next
section). This structure is the key tool in our implementation of SSA or
AF-index (Section 5).

2.4 The k-th Order Empirical Entropy

The empirical entropy resembles the entropy deﬁned in the probabilistic setting 
(for example, when the input comes from a Markov source), but now
it is deﬁned for any ﬁnite individual string and can be used to measure the
performance of compression algorithms without any assumption on the input
distribution [27].

The empirical zero-order entropy of a text T is deﬁned as

H0(T ) = X

c∈Σ

nc
n

log

n
nc

,

(1)

where nc is the number of occurrences of symbol c in T . This deﬁnition
extends to k > 0 as follows. Let Σk be the set of all sequences of length
k over Σ. For any string w ∈ Σk, called a context of size k, let wT be the
string consisting of the concatenation of individual symbols following w in
T . Then, the k-th order empirical entropy of T is deﬁned as

Hk(T ) =

1
n X
w∈Ak |wT| H0 (wT ) .

(2)

The k-th order empirical entropy captures the dependence of symbols
upon their k-long context. For k ≥ 0, nHk(T ) provides a lower bound to
the number of bits output by any compressor that considers a context of
size k to encode each symbol of T (e.g. PPM-like compressors). Note that
0 ≤ Hk(T ) ≤ Hk−1(T ) ≤ . . . ≤ H1(T ) ≤ H0(T ) ≤ log σ. Several compressed
indexes achieve O(nHk(T r)) bits of space, instead of O(nHk(T )), as they
work on the contexts following (rather than preceding) the symbol to be
encoded. Nonetheless, we will not point out such a diﬀerence because one
can always work on the reversed text (and patterns) if necessary, and also
because both k-th order entropies diﬀer by lower order terms [10].

8

2.5 The Burrows-Wheeler Transform

The Burrows-Wheeler Transform (BWT) [3] is a key tool in designing compressed 
full-text indexes. It is a reversible permutation of T , which has the
nice property of putting together symbols followed by the same context. This
ensures that the permuted T oﬀers better compression opportunities: a locally 
adaptive zero-order compressor is able to achieve on this string the k-th
order entropy of T (recall Eq. (2)). The BW-transform works as follows:

1. Create a conceptual matrix M, whose rows are cyclic shifts of T .

2. Sort the matrix rows lexicographically.

3. Deﬁne the last column of M as the BWT of T , and call it T bwt.

There is a close relationship between matrix M and the suﬃx array A of
text T , because when we lexicographically sort the rows, we are essentially
sorting the suﬃxes of T (recall indeed that tn = $ is smaller than any other
alphabet symbol). Speciﬁcally, A[i] points to the suﬃx of T which preﬁxes
the i-th row of M. Hence, another way to describe T bwt is to concatenate
the symbols that precede each suﬃx of T in the order listed by A, that is,
T bwt = tA[1]−1 tA[2]−1 . . . tA[n]−1, where we assume that t0 = tn.

Given the way matrix M has been built, all columns of M are permutations 
of T . So the ﬁrst and last column of M are indeed one a permutation
of the other. The question is how to map symbols in the last column T bwt
to symbols in the ﬁrst column. It is easy to see [3] that occurrences of equal
symbols preserve their relative order in the last and the ﬁrst columns of M.
Thus the j-th occurrence of a symbol c within T bwt corresponds to the jIf 
c = T bwt[i], then we have that
th occurrence of c in the ﬁrst column.
j = rankc(T bwt, i) in the last column; whereas in the ﬁrst column, where
the symbols are sorted alphabetically, the j-th occurrence of c is at position
C[c]+j, where C[c] counts the number of occurrences in T of symbols smaller
than c. By plugging one formula in the other we derive the so called Last-
to-First column mapping (or, LF-mapping): LF (i) = C[c] + rankc(T bwt, i).
We talk about LF-mapping because the symbol c = T bwt[i] is located in the
ﬁrst column of M at position LF (i).

The LF-mapping allows one to navigate T backwards:

if tk = T bwt[i],
then tk−1 = T bwt[LF (i)] because row LF (i) of M starts with tk and thus
ends with tk−1. As a result we can reconstruct T backwards by starting at
the ﬁrst row, equal to $T , and repeatedly applying LF for n steps.

9

3 Compressed Indexes

As explained in the Introduction, compressed indexes provide a viable alternative 
to classical indexes that are parsimonious in space and eﬃcient
in query time. They have undergone signiﬁcant development in the last
years, so that we count now in the literature many solutions that oﬀer a
plethora of space-time tradeoﬀs [34]. In theoretical terms, the most succinct
indexes achieve nHk(T ) + o(n log σ) bits of space, and for a ﬁxed ǫ > 0,
require O(m log σ) counting time, O(log1+ǫ n) time per located occurrence,
and O(ℓ log σ + log1+ǫ n) time to extract a substring of T of length ℓ.2 This
is a surprising result because it shows that whenever T [1, n] is compressible
it can be indexed into smaller space than its plain form and still oﬀer search
capabilities in eﬃcient time.

In the following we review the most competitive compressed indexes for
which there is an implementation we are aware of. We will review the FMindex 
family, which builds on the BWT and backward searching; Sadakane’s
Compressed Suﬃx Array (CSA), which is based on compressing the suﬃx
array via a so-called Ψ function that captures text regularities; and the LZindex,
 which is based on Lempel-Ziv compression. All of them are self-indexes
in that they include the indexed text, which therefore may be discarded.

3.1 The FM-index Family

The FM-index is composed of a compressed representation of T bwt plus auxiliary 
structures for eﬃciently computing generalized rank queries on it. The
main idea [10] is to obtain a text index from the BWT and then use backward 
searching for identifying the pattern occurrences (Sections 2.2 and 2.5).
Several variants of this algorithmic scheme do exist [8, 10, 24, 11] which induce 
several time/space tradeoﬀs for the counting, locating, and extracting
operations.

Counting. The counting procedure takes a pattern P and obtains the interval 
A[sp, ep] of text suﬃxes preﬁxed by it (or, which is equivalent, the
interval of rows of the matrix M preﬁxed by P , see Section 2.5). Fig. 1 gives
the pseudocode to compute sp and ep.

The algorithm is correct: Let [spi+1, epi+1] be the range of rows in M that
start with Pi+1,m, and we wish to know which of those rows are preceded by
pi. These correspond precisely to the occurrences of pi in T bwt[spi+1, epi+1].
Those occurrences, mapped to the ﬁrst column of M, form a (contiguous)

2These locating and extracting complexities are better than those reported in [11], and

can be obtained by setting the sampling step to log1+ǫ

n

log σ .

10

Algorithm FM-count(P1,m)
i ← m, sp ← 1, ep ← n;
while ((sp ≤ ep) and (i ≥ 1)) do

c ← pi;
sp ← C[c] + rankc(T bwt, sp − 1) + 1;
ep ← C[c] + rankc(T bwt, ep);
i ← i − 1;

if (sp > ep) then return “no occurrences of P ” else return hsp, epi;
Figure 1: Algorithm to get the interval A[sp, ep] of text suﬃxes preﬁxed by
P , using an FM-index.

range that is computed with a rationale similar to that for LF (·) in Section 
2.5, and thus via a just two rank operations.

Locating. Algorithm in Fig. 2 obtains the position of the suﬃx that preﬁxes
the i-th row of M. The basic idea is to logically mark a suitable set of rows
of M, and keep for each of them their position in T (that is, we store the
corresponding A values). Then, FM-locate(i) scans the text T backwards
using the LF-mapping until a marked row i′ is found, and then it reports
A[i′] + t, where t is the number of backward steps used to ﬁnd such i′. To
compute the position of all occurrences of a pattern P , it is thus enough to
call FM-locate(i) for every sp ≤ i ≤ ep.
Algorithm FM-locate(i)
i′ ← i, t ← 0;
while A[i′] is not explicitly stored do

i′ ← LF (i′);
t ← t + 1;
return A[i′] + t;

Figure 2: Algorithm to obtain A[i] using an FM-index.

The sampling rate of M’s rows, hereafter denoted by sA, is a crucial
parameter that trades space for query time. Most FM-index implementations 
mark all the A[i] that are a multiple of sA, via a bitmap B[1, n]. All
the marked A[i]s are stored contiguously in suﬃx array order, so that if
B[i] = 1 then one ﬁnds the corresponding A[i] at position rank1(B, i) in
that contiguous storage. This guarantees that at most sA LF-steps are necessary 
for locating the text position of any occurrence. The extra space is
n log n

+ n + o(n) bits.

sA

11

A way to avoid the need of bitmap B is to choose a symbol c having
some suitable frequency in T , and then store A[i] if T bwt[i] = c [8]. Then the
position of A[i] in the contiguous storage is rankc(T bwt, i), so no extra space
is needed other than T bwt. In exchange, there is no guarantee of ﬁnding a
marked cell after a given number of steps.

Extracting. The same text sampling mechanism used for locating permits
extracting text substrings. Given sA, we store the positions i such that A[i]
is a multiple of sA now in the text order (previously we followed the A-driven
order). To extract Tl,r, we start from the ﬁrst sample that follows the area
of interest, that is, sample number d = ⌈(r + 1)/sA⌉. From it we obtain the
desired text backwards with the same mechanism for inverting the BWT (see
Section 2.5), here starting with the value i stored for the d-th sample. We
need at most sA + r − l + 1 applications of the LF-step.

3.2

Implementing the FM-index

All the query complexities are governed by the time required to obtain C[c],
T bwt[i], and rankc(T bwt, i) (all of them implicit in LF as well). While C
is a small table of σ log n bits, the other two are problematic. Counting
requires up to 2m calls to rankc, locating requires sA calls to rankc and
T bwt, and extracting ℓ symbols requires sA + ℓ calls to rankc and T bwt. In
what follows we brieﬂy comment on the solutions adopted to implement those
basic operations.

The original FM-index implementation (FM-index [8]) compressed T bwt
by splitting it into blocks and using independent zero-order compression on
each block. Values of rankc are precomputed for all block beginnings, and the
rest of the occurrences of c from the beginning of the block to any position
i are obtained by sequentially decompressing the block. The same traversal
ﬁnds T bwt[i]. This is very space-eﬀective: It approaches in practice the kth 
order entropy because the partition into blocks takes advantage of the
local compressibility of T bwt. On the other hand, the time to decompress
the block makes computation of rankc relatively expensive. For locating,
this implementation marks the BWT positions where some chosen symbol c
occurs, as explained above.

A very simple and eﬀective alternative to represent T bwt has been proposed 
with the Succinct Suﬃx Array (SSA) [11, 24].
It uses a Huﬀmanshaped 
wavelet tree, plus the marking of one out-of sA text positions for
locating and extracting. The space is n(H0(T ) + 1) + o(n log σ) bits, and the
average time to determine rankc(T bwt, i) and T bwt[i] is O(H0(T ) + 1). The
space bound is not appealing because of the zero-order compression, but the

12

relative simplicity of this index makes it rather fast in practice. In particular,
it is an excellent option for DNA text, where the k-th order compression is
not much better than the zero-th order one, and the small alphabet makes
H0(T ) ≤ log σ small too.
The Run-Length FM-index (RLFM) [24] has been introduced to achieve
k-th order compression by applying run-length compression to T bwt prior to
building a wavelet tree on it. The BWT generates long runs of identical
symbols on compressible texts, which makes the RLFM an interesting alternative 
in practice. The price is that the mappings from the original to the
run-length compressed positions slow down the query operations a bit, in
comparison to the SSA.

3.3 The Compressed Suﬃx Array (CSA)

The compressed suﬃx array (CSA) was not originally a self-index, and required 
O(n log σ) bits of space [18]. Sadakane [37, 36] then proposed a variant
which is a self-index and achieves high-order compression.

The CSA represents the suﬃx array A[1, n] by a sequence of numbers ψ(i),
such that A[ψ(i)] = A[i] + 1. It is not hard to see [37] that ψ is piecewise
monotone increasing in the areas of A where the suﬃxes start with the same
symbol. In addition, there are long runs where ψ(i + 1) = ψ(i) + 1, and these
runs can be mapped one-to-one to the runs in T bwt [34]. These properties
permit a compact representation of ψ and its fast access. Essentially, we
diﬀerentially encode ψ(i) − ψ(i − 1), run-length encode the long runs of 1’s
occurring over those diﬀerences, and for the rest use an encoding favoring
small numbers. Absolute samples are stored at regular intervals to permit
the eﬃcient decoding of any ψ(i). The sampling rate (hereafter denoted by
sψ) gives a space/time tradeoﬀ for accessing and storing ψ.
In [37] it is
shown that the index requires O(nH0(T ) + n log log σ) bits of space. The
analysis has been then improved in [34] to nHk(T ) + O(n log log σ) for any
k ≤ α logσ n and constant 0 < α < 1.
Counting. The original CSA [37] used the classical binary searching to
count the number of pattern occurrences in T . The actual implementation,
proposed in [36], uses backward searching (Section 2.2): ψ is used to obtain
hspi, epii from hspi+1, epi+1i in O(log n) time, for a total of O(m log n) counting 
time. Precisely, let A[spi, epi] be the range of suﬃxes A[j] that start
with pi and such that A[j] + 1 (= A[ψ(j)]) starts with Pi+1,m. The former
is equivalent to the condition [spi, epi] ⊆ [C[pi] + 1, C[pi + 1]]. The latter is
equivalent to saying that spi+1 ≤ ψ(j) ≤ epi+1. Since ψ(i) is monotonically
increasing in the range C[pi] < j ≤ C[pi + 1] (since the ﬁrst characters of

13

suﬃxes in A[spi, epi] are the same), we can binary search this interval to ﬁnd
the range [spi, epi]. Fig. 3 shows the pseudocode for counting using the CSA.

Algorithm CSA-count(P1,m)
i ← m, sp ← 1, ep ← n;
while ((sp ≤ ep) and(i ≥ 1)) do

c ← pi;
hsp, epi ← hmin, maxi {j ∈ [C[c] + 1, C[c + 1]], ψ(j) ∈ [sp, ep]};
i ← i − 1;

if (ep < sp) then return “no occurrences of P ” else return hsp, epi;
Figure 3: Algorithm to get the interval A[sp, ep] preﬁxed by P , using the
CSA. The hmin, maxi interval is obtained via binary search.

Locating. Locating is similar to the FM-index, in that the suﬃx array is
sampled at regular intervals of size sA. However, instead of using the LFmapping 
to traverse the text backwards, this time we use ψ to traverse the
text forward, given that A[ψ(i)] = A[i] + 1. This points out an interesting
duality between the FM-index and the CSA. Yet, there is a fundamental
diﬀerence: function LF (·) is implicitly stored and calculated on the ﬂy over
T bwt, while function ψ(·) is explicitly stored. The way these functions are
calculated/stored makes the CSA a better alternative for large alphabets.

Extracting. Given C and ψ, we can obtain TA[i],n symbolwise from i, as
follows. The ﬁrst symbol of the suﬃx pointed to by A[i], namely tA[i], is
the character c such that C[c] < i ≤ C[c + 1], because all the suﬃxes
A[C[c]+1], . . . , A[C[c+1]] start with symbol c. Now, to obtain the next symbol,
 tA[i]+1, we compute i′ = ψ(i) and use the same procedure above to obtain
tA[i′] = tA[i]+1, and so on. The binary search in C can be avoided by representing 
it as a bit vector D[1, n] such that D[C[c]] = 1, thus c = rank1(D, i).
Now, given a text substring Tl,r to extract, we must ﬁrst ﬁnd the i such
that l = A[i] and then we can apply the procedure above. Again, we sample
the text at regular intervals by storing the i values such that A[i] is a multiple
of sA. To extract Tl,r we actually extract T⌊l/sA⌋·sA,r, so as to start from the
preceding sampled position. This takes sA + r − l + 1 applications of ψ.

3.4 The Lempel-Ziv Index

The Lempel-Ziv index (LZ-index) is a compressed self-index based on a
Lempel-Ziv partitioning of the text. There are several members of this family
[32, 2, 10], we focus on the version described in [32, 2] and available in the

14

Pizza&Chili site. This index uses LZ78 parsing [39] to generate a partitioning
of T1,n into n′ phrases, T = Z1, . . . , Zn′. These phrases are all diﬀerent, and
each phrase Zi is formed by appending a single symbol to a previous phrase
Zj, j < i (except for a virtual empty phrase Z0). Since it holds Zi = Zj · c,
for some j < i and c ∈ Σ, the set is preﬁx-closed. We can then build a trie on
these phrases, called LZ78-trie, which consists of n′ nodes, one per phrase.
The original LZ-index [32] is formed by (1) the LZ78 trie; (2) a trie
formed with the reverse phrases Z r
i , called the reverse trie; (3) a mapping
from phrase identiﬁers i to the LZ78 trie node that represents Zi; and (4) a
similar mapping to Z r
i in the reverse phrases. The tree shapes in (1) and (2)
are represented using parentheses and the encoding proposed in [31] so that
they take O(n′) bits and constant time to support various tree navigation
operations. Yet, we must also store the phrase identiﬁer in each trie node,
which accounts for the bulk of the space for the tries. Overall, we have
4n′ log n′ bits of space, which can be bounded by 4nHk(T ) + o(n log σ) for
k = o(logσ n) [34]. This can be reduced to (2 + ǫ)nHk(T ) + o(n log σ) by
noticing3 that the mapping (3) is essentially the inverse permutation of the
sequence of phrase identiﬁers in (1), and similarly (4) with (2). It is possible
to represent a permutation and its inverse using (1 + ǫ)n′ log n′ bits of space
and access the inverse permutation in O(1/ǫ) time [30].

An occurrence of P in T can be found according to one of the following

situations:

1. P lies within a phrase Zi. Unless the occurrence is a suﬃx of Zi, since
Zi = Zj · c, P also appears within Zj, which is the parent of Zi in the
LZ78 trie. A search for P r in the reverse trie ﬁnds all the phrases that
have P as a suﬃx. Then the node mapping permits, from the phrase
identiﬁers stored in the reverse trie, to reach their corresponding LZ78
nodes. All the subtrees of those nodes are occurrences.

2. P spans two consecutive phrases. This means that, for some j, P1,j is a
suﬃx of some Zi and Pj+1,m is a preﬁx of Zi+1. For each j, we search for
P r
1,j in the reverse trie and Pj+1,m in the LZ78 trie, choosing the smaller
subtree of the two nodes we arrived at. If we choose the descendants
of the reverse trie node for P r
1,j, then for each phrase identiﬁer i that
descends from the node, we check whether i + 1 descends from the
node that corresponds to Pj+1,m in the LZ78 trie. This can be done in
constant time by comparing preorder numbers.

3This kind of space reductions is explored in [2]. The one we describe here is not
reported there, but has been developed by those authors for the Pizza&Chili site and will
be included in the journal version of [2].

15

3. P spans three or more nodes. This implies that some phrase is completely 
contained in P , and since all phrases are diﬀerent, there are
only O(m2) diﬀerent phrases to check, one per substring of P . Those
are essentially veriﬁed one by one.

Notice that the LZ-index carries out counting and locating simultaneously,
 which renders the LZ-index not competitive for counting alone. Extracting 
text is done by traversing the LZ78 paths upwards from the desired
phrases, and then using mapping (3) to continue with the previous or next
phrases. The LZ-index is very competitive for locating and extracting.

3.5 Novel Implementations

We introduce two novel compressed index implementations in this paper.
Both are variants of the FM-index family. The ﬁrst one is interesting because
it is a re-engineering of the ﬁrst reported implementation of a self-index [8].
The second is relevant because it implements the self-index oﬀering the best
current theoretical space/time guarantees.
It is fortunate, as it does not
always happen, that theory and practice marry well and this second index is
also relevant in the practical space/time tradeoﬀ map.

3.5.1 The FMI-2

As the original FM-index [8], the FMI-2 adopts a two-level bucketing scheme
for implementing eﬃcient rank and access operations onto T bwt. In detail,
string T bwt is partitioned into buckets and superbuckets: a bucket consists of
lb symbols, a superbucket consists of lsb buckets. Additionally, the FMI-2
maintains two tables: Table Tsb stores, for each superbucket and for each
symbol c, the number of occurrences of c before that superbucket in T bwt;
table Tb stores, for each bucket and for each symbol c, the number of occurrences 
of c before that bucket and up to the beginning of its superbucket. In
other words, Tsb stores the value of the ranking function up to the beginning
of superbuckets; whereas Tb stores the ranking function up to the beginning of
buckets and relative to their enclosing superbuckets. Finally, every bucket is
individually compressed using the sequence of zero-order compressors: MTF,
RLE, Huﬀman (as in bzip2). This compression strategy does not guarantee
that the space of FMI-2 is bounded by the kth order entropy of T . Nevertheless,
 the practical performance is close to the one achievable by the best
known compressors, and can be traded by tuning parameters lb and lsb.

The main diﬀerence between the original FM-index and the novel FMI-2
relies in the strategy adopted to select the rows/positions of T which are

16

explicitly stored. The FMI-2 marks logically and uniformly the text T by
adding a special symbol every sA symbols of the original text. This way, all
of the M’s rows that start with that special symbol are contiguous, and thus
their positions can be stored and accessed easily.

The count algorithm is essentially a backward search (Algorithm 1), modiﬁed 
to take into account the presence of special symbols added to the indexed 
text. To search for a pattern P1,p, the FMI-2 actually searches for
min{p − 1, sA} patterns obtained by inserting the special symbols in P at
each sA-th position, and searches for the pattern P itself. This search is
implemented in parallel over all patterns above by exploiting the fact that,
at any step i, we have to search either for Pp−i or for the special symbol. As
a result, the overall search cost is quadratic in the pattern length, and the
output is now a set of at most p ranges of rows.

Therefore, the FMI-2 is slower in counting than the original FM-index,
but locating is faster, and this is crucial because this latter operation is
usually the bottleneck of compressed indexes. Indeed the locate algorithm
proceeds for at most sA phases. Let S0 be the range of rows to be located,
eventually identiﬁed via a count operation. At a generic phase k, Sk contains
the rows that may be reached in k backward steps from the rows in S0. Sk
consists of a set of ranges of rows, rather than a single range. To maintain the
invariant, the algorithm picks up a range of Sk, say [a, b], and determines the
z ≤ |Σ| distinct symbols that occur in the substring T bwt
a,b via two bucket scans
and some accesses to tables Tsb and Tb. Then it executes z backward steps,
one per such symbols, thus determining z new ranges of rows (to be inserted
in Sk+1) which are at distance k + 1 from the rows in S0. The algorithm
cycles over all ranges of Sk to form the new set Sk+1. Notice that if the
rows of a range start with the special symbol, their positions in the indexed
text are explicitly stored, and can be accessed in constant time. Then, the
position of the corresponding rows in S0 can be inferred by summing k to
those values. Notice that this range can be dropped from Sk. After no more
than sA phases the set Sk will be empty.

3.5.2 The Alphabet-Friendly FM-index

The Alphabet-Friendly FM-index (AF-index) [11] resorts to the deﬁnition
of k-th order entropy in Eq. (2), by encoding each substring wT up to its
zero-order entropy. Since all the wT are contiguous in T bwt (regardless of
which k value we are considering), it suﬃces to split T bwt into blocks given
by the k-th order contexts, for any desired k, and to use a Huﬀman-shaped
wavelet tree (see Section 2.3) to represent each such block.
In addition,
we need all rankc values precomputed for every block beginning, as the local

17

wavelet trees can only answer rankc within their blocks. In total, this achieves
nHk(T ) + o(n log σ) bits, for moderate and ﬁxed k ≤ α logσ n and 0 < α < 1.
Actually the AF-index does better, by splitting T bwt in an optimal way, thus
guaranteeing that the space bound above holds simultaneously for every k.
This is done by resorting to the idea of compression boosting [9, 14].

The compression booster ﬁnds the optimal partitioning of T bwt into t
nonempty blocks, s1, . . . , st, assuming that each block sj will be represented
using |sj|H0(sj) + f (|sj|) bits of space, where f (· ) is a nondecreasing concave
function supplied as a parameter. Given that the partition is optimal, it can
be shown that the resulting space is upper bounded by nHk + σkf (n/σk) bits
simultaneously for every k. That is, the index is not built for any speciﬁc k.

As explained, the AF-index represents each block sj by means of a Huﬀmanshaped 
wavelet tree wtj, which will take at most |sj|(H0(sj) + 1) + σ log n
bits. The last term accounts for the storage of the Huﬀman code. In addition,
 for each block j we store an array Cj[c], which tells the rankc values up
to block j. This accounts for other σ log n bits per block. Finally, we need
a bitmap R[1, n] indicating the starting positions of the t blocks in T bwt.
Overall, the formula giving the excess of storage over the entropy for block
j is f (|sj|) = 2|sj| + 2σ log n.
To carry out any operation at position i, we start by computing the block
where position i lies, j = rank1(R, i), and the starting position of that block,
i′ = select1(R, j). (This tells the position of the j-th 1 in R. As it is a sort
of inverse of rank, it is computed by binary search over rank values.) Hence
T bwt[i] = sj[i′′], where i′′ = i − i′ + 1 is the oﬀset of i within block j. Then,
the diﬀerent operations are carried out as follows.

• For counting, we use the algorithm of Fig. 1. In this case, we have
rankc(T bwt, i)= Cj[c]+rankc(sj, i′′), where the latter is computed using
the wavelet tree wtj of sj.

• For locating, we use the algorithm of Fig. 2.

In this case, we have
c = T bwt[i] = sj[i′′]. To compute sj[i′′], we also use the wavelet tree wtj
of sj.

Section 3.1.

• For extracting, we proceed similarly as for locating, as explained in
As a ﬁnal twist, R is actually stored using 2√nt rather than n bits. We
cut R into √nt chunks of length pn/t. There are at most t chunks which
are not all zeros. Concatenating them all requires only √nt bits. A second
bitmap of length √nt indicates whether each chunk is all-zero or not. It is
easy to translate rank/select operations into this representation.

18

4 The Pizza&Chili Site

The Pizza&Chili site has two mirrors: one in Chile (http://pizzachili.dcc.uchile.cl)
and one in Italy (http://pizzachili.di.unipi.it). Its ultimate goal is to push
towards the technology transfer of this fascinating algorithmic technology
lying at the crossing point of data compression and data structure design.
In order to achieve this goal, the Pizza&Chili site oﬀers publicly available
and highly tuned implementations of various compressed indexes. The implementations 
follow a suitable C/C++ API of functions which should, in
our intention, allow any programmer to plug easily the provided compressed
indexes within his/her own software. The site also oﬀers a collection of texts
for experimenting with and validating the compressed indexes. In detail, it
oﬀers three kinds of material:

• A set of compressed indexes which are able to support the search functionalities 
of classical full-text indexes (e.g., substring searches), but
requiring succinct space occupancy and oﬀering, in addition, some text
access operations that make them useful within text retrieval and data
mining software systems.

• A set of text collections of various types and sizes useful to test experimentally 
the available (or new) compressed indexes. The text collections 
have been selected to form a representative sample of diﬀerent
applications where indexed text searching might be useful. The size
of these texts is large enough to stress the impact of data compression
over memory usage and CPU performance. The goal of experimenting
with this testbed is to conclude whether, or not, compressed indexing
is beneﬁcial over uncompressed indexing approaches, like suﬃx trees
and suﬃx arrays. And, in case it is beneﬁcial, which compressed index
is preferable according to the various applicative scenarios represented
by the testbed.

• Additional material useful to experiment with compressed indexes, such
as scripts for their automatic validation and eﬃciency test over the
available text collections.

The Pizza&Chili site hopes to mimic the success and impact of other
initiatives, such as data-compression.info and the Calgary and Canterbury
corpora, just to cite a few. Actually, the Pizza&Chili site is a mix, as it
oﬀers both software and testbeds. Several people have already contributed
to make this site work and, hopefully, many more will contribute to turn
it into a reference for all researchers and software developers interested in

19

experimenting and developing the compressed-indexing technology. The API
we propose is thus intended to ease the deployment of this technology in real
software systems, and to provide a reference for any researcher who wishes to
contribute to the Pizza&Chili repository with his/her new compressed index.

4.1

Indexes

The Pizza&Chili site provides several index implementations, all adhering to
a common API. All indexes, except CSA and LZ-index, are built through the
deep-shallow algorithm of Manzini and Ferragina [28] which constructs the
Suﬃx Array data structure using little extra space and is fast in practice.

• The Suﬃx Array [26] is a plain implementation of the classical index
(see Section 2.1), using either n log n bits of space or simply n computer
integers, depending on the version. This was implemented by Rodrigo
Gonz´alez.

• The SSA [11, 24] uses a Huﬀman-based wavelet tree over the string T bwt
(Section 3.1). It achieves zero-order entropy in space with little extra
overhead and striking simplicity. It was implemented by Veli M¨akinen
and Rodrigo Gonz´alez.

• The AF-index [11] combines compression boosting [5] with the above
wavelet tree data structure (Section 3.5.2). It achieves high-order compression,
 at the cost of being more complex than SSA. It was implemented 
by Rodrigo Gonz´alez.

• The RLFM [24] is an improvement over the SSA (Section 3.1), which
exploits the equal-letter runs of the BWT to achieve k-th order compression,
 and in addition uses a Huﬀman-shaped wavelet tree.
It is
slightly larger than the AF-index. It was implemented by Veli M¨akinen
and Rodrigo Gonz´alez.

• The FMI-2 (Section 3.5.1) is an engineered implementation of the original 
FM-index [8], where a diﬀerent sampling strategy is designed in
It was
order to improve the performance of the locating operation.
implemented by Paolo Ferragina and Rossano Venturini.

• The CSA [37, 36] is the variant using backward search (Section 3.3).
It achieves high-order compression and is robust for large alphabets.
It was implemented by Kunihiko Sadakane and adapted by Rodrigo
Gonz´alez to adhere the API of the Pizza&Chili site. To construct

20

the suﬃx array, it uses the qsufsort by Jesper Larsson and Kunihiko
Sadakane [22].

• The LZ-index [32, 2] is a compressed index based on LZ78 compression
(Section 3.4), implemented by Diego Arroyuelo and Gonzalo Navarro.
It achieves high-order compression, yet with relatively large constants.
It is slow for counting but very competitive for locating and extracting.

These implementations support any byte-based alphabet of size up to
255 symbols: one symbol is automatically reserved by the indexes as the
terminator “$”.

In the following two sections we are going to explain the implementation

of FMI2 and AF-index.

4.2 Texts

We have chosen the texts forming the Pizza&Chili collection by following
three basic considerations. First, we wished to cover a representative set of
application areas where the problem of full-text indexing might be relevant,
and for each of them we selected texts freely available on the Web. Second,
we aimed at having one ﬁle per text type in order to avoid unreadable tables
of many results. Third, we have chosen the size of the texts to be large
enough in order to make indexing relevant and compression apparent. These
are the current collections provided in the repository:

• dna (DNA sequences). This ﬁle contains bare DNA sequences without
descriptions, separated by newline, obtained from ﬁles available at
the Gutenberg Project site: namely, from 01hgp10 to 21hgp10, plus
0xhgp10 and 0yhgp10. Each of the four DNA bases is coded as an
uppercase letter A,G,C,T, and there are a few occurrences of other
special symbols.

• english (English texts). This ﬁle is the concatenation of English texts
selected from the collections etext02 —etext05 available at the Gutenberg 
Project sitei. We deleted the headers related to the project so as
to leave just the real text.

• pitches (MIDI pitch values). This ﬁle is a sequence of pitch values
(bytes whose values are in the range 0-127, plus a few extra special
values) obtained from a myriad of MIDI ﬁles freely available on the
Internet. The MIDI ﬁles were converted into the IRP format by using
the semex tool by Kjell Lemstrom [23]. This is a human-readable tuple

21

Table 1: General statistics for our indexed texts.

Size (MB) Alphabet size
16
225
133
25
230
96

200
200
50
200
200
200

Text
dna
english
pitches
proteins
sources
xml

Inv. match prob.
3.86
15.12
40.07
16.90
24.81
28.65

format, where the 5th column is the pitch value. The pitch values were
coded in one byte each and concatenated all together.

• proteins (protein sequences). This ﬁle contains bare protein sequences
without descriptions, separated by newline, obtained from the Swissprot 
database (ftp.ebi.ac.uk/ pub/databases/swissprot/). Each of
the 20 amino acids is coded as an uppercase letter.

• sources (source program code). This ﬁle is formed by C/Java source
codes obtained by concatenating all the .c, .h, .C and .java ﬁles of the
linux-2.6.11.6 (ftp.kernel.org) and gcc-4.0.0 (ftp.gnu.org) distributions.


• xml (structured text). This ﬁle is in XML format and provides bibliographic 
information on major computer science journals and proceedings.
 It was downloaded from the DBLP archive at dblp.uni-trier.de.

For the experiments we have limited the short ﬁle pitches to its initial
50 MB, whereas all the other long ﬁles have been cut down to their initial
200 MB. We show now some statistics on those ﬁles. These statistics and
the tools used to compute them are available at the Pizza&Chili site.

Table 1 summarizes some general characteristics of the selected ﬁles. The
last column, inverse match probability, is the reciprocal of the probability of
matching between two randomly chosen text symbols. This may be considered 
as a measure of the eﬀective alphabet size — indeed, on a uniformly
distributed text, it would be precisely the alphabet size.

Table 2 provides some information about the compressibility of the texts
by reporting the value of Hk for 0 ≤ k ≤ 4, measured as number of bits per
input symbol. As a comparison on the real compressibility of these texts,
Table 3 shows the performance of three well-known compressors (sources
available in the site): gzip (Lempel-Ziv-based compressor), bzip2 (BWTbased 
compressor), and PPMDi (k-th order modeling compressor). Notice

22

Table 2: Ideal compressibility of our indexed texts. For every k-th order
model, with 0 ≤ k ≤ 4, we report the number of distinct contexts of length
k, and the empirical entropy Hk, measured as number of bits per input
symbol.

Text

dna
english
pitches
proteins
sources
xml

log σ
4.000
7.814
7.055
4.644
7.845
6.585

H0

1.974
4.525
5.633
4.201
5.465
5.257

1st order
#
H1
16
225
133
25
230
96

1.930
3.620
4.734
4.178
4.077
3.480

H2

2nd order
#
152
10829
10946
607
9525
7049

1.920
2.948
4.139
4.156
3.102
2.170

3rd order
#

H3

4th order
#

H4

1.916
2.422
3.457
4.066
2.337
1.434

683
102666
345078
11607
253831
141736

1.910
2.063
2.334
3.826
1.852
1.045

2222
589230
3845792
224132
1719387
907678

Table 3: Real compressibility of our indexed texts, as achieved by the bestknown 
compressors: gzip (option -9), bzip2 (option -9), and PPMDi (option
-l 9).

H4
Text
1.910
dna
2.063
english
2.334
pitches
proteins 3.826
1.852
sources
1.045
xml

gzip
2.162
3.011
2.448
3.721
1.790
1.369

bzip2 PPMDi
1.943
2.076
2.246
1.957
2.439
2.890
3.276
3.584
1.493
1.016
0.745
0.908

that, as k grows, the value of Hk decreases but the size of the dictionary of
length-k contexts grows signiﬁcantly, eventually approaching the size of the
text to be compressed. Typical values of k for PPMDi are around 5 or 6. It
is interesting to note in Table 3 that the compression ratios achievable by
the tested compressors may be superior to H4, because they use (explicitly
or implicitly) longer contexts.

5 Experimental Results

In this section we report experimental results from a subset of the compressed
indexes available at the Pizza&Chili site. All the experiments were executed
on a 2.6 GHz Pentium 4, with 1.5 GB of main memory, and running Fedora
Linux. The searching and building algorithms for all compressed indexes
were coded in C/C++ and compiled with gcc or g++ version 4.0.2.

We restricted our experiments to a few indexes: Succinct Suﬃx Array 
(version SSA v2 in Pizza&Chili), Alphabet-Friendly FM-index (version
AF-index v2 in Pizza&Chili), Compressed Suﬃx Array (CSA in Pizza&Chili),

23

Table 4: Parameters used for the diﬀerent indexes in our experiments. The
cases of multiple values correspond to space/time tradeoﬀ curves.

Index
AF-index
CSA
LZ-index
SSA

locate / extract

count
−
sψ = {128} sA = {4, 16, 32, 64, 128, 256}; sψ = {128}
ǫ = { 1
4}
−

sA = {4, 16, 32, 64, 128, 256}

sA = {4, 16, 32, 64, 128, 256}

ǫ = {1, 1

2, 1

3, 1

4, 1

5, 1

10, 1
20}

and LZ-index (version LZ-index4 in Pizza&Chili), because they are the best
representatives of the three classes of compressed indexes we discussed in
Section 3. This small number will provide us with a succinct, yet signiﬁcant,
picture of the performance of all known compressed indexes [34].

There is no need to say that further algorithmic engineering of the indexes 
experimented in this paper, as well of the other indexes available in
the Pizza&Chili site, could possibly change the charts and tables shown below.
 However, we believe that the overall conclusions drawn from our experiments 
should not change signiﬁcantly, unless new algorithmic ideas are
devised for them.
Indeed, the following list of experimental results has a
twofold goal: on one hand, to quantify the space and time performance of
compressed indexes over real datasets, and on the other hand, to motivate
further algorithmic research by highlighting the limitations of the present
indexes and their implementations.

5.1 Construction

Table 4 shows the parameters used to construct the indexes in our experiments.
 Table 5 shows construction time and space for one collection, namely
english, as all the others give roughly similar results. The bulk of the time
of SSA and CSA is that of suﬃx array construction (prior to its compres-
sion). The times diﬀer because diﬀerent suﬃx array construction algorithms
are used (see Section 4.1). The AF-index takes much more time because it
needs to run the compression boosting algorithm over the suﬃx array. The
LZ-index spends most of the time in parsing the text and creating the LZ78
and reverse tries. In all cases construction times are practical, 1–4 sec/MB
with our machine.

The memory usage might be problematic, as it is 5–9 times the text size.
Albeit the ﬁnal index is small, one needs much memory to build it ﬁrst4. This
is a problem of compressed indexes, which is attracting a lot of practical and

4In particular, this limited us to indexing up to 200 MB of text in our machine.

24

Table 5: Time and peak of main memory usage required to build the various
indexes over the 200 MB ﬁle english. The indexes are built using the default
value for the locate tradeoﬀ (that is, sA = 64 for AF-index and SSA; sA = 64
and sψ = 128 for CSA; and ǫ = 1

4 for the LZ-index).

Index
AF-index
CSA
LZ-index
SSA

Build Time (sec) Main Memory Usage (MB)

772
423
198
217

1, 751
1, 801
1, 037
1, 251

theoretical research [21, 1, 20, 25].

We remark that the indexes allow diﬀerent space/time tradeoﬀs. The SSA
and AF-index have a sampling rate parameter sA that trades locating and
extracting time for space. More precisely, they need O(sA) accesses to the
wavelet tree for locating, and O(sA + r − l + 1) accesses to extract Tl,r, in
exchange for n log n
additional bits of space. We can remove those structures
if we are only interested in counting.

sA

The CSA has two space/time tradeoﬀs. A ﬁrst one, sψ, governs the access
time to ψ, which is O(sψ) in exchange for n log n
bits of space required by the
samples. The second, sA, aﬀects locating and extracting time just as above.
For pure counting we can remove the sampling related to sA, whereas for
locating the best is to use the default value (given by Sadakane) of sψ = 128.
The best choice for extracting is less clear, as it depends on the length of the
substring to extract.

sψ

Finally, the LZ-index has one parameter ǫ which trades counting/locating
time per space occupancy: The cost per candidate occurrence is multiplied
by 1
ǫ , and the additional space is 2ǫnHk(T ) bits. No structure can be removed
in the case of counting, but space can be halved if the extract operation is
the only one needed (just remove the reverse trie).

5.2 Counting

We searched for 50, 000 patterns of length m = 20, randomly chosen from
the indexed texts. The average counting time was then divided by m to
display counting time per symbol. This is appropriate because the counting
time of the indexes is linear in m, and 20 is suﬃciently large to blur small
constant overheads. The exception is the LZ-index, whose counting time is
superlinear in m, and not competitive at all for this task.

Table 6 shows the results on this test. The space of the SSA, AF-index,

25

Table 6: Experiments on the counting of pattern occurrences. Time is measured 
in microseconds per pattern symbol. The space usage is expressed as
a fraction of the original text size. We put in boldface those results that lie
within 10% of the best space/time tradeoﬀs.

SSA

AF-index

CSA

LZ-index

plain SA

Text

Time

Space

0.956

2.147

dna
english
pitches
proteins 1.905
2.635
sources
2.764
xml

2.195

0.29
0.60
0.74
0.56
0.72
0.69

Time
1.914
2.694
2.921
3.082
2.946
2.256

0.42

0.28

Space Time
5.220
4.758
3.423
6.477
4.345
4.321

0.56
0.49
0.34

0.66

Space
0.46
0.44

0.63
0.67
0.38

0.29

Time
43.896
68.774
55.314
47.030
162.444
306.711

Space Time
0.542
0.512
0.363
0.479
0.499
0.605

0.93
1.27
1.95
1.81
1.27
0.71

Space
5
5
5
5
5
5

and CSA does not include what is necessary for locating and extracting. We
can see that, as expected, the AF-index is always smaller than the SSA, yet
they are rather close on dna and proteins (where the zero-order entropy
is not much larger than higher-order entropies). The space usages of the
AF-index and the CSA are similar and usually the best, albeit the CSA predictably 
loses in counting time on smaller alphabets (dna, proteins), due
to its O(m log n) rather than O(m log σ) complexity. The CSA takes advantage 
of larger alphabets with good high-order entropies (sources, xml), a
combination where the AF-index, despite of its name, proﬁts less. Note
that the space performance of the CSA on those texts conﬁrms that its space
occupancy may be below the zero-order entropy.

With respect to time, the SSA is usually the fastest thanks to its simplicity.
Sometimes the AF-index gets close and it is actually faster on xml. The CSA
is rarely competitive for counting, and the LZ-index is well out of bounds
for this experiment. Notice that the plain suﬃx array (last column in Table
6) is 2–6 times faster than any compressed index, but its space occupancy
can be up to 18 times larger.

5.3 Locate

We locate suﬃcient random patterns of length 5 to obtain a total of 2–3
million occurrences per text (see Table 7). This way we are able to evaluate
the average cost of a single locate operation, by making the impact of the
counting cost negligible. Fig. 4 reports the time/space tradeoﬀs achieved by
the diﬀerent indexes for the locate operation.

We remark that the implemented indexes include the sampling mechanism 
for locate and extract as a single module, and therefore the space for
both operations is included in these plots. Therefore, the space could be reduced 
if we only wished to locate. However, as extracting snippets of pattern

26

dna

english

)
e
c
n
e
r
r
u
c
c
o

 
r
e
p

 
s
c
e
s
o
r
c
m

i

(
 

e
m

i
t

)
e
c
n
e
r
r
u
c
c
o

 
r
e
p

 
s
c
e
s
o
r
c
m

i

(
 

e
m

i
t

)
e
c
n
e
r
r
u
c
c
o

 
r
e
p

 
s
c
e
s
o
r
c
m

i

(
 

e
m

i
t

 30

 25

 20

 15

 10

 5

 0

 30

 25

 20

 15

 10

 5

 0

 30

 25

 20

 15

 10

 5

 0

 0

 0.5

 1

 1.5

 2

 2.5

Space usage (fraction of text)

pitches

 0

 0.5

 1

 1.5

 2

 2.5

Space usage (fraction of text)

sources

 0

 0.5

 1

 1.5

 2

 2.5

)
e
c
n
e
r
r
u
c
c
o

 
r
e
p

 
s
c
e
s
o
r
c
m

i

(
 

e
m

i
t

)
e
c
n
e
r
r
u
c
c
o

 
r
e
p

 
s
c
e
s
o
r
c
m

i

(
 

e
m

i
t

)
e
c
n
e
r
r
u
c
c
o

 
r
e
p

 
s
c
e
s
o
r
c
m

i

(
 

e
m

i
t

 30

 25

 20

 15

 10

 5

 0

 30

 25

 20

 15

 10

 5

 0

 30

 25

 20

 15

 10

 5

 0

 0

 0.5

 1

 1.5

 2

 2.5

Space usage (fraction of text)

proteins

 0

 0.5

 1

 1.5

 2

 2.5

Space usage (fraction of text)

xml

 0

 0.5

 1

 1.5

 2

 2.5

Space usage (fraction of text)

Space usage (fraction of text)

∗

(cid:3)

AF-index
LZ-index

+ CSA
× SSA

Figure 4: Space-time tradeoﬀs for locating occurrences of patterns of length
5.

27

Table 7: Number of searched patterns of length 5 and total number of located
occurrences.

Text
dna
english
pitches
proteins
sources
xml

# patterns # occurrences
2, 491, 410
2, 969, 876
2, 117, 347
2, 259, 125
2, 130, 626
2, 831, 462

10
100
200
3, 500
50
20

Table 8: Locate time required by plain SA in microseconds per occurrence,
with m = 5. We recall that this implementation requires 5 bytes per indexed
symbol.

plain SA 0.005

0.006

0.007

dna english pitches proteins sources
0.007

0.005

xml
0.006

occurrences is an essential functionality of a self-index, we consider that the
space for eﬃcient extraction should always be included.5

The comparison shows that usually CSA can achieve the best results with
minimum space, except on dna where the SSA performs better as expected
(given its query time complexity, see before), and on proteins for which the
suﬃx-array-based indexes perform similarly (and the LZ-index does much
worse). The CSA is also the most attractive alternative if we ﬁx that the
space of the index should be equal to that of the text (recall that it includes
the text), being the exceptions dna and xml, where the LZ-index is superior.
The LZ-index can be much faster than the others if one is willing to pay
for some extra space. The exceptions are pitches, where the CSA is superior,
and proteins, where the LZ-index performs poorly. This may be caused by
the large number of patterns that were searched to collect the 2–3 million
occurrences (see Table 7), as the counting is expensive on the LZ-index.

Table 8 shows the locate time required by an implementation of the classical 
suﬃx array: it is between 100 and 1000 times faster than any compressed
index, but always 5 times larger than the indexed text. Unlike counting,
where compressed indexes are comparable in time with classical ones, locating 
is much slower on compressed indexes. This comes from the fact that
each locate operation (except on the LZ-index) requires to perform several
random memory accesses, depending on the sampling step. In contrast, all

5Of course, we could have a sparser sampling for extraction, but we did not want to

complicate the evaluation more than necessary.

28

the occurrences are contiguous in a classical suﬃx array. As a result, the compressed 
indexes are currently very eﬃcient in case of selective queries, but
traditional indexes become more eﬀective when locating many occurrences.
This fact has triggered recent research activity on this subject (e.g., [16])
but a deeper understanding on index performance on hierarchical memories
is still needed.

5.4 Extract

We extracted substrings of length 512 from random text positions, for a total
of 5 MB of extracted text. Fig. 5 reports the time/space tradeoﬀs achieved
by the tested indexes. We still include both space to locate and extract, but
we note that the sampling step aﬀects only the time to reach the text segment
to extract from the closest sample, and afterwards the time is independent of
the sampling. We chose length 512 to smooth out the eﬀect of this sampling.
The comparison shows that, for extraction purposes, the CSA is better
for sources and xml, whereas the SSA is better on dna and proteins. On
english and pitches both are rather similar, albeit the CSA is able to operate
on reduced space. On the other hand, the LZ-index is much faster than the
others on xml, english and sources, if one is willing to pay some additional
space.6

It is diﬃcult to compare these times with those of a classical index, because 
the latter has the text readily available. Nevertheless, we note that
the times are rather good: using the same space as the text (and some times
up to half the space) for all the functionalities implemented, the compressed
indexes are able to extract around 1 MB/sec, from arbitrary positions. This
shows that self-indexes are appealing as compressed-storage schemes with
the support of random accesses for snippet extraction.

6 Conclusion and Future Work

In this paper we have addressed the new fascinating technology of compressed
text indexing. We have explained the main principles used by those indexes
in practice, and presented the Pizza&Chili site, where implementations and
testbeds are readily available for use. Finally, we have presented experiments 
that demonstrate the practical relevance of this emerging technology.
Table 9 summarizes our experimental results by showing the most promising
compressed index(es) depending on the text type and task.

6Actually the LZ-index is not plotted for pitches and proteins because it needs more

than 1.5 times the text size.

29

dna

english

)
r
a
h
c
 
r
e
p

 
s
c
e
s
o
r
c
m

i

(
 

e
m

i
t

)
r
a
h
c
 
r
e
p

 
s
c
e
s
o
r
c
m

i

(
 

e
m

i
t

)
r
a
h
c
 
r
e
p

 
s
c
e
s
o
r
c
m

i

(
 

e
m

i
t

 5

 4

 3

 2

 1

 0

 5

 4

 3

 2

 1

 0

 5

 4

 3

 2

 1

 0

 0

 0.2

 0

 0.2

 0

 0.2

 0.6

 0.4
Space usage (fraction of text)

 0.8

 1

 1.2

pitches

 0.6

 0.4
Space usage (fraction of text)

 0.8

 1

 1.2

sources

 0.6

 0.4
Space usage (fraction of text)

 0.8

 1

 1.2

)
r
a
h
c
 
r
e
p

 
s
c
e
s
o
r
c
m

i

(
 

e
m

i
t

)
r
a
h
c
 
r
e
p

 
s
c
e
s
o
r
c
m

i

(
 

e
m

i
t

)
r
a
h
c
 
r
e
p

 
s
c
e
s
o
r
c
m

i

(
 

e
m

i
t

 5

 4

 3

 2

 1

 0

 5

 4

 3

 2

 1

 0

 5

 4

 3

 2

 1

 0

 0

 0.2

 0

 0.2

 0

 0.2

 1.4

 1.4

 1.4

 0.6

 0.4
Space usage (fraction of text)

 0.8

 1

 1.2

proteins

 0.6

 0.4
Space usage (fraction of text)

 0.8

 1

 1.2

xml

 0.6

 0.4
Space usage (fraction of text)

 0.8

 1

 1.2

 1.4

 1.4

 1.4

∗

(cid:3)

AF-index
LZ-index

+ CSA
× SSA

Figure 5: Space-time tradeoﬀs for extracting text symbols.

30

Table 9: The most promising indexes given the size and time they obtain for
each operation/text.
dna

pitches proteins

english

sources

xml

count

locate

extract

LZ-index

SSA

- AF-index
CSA
SSA LZ-index
CSA
SSA
- LZ-index

SSA AF-index
SSA
CSA
-
CSA
-

SSA

SSA

- AF-index
CSA

CSA AF-index
-
CSA
- LZ-index LZ-index
CSA
- LZ-index LZ-index

CSA

SSA

For counting the best indexes are SSA and AF-index. This stems from
the fact that they achieve very good zeroor 
high-order compression of the
indexed text, while their average counting complexity is O(mH0(T )). The
SSA has the advantage of a simpler search mechanism, but the AF-index is
superior for texts with small high-order entropy (i.e. xml, sources, english).
The CSA usually loses because of its O(m log n) counting complexity.

For locating and extracting, which are LF-computation intensive, the
AF-index is hardly better than the simpler SSA because the beneﬁt of a denser
sampling does not compensate for the presence of many wavelet trees. The
SSA wins for small-alphabet data, like dna and proteins. Conversely, for all
other high-order compressible texts the CSA takes over the other approaches.
We also notice that the LZ-index is a very competitive choice when extra
space is allowed and the texts are highly compressible.

The ultimate moral is that there is not a clear winner for all text collections.
 Nonetheless, our results provide an upper bound on what these
compressed indexes can achieve in practice:

Counting. We can compress the text within 30%–50% of its original size,
and search for 20,000–50,000 patterns of 20 chars each within a second.

Locate. We can compress the text within 40%–80% of its original size, and

locate about 100,000 pattern occurrences per second.

Extract. We can compress the text within 40%–80% of its original size, and

decompress its symbols at a rate of about 1 MB/second.

The above ﬁgures are from one (count) to three (locate) orders of magnitudes 
slower than what one can achieve with a plain suﬃx array, at the
beneﬁt of using up to 18 times less space. This slowdown is due to the fact
that search operations in compressed indexes access the memory in a nonlocal 
way thus eliciting many cache/IO misses, with a consequent degradation

31

of the overall time performance. Nonetheless compressed indexes achieve a
(search/extract) throughput which is signiﬁcant and may match the eﬃciency 
speciﬁcations of most software tools which run on a commodity PC.
We therefore hope that this paper will spread their use in any software that
needs to process, store and mine text collections of any size. Why using much
space when squeezing and searching is nowadays simultaneously aﬀordable?

References

[1] D. Arroyuelo and G. Navarro. Space-eﬃcient construction of LZ-index.
In Proceedings 16th Annual International Symposium on Algorithms and
Computation (ISAAC), LNCS 3827, pages 1143–1152, 2005.

[2] D. Arroyuelo, G. Navarro, and K. Sadakane. Reducing the space requirement 
of LZ-index. In Proceedings 17th Annual Symposium on Combinatorial 
Pattern Matching (CPM), LNCS 4009, pages 319–330, 2006.

[3] M. Burrows and D. Wheeler. A block sorting lossless data compression
algorithm. Technical Report 124, Digital Equipment Corporation, 1994.

[4] R. Cole, T. Kopelowitz, and M. Lewenstein. Suﬃx trays and suﬃx trists:
Structures for faster text indexing.
In Proceedings 33th International
Colloquium on Automata, Languages and Programming (ICALP), pages
358–369, 2006.

[5] P. Ferragina, R. Giancarlo, G. Manzini, and M. Sciortino. Boosting
textual compression in optimal linear time. Journal of the ACM, 52:688–
713, 2005.

[6] P. Ferragina, F. Luccio, G. Manzini, and S. Muthukrishnan. Structuring
labeled trees for optimal succinctness, and beyond. In Proceedings 46th
Annual IEEE Symposium on Foundations of Computer Science (FOCS),
pages 184–196, 2005.

[7] P. Ferragina, F. Luccio, G. Manzini, and S. Muthukrishnan. CompressIn 
Proceedings 15th World

ing and searching xml data via two zips.
Wide Web Conference (WWW), pages 751–760, 2006.

[8] P. Ferragina and G. Manzini. An experimental study of an opportunistic
index. In Proceedings 12th Annual ACM-SIAM Symposium on Discrete
Algorithms (SODA), pages 269–278, 2001.

32

[9] P. Ferragina and G. Manzini. Compression boosting in optimal linear
time using the burrows-wheeler transform. In Proceedings 15th Annual
ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 655–
663, 2004.

[10] P. Ferragina and G. Manzini. Indexing compressed texts. Journal of the

ACM, 52(4):552–581, 2005.

[11] P. Ferragina, G. Manzini, V. M¨akinen, and G. Navarro. Compressed
representations of sequences and full-text indexes. ACM Transactions
on Algorithms (TALG), 3(2):article 20, 2007.

[12] P. Ferragina and R. Venturini. Compressed permuterm index. In Proceedings 
30th Annual International ACM SIGIR Conference on Research
and Development in Information Retrieval, 2007.

[13] L. Foschini, R. Grossi, A. Gupta, and J. Vitter. When indexing equals
compression: Experiments with compressing suﬃx arrays and applications.
 ACM Transactions on Algorithms (TALG), 2(4):611–639, 2006.

[14] R. Giancarlo and M. Sciortino. Optimal partitions of strings: A new
class of burrows-wheeler compression algorithms.
In Proceedings 14th
Annual Symposium on Combinatorial Pattern Matching (CPM), pages
129–143, 2003.

[15] R. Gonz´alez, Sz. Grabowski, V. M¨akinen, and G. Navarro. Practical
implementation of rank and select queries. In Poster Proceedings Volume 
of 4th Workshop on Eﬃcient and Experimental Algorithms (WEA),
pages 27–38, 2005.

[16] R. Gonz´alez and G. Navarro. Compressed text indexes with fast locate.
 In Proceedings 18th Annual Symposium on Combinatorial Pattern
Matching (CPM), LNCS 4580, pages 216–227, 2007.

[17] R. Grossi, A. Gupta, and J. Vitter. High-order entropy-compressed
In Proceedings 14th Annual ACM-SIAM Symposium on

text indexes.
Discrete Algorithms (SODA), pages 841–850, 2003.

[18] R. Grossi and J. Vitter. Compressed suﬃx arrays and suﬃx trees with
applications to text indexing and string matching. SIAM Journal on
Computing, 35(2):378–407, 2006.

[19] D. Gusﬁeld. Algorithms on Strings, Trees and Sequences: Computer
Science and Computational Biology. Cambridge University Press, 1997.

33

[20] W. Hon, K. Sadakane, and W. Sung. Breaking a time-and-space barrier
in constructing full-text indices. In Proceedings 44th Annual IEEE Symposium 
on Foundations of Computer Science (FOCS), pages 251–260,
2003.

[21] T.-W. Lam, K. Sadakane, W.-K. Sung, and S.-M. Yiu. A space and time
eﬃcient algorithm for constructing compressed suﬃx arrays. In Proceedings 
8th Annual International Conference on Computing and Combinatorics 
(COCOON), pages 401–410, 2002.

[22] J. Larsson and K. Sadakane. Faster suﬃx sorting. Technical Report
LU-CS-TR:99-214, Department of Computer Science, Lund University,
Sweden, 1999.

[23] K. Lemstr¨om and S. Perttu. SEMEX – an eﬃcient music retrieval prototype.
 In Proceedings 1st International Symposium on Music Information
Retrieval (ISMIR), 2000.

[24] V. M¨akinen and G. Navarro. Succinct suﬃx arrays based on run-length

encoding. Nordic Journal of Computing, 12(1):40–66, 2005.

[25] V. M¨akinen and G. Navarro. Dynamic entropy-compressed sequences
and full-text indexes. In Proceedings 17th Annual Symposium on Combinatorial 
Pattern Matching (CPM), LNCS 4009, pages 307–318, 2006.
Extended version to appear in ACM TALG.

[26] U. Manber and G. Myers. Suﬃx arrays: A new method for on-line string

searches. SIAM Journal of Computing, 22:935–948, 1993.

[27] G. Manzini. An analysis of the Burrows-Wheeler transform. Journal of

the ACM, 48(3):407–430, 2001.

[28] G. Manzini and P. Ferragina. Engineering a lightweight suﬃx array

construction algorithm. Algorithmica, 40(1):33–50, 2004.

[29] I. Munro. Tables.

In Proceedings 16th Conference on Foundations
of Software Technology and Theoretical Computer Science (FSTTCS),
LNCS 1180, pages 37–42, 1996.

[30] I. Munro, R. Raman, V. Raman, and S. Rao. Succinct representations
of permutations. In Proceedings 30th International Colloquium on Automata,
 Languages and Programming (ICALP), pages 345–356, 2003.

34

[31] I. Munro and V. Raman. Succinct representation of balanced parentheses,
 static trees and planar graphs. In Proceedings 38th Annual IEEE
Symposium on Foundations of Computer Science (FOCS), pages 118–
126, 1997.

[32] G. Navarro. Indexing text using the Ziv-Lempel trie. Journal of Discrete

Algorithms (JDA), 2(1):87–114, 2004.

[33] G. Navarro. Compressing web graphs like texts. Technical Report DCC-

2007-2, Dept. of Computer Science, University of Chile, 2007.

[34] G. Navarro and V. M¨akinen. Compressed full-text indexes. ACM Computing 
Surveys, 39(1):article 2, 2007.

[35] S. Puglisi, W. Smyth, and A. Turpin. A taxonomy of suﬃx array construction 
algorithms. ACM Computing Surveys, 39(2):article 4, 2007.

[36] K. Sadakane. Succinct representations of lcp information and improveIn 
Proceedings 13th Annual
ments in the compressed suﬃx arrays.
ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 225–
232, 2002.

[37] K. Sadakane. New text indexing functionalities of the compressed suﬃx

arrays. Journal of Algorithms, 48(2):294–313, 2003.

[38] I. H. Witten, A. Moﬀat, and T. C. Bell. Managing Gigabytes: Compressing 
and Indexing Documents and Images. Morgan Kaufmann Publishers,
second edition, 1999.

[39] J. Ziv and A. Lempel. Compression of individual sequences via variablerate 
coding. IEEE Transactions on Information Theory, 24(5):530–536,
1978.

35

