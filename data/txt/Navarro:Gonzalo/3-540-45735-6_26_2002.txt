t-Spanners as a Data Structure

for Metric Space Searching(cid:2)

Gonzalo Navarro1, Rodrigo Paredes1, and Edgar Ch´avez2

1 Center for Web Research, Dept. of Computer Science, University of Chile

Blanco Encalada 2120, Santiago, Chile
{gnavarro,raparede}@dcc.uchile.cl

2 Escuela de Ciencias F´ısico-Matem´aticas, Univ. Michoacana

Morelia, Mich. M´exico

elchavez@zeus.ccu.umich.mx.

Abstract. A t-spanner, a subgraph that approximates graph distances
within a precision factor t, is a well known concept in graph theory. In this
paper we use it in a novel way, namely as a data structure for searching
metric spaces. The key idea is to consider the t-spanner as an approximation 
of the complete graph of distances among the objects, and use it
as a compact device to simulate the large matrix of distances required
by successful search algorithms like AESA [Vidal 1986]. The t-spanner
provides a time-space tradeoﬀ where full AESA is just one extreme.
We show that the resulting algorithm is competitive against current approaches,
 e.g., 1.5 times the time cost of AESA using only 3.21% of its
space requirement, in a metric space of strings; and 1.09 times the time
cost of AESA using only 3.83 % of its space requirement, in a metric
space of documents. We also show that t-spanners provide better spacetime 
tradeoﬀs than classical alternatives such as pivot-based indexes.
Furthermore, we show that the concept of t-spanners has potential for
large improvements.

1 Introduction

The concept of “approximate” searching has applications in a vast number of
ﬁelds. Some examples are non-traditional databases (where the concept of exact 
search is of no use and we search for similar objects, e.g. databases storing
images, ﬁngerprints or audio clips); machine learning and classiﬁcation (where
a new element must be classiﬁed according to its closest existing element); image 
quantization and compression (where only some vectors can be represented
and those that cannot must be coded as their closest representable point); text
retrieval (where we look for words in a text database allowing a small number
of errors, or we look for documents which are similar to a given query or docu-
ment); computational biology (where we want to ﬁnd a DNA or protein sequence

(cid:1) This work has been supported in part by the Millenium Nucleus Center for Web
Research, Grant P01-029-F, Mideplan, Chile (1st and 2nd authors), CYTED VII.19
RIBIDI Project (all authors), and AT&T LA Chile (2nd author).

A.H.F. Laender and A.L. Oliveira (Eds.): SPIRE 2002, LNCS 2476, pp. 298–309, 2002.
c(cid:1) Springer-Verlag Berlin Heidelberg 2002

t-Spanners as a Data Structure for Metric Space Searching

299

in a database allowing some errors due to typical variations); function prediction
(where we want to search the most similar behavior of a function in the past so
as to predict its probable future behavior); etc.
All those applications have some common characteristics. There is a universe
X of objects, and a nonnegative distance function d : X × X −→ R+ deﬁned
among them. This distance satisﬁes the three axioms that make the set a metric
space

d(x, y) = 0 ⇔ x = y
d(x, y) = d(y, x)
d(x, z) ≤ d(x, y) + d(y, z)

where the last one is called the “triangle inequality” and is valid for many reasonable 
similarity functions. The smaller the distance between two objects, the
more “similar” they are. This distance is considered expensive to compute (think,
for instance, in comparing two ﬁngerprints). We have a ﬁnite database U ⊆ X,
which is a subset of the universe of objects and can be preprocessed (to build an
index, for instance). Later, given a new object from the universe (a query q), we
must retrieve all similar elements found in the database. There are two typical
queries of this kind:

(a) Retrieve all elements which are within distance r to q.

(b) Retrieve the k closest elements to q in U.

This is, {x ∈ U / d(x, q) ≤ r}.
This is, A ⊆ U such that |A| = k and ∀x ∈ A, y ∈ U − A, d(x, q) ≤ d(y, q).
Given a database of |U| = n objects, all those queries can be trivially
answered by performing n distance evaluations. The goal is to structure the
database such that we perform less distance evaluations. Since the distance is
usually expensive to compute, we take the number of distance evaluations as the
measure of the search complexity. This is the approach we take in this paper.

A particular case of this problem arises when the space is R

k. There are eﬀective 
methods for this case, such as kd-trees, R-trees, X-trees, etc. [6]. However,
for roughly 20 dimensions or more those structures cease to work well. We focus
in this paper in general metric spaces, although the solutions are well suited
also for k-dimensional spaces. It is interesting to notice that the concept of “di-
mensionality” can be translated to metric spaces as well: the typical feature in
high dimensional spaces is that the probability distribution of distances among
elements has a very concentrated histogram (with larger mean as the dimension
grows), diﬃculting the work of any similarity search algorithm [4]. We say that
a general metric space is high dimensional when its histogram of distances is
concentrated.

There are a number of methods to preprocess the set in order to reduce the
number of distance evaluations. All them work by discarding elements with the
triangle inequality. See [4] for a recent survey.

By far, the most successful technique for searching metric spaces ever proposed 
is AESA [10]. Its main problem is that it requires precomputing and

