Approximate String Matching

with Lempel-Ziv Compressed Indexes

Lu´ıs M.S. Russo1,(cid:2), Gonzalo Navarro2,(cid:2)(cid:2), and Arlindo L. Oliveira1

1 INESC-ID, R. Alves Redol 9, 1000 Lisboa, Portugal

lsr@algos.inesc-id.pt, aml@inesc-id.pt

2 Dept. of Computer Science, University of Chile

gnavarro@dcc.uchile.cl

Abstract. A compressed full-text self-index for a text T is a data structure 
requiring reduced space and able of searching for patterns P in T .
Furthermore, the structure can reproduce any substring of T , thus it
actually replaces T . Despite the explosion of interest on self-indexes in
recent years, there has not been much progress on search functionalities 
beyond the basic exact search. In this paper we focus on indexed
approximate string matching (ASM), which is of great interest, say, in
computational biology applications. We present an ASM algorithm that
works on top of a Lempel-Ziv self-index. We consider the so-called hybrid
indexes, which are the best in practice for this problem. We show that a
Lempel-Ziv index can be seen as an extension of the classical q-samples
index. We give new insights on this type of index, which can be of independent 
interest, and then apply them to the Lempel-Ziv index. We
show experimentally that our algorithm has a competitive performance
and provides a useful space-time tradeoﬀ compared to classical indexes.

1 Introduction and Related Work

Approximate string matching (ASM) is an important problem that arises in
applications related to text searching, pattern recognition, signal processing, and
computational biology, to name a few. It consists in locating all the occurrences
of a given pattern string P [0, m − 1] in a larger text string T [0, u − 1], letting
the occurrences be at distance ed() at most k from P . In this paper we focus
on edit distance, that is, the minimum number of character insertions, deletions,
and substitutions of single characters to convert one string into the other.

The classical sequential search solution runs in O(um) worst-case time (see [1]).
An optimal average-case algorithm requires time O(u(k+logσ m)/m) [2,3], where
σ is the size of the alphabet Σ. Those good average-case algorithms are called
ﬁltration algorithms: they traverse the text fast while checking for a simple necessary 
condition, and only when this holds they verify the text area using a

(cid:2) Supported by the Portuguese Science and Technology Foundation by grant

SFRH/BD/12101/2003 and Project DBYEAST POSC/EIA/57398/2004.

(cid:2)(cid:2) Supported in part by Fondecyt Grant 1-050493 (Chile).

N. Ziviani and R. Baeza-Yates (Eds.): SPIRE 2007, LNCS 4726, pp. 264–275, 2007.
c(cid:2) Springer-Verlag Berlin Heidelberg 2007

Approximate String Matching with Lempel-Ziv Compressed Indexes

265

classical ASM algorithm. For long texts, however, sequential searching might be
impractical because it must scan all the text. To avoid this we use an index [4].
There exist indexes speciﬁcally devoted to ASM, e.g. [5,6,7,8], but these are
oriented to worst-case performance. There seems to exist an unbreakable spacetime 
barrier with indexed ASM: Either one obtains exponential times (on m
or k), or one obtains exponential index space (e.g. O(u logk u)). Another trend
is to reuse an index designed for exact searching, all of which are linear-space,
and try to do ASM over it. Indexes such as suﬃx trees [9], suﬃx arrays [10],
or based on so-called q-grams or q-samples, have been used. There exist several
algorithms, based on suﬃx trees or arrays, which focus on worst-case performance 
[11,12,13]. Given the mentioned time-space barrier, they achieve a search
time independent of u but exponential on m or k. Essentially, they simulate the
sequential search over all the possible text suﬃxes, taking advantage of the fact
that similar substrings are factored out in suﬃx trees or arrays.

Indexes based on q-grams (indexing all text substrings of length q) or qsamples 
(indexing non-overlapping text substrings of length q) are appealing
because they require less space than suﬃx trees or arrays. The algorithms on
those indexes do not oﬀer worst-case guarantees, but perform well on average
when the error level α = k/m is low enough, say O(1/ logσ u). Those indexes
basically simulate an on-line ﬁltration algorithm, such that the “necessary con-
dition” checked involves exact matching of pattern substrings, and as such can
be veriﬁed with any exact-searching index. Such ﬁltration indexes, e.g. [14,15],
cease to be useful for moderate k values, which are still of interest in applications.
The most successful approach, in practice, is in between the two techniques
described above, and is called “hybrid” indexing. The index determines the text
positions to verify using an approximate-matching condition instead of an exact
one. This requires a search of the ﬁrst kind (whose time is exponential on the
length of the string or the number of errors). Yet, these searches are done over
short strings and allowing few errors, so that the exponential cost is controlled.
Indexes of this kind oﬀer average-case guarantees of the form O(mnλ) for some
0 < λ < 1, and work well for higher error levels. They have been implemented
over q-gram indexes [16], suﬃx arrays [17], and q-sample indexes [18].

Yet, many of those linear-space indexes are very large anyway. For example,
suﬃx arrays require 4 times the text size and suﬃx trees require at the very least
10 times [19]. In recent years a new and extremely successful class of indexes
has emerged. Compressed full-text indexes use data compression techniques to
produce less space-demanding data structures [20,21,22,23,24]. It turns out that
data compression algorithms exploit the internal structure of a string much in the
same way indexes do, and therefore it is possible to build a compressed index that
takes space proportional to that of the compressed text, gives indexed searching,
and replaces the text as it can reproduce any text substring (in which case they
are called self-indexes). The size of those indexes is measured in terms of the
empirical text entropy, Hk [25], which gives a lower bound on the number of bits
per symbol achievable by a k-th order compressor. In this work we are interested
in indexes based on Lempel-Ziv compression [21,22,26,27,28].

266

L.M.S. Russo, G. Navarro, and A.L. Oliveira

Despite the great success of self-indexes, they have been mainly used for exact
log u) bits have
searching. Only very recently some indexes taking O(u) or O(u
appeared [29,30,7]. Yet, those are again of the worst-case type, and thus all
their times are exponential on k. In this paper we present a practical algorithm
that runs on a compressed self-index and belongs to the most successful class of
hybrid algorithms.

√

2 Our Contribution in Context

One can easily use any compressed self-index to implement a ﬁltration ASM
method that relies on looking for exact occurrences of pattern substrings, as this
is what all self-indexes provide. Indeed, this has been already attempted [31]
using the FM-index [21] and a Lempel-Ziv index [22]. The Lempel-Ziv index
worked better because it is faster to extract the text to verify (recall that in selfindexes 
the text is not directly available). The speciﬁc structure of the LempelZiv 
index used allowed several interesting optimizations (such as factoring out
the work of several text extractions) that we will not discuss further here.

Lempel-Ziv indexes split the text into a sequence of so-called phrases of varying 
length. They are eﬃcient to ﬁnd the (exact) occurrences that lie within
phrases, but those that span two or more phrases are costlier.

Our goal in this paper is to provide eﬃcient approximate searching over a
small and practical self-index. Based on the described previous experiences, (1)
we want an algorithm of the hybrid type, which implies that the self-index should
do approximate search for pattern pieces; (2) we want a Lempel-Ziv-based index,
so that the extraction of text to verify is fast; (3) we wish to avoid the problems
derived from pieces spanning several Lempel-Ziv phrases. We will focus on an
index [28] whose suﬃx-tree-like structure is useful for this approximate searching.
Mimicking q-sample indexes is particularly useful for our goals. Consider that
the text is partitioned into contiguous q-samples. Any occurrence O of P is of
length at least m − k. Wherever an occurrence lies, it must contain at least
j = (cid:3)(m − k − q + 1)/q(cid:4) complete q-samples. The following lemma, simpliﬁed
from [4], gives the connection to use approximate searching for pattern substrings
with a q-samples index [18].
Lemma 1. Let A and B be strings such that ed(A, B) ≤ k. Let A = A1A2 . . . Aj,
for strings Ai and for any j ≥ 1. Then there is a substring B
(cid:3) of B and an i
such that ed(B
Therefore, if we assume that B = P and A is contained in O, we index all
the diﬀerent text q-samples into, say, a trie data structure. Then the trie is
traversed to ﬁnd q-samples that match within P with at most (cid:3)k/j(cid:4) errors. All
the contexts around all occurrences of the matching q-samples are examined for
full occurrences of P . Note in passing that we could also take A = P and B
contained in O, in which case we choose how to partition P but we must be able
to ﬁnd any text substring with the index (exactly [15] or approximately [16,17],
depending on j). Thus we must use a suﬃx tree or array [17], or even a q-gram
index if we never use pieces of P longer than q [15,16].

, Ai) ≤ (cid:3)k/j(cid:4).

(cid:3)

Approximate String Matching with Lempel-Ziv Compressed Indexes

267

A Lempel-Ziv parsing can be regarded as an irregular sampling of the text, and
therefore our goal in principle is to adapt the techniques of [18] to an irregular
parsing (thus we must stick to the interpretation B = P ). As desired, we would
not need to consider occurrences spanning more than one phrase. Moreover, the
trie of phrases stored by all Lempel-Ziv self-indexes is the exact analogous of the
trie of q-samples, thus we could search without requiring further structures.

The irregular parsing poses several challenges. There is no way to ensure
that there will be a minimum number j of phrases contained in an occurrence.
Occurrences could even be fully contained in a phrase!

We develop several tools to face those challenges. (1) We give a new variant of
Lemma 1 that distributes the errors in a convenient way when the samples are
of varying length. (2) We introduce a new ﬁltration technique where the samples
that overlap the occurrence (not only those contained in the occurrence) can be
considered. This is of interest even for classical q-sample indexes. (3) We search
for q-samples within long phrases to detect occurrences even if they are within
a phrase. This technique also includes novel insights.

We implement our scheme and compare it with the best technique in practice
over classical indexes [17], and with the previous developments over compressed
self-indexes [31]. The experiments show that our technique provides a relevant
space-time tradeoﬀ for indexed ASM.

3 An Improved q-Samples Index

In this section we extend classical q-sample indexes by allowing samples to overlap 
the pattern occurrences. This is of interest by itself, and will be used for
an irregular sampling index later. Remind that a q-samples index stores the
locations, in T , of all the substrings T [qi..qi + q − 1].

3.1 Varying the Error Distribution

We will need to consider parts of samples in the sequel, as well as samples of
diﬀerent lengths. Lemma 1 gives the same number of errors to all the samples,
which is disadvantageous when pieces are of diﬀerent lengths. The next lemma
generalizes Lemma 1 to allow diﬀerent numbers of errors in each piece (all proofs
are in the Appendix for lack of space).

j

B

Lemma 2. Let A and B be strings, let A = A1A2 . . . Aj, for strings Ai and
(cid:2)
some j ≥ 1. Let ki ∈ R such that
i=1 ki > ed(A, B). Then there is a substring
(cid:3) of B and an i such that ed(Ai, B
(cid:3)) < ki.
Lemma 1 is a particular case of Lemma 2: set ki = k/j +  for suﬃciently
small  > 0. Our lemma reminds Lemma 2 of [4], and they can be proved to
be equivalent. The current formulation is more advantageous for us because
one does not need to know j. It can be used to adapt the error levels to the
length of the pieces. For example, to try to maintain a constant error level, take
ki = (1 + ) k · |Ai|/|A| with  > 0.

268

L.M.S. Russo, G. Navarro, and A.L. Oliveira

3.2 Partial q-Sample Matching
Contrary to all previous work, let us assume that A in Lemma 2 is not only that
part of an approximate occurrence O formed by full q-samples, but instead that
A = O, so that A1 is the suﬃx of a sample and Aj is the preﬁx of a sample.
An advantage of this is that now the number of involved q-samples is at least
j = (cid:8)(m − k)/q(cid:9), and therefore we can permit fewer errors per piece (e.g. (cid:3)k/j(cid:4)
using Lemma 1). On the other hand, we would like to allow fewer errors for
the pieces A1 and Aj. Yet, notice that any text q-sample can participate as A1,
Aj, or as a fully contained q-sample in diﬀerent occurrences at diﬀerent text
positions. Lemma 2 tells us that we could allow ki = (1 + ) k·|Ai|/|A| errors for
Ai, for any  > 0. Conservatively, this is ki = (1 + ) k · q/(m − k) for 1 < i < j,
and less for the extremes.
In order to adapt the trie searching technique to those partial q-samples, we
should not only search all the text q-samples with (1+) k·q/(m−k), but also all
their preﬁxes and suﬃxes with fewer errors. This includes, for example, verifying
all the q-samples whose ﬁrst or last character appears in P (cases |A1| = 1 and
|Aj| = 1). This is unaﬀordable. Our approach will be to redistribute the errors
across A using Lemma 2 in a diﬀerent way to ensure that only suﬃciently long
q-sample preﬁxes and suﬃxes are considered.

Let v be a non-negative integer parameter. We associate to every letter of A a
weight: the ﬁrst and last v letters have weight 0 and the remaining letters have
weight (1 + )/(|A|− 2v). We deﬁne |Ai|v as the sum of the weights of the letters
of Ai. For example if Ai is within the ﬁrst v letters of A then |Ai|v = 0; if it does
not contain any of the ﬁrst or last v letters then |Ai|v = (1 + ) |Ai|/(|A| − 2v).
We can now apply Lemma 2 with ki = k · |Ai|v provided that k > 0. Note
i=1 ki = (1 + ) k > k. In this case, if |A1| ≤ v we have that k1 = 0 and
that
therefore A1 can never be found with strictly less than zero errors. The same
holds for Aj. This eﬀectively relieves us from searching for any q-sample preﬁx
or suﬃx of length at most v.

(cid:2)

j

Parameter v is thus doing the job of discarding q-samples that have very
little overlap with the occurrence O = A, and maintaining the rest. It balances
between two exponential costs: one due to verifying all the occurrences of too
short preﬁxes/suﬃxes, and another due to permitting too many errors when
searching for the pieces in the trie. In practice tuning this parameter will have
a very signiﬁcant impact on performance.

3.3 A Hybrid q-Samples Index
We have explained all the ideas necessary to describe a hybrid q-samples index.
The algorithm works in two steps. First we determine all the q-samples Oi for
(cid:3)) < k · |Oi|v for some substring P
(cid:3) of P . In this phase we also
which ed(Oi, P
(cid:3)) < k·|O1|v
determine the q-samples that contain a suﬃx O1 for which ed(O1, P
(cid:3) of P (note that we do not need to consider substrings of P , just
for some preﬁx P
(cid:3)
j for
preﬁxes). Likewise we also determine the q-samples that contain a preﬁx O
(cid:3)) < k ·|Oj|v for some suﬃx P
(cid:3) of P (similar observation). The qwhich 
ed(Oj, P
samples that classify are potentially contained inside an approximate occurrence

Approximate String Matching with Lempel-Ziv Compressed Indexes

269
of P , i.e. Oi may be a substring of a string O such that ed(O, P ) ≤ k. In order
to verify whether this is the case, in the second phase we scan the text context
around Oi with a sequential algorithm.

As the reader might have noticed, the problem of verifying conditions such
(cid:3)) < k · |Oi|v is that we cannot know a priori which i does a given
as ed(Oi, P
text q-sample correspond to. Diﬀerent occurrences of the q-sample in the text
could participate in diﬀerent positions of an O, and even a single occurrence in
T could appear in several diﬀerent O’s. We do not know either the size |O|, as
it may range from m − k to m + k.
A simple solution is as follows. Conservatively assume |O| = m − k. Then,
search P for each diﬀerent text q-sample in three roles: (1) as a q-sample contained 
in O, so that |Oi| = q, assuming pessimistically |Oi|v = (1+) min(q/(m−
k − 2v), 1); (2) as an O1, matching a preﬁx of P for each of the q-sample suﬃxes
of lengths v < (cid:7) < q, assuming |O1| = (cid:7) and thus |O1|v = (1 + ) min(((cid:7) −
v)/(m− k− 2v), 1); (3) as an Oj, matching a suﬃx of P for each of the q-sample
preﬁxes, similarly to case (2) (that is, |Oj|v = |O1|v). We assume that q < m− k
and therefore the case of O contained inside a q-sample does not occur.

In practice, one does not search for each q-sample in isolation, but rather
factors out the work due to common q-gram preﬁxes by backtracking over the
trie and incrementally computing the dynamic programming matrix between
every diﬀerent q-sample and any substring of P (see [4]). We note that the trie
of q-samples is appropriate for role (3), but not particularly eﬃcient for roles
(1) and (2) (ﬁnding q-samples with some speciﬁc suﬃx). In our application to a
Lempel-Ziv index this will not be a problem because we will have also a trie of
the reversed phrases (that will replace the q-grams).

4 Using a Lempel-Ziv Self-index

We now adapt our technique to the irregular parsing of phrases produced by a
Lempel-Ziv-based index. Among the several alternatives [21,22,26,27,28], we will
focus on the ILZI [28], yet the results can be carried over to similar indexes.

The ILZI partitions the text into phrases such that every suﬃx of a phrase is
also a phrase (similarly to LZ78 compressors [32], where every preﬁx of a phrase
is also a phrase). It uses two tries, one storing the phrases and another storing
the reverse phrases. In addition, it stores a mapping that permits moving from
one trie to the other, and it stores the compressed text as a sequence of phrase
identiﬁers. This index [28] has been shown to require O(uHk) bits of space, and
to be eﬃcient in practice. We do not need more details for this paper.

4.1 Handling Diﬀerent Lengths

As explained, the main idea is to use the phrases instead of q-samples. For this
sake Lemma 2 solves the problem of distributing the errors homogeneously across
phrases. However, other problems arise especially for long phrases. For example,
an occurrence could be completely inside a phrase. In general, backtracking over
long phrases is too costly.

270

L.M.S. Russo, G. Navarro, and A.L. Oliveira

We resort again to q-samples, this time within phrases. We choose two nonnegative 
integer parameters q and s < q. We will look for any q-gram of P that
appears with less than s errors within any phrase. All phrases spotted along this
process must be veriﬁed. Still, some phrases not containing any pattern q-gram
with < s errors can participate in an occurrence of P (e.g. if (cid:3)(m−k−q+1)/q(cid:4)·s ≤
k or if the phrase is shorter than q). Next we show that those remaining phrases
have certain structure that makes them easy to ﬁnd.
Lemma 3. Let A and B be strings and q and s be integers such that 0 ≤ s <
(cid:3)| = q we have that
q ≤ |A| and for any substrings B
(cid:3)) ≥ s. Then for every preﬁx A
(cid:3) of B such
(cid:3)
ed(A
, B
(cid:3)) ≤ ed(A, B) − s(cid:3)(|A| − |A
(cid:3)
that ed(A
, B
The lemma implies that, if a phrase is close to a substring of P , but none of
its q-grams are suﬃciently close to any substring of P , then the errors must be
distributed uniformly along the phrase. Therefore we can check the phrase progressively 
(for increasing preﬁxes), so that the number of errors permitted grows
slowly. This severely limits the necessary backtracking to ﬁnd those phrases that
escape from the q-gram-based search.

(cid:3) of B and A
(cid:3) of A there is a substring B
(cid:3)|)/q(cid:4).

(cid:3) of A with |A

Parameter s permits us balancing between two search costs. If we set it low,
then the q-gram-based search will be stricter and faster, but the search for the
escaping phrases will be costlier. If we set it high, most of the cost will be
absorbed by the q-gram search.

4.2 A Hybrid Lempel-Ziv Index

The following lemma describes the way we combine previous results to search
using a Lempel-Ziv index.
Lemma 4. Let A and B be strings such that 0 < ed(A, B) ≤ k. Let A =
A1A2 . . . Aj, for strings Ai and some j ≥ 1. Let q, s and v be integers such
that 0 ≤ s < q ≤ |A| and 0 ≤ v < |A|/2. Then there is a substring B
(cid:3) of B and
an i such that either:

1. there is a substring A
2. ed(Ai, B

(cid:3)| = q and ed(A
(cid:3)) < s, or
(cid:3)
, B
(cid:3)) < k · |Ai|v in which case for any preﬁx A
(cid:3) of Ai there exists a
(cid:3)(cid:3)) < k · |Ai|v − s(cid:3)(|Ai| − |A

(cid:3) of Ai with |A
(cid:3) such that ed(A
(cid:3)

(cid:3)|)/q(cid:4).

substring B

(cid:3)(cid:3) of B

, B

As before the search runs in two phases. In the ﬁrst phase we ﬁnd the phrases
whose text context must be veriﬁed. In the second phase we verify those text
contexts for an approximate occurrence of P . Lemma 4 gives the key to the ﬁrst
phase. We ﬁnd the relevant phrases via two searches:

(1) We look for any q-gram contained in a phrase which matches within
P with less than s errors. We backtrack in the trie of phrases for every P [y1..],
descending in the trie and advancing y2 in P [y1, y2] while computing the dynamic
programming matrix between the current trie node and P [y1, y2]. We look for
all trie nodes at depth q that match some P [y1, y2] with less than s errors. Since
every suﬃx of a phrase is a phrase in the ILZI, every q-gram within any phrase

Approximate String Matching with Lempel-Ziv Compressed Indexes

271

can be found starting from the root of the trie of phrases. All the phrases Z
that descend from each q-gram trie node found must be veriﬁed (those are the
phrases that start with that q-gram). We must also spot the phrases suﬃxed by
each such Z. Hence we map each phrase Z to the trie of reverse phrases and also
verify all the descent of the reverse trie nodes. This covers case 1 of Lemma 4.
(2) We look for any phrase Ai matching a portion of P with less than k·|Ai|v
errors. This is done over the trie of phrases. Yet, as we go down in the trie (thus
considering longer phrases), we can enforce that the number of errors found
up to depth d must be less than k · |Ai|v − s(cid:3)(|Ai| − d)/q(cid:4). This covers case
2 in Lemma 4, where the equations vary according to the roles described in
Section 3.3 (that is, depending on i):
(2.1) 1 < i < j, in which case we are considering a phrase contained inside
O that is not a preﬁx nor a suﬃx. The k · |Ai|v formula (both for the matching
condition and the backtracking limit) can be bounded by (1+) k·min(|Ai|/(m−
k − 2v), 1), which depends on |Ai|. Since Ai may correspond to any trie node
that descends from the current one, we determine a priori which |Ai| ≤ m − k
maximizes the backtracking limit. We apply the backtracking for each P [y1..].
(2.2) i = j, in which case we are considering a phrase that starts by a suﬃx
of O. Now k · |Ai|v can be bounded by (1 + ) k · min((d − v)/(m − k − 2v), 1),
yet still the limit depends on |Ai| and must be maximized a priori. This time
we are only interested in suﬃxes of P , that is, we can perform m searches with
y2 = m and diﬀerent y1. If a node veriﬁes the condition we must consider also
those that descend from it, to get all the phrases that start with the same suﬃx.
(2.3) i = 1, in which case we are considering a phrase that ends in a preﬁx of
O. This search is as case i = j, with similar formulas. We are only interested in
preﬁxes of P , that is y1 = 0. As the phrases are suﬃx-closed, we can conduct a
single search for P [0..] from the trie root, ﬁnding all phrase suﬃxes that match
each preﬁx of P . Each such suﬃx node must be mapped to the reverse trie and
the descent there must be included. The case i = j = 1 is diﬀerent, as it includes
the case where O is contained inside a phrase. In this case we do not require
the matching trie nodes to be suﬃxes, but also preﬁxes of suﬃxes. That is, we
include the descent of the trie nodes and map each node in that descent to the
reverse trie, just as in case 1.

5 Practical Issues and Testing

We implemented a prototype to test our algorithm on the ILZI compressed index 
[28]. As a baseline we used eﬃcient sequential bit-parallel algorithms (namely
BPM, the bit-parallel dynamic programming matrix of Myers [33], and EXP, the
exact pattern partitioning by Navarro and Baeza-Yates [34]).

For the real prototype we used a stricter backtracking than as explained in
previous sections. For each pattern substring P [y1, y2] to be matched, we computed 
the maximum number of errors that could occur when matching it in the
text, also depending on the position O[x1, x2] where it would be matched, and
maximizing over the possible areas of O where the search would be necessary. For

272

L.M.S. Russo, G. Navarro, and A.L. Oliveira

Table 1. Memory peaks, in Megabytes, for the diﬀerent approaches when k = 6

ILZI Hybrid LZI DLZI FMIndex
English
55
45
DNA
Proteins 105

145 178
125 158
217 228

257
252
366

131
127
165

example, the extremes of P can be matched with fewer errors than the middle.
This process involves precomputing tables that depend on m and k. We omit
the details for lack of space.

We also included in the comparison an implementation of a ﬁltration index using 
the simple approach of Lemma 1 with A = P and B = O, as brieﬂy described
in the beginning of Section 2 [31]. The indexes used in that implementation are
the LZ-index [22] (LZI) and Navarro’s implementation of the FM-index [21]. We
also compare an improved variant over the LZ-index (DLZI [31]). Note that the
FM-Index does not divide the text into blocks, however it takes longer to locate
occurrences.

The machine was a Pentium 4, 3.2 GHz, 1 MB L2 cache, 1GB RAM, running 
Fedora Core 3, and compiling with gcc-3.4 -O9. We used the texts from
the Pizza&Chili corpus (http://pizzachili.dcc.uchile.cl), with 50 MB of
English and DNA and 64 MB of proteins. The pattern strings were sampled randomly 
from the text and each character was distorted with 10% of probability.
All the patterns had length m = 30. Every conﬁguration was tested during at
least 60 seconds using at least 5 repetitions. Hence the numbers of repetitions
varied between 5 and 130,000. To parametrize the hybrid index we tested all the
j values from 1 to k + 1 and reported the best time. To parametrize we choose
q = (cid:3)m/h(cid:4) and s = (cid:3)k/h(cid:4)+1 for some convenient h, since we can prove that this
is the best approach and it was corroborated by our experiments. To determine
the value of h and v we also tested the viable conﬁgurations and reported the
best results. In our examples choosing v and h such that 2v is slightly smaller
than q yielded the best conﬁguration.

The average query time, in seconds, is shown in Fig. 1 and the respective
memory heap peaks for indexed approaches are shown in Table 1. The hybrid
index provides the fastest approach to the problem, however it also requires
the most space. Aside from the hybrid index our approach is always either the
fastest or within reasonable distance from the fastest approach. For low error
level, k = 1 or k = 2, our approach is signiﬁcantly faster, up to an order of
magnitude better. This is very important since the compressed approaches seem
to saturate at a given performance for low error levels: in English k = 1 to 3, in
DNA k = 1 to 2, and in proteins k = 1 to 5. This is particularly troublesome
since indexed approaches are the best alternative only for low error levels. In
fact the sequential approaches outperform the compressed indexed approaches
for higher error levels. In DNA this occurs at k = 4 and in English at k = 5.

Our index performed particularly well on proteins, as did the hybrid index.
This could owe to the fact that proteins behave closer to random text, and this

Approximate String Matching with Lempel-Ziv Compressed Indexes

273

english

dna

proteins

1e+03    

1e+02    

1e+01    

1e+00    

1e-01    

1e-02    

1e-03    

1e-04    

ILZI
EXP

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 1

 2

 3

 4

 5

 6

 1

 2

 3

 4

 5

 6

 1

 2

 3

 4

 5

 6

k

BPM
Hybrid

k

LZI
DLZI

k

FMIndex

Fig. 1. Average user time for ﬁnding the occurrences of patterns of size 30 with k
errors. The y axis units are in seconds and common to the three plots.

means that the parametrization of ours and the hybrid index indeed balances
between exponential worst cases.

In terms of space the ILZI is also very competitive, as it occupies almost the
same space as the plain text, except for proteins that are not very compressible.
 We presented the space that the algorithms need to operate and not just
the index size, since the other approaches need intermediate data structures to
operate.

6 Conclusions and Future Work

In this paper we presented an adaptation of the hybrid index for Lempel-Ziv compressed 
indexes. We started by addressing the problem of approximate matching
with q-samples indexes, where we described a new approach to this problem. We
then adapted our algorithm to the irregular parsing produced by Lempel-Ziv
indexes. Our approach was ﬂexible enough to be used as a hybrid index instead
of an exact-searching-based ﬁltration index. We implemented our algorithm and
compared it with the simple ﬁltration approach built over diﬀerent compressed
indexes, with sequential algorithms, and with a good uncompressed index.

Our results show that our index provides a good space/time tradeoﬀ, using
a small amount of space (at best 0.9 times the text size, which is 5.6 times less
than a classical index) in exchange for searching from 6.2 to 33 times slower
than a classical index, for k = 1 to 3. This is better than the other compressed
approaches for low error levels. This is signiﬁcant since indexed approaches are
most valuable, if compared to sequential approaches, when the error level is low.
Therefore our work signiﬁcantly improves the usability of compressed indexes
for approximate matching.

274

L.M.S. Russo, G. Navarro, and A.L. Oliveira

A crucial part of our work was our approach for the preﬁxes/suﬃxes of O. This
approach is in fact not essential for q-samples indexes, however it can improve
previous approaches [18]. However for a Lempel-Ziv index it is essential.

Finally, our implementation can be further improved since we do no secondary
ﬁltering, that is, we do not apply any sequential ﬁlter over the text context
before fully verifying them. We also plan to further explore the idea of associating
weights to the letters of O. We will investigate the impact of as assigning smaller
weights to less frequent letters of O. This should decrease the number of positions
to verify and improve the overall performance.

Acknowledgments. We are thankful to Pedro Morales for lending us the LZI,
DLZI and FMIndex prototypes.

References

1. Navarro, G.: A guided tour to approximate string matching. ACM Comput.

Surv. 33(1), 31–88 (2001)

2. Chang, W.I., Marr, T.G.: Approximate string matching and local similarity. In:
Crochemore, M., Gusﬁeld, D. (eds.) CPM 1994. LNCS, vol. 807, pp. 259–273.
Springer, Heidelberg (1994)

3. Fredriksson, K., Navarro, G.: Average-optimal single and multiple approximate

string matching. ACM Journal of Experimental Algorithmics 9(1.4) (2004)

4. Navarro, G., Baeza-Yates, R., Sutinen, E., Tarhio, J.: Indexing methods for approximate 
string matching. IEEE Data Engineering Bulletin 24(4), 19–27 (2001)
5. Cole, R., Gottlieb, L.A., Lewenstein, M.: Dictionary matching and indexing with

errors and don’t cares. In: STOC, pp. 91–100 (2004)

6. Maaß, M., Nowak, J.: Text indexing with errors. In: CPM, pp. 21–32 (2005)
7. Chan, H.L., Lam, T.W., Sung, W.K., Tam, S.L., Wong, S.S.: A linear size index
for approximate pattern matching. In: Lewenstein, M., Valiente, G. (eds.) CPM
2006. LNCS, vol. 4009, pp. 49–59. Springer, Heidelberg (2006)

8. Coelho, L., Oliveira, A.: Dotted suﬃx trees: a structure for approximate text indexing.
 In: Crestani, F., Ferragina, P., Sanderson, M. (eds.) SPIRE 2006. LNCS,
vol. 4209, pp. 329–336. Springer, Heidelberg (2006)

9. Weiner, P.: Linear pattern matching algorithms. In: IEEE 14th Annual Symposium
on Switching and Automata Theory, pp. 1–11. IEEE Computer Society Press, Los
Alamitos (1973)

10. Manber, U., Myers, E.: Suﬃx arrays: a new method for on-line string searches.

SIAM Journal on Computing, 935–948 (1993)

11. Gonnet, G.: A tutorial introduction to Computational Biochemistry using Darwin.

Technical report, Informatik E.T.H., Zuerich, Switzerland (1992)

12. Ukkonen, E.: Approximate string matching over suﬃx trees. In: Apostolico, A.,
Crochemore, M., Galil, Z., Manber, U. (eds.) Combinatorial Pattern Matching.
LNCS, vol. 684, pp. 228–242. Springer, Heidelberg (1993)

13. Cobbs, A.: Fast approximate matching using suﬃx trees. In: Galil, Z., Ukkonen,
E. (eds.) Combinatorial Pattern Matching. LNCS, vol. 937, pp. 41–54. Springer,
Heidelberg (1995)

14. Sutinen, E., Tarhio, J.: Filtration with q-samples in approximate string matching.
In: Hirschberg, D.S., Meyers, G. (eds.) CPM 1996. LNCS, vol. 1075, pp. 50–63.
Springer, Heidelberg (1996)

Approximate String Matching with Lempel-Ziv Compressed Indexes

275

15. Navarro, G., Baeza-Yates, R.: A practical q-gram index for text retrieval allowing

errors. CLEI Electronic Journal 1(2) (1998)

16. Myers, E.W.: A sublinear algorithm for approximate keyword searching. Algorithmica 
12(4/5), 345–374 (1994)

17. Navarro, G., Baeza-Yates, R.: A hybrid indexing method for approximate string

matching. Journal of Discrete Algorithms 1(1), 205–239 (2000)

18. Navarro, G., Sutinen, E., Tarhio, J.: Indexing text with approximate q-grams. J.

Discrete Algorithms 3(2-4), 157–175 (2005)

19. Kurtz, S.: Reducing the space requirement of suﬃx trees. Pract. Exper. 29(13),

1149–1171 (1999)

20. Sadakane, K.: New text indexing functionalities of the compressed suﬃx arrays. J.

Algorithms 48(2), 294–313 (2003)

21. Ferragina, P., Manzini, G.: Indexing compressed text. Journal of the ACM 52(4),

22. Navarro, G.: Indexing text using the Ziv-Lempel trie. J. Discrete Algorithms 2(1),

552–581 (2005)

87–114 (2004)

23. Grossi, R., Vitter, J.S.: Compressed suﬃx arrays and suﬃx trees with applications

to text indexing and string matching. SIAM J. Comput. 35(2), 378–407 (2005)

24. Navarro, G., M¨akinen, V.: Compressed full-text indexes. ACM Computing Sur25.
 Manzini, G.: An analysis of the Burrows-Wheeler transform. Journal of the

veys 39(1) article 2 (2007)

ACM 48(3), 407–430 (2001)

26. K¨arkk¨ainen, J., Ukkonen, E.: Lempel-Ziv parsing and sublinear-size index structures 
for string matching. In: South American Workshop on String Processing, pp.
141–155. Carleton University Press (1996)

27. Arroyuelo, D., Navarro, G., Sadakane, K.: Reducing the space requirement of LZIndex.
 In: Lewenstein, M., Valiente, G. (eds.) CPM 2006. LNCS, vol. 4009, pp.
318–329. Springer, Heidelberg (2006)

28. Russo, L.M.S., Oliveira, A.L.: A compressed self-index using a Ziv-Lempel dictionary.
 In: Crestani, F., Ferragina, P., Sanderson, M. (eds.) SPIRE 2006. LNCS,
vol. 4209, pp. 163–180. Springer, Heidelberg (2006)

29. Huynh, T., Hon, W., Lam, T., Sung, W.: Approximate string matching using compressed 
suﬃx arrays. In: Sahinalp, S.C., Muthukrishnan, S.M., Dogrusoz, U. (eds.)
CPM 2004. LNCS, vol. 3109, pp. 434–444. Springer, Heidelberg (2004)

30. Lam, T., Sung, W., Wong, S.: Improved approximate string matching using compressed 
suﬃx data structures. In: Deng, X., Du, D.-Z. (eds.) ISAAC 2005. LNCS,
vol. 3827, pp. 339–348. Springer, Heidelberg (2005)

31. Morales, P.: Soluci´on de consultas complejas sobre un indice de texto comprimido
(solving complex queries over a compressed text index). Undergraduate thesis,
Dept. of Computer Science, University of Chile, G. Navarro, advisor (2005)

32. Ziv, J., Lempel, A.: Compression of individual sequences via variable length coding.

IEEE Transactions on Information Theory 24(5), 530–536 (1978)

33. Myers, G.: A fast bit-vector algorithm for approximate string matching based on

dynamic programming. Journal of the ACM 46(3), 395–415 (1999)

34. Navarro, G., Baeza-Yates, R.: Very fast and simple approximate string matching.

Information Processing Letters 72, 65–70 (1999)

