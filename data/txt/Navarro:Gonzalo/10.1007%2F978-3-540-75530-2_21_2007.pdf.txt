Implicit Compression Boosting with

Applications to Self-indexing

Veli M¨akinen1,(cid:2) and Gonzalo Navarro2,(cid:2)(cid:2)

1 Department of Computer Science, University of Helsinki, Finland

2 Department of Computer Science, University of Chile

vmakinen@cs.helsinki.fi

gnavarro@dcc.uchile.cl

Abstract. Compression boosting (Ferragina & Manzini, SODA 2004)
is a new technique to enhance zeroth order entropy compressors’ performance 
to k-th order entropy. It works by constructing the BurrowsWheeler 
transform of the input text, ﬁnding optimal partitioning of the
transform, and then compressing each piece using an arbitrary zeroth
order compressor. The optimal partitioning has the property that the
achieved compression is boosted to k-th order entropy, for any k. The
technique has an application to text indexing: Essentially, building a
wavelet tree (Grossi et al., SODA 2003) for each piece in the partitioning 
yields a k-th order compressed full-text self-index providing eﬃcient
substring searches on the indexed text (Ferragina et al., SPIRE 2004).
In this paper, we show that using explicit compression boosting with
wavelet trees is not necessary; our new analysis reveals that the size of the
wavelet tree built for the complete Burrows-Wheeler transformed text is,
in essence, the sum of those built for the pieces in the optimal partitioning.
 Hence, the technique provides a way to do compression boosting
implicitly, with a trivial linear time algorithm, but ﬁxed to a speciﬁc zeroth 
order compressor (Raman et al., SODA 2002). In addition to having
these consequences on compression and static full-text self-indexes, the
analysis shows that a recent dynamic zeroth order compressed self-index
(M¨akinen & Navarro, CPM 2006) occupies in fact space proportional to
k-th order entropy.

1 Introduction

The indexed string matching problem is that of, given a long text T [1, n] over
an alphabet Σ of size σ, building a data structure called full-text index on it,
to solve two types of queries: (a) Given a short pattern P [1, m] over Σ, count
the occurrences of P in T ; (b) locate those occ positions in T . There are several
classical full-text indexes requiring O(n log n) bits of space which can answer
counting queries in O(m log σ) time (like suﬃx trees [1]) or O(m + log n) time
(like suﬃx arrays [18]). Both locate each occurrence in constant time once the

(cid:2) Funded by the Academy of Finland under grant 108219.
(cid:2)(cid:2) Partially Funded by Fondecyt Grant 1-050493, Chile.

N. Ziviani and R. Baeza-Yates (Eds.): SPIRE 2007, LNCS 4726, pp. 229–241, 2007.
c(cid:2) Springer-Verlag Berlin Heidelberg 2007

230

V. M¨akinen and G. Navarro

counting is done. Similar complexities are obtained with modern compressed
data structures [6,11,9], requiring space nHk + o(n log σ) bits (for some small k),
where Hk ≤ log σ is the k-th order empirical entropy of T . These indexes are
often called compressed self-indexes refering to their space requirement and to
their ability to work without the text and even fully replace it, by delivering any
text substring without accessing T .

The main building blocks in compressed self-indexes are the Burrows-Wheeler
transform T bwt [3] and function rankc(T bwt, i) that counts how many times
symbol c appears in T bwt[1, i]. Function rankc can be eﬃciently provided by
building the wavelet tree [11] on T bwt; this reduces the problem to rank queries
on binary sequences, which are already studied by Jacobson [14] in his seminal
work on compressed data structures. Using a more recent binary rank solution
[23] inside wavelet trees, one almost automatically achieves a compressed selfindex 
taking nH0+o(n log σ) bits of space [11,9,16]. Let us call this index Succinct
Suﬃx Array (SSA) following [16].

What has remained unnoticed so far is that SSA actually takes only nHk +
o(n log σ) bits of space. This result makes some of the more complicated techniques 
to achieve the same result obsolete. However, our analysis builds on the
existence of the compression-boosted version of SSA [9], as we show that the
internal parts of the structures in the boosted version are compressed exactly
the same way in the basic SSA. This shows a remarkable property of wavelet
trees when used together with the encoding of Raman, Raman, and Rao [23].

In the following, we ﬁrst deﬁne the entropy concepts more formally, then
explain the encoding in [23], wavelet trees [11], Burrows-Wheeler transform [3],
and compression boosting [7] in order to give our new analysis in a self-contained
manner. We conclude with the application to space-eﬃcient construction of (dy-
namic) full-text self-indexes.

2 Deﬁnitions
We assume our text T = t1 . . . tn to be drawn from an alphabet {0, 1, . . . σ − 1}.
Let nc denote the number of occurrences of symbol c in T , i.e., nc = |{i | ti = c}|.
Then the zero-order empirical entropy is deﬁned as H0(T ) =
.
n log n
nc
nc
This is the lower bound for the average code word length of any compressor that
ﬁxes the code words to the symbols independently of the context they appear
in.

0≤c<σ

(cid:2)

w∈Σk

A tighter lower bound for texts is the k-th order empirical entropy Hk(T ),
where the compressor can ﬁx the code word based on the k-symbol context
following the symbol to be coded.1 Formally, it can be deﬁned as Hk(T ) =
(cid:2)
n H0(T|w), where nw denotes the number of occurrences of substring
1 It is more logical (and hence customary) to deﬁne the context as the k symbols
preceding a symbol, but we use the reverse deﬁnition for technical convenience. If
this is an issue, the texts can be handled reversed to obtain results on the more
standard deﬁnition. It is anyway known that both deﬁnitions do not diﬀer by much
[8].

nw

Implicit Compression Boosting with Applications to Self-indexing

231
w in T and T|w denotes the concatenation of the symbols appearing immediately
before those nw occurrences [19]. Substring w = T [i + 1, i + k] is called the kcontext 
of symbol ti. We take T here as a cyclic string, such that tn precedes t1,
and thus the amount of k-contexts is exactly n.

3 Previous Results

3.1 Entropy-Bound Structures for Bit Vectors

Raman et al. [23] proposed a data structure to solve rank and select (inverse of
rank) queries in constant time over a static bit vector A = a1 . . . an with binary
zero-order entropy H0. The structure requires nH0 + o(n) bits.

The idea is to split A into superblocks S1 . . . Sn/s of s = log2 n bits. Each
superblock Si is in turn divided into 2 log n blocks Bi(j), of b = (log n)/2 bits
each, thus 1 ≤ j ≤ s/b. Each such block Bi(j) is said to belong to class c if it
(cid:4)
(cid:3)
has exactly c bits set, for 0 ≤ c ≤ b. For each class c, a universal table Gc of
b
c
entries is precomputed. Each entry corresponds to a possible block belonging to
class c, and it stores all the local rank answers for that block. Overall all the Gc
tables add up 2b =

√
√
n entries, and O(

n polylog(n)) bits.

The scheme extends to sequences over small alphabets as well [9]. Let B =
a1 . . . ab be the symbols in a block, and call na the number of occurences of

(cid:3)

b
c2

(cid:4)

b(n/b)

(cid:3)

(cid:4)

b
c1

+ log

(cid:4)

(cid:3)
b
c

(cid:3)
c1+c2

2b

Each block Bi(j) of the sequence is represented by a pair Di(j) = (c, o), where
c is its class and o is the index of its corresponding entry in table Gc. A block
bits. The ﬁrst term is O(log log n),
of class c thus requires log(c + 1) + log
whereas all the second terms add up nH0 + O(n/ log n) bits. To see this, note
(cid:4)
. The pairs
that log
Di(j) are of variable length and are all concatenated into a single sequence.

, and that nH0 ≥ log

(cid:3)
c1+...+cn/b

(cid:4) ≤ log

Each superblock Si stores a pointer Pi to its ﬁrst block description in the
sequence (that is, the ﬁrst bit of Di(1)) and the rank value at the beginning
of the superblock, Ri = rank(A, (i − 1)s). P and R add up O(n/ log n) bits. In
addition, Si contains s/b numbers Li(j), giving the initial position of each of its
blocks in the sequence, relative to the beginning of the superblock. That is, Li(j)
is the position of Di(j) minus Pi. Similarly, Si stores s/b numbers Qi(j) giving
the rank value at the beginning of each of its blocks, relative to the beginning
of the superblock. That is, Qi(j) = rank(A, (i − 1)s + (j − 1)b) − Ri. As those
relative values are O(log n), sequences L and Q require O(n log log n/ log n) bits.
To solve rank(A, p), we compute the corresponding superblock i = 1 + (cid:5)p/s(cid:6)
and block j = 1 + (cid:5)(p − (i − 1)s)/b(cid:6). Then we add the rank value of the corresponding 
superblock, Ri, the relative rank value of the corresponding block,
Qi(j), and complete the computation by fetching the description (c, o) of the
block where p belongs (from bit position Pi + Li(j)) and performing a (precom-
puted) local rank query in the universal table, rank(Gc(o), p−(i−1)s−(j−1)b).
The overall space requirement is nH0 + O(n log log n/ log n) bits, and rank is
solved in constant time. We do not cover select because it is not necessary to
follow this paper.

V. M¨akinen and G. Navarro

232
symbol a ∈ [1, q] in B. We call (n1, . . . , nq) the class of B. Thus, in our (c, o)
pairs, c will be a number identifying the class of B and o an index within the
class. A simple upper bound to the number of classes is (b + 1)q (as a class is a
tuple of q numbers in [0, b], although they have to add up b). Thus O(q log log n)
bits suﬃce for c (a second bound on the number of classes is qb as there cannot
be more classes than diﬀerent sequences). Just as in the binary case, the sum of
the sizes of all o ﬁelds adds up nH0(A) + O(n/ logq n) [9].

3.2 Wavelet Trees and Entropy-Bound Structures for Sequences

We now extend the result of the previous section to larger alphabets. The idea
is to build a wavelet tree [11] over sequences represented using rank structures
for small alphabets.

A binary wavelet tree is a balanced binary tree whose leaves represent the
symbols in the alphabet. The root is associated with the whole sequence A =
a1 ··· an, its left child with the subsequence of A obtained by concatenating all
positions i having ai < σ/2, and its right child with the complementary subsequence 
(symbols ai ≥ σ/2). This subdivision is continued recursively, until each
leaf contains a repeat of one symbol. The sequence at each node is represented
by a bit vector that tells which positions (those marked with 0) go to the left
child, and which (marked with 1) go to the right child. It is easy to see that the
bit vectors alone are enough to determine the original sequence: To recover ai,
start at the root and go left or right depending on the bit vector value Bi at
the root. When going to the left child, replace i ← rank0(B, i), and similarly
i ← rank1(B, i) when going right. When arriving at the leaf of character c it
must hold that the original ai is c. This requires O(log σ) rank operations over
bit vectors.

It also turns out that operations rank and select on the original sequence can
be carried out via O(log σ) operations of the same type on the bit vectors of
the wavelet tree [11]. For example, to solve rankc(A, i), start at the root and go
to the left child if c < σ/2 and to the right child otherwise. When going down,
update i as in the previous paragraph. When arriving at the leaf of c, the current
i value is the answer.

A multiary wavelet tree, of arity q, is used in [9]. In this case the sequence
of each wavelet tree node ranges over alphabet [1, q], and symbol rank/select
queries are needed over those sequences. One needs logq σ operations on those
sequences to perform the corresponding operation on the original sequence.

Either for binary or general wavelet trees, it can be shown that the H0 entropies 
in the representations of the sequences at each level add up to nH0(A)
bits [11,9]. However, as we have O(σ) bit vectors, the sublinear terms sum up
to o(σn). The space occupancy of the sublinear structures can be improved to
o(n log σ) by concatenating all the bit vectors of the same level into a single
sequence, and handling only O(log σ) such sequences2. It is straightforward to
do rank, as well as obtaining symbol ai, without any extra information [9].

2 Note that o(n log σ) is sublinear in the size of A measured in bits.

Implicit Compression Boosting with Applications to Self-indexing

233

If we now represent the concatenated bit vectors of the binary wavelet tree
by the rank structures explained in the previous section, we obtain a structure
requiring nH0(A) + O(n log log n/ logσ n) = nH0(A) + o(n log σ) bits, solving
rank in O(log σ) time. Within the same bounds one can solve select as well
[23,11].

Theorem 1 ([11]). Let L be a string and Bv the corresponding binary sequence
for each node v of the wavelet tree of L. Then

|Bv|H0(Bv) = |L|H0(L).

(cid:2)

v

One can also use multiary wavelet trees and represent the sequences with alphabet 
size q using the techniques for small alphabets (see the end of previous
section). With a suitable value for q, one obtains a structure requiring the same
nH0(A) + o(n log σ) bits of space, but answering rank and select in constant
time when σ = O(polylog(n)), and O((cid:9)log σ/ log log n(cid:10)) time in general [9].

3.3 Full-Text Self-indexes

Many full-text self-indexes are based on representing the Burrows-Wheeler transform 
[3] of a text using wavelet trees to support eﬃcient substring searches. We
follow the description given in [16].

The Burrows-Wheeler Transform. The Burrows-Wheeler transform (BWT)
[3] of a text T produces a permutation of T , denoted by T bwt. We assume that T
is terminated by an endmarker “$” ∈ Σ, smaller than other symbols. String T bwt
is the result of the following transformation: (1) Form a conceptual matrix M
whose rows are the cyclic shifts of the string T , call F its ﬁrst column and L its
last column; (2) sort the rows of M in lexicographic order; (3) the transformed
text is T bwt = L.

The BWT is reversible, that is, given T bwt we can obtain T . Note the following

properties [3]:
a. Given the i-th row of M, its last character L[i] precedes its ﬁrst character
F [i] in the original text T , that is, T = . . . L[i]F [i] . . ..
b. Let L[i] = c and let ri be the number of occurrences of c in L[1, i]. Let M[j]
be the ri-th row of M starting with c. Then the character corresponding to
L[i] in the ﬁrst column F is located at F [j] (this is called the LF mapping:
LF (i) = j). This is because the occurrences of character c are sorted both
in F and L using the same criterion: by the text following the occurrences.

The BWT can then be reversed as follows:

1. Compute the array C[1, σ] storing in C[c] the number of occurrences of
characters {$, 1, . . . , c− 1} in the text T . Notice that C[c] + 1 is the position
of the ﬁrst occurrence of c in F (if any).

2. Deﬁne the LF mapping as follows: LF (i) = C[L[i]] + rankL[i](L, i).
3. Reconstruct T backwards as follows: set s = 1 (since M[1] = $t1t2 . . . tn−1)
and, for each i ∈ n − 1, . . . , 1 do T [i] ← L[s] and s ← LF [s]. Finally put the
endmarker T [n] = $.

234

V. M¨akinen and G. Navarro

The BWT transform by itself does not compress T , it just permutes its characters.
 However, this permutation is more compressible than the original T .
Actually, it is not hard to compress L to O(nHk + σk+1 log n) bits, for any k ≥ 0
[19]. The idea is as follows (we will reuse it in our new analysis later): Partition 
L into minimum number of pieces L1L2 ··· L(cid:5) such that the symbols inside
each piece Lp = L[ip, jp] have the same k-context. Note that the k-context of
a symbol L[i] is M[i][1, k]. By the deﬁnition of k-th order entropy, it follows
that |L1|H0(L1) + |L2|H0(L2) + ··· + |L(cid:5)|H0(L(cid:5)) = nHk. That is, if one is able
to compress each piece up to its zero-order entropy, then the end result is k-th
order entropy.

(cid:2)

1≤i≤(cid:5)

|Lj|H0(Lj) = nHk(T ).

Theorem 2 ([19]). Let L = L1L2 . . . L(cid:5) be a partition of L, the BWT of T ,
according to contexts of length k in M. Then
Using, say, arithmetic coding on each piece, one achieves nHk + σk+1 log n bits
encoding of T for a ﬁxed k. The latter term comes from the encoding of the
symbol frequencies in each piece separately. This fact is the base of compression
boosting [7]; they give a linear time algorithm to ﬁnd, for a given zero order compressor,
 the optimal partitioning of L such that when each piece is compressed
using the given zero order compressor, the compression result is the best over
all possible partitions. Notice that the partitions ﬁxed by the k-contexts are a
subset of all partitions, and hence the resulting compression can be bounded by
k-th order entropy for any k.
Suﬃx Arrays. The suﬃx array A[1, n] of text T is an array of pointers to all
the suﬃxes of T in lexicographic order. Since T is terminated by the endmarker
“$”, all lexicographic comparisons are well deﬁned. The i-th entry of A points to
text suﬃx T [A[i], n] = tA[i]tA[i]+1 . . . tn, and it holds T [A[i], n] < T [A[i + 1], n]
in lexicographic order.
Given the suﬃx array and T , the occurrences of the pattern P = p1p2 . . . pm
can be counted in O(m log n) time. The occurrences form an interval A[sp, ep]
such that suﬃxes tA[i]tA[i]+1 . . . tn, for all sp ≤ i ≤ ep, contain the pattern P
as a preﬁx. This interval can be searched for using two binary searches in time
O(m log n). Once the interval is obtained, a locating query is solved simply by
listing all its pointers in constant time each.
We note that the suﬃx array A is essentially the matrix M of the BWT
(Sect. 3.3), as sorting the cyclic shifts of T is the same as sorting its suﬃxes
given the endmarker “$”: A[i] = j if and only if the i-th row of M contains the
string tjtj+1 . . . tn−1$t1 . . . tj−1.

Backward Search. The FM-index [6] is a self-index based on the BurrowsWheeler 
transform. It solves counting queries by ﬁnding the interval of A that
contains the occurrences of pattern P . The FM-index uses the array C and function 
rankc(L, i) of the LF mapping to perform backward search of the pattern.
Fig. 1 shows the counting algorithm. Using the properties of the BWT, it is easy
to see that the algorithm maintains the following invariant [6]: At the i-th phase,
variables sp and ep point, respectively, to the ﬁrst and last row of M preﬁxed

Implicit Compression Boosting with Applications to Self-indexing

235

i ← m;

Algorithm. FMCount(P [1, m],L[1, n])
(1)
(2) sp ← 1; ep ← n;
(3) while (sp ≤ ep) and (i ≥ 1) do
(4)
(5)
(6)
(7)
(8)

c ← P [i];
sp ← C[c] + rankc(L, sp − 1)+1;
ep ← C[c] + rankc(L, ep);
i ← i − 1;

if (ep < sp) then return 0 else return ep − sp + 1;

Fig. 1. FM-index algorithm for counting the number of occurrences of P [1, m] in T [1, n]

by P [i, m]. The correctness of the algorithm follows from this observation. Note
that P is processed backwards, from pm to p1.

Note that array C can be explicitly stored in little space, and to implement
rankc(L, i) in little space we can directly use the wavelet tree as explained in
Sect. 3.2. The space usage is nH0 + o(n log σ) bits and the m steps of backward
search take overall O(m log σ) time [16].

4 Implicit Compression Boosting
In Sect. 3.3 we showed that if L is partitioned into (cid:4) pieces L1L2 ··· L(cid:5) according
to the k-contexts, then it is enough to achieve zero-order entropy within each
partition to obtain k-th order total entropy. We now prove that the simple solution 
of Sect. 3.3 supporting backward search requires only nHk bits of space.
We start with an important lemma.
Lemma 1. Let L = L1L2 ··· L(cid:5) be any partition of L, the BWT of T . The
number of bits used by a partition Lj in the wavelet tree of L is upper bounded
by |Lj|H0(Lj) + O(|Lj| log σ log log n/ log n + σ log n).
Proof. The bits corresponding to Lj form a substring of the bit vectors at each
node of the wavelet tree, as their positions are mapped to the left and right
child using rank0 or rank1, thus order is preserved. Let us consider a particular
node of the wavelet tree and call B its bit sequence. Let us also call Bj the
substring of B corresponding to partition Lj, and assume Bj has lj bits set.
Consider the blocks of b bits that compose B, according to the partitioning of
[23] (Section 3.1). Let Bj
t be the concatenation of those bit
blocks that are fully contained in Bj, so that Bj
blk is a substring of Bj of length
b · t. Assume Bj
≤ lj bits set. The
space the o ﬁelds of the (c, o) representations of blocks Bi
j take in the compressed
Bj

i bits set, so that Bj

1 + . . . + lj

blk = Bj

1Bj

2 . . . Bj

blk has lj

(cid:8)(cid:9)

(cid:7)

b
lj
i

≤ log

(cid:7)

b · t

(cid:8)

lj
1 + . . . + lj

t

+ t ≤ log

(cid:8)

(cid:7)|Bj|
lj

+ t ≤ |Bj|H0(Bj) + t

i has lj

t

blk is
(cid:6)
log

t(cid:5)

i=1

236

V. M¨akinen and G. Navarro

where all the inequalities hold by simple combinatorial arguments [21] and have
been reviewed in Section 3.1.

Note that those Bj bit vectors are precisely those that would result if we
built the wavelet tree just for Lj. According to Theorem 1, adding up those
|Bj|H0(Bj) over all the O(σ) wavelet tree nodes gives |Lj|H0(Lj). To this we
must add three space overheads. The ﬁrst is the extra t bits above, which add
up O(|Lj| log σ/ log n) over the whole wavelet tree because b · t ≤ |Bj| and the
|Bj| lengths add up |Lj| at each wavelet tree level. The second overhead is the
space of the blocks that overlap with Bj and thus were not counted: As Bj is
a substring of B, there can be at most 2 such blocks per wavelet tree node.
At worst they can take O(log n) bits each, adding up O(σ log n) bits over the
whole wavelet tree. The third overhead is that of the c ﬁelds, which add up
O(|Lj| log σ log log n/ log n).
The above lemma lets us split the wavelet tree “horizontally” into pieces. Let us
add up all the zero-order entropies for the pieces. If we partition L according to
contexts of length k in M, and add up all the space due to all partitions in the
|Lj|H0(Lj) = nHk(T ) (Theorem 2). To this we must
wavelet tree, we get
add (i) O(|Lj| log σ/ log n), which sums up to O(n log σ/ log n) = o(n log σ) bits
over all the partitions; (ii) O(σ log n) bits per partition, which gives O((cid:4)σ log n);
and (iii) O(|Lj| log σ log log n/ log n), which sums up to O(n log σ log log n/ log n)
= o(n log σ). In the partitioning we have chosen we have (cid:4) ≤ σk, thus the upper
bound nHk + o(n log σ) + O(σk+1 log n) holds for the total number of bits spent
in the wavelet tree. The next theorem immediately follows.

1≤j≤(cid:5)

(cid:2)

Theorem 3. The space required by the wavelet tree of L, the BWT of T , if the
bitmaps are compressed using [23], is nHk(T ) + o(n log σ) + O(σk+1 log n) bits
for any k ≥ 0. This is nHk(T ) + o(n log σ) bits for any k ≤ α logσ n− 1 and any
constant 0 < α < 1. Here n is the length of T and σ its alphabet size.

Note that this holds automatically and simultaneously for any k, and we do not
even have to care about k in the index. Fig. 2 illustrates. Our result obviously
applies as well to the BWT alone, without wavelet tree on top, if we use a
suitable local zero-order encoder [9].

5 Discussion

We have shown that the space produced by any splitting of L into pieces is
achieved in the simple arrangement having just one wavelet tree. In [7] they
introduce an algorithm to ﬁnd the optimal partitioning. We have just used their
analysis to show that it is not necessary to apply such a partitioning algorithm to
achieve the boosted result. Their technique, on the other hand, has more general
applications unrelated to wavelet trees.

Several full-text self-indexes in the literature build on the wavelet tree of the
BWT of the text [16,9], and engage in diﬀerent additional arrangements to reach
k-th order compression. In [16], they ﬁrst run-length compress the BWT in order

Implicit Compression Boosting with Applications to Self-indexing

237

bwt(T)

Fig. 2. Illustration of the argument used in the theorem. On top of bwt(T ) = T bwt, the
wavelet trees induced by the optimal partitioning. One of those is highlighted so that
we show the bitmaps stored in the wavelet tree. Below bwt(T ), a single wavelet tree
built for the whole sequence. The bitmaps of this large wavelet tree are also shown,
and they contain the bitmaps of the small highlighted wavelet tree on top.

to reduce its length to O(nHk) and then apply the BWT. In [9] they explicitly
cut the BWT into pieces Lj so that the sum of nH0 sizes of the pieces adds
up nHk. In both cases, the simpler version they build on (just the wavelet tree
of the BWT) would have been suﬃcient. Thus, we have achieved a signiﬁcant
simpliﬁcation in the design of full-text indexes.

Also the paper where the wavelet tree is originally proposed [11] as an internal
tool to design one of the most space-eﬃcient compressed full-text indexes, would
beneﬁt from our simpliﬁcation. They cut L into a table of lists (columns) and
contexts (rows). All the entries across a row correspond to a contiguous piece
of L, that is, some context Lj. A wavelet tree is built over each table row so
as to ensure, again, that the sum of zero-order entropies over the rows adds up
to global k-th order entropy. Our ﬁnding implies that all rows could have been
concatenated into a single wavelet tree and the same space would have been
achieved. This would greatly simplify the original arrangement. Interestingly, in
[12] they ﬁnd out that, if they use gap encoding over the successive values along
a column, and they then concatenate all the columns, the total space is O(nHk)
without any table partitioning as well. Both results stem from the same fact:
the cell entropies can be added in any order to get nHk.

Finally, it is interesting to point out that, in a recent paper [5], the possibility
of achieving k-th order compression when applying wavelet trees over the BWT is
explored (among many other results), yet they resort to run-length compression
to achieve this. Once more, our ﬁnding is that this is not really necessary to
achieve k-th order compression if the levels of the wavelet tree are represented
using the technique of block identiﬁer encoding [22].

6 Application to Space-Eﬃcient Construction of

(Dynamic) Self-indexes

Another consequence of our result is that we obtain an O(n log n log σ) time
construction algorithm for a compressed self-index requiring nHk + o(n log σ)

238

V. M¨akinen and G. Navarro

bits working space during construction: This is obtained by enhancing our recent
result on dynamic compressed self-indexes:
Theorem 4 ([17]). There is a data structure maintaining a collection
{T1, T2, . . . Tm} of texts in nH0(C) + o(n log σ) bits supporting counting of occurrences 
of a pattern P in O(|P| log n log σ) time, and inserting and deleting a
text T in O(|T| log n log σ) time. After counting, any occurrence can be located
in time O(log2 n log log n). Any substring of length (cid:4) from any T in the collection
can be displayed in time O(log n((cid:4) log σ + log n log log n)). Here n is the length
of the concatenation C = 0 T10 T2 ··· 0 Tm, and we assume σ = o(n).
The dynamic index above uses wavelet tree with the static encoding [23] (see
Sect. 3.1) replaced with a dynamic version of the same encoding: The dynamic
bit vector representation [17] achieves the same nH0 + o(n) space as the static,
but supports rank and select, and in addition insertions and deletions of bits,
in O(log n) time. This can then be used to improve the dynamic index of Chan
et al. [4] to obtain the above result.

Exactly the same analysis as in Sect. 4 applies to this dynamic variant, and
Theorem 4 is boosted into the following.
Corollary 1. There is a data structure maintaining a collection {T1, T2, . . . Tm}
of texts in nHk(C)+o(n log σ) bits, for any k ≤ α logσ n−1 and constant 0 < α <
1, supporting the same operations of Theorem 4 within the same complexities.
Now, just inserting text T into an empty collection, yields the promised spaceeﬃcient 
construction algorithm for compressed self-index. This index can be
easily converted into a more eﬃcient static self-index, where a static wavelet
tree requires the same space and reduces the O(log n log σ) time complexities to
O((cid:9)log σ/ log log n(cid:10)) [9].

Therefore, we have obtained the ﬁrst compressed self-index with space essentially 
equal to the k-th order empirical entropy of the text collection, which in
addition can be built within this working space. Alternative dynamic indexes or
constructions of self-indexes [6,13,2] achieve at best O(nHk) bits of space (with
constants larger than 4), and in many cases worse time complexities.

Note also that, from the dynamic index just built, it is very easy to obtain
the BWT of T . It is a matter of ﬁnding the characters of L one by one. This
takes O(n log n log σ) time, just as the construction, and gives an algorithm to
build the BWT of a text within entropy bounds. The best result in terms of
space complexity takes O(n) bits working space, O(n log2 n) time in the worst
case, and O(n log n) time in the expected case [15]. Using O(n log σ) working
space, there is a faster algorithm achieving O(n log log σ) time requirement [13].
Finally, one can achieve the optimal O(n) time with the price of O(n log n log σ)
bits of space, for some 0 <  < 1 [20].

7 Final Practical Considerations

Our main ﬁnding is that all the sophistications [16,9,11] built over the simple
“wavelet tree on top of the BWT” scheme in order to boost its zero-order to

Implicit Compression Boosting with Applications to Self-indexing

239

high-order compression are unnecessary; the basic arrangement already achieves
high-order entropy if combined with a local zero-order encoder [23].

Still, the sophisticated techniques have practical value. In the actual implementation 
of such methods (Pizza&Chili site, http://pizzachili.dcc.uchile.cl),
zero-order entropy is achieved by using uncompressed bit streams over a Huﬀmantree 
shaped wavelet tree, instead of compressed bit streams over a balanced
wavelet tree. In this case the locality property does not hold, and high-order
entropy would not be achieved if just the simple wavelet tree of the BWT was
used.

Huﬀman-shaped trees were chosen to reach zero-order compression because
of the considerable diﬃculty in implementing Raman et al.’s scheme [23]. As
both alternatives were believed to yield similar compression ratios, the Huﬀmanshaped 
option seemed to be far more attractive from a practical point of view.
The situation is rather diﬀerent now that we know that Raman et al.’s scheme
yields high-order by itself, thus avoiding the need of any further complication 
to achieve high-order compression such as run-length compression (RunLength 
FM-index, RLFM [16]) or compression boosting plus managing multiple
wavelet trees (Alphabet-Friendly FM-index, AFFM [9]). Those complications
not only make the implementation eﬀort comparable to that of using Raman
et al.’s scheme, but also involve a considerable space overhead for extra structures.


A prototype combining Raman et al.’s compression with balanced wavelet
trees has already been implemented by Francisco Claude, a student of the second 
author. Unlike the existing implementations, this one oﬀers a space-time
tradeoﬀ, related to the partial sums sampling rate. A preliminary comparison
with the implementations of the SSA (bare Huﬀman-shaped wavelet tree over
the BWT), RLFM, and AFFM, shows that our technique is 2–3 times slower
for counting when using the same amount of space, which conﬁrms the original
predictions about implementation overheads. In exchange, it can still operate
with reasonable eﬃciency using less than 75% of the space needed by the alternatives.
 This makes it a relevant choice when space reduction is the main
concern.

It is interesting that our technique can achieve such a low space even when
it has to pay for space overheads like the c components in the (c, o) pairs. This
opens the door to the study of other alternatives that, even when they do not
surpass the “nHk + o(n log σ) for k ≤ α logσ n” theoretical barrier, do behave
better in practice. We point out that this barrier is not as good as it may seem
when one puts numbers to the condition on k and realizes that the achievable k
values are rather low. Worse than that, it is unlikely that this theoretical limit
can be sensibly improved [10]. Yet, those limits are worst-case, and diﬀerent
methods may not have to pay the Θ(σk+1 log n) space overhead in practice. For
example, in our case, this overhead comes from the fact that we are unable to
analyze better the compression of blocks that are split among contexts, and thus
we assume the worst about them. On the other hand, the c components are real
space overhead in our scheme (2n bits!), and that perhaps could be improved.

240

V. M¨akinen and G. Navarro

References

1. Apostolico, A.: The myriad virtues of subword trees. In: Combinatorial Algorithms

on Words, NATO ISI Series, pp. 85–96. Springer, Heidelberg (1985)

2. Arroyuelo, D., Navarro, G.: Space-eﬃcient construction of LZ-index. In: Deng, X.,
Du, D.-Z. (eds.) ISAAC 2005. LNCS, vol. 3827, pp. 1143–1152. Springer, Heidelberg
(2005)

3. Burrows, M., Wheeler, D.: A block sorting lossless data compression algorithm.

Technical Report Technical Report 124, Digital Equipment Corporation (1994)

4. Chan, H.-L., Hon, W.-K., Lam, T.-W.: Compressed index for a dynamic collection
of texts. In: Sahinalp, S.C., Muthukrishnan, S.M., Dogrusoz, U. (eds.) CPM 2004.
LNCS, vol. 3109, pp. 445–456. Springer, Heidelberg (2004)

5. Ferragina, P., Giancarlo, R., Manzini, G.: The myriad virtues of wavelet trees.
In: Bugliesi, M., Preneel, B., Sassone, V., Wegener, I. (eds.) ICALP 2006. LNCS,
vol. 4051, pp. 560–571. Springer, Heidelberg (2006)

6. Ferragina, P., Manzini, G.: Opportunistic data structures with applications. In:

Proc. FOCS 2000, pp. 390–398 (2000)

7. Ferragina, P., Manzini, G.: Compression boosting in optimal linear time using the

Burrows-Wheeler transform. In: Proc. SODA 2004, pp. 655–663 (2004)

8. Ferragina, P., Manzini, G.: Indexing compressed texts. J. of the ACM 52(4), 552–

581 (2005)

9. Ferragina, P., Manzini, G., M¨akinen, V., Navarro, G.: Compressed representations

of sequences and full-text indexes. ACM TALG, 3(2):article 20 (2007)

10. Gagie, T.: Large alphabets and incompressibility. IPL 99(6), 246–251 (2006)
11. Grossi, R., Gupta, A., Vitter, J.: High-order entropy-compressed text indexes. In:

Proc. SODA 2003, pp. 841–850 (2003)

12. Grossi, R., Gupta, A., Vitter, J.: When indexing equals compression: Experiments
with compressing suﬃx arrays and applications. In: Proc. SODA 2004, pp. 636–645
(2004)

13. Hon, W.-K., Sadakane, K., Sung, W.-K.: Breaking a time-and-space barrier in

constructing full-text indexes. In: Proc. FOCS 2003, pp. 251–260 (2003)

14. Jacobson, G.: Space-eﬃcient static trees and graphs. In: Proc. FOCS 1989, pp.

549–554 (1989)

15. K¨arkk¨ainen, J.: Fast BWT in small space by blockwise suﬃx sorting. In: Proc.

DIMACS Working Group on the Burrows-Wheeler Transform (2004)

16. M¨akinen, V., Navarro, G.: Succinct suﬃx arrays based on run-length encoding.

Nordic J. of Computing 12(1), 40–66 (2005)

17. M¨akinen, V., Navarro, G.: Dynamic entropy-compressed sequences and full-text
indexes. In: Lewenstein, M., Valiente, G. (eds.) CPM 2006. LNCS, vol. 4009, pp.
307–318. Springer, Heidelberg (2006)

18. Manber, U., Myers, G.: Suﬃx arrays: a new method for on-line string searches.

SIAM J. on Computing, 935–948 (1993)

19. Manzini, G.: An analysis of the Burrows-Wheeler transform. J. of the ACM 48(3),

407–430 (2001)

20. Na, J.C.: Linear-time construction of compressed suﬃx arrays using o(n log n)-bit
working space for large alphabets. In: Apostolico, A., Crochemore, M., Park, K.
(eds.) CPM 2005. LNCS, vol. 3537, pp. 57–67. Springer, Heidelberg (2005)

21. Pagh, R.: Low redundancy in dictionaries with O(1) worst case lookup time. In:
Wiedermann, J., van Emde Boas, P., Nielsen, M. (eds.) ICALP 1999. LNCS,
vol. 1644, pp. 595–604. Springer, Heidelberg (1999)

Implicit Compression Boosting with Applications to Self-indexing

241

22. Raman, R., Raman, V., Srinivasa, S.: Succinct dynamic data structures. In: Dehne,
F., Sack, J.-R., Tamassia, R. (eds.) WADS 2001. LNCS, vol. 2125, pp. 426–437.
Springer, Heidelberg (2001)

23. Raman, R., Raman, V., Srinivasa Roa, S.: Succinct indexable dictionaries with
applications to encoding k-ary trees and multisets. In: Proc. SODA 2002, pp. 233–
242 (2002)

