4
1
0
2

 
t
c
O
3

 

 
 
]
S
D
.
s
c
[
 
 

3
v
7
7
6
2

.

4
0
4
1
:
v
i
X
r
a

Optimal Encodings for Range Majority Queries ✩

Gonzalo Navarroa,1, Sharma V. Thankachanb

aDepartment of Computer Science, University of Chile,Chile.

bGeorgia Institute of Technology, USA.

Abstract

We study the problem of designing a data structure that reports the positions
of the distinct τ -majorities within any range of an array A[1, n], without storing
A. A τ -majority in a range A[i, j], for 0 < τ < 1, is an element that occurs more
than τ (j − i + 1) times in A[i, j]. We show that Ω(n log(1/τ )) bits are necessary
for any data structure able just to count the number of distinct τ -majorities in
any range. Then, we design a structure using O(n log(1/τ )) bits that returns
one position of each τ -majority of A[i, j] in O((1/τ ) log logw(1/τ ) log n) time, on
a RAM machine with word size w (it can output any further position where each
τ -majority occurs in O(1) additional time). Finally, we show how to remove a
log n factor from the time by adding O(n log log n) bits of space to the structure.

1. Introduction

Given an array A[1, n] of n arbitrary elements, an array range query problem
asks us to build a data structure over A, such that whenever a range [i, j]
with 1 ≤ i ≤ j ≤ n arrives as an input, we can eﬃciently answer queries
on the elements in A[i, j] [27]. Many array range queries arise naturally as
subproblems of combinatorial problems, and are also of direct interest in data
mining applications. Well-known examples are range minimum queries (RMQs,
which seek the smallest element in A[i, j]) [2], top-k queries (which report the
k largest elements in A[i, j]) [4], range selection queries (which report the kth
largest element in A[i, j]) [7], and colored top-k queries (which report the k
largest distinct elements in A[i, j]) [17].

An encoding for array range queries is a data structure that answers the
queries without accessing A. This is useful when the values of A are not of
interest themselves, and thus A may be deleted, potentially saving a lot of
space. It is also useful when array A does not ﬁt in main memory, so it can
be kept in secondary storage while a much smaller encoding can be maintained

✩ An early version of this article appeared in Proc. CPM 2014 [20].

Email addresses: gnavarro@dcc.uchile.cl (Gonzalo Navarro),

sharma.thankachan@gmail.com (Sharma V. Thankachan)

1Partially funded by Millennium Nucleus Information and Coordination in Networks

ICM/FIC P10-024F, Chile.

Preprint submitted to Algorithmica

May 7, 2018

in main memory, speeding up queries. In this setting, instead of reporting an
element in A, we only report a position in A containing the element. Otherwise,
in many cases we would be able to reconstruct A via queries on the encodings,
and thus these could not be small (e.g., A[i] would be the only answer to the
range query A[i, i] for all the example queries given above). As examples of
encodings, RMQs can be solved in constant time using just 2n + o(n) bits [12]
and, using O(n log k) bits, top-k queries can be solved in O(k) time [15] and
range selection queries in O(log k/ log log n) time [19].

Frequency-based array range queries, in particular variants of heavy-hitterlike 
problems, are very popular in data mining. Queries such as ﬁnding the
most frequent element in a range (known as the range mode query) are known
to be harder than problems like RMQs. For range mode queries, known data
structures with constant query time require nearly quadratic space [22]. The

best known linear-space solution requires O(pn/ log n) query time [5], and conditional 
lower bounds given in that paper show that a signiﬁcant improvement
is highly unlikely.

Still, eﬃcient solutions exist for some useful variants of the range mode problem.
 An example are approximate range mode queries, where we are required
to output an element whose number of occurrences in A[i, j] is at least 1/(1 + ǫ)
times the number of occurrences of the mode in A[i, j] [14, 3].

In this paper we focus on a popular variant of range mode queries called
range τ -majority queries, which ask to report any element that occurs more
than τ (j − i + 1) times in A[i, j]. A version of the problem useful for encodings
can be stated as follows (other variants are possible).

Deﬁnition 1. Given an array A[1, n], a range τ -majority query receives a
range [i, j] and returns one position in the range where each τ -majority in A[i, j]
occurs. A τ -majority is any element that occurs more than τ (j − i + 1) times
in A[i, j]. When τ = 1/2 we simply call it a majority.

Range majority queries can be answered in constant time by maintaining
a linear space (i.e., O(n)-word or O(n log n)-bit) data structure [9]. Similarly,
range τ -majority queries can be solved in time O(1/τ ) and linear space if τ is
ﬁxed at construction time, or O(n log log n) space (i.e., O(n log n log log n) bits)
if τ is given at query time [1].

In this paper, we focus for the ﬁrst time on encodings for range τ -majority
queries. In this scenario, a valid question is how much space is necessary for
an encoding that correctly answers such queries (we recall that A itself is not
available at query time). We answer that question in Section 3, proving a lower
bound for any encoding that solves even a weaker query.

Theorem 1. Given a real number 0 < τ < 1, any encoding able to count the
number of range τ -majorities in any range A[i, j] must use Ω(n log(1/τ )) bits.

Since when using O(n log n) bits we have suﬃcient space to store A[1, n]2

2Or an equivalent array where each element is replaced by an identiﬁer in [1, n].

2

Condition

Space (bits)

Query time

1/τ = ω(polylog n)

1/τ = Θ(polylog n)

O(n log(1/τ )) * O((1/τ ) log logw(1/τ ))
O(n log(1/τ )) *

O(1/τ ) ∗

1/τ = o(polylog n)

O(n log(1/τ )) *

O((1/τ ) log n)

1/τ = o(polylog n)

O(n log log n)

O(1/τ ) ∗

Table 1: Space-time tradeoﬀs achieved. We mark the optimal spaces and times with a *.

(and achieve the optimal O(1/τ ) time [1]), encodings for range τ -majorities are
asymptotically interesting only for log(1/τ ) = o(log n).

In Section 4 we show how range τ -majority queries can be solved using
O((n/τ ) log log n) bits of space and O((1/τ ) log n) query time. In Section 5 we
reduce the space to the optimal O(n log(1/τ )) bits and slightly increase the
time. After spending this time, the structure can report any of the positions
of any majority in optimal time (e.g., the leftmost position of each τ -majority
in a negligible O(1/τ ) time). In Section 6 we show how to build our structure
in O(n log n) time. All the results hold on the RAM model with word size
w = Ω(log n) bits.

Theorem 2. Given a real number 0 < τ < 1, there exists an encoding using
the optimal O(n log(1/τ )) bits that answers range τ ′-majority queries, for any
τ ≤ τ ′ < 1, in time O((1/τ ) log logw(1/τ ) log n), where w = Ω(log n) is the
RAM word size in bits. It can report any occ further occurrence positions of the
majorities in O(occ) time. The encoding can be built in O(n log n) time.

We note that the query time is simply O((1/τ ) log n) for polylogarithmic
values of 1/τ . We also note that the time depends on τ , not τ ′. In Section 6
we also show how to obtain a query time that is a function of τ ′, yet using
O(n log2(1/τ )) bits of space.

Finally, in Section 7 we derive a new variant that may use more space but

slashes the log n term from the time complexity.

Theorem 3. Given a real number 0 < τ < 1, there exists an encoding using
O(n log(1/τ ) + n log log n) bits that answers range τ ′-majority queries, for any
τ ≤ τ ′ < 1, in time O((1/τ ) log logw(1/τ )), where w = Ω(log n) is the RAM
word size in bits.
It can report any occ further occurrence positions of the
majorities in O(occ) time. The encoding can be built in O(n log n) time.

By combining the results of Theorems 2 and 3, we obtain the combinations

given in Table 1.

2. Related Work

In this section we ﬁrst cover the state of the art for answering range τ -
majority queries. Then, we survey a few results on bitmap representation, and

3

give a new result that will be useful for this paper. Again, all these results hold
on the RAM model with word size w = Ω(log n) bits.

2.1. Range Majorities

Range τ -majority queries were introduced by Karpinski and Nekrich [16],
who presented an O(n/τ )-words structure with O((1/τ )(log log n)2) query time.
Durocher et al. [9] improved their word-space and query time to O(n log(1/τ ))
and O(1/τ ), respectively. Gagie et al. [13] presented another trade-oﬀ, where
the space is O(n(H + 1)) bits and the query time is O((1/τ ) log log n). Here
H ≤ lg n denotes the empirical entropy of the distribution of elements in A (we
use lg to denote the logarithm in base 2). The best current result in general is
by Belazzougui et al. [1], where the space is O(n) words and the query time is
O(1/τ ). All these results assume that τ is ﬁxed at construction time.

For the case where τ is also a part of the query input, data structures of
space (in words) O(n(H + 1)) and O(n log n) were proposed by Gagie et al. [13]
and Chan et al. [6], respectively. Very recently, Belazzougui et al. [1] brought
down the space occupancy to O(n log log σ) words, where σ is the number of
distinct elements in A. The query time is O(1/τ ) in all cases. Belazzougui
et al. [1] also presented a compressed solution using nH + o(n log σ) bits, with
slightly higher query time. All these solutions include a (sometimes compressed)
representation of A, thus they are not encodings. As far as we know, ours is the
ﬁrst encoding for this problem.

For further reading, we recommend the recent survey by Skala [27].

2.2. Bitmap Representations

Given a bitmap B[1, m] with n 1s, the operation rank(B, i) returns the
number of 1s in B[1, i], whereas operation select(B, j) gives the position of the
jth 1 in B. Both operations can be solved in constant time by storing o(m)
bits in addition to B [18, 8]. When n is signiﬁcantly smaller than m, another
useful representation [25] compresses B to n lg m
n + O(n) + o(m) bits and retains
constant time for both operations.

When n is much smaller than m, even the o(m) extra bits of that compressed 
representation [25] are troublesome, and an Elias-Fano-based [11, 10]
compressed representation [21] is useful. It requires n lg m
n + O(n) bits, solves
select in O(1) time and rank in O(log m
n ) time. The representation considers the
positions of all the 1s in B, xi = select(B, i), and encodes the lowest b = ⌈lg m
n ⌉
bits of each xi in an array L[1, n], L[i] = xi mod 2b. Then it deﬁnes a bitmap
H[1, 2n] that encodes the highest bits of the xi values: all the bits at positions
i + (xi div 2b) are set in H. Bitmap H is indexed for constant-time rank and
select queries [18, 8]. The space for L[1, n] is n⌈lg m
n ⌉ and H uses 2n + o(n) bits.
Now, select(B, j) = 2b(select(H, j) − j) + L[i] can be computed in constant
time. For rank(B, i), we observe that the hth 0 in H represents the point
where the position B[2bh] is reached in the process of setting the 1s at positions
i + (xi div 2b), that is, xi−1 < 2bh ≤ xi. The number of 1s in H up to that
position is rank(B, 2bh). Therefore, if we write i = 2bh + l, then rank(B, i) is

4

between j1 = rank(H, select0(H, h)) + 1 and j2 = rank(H, select0(H, h + 1)),
where select0(H, h) gives the position of the hth 0 in H and is also computed
in constant time and o(n) bits [18, 8]. Now we binary search for l in L[j1, j2],
which is increasing in that range. The range is of length at most 2b, so the
search takes O(b) = O(log m
n ) time. The ﬁnal position j returned by the search
is rank(B, i).

The time can be improved to O(log logw

m
n + log s) on a RAM machine of
w bits by sampling, for each increasing interval of L of length more than s,
one value out of s. Predecessor data structures are built on the samples of
each interval, taking at most O((n/s) log m
n ) bits. Then we ﬁrst run a predecesm

n ) [23], and ﬁnish with an
sor query on L[j1, j2], which takes time O(log logw
O(log s)-time binary search between the resulting samples.

n +O((n/s) log m
Lemma 1. A bitmap B[1, m] with n 1s can be stored in n log m
n) bits, so that select queries take O(1) time and rank queries take O(log logw
log s), for any s, on a RAM machine of w bits.

n +
m
n +

3. Lower Bounds

We derive a lower bound on the minimum size range τ -majority encodings
may have, even if we just ask them to count the number of distinct τ -majorities
present in any range. The idea is to show that we can encode a certain combinatorial 
object in the array A, so that the object can be recovered via range
τ -majority queries. Therefore, in the worst case, the number of bits needed
to solve such queries must be at least the logarithm of the number of distinct
combinatorial objects that can be encoded.

Consider a sequence of m permutations on [3k]. There are (3k)!m such
sequences, thus any encoding for them must use at least m lg((3k)!) bits in the
worst case. Now consider the following encoding. Array A will have length
n = 36 · k · m. To encode the ith permutation, πi = (x1 x2 . . . x3k), we will
write 9 chunks on A[36k(i − 1) + 1, 36ki]:

1, 2, 3, . . . , k, −1, −2, −3, . . . , −2k, x1, x2, x3, . . . , xk
k+1, k+2, k+3, . . . , 2k, −1, −2, −3, . . . , −2k, x1, x2, x3, . . . , xk
2k+1, 2k+2, 2k+3, . . . , 3k, −1, −2, −3, . . . , −2k, x1, x2, x3, . . . , xk

1, 2, 3, . . . , k, −1, −2, −3, . . . , −2k, xk+1, xk+2, xk+3, . . . , x2k
k+1, k+2, k+3, . . . , 2k, −1, −2, −3, . . . , −2k, xk+1, xk+2, xk+3, . . . , x2k
2k+1, 2k+2, 2k+3, . . . , 3k, −1, −2, −3, . . . , −2k, xk+1, xk+2, xk+3, . . . , x2k

1, 2, 3, . . . , k, −1, −2, −3, . . . , −2k, x2k+1, x2k+2, x2k+3, . . . , x3k
k+1, k+2, k+3, . . . , 2k, −1, −2, −3, . . . , −2k, x2k+1, x2k+2, x2k+3, . . . , x3k
2k+1, 2k+2, 2k+3, . . . , 3k, −1, −2, −3, . . . , −2k, x2k+1, x2k+2, x2k+3, . . . , x3k

5

We will set τ = 1/(2k + 2) and perform τ -majority queries on parts of A to

recover any permutation.

Let us start obtaining πi(1) = x1. Let C[1, 36] = A[36k(i − 1) + 1, 36ki].

Consider an interval of the form

C[ℓ, 3k + g] = ℓ, ℓ + 1, . . . , k, −1, −2, . . . , −2k, x1, x2, . . . , xg,

for 1 ≤ ℓ, g ≤ k. Note that x1, . . . , xg are the only values that may appear
twice in C[ℓ, 3k + g], precisely, if they belong to {ℓ, . . . , k}. Note that elements
appearing once in C[ℓ, 3k + g] are not τ -majorities, since 1 ≤ τ (3k + g − ℓ + 1) for
any values k, ℓ, g. On the other hand, if an element appears twice in C[ℓ, 3k + g],
then it is a τ -majority, since 2 > τ (3k + g − ℓ + 1) for any values k, ℓ, g.

With this tool, we can discover x1 as follows. First, we ask whether there
is a τ -majority in C[1, 3k + 1]. If there is none, then x1 6∈ {1, . . . , k}, and we
have to look for it elsewhere (in C[4k + 1, 8k] or C[8k + 1, 12k]). Assume there
is a τ -majority in C[1, 3k + 1]; then x1 ∈ {1, . . . , k}. Now we query the range
C[2, 3k + 1]. If there is no τ -majority, then x 6∈ {2, . . . , k}, and we conclude that
x1 = 1. If there is, then x ∈ {2, . . . , k} and we query the range C[3, 3k + 1]. If
there is no τ -majority, then x 6∈ {3, . . . , k} and we conclude that x1 = 2, and so
on. The process is continued, if necessary, until querying the range C[k, 3k + 1].
If, instead, we had originally found out that x 6∈ {1, . . . , k}, then we look for it
analogously in C[4k + 1, 8k] or C[8k + 1, 12k].

To look for x2, we consider similarly ranges of the form C[ℓ, 3k + 2], with
identical reasoning. This time, it is possible that element x1 is also counted as an
answer, but since we already know the value of x1, we simply subtract 1 from the
count in any range C[ℓ, 3k + 2] with ℓ ≤ x1. This process continues analogously
until we identify xk. The other two thirds of πi are extracted analogously from
C[12k + 1, 24k] and C[24k + 1, 36k].

Example. Consider encoding m = 1 permutation π = (1 5 3 9 2 4 6 8 7), of size
3k = 9. Then we set τ = 1/8 and the array A[1, 108] is as follows:

1, 2, 3, −1, −2, −3, −4, −5, −6, 1, 5, 3

4, 5, 6, −1, −2, −3, −4, −5, −6, 1, 5, 3
7, 8, 9, −1, −2, −3, −4, −5, −6, 1, 5, 3

1, 2, 3, −1, −2, −3, −4, −5, −6, 9, 2, 4
4, 5, 6, −1, −2, −3, −4, −5, −6, 9, 2, 4

7, 8, 9, −1, −2, −3, −4, −5, −6, 9, 2, 4
1, 2, 3, −1, −2, −3, −4, −5, −6, 6, 8, 7

4, 5, 6, −1, −2, −3, −4, −5, −6, 6, 8, 7
7, 8, 9, −1, −2, −3, −4, −5, −6, 6, 8, 7

Now we will ﬁnd x1 (which is 1, but we do not know it yet). Since A[1, 10]
has a τ -majority, we know that x1 ∈ {1, 2, 3}. Since A[2, 10] has no τ -majority,
we know that x2 6∈ {2, 3}, thus we learn x1 = 1.

6

Now let us ﬁnd x2. Since A[1, 11] has one τ -majority, which we know corresponds 
to x1 = 1, we conclude that x2 6∈ {1, 2, 3}. Thus we will have to ﬁnd it
analogously in A[13, 24] or in A[25, 36].

Now let us ﬁnd x3. Since A[1, 12] has two τ -majorities, one of which we
know corresponds to x1 = 1, and the other we know does not correspond to x2,
we conclude that x3 ∈ {1, 2, 3}. Now A[2, 12] has one τ -majority. We know it
does not correspond to x1 = 1 (as it falls outside the range) nor to x2 (as it is
not in this subset). Then it follows that x3 ∈ {2, 3}. Finally, since A[3, 12] still
has one τ -majority, we conclude x3 ∈ {3}, thus x3 = 3.

Element x2 will be found in A[13, 24]. Elements x4, x5, x6 will be obtained

from A[37, 72] and elements x7, x8, x9 from A[73, 108].

Now, since n = 36km and τ = 1/(2k + 2), we have that any encoding able

to answer the above queries requires at least

m lg ((3k)!) > m (3k lg(3k) − 3k lg e + 1) >

n

12 (cid:18)lg(cid:18) 3

2

1
τ

− 3(cid:19) − lg e(cid:19)

bits3. This is Ω(n log(1/τ )) unless 1/τ ≤ 2 + 2
we show that Ω(n) is a lower bound for any constant τ ≥ 1/(2 + 2

3 e = O(1), thus it is suﬃcient that

3 e) > 1/4.

To show that Ω(n) bits are necessary for any τ ≥ 1/4, consider encoding a
bitmap B[1, m] in an array A[1, 4m] so that, if B[i] = 0, then A[4(i − 1) + 1] = 1,
A[4(i − 1) + 2] = 2, A[4(i − 1)i + 3] = 3, and A[4i] = 4. Instead, if B[i] = 1,
then A[4(i − 1) + 1, 4i] = 1. Then, for any τ ≥ 1/4, there is a τ -majority in
A[4(i − 1) + 1, 4i] iﬀ B[i] = 1. As there are 2m possible bitmaps B and our array
is of length n = 4m, we need at least m = n/4 = Ω(n) bits for any encoding.
Then the proof of Theorem 1 is complete.

4. An O((n/τ ) log log n) Bits Encoding for Range τ -Majorities

In this section we obtain an encoding using O((n/τ ) log log n) bits and solving 
τ -majority queries in O((1/τ ) log n) time. In the next section we improve
the space usage. We assume that τ is ﬁxed at construction time. At query time,
we will be able to solve any τ ′-majority query for any τ ≤ τ ′ < 1.

4.1. The Basic Idea

Consider each distinct symbol x appearing in A[1, n]. Now consider the set
of all the segments Sx within [1, n] where x is a τ -majority (this includes, in
particular, all the segments [k, k] where A[k] = x). Segments in Sx may overlap
each other. Now let Ax[1, n] be a bitmap such that Ax[k] = 1 iﬀ position k
belongs to some segment in Sx. We deﬁne a second bitmap related to x, Mx, so
that if Ax[k] = 1, then Mx[rank(Ax, k)] = 1 iﬀ A[k] = x, where operation rank
was deﬁned in Section 2.2.

3Bounding lg(3k)! with integrals one obtains 3k lg(3k/e) + 1 ≤ lg(3k)! ≤ (3k + 1) lg((3k +

1)/e) + 1.

7

Example. Let our running example array be A[1, 7] = h1 3 2 3 3 1 1i, and
τ = 1/2. Then we have the segments Sx:

S1 = {[1, 1], [6, 6], [7, 7], [6, 7], [5, 7]},
S2 = {[3, 3]},
S3 = {[2, 2], [4, 4], [5, 5], [4, 5], [2, 4], [3, 5], [4, 6], [2, 5], [1, 5], [2, 6]},

and the corresponding bitmaps Ax:

A1 = h1 0 0 0 1 1 1i, A2 = h0 0 1 0 0 0 0i, A3 = h1 1 1 1 1 1 0i.

Finally, the corresponding bitmaps Mx are:

M1 = h1 0 1 1i, M2 = h1i, M3 = h0 1 0 1 1 0i.

Then, the following result is not diﬃcult to prove.

Lemma 2. An element x is a τ ′-majority in A[i, j] iﬀ Ax[k] = 1 for all i ≤
k ≤ j, and 1 is a τ ′-majority in Mx[rank(Ax, i), rank(Ax, j)].
Proof. If x is a τ ′-majority in A[i, j], then it is also a τ -majority. Thus,
by deﬁnition,
[i, j] ∈ Sx, and therefore all the positions k ∈ [i, j] are set
to 1 in Ax. Therefore, the whole segment Ax[i, j] is mapped bijectively to
Mx[rank(Ax, i), rank(Ax, j)], which is of the same length. Finally, the number
of occurrences of x in A[i, j] is the number of occurrences of 1 in Mx[rank(Ax, i),
rank(Ax, j)], which establishes the result.

Conversely, if Ax[k] = 1 for all i ≤ k ≤ j, then A[i, j] is bijectively mapped
to Mx[rank(Ax, i), rank(Ax, j)], and the 1s in this range correspond one to one
with occurrences of x in A[i, j]. Therefore, if 1 is a τ ′-majority in Mx[rank(Ax, i),
rank(Ax, j)], then x is a τ ′-majority in A[i, j].

Example. Value 1 is a majority in A[5, 7], and it holds that A1[5, 7] = h1 1 1i
and M1[rank(A1, 5), rank(A1, 7)] = M1[2, 4] = h0 1 1i, where 1 is a majority.

Thus, with Ax and Mx we can determine whether x is a majority in a range.

Lemma 3. It is suﬃcient to have rank-enabled bitmaps Ax and Mx to determine,
 in constant time, whether x is a τ ′-majority in any A[i, j].
Proof. We use Lemma 2. We compute i′ = rank(Ax, i) and j′ = rank(Ax, j). If
j′ − i′ 6= j − i, then Ax[k] = 0 for some i ≤ k ≤ j and thus x is not a τ -majority
in A[i, j], hence it is also not a τ ′-majority. Otherwise, we ﬁnd out whether 1 is a
τ ′-majority in Mx[i′, j′], by checking whether rank(Mx, j′) − rank(Mx, i′ − 1) >
τ ′(j′ − i′ + 1).

To ﬁnd any position i ≤ k ≤ j where A[k] = x, we need the operation
select(B, j), deﬁned in Section 2.2. Then, for example, if x is a τ ′-majority in
A[i, j], its leftmost occurrence in A[i, j] is i−i′ +select(Mx, rank(Mx, i′ −1)+1).
In general, for any 1 ≤ t ≤ rank(Mx, j′) − rank(Mx, i′ − 1), we can retrieve the
tth occurrence with i − i′ + select(Mx, rank(Mx, i′ − 1) + t).

8

4.2. Coalescing the Bitmaps

We cannot aﬀord to store (and probe!) all the bitmaps Ax and Mx for all x,
however. The next lemma is the ﬁrst step to reduce the total space to slightly
superlinear.

Lemma 4. For any position A[k] = x there are at most 2⌈1/τ ⌉ 1s in Ax.

Proof. Consider a process where we start with A[k] = ⊥ for all k, and set the
values A[k] = x progressively. We will distinguish three kinds of changes.

(1) New segments around A[k] are created in Sx. Setting A[k] = x creates in
Sx all the segments of the form [k − kl, k + kr] for 1 > τ (kr + kl + 1), or
kl +kr < 1/τ −1. Their union is the area Ax[k−⌈1/τ ⌉+2, . . . , k+⌈1/τ ⌉−2] = 1,
which may increase the number of 1s in Ax by up to 2⌈1/τ ⌉ − 3.

(2) Segments already covering A[k] are extended. Any maximal segment [l, r] ∈
Sx covering Ax[k] contains c > τ (r − l + 1) occurrences of x, but it holds that
c ≤ τ (r − l + 2), otherwise there would also exist segments [l − 1, r] and [l, r + 1]
in Sx, and [l, r] would not be maximal. Therefore, adding one more occurrence,
A[k] = 1, we get c + 1 ≤ τ (r − l + 2 + 1/τ ) occurrences in [l, r]. Now it holds that
x may be a τ -majority in segments [l − kl, r + kr] for all 0 ≤ kl + kr < 1 + 1/τ
(i.e., where c+1 > τ (r −l+1+kl +kr), using only that c+1 ≤ τ (r −l+2+1/τ )),
and therefore we can extend [l, r] to the left by up to ⌈1/τ ⌉, or to the right by
up to ⌈1/τ ⌉.

(3) Segments reaching close to A[k] are extended. The same reasoning as for
the previous case applies, even if [l, r] does not originally contain position k.
There are more restrictions, since now [l − kl, r + kr] must be so that it contains
k, and the same limit 0 ≤ kl + kr < 1 + 1/τ applies. Thus, in addition to being
possible to extend them by at most ⌈1/τ ⌉ cells in either direction, position k
must lie within the extended area.

Total extension. The three cases above are superimposed. Let ℓl and ℓr the
closest positions ℓl ≤ k ≤ ℓr where Ax[ℓl] = Ax[ℓr] = 1. Then, if ℓl = k, we can
set at most ⌈1/τ ⌉ new 1s in Ax to the left of k by extending segments using case
(2). Otherwise, if k − ℓl ≤ ⌈1/τ ⌉, we can cover the area Ax[ℓl + 1, . . . , k] and
add up to ⌈1/τ ⌉ − (k − ℓl) further cells to the left, using case (3). Otherwise, if
k − ℓl > ⌈1/τ ⌉, we set ⌈1/τ ⌉ − 2 cells to the left, apart from k, using case (1).
The same reasoning applies to the right, and therefore 2⌈1/τ ⌉ is an upper bound
to the number of 1s in Ax produced by each new occurrence of x in A.

The lemma shows that all the Ax bitmaps add up to O(n/τ ) 1s, and thus
the lengths of all the Mx bitmaps add up to O(n/τ ) as well (recall that Mx has
one position per 1 in Ax). Therefore, we can store all the Mx bitmaps within
O(n/τ ) bits of space. We cannot, however, store all the Ax bitmaps, as they
may add up to O(n2) 0s (note there can be O(n) distinct symbols x), and we
still cannot probe all the Ax bitmaps for all x in o(n) time.

9

Instead, we will coalesce all the bitmaps Ax into a smaller number of bitmaps
A′
r (which will be called coalesced bitmaps). Coalescing works as follows. Let us
write A[i, j] = b to mean A[ℓ] = b for all i ≤ ℓ ≤ j. We start with all A′
r[1, n] = 0
for all r. Then we take each maximal area of all 1s of each bitmap, Ax[i, j] = 1,
choose some r such that A′
r[i − 1, j + 1] = 0, and set A′
r[i, j] = 1. That is, we
copy the run of 1s from Ax to some coalesced bitmap A′
r such that the run does
not overlap nor touch other previous runs already copied (i.e., there must be
at least one 0 between any two copied runs of 1s). We associate to each such
A′
r where the areas of each Mx corresponding to each coalesced
area of Ax are concatenated, in the same order of the coalesced areas. That is,
if A′
r, was copied from Ax, then
Mx[rank(Ax, it), rank(Ax, jt)] will be the tth segment appended to M ′
r.

r[it, jt] = 1, the tth left-to-right run of 1s in A′

r a bitmap M ′

Example. We can coalesce the whole bitmaps A1 and A2 into A′ = h1 0 1 0 1 1 1i,
with the corresponding bitmap M ′ = h1 1 0 1 1i.

r and M ′

The coalesced bitmaps A′

r will replace the original bitmaps Ax and
Mx. At query time, we check for the area [i, j] of each coalesced bitmap using
Lemma 3. We cannot confuse the areas of diﬀerent symbols x because we force
that there is at least one 0 between any two areas. We cannot report the same
τ ′-majority x in more than one coalesced bitmap, as both areas should overlap
on [i, j] and then they would have been merged as a single area in Ax. If we ﬁnd
one τ ′-majority in one coalesced bitmap, we know that there is a τ ′-majority x
and can spot all of its occurrences (or the leftmost, if desired) in optimal time,
even if we cannot know the identity of x. Moreover, we will ﬁnd all the distinct
τ ′-majorities in this way.

4.3. Bounding the Number of Coalesced Bitmaps

This scheme will work well if we obtain just a few coalesced bitmaps overall.

Next we show how to obtain only O((1/τ ) log n) coalesced bitmaps.

Lemma 5. At most 2 log1+τ n distinct values of x can have Ax[k] = 1 for a
given k.

Proof. First, A[k] = x is a τ -majority in A[k, k], thus Ax[k] = 1. Now consider
6= x such that Ax′ [k] = 1. This means that x′ is a τ -
any other element x′
majority in some [i, j] that contains k. Since A[k] 6= x′, it must be that x′ is a τ -
majority in [i, k−1] or in [k+1, j] (or in both). We say x′ is a left-majority in the
ﬁrst case and a right-majority in the second. Let us call y1, y2, . . . the x′ values
that are left-majorities, and i1, i2, . . . the starting points of their segments (if
they are τ -majorities in several segments covering k, we choose one arbitrarily).
Similarly, let z1, z2, . . . be the x′ values that are right-majorities, and j1, j2, . . .
the ending points of their segments. Assume the left-majorities are sorted by
decreasing values of ir and the right-majorities are sorted by increasing values
of jr. If a same value x′ appears in both lists, we arbitrarily remove one of them.
As an exception, we will start both lists with y0 = z0 = x, with i0 = j0 = k.

10

It is easy to see by induction that yr must appear at least (1 + τ )r times
in the interval [ir, k] (or in [ir, k − 1], which is the same). This clearly holds
for y0 = x. Now, by the inductive hypothesis, values y0, y1, . . . , yr−1 appear at
least (1 + τ )0, (1 + τ )1, . . . , (1 + τ )r−1 times within [ir−1, k − 1] (which contains
all the intervals), adding up to (1+τ )r−1
occurrences. Thus k − 1 − ir−1 +
1 ≥ (1+τ )r−1
. In order to be a left-majority, element yr must appear strictly
more than τ (k − ir−1) ≥ (1 + τ )r − 1 times in [ir, k − 1], to outweight all the
occurrences of the previous symbols. The case of right-majorities is analogous.
This shows that there cannot be more than log1+τ n left-majorities and log1+τ n
right-majorities.

τ

τ

In the following it will be useful to deﬁne Cx as the set of maximal contiguous
areas of 1s in Ax. That is, Cx is obtained by merging all the segments of Sx that
touch or overlap. Note that segments of Cx do not overlap, unlike those of Sx.
Since a segment of Cx covers a position k iﬀ some segment of Sx covers position
k (and iﬀ Ax[k] = 1), it follows by Lemma 5 that any position is covered by at
most 2 log1+τ n segments of Cx of distinct symbols x.

Note that a pair of consecutive positions A[k] = x and A[k + 1] = y is also
covered by at most 2 log1+τ n such segments: the right-majorities for A[k] either
are y or are also right-majorities for A[k + 1], and those are already among the
log1+τ n right-majorities of A[k + 1]. And vice versa.

We obtain O(log1+τ n) coalesced bitmaps as follows. We take the union of all
the sets Cx of all the symbols x and sort the segments by their starting points.
Then we start ﬁlling coalesced bitmaps. We check if the current segment can
be added to an existing bitmap without producing overlaps (and leaving a 0 in
between). If we can, we choose any appropriate bitmap, otherwise we start a new
bitmap. If at some point we need more than 2 log1+τ n bitmaps, it is because all
the last segments of the current 2 log1+τ n bitmaps overlap either the starting
point of the current segment or the previous position, a contradiction.

Example. We have C1 = {[1, 1], [5, 7]}, C2 = {[3, 3]}, and C3 = {[1, 6]}. Now,
we take C1 ∪ C2 ∪ C3 = {[1, 1], [1, 6], [3, 3], [5, 7]}, and the process produces precisely 
the coalesced bitmaps A′, corresponding to the set {[1, 1], [3, 3], [5, 7]}, and
A3, corresponding to {[1, 6]}.

Note that in general the coalesced bitmaps may not correspond to the union
of complete original bitmaps Ax, but areas of a bitmap Ax may end up in
diﬀerent coalesced bitmaps.

Therefore, the coalescing process produces O(log1+τ n) = O((1/τ ) log n)
bitmaps. Consequently, we obtain O((1/τ ) log n) query time by simply checking
the coalesced bitmaps one by one using Lemma 3.

Finally, representing the O((1/τ ) log n) coalesced bitmaps A′, which have
total length O((n/τ ) log n) and contain O(n/τ ) 1s, requires O((n/τ ) log log n)
bits if we use a compressed bitmap representation [25] that still oﬀers constanttime 
rank and select queries (recall Section 2.2). The coalesced bitmaps M ′
still have total length O(n/τ ).

11

This completes the ﬁrst part of our result. Next, we will reduce the space

usage of our encoding.

5. Reducing the Space to O(n log(1/τ )) Bits

We introduce a diﬀerent representation of the coalesced bitmaps that allows
us to store them in O(n log(1/τ )) bits, while retaining the same mechanism
described above. We note that, although there can be O(n/τ ) bits set in the
bitmaps Ax, each new element x produces at most one new run of contiguous
1s (case (1) in the proof of Lemma 4). Therefore there are at most n runs in
total. We will use a representation of coalesced bitmaps that takes advantage
of these runs.

We will distinguish segments of Cx by their lengths, separating lengths by
ranges between ⌈2ℓ/τ ⌉ and ⌈2ℓ+1/τ ⌉ − 1, for any level 0 ≤ ℓ ≤ lg(τ n) (level 0
is special in that it contains lengths starting from 1). In the process of creating
the coalesced bitmaps described in the previous section, we will have separate
coalesced bitmaps for inserting segments within each range of lengths; these will
be called bitmaps of level ℓ. There may be several bitmaps of the same level.
It is important that, even with this restriction, our coalescing process will still
generate O((1/τ ) log n) bitmaps, because only O(1/τ ) coalesced bitmaps of each
level ℓ will be generated.

Lemma 6. There can be at most 4/τ segments of any Cx, of length between
⌈2ℓ/τ ⌉ and ⌈2ℓ+1/τ ⌉ − 1, covering a given position k, for any ℓ.

Proof. Any such segment must be contained in the area A[k − ⌈2ℓ+1/τ ⌉ + 1, k +
⌈2ℓ+1/τ ⌉ − 1], and if x is a τ -majority in it, it must appear more than τ ⌈2ℓ/τ ⌉ ≥
2ℓ times. There can be at most 4/τ diﬀerent values of x appearing more than
2ℓ times in an area of length less than 2ℓ+2/τ .

1[1, n/b] will have A′

2[1, n/b] will have A′

Consider a coalesced bitmap A′[1, n] of level ℓ. All of its 1s come in runs
of lengths at least b = ⌈2ℓ/τ ⌉. We cut A′ into chunks of length b and deﬁne
1[i] = 1 iﬀ the ith chunk of A′ is all 1s, and
two bitmaps: A′
A′
2[i] = 1 iﬀ the ith chunk of A′ has 0s and 1s. Note that,
since the runs of 1s are of length at least b, inside a chunk with 0s and 1s there
can be at most one 01 and at most one 10, and the 10 can only come before
the 01. Let p10[j] be the position, in the jth chunk with 0s and 1s, of the 1
preceding a 0, where p10[j] = 0 if the chunk starts with a 0. Similarly, let p01[j]
be the position of the 0 preceding a 1, with p01[j] = b if the chunk ends with
a 0. It always holds that p10[j] < p01[j], and the number of 1s in the chunk
is r(j) = p10[j] + (b − p01[j]). Also, the rank up to position k in the chunk,
r(j, k), is k if k ≤ p10[j], p10[j] if p10[j] < k ≤ p01[j], and p10[j] + (k − p01[j]) if
k > p01[j]. Then it holds that

rank(A′, i) = b · r1 +

r2

Xj=1

r(j) +

[if A′

2[1 + ⌊i/b⌋] = 1 then r(r2 + 1, k) else A′

1[1 + ⌊i/b⌋] · k],

12

where r1 = rank(A′
2, ⌊i/b⌋), and k = i mod b. Note this
can be computed in constant time as long as we have constant-time rank data
structures on A′

2, and constant-time access and sums on p10 and p01.

1, ⌊i/b⌋), r2 = rank(A′

1 and A′

Example. Using b = 2ℓ to make it more interesting, we would have three coalesced 
bitmaps: A′ = h1 0 1 0 0 0 0i, of level ℓ = 0, for the segments [1, 1]
and [3, 3]; A′′ = h0 0 0 0 1 1 1i, of level ℓ = 1, for the segment [5, 7]; and
A′′′ = h1 1 1 1 1 1 0i, of level ℓ = 2, for the segment [1, 6]. Consider level
ℓ = 0 and b = 2, and let us focus on A′. Then, we would have A′
1 = h0 0 0 0i,
A′

2 = h1 1 0 0i, p10 = h1 1i, and p01 = h2 2i.

a bitmap A′

j=1 p10[j] = select(A′

all r. Then we can recover Pr

10, where we set all the bits at positions r + Pr

To have constant-time sums on p10 (p01 is analogous), we store its values in
j=1 p10[j] to 1, for
10, r) − r. We use a bitmap
representation [21] that solves select in constant time (recall Section 2.2). Let
n′ be the number of segments Cx represented in bitmap A′. Then there are at
10 contains at most 2n′ 1s and 2n′b 0s
most 2n′ chunks with 0s and 1s, and A′
(as 0 ≤ p10[j] ≤ b). The size of the bitmap representation [21] is in this case
O(n′ log b) = O(n′(ℓ + log(1/τ ))) bits. On the other hand, bitmaps A′
1 and A′
2
are represented in plain form [18, 8], requiring O(n/b) = O(nτ /2ℓ) bits.

Considering that there are O(n/τ ) 1s overall, and that the runs of level ℓ
are of length at least 2ℓ/τ , we have that there can be at most n/2ℓ runs across
the O(1/τ ) bitmaps of level ℓ. Therefore, adding up the space over the bitmaps
of level ℓ, we have O(n(ℓ + log(1/τ ))/2ℓ) bits. Added over all the levels ℓ, this
gives O(n log(1/τ )) bits.

Let us now consider the representation of the coalesced bitmaps M ′. They
have total length O(n/τ ) and contain n 1s overall, therefore using the representation 
of Lemma 1 with s = 1, we have O(n log(1/τ )) bits of space. They solve
rank queries in time O(log logw(1/τ )), and select in constant time.

As we have to probe O((1/τ ) log n) coalesced bitmaps M ′ in the worst case,
this raises our query time to O((1/τ ) log logw(1/τ ) log n). This concludes the
proof of Theorem 2, except for the construction time (see the next section).

In our previous work [20], we had obtained O((1/τ ) log n) time, but using
O((n/τ ) log∗ n) bits of space. It is not hard to obtain that time, using O(n/τ )
bits, by simply representing the coalesced bitmaps M ′ using plain rank/select
structures [8, 18], or even using O(n log(1/τ ) + (n/τ )/ polylog n) bits, for any
polylog n, using compressed representations [24]. The extra O(log logw(1/τ ))
time factor arises when we insist in obtaining the optimal O(n log(1/τ )) bit
space. We note that this time penalty factor vanishes when 1/τ = wO(1), which
includes the case where 1/τ grows polylogarithmically with n.

6. Construction

The most complex part of the construction of our encoding is to build the
sets Cx. Once these are built, the structures described in Section 5 can be easily
constructed in o(n log n) time:

13

1. The O(n) segments Cx belong to [1, n], so they are sorted by starting point

in O(n) time.

2. We maintain a priority queue for each level ℓ, containing the last segment
of each coalesced bitmap. We use the queue to ﬁnd the segment that
ﬁnishes earliest in order to try to add the new segment of Cx after it. We
carry out, in total, O(n) operations on those queues, and each contains
O(1/τ ) elements, thus they take total time O(n log(1/τ )) = o(n log n).

3. The bitmaps A′ of each level ℓ, represented with A′

10, are
easily built in O(n/b) = O(nτ /2ℓ) time. Added over the O(1/τ ) coalesced
bitmaps of level ℓ this is O(n/2ℓ), and added over all the levels ℓ this gives
O(n) total time.

01 and A′

1, A′

2, A′

4. The coalesced bitmaps M ′ have O(n) 1s overall, so their representation
(Lemma 1) is also built in O(n) time, except for the predecessor structures,
which need construction of deterministic dictionaries. This can be done
in o(n log n) total time [26].

Now we show that the sets Cx can be built in O(n log n) time, thus ﬁnishing

the proof of Theorem 2.

We build the set of increasing positions Px where x appears in A, for each x,
in O(n log n) total time (the elements of A can be of any atomic type, so we only
rely on a comparison-based dictionary to maintain the set of diﬀerent x values
and their Px lists). Now we build Cx from each Px using a divide-and-conquer
approach, in O(|Px| log |Px|) time, for a total construction time of O(n log n).

We pick the middle element k ∈ Px and compute in linear time the segment
[l, r] ∈ Cx that contains k. To compute l, we ﬁnd the leftmost element pl ∈ Px
such that x is a τ -majority in [pl, kr], for some kr ∈ Px with kr ≥ k.

To ﬁnd pl, we note that it must hold that (w(pl, k − 1) + w(k, kr))/(kr − pl +
1) > τ , where w(i, j) is the number of occurrences of x in A[i, j]. The condition
is equivalent to w(pl, k − 1)/τ + pl − 1 > kr − w(k, kr)/τ . Thus we compute in
linear time the minimum value v of kr − w(k, kr)/τ over all those kr ∈ Px to
the right of k, and then traverse all those pl ∈ Px to the left of k, left to right,
to ﬁnd the ﬁrst one that satisﬁes w(pl, k − 1)/τ + pl + 1 > v, also in linear time.
Once we ﬁnd the proper pl and its corresponding kr, the starting position of
the segment is slightly adjusted to the left of pl, to be the smallest value that
satisﬁes w(pl, kr)/(kr − l + 1) > τ , that is, l satisﬁes l > −w(pl, kr)/τ + kr + 1,
or l = kr − ⌈w(pl, kr)/τ ⌉ + 2.

Once pr and then r are computed analogously, we insert [l, r] into Cx and
continue recursively with the elements of Px to the left of pl and to the right of
pr. Upon return, it might be necessary to join [l, r] with the rightmost segment
of the left part and/or with the leftmost segment of the right part, in constant
time. The total construction time is T (n) = O(n) + 2T (n/2) = O(n log n).

Building multiple structures. In order to answer τ ′-majority queries for any τ ≤
τ ′ < 1 in time related to 1/τ ′ and not to 1/τ , we build the encoding of Theorem 2
for values τ ′′ = 1/2, 1/4, 1/8, . . . , 1/2⌈lg 1/τ ⌉. Then, a τ ′-majority query is run

14

on the structure built for τ ′′ = 1/2⌈lg 1/τ ′⌉. Since τ ′/2 < τ ′′ ≤ τ ′, the query
time is O((1/τ ′′) log logw(1/τ ′′) log n) = O((1/τ ′) log logw(1/τ ′) log n).

As for the space, we build O(log(1/τ )) structures, so we use O(n log2(1/τ ))

bits, and the construction time is O(n log(1/τ ) log n).

Corollary 1. Given a real number 0 < τ < 1, there exists an encoding using
O(n log2(1/τ )) bits that answers range τ ′-majority queries, for any τ ≤ τ ′ < 1,
in time O((1/τ ′) log logw(1/τ ′) log n), where w = Ω(log n) is the RAM word size
in bits. The structure can be built in time O(n log(1/τ ) log n).

7. A Faster Data Structure

In this section we show how, by adding O(n log log n) bits to our data structure,
 we can slash a log n factor from the query time, that is, we prove Theorem 
3. The result, as discussed in the Introduction, yields the optimal query
time O(1/τ ) when 1/τ = O(polylog n), although the resulting space may not
be optimal anymore.

The idea is inspired in a previous non-encoding data structure for majority
queries [1]. Consider a value ℓ. Then we will cut A into consecutive pieces of
length 2ℓ (said to be of level ℓ) in two overlapped ways: A[2ℓk + 1, 2ℓ(k + 1)] and
A[2ℓk + 2ℓ−1 + 1, 2ℓ(k + 1) + 2ℓ−1], for all k ≥ 0. We carry out this partitioning
for every ⌈lg(1/τ )⌉ ≤ ℓ ≤ ⌈lg n⌉.

Note that there are O(n/2ℓ) pieces of level ℓ, and any interval A[i, j] of length
up to 2ℓ/2 is contained in some piece P of level ℓ. Now, given a query interval
A[i, j], let ℓ = ⌈lg(j − i + 1)⌉ + 1. Then, not only A[i, j] is contained in a piece
P of level ℓ, but also any τ -majority x in A[i, j] must be a τ /4-majority in P :
Since j − i + 1 > 2ℓ/4, x occurs more than τ (j − i + 1) > (τ /4)2ℓ times in A[i, j],
and thus in P .

Consider a τ /4-majority x in a given piece P of level ℓ that is also a τ -
majority for some range A[i, j] within P , where 2ℓ/4 < j − i + 1 ≤ 2ℓ/2. By
construction of our previous structures, there exists a maximal segment Cx that
If there is another range A[i′, j′] within P where x
contains the range [i, j].
is a τ -majority, then there exists another maximal segment C′
x for the same x
within P . By our construction, if C′
x is disjoint with Cx, and
thus each of them contains at least (τ /4)2ℓ distinct occurrences of x. Obviously,
segments Cy for τ -majorities y 6= x contain other (τ /4)2ℓ occurrences disjoint
from those of x. Therefore, the number of distinct maximal segments C that
contain τ -majorities at any range A[i, j] (with j − i + 1 > 2ℓ/4) within P is
upper bounded by 4/τ . We will say those segments C are relevant to P .

x 6= Cx, then C′

Therefore, for each piece P of level ℓ, we will store the index r of the
coalesced bitmap A′
r) to which each maximal segment 
C that is relevant to P belongs. Since there are at most 4/τ such coalesced 
bitmaps to record, out of a total of O((1/τ ) log n) coalesced bitmaps,
γ-codes on a diﬀerential encoding of the subset values requires O((1/τ ) log log n)

r (and its companion M ′

15

bits.4 Added up over the O(n/2ℓ) pieces of level ℓ ≥ ⌈lg(1/τ )⌉, this yields

r and M ′

Pℓ≥⌈lg(1/τ )⌉ O((n/2ℓ)(1/τ ) log log n) = O(n log log n) bits.

This information reduces the search eﬀort to that of verifying O(1/τ ) coalesced 
bitmaps A′
r for the range [i, j], and thus to O((1/τ ) log logw(1/τ ))
query time. However, for ranges shorter than 1/τ , where no piece structure has
been built, we still have the original query time. To speed up this case, we build a
second structure where, for each element A[k], we identify the coalesced bitmap
where the maximal segment CA[k] containing the segment A[k, k] belongs, and
store the identiﬁer r of the corresponding coalesced bitmap A′
r) associated 
to k. This requires O(n log((1/τ ) log n)) = O(n log(1/τ ) + n log log n)
further bits, and allows checking only one coalesced bitmap A′
r) for
each of the O(1/τ ) positions that need to be checked.

r (and M ′

r (and M ′

To ﬁnish the proof we must consider the construction time. The second
structure (for short ranges) is easily built with the general structure, taking
no additional time, by keeping track of which maximal segment CA[k] contains
each segment A[k, k] and which coalesced bitmap it is assigned. With this, the
structure for long ranges can be built as follows: for each position A[k] contained
in a piece P of level ℓ, consider the maximal segment CA[k] that contains it and
determine whether it is relevant to P . A weak test for this is to consider the
coalesced bitmap M ′ where CA[k] is represented (which is precisely what the
ﬁrst structure stores associated to k) and ask whether M ′ contains more than
(τ /4)2ℓ 1s in the range of P . This must be the case if CA[k] is relevant to P .
Although including the identiﬁer of each M ′ that passes the test may add some
nonrelevant ones, we still cannot include more than 4/τ coalesced bitmaps in
the set, as the 1s in the M ′ bitmaps are disjoint.

The rank operations on bitmaps M ′ take O(log logw(1/τ )) time, so we avoid
them to count how many 1s does M ′ contain in the range of P . Instead, we
perform a preprocessing pass over P as follows: We initialize to zero a set of
O((1/τ ) log n) counters, one per coalesced bitmap M ′, and process P left to
right. We increase the counter associated to the bitmap M ′ of each element
A[k] in P . At the end, we know all the desired values. This takes O(2ℓ) time,
and a similar postprocessing pass clears the counter for the next piece.

Therefore, we process all the pieces P of level ℓ in time O(2ℓ), which amounts
to O(n) time per level. Added over all the levels, this gives O(n log n) total time.
This concludes the proof of Theorem 3.

8. Conclusions

A τ -majority query on array A[1, n] receives a range [i, j] and returns all the
elements appearing more than τ (j − i + 1) times in A[i, j]. We have obtained the
ﬁrst results about encodings for answering range τ -majority queries. Encodings
are data structures that use less space than what is required to store A and

4We could also aﬀord to store them in plain form using O((1/τ )(log(1/τ ) + log log n)) bits.

16

answer queries without accessing A at all. In the encoding scenario we do not
report the τ -majorities themselves, but one of their positions in A[i, j].

We have proved that Ω(n log(1/τ )) bits are necessary for any such encoding,
even if it can only count the number of τ -majorities in any range. Then we
presented an encoding that uses the optimal O(n log(1/τ )) bits, and answers
queries in O((1/τ ) log logw(1/τ ) log n) time in the RAM model with word size
w = Ω(log n) bits. We also showed that this time can be divided by log n if we
add O(n log log n) bits to the space. This yields various space/time tradeoﬀs,
shown in Table 1. Our encoding can actually report any occurrence of each
τ -majority, in optimal extra time. The structure is built in O(n log n) time.

An open question is whether it is possible to achieve optimal query time
within optimal space for all values of 1/τ . As seen in Table 1, we reach this
only for log(1/τ ) = Θ(log log n). This is also possible when log(1/τ ) = Ω(log n),
where we leave the non-encoding scenario [1]. Instead, our results for log(1/τ )
between log log n and log n have a small factor O(log logw(1/τ )) over the optimal 
time, and those for log(1/τ ) below log log n either require nonoptimal
O(n log log n) bits of space, or an O(log n) factor over the optimal time. It is
not clear whether combined optimality can be reached.

Another open question is whether we can do better for weaker versions of
the problem we have not studied. For example, if we are only required to report
any occurrence of any τ -majority (or, even less, telling whether or not there
exists a τ -majority), our lower bound based on representing a bitmap B shows
that Ω(n) bits are necessary, but we do not know if this bound is tight.

References

[1] Belazzougui, D., Gagie, T., Navarro, G., 2013. Better space bounds for parameterized 
range majority and minority. In: Proc. 11th Annual Workshop
on Algorithms and Data Structures (WADS). pp. 121–132.

[2] Berkman, O., Vishkin, U., 1993. Recursive star-tree parallel data structure.

SIAM Journal on Computing 22 (2), 221–242.

[3] Bose, P., Kranakis, E., Morin, P., Tang, Y., 2005. Approximate range
mode and range median queries. In: Proc. 22nd International Symposium
on Theoretical Aspects of Computer Science (STACS). pp. 377–388.

[4] Brodal, G., Fagerberg, R., Greve, M., L´opez-Ortiz, A., 2009. Online sorted
range reporting. In: Proc. 20th Annual International Symposium on Algorithms 
and Computation (ISAAC). pp. 173–182.

[5] Chan, T., Durocher, S., Larsen, K., Morrison, J., Wilkinson, B., 2012.
Linear-space data structures for range mode query in arrays. In: Proc.
29th International Symposium on Theoretical Aspects of Computer Science
(STACS). pp. 290–301.

17

[6] Chan, T., Durocher, S., Skala, M., Wilkinson, B., 2012. Linear-space data
structures for range minority query in arrays. In: Proc. 13th Scandinavian
Symposium on Algorithmic Theory (SWAT). pp. 295–306.

[7] Chan, T., Wilkinson, B., 2013. Adaptive and approximate orthogonal range
counting. In: Proc. 24th Annual ACM-SIAM Symposium on Discrete Algorithms 
(SODA). pp. 241–251.

[8] Clark, D., 1996. Compact PAT trees. Ph.D. thesis, University of Waterloo,

Canada.

[9] Durocher, S., He, M., Munro, I., Nicholson, P., Skala, M., 2013. Range
majority in constant time and linear space. Information and Computation
222, 169–179.

[10] Elias, P., 1974. Eﬃcient storage and retrieval by content and address of

static ﬁles. Journal of the ACM 21, 246–260.

[11] Fano, R., 1971. On the number of bits required to implement an associative 
memory. Memo 61, Computer Structures Group, Project MAC, Massachusetts.


[12] Fischer, J., Heun, V., 2011. Space-eﬃcient preprocessing schemes for range
minimum queries on static arrays. SIAM Journal of Computing 40 (2),
465–492.

[13] Gagie, T., He, M., Munro, I., Nicholson, P., 2011. Finding frequent elements 
in compressed 2d arrays and strings. In: Proc. 18th International
Symposium on String Processing and Information Retrieval (SPIRE). pp.
295–300.

[14] Greve, M., Jørgensen, A., Larsen, K. D., Truelsen, J., 2010. Cell probe lower
bounds and approximations for range mode. In: Proc. 37th International
Colloquium on Automata, Languages and Programming (ICALP). pp. 605–
616.

[15] Grossi, R., Iacono, J., Navarro, G., Raman, R., Satti, S. R., 2013. Encodings 
for range selection and top-k queries. In: Proc. 21st Annual European
Symposium on Algorithms (ESA). pp. 553–564.

[16] Karpinski, M., Nekrich, Y., 2008. Searching for frequent colors in rectangles.
 In: Proc. 20th Canadian Conference on Computational Geometry
(CCCG). pp. 11–14.

[17] Karpinski, M., Nekrich, Y., 2011. Top-k color queries for document retrieval.
 In: Proc. 22nd Annual ACM-SIAM Symposium on Discrete Algorithms 
(SODA). pp. 401–411.

[18] Munro, I., 1996. Tables. In: Proc. 16th Conference on Foundations of Software 
Technology and Theoretical Computer Science (FSTTCS). pp. 37–42.

18

[19] Navarro, G., Raman, R., Rao, S. S., 2014. Asymptotically optimal encodings 
for range selection. In: Proc. 34th Annual Conference on Foundations
of Software Technology and Theoretical Computer Science (FSTTCS).
LNCS. To appear.

[20] Navarro, G., Thankachan, S., 2014. Encodings for range majority queries.
In: Proc. 25th Annual Symposium on Combinatorial Pattern Matching
(CPM). LNCS 8486. pp. 262–272.

[21] Okanohara, D., Sadakane, K., 2007. Practical entropy-compressed
rank/select dictionary. In: Proc. 9th Workshop on Algorithm Engineering
and Experiments (ALENEX). pp. 60–70.

[22] Petersen, H., Grabowski, S., 2009. Range mode and range median queries
in constant time and sub-quadratic space. Information Processing Letters
109 (4), 225–228.

[23] P˘atra¸scu, M., Thorup, M., 2008. Time-space trade-oﬀs for predecessor

search. CoRR cs/0603043v1, http://arxiv.org/pdf/cs/0603043v1.

[24] Pˇatra¸scu, M., 2008. Succincter. In: Proc. 49th Annual IEEE Symposium

on Foundations of Computer Science (FOCS). pp. 305–313.

[25] Raman, R., Raman, V., Rao, S. S., 2007. Succinct indexable dictionaries
with applications to encoding k-ary trees, preﬁx sums and multisets. ACM
Transactions on Algorithms 3 (4), article 43.

[26] Ruˇzi´c, M., 2008. Constructing eﬃcient dictionaries in close to sorting time.
In: Proc. 35th International Colloquium on Automata, Languages and
Programming (ICALP). LNCS 5125. pp. 84–95 (part I).

[27] Skala, M., 2013. Array range queries. In: Space-Eﬃcient Data Structures,

Streams, and Algorithms. LNCS. Springer, pp. 333–350.

19

