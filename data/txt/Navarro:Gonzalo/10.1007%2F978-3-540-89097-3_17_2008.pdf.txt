Run-Length Compressed Indexes Are Superior

for Highly Repetitive Sequence Collections

Jouni Sir´en1,(cid:2), Niko V¨alim¨aki1,(cid:2)(cid:2), Veli M¨akinen1,(cid:2)(cid:2), and Gonzalo Navarro2,(cid:2) (cid:2) (cid:2)

1 Dept. of Computer Science, Univ. of Helsinki, Finland

{jltsiren,nvalimak,vmakinen}@cs.helsinki.fi

2 Dept. of Computer Science, Univ. of Chile

gnavarro@dcc.uchile.cl

Abstract. A repetitive sequence collection is one where portions of a
base sequence of length n are repeated many times with small variations,
forming a collection of total length N . Examples of such collections are
version control data and genome sequences of individuals, where the differences 
can be expressed by lists of basic edit operations. This paper
is devoted to studying ways to store massive sets of highly repetitive
sequence collections in space-eﬃcient manner so that retrieval of the
content as well as queries on the content of the sequences can be provided 
time-eﬃciently. We show that the state-of-the-art entropy-bound
full-text self-indexes do not yet provide satisfactory space bounds for
this speciﬁc task. We engineer some new structures that use run-length
encoding and give empirical evidence that these structures are superior
to the current structures.

1 Introduction

Self-indexing [5, 9, 20, 24] is a new algorithmic approach to storing and retrieving
sequential data. The idea is to represent the text (a.k.a. sequence or string)
compressed so that random access to the content of the text is maintained, and
pattern retrieval queries on the content of the text are supported as well.

The self-indexing approach becomes especially interesting when applied to
collections of texts. A special case of a text collection is one which contains several
versions of one or more base sequences. Such collections are not uncommon. For
example, a version control system needs to store several versions of the same ﬁle
with only small edit diﬀerences between the consecutive entries. If the entries
are stored independently of each others, the version control system will end up
spending unnecessarily large amounts of memory. If the system stores only the
edits, queries on the content of one speciﬁc version becomes non-trivial.

An analogy to the storage and retrieval of version control data is soon becoming 
reality in the ﬁeld of molecular biology. Once the DNA sequencing technologies 
become faster and more cost-eﬀective, it may be that in the near future the

(cid:2) Funded by the Research Foundation of the University of Helsinki.
(cid:2)(cid:2) Funded by the Academy of Finland under grant 119815.
(cid:2) (cid:2) (cid:2) Partially funded by Millennium Institute for Cell Dynamics and Biotechnology,

Grant ICM P05-001-F, Mideplan, Chile.

A. Amir, A. Turpin, and A. Moﬀat (Eds.): SPIRE 2008, LNCS 5280, pp. 164–175, 2008.
c(cid:2) Springer-Verlag Berlin Heidelberg 2008

Run-Length Compressed Indexes Are Superior for Highly Repetitive

165

sequencing of individual genomes becomes a feasible task [3, 12, 21]. With such
data in hand, many fundamental issues such as storing and analyzing thousands
of individual genomes become a top concern. For the analysis of such collections
of biological sequences, one would need to use some variant of a generalized suﬃx
tree [11] as that provides a variety of algorithmic tools to do analysis in linear or
near-linear time. The memory requirement of such solution is unimaginable with
current random access memories and also challenging in permanent storage.

Self-indexes should, in principle, cope well with the two applications above as
both data types contain high amounts of repetitive structure. In particular, as
the main building blocks of compressed suﬃx trees [7, 22, 23, 25] they enable
compressing the collections to close to their high-order entropy and enabling
ﬂexible analysis tasks to be executed. However, there is a fundamental problem
with the fact that the high-order entropies are deﬁned by the frequencies of
symbols in their ﬁxed-length contexts. These contexts do not change at all when
more identical sequences are added to the collection. Hence, these self-indexes
are unable of exploiting the fact that the texts in the collection are highly similar.
In this paper, we propose new self-indexes based on run-length compression,
that are suitable for storing highly repetitive collections of texts. We implemented 
the new structures and compared them experimentally to existing structures.
 The experiments show that our new structures achieve superior compression 
both on DNA collections and on version control data. The superiority can
be explained in theory as well; the theoretical analysis together with related
extended results (see Sect. 7) is part of subsequent work [16].

The paper is structured as follows. Section 2 introduces the basic concepts
and goes through the related literature. Sections 3, 4, and 5 derive the new runlength 
compressed indexes. Section 6 gives the experimental results and Sect. 7
discusses the subsequent work.

2 Basic Concepts
A string S = S1,n = s1s2 ··· sn is a sequence of symbols (a.k.a. character or
letter). Each symbol is an element of a alphabet Σ = {1, 2, . . . , σ}. A substring
of S is written Si,j = sisi+1 . . . sj. A preﬁx of S is a substring of the form S1,j,
and a suﬃx is a substring of the form Si,n. If i > j then Si,j = ε, the empty
string of length |ε| = 0. A text string T = T1,n is a special string with tn = $.
The lexicographical order “<” among strings is deﬁned in the obvious way.
for which holds 0 ≤ Hk(T ) ≤ Hk−1(T ) ≤ ··· ≤ H0(T ) ≤ log σ [18].

We assume the reader is familiar with the empirical k-th order entropy Hk(T )

The compressors to be discussed are derivatives of the Burrows-Wheeler transform 
(BWT) [2]. The transform produces a permutation of T , denoted by T bwt,
as follows: (i) Build suﬃx array [17] SA[1, n] of T , that is an array of pointers
to all the suﬃxes of T in the lexicographic order; (ii) The transformed text is
T bwt = L, where L[i] = T [SA[i] − 1], taking T [0] = T [n].

The BWT is reversible, that is, given T bwt = L we can obtain T as follows:
(a) Compute the array C[1, σ] storing in C[c] the number of occurrences of

166

J. Sir´en et al.

k=1

characters {$, 1, . . . , c − 1} in the text T ; (b) Deﬁne the LF mapping as follows:
LF (i) = C[L[i]] + rankL[i](L, i), where rankc(L, i) is the number of occurrences
of character c in the preﬁx L[1, i]; (c) Reconstruct T backwards as follows: set
s = 1, for each n− 1, . . . , 1 do ti ← L[s] and s ← LF [s]. Finally, append the end
marker tn ← $. We study the following problem.
Deﬁnition 1. Given a collection C of r sequences T k ∈ C such that |T 1| = n and
(cid:2)r
|T k| = N, where T 2, T 3, . . . , T r contain overall s mutations (i.e., symbol
substitutions) from the base sequence T 1, the repetitive collection indexing problem 
is to store C in as small space as possible such that the following operations
are supported as eﬃciently as possible: count(P ) (How many times P appears
as a substring of the texts in C?); locate(P ) (List the occurrence positions of
P in C); and display(k, i, j) (Return T k
The above is an extension of the well-known basic indexing problem, where the
collection has only one sequence T . We call a data structure a self-index if it
does not need T to solve the three queries above.

i,j).

A comprehensive solution to the basic indexing problem uses the suﬃx array
SA[1, n]. Two binary searches are enough to ﬁnd the interval SA[sp, ep] such that
count and locate are immediately solved [17]. The solution is not as spaceeﬃcient 
as possible, since array SA requires n log n bits, and the solution is not
yet a self-index, since T is needed.

The FM-index [5] is a self-index based on the BWT. It solves counting queries
by ﬁnding the interval SA[sp, ep] that contains the occurrences of pattern P . The
FM-index uses the array C and function rankc(L, i) in the so-called backward
search algorithm, calling function rankc(L, i) O(m) times. The two other basic
indexing problem queries are solved e.g. using sampling of SA and its inverse
SA−1, and LF -mapping to derive the unsampled values from the sampled ones.
Many variants of the FM-index have been derived that diﬀer mainly in the way
the rankc(L, i)-queries are solved [20]. For example, on small alphabet sizes, one
can achieve nHk(1 + o(1)) space with constant time support for rankc(L, i) [6].
Now, the (repetitive) collection indexing problem can be solved using the normal 
self-index for the concatenation T 1#T 2#··· T r$, where # (cid:5)∈ Σ is a special
symbol. However, the space requirement achieved even with a high-entropy compressed 
index is not attractive for the case of repetitive collections. For example,
the solution by Ferragina et al. [6] requires N Hk(C) + o(N log σ) bits. Notice
that even with s = 0, Hk(C) ≈ Hk(T 1), and hence the space is about r times
more than what the same solution uses for the basic indexing problem.

In this paper, we derive solutions whose space requirements depend on the
number of runs in the Burrows-Wheeler transform. We will introduce some notations 
to talk about runs. A self-repetition is a maximal interval SA[i, i+l] of suﬃx
array SA having a target interval SA[j, j +l] such that SA[j +r] = SA[i+r]+1 for
all 0 ≤ r ≤ l. Let Ψ(i) = SA−1[SA[i]+1] [9, 24]. The intervals of Ψ corresponding
to a self-repetition in the suﬃx array are called runs. We have Ψ(i+1) = Ψ(i)+1
when both Ψ(i) and Ψ(i + 1) are contained in the same run.

Let RΨ (T ) be the number of runs in Ψ of text T and R(T ) = Rbwt(T ) the
number of equal letter runs in Tbwt. Both are tightly connected, RΨ and Rbwt,

Run-Length Compressed Indexes Are Superior for Highly Repetitive

167

namely RΨ ≤ Rbwt ≤ RΨ + σ [14], allowing one to use them interchangeably
under most circumstances. We will denote both with R when clear from context.
Now, it is easy to see that quantities Rbwt(T ) and Rbwt(C) are the same
when s = 0. Mutations make Rbwt(C) grow. It is possible to derive expected
case bounds on how these terms are related; these analyses are omitted here.
Instead, we introduce structures whose space depends on Rbwt(C) and study
empirically the growth of Rbwt(C) on varying s. We limit our attention to selfindexes 
providing query count(P ).

3 RLCSA: Run-Length Compressed Suﬃx Array

The Run-Length Compressed Suﬃx Array is based on the Compressed Suﬃx
Array by M¨akinen, Navarro and Sadakane [15]. We use run-length encoding
of the diﬀerences Ψ(i) − Ψ(i − 1) to store the array. Absolute Ψ(i) values are
sampled at regular intervals of the compressed array. The resulting structure
supports counting queries with backward searching.
Diﬀerential encoding of Ψ transforms a run Ψ(i)Ψ(i+1)··· Ψ(i+ l) into Ψ(i)−
Ψ(i − 1) followed by l 1s, where Ψ(i) − Ψ(i − 1) > 1. We say that the run is
trivial if l = 0. If we use run-length encoding on the 1s, we encode the trivial
runs simply as Ψ(i) − Ψ(i − 1). A nontrivial run, instead, is encoded as three
numbers, Ψ(i) − Ψ(i − 1), 1, l. That is, each time we encode a diﬀerence equal
to 1, the length of the run of 1s follows. This way, run-length compression pays
nothing for trivial runs, only for nontrivial runs where it has a potential beneﬁt.
(cid:3) the number of nontrivial runs.
The sum of all the diﬀerences Ψ(i) − Ψ(i − 1) is at most σN [15], and the total
length of the runs of 1s is N − R. Hence by using Elias delta coding to encode
the integers, we need at most

Let N be the total size of the collection and R

|Ψ| ≤

(cid:3)
R log σN
R

(cid:3)

+ R

(cid:3)

1 + log N − R
R(cid:3)

(cid:4)(cid:4)

(1 + o(1))

|Ψ|
bits for the array Ψ. By using sampling step of B bits, we need O((
B +σ) log N)
bits for the sampled Ψ(i) values, eﬀectively making the total size of RLCSA
|Ψ|(1 + ε) for any ε > 0.

To retrieve Ψ(i), we ﬁrst binary search the samples and then sum up the
diﬀerences in the corresponding part of the Ψ array until we reach position i.
This gives us count(P ) queries in O(|P|(log
|Ψ|
B + B)) time by using bacward
searching [15].

4 RLWT: Run-Length Encoded Wavelet Tree

Next we will describe a new data structure that we call Run-Length encoded
Wavelet Tree. We exploit well-known bit-vector operations: For a bit vector B
of length u, rankb(B, i) gives the number of b-bits in B[1, i] for all 1 ≤ i ≤ u

168

J. Sir´en et al.

and b ∈ {0, 1}. The inverse function selectb(B, x) gives the position of the x’th
b-bit in the bit vector B.

Wavelet tree [8] is a binary tree structure whose leaves represent the symbols
in the alphabet. The root is associated with the sequence T = T1,N. In a balanced
wavelet tree, the left (right) child of the root is a wavelet tree of the sequence T<
(T≥) obtained by concatenating all positions i having ti < σ/2 (ti ≥ σ/2). This
subdivision is represented by a bit vector of length n that marks which positions
go to the left subtree (by 0) and which go right (by 1). Recursion is continued
until the concatenated sequence contains a repeat of one symbol. One can reveal
ti, compute rankc(T, i), and selectc(T, j) with O(log σ) rank/select queries on
the bit-vectors on the path to the leaf (or back) containing c [8].

The space required by a balanced wavelet tree depends on how we encode
the bit vectors. Let R be the number of runs in a text T1,N . Let Ball be the
level-wise concatenation of all the bit vectors in the balanced wavelet tree for
the sequence T . In the worst case, each run in T equals one 0/1-bit run on each
of the log σ levels of the wavelet tree, so that the upper-bound for the number
of 0/1-bit runs in Ball is R log σ (the best case is 1 · log σ). Let b ≤ (cid:8) 1
2 R log σ(cid:9)
be the number of 1-bit runs in Ball. The RLWT data structure encodes Ball
into two separate bit vectors B1 and Brl such that the number of 1-bits in both
bit vectors is exactly b: bit vector B1 marks all the starting positions of 1-bit
runs in Ball, and bit vector Brl encodes the run-lengths of these runs in unary
coding. More precisely, B1[i] = 1 only if Ball[i] = 1 and Ball[i − 1] = 0, for all
1 < i ≤ N log σ, and B1[1] = 1 if Ball[1] = 1. Unary code for a bit run of length
j contains j − 1 zero bits concatenated with one 1-bit. The length of Brl is the
sum of the lengths of 1-bit runs in Ball, which is always at most N log σ bits.
Query rank1(Ball, i) can be solved using only the bit vectors B1 and Brl by
calculating the number of 1-bits in two closed intervals [0, j − 1] and [j, i], where
j is the starting position of the 1-bit run that precedes position i in Ball. For
the ﬁrst interval, let r be the number of 1-bit runs in Ball that start before or
at the position i, i.e. r = rank1(B1, i). From the deﬁnition of Brl follows that
rank1(Ball, j − 1) equals select1(Brl, r − 1). Now it remains to calculate the
number of 1-bits in the closed interval [j, i] of the bit vector Ball: Let k be the
length of the rth run, that is to say k ← select1(Brl, r)− rank1(Ball, j − 1). The
number of 1-bits in the closed interval is

rank1(Ball, i) − rank1(Ball, j − 1) =

(cid:5)

if i − j ≥ k,
k
i − j + 1 otherwise.

Finally, the answer to the original rank1(Ball, i) query is just the sum of the
above values rank1(Ball, j − 1) and rank1(Ball, i) − rank1(Ball, j − 1).
Gupta et al. [10] have shown that a binary searchable dictionary representation 
(BSD) of a bit-vector B of u bits containing b 1-bits, requires |gap(B)| +
O(|gap(B)|/ log b) = |gap(B)|(1 + o(1)) bits of space and supports rank queries
in tAT = AT (u, b) time, where AT (u, b) = o((log log u)2), and and select in
O(log log b) time. In the worst case, length of the gap encoded sequence |gap(B)|
is b log(u/b) + O(b log log(u/b)) bits.

Run-Length Compressed Indexes Are Superior for Highly Repetitive

169

For the bit vectors B1 and Brl, we have strict upper-bounds of u ≤ N log σ
2 R log σ(cid:9). Using the BSD, the bit vectors can be represented in at
bits. All the wavelet tree

and b ≤ (cid:8) 1
most R log σ log 2N
queries can be supported without storing the bit vector Ball itself.

(cid:6)
R log σ log log 2N
R

Using the RLWT structure with backward searching [5], we can count the
number of occurrences of a pattern of length m in O(m log(σtAT)) time. Table
C adds σ log N bits to the space requirement.

R (1 + o(1)) + O

(cid:7)

5 RLFM+: Improved Run-Length FM-Index

The RLWT structure can be improved in the case the input text is T bwt: The
Run-Length FM-Index (RLFM) of [14] uses a reduction such that the equal
letter runs of T bwt are marked into two bit-vectors, and the sequence of run
heads of length R is encoded using a normal wavelet tree. We can represent
the two bit-vectors using BSD, giving immediately the following result: The
RLFM data structure for the sequence T bwt takes (R log σ+2R log N
R )(1+o(1))+
(cid:6)
R log log N
+ σ log N bits of space.The structure supports count(P ) in time
O
O(|P|(log(σ) + tAT)).
R

(cid:7)

6 Experimental Results

We implemented the three proposed structures RLCSA, RLWT, and RLFM+,
each supporting count()-queries. Standard strategies to support display() and
locate() are trivial to add. (Almost all space/time tradeoﬀs are possible for
those queries, so the base structure for supporting count() is the crucial one.)
For comparison, we selected several well-engineered implementations of selfindexes 
from the Pizza&Chili site1. Unless otherwise noted, we used no extra
space for display() and locate(), and left the default options for the rest. We
also compared our indexes to several compressors and a version control system.
We performed experiments on two data sets. The synthetic DNA sequence
collections were based on the DNA sequences from Pizza&Chili. We took a 1,
4 or 16 MB preﬁx and repeated it 25, 50 or 100 times. Each character in the
repetitions was individually mutated into another character in {A, C, G, T} with
ten diﬀerent probabilities ranging from 0 to 0.05. This was intended to simulate
the case of one base sequence and r − 1 mutated sequences.

Our other data set is based on the source code for portable versions of
OpenSSH2. We used a 4.44 MB tar archive containing the source code for version 
4.7p1, as well as on another 176.55 MB archive containing the source code
for all 75 versions up to version 4.7p1. The latter contained multiple copies of
the same ﬁles as well as many highly similar ﬁles, making it highly compressible.
The experiments were performed on a 3 GHz Intel Pentium 4 Northwood

machine with 3 GB RAM running Fedora Core 7 based Linux.

1

2

http://pizzachili.dcc.uchile.cl/ or http://pizzachili.di.unipi.it/.
http://www.openssh.com/

170

J. Sir´en et al.

6.1 Implementations and Parameters

The implementations of Succinct Suﬃx Array (SSA, version 2) [6, 14], RunLength 
FM-index (RLFM) [14], Alphabet-Friendly FM-index (AFFM, version
2) [6] were taken from the Pizza&Chili site. All of them use a Huﬀman-shaped
wavelet tree to achieve compression. SSA achieves zero-order compression by
building the wavelet tree directly on the BWT, and is the fastest. RLFM builds
it on the run heads of the BWT, and thus its space is related to the number of
runs in the collection, yet the two extra bit-vectors it uses are not compressed.
AFFM achieves high-order compression, N Hk + o(N log σ) bits, by partitioning
the BWT into suitable chunks and building a wavelet tree per chunk. Its space
is not related to the runs in the BWT.

Sadakane’s Compressed Suﬃx Array (CSA) [24] implementation was also
taken from Pizza&Chili. It achieves high-order compression related to the runs
in Ψ, yet also includes less compressed bit vectors. We used sample rates 128
(default; CSA-128) and 1024 (CSA-1024 or CSA) for the Ψ values. The total
size of the samples for a 400 MB collection is 3.1 MB for CSA-1024 and 25 MB
for CSA-128. Suﬃx array sample rate was set to 65536 to make the size of these
unused samples negligible (not to confuse with the sampling to access Ψ).

Also included in the comparison was a self-index based on Lempel-Ziv parsing
(LZ-index, Pizza&Chili version 4) [1]. We selected 1/ = 15 as a reasonable
space/time tradeoﬀ and subtracted the space (41 MB for a 400 MB collection)
used for display() and locate() queries, for fairness with the other structures
(although the implementation does not let one discard it).

Our indexes RLCSA, RLWT, and RLFM+ can be seen as versions of CSA,
SSA, and RLFM, respectively, enhanced to proﬁt from highly repetitive collections.
 The implementation of RLCSA is optimized for secondary memory. Hence
we have used 32 kilobyte sampling step for Ψ (RLCSA-32k or RLCSA) in addition 
to the more reasonable 128 bytes (RLCSA-128). In practice, RLCSA-128 is
at most 20% larger than RLCSA-32k. The diﬀerence can be reduced by changing
the size of the samples from 24 bytes to 3(cid:8)log N(cid:9) bits per sample. In RLWT and
RLFM+, we used simpler encoding for the bit vectors than the original BSD.
The implemented structure solves rank in O(log b) time.

In addition to the existing self-indexes, we compared our new indexes to several 
plain compressors. The well-known gzip and bzip2 compressors were used
with parameter -9 to achieve maximum compression. Due to their small block
sizes, they cannot proﬁt from the large-scale repetitiveness in our data sets. We
have also used the highly eﬃcient LZ77-based compressor p7zip3 with options
-mx=9 -md=30 to see how much we pay for the retrieval functionality. With a
window of length up to 1 GB, p7zip can compress texts with long repeats much
better than standard Lempel-Ziv based compressors.

Finally we have used the Subversion (SVN)4 version control system for the
OpenSSH source code data set. The source codes were inserted into a repository

3

4

http://p7zip.sourceforge.net/
http://subversion.tigris.org/

Run-Length Compressed Indexes Are Superior for Highly Repetitive

171

25x1 MB
25x4 MB
25x16 MB
50x1 MB
100x1 MB

)

M

(
 
s
n
u
R

0
0
2

0
5
1

0
0
1

0
5

0

s
n
u
R

1
1

0
1

9

8

7

6

0.00

0.01

0.02

0.03

0.04

0.05

0.00

0.01

0.02

0.03

0.04

0.05

Mutation rate

Mutation rate

Fig. 1. The number of runs in Ψ (left) and the average number of new runs per mutation
(right) on repeated DNA sequences

25x1 MB
25x4 MB
25x16 MB
50x1 MB
100x1 MB

0
0
2

0
5
1

B
M

0
0
1

0
5

0

0
4

0
3

s
t
i

B

0
2

0
1

0

p7zip
RLCSA
RLWT
RLFM+

0.00

0.01

0.02

0.03

0.04

0.05

0.00

0.01

0.02

0.03

0.04

0.05

Mutation rate

Mutation rate

Fig. 2. The size of RLCSA on repeated DNA sequences (left) and the average number
of bits required to encode a run on 25x16 MB DNA (right)

using FSFS ﬁle system in a chronological order one version at a time. We measured 
the sizes of subdirectory db/ of the repository, using utility du.

6.2 Results

Fig. 1 shows the number of runs in Ψ of repeated DNA sequences. The number of
runs grows somewhat sublinearly in the number of mutations. New runs are created 
when mutations move suﬃxes to new positions in the suﬃx array. However,
as the mutations accumulate, it becomes more likely that a similar mutation has
already happened before, reducing the number of new runs created.

Fig. 2 shows the sensitivity of the sizes of our new self-indexes to the number
of runs in the collection. RLCSA is clearly smaller than the two other indexes.
It is interesting to see that with high mutation rates, p7zip requires only about

172

J. Sir´en et al.

B
M

0
5
3

0
0
3

0
5
2

0
0
2

0
5
1

0
0
1

0
5

0

CSA
LZI
RLFM
SSA

AFFM
RLCSA
RLWT
RLFM+

0.00

0.01

0.02

0.03

0.04

0.05

Mutation rate

Fig. 3. A comparison of our indexes with existing self-indexes. The peak of LZI at
0.003 is an artifact of the implementation.

1.2 bits per run, suggesting some connection between the number of runs in Ψ
and the space requirements of Lempel-Ziv compression (see Sect. 7).

We select the 25 times repeated 16 MB DNA preﬁx for comparisons between
new and existing self-indexes. As Fig. 3 shows, our indexes clearly outperform the
existing self-indexes when the number of mutations is small. In particular, it can
be seen that our indexes are the most sensitive to high repetitiveness, followed by
CSA and RLFM, and then LZ-index. SSA and AFFM are completely insensitive.
As predicted by the theoretical space bounds, RLCSA outperforms RLWT.
Surprisingly, RLWT outperforms RLFM+. This is explained by the fact that
RLFM+ always uses two bit-vectors with R bits set, and a separate wavelet
tree taking close to R log σ bits (or slightly less in practice due to the Huﬀman
shape). RLWT instead uses a wavelet tree formed by log σ levels of bit vectors
each with at most R bits set. This worst case does not happen in practice. On
random text the expected number of bits set is σ/2
σ−1 R log σ, and this decreases on
non-random text due to the BWT eﬀect. For example on DNA (log σ = 2) there
are only 1.25R bits set in RLWT. Assuming a δ-encoding of the run lengths, we
get a pretty good approximation of 14.34 bits for RLWT, and 18.58 for RLFM+.
The size diﬀerence between RLCSA and RLFM+ is also surprising, given the
similar high-order terms in the space bounds. This is partially explained by the
/R decreasing from 0.80 at mutation rate
ratio of non-trivial runs to total runs R
0.001 to 0.37 at 0.05. Additionally, the size bound for RLFM+ has a signiﬁcant
low-order term. Also note the size diﬀerence of the similar CSA and RLFM.

(cid:3)

Run-Length Compressed Indexes Are Superior for Highly Repetitive

173

B
M

0
0
2

0
5
1

0
0
1

0
5

0

5
5
.
6
7
1

6
1
.
1
4

8
2
.
2
3

4
4
4

.

4
9
0

.

6
7
0

.

5
6
0

.

7
9
1

.

4
3
.
0
1

5
5
1

.

8
7
.
6
2

9
3
1

.

Version 4.7p1
All 75 versions

2
1
.
7
4
1

6
0
.
8
2
1

8
7
.
6
8

6
8
.
2
6

1
3

.

6

1
4
2

.

7
2
3

.

2
3
3

.

2
3
5

.

1
1
1

.

4
0

.

7

9
0
2

.

9
5

.

9

7
4
2

.

original

gzip

bzip2

p7zip

SVN

CSA

LZI

RLFM

SSA

AFFM RLCSA RLWT RLFM+

Fig. 4. Compression results for OpenSSH sources

Table 1. Time for counting on the diﬀerent indexes. We remind that the LZ-index is
not designed for counting.

Structure
CSA-128
CSA-1024
LZ-index
RLFM
SSA
AFFM

Time (μs)
103.0
347.0
198596.8
29.5
13.0
19.4

Size (MB)
112.29
90.41
281.92
156.50
116.37
124.15

Structure
RLCSA-128
RLCSA-32k
RLWT
RLFM+

Time (μs)
72.7
11130.0
1050.0
189.7

Size (MB)
77.54
65.52
89.30
124.48

Next we compare our indexes with existing self-indexes as well as plain compressors 
on OpenSSH sources. As seen in Fig. 4, our indexes clearly outperform 
the existing self-indexes. Again RLWT outperforms RLFM+ even with
this larger alphabet size, indicating that the average RLWT space requirement
is better than the worst case (see also [4] for a more rigorous analysis of runs in
wavelet tree). It is interesting to note that despite the search functionality, our
indexes remain smaller than the SVN repository.

The increased space eﬃciency of our indexes has been paid in time eﬃciency.
To test this, we extract 1000 random substrings of length 10 from the 16 MB
DNA preﬁx. We then repeat the preﬁx 25 times with mutation rate 0.01 and
measure counting query times. Table 1 gives average query times and structure
sizes, showing the competitiveness of RLCSA-128.

7 Discussion

In this study, we have mainly considered self-indexes based on the BurrowsWheeler 
framework. There is also a family of (self-)indexes which is based on
the Lempel-Ziv parsing, see [13, 19, 20]. It is easy to see that the LZ77 parsing
of a repetitive text collection consists of at most of P (T 1) + s + 1 phrases, where
P (T 1) gives the number of phrases in T 1. It follows that LZ77 based indexes

174

J. Sir´en et al.

require at most O(n log σ + s log n) bits of space. However, there does not exist
a LZ77 based self-index, as they require the uncompressed text to operate. All
the Lempel-Ziv self-indexes (like the one experimented here) are based on the
LZ78 parsing, which does not guarantee equally good performance. Hence, a
promising future direction is to develop LZ77 based self-indexes.

Our experiments considered only point mutations on DNA, although there
are many other types of mutations, like insertions, deletions, translocations, and
reversals. The runs in the Burrows-Wheeler transform change only for those sufﬁxes 
whose lexicographic order is aﬀected by a mutation. In all mutation types
(except in reversals5) the eﬀect is identical to point mutations, so the compression 
result should be similar. We emphasize that the proposed indexes are
completely universal, as they do not need to know what and where the mutations
are. This is also illustrated by the experiment on version control data, where the
changes are cumulative, and there is no base sequence, but rather a “founder
sequence”. The founder model also characterizes genome collections, but again
the index does not need to know the phylogeny to succeed in compression.

In subsequent (still theoretical) work [16], we have derived dynamic versions of
all the proposed self-indexes, where sequences can be deleted from and inserted
to the collection at any time. These indexes take basically the same space as the
static ones discussed here. We have also considered new structures for display
and locate, where the number of suﬃx array samples depend on s as well. One
can use both the static and the dynamic versions of these indexes as building
blocks of recent compressed suﬃx trees [7, 22, 23].

References

1. Arroyuelo, D., Navarro, G., Sadakane, K.: Reducing the space requirement of LZindex.
 In: Lewenstein, M., Valiente, G. (eds.) CPM 2006. LNCS, vol. 4009, pp.
318–329. Springer, Heidelberg (2006)

2. Burrows, M., Wheeler, D.: A block sorting lossless data compression algorithm.

Technical Report Technical Report 124, Digital Equipment Corporation (1994)

3. Church, G.M.: Genomes for all. Scientiﬁc American 294(1), 47–54 (2006)
4. Ferragina, P., Giancarlo, R., Manzini, G.: The myriad virtues of wavelet trees.
In: Bugliesi, M., Preneel, B., Sassone, V., Wegener, I. (eds.) ICALP 2006. LNCS,
vol. 4051, pp. 560–571. Springer, Heidelberg (2006)

5. Ferragina, P., Manzini, G.: Indexing compressed texts. J. of the ACM 52(4), 552–

581 (2005)

6. Ferragina, P., Manzini, G., M¨akinen, V., Navarro, G.: Compressed representations

of sequences and full-text indexes. ACM TALG 3(2) article 20 (2007)

7. Fischer, J., M¨akinen, V., Navarro, G.: An(other) entropy-bounded compressed sufﬁx 
tree. In: Ferragina, P., Landau, G.M. (eds.) CPM 2008. LNCS, vol. 5029, pp.
152–165. Springer, Heidelberg (2008)

8. Grossi, R., Gupta, A., Vitter, J.: High-order entropy-compressed text indexes. In:

Proc. 14th SODA, pp. 841–850 (2003)

9. Grossi, R., Vitter, J.: Compressed suﬃx arrays and suﬃx trees with applications to
text indexing and string matching. SIAM J. on Computing 35(2), 378–407 (2006)

5 Adding the reverse complement of the base sequence to the collection solves this.

Run-Length Compressed Indexes Are Superior for Highly Repetitive

175

10. Gupta, A., Hon, W.-K., Shah, R., Vitter, J.S.: Compressed data structures: Dictionaries 
and data-aware measures. In: Proc. 16th DCC, pp. 213–222 (2006)

11. Gusﬁeld, D.: Algorithms on Strings, Trees and Sequences: Computer Science and

Computational Biology. Cambridge University Press, Cambridge (1997)

12. Hall, N.: Advanced sequencing technologies and their wider impact in microbiology.

The Journal of Experimental Biology 209, 1518–1525 (2007)

13. K¨arkk¨ainen, J.: Repetition-based text indexes. Technical Report A-1999-4, Department 
of Computer Science, University of Helsinki, Finland (1999)

14. M¨akinen, V., Navarro, G.: Succinct suﬃx arrays based on run-length encoding.

Nordic Journal of Computing 12(1), 40–66 (2005)

15. M¨akinen, V., Navarro, G., Sadakane, K.: Advantages of backward searching —
eﬃcient secondary memory and distributed implementation of compressed suﬃx
arrays. In: Fleischer, R., Trippen, G. (eds.) ISAAC 2004, vol. 3341, pp. 681–692.
Springer, Heidelberg (2004)

16. M¨akinen, V., Navarro, G., Sir´en, J., V¨alim¨aki, N.: Run-length compressed indexes
for repetitive sequence collections. Technical Report C-2008-42, Department of
Computer Science, University of Helsinki, Finland (2008)

17. Manber, U., Myers, G.: Suﬃx arrays: a new method for on-line string searches.

SIAM J. on Computing 22(5), 935–948 (1993)

18. Manzini, G.: An analysis of the Burrows-Wheeler transform. J. of the ACM 48(3),

407–430 (2001)

19. Navarro, G.: Indexing text using the ziv-lempel trie. J. of Discrete Algorithms

(JDA) 2(1), 87–114 (2004)

20. Navarro, G., M¨akinen, V.: Compressed full-text indexes. ACM Computing Surveys 
39(1) article 2 (2007)

21. Pennisi, E.: Breakthrough of the year: Human genetic variation. Science 21, 1842–

1843 (2007)

22. Russo, L., Navarro, G., Oliveira, A.: Dynamic fully-compressed suﬃx trees. In:
Ferragina, P., Landau, G.M. (eds.) CPM 2008. LNCS, vol. 5029, pp. 191–203.
Springer, Heidelberg (2008)

23. Russo, L., Navarro, G., Oliveira, A.: Fully-compressed suﬃx trees. In: Laber, E.S.,
Bornstein, C., Nogueira, L.T., Faria, L. (eds.) LATIN 2008. LNCS, vol. 4957, pp.
362–373. Springer, Heidelberg (2008)

24. Sadakane, K.: New text indexing functionalities of the compressed suﬃx arrays. J.

of Algorithms 48(2), 294–313 (2003)

25. Sadakane, K.: Compressed suﬃx trees with full functionality. Theory of Computing

Systems 41(4), 589–607 (2007)

