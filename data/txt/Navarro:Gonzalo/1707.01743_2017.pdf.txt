Fast Compressed Self-Indexes with Deterministic
Linear-Time Construction∗
J. Ian Munro1, Gonzalo Navarro2, and Yakov Nekrich1

1 Cheriton School of Computer Science, University of Waterloo.

imunro@uwaterloo.ca, yakov.nekrich@googlemail.com.

2 CeBiB — Center of Biotechnology and Bioengineering, Department of

Computer Science, University of Chile. gnavarro@dcc.uchile.cl.

7
1
0
2

 

p
e
S
1

 

 
 
]
S
D
.
s
c
[
 
 

3
v
3
4
7
1
0

.

7
0
7
1
:
v
i
X
r
a

Abstract

We introduce a compressed suﬃx array representation that, on a text T of length n over an
alphabet of size σ, can be built in O(n) deterministic time, within O(n log σ) bits of working
space, and counts the number of occurrences of any pattern P in T in time O(|P|+log logw σ) on
a RAM machine of w = Ω(log n)-bit words. This new index outperforms all the other compressed
indexes that can be built in linear deterministic time, and some others. The only faster indexes
can be built in linear time only in expectation, or require Θ(n log n) bits. We also show that, by
using O(n log σ) bits, we can build in linear time an index that counts in time O(|P|/ logσ n +
log n(log log n)2), which is RAM-optimal for w = Θ(log n) and suﬃciently long patterns.

1998 ACM Subject Classiﬁcation E.1 Data Structures; E.4 Coding and Information Theory

Keywords and phrases Succinct data structures; Self-indexes; Suﬃx arrays; Deterministic construction


Digital Object Identiﬁer 10.4230/LIPIcs...

Introduction

1
The string indexing problem consists in preprocessing a string T so that, later, we can
eﬃciently ﬁnd occurrences of patterns P in T. The most popular solutions to this problem
are suﬃx trees [35] and suﬃx arrays [24]. Both can be built in O(n) deterministic time
on a text T of length n over an alphabet of size σ, and the best variants can count the
number of times a string P appears in T in time O(|P|), and even in time O(|P|/ logσ n)
in the word-RAM model if P is given packed into |P|/ logσ n words [31]. Once counted,
each occurrence can be located in O(1) time. Those optimal times, however, come with two
important drawbacks:

The variants with this counting time cannot be built in O(n) worst-case time.
The data structures use Θ(n log n) bits of space.
The reason of the ﬁrst drawback is that some form of perfect hashing is always used to
ensure constant time per pattern symbol (or pack of symbols). The classical suﬃx trees
and arrays with linear-time deterministic construction oﬀer O(|P| log σ) or O(|P| + log n)
counting time, respectively. More recently, those times have been reduced to O(|P| + log σ)
[10] and even to O(|P| + log log σ) [15]. Simultaneously with our work, a suﬃx tree variant
was introduced by Bille et al. [7], which can be built in linear deterministic time and counts
in time O(|P|/ logσ n + log |P| + log log σ). All those indexes, however, still suﬀer from the

∗ Funded with Basal Funds FB0001, Conicyt, Chile.

© J. Ian Munro, Gonzalo Navarro, Yakov Nekrich;
licensed under Creative Commons License CC-BY

Leibniz International Proceedings in Informatics
Schloss Dagstuhl – Leibniz-Zentrum für Informatik, Dagstuhl Publishing, Germany

second drawback, that is, they use Θ(n log n) bits of space. This makes them impractical in
most applications that handle large text collections.
Research on the second drawback dates back to almost two decades [30], and has led
to indexes using nHk(T) + o(n(Hk(T) + 1)) bits, where Hk(T) ≤ log σ is the k-th order
entropy of T [25], for any k ≤ α logσ n − 1 and any constant 0 < α < 1. That is, the
indexes use asymptotically the same space of the compressed text, and can reproduce the
text and search it; thus they are called self-indexes. The fastest compressed self-indexes
that can be built in linear deterministic time are able to count in time O(|P| log log σ) [1] or
O(|P|(1 + logw σ)) [6]. There exist other compressed self-indexes that obtain times O(|P|) [5]
or O(|P|/ logσ n + log
σ n) for any constant  > 0 [19], but both rely on perfect hashing and
are not built in linear deterministic time. All those compressed self-indexes use O(n log n
b )
further bits to locate the position of each occurrence found in time O(b), and to extract any
substring S of T in time O(|S| + b).

In this paper we introduce the ﬁrst compressed self-index that can be built in O(n)
deterministic time (moreover, using O(n log σ) bits of space [28]) and with counting time
O(|P| + log logw σ), where w = Ω(log n) is the size in bits of the computer word. More
precisely, we prove the following result.
(cid:73) Theorem 1. On a RAM machine of w = Ω(log n) bits, we can construct an index for
a text T of length n over an alphabet of size σ = O(n/ log n) in O(n) deterministic time
using O(n log σ) bits of working space. This index occupies nHk(T) + o(n log σ) + O(n log n
b )
bits of space for a parameter b and any k ≤ α logσ n − 1, for any constant 0 < α < 1. The
occurrences of a pattern string P can be counted in O(|P| + log logw σ) time, and then each
such occurrence can be located in O(b) time. An arbitrary substring S of T can be extracted
in time O(|S| + b).

We obtain our results with a combination of the compressed suﬃx tree T of T and the
Burrows-Wheeler transform B of the reversed text T. We manage to simulate the suﬃx tree
traversal for P, simultaneously on T and on B. With a combination of storing deterministic
dictionaries and precomputed rank values for sampled nodes of T , and a constant-time
method to compute an extension of partial rank queries that considers small ranges in B,
we manage to ensure that all the suﬃx tree steps, except one, require constant time. The
remaining one is solved with general rank queries in time O(log logw σ). As a byproduct, we
show that the compressed sequence representations that obtain those rank times [6] can also
be built in linear deterministic time.

Compared with previous work, other indexes may be faster at counting, but either they
are not built in linear deterministic time [5, 19, 31] or they are not compressed [31, 7]. Our
index outperforms all the previous compressed [13, 1, 6], as well as some uncompressed [15],
indexes that can be built deterministically.
As an application of our tools, we also show that an index using O(n log σ) bits of
space can be built in linear deterministic time, so that it can count in time O(|P|/ logσ n +
log n(log log n)2), which is RAM-optimal for w = Θ(log n) and suﬃciently long patterns.
Current indexes obtaining similar counting time require O(n log σ) construction time [19] or
higher [31], or O(n log n) bits of space [31, 7].

2

Related Work

Let T be a string of length n over an alphabet of size σ that is indexed to support searches
for patterns P. It is generally assumed that σ = o(n), a reasonable convention we will follow.

© J. Ian Munro, Gonzalo Navarro, Yakov Nekrich;
licensed under Creative Commons License CC-BY

Leibniz International Proceedings in Informatics
Schloss Dagstuhl – Leibniz-Zentrum für Informatik, Dagstuhl Publishing, Germany

Compressed

Compact

Uncompressed

|P| log log σ [1]
|P|(1 + logw σ) [6]
|P|+log logw σ (ours)

|P|/ log n + log n [19]
(constant σ)
|P|/ logσ n+log n(log log n)2
(ours)

|P| + log log σ [15]
|P|/ logσ n+log |P|+log log σ [7]

|P|(1 + log logw σ) [6]
|P| [5]

|P|/ logσ n + log

σ n [19, 31]

c
i
t
s
i

i

n
m
r
e
t
e
D

d
e
z
i
m
o
d
n
a
R

Table 1 Our results in context. The x axis refers to the space used by the indexes (compressed
meaning nHk(T ) + o(n log σ) bits, compact meaning O(n log σ) bits, and uncompressed meaning
Θ(n log n) bits), and the y axis refers to the linear-time construction. In the cells we show the
counting time for a pattern P . We only list the dominant alternatives, graying out those outperformed
by our new results.

Searches typically require to count the number of times P appears in T, and then locate the
positions of T where P occurs. The vast majority of the indexes for this task are suﬃx tree
[35] or suﬃx array [24] variants.

The suﬃx tree can be built in linear deterministic time [35, 26, 34], even on arbitrarily
large integer alphabets [11]. The suﬃx array can be easily derived from the suﬃx tree in
linear time, but it can also be built independently in linear deterministic time [23, 22, 21].
In their basic forms, these structures allow counting the number of occurrences of a pattern
P in T in time O(|P| log σ) (suﬃx tree) or O(|P| + log n) (suﬃx array). Once counted, the
occurrences can be located in constant time each.

Cole et al. [10] introduced the suﬃx trays, a simple twist on suﬃx trees that reduces their
counting time to O(|P| + log σ). Fischer and Gawrychowski [15] introduced the wexponential
search trees, which yield suﬃx trees with counting time O(|P| + log log σ) and support
dynamism.

All these structures can be built in linear deterministic time, but require Θ(n log n) bits

of space, which challenges their practicality when handling large text collections.

Faster counting is possible if we resort to perfect hashing and give away the linear
deterministic construction time. In the classical suﬃx tree, we can easily achieve O(|P|) time
by hashing the children of suﬃx tree nodes, and this is optimal in general. In the RAM model
with word size Θ(log n), and if the consecutive symbols of P come packed into |P|/ logσ n
words, the optimal time is instead O(|P|/ logσ n). This optimal time was recently reached by
Navarro and Nekrich [31] (note that their time is not optimal if w = ω(log n)), with a simple
application of weak-preﬁx search, already hinted in the original article [2]. However, even
the randomized construction time of the weak-preﬁx search structure is O(n log n), for any
constant  > 0. By replacing the weak-preﬁx search with the solution of Grossi and Vitter
[19] for the last nodes of the search, and using a randomized construction of their perfect
hash functions, the index of Navarro and Nekrich [31] can be built in linear randomized time
and count in time O(|P|/ logσ n + log
σ n). Only recently, simultaneously with our work, a
deterministic linear-time construction algorithm was ﬁnally obtained for an index obtaining
O(|P|/ logσ n + log |P| + log log σ) counting time [7].

Still, these structures are not compressed. Compressed suﬃx trees and arrays appeared in

© J. Ian Munro, Gonzalo Navarro, Yakov Nekrich;
licensed under Creative Commons License CC-BY

Leibniz International Proceedings in Informatics
Schloss Dagstuhl – Leibniz-Zentrum für Informatik, Dagstuhl Publishing, Germany

the year 2000 [30]. To date, they take the space of the compressed text and replace it, in the
sense that they can extract any desired substring of T; they are thus called self-indexes. The
space occupied is measured in terms of the k-th order empirical entropy of T, Hk(T) ≤ log σ
[25], which is a lower bound on the space reached by any statistical compressor that encodes
each symbol considering only the k previous ones. Self-indexes may occupy as little as
nHk(T) + o(n(Hk(T) + 1)) bits, for any k ≤ α logσ n − 1, for any constant 0 < α < 1.
The fastest self-indexes with linear-time deterministic construction are those of Barbay
et al. [1], which counts in time O(|P| log log σ), and Belazzougui and Navarro [6, Thm. 7],
which counts in time O(|P|(1 + logw σ)). The latter requires O(n(1 + logw σ)) construction
time, but if log σ = O(log w), its counting time is O(|P|) and its construction time is O(n).
If we admit randomized linear-time constructions, then Belazzougui and Navarro [6,
Thm. 10] reach O(|P|(1 + log logw σ)) counting time. At the expense of O(n) further bits,
in another work [5] they reach O(|P|) counting time. Using O(n log σ) bits, and if P comes
in packed form, Grossi and Vitter [19] can count in time O(|P|/ logσ n + log
σ n), for any
constant  > 0, however their construction requires O(n log σ) time.
Table 1 puts those results and our contribution in context. Our new self-index, with
O(|P| + log logw σ) counting time, linear-time deterministic construction, and nHk(T) +
o(n log σ) bits of space, dominates all the compressed indexes with linear-time deterministic
construction [1, 6], as well as some uncompressed ones [15] (to be fair, we do not cover the
case log σ = O(log w), as in this case the previous work [6, Thm. 7] already obtains our result).
Our self-index also dominates a previous one with linear-time randomized construction [6,
Thm. 10], which we incidentally show can also be built deterministically. The only aspect
in which some of those dominated indexes may outperform ours is in that they may use
o(n(Hk(T) + 1)) [6, Thm. 10] or o(n) [6, Thm. 7] bits of redundancy, instead of our o(n log σ)
bits. We also derive a compact index (i.e., using O(n log σ) bits) that is built in linear
deterministic time and counts in time O(|P|/ logσ n + log n(log log n)2), which is the only
one in this category unless we consider constant σ for Grossi and Vitter [19].

Preliminaries

3
We denote by T[i..] the suﬃx of T[0, n − 1] starting at position i and by T[i..j] the substring
that begins with T[i] and ends with T[j], T[i..] = T[i]T[i + 1] . . . T[n − 1] and T[i..j] =
T[i]T[i + 1] . . . T[j − 1]T[j]. We assume that the text T ends with a special symbol $ that
lexicographically precedes all other symbols in T. The alphabet size is σ and symbols
are integers in [0..σ − 1] (so $ corresponds to 0). In this paper, as in the previous work
on this topic, we use the word RAM model of computation. A machine word consists of
w = Ω(log n) bits and we can execute standard bit and arithmetic operations in constant
time. We assume for simplicity that the alphabet size σ = O(n/ log n) (otherwise the text is
almost incompressible anyway [16]). We also assume log σ = ω(log w), since otherwise our
goal is already reached in previous work [6, Thm. 7].

3.1 Rank and Select Queries
We deﬁne three basic queries on sequences. Let B[0..n − 1] be a sequence of symbols over
alphabet [0..σ − 1]. The rank query, ranka(i, B), counts how many times a occurs among the
ﬁrst i + 1 symbols in B, ranka(i, B) = |{ j ≤ i, B[j] = a}|. The select query, selecta(i, B),
ﬁnds the position in B where a occurs for the i-th time, selecta(i, B) = j iﬀ B[j] = a and
ranka(j, B) = i. The third query is access(i, B), which returns simply B[i].

© J. Ian Munro, Gonzalo Navarro, Yakov Nekrich;
licensed under Creative Commons License CC-BY

Leibniz International Proceedings in Informatics
Schloss Dagstuhl – Leibniz-Zentrum für Informatik, Dagstuhl Publishing, Germany

We can answer access queries in O(1) time and select queries in any ω(1) time, or vice
versa, and rank queries in time O(log logw σ), which is optimal [6]. These structures use
n log σ + o(n log σ) bits, and we will use variants that require only compressed space. In this
paper, we will show that those structures can be built in linear deterministic time.

An important special case of rank queries is the partial rank query, rankB[i](i, B), which
asks how many times B[i] occurrs in B[0..i]. Unlike general rank queries, partial rank queries
can be answered in O(1) time [6]. Such a structure can be built in O(n) deterministic time
and requires O(n log log σ) bits of working and ﬁnal space [28, Thm. A.4.1].
For this paper, we deﬁne a generalization of partial rank queries called interval rank
queries, ranka(i, j, B) = hranka(i − 1, B), ranka(j, B)i, from where in particular we can
deduce the number of times a occurs in B[i..j]. If a does not occur in B[i..j], however, this
query just returns null (this is why it can be regarded as a generalized partial rank query).
In the special case where the alphabet size is small, log σ = O(log w), we can represent B
so that rank, select, and access queries are answered in O(1) time [6, Thm. 7], but we are
not focusing on this case in this paper, as the problem has already been solved for this case.

3.2 Suﬃx Array and Suﬃx Tree
The suﬃx tree [35] for a string T[0..n − 1] is a compacted digital tree on the suﬃxes of T,
where the leaves point to the starting positions of the suﬃxes. We call Xu the string leading
to suﬃx tree node u. The suﬃx array [24] is an array SA[0..n − 1] such that SA[i] = j if and
only if T[j..] is the (i + 1)-th lexicographically smallest suﬃx of T. All the occurrences of a
substring P in T correspond to suﬃxes of T that start with P. These suﬃxes descend from
a single suﬃx tree node, called the locus of P, and also occupy a contiguous interval in the
suﬃx array SA. Note that the locus of P is the node u closest to the root for which P is a
preﬁx of Xu. If P has no locus node, then it does not occur in T.

3.3 Compressed Suﬃx Array and Tree
A compressed suﬃx array (CSA) is a compact data structure that provides the same
functionality as the suﬃx array. The main component of a CSA is the one that allows
determining, given a pattern P, the suﬃx array range SA[i..j] of the preﬁxes starting with
P. Counting is then solved as j − i + 1. For locating any cell SA[k], and for extracting any
substring S from T, most CSAs make use of a sampled array SAMb, which contains the
values of SA[i] such that SA[i] mod b = 0 or SA[i] = n − 1. Here b is a tradeoﬀ parameter:
CSAs require O(n log n
b ) further bits and can locate in time proportional to b and extract S
in time proportional to b + |S|. We refer to a survey [30] for a more detailed description.

A compressed suﬃx tree [33] is formed by a compressed suﬃx array and other components
that add up to O(n) bits. These include in particular a representation of the tree topology
that supports constant-time computation of the preorder of a node, its number of children,
its j-th child, its number of descendant leaves, and lowest common ancestors, among others
[32]. Computing node preorders is useful to associate satellite information to the nodes.

Both the compressed suﬃx array and tree can be built in O(n) deterministic time using

O(n log σ) bits of space [28].

3.4 Burrows-Wheeler Transform and FM-index
The Burrows-Wheeler Transform (BWT) [8] of a string T[0..n−1] is another string B[0..n−1]
obtained by sorting all possible rotations of T and writing the last symbol of every rotation

© J. Ian Munro, Gonzalo Navarro, Yakov Nekrich;
licensed under Creative Commons License CC-BY

Leibniz International Proceedings in Informatics
Schloss Dagstuhl – Leibniz-Zentrum für Informatik, Dagstuhl Publishing, Germany

(in sorted order). The BWT is related to the suﬃx array by the identity B[i] = T[(SA[i] − 1)
mod n]. Hence, we can build the BWT by sorting the suﬃxes and writing the symbols that
precede the suﬃxes in lexicographical order.

The FM-index [12, 13] is a CSA that builds on the BWT. It consists of the following

three main components:

The BWT B of T.
The array Acc[0..σ − 1] where Acc[i] holds the total number of symbols a < i in T (or
equivalently, the total number of symbols a < i in B).
The sampled array SAMb.
The interval of a pattern string P[0..m − 1] in the suﬃx array SA can be computed on
the BWT B. The interval is computed backwards: for i = m − 1, m − 2, . . ., we identify
the interval of P[i..m − 1] in B. The interval is initially the whole B[0..n − 1]. Suppose
that we know the interval B[i1..j1] that corresponds to P[i + 1..m − 1]. Then the interval
B[i2..j2] that corresponds to P[i..m − 1] is computed as i2 = Acc[a] + rankc(i1 − 1, B) and
j2 = Acc[a] + rankc(j1, B) − 1, where a = P[i]. Thus the interval of P is found by answering
2m rank queries. Any sequence representation oﬀering rank and access queries can then be
applied on B to obtain an FM-index.

An important procedure on the FM-index is the computation of the function LF, deﬁned
as follows:
if SA[j] = i + 1, then SA[LF(j)] = i. LF can be computed with access and
partial rank queries on B, LF(j) = rankB[j](i, B) + Acc[B[j]] − 1, and thus constant-time
computation of LF is possible. Using SAMb and O(b) applications of LF, we can locate
any cell SA[r]. A similar procedure allows extracting any substring S of T with O(b + |S|)
applications of LF.

Small Interval Rank Queries

4
We start by showing how a compressed data structure that supports select queries can be
extended to support a new kind of queries that we dub small interval rank queries. An
interval query ranka(i, j, B) is a small interval rank query if j − i ≤ log2 σ. Our compressed
index relies on the following result.
(cid:73) Lemma 2. Suppose that we are given a data structure that supports access queries on a
sequence C[0..m − 1], on alphabet [0..σ − 1], in time t. Then, using O(m log log σ) additional
bits, we can support small interval rank queries on C in O(t) time.
Proof. We split C into groups Gi of log2 σ consecutive symbols, Gi = C[i log2 σ..(i +
1) log2 σ − 1]. Let Ai denote the sequence of the distinct symbols that occur in Gi. Storing
Ai directly would need log σ bits per symbol. Instead, we encode each element of Ai as its
ﬁrst position in Gi, which needs only O(log log σ) bits. With this encoded sequence, since we
have O(t)-time access to C, we have access to any element of Ai in time O(t). In addition,
we store a succinct SB-tree [18] on the elements of Ai. This structure uses O(p log log u) bits
to index p elements in [1..u], and supports predecessor (and membership) queries in time
O(log p/ log log u) plus one access to Ai. Since u = σ and p ≤ log2 σ, the query time is O(t)
and the space usage is bounded by O(m log log σ) bits.
For each a ∈ Ai we also keep the increasing list Ia,i of all the positions where a occurs
in Gi. Positions are stored as diﬀerences with the left border of Gi: if C[j] = a, we store
the diﬀerence j − i log2 σ. Hence elements of Ia,i can also be stored in O(log log σ) bits per
symbol, adding up to O(m log log σ) bits. We also build an SB-tree on top of each Ia,i to
provide for predecessor searches.

© J. Ian Munro, Gonzalo Navarro, Yakov Nekrich;
licensed under Creative Commons License CC-BY

Leibniz International Proceedings in Informatics
Schloss Dagstuhl – Leibniz-Zentrum für Informatik, Dagstuhl Publishing, Germany

Using the SB-trees on Ai and Ia,i, we can answer small interval rank queries ranka(x, y, C).
Consider a group Gi = C[i log2 σ..(i + 1) log2 σ − 1], an index k such that i log2 σ ≤ k ≤
(i + 1) log2 σ, and a symbol a. We can ﬁnd the largest i log2 σ ≤ r ≤ k such that C[r] = a,
or determine it does not exist: First we look for the symbol a in Ai; if a ∈ Ai, we ﬁnd the
predecessor of k − i log2 σ in Ia,i.

Now consider an interval C[x..y] of size at most log2 σ. It intersects at most two groups,
Gi and Gi−1. We ﬁnd the rightmost occurrence of symbol a in C[x..y] as follows. First
we look for the rightmost occurrence y0 ≤ y of a in Gi; if a does not occur in C[i log2 σ..y],
we look for the rightmost occurrence y0 ≤ i log2 σ − 1 of a in Gi−1. If this is ≥ x, we ﬁnd
the leftmost occurrence x0 of a in C[x..y] using a symmetric procedure. When x0 ≤ y0 are
found, we can compute ranka(x0, C) and ranka(y0, C) in O(1) time by answering partial rank
queries (Section 3.1). These are supported in O(1) time and O(m log log σ) bits. The answer
is then hranka(x0, C) − 1, ranka(y0, C)i, or null if a does not occur in C[x..y].
(cid:74)

√

The construction of the small interval rank data structure is dominated by the time
needed to build the succinct SB-trees [18]. These are simply B-trees with arity O(
log u)
and height O(log p/ log log u), where in each node a Patricia tree for O(log log u)-bit chunks
of the keys are stored. To build the structure in O(log p/ log log u) time per key, we only
need to build those Patricia trees in linear time. Given that the total number of bits of all
the keys to insert in a Patricia tree is O(
log u log log u), we do not even need to build the
Patricia tree. Instead, a universal precomputed table may answer any Patricia tree search
for any possible set of keys and any possible pattern, in constant time. The size of the table
is O(2O(
log u) = o(u) bits (the authors [18] actually use a similar table to
answer queries). For our values of p and u, the construction requires O(mt) time and the
universal table is of o(σ) bits.

log u log log u)√

√

√

Compressed Index

5
We classify the nodes of the suﬃx tree T of T into heavy, light, and special, as in previous
work [31, 28]. Let d = log σ. A node u of T is heavy if it has at least d leaf descendants and
light otherwise. We say that a heavy node u is special if it has at least two heavy children.
For every special node u, we construct a deterministic dictionary [20] Du that contains
the labels of all the heavy children of u: If the jth child of u, uj, is heavy and the ﬁrst
symbol on the edge from to u to uj is aj, then we store the key aj in Du with j as satellite
data. If a heavy node u has only one heavy child uj and d or more light children, then we
also store the data structure Du (containing only that heavy child of u). If, instead, a heavy
node has one heavy child and less than d light children, we just keep the index of the heavy
child using O(log d) = O(log log σ) bits.

The second component of our index is the Burrows-Wheeler Transform B of the reverse
text T. We store a data structure that supports rank, partial rank, select, and access queries
on B. It is suﬃcient for us to support access and partial rank queries in O(1) time and rank
queries in O(log logw σ) time. We also construct the data structure described in Lemma 2,
which supports small interval rank queries in O(1) time. Finally, we explicitly store the
answers to some rank queries. Let B[lu..ru] denote the range of Xu, where Xu is the reverse
of Xu, for a suﬃx tree node u. For all data structures Du and for every symbol a ∈ Du we
store the values of ranka(lu − 1, B) and ranka(ru, B).

Let us show how to store the selected precomputed answers to rank queries in O(log σ)
bits per query. Following a known scheme [17], we divide the sequence B into chunks of
size σ. For each symbol a, we encode the number dk of times a occurs in each chunk k

© J. Ian Munro, Gonzalo Navarro, Yakov Nekrich;
licensed under Creative Commons License CC-BY

Leibniz International Proceedings in Informatics
Schloss Dagstuhl – Leibniz-Zentrum für Informatik, Dagstuhl Publishing, Germany

in a binary sequence Aa = 01d001d101d2 . . .. If a symbol B[i] belongs to chunk k = bi/σc,
then ranka(i, B) is select0(k + 1, Aa) − k plus the number of times a occurs in B[kσ..i]. The
former value is computed in O(1) time with a structure that uses |Aa| + o(|Aa|) bits [9, 27],
whereas the latter value is in [0, σ] and thus can be stored in Du using just O(log σ) bits.
The total size of all the sequences Aa is O(n) bits.

Therefore, Du needs O(log σ) bits per element. The total number of elements in all the
structures Du is equal to the number of special nodes plus the number of heavy nodes with
one heavy child and at least d light children. Hence all Du contain O(n/d) symbols and use
O((n/d) log σ) = O(n) bits of space. Indexes of heavy children for nodes with only one heavy
child and less than d light children add up to O(n log log σ) bits. The structures for partial
rank and small interval rank queries on B use O(n log log σ) further bits. Since we assume
that σ is ω(1), we can simplify O(n log log σ) = o(n log σ).

The sequence representation that supports access and rank queries on B can be made
to use nHk(T) + o(n(Hk(T) + 1)) bits, by exploiting the fact that it is built on a BWT [6,
Thm. 10].1 We note that they use constant-time select queries on B instead of constant-time
access, so they can use select queries to perform LF −1-steps in constant time. Instead, with
our partial rank queries, we can perform LF-steps in constant time (recall Section 3.4), and
thus have constant-time access instead of constant-time select on B (we actually do not use
query select at all). They avoid this solution because partial rank queries require o(n log σ)
bits, which can be more than o(n(Hk(T) + 1)), but we are already paying this price.

Apart from this space, array Acc needs O(σ log n) = O(n) bits and SAMb uses O(n log n
).
) bits.

The total space usage of our self-index then adds up to nHk(T) + o(n log σ) + O(n log n

b

b

Pattern Search

6
Given a query string P, we will ﬁnd in time O(|P| + log logw σ) the range of the reversed
string P in B. A backward search for P in B will be replaced by an analogous backward
search for P in B, that is, we will ﬁnd the range of P[0..i] if the range of P[0..i − 1] is
known. Let [li..ri] be the range of P[0..i]. We can compute li and ri from li−1 and ri−1 as
li = Acc[a] + ranka(li−1 − 1, B) and ri = Acc[a] + ranka(ri−1, B) − 1, for a = P[i]. Using
our auxiliary data structures on B and the additional information stored in the nodes of the
suﬃx tree T , we can answer the necessary rank queries in constant time (with one exception).
The idea is to traverse the suﬃx tree T in synchronization with the forward search on B,
until the locus of P is found or we determine that P does not occur in T.
Our procedure starts at the root node of T , with l−1 = 0, r−1 = n − 1, and i = 0. We
compute the ranges B[li..ri] that correspond to P[0..i] for i = 0, . . . ,|P| − 1. Simultaneously,
we move down in the suﬃx tree. Let u denote the last visited node of T and let a = P[i].
We denote by ua the next node that we must visit in the suﬃx tree, i.e., ua is the locus of
P[0..i]. We can compute li and ri in O(1) time if ranka(ri−1, B) and ranka(li−1 − 1, B) are
known. We will show below that these queries can be answered in constant time because
either (a) the answers to rank queries are explicitly stored in Du or (b) the rank query that
must be answered is a small interval rank query. The only exception is the situation when
we move from a heavy node to a light node in the suﬃx tree; in this case the rank query
takes O(log logw σ) time. We note that, once we are in a light node, we need not descend in
T anymore; it is suﬃcient to maintain the interval in B.

For ease of description we distinguish between the following cases.

1 In fact it is nHk(T ), but this is nHk(T ) + O(log n) [12, Thm. A.3].

© J. Ian Munro, Gonzalo Navarro, Yakov Nekrich;
licensed under Creative Commons License CC-BY

Leibniz International Proceedings in Informatics
Schloss Dagstuhl – Leibniz-Zentrum für Informatik, Dagstuhl Publishing, Germany

most one heavy child and less than d light children. We have two subcases:
a. If ua is the (only) heavy node, we ﬁnd this out with a single comparison, as the heavy
node is identiﬁed in u. However, the values ranka(li−1 − 1, B) and ranka(ri−1, B) are
not stored in u. To compute them, we exploit the fact that the number of non-a’s in
B[li−1..ri−1] is less than d2, as all the children apart from ua are light and less than d.
Therefore, the ﬁrst and the last occurrences of a in B[li−1..ri−1] must be at distance
less than d2 from the extremes li−1 and ri−1, respectively. Therefore, a small interval
rank query, ranka(li−1, li−1 + d2, B), gives us ranka(li−1 − 1, B), since there is for sure
an a in the range. Analogously, ranka(ri−1 − d2, ri−1, B) gives us ranka(ri−1, B).

b. If ua is a light node, we compute li and ri with two standard rank queries on B (or

we might determine that P does not appear in T).

1. Node u is heavy and a ∈ Du. In this case we identify the heavy child ua of u that is
labeled with a in constant time using the deterministic dictionary. We can also ﬁnd li
and ri in time O(1) because ranka(li−1 − 1, B) and ranka(ri−1, B) are stored in Du.

2. Node u is heavy and a 6∈ Du. In this case ua, if it exists, is a light node. We then ﬁnd it
with two standard rank queries on B, in order to compute li and ri or determine that P
does not occur in T.

3. Node u is heavy but we do not keep a dictionary Du for the node u. In this case u has at

4. Node u is light. In this case, P[0..i−1] occurs at most d times in T. Hence P[0..i − 1] also
occurs at most d times in T and ri−1 − li−1 ≤ d. Therefore we can compute ri and li in
O(1) time by answering a small interval rank query, hranka(li−1 − 1, B), ranka(ri−1, B)i.
If this returns null, then P does not occur in T.

5. We are on an edge of the suﬃx tree between a node u and some child uj of u. In this
case all the occurrences of P[0..i − 1] in T are followed by the same symbol, c, and all
the occurrences of P[0..i − 1] are preceded by c in T. Therefore B[li−1..ri−1] contains
only the symbol c. This situation can be veriﬁed with access and partial rank queries on
B: B[ri−1] = B[li−1] = c and rankc(ri−1, B) − rankc(li−1, B) = ri−1 − li−1. In this case,
if a 6= c, then P does not occur in T; otherwise we obtain the new range with the partial
rank query rankc(ri−1, B), and rankc(li−1 − 1, B) = rankc(ri−1, B) − (ri−1 − li−1 + 1).
Note that if u is light we do not need to consider this case; we may directly apply case 4.

Except for the cases 2 and 3b, we can ﬁnd li and ri in O(1) time. In cases 2 and 3b we
need O(log logw σ) time to answer general rank queries. However, these cases only take place
when the node u is heavy and its child ua is light. Since all descendants of a light node are
light, those cases occur only once along the traversal of P. Hence the total time to ﬁnd the
range of P in B is O(|P| + log logw σ). Once the range is known, we can count and report
all occurrences of P in the standard way.

Linear-Time Construction

7
7.1 Sequences and Related Structures
Apart from constructing the BWT B of T, which is a component of the ﬁnal structure, the
linear-time construction of the other components requires that we also build, as intermediate
structures, the BWT B of T, and the compressed suﬃx trees T and T of T and T, respectively.
All these are built in O(n) deterministic time and using O(n log σ) bits of space [28]. We
also keep, on top of both B and B, O(n log log σ)-bit data structures able to report, for any
interval B[i..j] or B[i..j], all the distinct symbols from this interval, and their frequencies in

© J. Ian Munro, Gonzalo Navarro, Yakov Nekrich;
licensed under Creative Commons License CC-BY

Leibniz International Proceedings in Informatics
Schloss Dagstuhl – Leibniz-Zentrum für Informatik, Dagstuhl Publishing, Germany

the interval. The symbols are retrieved in arbitrary order. These auxiliary data structures
can also be constructed in O(n) time [28, Sec. A.5].

On top of the sequences B and B, we build the representation that supports access in
O(1) and rank in O(log logw σ) time [6]. In their original paper, those structures are built
using perfect hashing, but a deterministic construction is also possible [4, Lem. 11]; we give
the details next.

The key part of the construction is that, within a chunk of σ symbols, we must build a
virtual list Ia of the positions where each symbol a occurs, and provide predecessor search on
those lists in O(log logw σ) time. We divide each list into blocks of log2 σ elements, and create
a succinct SB-tree [18] on the block elements, much as in Section 4. The search time inside a
block is then O(t), where t is the time to access an element in Ia, and the total extra space is
O(n log log σ) bits. If there is more than one block in Ia, then the block minima are inserted
into a predecessor structure [6, App. A] that will ﬁnd the closest preceding block minimum
in time O(log logw σ) and use O(n log log σ) bits. This structure uses perfect hash functions
called I(P), which provide constant-time membership queries. Instead, we replace them with
deterministic dictionaries [20]. The only disadvantage of these dictionaries is that they require
O(log σ) construction time per element, and since each element is inserted into O(log logw σ)
structures I(P), the total construction time per element is O(log σ log logw σ). However,
since we build these structures only on O(n/ log2 σ) block minima, the total construction
time is only O(n).

On the variant of the structure that provides constant-time access, the access to an element
in Ia is provided via a permutation structure [29] which oﬀers access time t with extra space
O((n/t) log σ) bits. Therefore, for any log σ = ω(log w), we can have t = O(log logw σ) with
o(n log σ) bits of space.

7.2 Structures Du
The most complex part of the construction is to ﬁll the data of the Du structures. We visit
all the nodes of T and identify those nodes u for which the data structure Du must be
constructed. This can be easily done in linear time, by using the constant-time computation
of the number of descendant leaves. To determine if we must build Du, we traverse its
children u1, u2, . . . and count their descendant leaves to decide if they are heavy or light.

We use a bit vector D to mark the preorders of the nodes u for which Du will be
constructed: If p is the preorder of node u, then it stores a structure Du iﬀ D[p] = 1, in
which case Du is stored in an array at position rank1(D, p). If, instead, u does not store Du
but it has one heavy child, we store its child rank in another array indexed by rank0(D, p),
using log log σ bits per cell.
The main diﬃculty is how to compute the symbols a to be stored in Du, and the ranges
B[lu, ru], for all the selected nodes u. It is not easy to do this through a preorder traversal of T
because we would need to traverse edges that represent many symbols. Our approach, instead,
is inspired by the navigation of the suﬃx-link tree using two BWTs given by Belazzougui
et al. [3]. Let Tw denote the tree whose edges correspond to Weiner links between internal
nodes in T . That is, the root of Tw is the same root of T and, if we have internal nodes
u, v ∈ T where Xv = a · Xu for some symbol a, then v descends from u by the symbol a in
Tw. We ﬁrst show that the nodes of Tw are the internal nodes of T . The inclusion is clear by
deﬁnition in one direction; the other is well-known but we prove it for completeness.
(cid:73) Lemma 3. All internal nodes of the suﬃx tree T are nodes of Tw.
Proof. We proceed by induction on |Xu|, where the base case holds by deﬁnition. Now let a

© J. Ian Munro, Gonzalo Navarro, Yakov Nekrich;
licensed under Creative Commons License CC-BY

Leibniz International Proceedings in Informatics
Schloss Dagstuhl – Leibniz-Zentrum für Informatik, Dagstuhl Publishing, Germany

non-root internal node u of T be labeled by string Xu = aX. This means that there are at
least two diﬀerent symbols a1 and a2 such that both aXa1 and aXa2 occur in the text T.
Then both Xa1 and Xa2 also occur in T. Hence there is an internal node u0 with Xu0 = X
in T and a Weiner link from u0 to u. Since |Xu0| = |Xu| − 1, it holds by the inductive
hypothesis that u0 belongs to Tw, and thus u belongs to Tw as a child of u0.
(cid:74)

We do not build Tw explicitly, but just traverse its nodes conceptually in depth-ﬁrst order
and compute the symbols to store in the structures Du and the intervals in B. Let u be
the current node of T in this traversal and u its corresponding locus in T . Assume for now
that u is a nod, too. Let [lu, ru] be the interval of Xu in B and [lu, ru] be the interval of the
reverse string Xu in B.2 Our algorithm starts at the root nodes of Tw, T , and T , which
correspond to the empty string, and the intervals in B and B are [lu, ru] = [lu, ru] = [0, n−1].
We will traverse only the heavy nodes, yet in some cases we will have to work on all the
nodes. We ensure that on heavy nodes we work at most O(log σ) time, and at most O(1)
time on arbitrary nodes.

Upon arriving at each node u, we ﬁrst compute its heavy children. From the topology of
T we identify the interval [li, ri] for every child ui of u, by counting leaves in the subtrees
of the successive children of u. By reporting all the distinct symbols in B[lu..ru] with their
frequencies, we identify the labels of those children. However, the labels are retrieved in
arbitrary order and we cannot aﬀord sorting them all. Yet, since the labels are associated
with their frequencies in B[lu..ru], which match their number of leaves in the subtrees of u,
we can discard the labels of the light children, that is, those appearing less than d times in
B[lu..ru]. The remaining, heavy, children are then sorted and associated with the successive
heavy children ui of u in T .

If our preliminary pass marked that a Du structure must be built, we construct at this
moment the deterministic dictionary [20] with the labels a of the heavy children of u we have
just identiﬁed, and associate them with the satellite data ranka(lu − 1, B) and ranka(ru, B).
This construction takes O(log σ) time per element, but it includes only heavy nodes.

We now ﬁnd all the Weiner links from u. For every (heavy or light) child ui of u, we
compute the list Li of all the distinct symbols that occur in B[li..ri]. We mark those symbols
a in an array V [0..σ − 1] that holds three possible values: not seen, seen, and seen (at least)
twice. If V [a] is not seen, then we mark it as seen; if it is seen, we mark it as seen twice;
otherwise we leave it as seen twice. We collect a list Eu of the symbols that are seen twice
along this process, in arbitrary order. For every symbol a in Eu, there is an explicit Weiner
link from u labeled by a: Let X = Xu; if a occurred in Li and Lj then both aXai and aXaj
occur in T and there is a suﬃx tree node that corresponds to the string aX. The total time
to build Eu amortizes to O(n): for each child v of u, we pay O(1) time for each child the
node v has in T ; each node in T contributes once to the cost.
The targets of the Weiner links from u in T correspond to the children of the node u in
T . To ﬁnd them, we collect all the distinct symbols in B[lu..ru] and their frequencies. Again,
we discard the symbols with frequency less than d, as they will lead to light nodes, which
we do not have to traverse. The others are sorted and associated with the successive heavy
children of u. By counting leaves in the successive children, we obtain the intervals B[l0
i..r0
i]
corresponding to the heavy children u0

i of u.

We are now ready to continue the traversal of Tw:

for each Weiner link from u by

2 In the rest of the paper we wrote B[lu..ru] instead of B[lu..ru] for simplicity, but this may cause

confusion in this section.

© J. Ian Munro, Gonzalo Navarro, Yakov Nekrich;
licensed under Creative Commons License CC-BY

Leibniz International Proceedings in Informatics
Schloss Dagstuhl – Leibniz-Zentrum für Informatik, Dagstuhl Publishing, Germany

i..r0

symbol a leading to a heavy node, which turns out to be the i-th child of u, we know
that its node in T is u0
i (computed from u using the tree topology) and its interval is
B[l0
i]. To compute the corresponding interval on B, we use the backward step operation,
B[x, y] = B[Acc[a] + ranka(lu − 1, B), Acc[a] + ranka(ru, B) − 1]. This requires O(log logw σ)
time, but applies only to heavy nodes. Finally, the corresponding node in T is obtained in
constant time as the lowest common ancestor of the x-th and the y-th leaves of T .

In the description above we assumed for simplicity that u is a node in T . In the general
case u can be located on an edge of T . This situation arises when all occurrences of Xu
in the reverse text T are followed by the same symbol a. In this case there is at most one
Weiner link from u; the interval in B does not change as we follow that link.

A recursive traversal of Tw might require O(nσ log n) bits for the stack, because we store
several integers associated to heavy children during the computation of each node u. We
can limit the stack height by determining the largest subtree among the Weiner links of
u, traversing all the others recursively, and then moving to that largest Weiner link target
without recursion [3, Lem. 1]. Since only the largest subtree of a Weiner link target can
contain more than half of the nodes of the subtree of u, the stack is guaranteed to be of
height only O(log n). The space usage is thus O(σ log2 n) = O(n log σ).

As promised, we have spent at most O(log σ) time on heavy nodes, which are O(n/d) =
O(n/ log σ) in total, thus these costs add up to O(n). All other costs that apply to arbitrary
nodes are O(1). The structures for partial rank queries (and the succinct SB-trees) can also
be built in linear deterministic time, as shown in Section 4. Therefore our index can be
constructed in O(n) time.

A Compact Index

8
As an application of our techniques, we show that it is possible to obtain O(|P|/ logσ n+log2 n)
search time, and even O(|P|/ logσ n + log n(log log n)2), with an index that uses O(n log σ)
bits and is built in linear deterministic time.
We store B in compressed form and a sample of the heavy nodes of T . Following previous
work [19, 31], we start from the root and store a deterministic dictionary [20] with all the
highest suﬃx tree nodes v representing strings of depth ≥ ‘ = logσ n. The key associated
with each node is a log(n)-bit integer formed with the ﬁrst ‘ symbols of the strings Pv. The
satellite data are the length |Pv|, a position where Pv occurs in T, and the range B[lv..rv] of
v. From each of those nodes v, we repeat the process with the ﬁrst ‘ symbols that follow
after Pv, and so on. The diﬀerence is that no light node will be inserted in those dictionaries.
Let us charge the O(log n) bits of space to the children nodes, which are all heavy. If we
count only the special nodes, which are O(n/d), this amounts to O((n log n)/d) total bits
and construction time. Recall that d is the maximum subtree size of light nodes. This time
will use d = Θ(log n) to have linear construction time and bit space, and thus will not take
advantage of small rank interval queries.

There are, however, heavy nodes that are not special. These form possibly long chains
between special nodes, and these will also induce chains of sampled nodes. While building
the dictionaries for those nodes is trivial because they have only one sampled child, the total
space may add up to O(n log σ) bits, if there are Θ(n) heavy nodes and the sampling chooses
one out of ‘ in the chains. To avoid this, we increase the sampling step in those chains,
enlarging it to ‘0 = log n. This makes the extra space spent in sampling heavy non-special
nodes to be O(n) bits as well.

In addition, we store the text T with a data structure that uses nHk(T) + o(n log σ) for

© J. Ian Munro, Gonzalo Navarro, Yakov Nekrich;
licensed under Creative Commons License CC-BY

Leibniz International Proceedings in Informatics
Schloss Dagstuhl – Leibniz-Zentrum für Informatik, Dagstuhl Publishing, Germany

any k = o(logσ n), and allows us extract O(logσ n) consecutive symbols in constant time [14].
The search for P starts at the root, where its ﬁrst ‘ symbols are used to ﬁnd directly the
right descendant node v in the dictionary stored at the root. If |Pv| > ‘, we directly compare
the other |Pv| − ‘ symbols of P with the text, from the stored position where v appears, by
chunks of ‘ symbols. Then we continue the search from v (ignore the chains for now).

When the next ‘ symbols of P are not found in the dictionary of the current node v,
or there are less than ‘ remaining symbols in P, we continue using backward search on B
from the interval B[lv..rv] stored at v, and do not use the suﬃx tree topology anymore. If
there are less than ‘ remaining symbols in P, we just proceed symbolwise and complete the
search in O(‘ log logw σ) time. Before this point, we still proceed by chunks of ‘ symbols,
except when (conceptually) traversing the explicit light suﬃx tree nodes, where we perform
individual backward steps. Because there are only O(d) suﬃx tree nodes in the remaining
subtree, the number of backward steps to traverse such light nodes is O(d) and can be
performed in time O(d log logw σ). All the symbols between consecutive light nodes must
be traversed in chunks of ‘. Let v be our current node and u its desired child, and let
k + 1 be the number of symbols labeling the edge between them. After the ﬁrst backward
step from v to u leads us from the interval B[lv..rv] to B[l..r], we must perform k further
backward steps, where each intermediate interval is formed by just one symbol. To traverse
them fast, we ﬁrst compute tl = SA[l] and tr = SA[r]. The desired symbols are then
T[tl − 1] = T[tr − 1], . . . , T[tl − k] = T[tr − k]. We thus compare the suﬃxes T[n − tl..] and
T[n − tr..] with what remains of P, by chunks of ‘ symbols, until ﬁnding the ﬁrst diﬀerence;
k is then the number of coincident symbols seen. If the two suﬃxes coincide up to k + 1
but they diﬀer from P, then P is not in T. Otherwise, we can move (conceptually) to
node u and consume the k coincident characters from P. We can compute the new interval
[lu..ru] = [SA −1[tl − k], SA −1[tr − k]]. Since SA and its inverse are computed in O(b) time
with arrays similar to SAMb, we spend O(b) time to cross each of the O(d) edges in the ﬁnal
part of the search.

Let us now regard the case where we reach a sampled node v that starts a sampled chain.
In this case the sampling step grows to ‘0. We still compare P with a suﬃx of T where its
heavy sampled child u appears, in chunks of ‘ symbols. If they coincide, we continue the
search from u. Otherwise, we resume the search from B[lv..rv] using backward search. We
might have to process ‘0 symbols of P, in time O(‘0 log logw σ), before reaching a light node.
Then, we proceed as explained.

Overall, the total space is 2nHk(T) + o(n log σ) + O(n) + O((n log n)/b) bits, and the
construction time is O(n). To have o(n log σ) + O(n) bits of redundancy in total, we may
choose b = Θ(log n) or any b = ω(logσ n). The search time is O(|P|/ logσ n + ‘0 log logw σ +
d log logw σ + db). We can, for example, choose b = Θ(log n), to obtain the following result.
(cid:73) Theorem 4. On a RAM machine of w = Ω(log n) bits, we can construct an index for
a text T of length n over an alphabet of size σ = O(n/ log n) in O(n) deterministic time
using O(n log σ) bits of working space. This index occupies 2nHk(T) + o(n log σ) + O(n) bits
of space for any k = o(logσ n). The occurrences of a pattern string P can be counted in
O(|P|/ logσ n + log2 n) time, and then each such occurrence can be located in O(log n) time.
An arbitrary substring S of T can be extracted in time O(|S|/ logσ n).

8.1 Faster and Larger
By storing the BWT B of T and the O(n) additional bits to support a compressed suﬃx
tree T [33], we can reduce the O(log2 n) extra time to O(log n(log log n)2).

© J. Ian Munro, Gonzalo Navarro, Yakov Nekrich;
licensed under Creative Commons License CC-BY

Leibniz International Proceedings in Informatics
Schloss Dagstuhl – Leibniz-Zentrum für Informatik, Dagstuhl Publishing, Germany

The idea is to speed up the traversal on the light nodes, as follows. For each light node v,
we store the leaf in T where the heavy path starting at v ends. The heavy path chooses at
each node the subtree with the most leaves, thus any traversal towards a leaf has to switch to
another heavy path only O(log d) times. At each light node v, we go to the leaf u of its heavy
path, obtain its position in T using the sampled array SAMb of B, and compare the rest
of P with the corresponding part of the suﬃx, by chunks of ‘ symbols. Once we determine
the number k of symbols that coincide with P in the path from v to u, we perform a binary
search for the highest ancestor v0 of u where |Pv0 − Pv| ≥ k. If |Pv0 − Pv| > k, then P does
not appear in T (unless P ends at the k-th character compared, in which case the locus of P
is v0). Otherwise, we continue the search from v0. Each binary search step requires O(b) time
to determine |Px| [33], so it takes time O(b log d). Since we switch to another heavy path
(now the one starting at v0) O(log d) times, the total time is O(b log2 d) = O(log n(log log n)2)
instead of O(bd) = O(log2 n).

To store the leaf u corresponding to each light node v, we record the diﬀerence between
the preorder numbers of u and v, which requires O(log d) bits. The node u is easily found
in constant time from this information [33]. We have the problem, however, that we spend
O(log d) = O(log log n) bits per light node, which adds up to O(n log log n) bits. To reduce
this to O(n), we choose a second sampling step e = O(log log n), and do not store this
information on nodes with less than e leaves, which are called light-light. Those light
nodes with e leaves or more are called light-heavy, and those with at least two light-heavy
children are called light-special. There are O(n/e) light-special nodes. We store heavy path
information only for light-special nodes or for light-heavy nodes that are children of heavy
nodes; both are O(n/e) in total. A light-heavy node v that is not light-special has at most
one light-heavy child u, and the heavy path that passes through v must continue towards u.
Therefore, if it turns out that the search must continue from v after the binary search on the
heavy path, then the search must continue towards the light-light children of v, therefore no
heavy-path information is needed at node v.

Once we reach the ﬁrst light-light node v, we proceed as we did for Theorem 4 on light
nodes, in total time O(eb) = O(log n log log n). We need, however, the interval B[lv, rv]
before we can start the search from v. The interval B[lv, rv] is indeed known by counting
leaves in T , and we can compute lv = SA −1[n − 1 − SA[lv]] and rv = SA −1[n − 1 − SA[rv]],
in time O(b).
(cid:73) Theorem 5. On a RAM machine of w = Ω(log n) bits, we can construct an index for
a text T of length n over an alphabet of size σ = O(n/ log n) in O(n) deterministic time
using O(n log σ) bits of working space. This index occupies 3nHk(T) + o(n log σ) + O(n)
bits of space for any k = o(logσ n). The occurrences of a pattern string P can be counted
in O(|P|/ logσ n + log n(log log n)2) time, and then each such occurrence can be located in
O(log n) time. An arbitrary substring S of T can be extracted in time O(|S|/ logσ n).

9

Conclusions

We have shown how to build, in O(n) deterministic time and using O(n log σ) bits of working
space, a compressed self-index for a text T of length n over an alphabet of size σ that searches
for patterns P in time O(|P| + log logw σ), on a w-bit word RAM machine. This improves
upon previous compressed self-indexes requiring O(|P| log log σ) [1] or O(|P|(1 + logw σ))
[6] time, on previous uncompressed indexes requiring O(|P| + log log σ) time [15] (but that
supports dynamism), and on previous compressed self-indexes requiring O(|P|(1+log logw σ))
time and randomized construction (which we now showed how to build in linear deterministic

© J. Ian Munro, Gonzalo Navarro, Yakov Nekrich;
licensed under Creative Commons License CC-BY

Leibniz International Proceedings in Informatics
Schloss Dagstuhl – Leibniz-Zentrum für Informatik, Dagstuhl Publishing, Germany

time) [6]. The only indexes oﬀering better search time require randomized construction
[5, 19, 31] or Θ(n log n) bits of space [31, 7].
deterministic time an index that searches in time O(|P|/ logσ n + log n(log log n)2).

As an application, we showed that using O(n log σ) bits of space, we can build in O(n)
It is not clear if O(|P|) time, or even O(|P|/ logσ n), query time can be achieved with a
linear deterministic construction time, even if we allow O(n log n) bits of space for the index
(this was recently approached, but some additive polylog factors remain [7]). This is the
most interesting open problem for future research.

1

References
J. Barbay, F. Claude, T. Gagie, G. Navarro, and Y. Nekrich. Eﬃcient fully-compressed
sequence representations. Algorithmica, 69(1):232–268, 2014.

2 D. Belazzougui, P. Boldi, R. Pagh, and S. Vigna. Fast preﬁx search in little space, with
applications. In Proc. 18th Annual European Symposium on Algorithms (ESA), LNCS 6346,
pages 427–438, 2010.

3 D. Belazzougui, F. Cunial, J. Kärkkäinen, and V. Mäkinen. Versatile succinct representIn 
Proc. 21st Annual European

ations of the bidirectional Burrows-Wheeler transform.
Symposium on Algorithms (ESA), pages 133–144, 2013.

4 D. Belazzougui, F. Cunial, J. Kärkkäinen, and V. Mäkinen. Linear-time string indexing

and analysis in small space. CoRR, abs/1609.06378, 2016.

5 D. Belazzougui and G. Navarro. Alphabet-independent compressed text indexing. ACM

Transactions on Algorithms, 10(4):article 23, 2014.

6 D. Belazzougui and G. Navarro. Optimal lower and upper bounds for representing sequences.

ACM Transactions on Algorithms, 11(4):article 31, 2015.
P. Bille, I. L. Gørtz, and F. R. Skjoldjensen. Deterministic indexing for packed strings.
In Proc. 28th Annual Symposium on Combinatorial Pattern Matching (CPM), LIPIcs 78,
page article 6, 2017.

7

8 M. Burrows and D. Wheeler. A block sorting lossless data compression algorithm. Technical

Report 124, Digital Equipment Corporation, 1994.

9 D. R. Clark. Compact PAT Trees. PhD thesis, University of Waterloo, Canada, 1996.
10 R. Cole, T. Kopelowitz, and M. Lewenstein. Suﬃx trays and suﬃx trists: Structures for

faster text indexing. Algorithmica, 72(2):450–466, 2015.

11 M. Farach. Optimal suﬃx tree construction with large alphabets. In Proc. 38th Annual

12

13

14

15

Symposium on Foundations of Computer Science (FOCS), pages 137–143, 1997.
P. Ferragina and G. Manzini. Indexing compressed text. Journal of the ACM, 52(4):552–
581, 2005.
P. Ferragina, G. Manzini, V. Mäkinen, and G. Navarro. Compressed representations of
sequences and full-text indexes. ACM Transactions on Algorithms, 3(2):article 20, 2007.
P. Ferragina and R. Venturini. A simple storage scheme for strings achieving entropy
bounds. Theoretical Computer Science, 371(1):115–121, 2007.
J. Fischer and P. Gawrychowski. Alphabet-dependent string searching with wexponential
search trees. In Proc. 26th Annual Symposium on Combinatorial Pattern Matching (CPM),
LNCS 9133, pages 160–171, 2015.

16 T. Gagie. Large alphabets and incompressibility. Information Processing Letters, 99(6):246–

251, 2006.

17 A. Golynski, J. I. Munro, and S. S. Rao. Rank/select operations on large alphabets: a tool
for text indexing. In Proc. 17th Annual ACM-SIAM Symposium on Discrete Algorithms,
(SODA), pages 368–373, 2006.

© J. Ian Munro, Gonzalo Navarro, Yakov Nekrich;
licensed under Creative Commons License CC-BY

Leibniz International Proceedings in Informatics
Schloss Dagstuhl – Leibniz-Zentrum für Informatik, Dagstuhl Publishing, Germany

18 R. Grossi, A. Orlandi, R. Raman, and S. S. Rao. More haste, less waste: Lowering the
In Proc. 26th International Symposium on

redundancy in fully indexable dictionaries.
Theoretical Aspects of Computer Science (STACS), pages 517–528, 2009.

19 R. Grossi and J. S. Vitter. Compressed suﬃx arrays and suﬃx trees with applications to

text indexing and string matching. SIAM Journal on Computing, 35(2):378–407, 2005.

20 T. Hagerup, P. Bro Miltersen, and R. Pagh. Deterministic dictionaries. Journal of Algorithms,
 41(1):69 – 85, 2001.
J. Kärkkäinen, P. Sanders, and S. Burkhardt. Linear work suﬃx array construction. Journal
of the ACM, 53(6):918–936, 2006.

22 D. K. Kim, J. S. Sim, H. Park, and K. Park. Constructing suﬃx arrays in linear time.

Journal of Discrete Algorithms, 3(2-4):126–142, 2005.
P. Ko and S. Aluru. Space eﬃcient linear time construction of suﬃx arrays. Journal of
Discrete Algorithms, 3(2-4):143–156, 2005.

24 U. Manber and G. Myers. Suﬃx arrays: a new method for on-line string searches. SIAM

Journal on Computing, 22(5):935–948, 1993.

25 G. Manzini. An analysis of the Burrows-Wheeler transform. Journal of the ACM, 48(3):407–

430, 2001.
E. M. McCreight. A space-economical suﬃx tree construction algorithm. Journal of the
ACM, 23(2):262–272, 1976.
J. I. Munro. Tables. In Proc. 16th Conference on Foundations of Software Technology and
Theoretical Computer Science (FSTTCS), LNCS 1180, pages 37–42, 1996.
J. I. Munro, G. Navarro, and Y. Nekrich. Space-eﬃcient construction of compressed indexes
in deterministic linear time.
In Proc. 28th Annual ACM-SIAM Symposium on Discrete
Algorithms (SODA), pages 408–424, 2017.
J. I. Munro, R. Raman, V. Raman, and S. S. Rao. Succinct representations of permutations
and functions. Theoretical Computer Science, 438:74–88, 2012.

30 G. Navarro and V. Mäkinen. Compressed full-text indexes. ACM Computing Surveys,

21

23

26

27

28

29

39(1):article 2, 2007.

Computing, 46(1):89–113, 2017.

31 G. Navarro and Y. Nekrich. Time-optimal top-k document retrieval. SIAM Journal on

32 G. Navarro and K. Sadakane. Fully-functional static and dynamic succinct trees. ACM

Transactions on Algorithms, 10(3):article 16, 2014.

33 K. Sadakane. Compressed suﬃx trees with full functionality. Theory of Computing Systems,

41(4):589–607, 2007.
E. Ukkonen. On-line construction of suﬃx trees. Algorithmica, 14(3):249–260, 1995.
P. Weiner. Linear pattern matching algorithms.
Switching and Automata Theory (FOCS), pages 1–11, 1973.

In Proc. 14th Annual Symposium on

34
35

© J. Ian Munro, Gonzalo Navarro, Yakov Nekrich;
licensed under Creative Commons License CC-BY

Leibniz International Proceedings in Informatics
Schloss Dagstuhl – Leibniz-Zentrum für Informatik, Dagstuhl Publishing, Germany

