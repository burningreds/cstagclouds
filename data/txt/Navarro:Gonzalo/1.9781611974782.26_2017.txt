Space-Eﬃcient Construction of Compressed Indexes in Deterministic

J. Ian Munro∗

Linear Time
Gonzalo Navarro†

Yakov Nekrich‡

Abstract

We show that the compressed suﬃx array and the
compressed suﬃx tree of a string T can be built in
O(n) deterministic time using O(n log σ) bits of space,
where n is the string length and σ is the alphabet
size.
Previously described deterministic algorithms
either run in time that depends on the alphabet size
or need ω(n log σ) bits of working space. Our result
has immediate applications to other problems, such as
yielding the ﬁrst deterministic linear-time LZ77 and
LZ78 parsing algorithms that use O(n log σ) bits.

1

Introduction

In the string indexing problem we pre-process a string
T , so that for any query string P all occurrences of P in
T can be found eﬃciently. Suﬃx trees and suﬃx arrays
are two most popular solutions of this fundamental
problem. A suﬃx tree is a compressed trie on suﬃxes of
T ; it enables us to ﬁnd all occurrences of a string P in T
in time O(|P| + occ) where occ is the number of times P
occurs in T and |P| denotes the length of P . In addition
to indexing, suﬃx trees also support a number of other,
more sophisticated, queries. The suﬃx array of a string
T is the lexicographically sorted array of its suﬃxes.
Although suﬃx arrays do not support all queries that
can be answered by the suﬃx tree, they use less space
and are more popular in practical
implementations.
While the suﬃx tree occupies O(n log n) bits of space,
the suﬃx array can be stored in n log n bits.

During the last twenty years there has been a
signiﬁcant increase in interest in compressed indexes,
i.e., data structures that keep T in compressed form and
support string matching queries. The compressed suﬃx
array (CSA) [19, 13, 38] and the compressed suﬃx tree
(CST) [39] are compressed counterparts of the suﬃx

Email imunro@uwaterloo.ca.

∗Cheriton School of Computer Science, University of Waterloo.
†CeBiB — Center of Biotechnology and Bioengineering, Department 
of Computer Science, University of Chile.
Email
gnavarro@dcc.uchile.cl. Funded with Basal Funds FB0001,
Conicyt, Chile.
‡Cheriton School of Computer Science, University of Waterloo.

Email: yakov.nekrich@googlemail.com.

array and the suﬃx tree respectively. A signiﬁcant
part of compressed indexes relies on these two data
structures or their variants. Both CSA and CST can
be stored in O(n log σ) bits or less; we refer to e.g. [6]
or [32] for an overview of compressed indexes.

It is well known that both the suﬃx array and
the suﬃx tree can be constructed in O(n) time [28,
42, 43, 23]. The ﬁrst algorithm that constructs the
suﬃx tree in linear time independently of the alphabet
size was presented by Farach [12]. There are also
algorithms that directly construct the suﬃx array of
T in O(n) time [22, 24]. If the (uncompressed) suﬃx
tree is available, we can obtain CST and CSA in O(n)
time. However this approach requires O(n log n) bits of
working space. The situation is diﬀerent if we want to
construct compressed variants of these data structures
using only O(n log σ) bits of space. Within this space
the algorithm of Hon et al. [21] constructs the CST in
O(n logε n) time for an arbitrarily small constant ε > 0.
In the same paper the authors also showed that CSA can
be constructed in O(n log log σ) time. The algorithm of
Okanohara and Sadakane constructs the CSA in linear
time, but needs O(n log σ log log n) bits of space [36].
Belazzougui [1] described randomized algorithms that
build both CSA and CST in O(n) time and O(n log σ)
bits of space. His approach also provides deterministic
algorithms with runtime O(n log log σ) [2]. In this paper
we show that randomization is not necessary in order to
construct CSA and CST in linear time. Our algorithms
run in O(n) deterministic time and require O(n log σ)
bits of space.

Suﬃx trees, in addition to being an important part
of many compressed indexes, also play an important
role in many string algorithms. One prominent example
is Lempel-Ziv parsing of a string using O(n log σ) bits.
The best previous solutions for this problem either take
O(n log log σ) deterministic time or O(n) randomized
time [25, 9]. For instance K¨oppl and Sadakane [25]
showed how we can obtain LZ77and 
LZ78-parsing for
a string T in O(n) deterministic time and O(n log σ)
bits, provided that the CST of T is constructed. Thus
our algorithm, combined with their results, leads to the
ﬁrst linear-time deterministic LZ-parsing algorithm that

408

Copyright © 2016
J. Ian Munro,  Gonzalo Navarroy, Yakov Nekrichz

Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpneeds O(n log σ) bits of space.

Overview. The main idea of our approach is the
use of batch processing. Certain operations, such as
rank and select queries on sequences, are a bottleneck
of previous deterministic solutions. Our algorithms are
divided into a large number of small tasks that can be
executed independently. Hence, we can collect large
batches of queries and answer all queries in a batch.
This approach speeds up the computation because, as
will be shown later, answering all queries in a batch
takes less time than answering the same set of queries
one-by-one. For example, our algorithm for generating
the Burrows-Wheeler Transform of a text T works as
follows. We cut the original text into slices of ∆ =
logσ n symbols. The BWT sequence is constructed
by scanning all slices in the right-to-left order. All
slices are processed at the same time. That is, the
algorithm works in ∆ steps and during the j-th step,
for 0 ≤ j ≤ ∆ − 1, we process all suﬃxes that start
at position i∆ − j − 1 for all 1 ≤ i ≤ n/∆. Our
algorithm maintains the sorted list of suﬃxes and keeps
information about those suﬃxes in a symbol sequence
B. For every suﬃx Si = T [i∆−j−1..] processed during
the step j, we must ﬁnd its position in the sorted list of
suﬃxes. Then the symbol T [i∆ − j − 2] is inserted at
the position that corresponds to Si in B. Essentially we
can ﬁnd the position of every new suﬃx Si by answering
a rank query on the sequence B. Details are given
in Section 2. Next we must update the sequence by
inserting the new symbols into B. Unfortunately we
need Ω(log n/ log log n) time in general to answer rank
queries on a dynamic sequence [15]. Even if we do not
have to update the sequence, we need Ω(log log σ) time
to answer a rank query [7]. In our case, however, the
scenario is diﬀerent: There is no need to answer queries
one-by-one. We must provide answers to a large batch
of n/∆ rank queries with one procedure. In this paper
we show that the lower bounds for rank queries can be
circumvented in the batched scenario: we can answer
the batch of queries in O(n/∆) time, i.e., in constant
time per query. We also demonstrate that a batch of
n/∆ insertions can be processed in O(n/∆) time. This
result is of independent interest.

Data structures that answer batches of rank queries
and support batched updates are described in Sections 
3, A.2, and A.3. This is the most technically
involved aspect of our result.
In Section 3 we show
how answers to a large batch of queries can be provided.

In Section A.2 we describe a special labeling
scheme that assigns monotonously increasing labels to
elements of a list. We conclude this portion in Section 
A.3 where we show how the static data structure
can be dynamized. Next we turn to the problem of constructing 
the compressed suﬃx tree. First we describe a
data structure that answers partial rank queries in constant 
time and uses O(n log log σ) additional bits in Section 
A.4; unlike previous solutions, our data structure
can be constructed in O(n) deterministic time. This
result is plugged into the algorithm of Belazzougui [1]
to obtain the suﬃx tree topology in O(n) deterministic
time. Finally we show how the permuted LCP array
(PLCP) can be constructed in O(n) time, provided we
already built the suﬃx array and the suﬃx tree topology;
 the algorithm is described in Section 5. Our algorithm 
for constructing PLCP is also based on batch
processing of rank queries. To make this paper selfcontained 
we provide some background on compressed
data structures and indexes in Section A.1.

We denote by T [i..] the suﬃx of T starting at
position i and we denote by T [i..j] the substring of
T that begins with T [i] and ends with T [j], T [i..] =
T [i]T [i+1] . . . T [n−1] and T [i..j] = T [i]T [i+1] . . . T [j−
1]T [j]. We assume that the text T ends with a special
symbol $ and $ lexicographically precedes all other
symbols in T . The alphabet size is σ and symbols
are integers in [0..σ − 1] (so $ corresponds to 0).
In
this paper, as in the previous papers on this topic, we
use the word RAM model of computation. A machine
word consists of log n bits and we can execute standard
bit operations, addition and subtraction in constant
time. We will assume for simplicity that the alphabet
size σ ≤ n1/4. This assumption is not restrictive
because for σ > n1/4 linear-time algorithms that use
O(n log σ) = O(n log n) bits are already known.

2 Linear

Time

Construction

of

the

Burrows-Wheeler Transform

In this section we show how the Burrows-Wheeler
transform (BWT) of a text T can be constructed in O(n)
time using O(n log σ) bits of space. Let ∆ = logσ n. We
can assume w.l.o.g.
that the text length is divisible
by ∆ (if this is not the case we can pad the text T
with (cid:100)n/∆(cid:101)∆ − n $-symbols). The BWT of T is a
sequence B deﬁned as follows: if T [k..] is the (i + 1)-th
lexicographically smallest suﬃx, then B[i] = T [k − 1]1.
Thus the symbols of B are the symbols that precede
the suﬃxes of T , sorted in lexicographic order. We
will say that T [k − 1] represents the suﬃx T [k..] in B.
Our algorithm divides the suﬃxes of T into ∆ classes
and constructs B in ∆ steps. We say that a suﬃx S
is a j-suﬃx for 0 ≤ j < ∆ if S = T [i∆ − j − 1..]
for some i, and denote by Sj the set of all j-suﬃxes,

1So B[0] has the lexicographically smallest suﬃx (i + 1 = 1)
and so on. The exact formula is B[i] = T [(k − 1)mod n]. We will
write B[i] = T [k − 1] to avoid tedious details.

409

Copyright © 2016
J. Ian Munro,  Gonzalo Navarroy, Yakov Nekrichz

Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpSj = { T [i∆ − j − 1..]| 1 ≤ i ≤ n/∆}. During the
j-th step we process all j-suﬃxes and insert symbols
representing j-suﬃxes at appropriate positions of the
sequence B.

Steps 0 − 1. We sort suﬃxes in S0 and S1 by constructing 
a new text and representing it as a sequence of
n/∆ meta-symbols. Let T1 = T [n− 1]T [0]T [1] . . . T [n−
2] be the text T rotated by one symbol to the right and
let T2 = T [n − 2]T [n − 1]T [0] . . . T [n − 3] be the text
obtained by rotating T1 one symbol to the right. We
represent T1 and T2 as sequences of length n/∆ over
meta-alphabet σ∆ (each meta-symbol corresponds to a
string of length ∆). Thus we view T1 and T2 as sequences 
of meta-symbols; see Fig. 1.

Let T3 = T1 ◦ T2 denote the concatenation of T1
and T2. To sort the suﬃxes of T3, we sort the metasymbols 
of T3 and rename them with their ranks. Since
meta-symbols correspond to (log n)-bit integers, we can
sort them in time O(n) using radix sort. Then we apply
a linear-time and linear-space suﬃx array construction
algorithm [22] to T3. We thus obtain a sorted list of
suﬃxes L for the meta-symbol sequence T3. Suﬃxes of
T3 correspond to the suﬃxes from S0∪S1 in the original
text T : the suﬃx T [i∆ − 1..] corresponds to the suﬃx
of S0 starting with meta-symbol T [i∆ − 1]T [i∆] . . .
in
T3 and the suﬃx T [i∆ − 2 . . .] corresponds to the suﬃx
of S1 starting with T [i∆ − 2]T [i∆ − 1] . . . . Since we
assume that the special symbol $ is smaller than all
other symbols, this correspondence is order-preserving.
Hence by sorting the suﬃxes of T3 we obtain the sorted
list L(cid:48) of suﬃxes in S0∪S1. Now we are ready to insert
symbols representing j-suﬃxes into B:
Initially B is
empty. Then the list L(cid:48) is traversed and for every suﬃx
T [k..] that appears in L(cid:48) we add the symbol T [k − 1] at
the end of B.
When suﬃxes in S0 and S1 are processed, we need
to record some information for the next step of our
algorithm. For every suﬃx S ∈ S1 we keep its position
in the sorted list of suﬃxes. The position of suﬃx
T [i∆ − 2..] is stored in the entry W [i] of an auxiliary
array W , which at the end of the j-th step will contain
the positions of the suﬃxes T [i∆ − j − 1..]. We also
keep an auxiliary array Acc of size σ: Acc[a] is equal to
the number of occurrences of symbols i ≤ a − 1 in the
current sequence B.
Step j for j ≥ 2. Suppose that suﬃxes from
S0,
. . ., Sj−1 are already processed. The symbols
that precede suﬃxes from these sets are stored in the
sequence B; the k-th symbol B[k] in B is the symbol
that precedes the k-th lexicographically smallest suﬃx
from ∪j−1
t=0St. For every suﬃx T [i∆ − j..], we know its
position W [i] in B. Every suﬃx Si = T [i∆−j−1..] ∈ Sj

can be represented as Si = aS(cid:48)i for some symbol a and
the suﬃx S(cid:48)i = T [i∆ − j..] ∈ Sj−1. We look up the
position ti = W [i] of S(cid:48)i and answer rank query ri =
ranka(ti, B). We need Ω(log log σ
log log n ) time to answer a
single rank query on a static sequence [7]. If updates
are to be supported, then we need Ω(log n/ log log n)
time to answer such a query [15]. However in our case
the scenario is diﬀerent: we perform a batch of n/∆
queries to sequence B, i.e., we have to ﬁnd ri for all ti.
During Step 2 the number of queries is equal to |B|/2
where |B| denotes the number of symbols in B. During
step j the number of queries is |B|/j ≥ |B|/∆. We
will show in Section 3 that such a large batch of rank
queries can be answered in O(1) time per query. Now
we can ﬁnd the rank pi of Si among ∪j
t=1St: there are
exactly pi suﬃxes in ∪j
t=1St that are smaller than Si,
where pi = Acc[a] + ri. Correctness of this computation
can be proved as follows.
Proposition 2.1. Let Si = aS(cid:48)i be an arbitrary suﬃx
from the set Sj. For every occurrence of a symbol a(cid:48) < a
in the sequence B, there is exactly one suﬃx Sp < Si in
∪j
t=1St, such that Sp starts with a(cid:48). Further, there are
exactly ri suﬃxes Sv in ∪j
t=1St such that Sv ≤ Si and
Sv starts with a.

Proof. Suppose that a suﬃx Sp from St, such that
j ≥ t ≥ 1, starts with a(cid:48) < a. Then Sp = a(cid:48)S(cid:48)p for
some S(cid:48)p ∈ St−1. By deﬁnition of the sequence B, there
is exactly one occurrence of a(cid:48) in B for every such S(cid:48)p.
Now suppose that a suﬃx Sv ∈ St, such that j ≥ t ≥ 1,
starts with a and Sv ≤ Si. Then Sv = aS(cid:48)v for S(cid:48)v ∈ St−1
and S(cid:48)v ≤ S(cid:48)i. For every such S(cid:48)v there is exactly one
occurrence of the symbol a in B[1..ti], where ti is the
position of S(cid:48)i in B.

The above calculation did not take into account the
suﬃxes from S0. We compute the number of suﬃxes
Sk ∈ S0 such that Sk < Si using the approach of
Step 0 − 1. Let T1 be the text obtained by rotating T
one symbol to the right. Let T (cid:48) be the text obtained
by rotating T j + 1 symbols to the right. We can
sort suﬃxes of S0 and Sj by concatenating T1 and T (cid:48),
viewing the resulting text T (cid:48)(cid:48) as a sequence of 2n/∆
meta-symbols and constructing the suﬃx array for T (cid:48)(cid:48).
When suﬃxes in S0 ∪ Sj are sorted, we traverse the
sorted list of suﬃxes; for every suﬃx Si ∈ Sj we know
the number qi of lexicographically smaller suﬃxes from
S0.
We then modify the sequence B: We sort new
suﬃxes Si by oi = pi + qi. Next we insert the symbol
T [i∆ − j − 1] at position oi − 1 in B (assuming the
ﬁrst index of B is B[0]); insertions are performed in
increasing order of oi. We will show that this procedure

3

410

Copyright © 2016
J. Ian Munro,  Gonzalo Navarroy, Yakov Nekrichz

Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpT1 = T [n − 1] . . . T [∆ − 2] T [∆ − 1] . . . T [2∆ − 2] T [2∆ − 1] . . . T [3∆ − 2] T [3∆ − 1] . . . . . .

T2 = T [n − 2] . . . T [∆ − 3] T [∆ − 2] . . . T [2∆ − 3] T [2∆ − 2] . . . T [3∆ − 3] T [3∆ − 2] . . . . . .

Figure 1: T1 and T2 as sequences of meta-symbols (shown in boxes).

also takes O(1) time per update for a large batch of
insertions. Finally we record the position of every new
suﬃx from Sj in the sequence B. Since the positions of
suﬃxes from Sj−1 are not needed any more, we use the
entry W [i] of W to store the position of T [i∆− j − 1..].
The array Acc is also updated.
When Step ∆ − 1 is completed, the sequence B
contains n symbols and B[i] is the symbol that precedes 
the (i + 1)-th smallest suﬃx of T . Thus we obtained 
the BWT of T . Step 0 of our algorithm uses
O((n/∆) log n) = O(n log σ) bits. For all the following
steps we need to maintain the sequence B and the array 
W . B uses O(log σ) bits per symbol and W needs
O((n/∆) log n) = O(n log σ) bits. Hence our algorithm
uses O(n log σ) bits of workspace. Procedures for querying 
and updating B are described in the following section.
 Our result can be summed up as follows.
Theorem 2.1. Given a string T [0..n − 1] over an alphabet 
of size σ, we can construct the BWT of T in
O(n) deterministic time using O(n log σ) bits.

3 Batched Rank Queries on a Sequence

n

In this section we show how a batch of m rank queries
log2 n ≤ m ≤ n can be answered in O(m) time on a
for
sequence B of length n. We start by describing a static
data structure. A data structure that supports batches
of queries and batches of insertions will be described
later. We will assume σ ≥ log4 n; if this is not the case,
the data structure from [14] can be used to answer rank
queries in time O(1).

Following previous work [17], we divide B into
chunks of size σ (except for the last chunk that contains
at most σ symbols). For every symbol a we keep
a binary sequence Ma = 1d101d2 0 . . . 1df where f is
the total number of chunks and di is the number of
occurrences of a in the chunk. We keep the following
information for every chunk C. Symbols in a chunk C
are represented as pairs (a, i): we store a pair (a, i) if
and only if C[i] = a. These pairs are sorted by symbols
and pairs representing the same symbol a are sorted
by their positions in C; all sorted pairs from a chunk
are kept in a sequence R. The array F consists of σ
entries; F [a] contains a pointer to the ﬁrst occurrence
of a symbol a in R (or null if a does not occur in C). Let

Ra denote the subsequence of R that contains all pairs
(a,·) for some symbol a. If Ra contains at least log2 n
pairs, we split Ra into groups Ha,r of size Θ(log2 n).
For every group, we keep its ﬁrst pair in the sequence
R(cid:48). Thus R(cid:48) is also a subsequence of R. For each pair
(a(cid:48), i(cid:48)) in R(cid:48) we also store the partial rank of C[i(cid:48)] in C,
rankC[i(cid:48)](i(cid:48), C).

All pairs in Ha,r are kept in a data structure Da,r
that contains the second components of pairs (a, i) ∈
Ha,r. Thus Da,r contains positions of Θ(log2 n) conIf 
Ra contains less than log2 n
secutive symbols a.
pairs, then we keep all pairs starting with symbol a
in one group Ha,0. Every Da,r contains O(log2 n) elements.
 Hence we can implement Da,r so that predecessor 
queries are answered in constant time:
for any
integer q, we can ﬁnd the largest x ∈ Ha,r satisfying
x ≤ q in O(1) time [16]. We can also ﬁnd the number of
elements x ∈ Ha,r satisfying x ≤ q in O(1) time. This
operation on Ha,r can be implemented using bit techniques 
similar to those suggested in [34]; details are to
be given in the full version of this paper.

Queries on a Chunk. Now we are ready to
answer a batch of queries in O(1) time per query.
First we describe how queries on a chunk can be
answered. Answering a query ranka(i, C) on a chunk
C is equivalent to counting the number of pairs (a, j)
in R such that j ≤ i. Our method works in three
steps. We start by sorting the sequence of all queries
on C. Then we “merge” the sorted query sequence
with R(cid:48). That is, we ﬁnd for every ranka(i, C) the
rightmost pair (a, j(cid:48)) in R(cid:48), such that j(cid:48) ≤ i. Pair (a, j(cid:48))
provides us with an approximate answer to ranka(i, C)
(up to an additive O(log2 n) term). Then we obtain
the exact answer to each query by searching in some
data structure Da,j. Since Da,j contains only O(log2 n)
elements, the search can be completed in O(1) time. A
more detailed description follows.

answer

v

. . .,

that we must
ranka2 (i2, C),

queries
Suppose
ranka1(i1, C),
rankav (iv, C) on
a chunk C. We sort the sequence of queries by pairs
(aj, ij) in increasing order. This sorting step takes
O(σ/ log2 n + v) time, where v is the number of queries:
if v < σ/ log3 n, we sort in O(v log n) = O(σ/ log2 n)
time; if v ≥ σ/ log3 n, we sort in O(v) time using radix

411

Copyright © 2016
J. Ian Munro,  Gonzalo Navarroy, Yakov Nekrichz

Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpsort (e.g., with radix √σ). Then we simultaneously
traverse the sorted sequence of queries and R(cid:48); for each
query pair (aj, ij) we identify the pair (at, pt) in R(cid:48)
such that either (i) pt ≤ ij ≤ pt+1 and aj = at = at+1
or (ii) pt ≤ ij, aj = at, and at
(cid:54)= at+1. That is,
we ﬁnd the largest pt ≤ ij such that (aj, pt) ∈ R(cid:48)
for every query pair (aj, ij).
If (at, pt) is found, we
search in the group Hat,pt that starts with the pair
If the symbol aj does not occur in R(cid:48), then
(at, pt).
we search in the leftmost group Haj ,0. Using Dat,pt
(resp. Dat,0), we ﬁnd the largest position xt ∈ Hat,pt
such that xt ≤ ij. Thus xt is the largest position in
C satisfying xt ≤ ij and C[xt] = aj. We can then
compute rankat(xt, C) as follows: Let n1 be the partial
rank of C[pt], n1 = rankC[pt](pt, C). Recall that we
explicitly store this information for every position in
R(cid:48). Let n2 be the number of positions i ∈ Hat,pt
satisfying i ≤ xt. We can compute n2 in O(1) time
using Dat,pt . Then rankaj (xt, C) = n1 + n2. Since
C[xt] is the rightmost occurrence of aj up to C[ij],
rankaj (ij, C) = rankaj (xt, C). The time needed to
is O(σ/ log2 n) for all the
traverse the sequence R(cid:48)
queries. Other computations take O(1) time per query.
Hence the sequence of v queries on a chunk is answered
in O(v + σ/ log2 n) time.

Global Sequence. Now we consider the global
sequence of queries ranka1(i1, B), . . ., rankam(im, B).
First we assign queries to chunks (e.g., by sorting all
queries by ((cid:98)i/σ(cid:99) + 1) using radix sort). We answer the
batch of queries on the j-th chunk in O(mj + σ/ log2 n)
time where mj is the number of queries on the j-th

chunk. Since (cid:80) mj = m, all m queries are answered

in O(m + n/ log2 n) = O(m) time. Now we know the
rank nj,2 = rankaj (i(cid:48)j, C), where i(cid:48)j = ij − (cid:98)i/σ(cid:99)σ is the
relative position of B[ij] in its chunk C.
The binary sequences Ma allows us reduce rank
queries on B to rank queries on a chunk C. All
sequences Ma contain n + (cid:98)n/σ(cid:99)σ bits; hence they use
O(n) bits of space. We can compute the number of
occurrences of a in the ﬁrst j chunks in O(1) time
by answering one select query. Consider a rank query
rankaj (ij, B) and suppose that nj,2 is already known.
We compute nj,1, where nj,1 = select0((cid:98)ij/σ(cid:99), Maj ) −
(cid:98)ij/σ(cid:99) is the number of times aj occurs in the ﬁrst
(cid:98)ij/σ(cid:99) chunks. Then we compute rankaj (ij, B) = nj,1 +
nj,2.
Theorem 3.1. We can keep a sequence B[0..n−1] over
an alphabet of size σ in O(n log σ) bits of space so that a
batch of m rank queries can be answered in O(m) time,
where

n

log2 n ≤ m ≤ n.

The static data structure of Theorem 3.1 can be dynamized 
so that batched queries and batched insertions

are supported. Our dynamic data structures supports
a batch of m queries in time O(m) and a batch of m
insertions in amortized time O(m) for any m that satlogσ 
n ≤ m ≤ n. We describe the dynamic data
isﬁes
structure in Sections A.2 and A.3.

n

4 Building the Suﬃx Tree

Belazzougui proved the following result [1]:
if we are
given the BWT B of a text T and if we can report all the
distinct symbols in a range of B in optimal time, then
in O(n) time we can: (i) enumerate all the suﬃx array
intervals corresponding to internal nodes of the suﬃx
tree and (ii) for every internal node list the labels of its
children and their intervals. Further he showed that, if
we can enumerate all the suﬃx tree intervals in O(n)
time, then we can build the suﬃx tree topology [39] in
O(n) time. The algorithms need only O(n) additional
bits of space. We refer to Lemmas 4 and 1 and their
proofs in [1] for details.

In Section A.4 we show that a partial rank data
structure can be built in O(n) deterministic time. This
can be used to build the desired structure that reports
the distinct symbols in a range, in O(n) time and using
O(n log log σ) bits. The details are given in Section A.5.
Therefore, we obtain the following result.

Lemma 4.1. If we already constructed the BWT of a
text T , then we can build the suﬃx tree topology in O(n)
time using O(n log log σ) additional bits.

In Section 5 we show that the permuted LCP array
of T can be constructed in O(n) time using O(n log σ)
bits of space. Thus we obtain our main result on
building compressed suﬃx trees.
Theorem 4.1. Given a string T [0..n − 1] over an alphabet 
of size σ, we can construct the compressed suﬃx
tree of T in O(n) deterministic time using O(n log σ)
additional bits.

5 Constructing the Permuted LCP Array

The permuted LCP array is deﬁned as P LCP [i] = j if
and only if SA[r] = i and the longest common preﬁx
of T [SA[r]..] and T [SA[r − 1]..] is of length j. In other
words P LCP [i] is the length of the longest common
preﬁx of T [i..] and the suﬃx that precedes it in the
lexicographic ordering. In this section we show how the
permuted LCP array P LCP [0..n − 1] can be built in
linear time.
Preliminaries. For i = 0, 1, . . . , n let (cid:96)i =
P LCP [i]. It is easy to observe that (cid:96)i ≤ (cid:96)i+1 + 1:
if
the longest common preﬁx of T [i..] and T [j..] is q, then
the longest common preﬁx of T [i + 1..] and T [j + 1..]
is at least q − 1. Let ∆(cid:48) = ∆ log log σ for ∆ = logσ n.

5

412

Copyright © 2016
J. Ian Munro,  Gonzalo Navarroy, Yakov Nekrichz

Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpBy the same argument (cid:96)i ≤ (cid:96)i+∆(cid:48) + ∆(cid:48). To simplify the
description we will further assume that (cid:96)−1 = 0. It can

also be shown that(cid:80)n−1

i=0 ((cid:96)i − (cid:96)i−1) = O(n).

We will denote by B the BWT sequence of T ; B
denotes the BWT of the reversed text T = T [n−1]T [n−
2] . . . T [1]T [0]. Let p be a factor (substring) of T and
let c be a character. The operation extendright(p, c)
computes the suﬃx interval of pc in B and the suﬃx
interval of pc in B provided that the intervals of p
and p are known. The operation contractleft(cp)
computes the suﬃx intervals of p and p provided that
the suﬃx intervals of factors cp and cp are known2. It
was demonstrated [41, 4] that both operations can be
supported by answering O(1) rank queries on B and B.
Belazzougui [1] proposed the following algorithm
for consecutive computing of (cid:96)0, (cid:96)1, . . ., (cid:96)n. Suppose
that (cid:96)i−1 is already known. We already know the rank
ri−1 of T [i − 1..], the interval of T [i − 1..i + (cid:96)i−1 − 1]
in B, and the interval of T [i − 1..i + (cid:96)i−1 − 1] in B.
We compute the rank ri of T [i..].
If ri−1 is known,
we can compute ri
in O(1) time by answering one
select query on B; see Section A.1. Then we ﬁnd
the interval
in B and
the interval [r(cid:48)s, r(cid:48)e] of T [i..i + (cid:96)i−1 − 1] in B. These
In
two intervals can be computed by contractleft.
the special case when i = 0 or (cid:96)i−1 = 0, we set
[rs, re] = [r(cid:48)s, r(cid:48)e] = [0, n − 1]. Then for j = 1, 2, . . .
we ﬁnd the intervals for T [i..i + ((cid:96)i−1 − 1) + j] and
T [i..i + ((cid:96)i−1 − 1) + j]. Every following pair of intervals
is found by operation extendright. We stop when the
interval of T [i..i + (cid:96)i−1 − 1 + j] is [rs,j, re,j] such that
rs,j = ri. For all j(cid:48), such that 0 ≤ j(cid:48) < j, we have
rs,j(cid:48) < ri. It can be shown that (cid:96)i = (cid:96)i−1 + j − 1; see
the proof of [1, Lemma 2]. Once (cid:96)i is computed, we
increment i and ﬁnd the next (cid:96)i in the same way. All (cid:96)i
are computed by O(n) contractleft and extendright
operations.

[rs, re] of T [i..i + (cid:96)i−1 − 1]

Implementing contractleft and extendright.
We create the succinct representation of the suﬃx tree
topology both for T and T ; they will be denoted by
T and T respectively. We keep both B and B in the
data structure that supports access in O(1) time. We
also store B in the data structure that answers select
queries in O(1) time. The array Acc keeps information
about accumulated frequencies of symbols: Acc[i] is
the number of occurrences of all symbols a ≤ i − 1
in B. Operation contractleft is implemented as
follows. Suppose that we know the interval [i, j] for
a factor cp and the interval [i(cid:48), j(cid:48)] for the factor cp.
We can compute the interval [i1, j1] of p by ﬁnding l =

2Throughout this paper reverse strings are overscored. Thus p

and pc are reverse strings of p and pc respectively.

selectc(i−Acc[c], B) and r = selectc(j−Acc[c], B). Then
we ﬁnd the lowest common ancestor x of leaves l and
r in the suﬃx tree T . We set i1 = leftmost leaf(x)
and j1 = rightmost leaf(x). Then we consider the
number of distinct symbols in B[i1..j1].
If c is the
only symbol that occurs in B[i1..j1], then all factors
p in T are preceded by c. Hence all factors p in T
are followed by c and [i(cid:48)1, j(cid:48)1] = [i(cid:48), j(cid:48)]. Otherwise we
ﬁnd the lowest common ancestor y of leaves i(cid:48) and j(cid:48)
in T . Then we identify y(cid:48) = parent(y) in T and let
i(cid:48)1 = leftmost leaf(y(cid:48)) and j(cid:48)1 = rightmost leaf(y(cid:48)).
Thus contractleft can be supported in O(1) time.

Now we consider the operation extendright. Suppose 
that [i, j] and [i(cid:48), j(cid:48)] are intervals of p and p in
B and B respectively. We compute the interval of
pc by using the standard BWT machinery. Let i(cid:48)1 =
rankc(i(cid:48)−1, B)+Acc[c] and j(cid:48)1 = rankc(j(cid:48), B)+Acc[c]−1.
We check whether c is the only symbol in B[i(cid:48)..j(cid:48)]. If
this is the case, then all occurrences of p in T are preceded 
by c and all occurrences of p in T are followed
by c. Hence the interval of pc in B is [i1, j1] = [i, j].
Otherwise there is at least one other symbol besides c
that can follow p. Let x denote the lowest common ancestor 
of leaves i and j.
If y is the child of x that is
labeled with c, then the interval of pc is [i1, j1] where
i1 = leftmost leaf(y) and j1 = rightmost leaf(y).

We can ﬁnd the child y of x that is labeled with c
by answering rank and select queries on two additional
sequences, L and D. The sequence L contains labels
of children for all nodes of T ;
labels are ordered
by nodes and labels of the same node are ordered
lexicographically. We encode the degrees of all nodes in
a sequence D = 1d1 01d20 . . . 1dn , where di is the degree
of the i-th node. We compute v = select0(x, D) − x,
p1 = rankc(v, L), p2 = selectc(p1 + 1, L), and j = p2− v.
Then y is the j-th child of x. The bottleneck of
extendright are the computations of p1, i(cid:48)1, and j(cid:48)1
because we need Ω(log log σ
log log n ) time to answer a rank
query on L (resp. on B); all other calculations can be
executed in O(1) time.

Our Approach. Our algorithm follows the technique 
of [1] that relies on operations extendright and
contractleft for building the PLCP. We implement
these two operations as described above; hence we will
have to perform Θ(n) rank queries on sequences L and
B. Our method creates large batches of queries; each
query in a batch is answered in O(1) time using Theorem 
3.1.

During the pre-processing stage we create the machinery 
for supporting operations extendright and
contractleft. We compute the BWT B of T and the
BWT B for the reverse text T . We also construct the
suﬃx tree topologies T and T . When B is constructed,

413

Copyright © 2016
J. Ian Munro,  Gonzalo Navarroy, Yakov Nekrichz

Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpwe record the positions in B that correspond to suﬃxes
T [i · ∆(cid:48)..] for i = 0, . . . ,(cid:98)n/∆(cid:48)(cid:99). PLCP construction is
divided into three stages: ﬁrst we compute the values
of (cid:96)i for selected evenly spaced indices i, i = j · ∆(cid:48) and
j = 0, 1,. . .,(cid:98)n/∆(cid:48)(cid:99). We use a slow algorithm for computing 
lengths that takes O(∆(cid:48)) extra time for every
(cid:96)i. During the second stage we compute all remaining
values of (cid:96)i. We use the method from [1] during Stage
2. The key to a fast implementation is “parallel” computation.
 We divide all lengths into groups and assign
each group of lengths to a job. At any time we process
a list containing at least 2n/ log2 n jobs. We answer
rank queries in batches: when a job Ji must answer a
slow rank query on L or B, we pause Ji and add the
rank query to the corresponding pool of queries. When
a pool of queries on L or the pool of queries on B contains 
n/ log2 n items, we answer the batch of queries
in O(n/ log2 n) time. The third stage starts when the
number of jobs becomes smaller than 2n/ log2 n. All
lengths that were not computed earlier are computed
during Stage 3 using the slow algorithm. Stage 2 can
be executed in O(n) time because rank queries are answered 
in O(1) time per query. Since the number of
lengths that we compute during the ﬁrst and the third
stages is small, Stage 1 and Stage 3 also take time O(n).
A more detailed description follows.

Stage 1. Our algorithm starts by computing (cid:96)i for
i = j · ∆(cid:48) and j = 0, 1, . . . ,(cid:98)n/∆(cid:48)(cid:99). Let j = 0 and
f = j∆(cid:48). We already know the rank rf of Sf = T [j∆(cid:48)..]
in B (rf was computed and recorded when B was
constructed). We can also ﬁnd the starting position f(cid:48) of
the suﬃx S(cid:48) of rank rf − 1, S(cid:48) = T [f(cid:48)..]. Since f(cid:48) can be
found by employing the function LF at most ∆(cid:48) times,
we can compute f(cid:48) in O(∆(cid:48)) time; see Section A.13.
When f and f(cid:48) are known, we scan T [f..] and T [f(cid:48)..]
until the ﬁrst symbol T [f + pf ] (cid:54)= T [f(cid:48) + pf ] is found.
By deﬁnition of (cid:96)j, (cid:96)0 = pf − 1. Suppose that (cid:96)s∆(cid:48)
. . ., j − 1 are already computed and we
for s = 0,
have to compute (cid:96)f for f = j∆(cid:48) and some j ≥ 1. We
already know the rank rf of suﬃx T [f..]. We ﬁnd f(cid:48) such
that the suﬃx T [f(cid:48)..] is of rank rf − 1 in time O(∆(cid:48)).
We showed above that (cid:96)f ≥ (cid:96)(j−1)∆(cid:48) − ∆(cid:48). Hence the
ﬁrst of symbols in T [f..] and T [f(cid:48)..] are equal, where
of = max(0, (cid:96)(j−1)∆(cid:48) − ∆(cid:48)). We scan T [f + of ..] and
T [f(cid:48) + of ..] until the ﬁrst symbol T [f + of + pf ]
(cid:54)=
T [f(cid:48) + of + pf ] is found. By deﬁnition, (cid:96)f = of + pf .
Hence we compute (cid:96)f in O(∆(cid:48)+pf ) time for f = j∆(cid:48) and
f pf = O(n).
Hence the total time needed to compute all selected (cid:96)f
f pf ) = O(n). For every f = j∆(cid:48)
we also compute the interval of T [j∆(cid:48)..j∆(cid:48) + (cid:96)f ] in B

j = 1, . . ., (cid:98)n/∆(cid:48)(cid:99). It can be shown that(cid:80)
is O((n/∆(cid:48))∆(cid:48) +(cid:80)

3A faster computation is possible, but we do not need it here.

Figure 2: Computing lengths during Stage 2. Groups
corresponding to paused jobs are shown shaded by
slanted lines. Only selected groups are shown. The i-th
job Ji is paused because we have to answer a rank query
on B; the job J1 is paused because we have to answer
a rank query on L. When Ql or Qb contains n/ log2 n
queries, we answer a batch of rank queries contained in
Ql or Qb.

and the interval of T [j∆(cid:48)..j∆(cid:48) + (cid:96)f ] in B. We show in
Section A.6 that all needed intervals can be computed
in O(n) time.

Stage 2. We divide (cid:96)i into groups of size ∆(cid:48) − 1
and compute the values of (cid:96)k in every group using a
job. The i-th group contains lengths (cid:96)k+1, (cid:96)k+2, . . .,
(cid:96)k+∆(cid:48)−1 for k = i∆(cid:48) and i = 0, 1, . . .. All (cid:96)k in the i-th
group will be computed by the i-th job Ji. Every Ji is
either active or paused. Thus originally we start with a
list of n/∆(cid:48) jobs and all of them are active. All active
jobs are executed at the same time. That is, we scan the
list of active jobs, spend O(1) time on every active job,
and then move on to the next job. When a job must
answer a rank query, we pause it and insert the query
into a query list. There are two query lists: Ql contain
rank queries on sequence L and Qb contains rank queries
on B. When Ql or Qb contains n/ log2 n queries, we
answer all queries in Ql (resp. in Qb). The batch of
queries is answered using Theorem 3.1, so that every
query is answered in O(1) time. Answers to queries are
returned to jobs, corresponding jobs are re-activated,
and we continue scanning the list of active jobs. When
all (cid:96)k for i∆(cid:48) ≤ k < (i + 1)∆(cid:48) are computed, the i-th
job is ﬁnished; we remove this job from the pool of jobs
and decrement by 1 the number of jobs. See Fig. 2.

Every job Ji computes (cid:96)k+1, (cid:96)k+2,

. . ., (cid:96)k+∆(cid:48)−1
for k = i∆(cid:48) using the algorithm of Belazzougui [1].
When the interval of T [i + (cid:96)k..] in B and the interval 
of T [i + (cid:96)k..] in B are known, we compute (cid:96)k+1.
The procedure for computing (cid:96)k+1 must execute one
operation contractleft and (cid:96)k+1 − (cid:96)k + 1 operations 
extendright. Operations contractleft and

7

414

Copyright © 2016
J. Ian Munro,  Gonzalo Navarroy, Yakov Nekrichz

∆0∆0∆0J0J1Ji‘0‘1...‘∆0−1QlQbJ1:rank(j1,a1)Ji:rank(ji,ai)Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpextendright are implemented as described above. We
must answer two rank queries on B and one rank query
on L for every extendright. Ignoring the time for these
three rank queries, extendright takes constant time.
Rank queries on B and L are answered in batches, so
that each rank query takes O(1) time. Hence every operation 
extendright needs O(1) time. The job Ji needs
O((cid:96)i∆(cid:48)+j − (cid:96)i∆(cid:48) + j) time to compute (cid:96)i∆(cid:48)+1, (cid:96)i∆(cid:48)+1, . . .,
(cid:96)i∆(cid:48)+j. All Ji are executed in O(n) time.
Stage 3. “Parallel processing” of jobs terminates
when the number of jobs in the pool becomes smaller
than 2n/ log2 n. Since every job computes ∆(cid:48) values
of (cid:96)i, there are at most 2n(log log σ/(log n log σ)) <
2n/ log n unknown values of (cid:96)i at this point. We then
switch to the method of Stage 1 to compute the values
of unknown (cid:96)i. All remaining (cid:96)i are sorted by i and
processed in order of increasing i. For every unknown
(cid:96)i we compute the rank r of T [i..] in B. For the suﬃx
S(cid:48) of rank r − 1 we ﬁnd its starting position f(cid:48) in T ,
S(cid:48) = T [f(cid:48)..]. Then we scan T [f(cid:48) + (cid:96)i−1 − 1..] and
T [i+(cid:96)i−1−1..] until the ﬁrst symbol T [f(cid:48)+(cid:96)i−1+j−1] (cid:54)=
T [f + (cid:96)i−1 + j − 1] is found. We set (cid:96)i = (cid:96)i−1 + j − 2 and
continue with the next unknown (cid:96)i. We spend O(∆(cid:48)+(cid:96)i)
additional time for every remaining (cid:96)i; hence the total
time needed to compute all (cid:96)i is O(n + (n/ log n)∆(cid:48)) =
O(n).

Every job during Stage 2 uses O(log n) bits of
workspace. The total number of jobs in the job list does
not exceed n/∆(cid:48). The total number of queries stored at
any time in lists Ql and Qb does not exceed n/ log2 n.
Hence our algorithm uses O(n log σ) bits of workspace.

Lemma 5.1. If the BWT of a string T and the suﬃx
tree topology for T are already known, then we can
compute the permuted LCP array in O(n) time and
O(n log σ) bits.

6 Conclusions

We have shown that the Burrows-Wheeler Transform
(BWT), the Compressed Suﬃx Array (CSA), and the
Compressed Suﬃx Tree (CST) can be built in deterministic 
O(n) time by an algorithm that requires O(n log σ)
bits of working space. Belazzougui independently developed 
an alternative solution, which also builds within
the same time and space the simpler part of our structures,
 that is, the BWT and the CSA, but not the CST.
His solution, that uses diﬀerent techniques, is described
in the updated version of his ArXiV report [2] that extends 
his conference paper [1].

Our results have many interesting applications.
For example, we can now construct an FM-index [13,
14] in O(n) deterministic time using O(n log σ) bits.
Previous results need O(n log log σ) time or rely on

randomization [21, 1]. Furthermore Theorem A.4.1
enables us to support the function LF in O(1) time on
an FM-index. In the extended version of this paper [29]
we also describe a new index based on these ideas.

Another application is that we can now compute the
Lempel-Ziv 77 and 78 parsings [27, 45, 46] of a string
T [0..n − 1] in deterministic linear time using O(n log σ)
bits: K¨oppl and Sadakane [25] recently showed that, if
one has a compressed suﬃx tree on T , then they need
only O(n) additional (deterministic) time and O(z log n)
bits to produce the parsing, where z is the resulting
number of phrases. Since z ≤ n/ logσ n, the space
is O(n log σ) bits. With the suﬃx tree, they need to
compute in constant time any Ψ(i) and to move in
constant time from a suﬃx tree node to its i-th child.
The former is easily supported as the inverse of the LF
function using constant-time select queries on B [17];
the latter is also easily obtained with current topology
representations using parentheses [35].

Yet another immediate application of our algorithm
are index data structures for dynamic document collections.
 If we use our compressed index, described in the
extended version of this paper [29], and apply Transformation 
2 from [31], then we obtain an index data
structure for a dynamic collection of documents that
uses nHk + o(n log σ) + O(n log n
s ) bits where Hk is the
k-th order entropy and s is a parameter. This index can
count how many times a query pattern P occurs in a collection 
in O(|P| log log n+log log σ log log n) time; every
occurrence can be then reported in time O(s). An insertion 
or a deletion of some document Tu is supported
in O(|Tu| logε n) and O(|Tu|(logε n + s)) deterministic
time respectively.
We believe that our technique can also improve
upon some of the recently presented results on bidirectional 
FM-indices [41, 4] and other scenarios where
compressed suﬃx trees are used [5].

Acknowledgment. The authors wish to thank an
anonymous reviewer of this paper for careful reading
and helpful comments.

References

[1] D. Belazzougui. Linear time construction of compressed 
text indices in compact space. In Proc. Symposium 
on Theory of Computing (STOC), pages 148–193,
2014.

[2] D. Belazzougui. Linear time construction of comCoRR,


in compact

pressed text
indices
abs/1401.0936, 2014.

space.

[3] D. Belazzougui, P. Boldi, R. Pagh, and S. Vigna.
Monotone minimal perfect hashing: searching a sorted
table with o(1) accesses. In Proc. 20th Annual ACM415


Copyright © 2016
J. Ian Munro,  Gonzalo Navarroy, Yakov Nekrichz

Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpSIAM Symposium on Discrete Algorithms (SODA),
pages 785–794, 2009.

[4] D. Belazzougui, F. Cunial, J. K¨arkk¨ainen, and
V. M¨akinen. Versatile succinct representations of
the bidirectional Burrows-Wheeler transform.
In
Proc. 21st Annual European Symposium on Algorithms
(ESA), pages 133–144, 2013.

[5] D. Belazzougui, F. Cunial, J. K¨arkk¨ainen, and
V. M¨akinen. Linear-time string indexing and analysis 
in small space. CoRR, abs/1609.06378, 2016.

[6] D. Belazzougui and G. Navarro. Alphabet-independent
compressed text indexing. ACM Transactions on
Algorithms, 10(4):23:1–23:19, 2014.

[7] D. Belazzougui and G. Navarro. Optimal lower and
upper bounds for representing sequences. ACM Transactions 
on Algorithms, 11(4):31:1–31:21, Apr. 2015.

[8] D. Belazzougui, G. Navarro, and D. Valenzuela. Improved 
compressed indexes for full-text document retrieval.
 Journal of Discrete Algorithms, 18:3–13, 2013.
[9] D. Belazzougui and S. J. Puglisi. Range predecessor
and Lempel-Ziv parsing. In Proc. 27th Annual ACMSIAM 
Symposium on Discrete Algorithms (SODA),
pages 2053–2071, 2016.

[10] M. Bender, R. Cole, E. Demaine, M. Farach-Colton,
and J. Zito. Two simpliﬁed algorithms for maintaining 
order in a list.
In Proc. 10th Annual European
Symposium on Algorithms (ESA), LNCS 2461, pages
152–164, 2002.

[11] P. Dietz and D. Sleator. Two algorithms for maintaining 
order in a list. In Proc. 19th Annual ACM Symposium 
on Theory of Computing (STOC), pages 365–372,
1987.

[12] M. Farach. Optimal suﬃx tree construction with
large alphabets. In Proc. 38th Annual Symposium on
Foundations of Computer Science (FOCS), pages 137–
143, 1997.

[13] P. Ferragina and G. Manzini.

Indexing compressed

text. Journal of the ACM, 52(4):552–581, 2005.

[14] P. Ferragina, G. Manzini, V. M¨akinen, and G. Navarro.
Compressed representations of sequences and full-text
indexes. ACM Transactions on Algorithms, 3(2):article
20, 2007.

[15] M. Fredman and M. Saks. The cell probe complexity of
dynamic data structures. In Proc. 21st Annual ACM
Symposium on Theory of Computing (STOC), pages
345–354, 1989.

[16] M. L. Fredman and D. E. Willard. Trans-dichotomous
algorithms for minimum spanning trees and shortest
paths. Journal of Computing and System Sciences,
48(3):533–551, 1994.

[17] A. Golynski, J. I. Munro, and S. S. Rao. Rank/select
operations on large alphabets: a tool for text indexing.
 In Proc. 17th Annual ACM-SIAM Symposium on
Discrete Algorithms, (SODA), pages 368–373, 2006.

[18] R. Grossi, A. Orlandi, R. Raman, and S. S. Rao. More
haste, less waste: Lowering the redundancy in fully indexable 
dictionaries. In Proc. 26th International Symposium 
on Theoretical Aspects of Computer Science

(STACS), pages 517–528, 2009.

[19] R. Grossi and J. S. Vitter. Compressed suﬃx arrays
and suﬃx trees with applications to text indexing
and string matching. SIAM Journal on Computing,
35(2):378–407, 2005.

[20] T. Hagerup, P. B. Miltersen, and R. Pagh. Deterministic 
dictionaries. Journal of Algorithms, 41(1):69 – 85,
2001.

[21] W. Hon, K. Sadakane, and W. Sung. Breaking a
time-and-space barrier in constructing full-text indices.
SIAM Journal on Computing, 38(6):2162–2178, 2009.
[22] J. K¨arkk¨ainen, P. Sanders, and S. Burkhardt. Linear
work suﬃx array construction. Journal of the ACM,
53(6):918–936, 2006.

[23] D. K. Kim, J. S. Sim, H. Park, and K. Park. Constructing 
suﬃx arrays in linear time. Journal of Discrete 
Algorithms, 3(2-4):126–142, 2005.

[24] P. Ko and S. Aluru.

construction of suﬃx arrays.
Algorithms, 3(2-4):143–156, 2005.

Space eﬃcient linear time
Journal of Discrete

[25] D. K¨oppl and K. Sadakane. Lempel-Ziv computation
In Proc. 26th Data

in compressed space (LZ-CICS).
Compression Conference (DCC), pages 3–12, 2016.

[26] S. Lee and K. Park. Dynamic rank-select structures
with applications to run-length encoded texts. In Proc.
18th Annual Symposium on Combinatorial Pattern
Matching ( CPM), pages 95–106, 2007.

[27] A. Lempel and J. Ziv. On the complexity of ﬁnite
sequences. IEEE Transactions on Information Theory,
22(1):75–81, 1976.

[28] E. M. McCreight. A space-economical suﬃx tree construction 
algorithm. Journal of the ACM, 23(2):262–
272, 1976.

[29] J. I. Munro, G. Navarro, and Y. Nekrich.

Spaceeﬃcient 
construction of compressed indexes in deterministic 
linear time. CoRR, abs/1607.04346, 2016.

[30] J. I. Munro, Y. Nekrich, and J. S. Vitter. Fast construction 
of wavelet trees. In Proc. 21st International
Symposium on String Processing and Information Retrieval 
(SPIRE), pages 101–110, 2014.

[31] J. I. Munro, Y. Nekrich, and J. S. Vitter. Dynamic 
data structures for document collections and
graphs. In Proc. 34th ACM Symposium on Principles
of Database Systems (PODS), pages 277–289, 2015.

[32] G. Navarro and V. M¨akinen. Compressed full-text
indexes. ACM Computing Surveys, 39(1):article 2,
2007.

[33] G. Navarro and Y. Nekrich. Top-k document retrieval
in optimal time and linear space.
In Proc. 23rd Annual 
ACM-SIAM Symposium on Discrete Algorithms
(SODA), pages 1066–1077, 2012.

[34] G. Navarro and Y. Nekrich. Optimal dynamic sequence
representations.
In Proc. 24th Annual ACM-SIAM
Symposium on Discrete Algorithms (SODA), pages
865–876. SIAM, 2013.

[35] G. Navarro and K. Sadakane. Fully-functional static
and dynamic succinct trees. ACM Transactions on
Algorithms, 10(3):article 16, 2014.

9

416

Copyright © 2016
J. Ian Munro,  Gonzalo Navarroy, Yakov Nekrichz

Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php[36] D. Okanohara and K. Sadakane.

A linear-time
burrows-wheeler transform using induced sorting.
In
Proc. 16th International Symposium String Processing 
and Information Retrieval (SPIRE), pages 90–101,
2009.

[37] M. Ruˇzi´c. Constructing eﬃcient dictionaries in close to
sorting time. In Proc. 35th International Colloquium
on Automata, Languages and Programming (ICALP),
pages 84–95, 2008.

[38] K. Sadakane. New text indexing functionalities of
the compressed suﬃx arrays. Journal of Algorithms,
48(2):294–313, 2003.

[39] K. Sadakane. Compressed suﬃx trees with full functionality.
 Theory of Computing Systems, 41(4):589–
607, 2007.

[40] K. Sadakane.

Succinct data structures for ﬂexible
text retrieval systems. Journal of Discrete Algorithms,
5(1):12–22, 2007.

[41] T. Schnattinger, E. Ohlebusch, and S. Gog. Bidirectional 
search in a string with wavelet trees and bidirectional 
matching statistics. Inf. Comput., 213:13–22,
2012.

[42] E. Ukkonen. On-line construction of suﬃx trees.

Algorithmica, 14(3):249–260, 1995.

[43] P. Weiner. Linear pattern matching algorithms.

In
Proc. 14th Annual Symposium on Switching and Automata 
Theory (FOCS), pages 1–11, 1973.

[44] D. E. Willard. A density control algorithm for doing
insertions and deletions in a sequentially ordered ﬁle in
good worst-case time. Information and Computation,
97(2):150–204, 1992.

[45] J. Ziv and A. Lempel. A universal algorithm for
IEEE Transactions on

sequential data compression.
Information Theory, 23(3):337–343, 1977.

[46] J. Ziv and A. Lempel. Compression of individual sequences 
via variable length coding. IEEE Transactions
on Information Theory, 24(5):530–536, 1978.

A.1 Preliminaries

Rank and Select Queries The following two
kinds of queries play a crucial role in compressed indexes
and other succinct data structures. Consider a sequence
B[0..n − 1] of symbols over an alphabet of size σ. The
rank query ranka(i, B) counts how many times a occurs
among the ﬁrst i + 1 symbols in B, ranka(i, B) =
|{ j | B[j] = a and 0 ≤ j < i}|. The select query
selecta(i, B) ﬁnds the position in B where a occurs for
the i-th time, selecta(i, B) = j where j is such that
B[j] = a and ranka(j, B) = i. The third kind of
query is the access query, access(i, B), which returns the
(i + 1)-th symbol in B, B[i]. If insertions and deletions
of symbols in B must be supported, then both kinds
of queries require Ω(log n/ log log n) time [15].
If the
sequence B is static, then we can answer select queries
in O(1) time and the cost of rank queries is reduced to

Θ(log log σ
log log n ) [7].4 One important special case of rank
queries is the partial rank query, rankB[i](i, B). Thus a
partial rank query asks how many times B[i] occurred in
B[0..i]. Unlike general rank queries, partial rank queries
can be answered in O(1) time [7].
In Section A.4 we
describe a data structure for partial rank queries that
can be constructed in O(n) deterministic time. Better
results can be achieved in the special case when the
alphabet size is σ = logO(1) n;
in this case we can
represent B so that rank, select, and access queries are
answered in O(1) time [14].

Suﬃx Tree and Suﬃx Array. A suﬃx tree for
a string T [0..n − 1] is a compacted tree on the suﬃxes
of T . The suﬃx array is an array SA[0..n − 1] such
that SA[i] = j if and only if T [j..] is the (i + 1)-th
lexicographically smallest suﬃx of T . All occurrences
of a substring p in T correspond to suﬃxes of T that
start with p; these suﬃxes occupy a contiguous interval
in the suﬃx array SA.

Compressed Suﬃx Array. A compressed suﬃx
array (CSA) is a compact data structure that provides
the same functionality as the suﬃx array. The main
component of CSA is the function Ψ, deﬁned by the
equality SA[Ψ(i+1)] = (SA[i]+1) mod n. It is possible
to regenerate the suﬃx array from Ψ. We refer to [32]
and references therein for a detailed description of CSA
and for trade-oﬀs between space usage and access time.
Burrows-Wheeler Transform and FM-index.
The Burrows-Wheeler Transform (BWT) of a string T
is obtained by sorting all possible rotations of T and
writing the last symbol of every rotation (in sorted
order). The BWT is related to the suﬃx array as
follows: BW T [i] = T [(SA[i] − 1) mod n]. Hence, we
can build the BWT by sorting the suﬃxes and writing
the symbols that precede the suﬃxes in lexicographical
order. This method is used in Section 2.

The FM-index uses the BWT for eﬃcient searching
in T . It consists of the following three main components:

• The BWT of T .
• The array Acc[0..σ− 1] where Acc[i] holds the total
number of symbols a ≤ i − 1 in T (or equivalently,
the total number of symbols a ≤ i − 1 in B).

• A sampled array SAMb for a sampling factor b:
SAMb contains values of SA[i] if and only if SA[i]
mod b = 0 or SA[i] = n − 1.

4If we aim to use n log σ + o(n log σ) bits, then either select or
access must cost ω(1). If, however, (1+)n log σ bits are available,
for any constant  > 0, then we can support both queries in O(1)
time.

417

Copyright © 2016
J. Ian Munro,  Gonzalo Navarroy, Yakov Nekrichz

Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpThe search for a substring P of length m is performed 
backwards: for i = m − 1, m − 2, . . ., we identify 
the interval of p[i..m] in the BWT. Let B denote
the BWT of T . Suppose that we know the interval
B[i1..j1] that corresponds to p[i + 1..m − 1]. Then
the interval B[i2..j2] that corresponds to p[i..m − 1]
is computed as i2 = rankc(i1 − 1, B) + Acc[c] and
j2 = rankc(i2, B) + Acc[c] − 1, where c = P [i]. Thus
the interval of p is found by answering 2m rank queries.
We observe that the interval of p in B is exactly the
same as the interval of p in the suﬃx array SA.

Another important component of an FM-index is
if SA[j] = i + 1,
the function LF , deﬁned as follows:
then SA[LF (j)] = i. LF can be computed by answering
rank queries on B. Using LF we can ﬁnd the starting
position of the r-th smallest suﬃx, SA[r],
in O(b)
applications of LF , where b is the sampling factor; we
refer to [32] for details. It is also possible to compute
the function Ψ by using select queries the BWT [26].
Therefore the BWT can be viewed as a variant of the
CSA. Using Ψ we can consecutively obtain positions
of suﬃxes T [i..] in the suﬃx array: Let ri denote the
position of T [i..] in SA. Since T [n − 1..] = $ is the
smallest suﬃx, r0 = Ψ(0). For i ≥ 1, ri = Ψ(ri−1) by
deﬁnition of Ψ. Hence we can consecutively compute
each ri in O(1) time if we have constant-time select
queries on the BWT.

Compressed Suﬃx Tree. A compressed suﬃx

tree consists of the following components:

• The compressed suﬃx array of T . We can use the

FM-index as an implementation.

• The suﬃx tree topology. This component can be

stored in 4n + o(n) bits [39].

• The permuted LCP array, or PLCP. The longest
common preﬁx array LCP is deﬁned as follows:
LCP [r] = j if and only if the longest common preﬁx
between the suﬃxes of rank r and r− 1 is of length
j. The permuted LCP array is deﬁned as follows:
P LCP [i] = j if and only if the rank of T [i..] is r and
LCP [r] = j. A careful implementation of P LCP
occupies 2n + o(n) bits [39].

A.2 Monotone List Labelling with Batched

Updates

A direct attempt to dynamize the data structure of
Section 3 encounters one signiﬁcant diﬃculty. An
insertion of a new symbol a into a chunk C changes
the positions of all the symbols that follow it. Since
symbols are stored in pairs (aj, i) grouped by symbol,
even a single insertion into C can lead to a linear
number of updates. Thus it appears that we cannot

support the batch of updates on C in less than Θ(|C|)
time. In order to overcome this diﬃculty we employ a
monotone labeling method and assign labels to positions
of symbols. Every position i in the chunk is assigned an
integer label lab(i) satisfying 0 ≤ lab(i) ≤ σ · nO(1) and
lab(i1) < lab(i2) if and only if i1 < i2. Instead of pairs
(a, i) the sequence R will contain pairs (a, lab(i)).

When a new element is inserted, we have to change
the labels of some other elements in order to maintain 
the monotonicity of the labeling. Existing labeling 
schemes [44, 10, 11] require O(log2 n) or O(log n)
changes of labels after every insertion. In our case, however,
 we have to process large batches of insertions. We
can also assume that at most log n batches need to be
processed. In our scenario O(1) amortized modiﬁcations
per insertion can be achieved, as shown below.

In this section we denote by C an ordered set
that contains between σ and 2σ elements. Let x1 ≤
x2 ≤ . . . ≤ xt denote the elements of C. Initially we
assign the label lab(xi) = i · d to the i-th smallest
element xi, where d = 4n. We associate an interval
[lab(xi), lab(xi+1) − 1] with xi. Thus initially the
interval of xi is [id, (i + 1)d− 1]. We assume that C also
contains a dummy element x0 = −∞ and lab(−∞) = 0.
Thus all labels are non-negative integers bounded by
O(σ · n).
Suppose that the k-th batch of insertions consists
of m new elements y1 ≤ y2 ≤ . . . ≤ ym. Since at
most log n batches of insertions must be supported,
1 ≤ k ≤ log n. We say that an element yj is in an
interval I = [lab(xs), lab(xe)] if xs < yj < xe. We
denote by new(I) the number of inserted elements in I.
The parameter ρ(I) for an interval I is deﬁned as the
ratio of old and new elements in I = [lab(xs), lab(xe)],
ρ(I) = e−s+1
new(I) . We identify the set of non-overlapping
intervals I1, . . ., Ir such that every new element yt is in
some interval Ij, and 1 ≤ ρ(Ij) ≤ 2 for all j, 1 ≤ j ≤ r.
(This is always possible if m ≤ |C|; otherwise we simply
merge the insertions with C in O(|C| + m) = O(m)
time and restart all the labels.) We can ﬁnd I1, . . ., Ir
in O(m) time. For every Ij, 1 ≤ j ≤ r, we evenly
distribute the labels of old and new elements in the
interval I(cid:48)j ⊆ Ij. Suppose that f new elements yp, . . .,
yp+f−1 are inserted into interval Ij = [lab(xs), lab(xe)]
so that now there are v = f + (e− s) + 1 elements in this
interval. We assign the label lab(xs) + dj · (i− 1) to the
i-th smallest element in Ij where dj = lab(xe)−lab(xs)
.
By our choice of Ij, f ≤ e − s + 1 and the number of
elements in Ij increased at most by twofold. Hence the
minimal distance between two consecutive labels does
not decrease by more than a factor of 2 after insertion of
new elements into Ij. We inserted f new elements into
Ij and changed the labels of at most 2f old elements.

v−1

11

418

Copyright © 2016
J. Ian Munro,  Gonzalo Navarroy, Yakov Nekrichz

Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpHence the amortized number of labels that we must
change after every insertion is O(1). The initial distance
between labels is d = 4n and this distance becomes at
most two times smaller after every batch of insertions.
Hence the distance between consecutive labels is an
integer larger than 2 during the ﬁrst log n batches.

One remaining problem with our scheme is the large
range of the labels. Since labels are integers bounded by
4|C|n, we need Θ(log σ + log n) bits per label. To solve
this problem, we will split the chunk C into blocks and
assign the same label to all the symbols in a block. A
label assigned to the symbols in a block will be stored
only once. Details are provided in Section A.3.

A.3 Batched Rank Queries and Insertions on a

Sequence

In this section we describe a dynamic data structure
that supports both batches of rank queries and batches
of insertions. First we describe how queries and updates
on a chunk C are supported.

The linked list L contains all the symbols of C in
the same order as they appear in C. Each node of
L stores a block of Θ(logσ n) symbols, containing at
most (1/4) logσ n of them. We will identify list nodes
with the blocks they contain; however, the node storing
block b also stores the total number of symbols in all
preceding blocks and a label lab(b) for the block. Labels
are assigned to blocks with the method described in
Section A.2. The pointer to (the list node containing)
block b will be called pb; these pointers use O(log σ) bits.
We also maintain a data structure that can answer
rank queries on any block. The data structure for
a block supports queries and insertions in O(1) time
using a look-up table: Since σ ≤ n1/4 and the block
size is (1/4) logσ n, we can keep pre-computed answers
to all rank queries for all possible blocks in a table
T bl[0..n1/4 − 1][0..n1/4 − 1][0.. logσ n − 1]. The entry
T bl[b][a][i] contains the answer to the query ranka(i, b)
on a block b. T bl contains O(n1/2 logσ n) = o(n) entries
and can be constructed in o(n) time. Updates can be
supported by a similar look-up table or by bit operations
on the block b.

We also use sequences R and R(cid:48), deﬁned in Section 
3, but we make the following modiﬁcations. For
every occurrence C[i] = a of a symbol a in C, the sequence 
R contains pair (a, pb), where pb is a pointer to
the block b of L that contains C[i]. Pairs are sorted
by symbol in increasing order, and pairs with the same
symbol are sorted by their position in C. Unlike in
Section 3, the chunk C can be updated and we cannot
maintain the exact position i of C[i] for all symbols in C;
we only maintain the pointers pb in the pairs (a, pb) ∈ R.
Note that we cannot use block pointers for searching

in L (or in C). Instead, block labels are monotonously
increasing and lab(b1) < lab(b2) if the block b2 follows b1
in L. Hence block labels will be used for searching and
answering rank queries. Block labels lab(b) use Θ(log n)
bits of space, so we store them only once with the list
nodes b and access them via the pointers pb.

Groups Ha,j are deﬁned as in Section 3; each
Ha,j contains all the pairs of R that are between
two consecutive elements of R(cid:48)a for some a. The
data structure Da,j that permits searching in Ha,j is
deﬁned as follows. Suppose that Ha,j contains pairs
(a, pb1), . . ., (a, pbf ). We then keep a Succinct SB-tree
data structure [18] on lab(b1), . . ., lab(bf ). This data
structure requires O(log log n) additional bits per label.
For any integer q, it can ﬁnd the largest block label
lab(bi) < q in O(1) time or count the number of blocks
bi such that lab(bi) < q in O(1) time (because our sets
Ha,r contain a logarithmic number of elements). The
search procedure needs to access one block label, which
we read from the corresponding block pointer.

. . .,

ranka2 (i2, C),

Queries. Suppose that we want to answer queries
ranka1(i1, C),
rankat(it, C) on a
chunk C. We traverse all the blocks of L and ﬁnd
for every ij the label
lj of the block bj that contains 
the ij-th symbol, lj = lab(bj). We also compute
rj,1 = rankaj (i(cid:48)j, bj) using T bl, where i(cid:48)j is the relative
position of the ij-th symbol in bj. Since we know the
total number of symbols in all the blocks that precede
bj, we can compute i(cid:48)j in O(1) time.

We then represent the queries by pairs (aj, lj) and
sort these pairs stably in increasing order of aj. Then we
traverse the list of query pairs (aj, lj) and the sequence
R(cid:48). For every query (aj, lj) we ﬁnd the rightmost pair
(aj, pj) ∈ R(cid:48) satisfying lab(pj) ≤ lj. Let rj,2 denote
the rank of (aj, pj) in Raj , i.e., the number of pairs
(aj, i) ∈ R preceding (aj, pj). We keep this information
for every pair in R(cid:48) using O(log σ) additional bits.
Then we use the succinct SB-tree Daj ,pj , which contains
information about the pairs in Haj ,pj (i.e., the pairs
in the group starting with (aj, pj)). The structure
ﬁnds in constant time the largest lab(bg) ∈ Daj ,pj such
that lab(bg) < lj, as well as the number rj,3 of pairs
from the beginning of Haj ,pj up to the pair with label
lab(bg). The answer to the j-th rank query is then
rankaj (ij, C) = rj,1 + rj,2 + rj,3.

The total query time is then O(σ/ logσ n + t).
Insertions. Suppose that symbols a1, . . ., at are to
be inserted at positions i1, . . ., it, respectively. We traverse 
the list L and identify the nodes where new symbols 
must be inserted. We simultaneously update the
information about the number of preceding elements,
for all nodes. All this is done in time O(σ/ logσ n + t).
We also perform the insertions into the blocks. If, as

419

Copyright © 2016
J. Ian Munro,  Gonzalo Navarroy, Yakov Nekrichz

Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpa result, some block contains more than (1/4) logσ n
symbols, we split it into an appropriate number of
blocks, so that each block contains Θ(logσ n) but at
most (1/4) logσ n symbols. Nodes for the new blocks
are allocated5, linked to the list L, and assigned appropriate 
labels using the method described in Section A.2.
After t insertions, we create at most O(t/ logσ n) new
blocks (in the amortized sense, i.e., if we consider the
insertions from the beginning). Each such new block
b(cid:48), coming from splitting an existing block b, requires
that we change all the corresponding pointers pb from
the pairs (az, pb) in R (and R(cid:48)), so that they become
(az, pb(cid:48)). To ﬁnd those pairs eﬃciently, the list node
holding b also contains the O(logσ n) pointers to those
pairs (using O(log σ) bits each); we can then update the
required pointers in O(t) total time.

The new blocks also require creating their labels.
 Those O(t/ logσ n) label insertions also trigger
O(t/ logσ n) changes of other labels, with the technique
of Section A.2. If the label of a block b was changed,
we visit all pairs (az, pb) in R that point to b. Each
such (az, pb) is kept in some group Haz,k and in some
succinct SB-tree Daz,k. We then delete the old label of
b from Daz,k and insert the new modiﬁed label. The total 
number of updates is thus bounded by O(t). While
not mntioned in the original paper [18], one can easily
perform constant-time insertions and deletions of labels
in a succinct SB-tree: The structure is a two-level Btree 
of arity √log n holding encoded Patricia trees on
the bits of the keys, and storing at the leaves the positions 
of the keys in Ha,r using O(log log n) bits each.
To insert or delete a label we follow the usual B-tree
procedures. The insertion or deletion of a key in a
B-tree node is done in constant time with a precomputed 
table that, in the same spirit of T bl, yields the
resulting Patricia tree if we delete or insert a certain
node; this is possible because internal nodes store only
O(√log n log log n) = o(log n) bits. Similarly, we can
delete or insert a key at the leaves of the tree.

Apart from handling the block overﬂows, we must
insert in R the pairs corresponding to the new t symbols
we are actually inserting. We perform t rank queries
ranka1(i1, C), . . ., rankat(it, C), just as described above,
and sort the symbols to insert by those ranks using radix
sort. We then traverse R(cid:48) and identify the groups Ha1,j1,
. . ., Hat,jt where new symbols must be inserted; the
counters of preceding pairs for the pairs in R(cid:48) is easily
updated in the way. We allocate the pairs (ak, pbk ) that
will belong to Hai,ji and insert the labels lab(bk) in the
corresponding data structures Dak,jk , for all 1 ≤ k ≤ t.
5Constant-time allocation is possible because we use ﬁxed-size
nodes, leaving the maximum possible space, (1/4) log n bits, for
the block contents.

If some groups Hat,jt become larger than permitted, we
split them as necessary and insert the corresponding
pairs in R(cid:48). We can answer the rank queries, traverse
R, and update the groups Hak,jk all in O(σ/ logσ n + t)
time.

Global Sequence. In addition to chunk data
structures, we keep a static bitvector Ma = 1d1 0 . . . 1ds
for every symbol a; di denotes the number of times a
occurs in the i-th chunk.

Given a global sequence of m ≥ n/ logσ n queries,
ranka1 (i1, B), . . ., rankam(im, B) on B, we can assign
them to chunks in O(m) time. Then we answer queries
on chunks as shown above. If mj queries are asked on
chunk Cj, then these queries are processed in O(mj +
σ/ logσ n) time. Hence all queries on all chunks are
answered in O(m + n/ logσ n) = O(m) time. We can
answer a query rankak (ik, B) by answering a rank query
on the chunk that contains B[ik] and O(1) queries on
the sequence Mak [17]. Queries on Mak are supported
in O(1) time because the bitvector is static. Hence the
total time to answer m queries on B is O(m).

When a batch of symbols is inserted, we update the
corresponding chunks as described above. If some chunk
contains more than 4σ symbols, we split it into several
chunks of size Θ(σ) using standard techniques. Finally
we update the global sequences Ma, both because of
the insertions and due to the possible chunk splits. We
simply rebuild the bitvectors Ma from scratch; this is
easily done in O(na/ log n) time, where na is the number
of bits in Ma; see e.g. [30]. This adds up to O(m/ log n)
time.

Hence the total amortized cost for a batch of m ≥

n/∆ insertions is O(m).
Theorem A.3.1. We can keep a sequence B[0..n − 1]
over an alphabet of size σ in O(n log σ) bits of space so
that a batch of m rank queries can be answered in O(m)
time and a batch of m insertions is supported in O(m)
amortized time, for

n

logσ n ≤ m ≤ n.

A.4 Sequences with Partial Rank Operation
If σ = logO(1) n, then we can keep a sequence S in
O(n log σ) bits so that select and rank queries (including
partial rank queries) are answered in constant time [14].
In the remaining part of this section we will assume that
σ ≥ log3 n.
Lemma A.4.1. Let σ ≤ m ≤ n. We can support partial
rank queries on a sequence C[0..m− 1] over an alphabet
of size σ in time O(1). The data structure needs
O(m log log m) additional bits and can be constructed
in O(m) deterministic time.

Proof. Our method employs the idea of buckets intro13


420

Copyright © 2016
J. Ian Munro,  Gonzalo Navarroy, Yakov Nekrichz

Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpduced in [3]. Our structure does not use monotone perfect 
hashing, however. Let Ia denote the set of positions
where a symbol a occurs in C, i.e., Ia contains all integers 
i satisfying C[i] = a. If Ia contains more than
2 log2 m integers, we divide Ia into buckets Ba,s of size
log2 m. Let pa,s denote the longest common preﬁx of
all integers (seen as bit strings) in the bucket Ba,s and
let la,s denote the length of pa,s. For every element
C[i] in the sequence we keep the value of lC[i],t where
BC[i],t is the bucket containing i.
If IC[i] was not divided 
into buckets, we assume lC[i],t = null, a dummy
value. We will show below how the index t of BC[i],t
can be identiﬁed if lC[i],t is known. For every symbol
C[i] we also keep the rank r of i in its bucket BC[i],t.
That is, for every C[i] we store the value of r such that
i is the r-th smallest element in its bucket BC[i],t. Both
lC[i],t and r can be stored in O(log log m) bits. The partial 
rank of C[i] in C can be computed from t and r,
rankC[i](i, C) = t log2 m + r.

It remains to describe how the index t of the bucket
containing C[i] can be found. Our method uses o(m)
additional bits. First we observe that pa,i (cid:54)= pa,j for
any ﬁxed a and i (cid:54)= j; see [3] for a proof. Let Tw denote
the full binary trie on the interval [0..m − 1]. Nodes
of Tw correspond to all possible bit preﬁxes of integers
0, . . . , m − 1. We say that a bucket Ba,j is assigned to
a node u ∈ Tw if pa,j corresponds to the node u. Thus
many diﬀerent buckets can be assigned to the same node
u. But for any symbol a at most one bucket Ba,k is
assigned to u. If a bucket is assigned to a node u, then
there are at least log2 m leaves below u. Hence buckets
can be assigned to nodes of height at least 2 log log m;
such nodes will be further called bucket nodes. We store
all buckets assigned to bucket nodes of Tw using the
structure described below.

We order the nodes u level-by-level starting at the
top of the tree. Let mj denote the number of buckets
assigned to uj. The data structure Gj contains all
symbols a such that some bucket Ba,ka is assigned to
uj. For every symbol a in Gj we can ﬁnd in O(1) time
the index ka of the bucket Ba,ka that is assigned to
uj. We implement Gj as deterministic dictionaries of
Hagerup et al. [20]. Gj uses O(mj log σ) bits and can
be constructed in O(mj log σ) time. We store Gj only
for bucket nodes uj such that mj > 0. We also keep an
array W [1.. m
log2 m ] whose entries correspond to bucket
nodes of Tw: W [j] contains a pointer to Gj or null if Gj
does not exist.

Using W and Gj we can answer a partial rank query
rankC[i](i, C). Let C[i] = a. Although the bucket Ba,t
containing i is not known, we know the length la,t of the
preﬁx pa,t. Hence pa,t can be computed by extracting
the ﬁrst la,t bits of i. We can then ﬁnd the index j of the

node uj that corresponds to pa,t, j = (2la,t − 1) + pa,t.
We lookup the address of the data structure Gj in W [j].
Finally the index t of the bucket Ba,t is computed as
t = Gj[a].

Since (cid:80)

j mj ≤ m

A data structure Gj consumes O(mj log m) bits.
log2 m , all Gj use O(m/ log m) bits of
space. The array W also uses O(m/ log m) bits. Hence
our data structure uses O(log log m) additional bits per
symbol.

Theorem A.4.1. We can support partial rank queries
on a sequence B using O(n log log σ) additional bits.
The underlying data structure can be constructed in
O(n) deterministic time.

Proof. We divide the sequence B into chunks of size σ
(except for the last chunk that contains n − ((cid:98)n/σ(cid:99)σ)
symbols). Global sequences Ma are deﬁned in the same
way as in Section 3. A partial rank query on B can be
answered by a partial rank query on a chunk and two
queries on Ma.

A.5 Reporting All Symbols in a Range

We prove the following lemma in this section.
Lemma A.5.1. Given a sequence B[0..n − 1] over an
alphabet σ, we can build in O(n) time a data structure
that uses O(n log log σ) additional bits and answers the
following queries: for any range [i..j], report occ distinct
symbols that occur in B[i..j] in O(occ) time, and for
every reported symbol a, give its frequency in B[i..j] and
its frequency in B[0..i − 1].

The proof is the same as that of Lemma 3 in [1], but
we use the result of Theorem A.4.1 to answer partial
rank queries. This allows us to construct the data
structure in O(n) deterministic time (while the data
structure in [1] achieves the same query time, but the
construction algorithm requires randomization). For
completeness we sketch the proof below.

Augmenting B with O(n) additional bits, we can
report all distinct symbols occurring in B[i..j]
in
O(occ) time using the idea originally introduced by
Sadakane [40]. For every reported symbol we can ﬁnd
in O(1) time its leftmost and its rightmost occurrences
in B[i..j]. Suppose ia and ja are the leftmost and rightmost 
occurrences of a in B[i..j]. Then the frequencies 
of a in B[i..j] and B[0..i − 1] can be computed as
ranka(ja, B)− ranka(ia, B) + 1 and ranka(ia, B)− 1 respectively.
 Since ranka(ia, B) and ranka(ja, B) are partial 
rank queries, they are answered in O(1) time. The
data structure that reports the leftmost and the rightmost 
occurrences can be constructed in O(n) time. Details 
and references can be found in [8]. Partial rank

421

Copyright © 2016
J. Ian Munro,  Gonzalo Navarroy, Yakov Nekrichz

Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpqueries are answered by the data structure of Theorem 
A.4.1. Hence the data structure of Lemma A.5.1
can be built in O(n) deterministic time. We can also
use the data structure of Lemma A.4.1 to determine
whether the range B[i..j] contains only one distinct
symbol in O(1) time by using the following observation.
If B[i..j] contains only one symbol, then B[i] = B[j] and
rankB[i](j, B)− rankB[i](i, B) = j − i + 1. Hence we can
ﬁnd out whether B[i..j] contains exactly one symbol in
O(1) time by answering two partial rank queries. This
observation will be helpful in Section 5.

A.6 Computing the Intervals

The algorithm for constructing PLCP, described in
Section 5, requires that we compute the intervals of
T [j∆(cid:48)..j∆(cid:48) + (cid:96)i] and T [j∆(cid:48)..j∆(cid:48) + (cid:96)i] for i = j∆(cid:48) and
j = 0, 1, . . . , n/∆(cid:48). We will show in this section how
all necessary intervals can be computed in linear time
when (cid:96)i for i = j∆(cid:48) are known. Our algorithm uses the
suﬃx tree topology. We construct some additional data
structures and pointers for selected nodes of the suﬃx
tree T . First, we will describe auxiliary data structures
on T . Then we show how these structures can be used
to ﬁnd all needed intervals in linear time.
Marking Nodes in a Tree. We use the marking
scheme described in [33]. Let d = log n. A node u of
T is heavy if it has at least d leaf descendants and light
otherwise. We say that a heavy node u is a special or
marked node if u has at least two heavy children. If a
non-special heavy node u has more than d children and
among them is one heavy child, then we keep the index
of the heavy child in u.

We keep all children of a node u in the data
structure Fu, so that the child of u that is labeled by a
symbol a can be found eﬃciently. If u has at most d + 1
children, then Fu is implemented as a fusion tree [16];
we can ﬁnd the child of u labeled by any symbol a in
O(1) time. If u has more than d + 1 children, then Fu
is implemented as the van Emde Boas data structure
and we can ﬁnd the child labeled by a in O(log log σ)
time. If the node u is special, we keep labels of its heavy
children in the data structure Du. Du is implemented as
a dictionary data structure [20] so that we can ﬁnd any
heavy child of a special node in O(1) time. We will say
that a node u is diﬃcult if u is light but the parent of
u is heavy. We can quickly navigate from a node u ∈ T
to its child ui unless the node ui is diﬃcult.
Proposition A.6.1. We can ﬁnd the child ui of u that
is labeled with a symbol a in O(1) time unless the node
in
ui
O(log log σ) time.

is diﬃcult, we can ﬁnd ui

is diﬃcult.

If ui

Proof. Suppose that ui is heavy. If u is special, we can

ﬁnd ui in O(1) time using Du. If u is not special and
it has at most d + 1 children, then we ﬁnd ui in O(1)
time using Fu. If u is not special and it has more than
d + 1 children, then ui is the only heavy child of u and
its index i is stored with the node u. Suppose that ui is
light and u is also light. Then u has at most d children
and we can ﬁnd ui in O(1) time using Fu. If u is heavy
and ui is light, then ui is a diﬃcult node. In this case
we can ﬁnd the index i of ui in O(log log σ) time using
Fu.

Proposition A.6.2. Any path from a node u to its
descendant v contains at most one diﬃcult node.

Proof. Suppose that a node u is a heavy node and its
descendant v is a light node. Let u(cid:48) denote the ﬁrst light
node on the path from u to v. Then all descendants of
u(cid:48) are light nodes and u(cid:48) is the only diﬃcult node on the
path from u to v. If u is light or v is heavy, then there
are apparently no diﬃcult nodes between u and v.

Weiner Links. A Weiner

link (or w-link)
wlink(v, c) connects a node v of the suﬃx tree T labeled
by the path p to the node u, such that u is the locus of
cp. If wlink(v, c) = u we will say that u is the target
node and v is the source of wlink(v, c) and c is the label
of wlink(v, c). If the target node u is labeled by cp, we
say that the w-link is explicit. If u is labeled by some
path cp(cid:48), such that cp is a proper preﬁx of cp(cid:48), then the
Weiner link is implicit. We classify Weiner links using
the same technique that was applied to nodes of the sufﬁx 
tree above. Weiner links that share the same source
node are called sibling links. A Weiner link from v to
u is heavy if the node u has at least d leaf descendants
and light otherwise. A node v is w-special iﬀ there are
at least two heavy w-links connecting v and some other
nodes. For every special node v the dictionary D(cid:48)v contains 
the labels c of all heavy w-links wlink(v, c). For
every c such that wlink(v, c) is heavy, we also keep the
target node u = wlink(v, c). D(cid:48)v is implemented as in
[20] so that queries are answered in O(1) time. Suppose
that v is the source node of at least d + 1 w-links, but
u = wlink(v, c) is the only heavy link that starts at v.
In this case we say that wlink(v, c) is unique and we
store the index of u and the symbol c in v. Summing
up, we store only heavy w-links that start in a w-special
node or unique w-links. All other w-links are not stored
explicitly; if they are needed, we compute them using
additional data structures that will be described below.
Let B denote the BWT of T . We split B into
intervals Gj of size 4d2. For every Gj we keep the
dictionary Aj of symbols that occur in Gj. For each
symbol a that occurs in Gj, the data structure Gj,a
contains all positions of a in Gj. Using Aj, we can

15

422

Copyright © 2016
J. Ian Munro,  Gonzalo Navarroy, Yakov Nekrichz

Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpﬁnd out whether a symbol a occurs in Gj. Using Gj,a,
we can ﬁnd for any position i the smallest i(cid:48) ≥ i such
that B[i(cid:48)] = a and B[i(cid:48)] is in Gj (or the largest i(cid:48)(cid:48) ≤ i
such that B[i(cid:48)(cid:48)] = a and B[i(cid:48)(cid:48)] is in Gj). We implement
both Aj and Gj,a as fusion trees [16] so that queries are
answered in O(1) time. Data structures Aj and Gj,a
for a ﬁxed j need O(d2 log σ) bits. We also keep (1) the
data structure from [17] that supports select queries on
B in O(1) time and rank queries on B in O(log log σ)
time and (2) the data structure from Theorem A.4.1
that supports partial rank queries in O(1) time. All
additional data structures on the sequence B need
O(n log σ) bits.

Proposition A.6.3. The total number of heavy wlinks 
that start in w-special nodes is O(n/d).

Proof. Suppose that u is a w-special node and let p be
the label of u. Let c1, . . ., cs denote the labels of heavy
w-links with source node u. This means that each c1p,
c2p, . . ., csp occurs at least d times in T . Consider the
suﬃx tree T of the reverse text T . T contains the node
u that is labeled with p. The node u has (at least) s
children u1, . . ., us. The edge connecting u and ui is
a string that starts with ci. In other words each ui is
the locus of pci. Since cip occurs at least d times in
T , pci occurs at least d times in T . Hence each ui has
at least d descendants. Thus every w-special node in
T correspond to a special node in T and every heavy
w-link outgoing from a w-special node corresponds to
some heavy child of a special node in T . Since the
number of heavy children of special nodes in a suﬃx
tree is O(n/d), the number of heavy w-links starting in
a w-special node is also O(n/d).

Proposition A.6.4. The total number of unique wlinks 
is O(n/d).

Proof. A Weiner link wlink(v, a) is unique only if
wlink(v, a) is heavy, all other w-links outgoing from v
are light, and there are at least d light outgoing w-links
from v. Hence there are at least d w-links for every
explicitly stored target node of a unique Weiner link.

We say that wlink(v, a) is diﬃcult if its target node
u = wlink(v, a) is light and its source node v is heavy.

Proposition A.6.5. We can compute u = wlink(v, a)
of u in O(1) time unless wlink(v, a) is diﬃcult. If the
wlink(v, a) is diﬃcult, we can compute u = wlink(v, a)
in O(log log σ) time.

Proof. Suppose that u is heavy.
If v is w-special, we
can ﬁnd u in O(1) time using Du. If v is not w-special
and it has at most d + 1 w-children, then we ﬁnd ui

in O(1) time using data structures on B. Let [lv, rv]
denote the suﬃx range of v. The suﬃx range of u is
[lu, ru] where lu = Acc[a] + ranka(lv − 1, B) + 1 and
ru = Acc[a] + ranka(rv, B). We can ﬁnd ranka(rv, B)
as follows. Since v has at most d light w-children,
the rightmost occurrence of a in B[lv, rv] is within the
distance d2 from rv. Hence we can ﬁnd the rightmost
ia ≤ rv such that B[ia] = a by searching in the interval
Gj that contains rv or the preceding interval Gj−1.
When ia is found, ranka(rv, B) = ranka(ia, B) can be
computed in O(1) time because partial rank queries
on B are supported in time O(1). We can compute
ranka(lv−1, B) in the same way. When rank queries are
answered, we can ﬁnd lu and ru in constant time. Then
we can identify the node u by computing the lowest
common ancestor of lu-th and ru-th leaves in T .
If v is not special and it has more than d+1 outgoing
w-links, then u is the only heavy target node of a wlink 
starting at v; hence, its index i is stored in the
node v. Suppose that u is light and v is also light.
Then the suﬃx range [lv, rv] of v has length at most d.
B[lv, rv] intersects at most two intervals Gj. Hence we
can ﬁnd ranka(lv − 1, B) and ranka(rv, B) in constant
time. Then we can ﬁnd the range [lu, ru] of the node u
and identify u in time O(1) as described above. If v is
heavy and u is light, then wlink(v, a) is a diﬃcult wlink.
 In this case we need O(log log σ) time to compute
ranka(lv − 1, B) and ranka(rv, B). Then we ﬁnd the
range [lu, ru] and the node u is found as described above.

Proposition A.6.6. Any sequence of nodes u1, . . .,
ut where ui = wlink(ui−1, ai−1) for some symbol ai−1
contains at most one diﬃcult w-link.

Proof. Let π denote the path of w-links that contains
nodes u1, . . ., ut. Suppose that a node u1 is a heavy
node and ut is a light node. Let ul denote the ﬁrst light
node on the path π. Then all nodes on the path from ul
to ut are light nodes and wlink(ul−1, al−1) is the only
diﬃcult w-link on the path from u1 to ut. If u1 is light
or ut is heavy, then all nodes on π are light nodes (resp.
all nodes on π are heavy nodes). In this case there are
apparently no diﬃcult w-links between u1 and ut.

Pre-processing. Now we show how we can construct 
above described auxiliary data structures in linear 
time. We start by generating the suﬃx tree topology
and creating data structures Fu and Du for all nodes u.
For every node u in the suﬃx tree we create the list of its
children ui and their labels in O(n) time. For every tree
node u we can ﬁnd the number of its leaf descendants
using standard operations on the suﬃx tree topology.
Hence, we can determine whether u is a heavy or a light
node and whether u is a special node. When this infor423


Copyright © 2016
J. Ian Munro,  Gonzalo Navarroy, Yakov Nekrichz

Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpmation is available, we generate the data structures Fu
and Du.

We can create data structures necessary for navigating 
along w-links in a similar way. We visit all nodes
u of T . Let lu and ru denote the indexes of leftmost
and rightmost leaves in the subtree of u. Let B denote
the BWT of T . Using the method of Lemma A.5.1,
we can generate the list of distinct symbols in B[lu..ru]
and count how many times every symbol occurred in
B[lu..ru] in O(1) time per symbol.
If a symbol a occurred 
more than d times, then wlink(u, a) is heavy.
Using this information, we can identify w-special nodes
and create data structures D(cid:48)u. Using the method
of [37], we can construct D(cid:48)u in O(nu log log nu) time.
By Lemma A.6.3 the total number of target nodes in
all D(cid:48)u is O(n/d); hence we can construct all D(cid:48)u in o(n)
time. We can also ﬁnd all nodes u with a unique wlink.
 All dictionaries D(cid:48)u and all unique w-links need
O((n/d) log n) = O(n) bits of space.

Supporting a Sequence of extendright Operations.

Lemma A.6.1. If we know the suﬃx interval of a rightmaximal 
factor T [i..i + j] in B and the suﬃx interval
of T [i..i + j] in B, the we can ﬁnd the intervals of
T [i..i + j + t] and T [i..i + j + t] in O(t + log log σ) time.
Proof. Let T and T denote the suﬃx tree for the text
T and let T denote the suﬃx tree of the reverse text
T . We keep the data structure for navigating the
suﬃx tree T , described in Proposition A.6.1 and the
data structure for computing Weiner links described
in Proposition A.6.5. We also keep the same data
structures for T . Let [(cid:96)0,s, (cid:96)0,e] denote the suﬃx interval
of T [i..i + j]; let [(cid:96)(cid:48)0,s, (cid:96)(cid:48)0,e] denote the suﬃx interval of
T [i..i + j]. We navigate down the tree following the
symbols T [i+j+1], . . ., T [i+j+t]. Let a = T [i+j+k] for
some k such that 1 ≤ k ≤ t and suppose that the suﬃx
interval [(cid:96)k−1,s, (cid:96)k−1,e] of T [i..i+j +k−1] and the suﬃx
interval [(cid:96)(cid:48)k−1,s, (cid:96)(cid:48)k−1,e] of T [i..i + j + k − 1] are already
known. First, we check whether our current location is a
node of T . If B[(cid:96)(cid:48)k−1,s, (cid:96)(cid:48)k−1,e] contains only one symbol
T [i + j + k], then the range of T [i..i + j + k] is identical

with the range of T [i..i+j +k−1]. We can calculate the
range of T [i..i + j + k] in a standard way by answering
two rank queries on B and O(1) arithmetic operations;
see Section A.1. Since B[(cid:96)(cid:48)k−1,s, (cid:96)(cid:48)k−1,e] contains only
one symbol, rank queries that we need to answer are
partial rank queries. Hence we can ﬁnd the range of
T [i..i + j + k] in time O(1). If B[(cid:96)(cid:48)k−1,s, (cid:96)(cid:48)k−1,e] contains
more than one symbol, then there is a node u ∈ T that
is labeled with T [i..i + j + k − 1]; u = lca((cid:96)k−1,s, (cid:96)k−1,e)
where lca(f, g) denotes the lowest common ancestor of
the f -th and the g-th leaves. We ﬁnd the child u(cid:48) of
the node u in T that is labeled with a = T [i + j + k].
We also compute the Weiner link u(cid:48) = wlink(u, a) for
a node u(cid:48) = lca((cid:96)(cid:48)k−1,s, (cid:96)(cid:48)k−1,e) in T . Then (cid:96)(cid:48)k,s =
leftmost leaf(u(cid:48)) and (cid:96)(cid:48)k,e = rightmost leaf(u(cid:48)).
We need to visit at most t nodes of T and at most
t nodes of T in order to ﬁnd the desired interval.
By Proposition A.6.1 and Proposition A.6.2, the total
time needed to move down in T is O(t + log log σ).
By Proposition A.6.5 and Proposition A.6.6, the total
time to compute all necessary w-links in T is also
O(t + log log σ).

Finding the Intervals. The algorithm for computing 
PLCP, described in Section 5, assumes that
intervals of T [j∆(cid:48)..j∆(cid:48) + (cid:96)i] and
we know the
T [j∆(cid:48)..j∆(cid:48) + (cid:96)i] for i = j∆(cid:48) and j = 0, 1, . . . , n/∆(cid:48).
These values can be found as follows. We start
by computing the intervals of T [0..(cid:96)0] and T [0..(cid:96)0].
Suppose that the intervals of T [j∆(cid:48)..j∆(cid:48) + (cid:96)i] and
T [j∆(cid:48)..j∆(cid:48) + (cid:96)i] are known. We can compute (cid:96)(j+1)∆(cid:48)
as shown in Section 5. We ﬁnd the intervals of T [(j +
1)∆(cid:48)..j∆(cid:48) + (cid:96)i] and T [(j + 1)∆(cid:48)..j∆(cid:48) + (cid:96)i] in time O(∆(cid:48))
by executing ∆(cid:48) operations contractleft. Each operation 
contractleft takes constant time. Then we calculate 
the intervals of T [(j + 1)∆(cid:48)..(j + 1)∆(cid:48) + (cid:96)i+1] and
T [(j + 1)∆(cid:48)..(j + 1)∆(cid:48) + (cid:96)i+1] in O(log log σ + ((cid:96)i+1 −
(cid:96)i + ∆(cid:48))) time using Lemma A.6.1. We know from Section 
5 that (cid:80)((cid:96)i+1 − (cid:96)i) = O(n). Hence we compute

all necessary intervals in time O(n + (n/∆(cid:48)) log log σ) =
O(n).

17

424

Copyright © 2016
J. Ian Munro,  Gonzalo Navarroy, Yakov Nekrichz

Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php