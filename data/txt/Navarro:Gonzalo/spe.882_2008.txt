SOFTWARE—PRACTICE AND EXPERIENCE
Softw. Pract. Exper. 2008; 38:1429–1450
Published online 1 April 2008 in Wiley InterScience (www.interscience.wiley.com). DOI: 10.1002/spe.882

New adaptive compressors for
natural language text‡
N. R. Brisaboa1, A. Fari˜na1,∗,†
J. R. Parama1

, G. Navarro2 and

1Database Laboratory, Department of Computer Science, University of A Coru˜na,
Campus de Elvi˜na s/n, 15071, A Coru˜na, Spain
2Center for Web Research, Department of Computer Science, University of Chile,
Blanco Encalada 2120, Santiago, Chile

SUMMARY

Semistatic byte-oriented word-based compression codes have been shown to be an attractive alternative
to compress natural language text databases, because of the combination of speed, effectiveness, and
direct searchability they offer. In particular, our recently proposed family of dense compression codes
has been shown to be superior to the more traditional byte-oriented word-based Huffman codes in most
aspects. In this paper, we focus on the problem of transmitting texts among peers that do not share the
vocabulary. This is the typical scenario for adaptive compression methods. We design adaptive variants of
our semistatic dense codes, showing that they are much simpler and faster than dynamic Huffman codes
and reach almost the same compression effectiveness. We show that our variants have a very compelling
trade-off between compression/decompression speed, compression ratio, and search speed compared with
most of the state-of-the-art general compressors. Copyright © 2008 John Wiley & Sons, Ltd.

Received 1 December 2006; Revised 22 January 2008; Accepted 28 January 2008

KEY WORDS:

text databases; natural language text compression; dynamic compression; searching compressed
text

∗

†
‡

Correspondence to: A. Fari˜na, Database Laboratory, Department of Computer Science, University of A Coru˜na, Campus
de Elvi˜na s/n, 15071, A Coru˜na, Spain.
E-mail: fari@udc.es
A preliminary partial version on this work appeared in [1].

Contract/grant sponsor: MEC; contract/grant number: TIN2006-15071-C03-03
Contract/grant sponsor: Xunta de Galicia; contract/grant number: PGIDIT05-SIN-10502PR, 2006/4
Contract/grant sponsor: Millennium Nucleus Center for Web Research; contract/grant number: P04-067-F

Copyright q

2008 John Wiley & Sons, Ltd.

1430

N. R. BRISABOA ET AL.

1.

INTRODUCTION

Text compression is of special interest in data transmission. In some scenarios, it is feasible that
compression and transmission are completed before reception and decompression start. In these
cases, statistical two-pass techniques, also called semistatic, can be used. A ﬁrst pass over the text
gathers global statistical information about the vocabulary (list of source symbols) in order to obtain
a model of the text. The model is used to compute the codeword corresponding to each source
symbol and then, in a second pass, each original symbol is substituted by its codeword. Therefore,
the model must be stored/transmitted with the compressed text; so that the decompressor can know
the model for performing decompression.

In real-time transmission, the sender should be able to start the transmission of compressed data
without preprocessing the whole text, and simultaneously the receiver should start the reception
and decompression of the text as it arrives. Real-time transmission is handled with the so-called
dynamic or adaptive compression techniques. These perform a single pass over the text (so they
are also called one-pass) and begin compression and transmission as they read the data. Adaptive
or dynamic compression methods do not need to transmit the model because the receiver can learn
it as it receives the compressed text.

In recent years, statistical semistatic compression techniques, especially designed for natural
language texts, have not only proven extremely effective (with compression ratios around 25–
30%), but have also permitted searching the compressed text much faster (up to 8 times) than the
original text. The success of these techniques is based on considering the text to be compressed as a
sequence of words instead of characters [2]. In [3], a word-based Huffman code, which reaches 25%
of compression ratio, was presented. Moura et al. also presented important improvements in two
compression codes called Plain Huffman (PH) and Tagged Huffman (TH) [4]. The byte-oriented
PH achieves compression ratios close to 30%, as opposed to the 25% that is achieved by using
bit-oriented codes [5]. In exchange, decompression is much faster because bit manipulations are
not necessary. TH adds a ﬂag bit to mark the limits of each codeword and, therefore, it permits
fast direct search of the compressed text reaching compression ratios around 34%. Owing to the
ﬂag bit, a pattern can be compressed and directly searched for in the compressed text without
decompressing it. This property is also essential to permit local decompression of text passages in
order to present them to the ﬁnal users.

Recently, a family of compression codes called Dense Codes has been shown to offer
language [6]. Dense codes
several advantages over Huffman-based compression for natural
are simpler and faster to build than Huffman codes, and they permit the same fast direct
searchability of TH, yet with better compression ratios. The simplest variant is End-Tagged
Dense Code (ETDC), which is just a variable-length integer representation for the position
of the word in a frequency rank. A more sophisticated variant is (s, c)-Dense Code (SCDC),
which adapts better to the text distribution. ETDC reaches around 31% compression ratio and
SCDC reaches less than 0.3 points over PH compression ratio. Another recent competitive
semistatic proposal is the Restricted Preﬁx Byte Code [7], which gets better compression ratio
than SCDC but, as PH, it does not use a ﬂag bit to mark the limits of each codeword in the
compressed text. This absence implies that searchers and decompressors have more difﬁculties
in performing random access and local decompression of text passages owing to a problem of
synchronization.

Copyright q

2008 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2008; 38:1429–1450
DOI: 10.1002/spe

NEW ADAPTIVE COMPRESSORS FOR NATURAL LANGUAGE TEXT

1431

Among the adaptive compressors, dynamic arithmetic coding over Prediction by Partial Matching
(PPM)-like modelling [8] obtains compression ratios around 24%, but it requires signiﬁcant computational 
effort by both the sender and the receiver, which is quite slow at both ends.
Compression methods based on the Ziv–Lempel family [9,10] (used in zip, gzip, arj, winzip, etc.)
obtain reasonable but not spectacular compression ratios on natural language text (around 40%),
yet they are very fast at decompression.

In this paper, we introduce two dynamic compressors, called Dynamic End-Tagged Dense Code
(DETDC) and Dynamic (s, c)-Dense Code (DSCDC), which adapt ETDC and SCDC to real-time
transmission. We have also implemented a byte-oriented word-based dynamic Huffman compressor,
which we call DPH, to have a powerful statistical compressor to compare with our dense dynamic
compressors. Details about its implementation can be found in [11].
In this paper, we show experimentally that DETDC and DSCDC offer several advantages over the
state-of-the-art adaptive compression methods in scenarios where real-time transmission of natural
text is needed. Some concrete examples of these scenarios follow:

1. News agency: These types of organizations are continuously disseminating news, in real time,
to newspapers, TV channels, radio stations, etc. Each piece of news is broadcast to all the
registered organizations, and this process is done in real time as news arise.

2. Digital library: When a user chooses a literary work, the digital library usually offers the user
the ability to download the literary work split in some sort of parts (sections, chapters, pages,
etc.). After choosing one of these parts, the server sends the associated text. These parts can
be requested in an arbitrary order.

3. Chat session established between two Internet users: Again, the messages are short, and they

should be delivered as soon as they are written.

4. HTTP session established between a server and a client: The HTML pages are sent by the

server when the client requests them.

All these situations can be described as scenarios where a sender sends short messages to a
receiver during a certain period of time (session). The individual messages are not long enough to
obtain good compression ratios using word-based semistatic compression, as they need to process at
least 5–10 Mbytes to compensate for the burden of storing the vocabulary (the model), according to
Moura et al. [4] and our own results. Yet, the whole session is long enough. This type of transmission
must be carried out in real time; therefore, it is not feasible to accumulate short messages along
time so as to send them together using a semistatic compressor. Therefore, dynamic compression
turns out to be the most suitable alternative.

In dynamic compression, the model changes each time a text word is processed. These frequent
changes of the model make it difﬁcult to carry out direct searches over text compressed with
adaptive methods, as the search pattern looks different in different points of the compressed text.
Yet, there are several adaptive compression scenarios where a direct search on the compressed text
(without decompressing it) is of interest. For example, for classiﬁcation or distribution purposes,
the receiver may be interested not in uncompressing all the arriving text, but in searching it for
some speciﬁc words. In ubiquitous computation or mobile databases, servers broadcast information
to the devices (PDAs, mobile phones, etc.) in their cell. Probably, these devices are not interested
in decompressing all the information they receive. Therefore, they would perform a multipattern
search on the arriving compressed text seeking some keywords, which denote topics of interest

Copyright q

2008 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2008; 38:1429–1450
DOI: 10.1002/spe

1432

N. R. BRISABOA ET AL.

(for example, sports, tourism, or trafﬁc information). When some of the keywords are found, the
device decompresses the information and stores it or points it to speciﬁc places in the device.
There exist some direct search techniques for adaptive compression that, even being slower than
searching the uncompressed text, are faster than uncompressing plus searching [12]. In this paper
we also show how direct search (without decompression) can be done over text compressed with our
adaptive dense compressors DETDC and DSCDC. In practice, searches over text compressed with
DETDC are faster than searches over text compressed with previous non-dense adaptive techniques.
Moreover, our searches are more efﬁcient than searches over uncompressed text when a large
number of patterns are searched for, which is usually the case in the applications given above.

The outline of this paper is as follows. In Sections 2 and 3, we describe our two adaptive techniques,
 DETDC and DSCDC, with sufﬁcient detail to be useful for a practitioner. In Section 4, we
brieﬂy comment on the advantages and disadvantages of the block-wise versions of the semistatic
alternatives. Section 5 presents the experimental results comparing our methods, in terms of
compression ratio and compression/decompression speed, with several state-of-the-art compressors.
 In Section 6, it is shown how to search text compressed with either DETDC or DSCDC
without previously decompressing it. We present experimental results comparing those searchers
with other search algorithms that work over compressed and uncompressed text. Finally, Section 7
gives our conclusions and directions for future work.

2. DYNAMIC END-TAGGED DENSE CODES

2.1. End-tagged dense codes
As explained in [4], PH is simply a word-based byte-oriented Huffman code. TH reserves the ﬁrst
bit of each byte to ﬂag whether the byte is the ﬁrst of its codeword. Hence, only 7 bits of each
byte are used for the Huffman code. Note that the use of a Huffman code over the remaining 7 bits
is mandatory, as the ﬂag is not useful by itself to make the code a preﬁx code. While searching
PH-compressed text requires inspecting all its bytes from the beginning, the tag bit in TH permits a
Boyer–Moore-type searching [13] (that is, skipping bytes) by simply compressing the pattern and
then running the string-matching algorithm. On PH this does not work, as the pattern could occur
in the text not aligned to any codeword [4].
ETDC has, as TH, a ﬂag bit, but now this bit signals the end of a codeword. That is, the leading
bit of a codeword byte is 1 for the last byte (not the ﬁrst) and 0 for the others. The remaining 7
bits of each byte are responsible for carrying the information. Observe that the ﬂag bit is enough
to ensure that the code is a preﬁx code regardless of the content of the other 7 bits of each byte.
Therefore, there is no need at all to use Huffman coding in order to maintain a preﬁx code. Thus, all
the possible combinations of bits can be used to ﬁll the remaining 7 bits of each byte. ETDC obtains
better performance than TH in all aspects, whereas it maintains all its good search capabilities.
In fact, the coding scheme used by ETDC had already been used to compress integers, such as the
document identiﬁers in inverted indexes [7,14], receiving different names like bc or variable-byte
coding (Vbyte).

In ETDC, the model is just the vocabulary sorted by frequency, because the codeword assigned to
each source word depends only on the rank of such a word in the vocabulary ordered by frequency,
and not on its actual frequency.

Copyright q

2008 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2008; 38:1429–1450
DOI: 10.1002/spe

NEW ADAPTIVE COMPRESSORS FOR NATURAL LANGUAGE TEXT

1433
Deﬁnition 1. Given source symbols with non-increasing probabilities { pi}0≤i <n, the corresponding
ETDC codeword for the symbol in position i has k bytes (k≥1), for k that satisﬁes

2b−1 2(b−1)(k−1)−1
2b−1−1

≤i <2b−1 2(b−1)k−1
2b−1−1

Thus, the codeword corresponding to source symbol i is formed by k−1 digits in base 2b−1, and
a ﬁnal base-2b−1 digit added to 2b−1. If k=1 then the codeword is simply i+2b−1. Otherwise the
codeword is formed by the number x written in base 2b−1, where x =i−(2(b−1)k−2b−1)/(2b−1−1),
and adding 2b−1 to the last digit.
ETDC can be deﬁned over symbols of b bits, although the byte-oriented version (b=8) is the
most common one. That is, the ﬁrst word (i =0) is encoded as (cid:4)128(cid:5), the second (i =1) as (cid:4)129(cid:5),
until the 128th, which is encoded as (cid:4)255(cid:5). The 129th word (i =128) is encoded as (cid:4)0:128(cid:5), the
130th as (cid:4)0:129(cid:5), and so on until the (1282+128)th word (cid:4)127:255(cid:5).
The simplicity of the code also allows simple encode and decode procedures, and makes ETDC
codiﬁcation faster than that those based on Huffman, as it does not have to deal with a tree. We
denote encode as the function that obtains the codeword Ci = encode(i ) for a word at the ith position
in the ranked vocabulary; decode computes the position i = decode(Ci ) in the rank, for a codeword
Ci . Both functions take just O(l) time, where l= O(log(i )/b) is the length in digits of codeword
Ci , and are efﬁciently implemented through bit shifts and masking. These algorithms are based on
Deﬁnition 1.
A complete description of ETDC as well as empirical results comparing ETDC with PH and TH
can be found in [6].

2.2. Towards DETDC

The main challenge to make ETDC dynamic is how to maintain the model updated as compression
progresses, as this process implies the insertion of new source symbols and frequency increments.
In the case of ETDC, the model is essentially the array of source symbols sorted by frequency;
therefore, this array must be kept ordered on insertions and frequency changes.

Both sender (compressor) and receiver (decompressor) increase the frequency of a word each
time it arrives, and maintain the vocabulary ordered by frequency, carrying out two symmetric
processes. Therefore, the sender does not transmit the model, as the receiver can ﬁgure it out by
itself from the received codewords. The sender only informs the receiver of new source symbols
appearing in the text using a special codeword that we denote as CzeroNode. The sender transmits
CzeroNode followed by the source word in ASCII. The receiver inserts it in its vocabulary and sets
its frequency to 1. In DETDC, CzeroNode is always the ﬁrst unused codeword, that is, the codeword
that follows that of the last word in the vocabulary. When a word arrives, and it is already in the
vocabulary, the sender transmits its codeword, increases its frequency, and reorders the vocabulary
if necessary. When the receiver obtains a codeword other than CzeroNode, it just decodes it to obtain
the corresponding vocabulary position, recovers the word, and increases its frequency, reordering
the vocabulary if necessary.

Figure 1 shows how the compressor operates. At ﬁrst (step 0), no words have been read; therefore,
zeroNode is the only word in the vocabulary (it is implicitly placed at position 0). In step 1, a new
symbol "land" is read. Since it is not in the vocabulary, C0 (the codeword of zeroNode) is sent,

Copyright q

2008 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2008; 38:1429–1450
DOI: 10.1002/spe

1434

N. R. BRISABOA ET AL.

Figure 1. Transmission of "land far far away long long (ago)".

followed by "land". Then "land" is added to the vocabulary with frequency 1, at position 0.
Step 2 shows the transmission of "far", which was not in the vocabulary yet. In step 3, "far"
is read again. As it was in the vocabulary at position 1, the codeword C1 is sent. Now "far"
becomes more frequent than "land"; therefore, it moves upwards in the ordered vocabulary. Note
that a hypothetical new occurrence of "far" would be transmitted as C0, though it was sent as C1
in step 3. In steps 4 and 5, two more new words, "away" and "long", are transmitted and added
to the vocabulary. Finally, in step 6, "long" is read again, and when its frequency is updated, it
becomes more frequent than "away" and "land". Therefore, it moves upwards in the vocabulary
by means of an exchange with "land" (which is the ﬁrst word in the ranked vocabulary with its
same frequency).

The main issue is how to efﬁciently maintain the vocabulary sorted. We show next how to do
this with a complexity equal to the number of source symbols transmitted. Essentially, we must
be able to identify blocks of words with the same frequency in the ordered vocabulary, and to
quickly promote a word to the next block when its frequency increases. Promoting a word wi with
frequency f to the next frequency ( f +1) block consists of
• Sliding wi over all words whose frequency is f . This implies two operations:
◦ Locating the ﬁrst word in the ordered vocabulary whose frequency is f . This word is called
top f .
◦ Exchanging wi with top f .
• Increasing the frequency of wi .
2.3. Data structures for DETDC

The sender maintains a hash table that permits fast searching for a source word wi . The hash table
is also used to obtain the rank i in the vocabulary vector (remember that, to encode a word wi ,
using ETDC, only its rank i is needed), as well as its current frequency fi (which is used to rapidly
ﬁnd the position of word top fi ).

The receiver does not need to maintain a hash table to hold words because ﬁnding a word
lexicographically is never necessary at decompression. It only needs to use a word vector where

Copyright q

2008 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2008; 38:1429–1450
DOI: 10.1002/spe

NEW ADAPTIVE COMPRESSORS FOR NATURAL LANGUAGE TEXT

1435

words are kept sorted by frequency, because the decoding process uses the codeword to directly
obtain the rank value i that can be used to index the word vector.

Let n be the vocabulary size and F the maximum frequency value for any word in the vocabulary.
The data structures used by both the sender and the receiver, as well as their functionality, are given
next.

2.3.1. Sender’s data structures

The following three main data structures, shown in Figure 2, are needed:
• A hash table with space for H words (where H = nextPrime(2n)) keeps in its component word
the source word, in posInVoc the rank (or position) of the word in the ordered vocabulary, and
in freq its frequency.
• posInHT is an n-element vector. posInTH[i] points to the entry in the hash table that stores
the ith most frequent word in the vocabulary.
• Array top contains F elements, where F is the maximum frequency. Each position implicitly
represents a frequency value, that is, top[ f ] is associated with words with frequency equal to
f . For each possible frequency, vector top keeps a pointer to the entry in posInHT that points
to the ﬁrst (top) word with that frequency. If there are no words of frequency fi , then top[ fi]
will point to the position of the ﬁrst word j such that f j < fi .

A variable zeroNode is also needed to indicate the ﬁrst free position in the vocabulary, that is,

the position in posInHT where the next new word will be inserted.
One concern is how to estimate F in a dynamic setup. It can be estimated heuristically using
Heaps’ Law [15]. Alternatively, in order to avoid vector top using up much more space than
necessary, it can be implemented as a growing array that reallocates dynamically, doubling its size
each time. It is also possible to substitute vector top by more sophisticated solutions [16].
To have an idea of the spaces involved, we present an example considering a text of 1 Gbyte
from our experiments in Section 5. In this case, the highest frequency of a word is F =8205778.
Therefore, the space requirements to keep vector top is 8205778×4bytes≈31Mbytes, which is
perfectly reasonable for current computers, though it can be reduced to ≈20Mbytes with the
aforementioned improvements.

Figure 2. Transmission of words C, C, D, and D having transmitted ABABBC earlier.

Copyright q

2008 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2008; 38:1429–1450
DOI: 10.1002/spe

1436

N. R. BRISABOA ET AL.

2.3.2. Receiver’s data structures

The structures for the receiver are even simpler than those of the sender. The following three vectors
are needed:
• A word vector that keeps the source words sorted by frequency. Its size is n.
• A freq vector that keeps the frequency of each word. That is, freq[i]= f , if the number of
occurrences of the word stored in word[i] is f . As the array word, this vector can keep up to
n elements.
• Array top: As in the sender, this array gives, for each possible frequency, the word position of
the ﬁrst word with that frequency. It also has F positions.

Figure 3. Reception of c2, c2, c3 D#, and c3 having received c0A#c1B#c0c1c1c2C# previously.

Figure 4. Pseudo-code for sender and receiver processes in DETDC.

Copyright q

2008 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2008; 38:1429–1450
DOI: 10.1002/spe

NEW ADAPTIVE COMPRESSORS FOR NATURAL LANGUAGE TEXT

1437

The variable zeroNode is also maintained by the receiver. The structures needed by the receiver

are illustrated in Figure 3.

2.4. Sender and receiver processes

When the sender reads a word wi , it uses the hash function to obtain its position p in the hash table,
so that hash(wi )= p and therefore word[ p]= wi . After reading f = freq[ p], it increments freq[ p].
The position of wi in the vocabulary array is obtained as i = posInVoc[ p], so that codeword Ci is
computed and sent. Now, word wi must be promoted to the next block. For this reason, the sender
algorithm ﬁnds the head of its block j = top[ f ] and the corresponding position h of the word in
the hash table h= posInHT[ j]. Now, it is necessary to swap words i and j in vector posInHT.
The swapping requires exchanging posInHT[ j]= h with posInHT[i]= p, setting posInVoc[ p]=
j and posInVoc[h]=i. Once the swapping is done,
j is promoted to the next block by setting
top[ f ]= j+1. If wi turns out to be a new word, the sender will set word[ p]= wi , freq[ p]=0, and
posInVoc[ p]= zeroNode. Then, the above procedure is followed with f =0. Finally, zeroNode is
also increased.

The receiver works very similarly to the sender, and it is even simpler. Its algorithm pseudo-code,
plus that of the sender, are shown in Figure 4. Figures 2 and 3 give an example of how the sender
encodes the sequence of words ABABBCCDD and how the receiver decodes them.

3. DYNAMIC (s, c)-DENSE CODES

(s,c)-Dense codes

3.1.
ETDC uses 2b−1 digits, from 0 to 2b−1−1, for the bytes that do not end a codeword (continuers),
and the other 2b−1 digits, from 2b−1 to 2b−1, for the last byte of the codeword (stoppers)§. Instead
of using a ﬁxed number of stoppers and continuers, SCDC [6] adapts their number to the word
frequency distribution in the corpus.
Deﬁnition 2. Given source symbols with non-increasing probabilities { pi}0≤i <n, the corresponding
SCDC for the symbol in position i has k bytes (k≥1), for the k that satisﬁes

ck−1−1
c−1

s

≤i <s

ck−1
c−1

Thus, the codeword corresponding to source symbol i is formed by k−1 digits in base c added
to s, and a ﬁnal base-s digit. If k=1 then the codeword is simply the stopper i. Otherwise the
codeword is formed by the number (cid:8)x /s(cid:9) written in base c, and adding s to each digit, followed
by x mod s, where x =i−(sck−1−s)/(c−1).
That is, using symbols of b=8 bits, the encoding process can be described as follows:
• One-byte codewords from 0 to s−1 are given to the ﬁrst s words in the vocabulary.

§ For generality, we will keep considering bytes of b bits, not only 8.

Copyright q

2008 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2008; 38:1429–1450
DOI: 10.1002/spe

N. R. BRISABOA ET AL.

1438
• Words ranked from s to s+sc−1 are sequentially assigned two-byte codewords. The ﬁrst
byte of each codeword has a value in the range [s, s+c−1] and the second in the range
[0, s−1].
• Words from s+sc to s+sc+sc2−1 are assigned three-byte codewords, and so on.
Example 3.1. The codes assigned to symbols i ∈0 . . . 15 by a (2,3)-dense code are as follows:
(cid:4)0(cid:5), (cid:4)1(cid:5), (cid:4)2:0(cid:5), (cid:4)2:1(cid:5), (cid:4)3:0(cid:5), (cid:4)3:1(cid:5), (cid:4)4:0(cid:5), (cid:4)4:1(cid:5), (cid:4)2:2:0(cid:5), (cid:4)2:2:1(cid:5), (cid:4)2:3:0(cid:5), (cid:4)2:3:1(cid:5), (cid:4)2:4:0(cid:5),
(cid:4)2:4:1(cid:5), (cid:4)3:2:0(cid:5), and (cid:4)3:2:1(cid:5).
It is clear from Deﬁnition 2 that ETDC is a (2b−1,2b−1)-dense code and therefore SCDC is
a generalization of ETDC that can obtain better compression by adjusting s and c to the text
distribution. As in ETDC, the code does not depend on the exact symbol probabilities, just on their
ordering by frequency.
The problem now consists of ﬁnding the s and c values (assuming a ﬁxed b where 2b= s+c) that
minimize the size of the compressed text for a speciﬁc word frequency distribution. A discussion on
how to obtain the values that minimize the size of the compressed text for a speciﬁc word frequency
distribution can be found in [6,11].
The encoding and decoding algorithms are the same as those of ETDC, taking into account that
s and c depend on the text (while with ETDC, both are always 128). Thus, on-the-ﬂy encode and
decode algorithms are also available.

SCDC has only 0.2 percentage points of excess over the optimal PH code, improving upon ETDC
by 0.7 percentage points and TH by 3.2 points. SCDC is simpler to build than Huffman Codes as
well, and code generation is 45% faster than that of Huffman codes, although a little bit slower than
ETDC (which is 60% faster than Huffman coding) because multiplications and divisions cannot be
translated into faster bit shifts. To all these properties of SCDC, we have to add, as in the case of
ETDC, all the search capabilities of TH.
As ETDC, SCDC has concepts in common with previous existing codes to compress integers.
Golomb code [17] is a bit-oriented code, instead of byte oriented, but it is also parameterized. Like
SCDC, Golomb code has a parameter (sometimes called k), which is computed to best adapt the
code to the distribution of the source symbols.

3.2. Towards DSCDCs

The main difference with respect to DETDC is that, at each step of the compression/decompression
processes, it is mandatory not only to maintain the vocabulary sorted, but also to check whether
the current value of s (and c) remains well tuned or if it should change.

The update() algorithm that maintains the list of words sorted by frequency is the same used in
the case of DETDC. In addition, the test for a possible change of s has to be performed after calling
this update process.
Both encoder and decoder start with s=256. This s value is optimal for the ﬁrst 255 words of
the vocabulary, because it permits one to encode all of them with just 1 byte. When the 256th word
arrives, s has to be decreased by 1 as a 2-byte codeword is needed. From this point on, s and c
values are modiﬁed depending on the word frequency distribution.
We present next a heuristic technique to keep well tuned the values of s and c. Other heuristics
that work well in most cases are described in [11].

Copyright q

2008 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2008; 38:1429–1450
DOI: 10.1002/spe

NEW ADAPTIVE COMPRESSORS FOR NATURAL LANGUAGE TEXT

1439

3.3. Tuning the s and c values

The simplest approach to keep the s and c values well tuned as the compression/decompression
progresses is based on comparing the size of the compressed text depending on the s and c values
used to encode it.
The general idea is to compare the number of bytes that the compressed text, up to including
word wi , would occupy if it were encoded using s−1, s, and s+1. If that number becomes smaller
by using either s−1 or s+1 instead of s, then the compressor switches to the new value of s from
this point on. Therefore, in each step of the compression/decompression process, the value of s
changes at most by 1.
Three variables are needed: prev, curr, and next. Variable prev stores the size of the compressed
text assuming that s−1 was used in the encoding/decoding process. In the same way, curr and next
accumulate the size of the compressed text, assuming that it was encoded using the current s and
s+1, respectively. At the beginning, the three variables are initialized to zero. Each time a word wi
is processed, prev, curr, and next are increased as follows: Let countBytes(i) be the function that
computes the number of bytes needed to encode the ith word of the vocabulary. Then, the three
variables are increased as follows:
• prev← prev+countBytes(s−1,i );
• curr← curr+countBytes(s,i );
• next← next+countBytes(s+1,i ).
A change of the s value takes place either if prev<curr or if next<curr. If prev<curr, then
s−1 will become the new value of s (s← s−1). On the other hand, if next<curr, then s will be
increased (s← s+1).
Therefore, we can easily decide in which direction s should be modiﬁed. Each time s changes, the
values prev, curr, and next are initialized again, and then the process continues. This initialization
depends on the change of s that took place.

In order to keep the history of the process, we do not initialize the three values to zero, but we
use the previous values. Of course, one of the three values (either prev or next, depending on the
direction of the change of s) is unknown and it is set to the same value of curr. That is,
• If s is increased then prev← curr and curr← next (next does not change)

10 6 5 → 6 5 5

• If s is decreased then next← curr and curr← prev (prev does not change)

20 21 23 → 20 20 21

prev curr next

prev curr next

prev curr next

prev curr next

There are other alternatives for this basic algorithm. For example, it would be possible to use an ♦
value as a threshold for the change in s. That is, the value of s would change only if prev+♦<curr or
next+♦<curr. In this way, fewer changes would take place, but in our experiments the differences
were negligible.

Another possible choice would be to initialize the three variables prev, curr, and next to zero when
s changes. This choice would make the algorithm free from the previous history. This approach can

Copyright q

2008 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2008; 38:1429–1450
DOI: 10.1002/spe

1440

N. R. BRISABOA ET AL.

Figure 5. countBytes and CheckAndUpdateS algorithms.

Figure 6. Evolution of s as the vocabulary grows.

be interesting in natural language documents where the vocabulary, and consequently its frequency
distribution, changes frequently along the text. However, our experiments showed again that the
differences in compression ratio were less than 0.01%.

Checking whether s and c should change is carried out by CheckAndUpdateS() algorithm. The
pseudo-code of this algorithm and the one of countBytes() are shown in Figure 5. Notice that
countBytes() is called at least twice in each execution of the CheckAndUpdateS() algorithm. The
cost of countBytes() depends on the maximum codeword length, so its overall cost is proportional
to the number of output symbols. This shows that s and c can be maintained well tuned without
altering the overall complexity.

Figure 6 shows how the s value evolves in practice as compression progresses (the collections are
described in Section 5). It can be seen that the dynamic encoder adapts the s value rapidly in order
to reduce the codeword length. Therefore, the s value falls from 256 to 129 (recall that b=8) when
the ﬁrst 16512 words are processed. When n>16512, 3-byte codewords are needed; therefore, the
s value is increased. Fluctuations of s beyond that point depend on the word distribution.

4. BLOCK-WISE VERSIONS OF DENSE CODES

A natural choice to cope with the real-time scenario is to cut the text into blocks that can be
compressed separately using a semistatic compressor. This solution is simple and likely to provide
efﬁcient decompression and searching. The idea behind this technique is, on the one hand, that it

Copyright q

2008 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2008; 38:1429–1450
DOI: 10.1002/spe

NEW ADAPTIVE COMPRESSORS FOR NATURAL LANGUAGE TEXT

1441

Table I. Compression ratio (in percentage) of a semistatic block-wise ETDC with different block sizes

(in Mbytes) vs ETDC and DETDC.

0.5 Mb

39.26
42.07
41.53

1 Mb

36.80
39.35
39.04

2 Mb

35.21
37.45
37.33

5 Mb

33.50
35.60
35.55

10 Mb

32.58
34.57
34.58

15 Mb

32.28
34.12
34.13

ETDC

31.94
32.90
33.66

DETDC

31.99
32.91
33.66

CR
AP
ALL

can adapt better to different distributions on different parts of the text, and on the other hand, that
the codewords used in each block are shorter on average, as there are fewer different words in each
block than in the whole text.

However, it is not obvious whether or not this is a good idea. If the blocks have to be too large to
provide good compression, due to the burden of storing the local vocabulary, the real-time nature
of the scheme might be questionable. We performed several studies in this line, and we found
that the simple approach of cutting the text into blocks, and compressing them with a semistatic
approach, does not obtain good results, as can be seen in Table I (the collections are described
in Section 5). As shown, using a simple block-wise compressor represents a severe restriction for
real-time transmission, since to achieve competitive compression ratios the sender would have to
delay the transmission of the compressed text until the available text reaches a considerable size
(recall the scenarios depicted in Section 1).
In [7], some techniques to store the vocabulary of each block in the form of a short prelude
were shown. However, in this work it is assumed that the sender and the receiver share the entire
vocabulary, and therefore the prelude only provides information about which words, from the general
vocabulary, are present in the block, in addition to some information needed for the encoding. Yet,
in the case of a dynamic scenario, it is necessary to send new words (or separators), which were not
known until then, as compression progresses. Therefore, further development is required to proﬁt
from this research line.

5. EXPERIMENTAL RESULTS

We used a large text collection from TREC-2¶ , namely AP Newswire 1988 (AP), as well as from
TREC-4, namely Congressional Record 1993 (CR). As a small collection we used the Calgary
(cid:13)
corpus
(CALGARY). We created two larger corpora ALL FT and ALL by aggregating several
texts from TREC-2, TREC-4, and the Calgary corpus. We used the spaceless word model [4] to create
the vocabulary, that is, if a word was followed by a space, we just encoded the word, otherwise
both the word and the separator were encoded.

We empirically compared the compression ratio and compression/decompression speed of
DETDC and DSCDC with our own implementation of a dynamic word-based byte-oriented PH

¶ http://trec.nist.gov.
(cid:13)
ftp://ftp.cpsc.ucalgary.ca/pub/projects/text.compression.corpus.

Copyright q

2008 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2008; 38:1429–1450
DOI: 10.1002/spe

1442

N. R. BRISABOA ET AL.

††

, Gzip

‡‡

∗∗

, a Ziv–Lempel compressor with performance very similar to zip [9], Bzip2

(DPH)
, a
block-sorting compressor [18] and an arithmetic encoder coupled with a word-based modeller [19].
Finally, as a baseline, we included dynamic (DVbyte) and semistatic (SVbyte) implementations of
a variable byte code [7,14]. DVbyte and SVbyte do not use any statistical information of the text in
order to obtain a good compression ratio. They just assign the ﬁrst codeword to the ﬁrst word in
the text, the second codeword to the second distinct word in the text, and so on. As shown later,
this yields worse compression ratio than ETDC and DETDC, but better compression times. Tables
show the default compression setting of Gzip and Bzip2. ETDC, SCDC, and PH, the semistatic
counterparts of DETDC, DSCDC, and DPH, respectively, were also included in the comparisons
to discuss the effects of dynamism.
An isolated Intel®Pentium®-IV 3.00 GHz system (16 kb L1+1024kb L2 cache), with 4 Gb dualchannel 
DDR-400 MHz RAM, was used in our tests. It ran Debian GNU/Linux (kernel version
2.4.27). The compiler used was gcc version 3.3.5 and -O9 compiler optimizations were set. Time
results measure CPU user time in seconds.

5.1. Compression ratio, compression, and decompression time

Table IIa shows the compression ratios obtained when compressing the different corpora. As
expected, Bzip2 yields the best compression ratio. Gzip obtains the worst compression ratio in
medium-sized texts, whereas in the shortest and largest texts, both versions of the Vbyte code are the
worst. Arithmetic compression also obtains good results (1–2 percentage points over Bzip2). Our
techniques compress more than Gzip except in small collections (CALGARY), where the vocabulary
size is still signiﬁcant compared with the text size. DPH, which generates optimal preﬁx-free codes,
overcomes DSCDC by less than 0.3 percentage points, and DETDC loses around 0.6 percentage
points with respect to DSCDC. It is important to note the gain in compression achieved by DETDC
and ETDC with respect to DVbyte and SVbyte, respectively; the use of a statistical model results
in 3–11 percentage points of improvement, depending on the size of the text. Finally, it is also
interesting to point out that adding dynamism to PH, ETDC, and SCDC involves only a slight loss
of compression ratio. In most cases, such a loss is less than 0.1 percentage points.

Table IIb shows compression times. As expected, DVbyte is the fastest alternative, as it neither
computes statistics of the source text nor performs vocabulary permutations during the compression
process. Yet, DETDC is also very fast, obtaining much better compression.

DSCDC is slightly slower than DETDC due to the need of maintaining parameters s and c well
tuned. DPH also obtains good performance, but it is overcome by the dense compressors because
of the complexity of dynamically maintaining a well-formed byte-oriented Huffman tree. Bzip2,
Arith, and Gzip are signiﬁcantly slower than the rest of the techniques.

Comparing the dynamic techniques with their semistatic counterparts (PH, ETDC, SCDC, and
SVbyte), it can be observed that performing only one pass over the text to compress makes dynamic
techniques faster. In fact, DETDC is around 30% faster than ETDC, DSCDC overcomes SCDC
by 20%, and ﬁnally the difference between DVbyte and SVbyte is around 80%. DPH is also able

∗∗
††
‡‡

Open-source implementations of DETDC, DSCDC and DPH, as well as pseudo-codes for their compression and
decompression processes, are available at http://rosalia.dc.ﬁ.udc.es/codes/.
http://www.gzip.org/.
http://www.bzip.org/.

Copyright q

2008 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2008; 38:1429–1450
DOI: 10.1002/spe

NEW ADAPTIVE COMPRESSORS FOR NATURAL LANGUAGE TEXT

1443

to overcome PH in compression speed. However, as DPH might have to update a higher Huffman
tree for each source word, gaps (in compression speed) between DPH and PH decrease as the size
of the collection grows.

Table IIc shows decompression times. We remark that Gzip is regarded as a very efﬁcient technique 
for decoding. However, dynamic dense codes still obtain good times. DETDC has a decompression 
performance similar to that of Gzip (except in corpus ALL). DSCDC pays the extra cost
of maintaining the values of s and c tuned (as well as a slightly slower decoding algorithm) and is
overcome by DETDC by around 10%. DPH, due to its complex update algorithm, is about 2 times
slower than DETDC and DSCDC. Finally, DVbyte is faster than Gzip, DETDC, and DSCDC. More
precisely, it is around 40% faster than DETDC, again due to its simplicity.
As expected [6], the semistatic techniques obtain the best decompression times. The arithmetic
compressor and Bzip2 are by far the slowest techniques.

To sum up, DETDC is easier to program, compresses more and faster than Gzip, also being
very fast at decompression. Yet, DETDC requires more memory than Gzip. We used the ALL
corpus to test the memory consumption of DETDC and DSCDC compared with that of Gzip.
In compression, DETDC and DSCDC consume around 90 Mbytes, whereas Gzip only consumes
720 kbytes. Decompression depicts a similar situation: dense compressors consume 60 Mbytes and
Gzip uses 520 kbytes. Note, however, that Gzip cannot proﬁt from using more memory, as that
could only be used to enlarge the window size, and this is chosen to be generally optimal (a longer
window requires longer pointers in the compressed text). Thus, there is really no compression ratio

Table II. Compression ratio (in percentage) (a) and, compression (b) and decompression (c) times (in seconds).

Corpus

Size (kb) Gzip

DPH DETDC DSCDC DVbyte

PH

ETDC SCDC SVbyte Arith Bzip2

(a)
CALGARY
CR
AP
ALL FT
ALL

Corpus

(b)
CALGARY
CR
AP
ALL FT
ALL

Corpus

(c)
CALGARY
CR
AP
ALL FT
ALL

2 081
49 888
244 760
577 704
1 055 391

36.95 46.55 47.73
33.29 31.10 31.99
37.32 32.09 32.91
34.94 31.71 32.54
35.09 32.85 33.66

46.81
31.33
32.36
31.85
33.03

55.18
34.83
36.96
41.59
45.00

46.24 47.40 46.61 53.16
31.06 31.94 31.29 34.83
32.07 32.90 32.35 36.95
31.70 32.53 31.84 41.59
32.83 33.66 33.02 45.00

34.68
26.30
27.94
27.85
27.98

28.92
24.14
27.25
25.87
25.98

Gzip

DPH DETDC DSCDC DVbyte

PH

ETDC SCDC SVbyte Arith Bzip2

0.12
2.78

0.09
0.34
7.47
2.16
39.09 15.13 11.91
85.05 35.79 28.20
160.01 71.54 55.31

0.11
2.41
13.39
31.52
61.35

0.07
1.66
8.79
20.44
39.55

0.16
3.05

0.16
3.07

0.70
0.41
0.15
0.16
17.58
7.62
3.07
2.92
39.65
85.69
16.21 16.55 16.38 16.05
37.90 39.27 38.70 38.25
93.58 208.58
72.77 75.58 75.20 73.78 171.56 375.55

Gzip

DPH DETDC DSCDC DVbyte

PH

ETDC SCDC SVbyte Arith Bzip2

0.04
0.04
0.07
0.94
0.94
1.83
5.19 10.24
5.27
11.38 24.01 12.63
21.05 50.82 25.27

0.05
1.10
6.10
14.56
28.62

0.03
0.63
3.67
8.52
17.45

0.30
0.04
7.14
0.62
37.75
3.29
7.60
85.11
14.26 14.56 15.08 16.96 147.15 156.18

0.38
6.57
33.89
80.29

0.03
0.59
3.34
7.54

0.04
0.67
3.46
7.96

0.03
0.62
3.44
8.13

Copyright q

2008 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2008; 38:1429–1450
DOI: 10.1002/spe

1444

N. R. BRISABOA ET AL.

Figure 7. Compression ratio (left), compression time (middle), and decompression time (right).

vs memory space trade-off. In current computers, the space required by the dense coders is perfectly
affordable.

The good features inherited from ETDC make DETDC an interesting choice for dynamic
compression of natural language texts. DSCDC is also a good alternative to Gzip. It compresses
faster than Gzip and its compression ratio is much better. DVbyte is a very fast alternative, but its
compression ratios make DETDC a better choice to obtain a good balance between space and time.
An overall comparison among all the discussed compression techniques is given in Figure 7.

6. SEARCHING COMPRESSED AND UNCOMPRESSED TEXT

We performed multi-pattern searches for randomly chosen patterns over both the compressed and
uncompressed versions of collection ALL. We present results for four search algorithms that work
over compressed text and four well-known algorithms for searching plain text.
The ﬁrst technique works over text compressed with ETDC and SCDC. We use our own implementation 
of Set-Horspool algorithm [20,21], with the small modiﬁcation needed to deal with
ETDC and SCDC [6,11] (namely, it is necessary to verify that the byte that precedes an occurrence
is actually a stopper, as in a dense code a codeword can be a sufﬁx of a longer codeword). In this
case, the search patterns are ﬁrst encoded and then directly searched for in the compressed text.
Results regarding searches over text compressed with PH are not included here as they are known
to be much worse than those on ETDC and SCDC [6].
On the other hand, we consider searches over text compressed with DETDC. Since the codewords
generated by DETDC might vary each time a source word is input, a Boyer–Moore-type search is
not suitable. In practice, searching text compressed with DETDC (and also with DSCDC) consists
in simulating the decompression process (just without emitting the source words), so that all bytes
in the compressed ﬁle are processed. This is the reason why we call it all-bytes. Basically, the
searcher processes the whole ﬁle one codeword at a time keeping track of the codewords associated
with the searched patterns along the compressed text, and reporting their occurrences. In DETDC,
the searcher might only be interested in counting the occurrences of the patterns (for example,
to classify documents) or might be interested in displaying an uncompressed context around each
occurrence. If local decompression is needed, the searcher must not only search for the patterns, but
also be able to rebuild the vocabulary of the decompressor. This variant is marked as all-bytes+dec

Copyright q

2008 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2008; 38:1429–1450
DOI: 10.1002/spe

NEW ADAPTIVE COMPRESSORS FOR NATURAL LANGUAGE TEXT

1445

in the experiments. We do not include search times for DSCDC nor for DPH as, just as they are
slower than DETDC at decompression, they also obtain worse results at searches.
The fourth search tool included in our comparison is the author’s implementation of LZgrep [12].
LZgrep permits searching text compressed with LZ77/LZ78/LZW formats [9,10] faster than
performing decompression plus searching. In our experiments, we aimed at using the best alternative
for decompression and searching. Since LZ77 is the fastest Ziv–Lempel variant at decompression,
applying LZgrep over text compressed with a LZ77-based technique, such as Gzip, is the fastest
choice. Therefore, LZgrep was run over text compressed with Gzip-9§§.

Four different algorithms were tested to search the uncompressed text: (i) our own implementation 
of Set-Horspool algorithm; (ii) the author’s implementation of Set Backward Oracle Matching
(SBOM) algorithm [22], (iii) the author’s implementation of Simpliﬁed Set Backward Oracle
Matching (SSBOM) algorithm [21]; and (v) the agrep¶¶ software [23,24], a fast pattern-matching
tool that allows, among other things, searching a text for multiple patterns. Agrep searches the text
and returns those chunks containing one or more search patterns. The default chunk is a line, and
the default chunk separator is the newline character. Once the ﬁrst search pattern is found in a
chunk, agrep skips processing the remaining bytes in the chunk, which signiﬁcantly distorts the
experiment. To achieve a fair comparison, we measure the times of agrep over the text lines that
do not match any pattern. More precisely, we run agrep −s over a text T
obtained by removing
all pattern occurrences from the ALL corpus (using agrep −v>T
in a ﬁrst stage). Finally, the
obtained times were scaled to obtain statistics assuming that |T
(cid:14)|=|ALL|. This maintains essentially
the same statistics of the searched patterns and reﬂects better the real search cost of agrep.

(cid:14)

(cid:14)

By default, the search tools compared in our experiments (except agrep and LZgrep) run in silent
mode, and count the number of occurrences of the patterns in the text. LZgrep was forced to use
these two options by setting the parameters −s −c and, as shown, agrep was run with the −s
(silent) option.

To choose the search patterns, we considered the vocabulary associated with corpus ALL. From
that vocabulary, we skipped both the stopwords (prepositions, articles, etc.) and the separators
(sequences of non-alphanumerical characters), as these are almost never search targets. Yet, with
the aim of avoiding the search for misspellings, we also skipped words appearing only once in
the text. As a result, we obtained a list of candidate patterns. Then, following the model [4]
where each vocabulary word was sought with uniform probability, we extracted 100 sets with K
words of length L at random from the list of candidate patterns. As is shown in Table III, we
consider lengths L =5, 10, and >10, and each set can consist of K =5, 10, 35, 50, 100, 200,
400, and 1000 patterns. Therefore, for each pair (Li , K j ), the values shown in Table III give the
average time needed to perform 100 searches (using the same 100 sets of preselected K j search
patterns) with each of the search techniques compared. Figure 8 summarizes the results obtained
by searching for K j patterns of 5 bytes, in a graphical form. This is less precise but easier to
visualize.
Results show that searches over text compressed with DETDC can be done much faster
than decompressing plus searching. This is actually an interesting property [12] for a dynamic

§§ Searching or decompressing text compressed with Gzip is more efﬁcient as less data have to be processed. Therefore,

using Gzip -9 at compression yields the best search/decompression times.

¶¶ ftp://ftp.cs.arizona.edu/agrep/agrep-2.04.tar.Z.

Copyright q

2008 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2008; 38:1429–1450
DOI: 10.1002/spe

1446

N. R. BRISABOA ET AL.

Table III. Multi-pattern search times over corpus ALL (in seconds).

Length of

pattern

5
10
>10

5
10
>10

5
10
>10

5
10
>10

5
10
>10

5
10
>10

5
10
>10

5
10
>10

5
10
>10

5

0.645
0.681
0.657

0.609
0.638
0.615

10.484
10.501
10.521

14.559
14.476
14.529

15.129
15.171
15.121

5.815
2.975
2.944

2.065
1.913
1.921

3.796
2.803
2.847

4.106
2.902
2.935

10

0.764
0.794
0.759

0.701
0.726
0.702

10.541
10.541
10.534

14.495
14.551
14.616

15.085
15.120
15.168

5.993
3.306
3.372

3.194
2.992
2.975

4.784
3.491
3.611

5.161
3.503
3.623

Number of patterns

50

1.807
1.732
1.750

1.584
1.535
1.522

10.471
10.535
10.507

14.525
14.511
14.522

15.066
15.083
15.176

8.355
5.109
5.190

7.469
5.712
5.748

8.174
6.796
6.883

8.574
6.405
6.459

100

2.497
2.470
2.488

2.283
2.299
2.282

10.667
10.707
10.688

14.602
14.664
14.677

15.139
15.162
15.130

12.206
5.699
5.728

9.143
6.724
6.815

10.934
8.213
8.201

11.106
7.684
7.726

200

3.223
3.148
3.164

3.289
3.195
3.151

10.557
10.539
10.573

14.574
14.550
14.560

—
—
—

12.756
6.535
6.703

11.138
8.030
8.070

13.290
9.924
10.051

13.544
9.447
9.617

25

1.158
1.137
1.151

1.017
1.017
0.993

10.557
10.512
10.624

14.516
14.521
14.509

15.200
15.201
15.182

6.949
4.482
4.294

5.491
4.566
4.677

6.020
5.238
5.384

6.432
5.046
5.160

400

3.485
3.393
3.419

3.826
3.675
3.598

10.643
10.634
10.625

14.579
14.586
14.603

—
—
—

14.056
7.875
8.368

13.262
9.633
9.644

15.996
9.924
12.616

16.680
12.117
12.373

1000

3.902
3.732
3.746

4.528
4.031
4.067

10.660
10.693
10.734

14.619
14.669
14.662

—
—
—

18.049
10.614
11.677

16.456
12.847
13.067

21.688
12.373
17.372

23.102
17.369
18.103

Search type

ETDC
Set-Horspool

SCDC
Set-Horspool

DETDC
all bytes
DETDC + dec
all bytes
LZgrep −s −c

Agrep

Set-Horspool

SBOM

SSBOM

compressor. Moreover, all-bytes+dec obtains slightly better search times than LZgrep, its main
competitor in a dynamic scenario.

Of course, searches over text compressed with a dynamic compressor cannot compete against
searches over text compressed with a semistatic compressor, being also usually slower than just
searching the uncompressed text when a few patterns are sought.
As shown in previous studies [6], searching text compressed with ETDC and SCDC is much
faster than searching the uncompressed text, and as the results of the experiments of this work
show, the semistatic dense codes are around 3–4 times faster than searching text compressed with
DETDC.

As expected, the use of longer patterns improves the search speed in the uncompressed text.
However, this has little effect in the search time over text compressed with ETDC and SCDC, as

Copyright q

2008 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2008; 38:1429–1450
DOI: 10.1002/spe

NEW ADAPTIVE COMPRESSORS FOR NATURAL LANGUAGE TEXT

1447

Figure 8. Searching for a variable number of patterns of ﬁve letters.

in this case the searcher usually looks for codes of 2–3 bytes. In general, 1-byte codes are rarely
searched for when we search for less than 400 patterns (because there are few of them, and they
usually correspond to stopwords). In the case of searches over text compressed with DETDC, search
times are independent of the length of the patterns, as they only depend on the number of codewords
in the compressed ﬁle.

If we focus on the search algorithms based on Set-Horspool, we realize that a larger number of
patterns favor the search on the compressed over the uncompressed text. The main reason is that
Horspool’s algorithm beneﬁts from a lower probability of two characters (from the text and the
pattern) being equal. The lower the probability, the more the patterns that can be handled efﬁciently.
≈0.008,
In the compressed version (using ETDC) of the corpus ALL, this probability is
whereas in the plain version, it is

≈0.052.

119.4

1

1

19.3

As was introduced above, searching text compressed with DETDC can be done more efﬁciently
than decompressing plus searching, as happens in LZgrep, but not as fast as just searching the
uncompressed version of the text. However, when a large number of patterns (>100) are sought,
searching DETDC becomes faster than searching the uncompressed text. This occurs because the
simple all-bytes searcher is almost independent of the number of search patterns. The all-bytes+dec
searcher that works on DETDC is around 40% slower than the all-bytes variant. Those gaps are
the result of having to perform the whole update process of the vocabulary for each codeword
that appears in the compressed text, instead of just keeping track of the positions of the searched
patterns in the vocabulary. As in DETDC, the results obtained with LZgrep are almost independent
of the number of searched patterns and their length. Comparing DETDC with LZgrep, we found that
LZgrep is around 40–50 and 5–8% slower than all bytes and all bytes+dec searchers, respectively.

7. CONCLUSIONS

We have addressed the problem of efﬁcient transmission of natural language text documents. This
was done by adding dynamism to two existing word-based byte-oriented semistatic compressors
such as ETDC and SCDC. They obtain compression ratios slightly worse than those of their
semistatic counterparts, but better compression times.

Copyright q

2008 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2008; 38:1429–1450
DOI: 10.1002/spe

1448

N. R. BRISABOA ET AL.

More precisely, DETDC and DSCDC enjoy several desirable features: full real-time transmission,
simplicity, good compression ratios (around 31–34%), fast compression and decompression, and the
ability to search the compressed text without decompressing (searching simulates decompression,
but it is faster as the searcher does not have to output the text). The new compressors stand out as
attractive space/time trade-offs within the current state-of-the-art, and have the additional beneﬁt
of being very simple to program.

In Figure 9, we focus on corpus ALL, showing the trade-off between compression ratio and
compression and decompression speed for most of the compressors used in our experiments. The
left part of each graphic shows an enlargement of the clump of values (within a rectangle) that
appears in the main plot.

Our dynamic compressors are faster than the others at compression, with the exception of the
DVbyte, which loses between 3 and 11 percentage points of compression ratio, depending on the
size of the text. Arith and Bzip2 compress up to 20% more than ETDC, SCDC, PH, and their
dynamic versions. However, they are 3–6 times slower at compression and 7–9 times slower at
decompression. Regarding decompression speed, DETDC and DSCDC are around 20–25% slower
than their semistatic counterparts, but obtain a performance close to that of Gzip.

Our empirical results showed that searches performed over text compressed with DETDC obtain
better results than those obtained with LZgrep, and therefore, they are faster than just decompressing
plus searching. Moreover, the search times obtained are independent of the length and number of
searched patterns. This fact implies that the search over text compressed with dynamic dense codes
is faster than the well-known techniques that work over uncompressed text when many patterns are

Figure 9. Space/time trade-offs among dynamic techniques, on corpus ALL, related to

compression/decompression time.

Copyright q

2008 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2008; 38:1429–1450
DOI: 10.1002/spe

NEW ADAPTIVE COMPRESSORS FOR NATURAL LANGUAGE TEXT

1449

Figure 10. Space/time trade-offs on corpus ALL related to compression/search time when

patterns of ﬁve letters are used.

searched for (>100). Figure 10 shows the trade-off between compression ratio and search time for
all the search tools used in our experiments assuming that either 5 or 1000 patterns of 5 bytes are
searched.

As future work we are interested in the development of new dynamic codes that permit us to
search the compressed text more efﬁciently. We are now targeting at improving the promising
preliminary results obtained, in both decompression and searches, by dynamic lightweight ETDC,
an asymmetric version of DETDC presented in [25], and to extend the result to SCDC. In these
two asymmetric techniques, some loss of compression effectiveness and compression speed (with
respect to DETDC and DSCDC) is permitted in order to improve decompression time and mainly
search capabilities. In addition, we aim to develop these compressors to be used in low computational
power devices such as PDAs or mobile phones, where restrictions of memory can be found as well.

ACKNOWLEDGEMENTS

This work is funded in part (for the Spanish group) by MEC (TIN2006-15071-C03-03), Xunta de Galicia
(PGIDIT05-SIN-10502PR and 2006/4) and (for the third author) by Millennium Nucleus Center for Web
Research, grant (P04-067-F), Mideplan, Chile.

REFERENCES

1. Brisaboa N, Fari˜na A, Navarro G, Param´a J. Simple, fast, and efﬁcient natural language adaptive compression. Proceedings
of the 11th International Symposium on String Processing and Information Retrieval (SPIRE’04) (Lecture Notes in
Computer Science, vol. 3246). Springer: Berlin, 2004; 230–241.

2. Bentley JL, Sleator DD, Tarjan RE, Wei VK. A locally adaptive data compression scheme. Communications of the ACM

1986; 29(4):320–330.

Copyright q

2008 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2008; 38:1429–1450
DOI: 10.1002/spe

1450

N. R. BRISABOA ET AL.

3. Moffat A. Word-based text compression. Software—Practice and Experience 1989; 19(2):185–198.
4. Moura E, Navarro G, Ziviani N, Baeza-Yates R. Fast and ﬂexible word searching on compressed text. ACM Transactions

on Information Systems 2000; 18(2):113–139.

5. Turpin A, Moffat A. Fast ﬁle search using text compression. Proceedings of the 20th Australian Computer Science

Conference (ACSC’97), Sydney, Australia, 1997; 1–8.

6. Brisaboa N, Fari˜na A, Navarro G, Param´a J. Lightweight natural language text compression. Information Retrieval 2007;

10(1):1–33.

7. Culpepper JS, Moffat A. Enhanced byte codes with restricted preﬁx properties. Proceedings of the 12th International
Symposium on String Processing and Information Retrieval (SPIRE’05) (Lecture Notes in Computer Science, vol. 3772).
Springer: Berlin, 2005; 1–12.

8. Moffat A, Turpin A. Compression and Coding Algorithms. Kluwer Academic Publishers: Dordrecht, 2002.
9. Ziv J, Lempel A. A universal algorithm for sequential data compression. IEEE Transactions on Information Theory

1977; 23(3):337–343.

10. Ziv J, Lempel A. Compression of individual sequences via variable-rate coding. IEEE Transactions on Information

Theory 1978; 24(5):530–536.

11. Fari˜na A. New compression codes for text databases. PhD Thesis, Database Laboratory, University of A Coru˜na, Spain,

2005. Available at: http://coba.dc.ﬁ.udc.es/˜fari/phd/ [6 June 2005].

12. Navarro G, Tarhio J. LZgrep: A Boyer–Moore string matching tool for Ziv–Lempel compressed text. Software—
Practice and Experience 2005; 35(12):1107–1130. Relevant software available at http://www.dcc.uchile.cl/˜gnavarro/
software/lzgrep.tar.gz [7 November 2006].

13. Boyer RS, Moore JS. A fast string searching algorithm. Communications of the ACM 1977; 20(10):762–772.
14. Williams HE, Zobel J. Compressing integers for fast ﬁle access. COMPJ: The Computer Journal 1999; 42(3):193–201.
15. Heaps HS. Information Retrieval: Computational and Theoretical Aspects. Academic Press: New York, 1978.
16. Knuth DE. Dynamic Huffman coding. Journal of Algorithms 1985; 6(2):163–180.
17. Golomb SW. Run-length encodings. IEEE Transactions on Information Theory 1966; IT-12:399–401.
18. Burrows M, Wheeler DJ. A block-sorting lossless data compression algorithm. Technical Report 124, Digital Equipment

Corporation, 1994.

19. Carpinelli J, Moffat A, Neal R, Salamonsen W, Stuiver L, Turpin A, Witten I. Word, Character, Integer, and Bit Based
Compression Using Arithmetic Coding. Relevant software available at http://www.cs.mu.oz.au/˜alistair/arith coder/, 1999
[20 July 2004].

20. Horspool RN. Practical fast searching in strings. Software—Practice and Experience 1980; 10(6):501–506.
21. Navarro G, Rafﬁnot M. Flexible Pattern Matching in Strings—Practical On-line Search Algorithms for Texts and

Biological Sequences. Cambridge University Press: Cambridge, 2002.

22. Allauzen C, Crochemore M, Rafﬁnot M. Factor oracle: A new structure for pattern matching. Proceedings of the 26th
Annual Conference on Current Trends in Theory and Practice of Informatics (SOFSEM’99) (Lecture Notes in Computer
Science, vol. 1725). Springer: Berlin, 1999; 291–306.

23. Wu S, Manber U. Fast text searching allowing errors. Communications of the ACM 1992; 35(10):83–91.
24. Wu S, Manber U. Agrep—A fast approximate pattern-matching tool. Proceedings of the USENIX Winter 1992 Technical

Conference, San Francisco, CA, U.S.A., 1992; 153–162.

25. Brisaboa N, Fari˜na A, Navarro G, Param´a J. Efﬁciently decodable and searchable natural language adaptive compression.
Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information
Retrieval (SIGIR’05). ACM Press: New York City, NY, 2005; 234–241.

Copyright q

2008 John Wiley & Sons, Ltd.

Softw. Pract. Exper. 2008; 38:1429–1450
DOI: 10.1002/spe

