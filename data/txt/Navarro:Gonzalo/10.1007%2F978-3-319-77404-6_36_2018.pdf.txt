On the Approximation Ratio

of Lempel-Ziv Parsing

Travis Gagie1,2, Gonzalo Navarro2,3(B), and Nicola Prezza4

1 EIT, Diego Portales University, Santiago, Chile

2 Center for Biotechnology and Bioengineering (CeBiB), Santiago, Chile

3 Department of Computer Science, University of Chile, Santiago, Chile

4 DTU Compute, Technical University of Denmark, Kongens Lyngby, Denmark

gnavarro@dcc.uchile.cl

Abstract. Shannon’s entropy is a clear lower bound for statistical compression.
 The situation is not so well understood for dictionary-based
compression. A plausible lower bound is b, the least number of phrases
of a general bidirectional parse of a text, where phrases can be copied
from anywhere else in the text. Since computing b is NP-complete, a
popular gold standard is z, the number of phrases in the Lempel-Ziv
parse of the text, where phrases can be copied only from the left. While
z can be computed in linear time, almost nothing has been known for
decades about its approximation ratio with respect to b. In this paper
we prove that z = O(b log(n/b)), where n is the text length. We also
show that the bound is tight as a function of n, by exhibiting a string
family where z = Ω(b log n). Our upper bound is obtained by building a
run-length context-free grammar based on a locally consistent parsing of
the text. Our lower bound is obtained by relating b with r, the number of
equal-letter runs in the Burrows-Wheeler transform of the text. On our
way, we prove other relevant bounds between compressibility measures.

1 Introduction

Shannon [33] deﬁned a measure of entropy that serves as a lower bound to
the attainable compression ratio on any source that emits symbols according
to a certain probabilistic model. An attempt to measure the compressibility of
ﬁnite texts T [1 . . . n], other than the non-computable Kolmogorov complexity
[21], is the notion of empirical entropy [7], where some probabilistic model is
assumed and its parameters are estimated from the frequencies observed in the
text. Other measures that, if the text is generated from a probabilistic source,
converge to its Shannon entropy, are derived from the Lempel-Ziv parsing [23]
or the grammar-compression [20] of the text.

Some text families, however, are not well modeled as coming from a probabilistic 
source. A very current case is that of highly repetitive texts, where most

Partially funded by Basal Funds FB0001, Conicyt, by Fondecyt Grants 1-171058
and 1-170048, Chile, and by the Danish Research Council DFF-4005-00267.

c(cid:2) Springer International Publishing AG, part of Springer Nature 2018
M. A. Bender et al. (Eds.): LATIN 2018, LNCS 10807, pp. 490–503, 2018.
https://doi.org/10.1007/978-3-319-77404-6_36

On the Approximation Ratio of Lempel-Ziv Parsing

491

of the text can be obtained by copying long blocks from elsewhere in the same
text. Huge highly repetitive text collections are arising from the sequencing of
myriads of genomes of the same species, from versioned document repositories
like Wikipedia, from source code repositories like GitHub, etc. Their growth is
outpacing Moore’s Law by a wide margin [34]. Understanding the compressibility 
of highly repetitive texts is important to properly compress those huge
collections.

Lempel-Ziv and grammar compression are particular cases of so-called dictionary 
techniques, where a set of strings is deﬁned and the text is parsed as a
concatenation of those strings. On repetitive collections, the empirical entropy
ceases to be a relevant compressibility measure; for example the kth order persymbol 
entropy of T T is the same as that of T , if k (cid:2) n [22, Lemma 2.6], whereas
this entropy measure is generally meaningless for k > log n [12]. Some dictionary
measures, instead, capture much better the compressibility of repetitive texts.
For example, while an individual genome can rarely be compressed to much less
than 2 bits per symbol, Lempel-Ziv has been reported to compress collections
of human genomes to less than 1% [11]. Similar compression ratios are reported
in Wikipedia.1

Despite the obvious practical relevance of these compressibility measures,
there is not a clear entropy measure useful for highly repetitive texts. The number 
z of phrases generated by the Lempel-Ziv parse [23] is often used as a gold
standard, possibly because it can be implemented in linear time [30] and is never
larger than g, the size of the smallest context-free grammar that generates the
text [6,31]. However, z is not so satisfactory as an entropy measure: the value
changes if we reverse the text, for example. A much more robust lower bound
on compressibility is b, the size of the smallest bidirectional (macro) scheme
[35]. Such a scheme parses the text into phrases such that each phrase appears
somewhere else in the text (or it is a single explicit symbol), so that it is possible 
to recover the text by copying source to target positions in an appropriate
order. This is arguably the strongest possible dictionary method, but ﬁnding the
smallest bidirectional scheme is NP-complete [13]. A relevant question is then
how good is the Lempel-Ziv parse as an eﬃciently implementable approximation
to the smallest bidirectional scheme. Almost nothing is known in this respect,
except that there are string families where z is nearly 2b [35].

In this paper we ﬁnally give a tight approximation ratio for z, showing 
that the gap is larger than what was previously known. We prove that
z = O(b log(n/b)), and that this bound is tight as a function of n, by exhibiting
a string family where z = Ω(b log n). To prove the upper bound, we show how
to build a run-length context-free grammar [28] (i.e., allowing rules of the form
X → Y t that count as size 1) of size grl = O(b log(n/b)). This is done by carrying
out several rounds of locally consistent parsing [17] on top of T , reducing the
resulting blocks to nonterminals in each round, and showing that new nonterminals 
appear only in the boundaries of the phrases of the bidirectional scheme. We
then further prove that z ≤ 2grl, by extending a classical proof [6] that relates

1 https://en.wikipedia.org/wiki/Wikipedia:Size of Wikipedia.

492

T. Gagie et al.

grammar with Lempel-Ziv compression. To prove the lower bound, we consider
another plausible compressibility measure: the number r of equal-symbol runs in
the Burrows-Wheeler transform (BWT) of the text [5]. We prove that the BWT
induces a valid bidirectional scheme, and thus r = Ω(b). Then the bound follows
from known string families where z = Ω(r log n) [29].

2 Basic Concepts

A string is a sequence S[1 . . . (cid:3)] = S[1]S[2] . . . S[(cid:3)] of symbols. A substring
S[i] . . . S[j] of S is denoted S[i . . . j]. A suﬃx of S is a substring of the form
S[i . . . (cid:3)]. The juxtaposition of strings and/or symbols represents their concatenation.
 We will consider compressing a string T [1 . . . n], called the text.

2.1 Bidirectional Schemes

A bidirectional scheme [35] partitions T [1 . . . n] into b chunks B1, . . . , Bb, such
that each Bi = T [ti . . . ti + (cid:3)i − 1] (called a target) is either (1) copied from
another substring T [si . . . si + (cid:3)i − 1] (called a source) with si (cid:5)= ti, which may
overlap T [ti . . . ti + (cid:3)i − 1], or (2) formed by (cid:3)i = 1 explicit symbol.
We deﬁne the function f : [1 . . . n] → [1 . . . n] so that, in case (1), f(ti + j) =
si + j for all 0 ≤ j < (cid:3)i, and in case (2), f(ti) = −1. Then, the bidirectional
scheme is valid if there is an order in which the sources si + j can be copied onto
the targets ti + j so that all the positions of T can be inferred.

Being a valid scheme is equivalent to saying that f has no cycles, that is,
there is no k > 0 and p such that f k(p) = p: Initially we can set all the explicit
positions (type (2)), and then copy sources with known values to their targets.
If f has no cycles, we will eventually complete all the positions in T because, for
every T [p], there is a k > 0 such that f k(p) = −1, so we can obtain T [p] from
the symbol explicitly stored for T [f k−1(p)].

We use b to denote the smallest bidirectional scheme, which is NP-complete

to compute [13].

2.2 Lempel-Ziv Parsing

Lempel and Ziv [23] deﬁne a parsing of T into the fewest possible phrases T =
Z1 . . . Zz, so that each phrase Zi is a substring (but not a suﬃx) of Z1 . . . Zi,
or a single symbol. This means that the source T [si . . . si + (cid:3)i − 1] of the target
Zi = T [ti . . . ti + (cid:3)i−1] must satisfy si < ti, but sources and targets may overlap.
It turns out that the greedy left-to-right parsing indeed produces the optimal
number z of phrases [23, Theorem 1]. Further, the parsing can be obtained in
O(n) time [30,35].

If we disallow that a phrase overlaps its source, that is, Zi must be a substring
of Z1 . . . Zi−1 or a single symbol, then we call zno the number of phrases obtained.
In this case it is also true that the greedy left-to-right parsing produces the

On the Approximation Ratio of Lempel-Ziv Parsing

493

optimal number zno of phrases [35, Theorem 10 with p = 1]. Since the LempelZiv 
parsing allowing overlaps is optimal among all left-to-right parsings, we also
have that zno ≥ z. This parsing can also be computed in O(n) time [8]. Note
that, on a text family like T = an, it holds that zno = Ω(z log n).
Little is known about the relation between b and z except that z ≥ b by
deﬁnition (z is the smallest unidirectional parsing) and that, for any constant
2 + ) · min(z, zR) [35,
 > 0, there is an inﬁnite family of strings for which b < ( 1
Correlation 7.1], where zR is the z of the reversed string.

2.3 Grammar Compression

Consider a context-free grammar (CFG) that generates T and only T [20]. Each
nonterminal must be the left-hand side in exactly one rule, and the size g of the
grammar is the sum of the right-hand sides of the rules. In general, we will use
g to denote the minimum possible size of a grammar that generates T , which is
NP-complete to compute [6,31].
If we allow, in addition, rules of the form X → Y t, of size 1, the result is a
run-length context-free grammar (RLCFG) [28]. We will use grl to denote the
size of the smallest RLCFG that generates T . Thus, it is clear that grl ≤ g.
Further, on the string family T = an it holds that g = Ω(grl log n).
A well-known relation between zno and g is zno ≤ g = O(zno log(n/zno))
[6,31]. Further,
it is known that g = O(z log(n/z)) [14, Lemma 8]. Those
papers exhibit O(log n)-approximations to the smallest grammar, as well as
several others [17,18,32]. A negative result about the approximation are
string families where g = Ω(zno log n/ log log n) [6,15] and, recently, grl =
Ω(zno log n/ log log n) [3].

2.4 Runs in the Burrows-Wheeler Transform

Assume that T is terminated by the special symbol T [n] = $, which is lexicographically 
smaller than all the others. This makes any lexicographic comparison
between suﬃxes well deﬁned.
The suﬃx array [25] of T [1 . . . n] is an array SA[1 . . . n] storing a permutation
of [1 . . . n] so that, for all 1 ≤ i < n, the suﬃx T [SA[i] . . .] is lexicographically
smaller than the suﬃx T [SA[i + 1] . . .]. Thus SA[i] is the starting position in T
of the ith smallest suﬃx of T in lexicographic order.

The inverse permutation of SA, ISA[1 . . . n], is called the inverse suﬃx array,
so that ISA[j] is the lexicographical position of the suﬃx T [j . . . n] among the
suﬃxes of T .
The Burrows-Wheeler Transform of T [1 . . . n], BWT [1 . . . n] [5], is a string
deﬁned as BWT [i] = T [SA[i] − 1] if SA[i] > 1, and BWT [i] = T [n] = $ if
SA[i] = 1. That is, BWT has the same symbols of T in a diﬀerent order, and is
a reversible transform.

The array BWT can be easily obtained from T and SA, which can be built in
O(n) time [19]. To obtain T from BWT [5], one considers two arrays, L[1 . . . n] =

494

T. Gagie et al.

BWT and F [1 . . . n], which contains all the symbols of L (or T ) in ascending
order. Alternatively, F [i] = T [SA[i]], so F [i] follows L[i] in T . We need a function
that maps any L[i] to the position j of that same symbol in F . The formula is
LF (i) = C[c] + rank[i], where c = L[i], C[c] is the number of occurrences of
symbols less than c in L, and rank[i] is the number of occurrences of symbol
L[i] in L[1 . . . i]. Once C and rank are computed, we reconstruct T [n] = $ and
T [n − k] ← L[LF k−1(1)] for k = 1, . . . , n − 1.

The number of equal-symbol runs r in the BWT of T can be bounded in
terms of the empirical entropy [24]. However, the measure is also interesting
on highly repetitive collections (where, in particular, z and zno are small). For
example, there are string families where z = Ω(r log n) [29], and others where
r = Ω(zno log n) [2,29].

2.5 Locally Consistent Parsing

A string can be parsed in a locally consistent way, in the sense that equal substrings 
are largely parsed in the same form. We use a variant of locally consistent
parsing called recompression [16,17].

Deﬁnition 1. A repetitive area in a string is a maximal run of the same symbol,
of length 2 or more.

Deﬁnition 2. Two segments contained in [1 . . . n] overlap if they are not disjoint 
nor one contained in the other.

(cid:3) + 1 . . . j

(cid:3) + 1 . . . j

. . . j

(cid:3) − 1] are identical.

Lemma 1 ([17]). We can partition a string S[1 . . . (cid:3)] into at most (3/4)(cid:3) blocks
(cid:3)], if neither
(cid:3)
so that, for every pair of identical substrings S[i . . . j] = S[i
(cid:3) − 1] overlap a repetitive area, then the sequence
S[i + 1 . . . j − 1] or S[i
of blocks covering S[i + 1 . . . j − 1] and S[i
Proof. The parsing is obtained by, ﬁrst, creating new symbols that represent
the repetitive areas. On the resulting sequence, the alphabet (which contains
original symbols and created ones) is partitioned into two subsets, left-symbols
and right-symbols. Then, every left-symbol followed by a right-symbol are paired
in a block. It is then clear that, if S[i + 1 . . . j − 1] and S[i
(cid:3) − 1] do not
(cid:3)] may diﬀer
(cid:3)
overlap repetitive areas, then the parsing of S[i . . . j] and S[i
only in their ﬁrst position (if it is part of a repetitive area ending there, or if it
is a right-symbol that becomes paired with the preceding one) and in their last
position (if it is part of a repetitive area starting there, or if it is a left-symbol
that becomes paired with the following one). Jez [17] shows how to choose the
(cid:8)(cid:9)
pairs so that S contains at most (3/4)(cid:3) blocks.

(cid:3) + 1 . . . j
. . . j

The lemma ensures a locally consistent parsing into blocks as long as the substrings 
do not overlap repetitive areas, though the substrings may fully contain
repetitive areas.

On the Approximation Ratio of Lempel-Ziv Parsing

495

3 Upper Bounds

In this section we obtain our main upper bound, z = O(b log(n/b)), along with
other byproducts. To this end, we ﬁrst prove that grl = O(b log(n/b)), and
then that z ≤ 2grl. To prove the ﬁrst bound, we build a RLCFG on top of a
bidirectional scheme. The grammar is built in several rounds of locally consistent
parsing on the text. In each round, the blocks of the locally consistent parsing
are converted into nonterminals and fed to the next round. The key is to prove
that distinct nonterminals are produced only at the boundaries of the phrases of
the bidirectional scheme. The second bound is an easy extension to the known
result zno ≤ g.
Theorem 1. Let T [1 . . . n] have a bidirectional scheme of size b. Then there
exists a run-length context-free grammar of size grl = O(b log(n/b)) that generates 
T .

Proof. Recalling Lemma 1, consider a locally consistent parsing of W = T into
blocks. We will count the number of diﬀerent blocks we form, as this corresponds
to the number of nonterminals produced in the ﬁrst round.

Recall from Sect. 2.1 that our bidirectional scheme represents T as a sequence
of chunks, by means of a function f. To count the number of diﬀerent blocks
produced, we will pessimistically assume that the ﬁrst two and the last two
blocks intersecting each chunk are all diﬀerent. The number of such bordering
blocks is at most 4b. On the other hand, we will show that non-bordering blocks
do not need to be considered, because they will be counted somewhere else, when
they appear near the extreme of a chunk.

We show that this is true in both types of non-bordering blocks resulting

from Lemma 1:

1. The block is a pair of leftand 
right-alphabet symbols.2 As these symbols
can be an original symbol or a maximal area, let us write the pair generically
as X = a(cid:2)ab(cid:2)b, for some (cid:3)a, (cid:3)b ≥ 1, and let (cid:3) = (cid:3)a + (cid:3)b be the length of
the block X. If W [p . . . p + (cid:3) − 1] = X is not bordering, then it is strictly
contained in a chunk. Thus, by the deﬁnition of a chunk, it holds that [f(p −
1) . . . f(p + (cid:3))] = [f(p) − 1 . . . f(p) + (cid:3)], and that W [f(p) − 1 . . . f(p) + (cid:3)] =
W [p − 1 . . . p + (cid:3)]. That is, the block appears again at [f(p) . . . f(p) + (cid:3) − 1],
surrounded by the same symbols. Since, by the way Lemma 1 works, it must
be W [f(p) − 1] = W [p − 1] (cid:5)= a and W [f(p) + (cid:3)] = W [p + (cid:3)] (cid:5)= b, and a(cid:2)a is a
left-symbol and b(cid:2)b is a right-symbol, the locally consistent parsing must also
form a block W [f(p) . . . f(p) + (cid:3) − 1] = X. If this block is bordering, then it
will be counted. Otherwise, by the same argument, W [f(p) − 1 . . . f(p) + (cid:3)]
will be equal to W [f 2(p) − 1 . . . f 2(p) + (cid:3)] and a block will be formed with
W [f 2(p) . . . f 2(p) + (cid:3) − 1]. Since f has no cycles, there is a k > 0 for which
f k(p) = −1. Thus for some l < k it must be that X = W [f l(p) . . . f l(p)+(cid:3)−1]

2 For this case, we could have deﬁned bordering in a stricter way, as the ﬁrst or last

block of a chunk.

496

T. Gagie et al.

is not bordering. At the smallest such l, the block W [f l(p) . . . f l(p)+(cid:3)−1] will
be counted. Therefore, X = W [p . . . p + (cid:3) − 1] is already counted somewhere
else and we do not need to count it at W [p . . . p + (cid:3) − 1].
2. The block is a single (original or maximal-run) symbol W [p . . . p+ (cid:3)−1] = a(cid:2),
for some (cid:3) ≥ 1. It also holds that [f(p− 1) . . . f(p + (cid:3))] = [f(p)− 1 . . . f(p) + (cid:3)]
and W [f(p) − 1..f(p) + (cid:3)] = W [p − 1 . . . p + (cid:3)], because a(cid:2) is strictly inside
a chunk. Since W [f(p) − 1] = W [p − 1] (cid:5)= a and W [f(p) + (cid:3)] = W [p + (cid:3)] (cid:5)=
a, the parsing forms the same maximal run a(cid:2) = W [f(p) . . . f(p) + (cid:3) − 1].
Moreover, since W [p . . . p + (cid:3) − 1] is not bordering, the previous and next
(cid:3)(cid:3)], are
(cid:3)
blocks produced by the parsing, X = W [p
also strictly inside the same chunk, and therefore they also appear preceding
(cid:3)) . . . f(p) − 1] and Y =
and following W [f(p) . . . f(p) + (cid:3) − 1], at X = W [f(p
(cid:3)(cid:3))]. Since a(cid:2) was not paired with X nor Y at W [p . . . p+ (cid:3)−1],
[f(p)+ (cid:3) . . . f(p
the parsing will also not pair them at W [f(p) . . . f(p) + (cid:3) − 1]. Therefore, the
parsing will leave a(cid:2) as a block also in [f(p) . . . f(p)+(cid:3)−1]. If W [f(p) . . . f(p+
(cid:3) − 1)] is bordering, then it will be counted, otherwise we can repeat the
argument with W [f 2(p) − 1 . . . f 2(p) + (cid:3)] and so on, as in the previous item.
Therefore, we produce at most 4b distinct blocks, and the RLCFG has at
most 12b nonterminals (for X = a(cid:2)ab(cid:2)b we may need 3 nonterminals, A → a(cid:2)a,
B → b(cid:2)b, and C → AB).
(cid:3) from W by replacing
all the blocks of length 2 or more by their corresponding nonterminals. The new
sequence is guaranteed to have length at most (3/4)n by Lemma 1.

For the second round, we create a reduced sequence W

. . . p−1] and Y = [p+ (cid:3) . . . p

We deﬁne a new bidirectional scheme (recall Sect. 2.1) on W

(cid:3), as follows:

1. For each bordering block in W , its nonterminal symbol position in W

explicit in the bidirectional scheme of W
covering the explicit symbols in the bidirectional scheme of W .

(cid:3) is made
(cid:3). Note that this includes the blocks
2. For the chunks Bi = W [ti . . . ti +(cid:3)i−1] of W containing non-bordering blocks
(cid:3)
(note Bi cannot be an explicit chunk), let B
i be obtained by trimming from
(cid:3)
i appears inside
Bi the bordering blocks near the extremes of Bi. Then B
W [si . . . si + (cid:3)i − 1] (with si = f(ti)), where the same sequence of blocks is
(cid:3) with sequence of
formed by our arguments above. We then form a chunk in W
(cid:3)
i (all of which are non-bordering),
nonterminals associated with the blocks of B
pointing to the identical sequence of nonterminals that appear as blocks inside
W [si . . . si + (cid:3)i − 1].
To bound the total number of nonterminals generated, let us call Wk the
sequence W after k iterations (so T = W0) and Nk the number of distinct
blocks created when converting Wk into Wk+1.
In the ﬁrst iteration, since there may be up to 4 bordering blocks around
each chunk limit, we may create N1 ≤ 4b distinct blocks. Those blocks become
(cid:3) = W1. Note that those
new explicit chunks in the bidirectional scheme of W
explicit chunks are grouped into b regions of up to 4 consecutive chunks. In each
new iteration, Wk is parsed into blocks again. We have shown that the blocks
formed outside regions (i.e., non-bordering blocks) are not distinct, so we can

On the Approximation Ratio of Lempel-Ziv Parsing

497

focus on the number of new blocks produced to parse each of the b regions. The
parsing produces at most 4 new distinct blocks extending each region. However,
the parsing of the regions themselves may also produce new distinct blocks. Our
aim is to show that the number of those blocks is also bounded because they
decrease the length of the regions, which only grow by 4b per iteration.

.  .  .  .  .

. . . . .

.  .  .  .  .

. . . . .

.....

.....

Fig. 1. Illustration of Theorem 1. On top we see the limit between two long chunks of
W0. In this example, the blocking always pairs two symbols. We show below W0 the 4
bordering blocks formed with the symbols nearby the limit. Below, in W1, those blocks
are converted into 4 explicit chunks (of length 1). This region of 4 symbols is then
parsed into 2 blocks. The parsing also creates 4 new bordering blocks from the ends of
the long chunks. In W2, below, we have now a region of 6 explicit chunks. They could
have been 8, but we created 2 distinct blocks that reduced their number to 6.

Let nk be the number of new distinct blocks produced when parsing the
regions themselves. Therefore it holds that the number of distinct blocks Nk
produced in the kth iteration is at most 4b+ nk, and the total number of distinct
blocks created up to building Wk is

(cid:2)k−1
i=0 Ni ≤ 4bk +

i=0 ni.

(cid:2)k−1

On the other hand, for each of the nk blocks created when parsing a region,
the length of the region decreases at least by 1 in Wk+1. Let us call Ck the
number of explicit chunks in Wk. Since only the 4 new bordering blocks at each
region are converted into explicit chunks, it holds that Ck ≤ 4bk for all k > 0.
Moreover, it holds Ck+1 ≤ Ck + 4b − nk, and thus 0 ≤ Ck ≤ 4bk − (cid:2)k−1
i=0 ni.
(cid:2)k−1
i=0 Ni ≤ 8bk. Since each nonterminal
Therefore,
may need 3 rules to represent a block, a bound on the number of nonterminals
created is 24bk.

(cid:2)k−1
i=0 ni ≤ 4bk and thus

After k rounds, the sequence is of length at most (3/4)kn and we have
generated at most 24bk nonterminals. Therefore, if we choose to perform k =
log4/3(n/b) rounds, the sequence will be of length at most b and the grammar
size will be O(b log(n/b)). To complete the process, we add O(b) nonterminals
to reduce the sequence to a single initial symbol.
(cid:8)(cid:9)

The idea is illustrated in Fig. 1.

With Theorem 1, we can also bound the size z of the Lempel-Ziv parse [23]
that allows overlaps. The size without allowing overlaps is known to be bounded
by the size of the smallest CFG, zno ≤ g [6,31]. We can easily see that z ≤ 2grl
also holds by extending an existing proof [6, Lemma 9] to handle the run-length

498

T. Gagie et al.

rules. We call left-to-right parse of T any parsing in which each new phrase is a
symbol or it occurs previously in T .

Theorem 2. Let a RLCFG of size grl expand to a text T . Then the Lempel-Ziv
parse (allowing overlaps) of T produces z ≤ 2grl phrases.
Proof. Consider the parse tree of T , where all internal nodes representing any
but the leftmost occurrence of a nonterminal are pruned and left as leaves. The
number of nodes in this tree is precisely grl. We say that the internal node of
nonterminal X is its deﬁnition. Our left-to-right parse of T is a sequence Z[1 . . . z]
obtained by traversing the leaves of the pruned parse tree left to right. For a
terminal leaf, we append the symbol to Z. For a leaf representing nonterminal
X, we append to Z a reference to the area T [x . . . y] expanded by the leftmost
occurrence of X.
Rules X → Y t are handled as follows. First, we expand them to X → Y ·Y t−1,
that is, the node for X has two children for Y , and it is annotated with t−1. Since
the right child of X is not the ﬁrst occurrence of Y , it must be a leaf. The left
child of X may or may not be a leaf, depending on whether Y occurred before or
not. Now, when our leaf traversal reaches the right child Y of a node X indicating
t − 1 repetitions, we append to Z a reference to T [x . . . y + (t − 2)(y − x + 1)],
where T [x . . . y] is the area expanded by the ﬁrst child of X. Note that source
and target overlap if t > 2. Thus a left-to-right parse of size 2grl exists, and
(cid:8)(cid:9)
Lempel-Ziv is the optimal left-to-right parse [23, Theorem 1].

By combining Theorems 1 and 2, we obtain a result on the long-standing
open problem of ﬁnding the approximation ratio of Lempel-Ziv compared to the
smallest bidirectional scheme.

Theorem 3. Let T [1 . . . n] have a bidirectional scheme of size b. Then the
Lempel-Ziv parsing of T allowing overlaps has z = O(b log(n/b)) phrases.

We can also derive upper bounds for g, the size of the smallest CFG, and
for zno, the size of the Lempel-Ziv parse that does not allow overlaps. It is
suﬃcient to combine the previous results with the facts that g = O(z log(n/z))
[14, Lemma 8] and zno ≤ g [6,31].
Theorem 4. Let T [1 . . . n] have a bidirectional scheme of size b. Then there
exists a context-free grammar of size g = O(b log2(n/b)) that generates T .

Theorem 5. Let T [1 . . . n] have a bidirectional scheme of size b. Then the
Lempel-Ziv parsing of T without allowing overlaps has zno = O(b log2(n/b))
phrases.

4 Lower Bounds

In this section we prove that the upper bound of Theorem 3 is tight as a function
of n, by exhibiting a family of strings for which z = Ω(b log n). This conﬁrms that

On the Approximation Ratio of Lempel-Ziv Parsing

499

ISA

SA

y
j−1

x
j

i

x−1

j
x

φ

LF

y−1

i−1

x−1
i

i−1
y−1

j−1
y

Fig. 2. Illustration of Lemma 2.

the gap between bidirectionality and unidirectionality is signiﬁcantly larger than
what was previously known. The idea is to deﬁne phrases in T accordingly to the
r runs in the BWT, and to show that these phrases induce a valid bidirectional
macro scheme of size 2r. This proves that r = Ω(b). Then we use a well-known
family of strings where z = Ω(r log n).

Deﬁnition 3. Let p1, p2, . . . , pr be the positions that start runs in the BWT, and
let s1 < s2 < . . . < sr be the corresponding positions in T , {SA[pi], 1 ≤ i ≤ r},
in increasing order. Note that s1 = 1 because BWT [ISA[1]] = $ is a size-1 run,
and assume sr+1 = n + 1, so that T is partitioned into phrases T [si . . . si+1 − 1].
Let also φ(i) = SA[ISA[i] − 1] if ISA[i] > 1 and φ(i) = SA[n] otherwise. Then
we deﬁne the bidirectional scheme of the BWT:
1. For each 1 ≤ i ≤ r, T [φ(si) . . . φ(si+1 − 2)] is copied from T [si . . . si+1 − 2].
2. For each 1 ≤ i ≤ r, T [φ(si+1 − 1)] is stored explicitly.
We build on the following lemma, illustrated in Fig. 2.

Lemma 2. Let [j−1 . . . j] be within a phrase of T . Then it holds that φ(j−1) =
φ(j) − 1 and T [j − 1] = T [φ(j) − 1].
Proof. Consider the pair of positions T [j − 1 . . . j] within a phrase. Let them be
pointed from SA[x] = j and SA[y] = j − 1, therefore ISA[j] = x, ISA[j − 1] = y,
and LF (x) = y. Now, since j is not a position at the beginning of a phrase, x is
not the ﬁrst position in a BWT run. Therefore, BWT [x − 1] = BWT [x], from
which it follows that LF (x− 1) = LF (x)− 1 = y− 1. Now let SA[x− 1] = i, that
is, i = φ(j). Then φ(j − 1) = SA[ISA[j − 1] − 1] = SA[y − 1] = SA[LF (x − 1)] =
SA[x − 1] − 1 = i − 1 = φ(j) − 1. It also follows that T [j − 1] = BWT [x] =
(cid:8)(cid:9)
BWT [x − 1] = T [i − 1] = T [φ(j) − 1].
Lemma 3. The bidirectional scheme of the BWT is a valid bidirectional scheme,
thus 2r ≥ b.
Proof. By Lemma 2, it holds that φ(j − 1) = φ(j) − 1 if [j − 1 . . . j] is within a
phrase, and that T [j − 1] = T [φ(j) − 1]. Therefore, we have that φ(si + k) =
φ(si) + k for 0 ≤ k < si+1 − si − 1, and then T [φ(si), . . . , φ(si+1 − 2)] is indeed a

500

T. Gagie et al.

Fig. 3. Known and new asymptotic bounds between repetitiveness measures. The
bounds on the left hold for every string family: an edge means that the lower measure 
is of the order of the upper. The thicker lines were proved in this paper. The
dashed lines on the right are lower bounds that hold for some string family. The solid
lines are inherited from the left, and since they always hold, they permit propagating
the lower bounds. Note that r appears twice.

contiguous range. We also have that T [φ(si) . . . φ(si+1 − 2)] = T [si . . . si+1 − 2],
and therefore it is correct to make the copy. Since φ is a permutation, every
position of T is mentioned exactly once as a target in points 1 and 2.
Finally, it is easy to see that we can recover the whole T from those 2r
directives. We can, for example, follow the cycle φk(n), k = 0, . . . , n − 1 (note
that T [φ0(n)] = T [n] is stored explicitly), and copy T [φk(n)] to T [φk+1(n)] unless
the latter is explicitly stored.
that 2r ≥ b.

Since the bidirectional scheme of the BWT is of size 2r, it follows by deﬁnition
(cid:8)(cid:9)
We are now ready to obtain the lower bound on bidirectional versus unidirectional 
parsings.
Theorem 6. There is an inﬁnite family of strings over an alphabet of size 2 for
which z = Ω(b log n).
Proof. Consider the family of the Fibonacci stings, F1 = a, F2 = b, and Fk =
Fk−1Fk−2 for all k > 2. As observed by Prezza [29, Theorem 25], for Fk we have
r = O(1) [26] and z = Θ(log n) [10]. By Lemma 3, it also holds that b = O(1),
(cid:8)(cid:9)
and therefore z = Ω(b log n).

5 Conclusions

We have essentially closed the question of which is the approximation ratio of
the (unidirectional) Lempel-Ziv parse with respect to the optimal bidirectional

On the Approximation Ratio of Lempel-Ziv Parsing

501

parse, therefore contributing to the understanding of the quality of this popular
heuristic that can be computed in linear time, whereas computing the optimal
bidirectional parse is NP-complete. Our bounds, which are shown to be tight,
show that the gap is in fact wider than what was previously known.

Figure 3(left) illustrates the known asymptotic bounds that relate the repetitiveness 
measures we have studied: b, z, zno, g, grl, and r. We also include e,
the size of the CDAWG [4] of T (i.e., the smallest compact automaton that recognizes 
the substrings of T ), which has received some attention recently [2]. It
is known that e ≥ max(z, r) [2] and e = Ω(g) [1].

Figure 3(right) shows known lower bounds that hold for speciﬁc string families.
 Apart from the lower bounds mentioned in Sect. 2, there are text families 
for which e = Ω(max(r, z) · n) [2] and thus e = Ω(g · n/ log n) since
g = O(z log n); and r = Ω(g log n/ log log n) (since on a de Bruijn sequence
of order k on a binary alphabet we have r = Θ(n) [2], z = O(n/ log n), and thus
g = O(z log(n/z)) = O(n log log n/ log n)). From the upper bounds that hold for
every string family, we can also deduce that, for example, there are string families 
where r = Ω(z log n) and thus r = Ω(b log n) (since r = Ω(zno log n));
{g, grl, zno} = Ω(r log n) (since z = Ω(r log n)) and z = Ω(b log n) (since
r = Ω(b), Theorem 6). We nevertheless included explicitly the most important
of these in the ﬁgure.

There are various interesting avenues of future work. For example, it is
unknown if r can be more than O(log n) times larger than z or g. It might also
be that our Theorem 1 can be proved without using run-length rules, yielding
g = O(b log(n/b)). These are questions of theoretical and also practical relevance,
since for example there exist compressed indexes for highly repetitive collections
that obtain diﬀerent search performance depending on which compressibility
measure their space is bounded by [27, Sect. 13.2].

Another relevant research avenue is to look for alternatives to Lempel-Ziv
compression with a better approximation ratio. For example, a recent bidirectional 
scheme, lcpcomp, seems to always perform better than Lempel-Ziv in practice 
[9]. It would be interesting to research its approximation ratio with respect
to the optimal bidirectional parsing.

Acknowledgements. We thank the reviewers for their insightful comments, which
helped us improve the presentation signiﬁcantly.

References

1. Belazzougui, D., Cunial, F.: Representing the suﬃx tree with the CDAWG. In: Proceedings 
of 28th Annual Symposium on Combinatorial Pattern Matching (CPM).
LIPIcs, vol. 78, pp. 7:1–7:13 (2017)

2. Belazzougui, D., Cunial, F., Gagie, T., Prezza, N., Raﬃnot, M.: Composite
repetition-aware data structures. In: Cicalese, F., Porat, E., Vaccaro, U. (eds.)
CPM 2015. LNCS, vol. 9133, pp. 26–39. Springer, Cham (2015). https://doi.org/
10.1007/978-3-319-19929-0 3

502

T. Gagie et al.

3. Bille, P., Gagie, T., Li Gørtz, I., Prezza, N.: A separation between run-length SLPs

and LZ77. CoRR, abs/1711.07270 (2017)

4. Blumer, A., Blumer, J., Haussler, D., McConnell, R.M., Ehrenfeucht, A.: Complete
inverted ﬁles for eﬃcient text retrieval and analysis. J. ACM 34(3), 578–595 (1987)
5. Burrows, M., Wheeler, D.: A block sorting lossless data compression algorithm.

Technical report 124, Digital Equipment Corporation (1994)

6. Charikar, M., Lehman, E., Liu, D., Panigrahy, R., Prabhakaran, M., Sahai, A.,
Shelat, A.: The smallest grammar problem. IEEE Trans. Inf. Theory 51(7), 2554–
2576 (2005)

7. Cover, T., Thomas, J.: Elements of Information Theory, 2nd edn. Wiley, Hoboken

(2006)

8. Crochemore, M., Iliopoulos, C.S., Kubica, M., Rytter, W., Wale´n, T.: Eﬃcient
algorithms for three variants of the LPF table. J. Discrete Algorithms 11, 51–61
(2012)

9. Dinklage, P., Fischer, J., K¨oppl, D., L¨obel, M., Sadakane, K.: Compression with

the tudocomp framework. CoRR, abs/1702.07577 (2017)

10. Fici, G.: Factorizations of the Fibonacci inﬁnite word. J. Integer Sequences, 18(9),

Article 3 (2015)

11. Fritz, M.H.-Y., Leinonen, R., Cochrane, G., Birney, E.: Eﬃcient storage of high
throughput DNA sequencing data using reference-based compression. Genome Res.
21, 734–740 (2011)

12. Gagie, T.: Large alphabets and incompressibility. Inf. Process. Lett. 99(6), 246–251

(2006)

13. Gallant, J.K.: String Compression Algorithms. Ph.D thesis. Princeton University

(1982)

14. Gawrychowski, P.: Pattern matching in Lempel-Ziv compressed strings: fast, simple,
 and deterministic. CoRR, abs/1104.4203 (2011)

15. Hucke, D., Lohrey, M., Reh, C.P.: The smallest grammar problem revisited. In:
Inenaga, S., Sadakane, K., Sakai, T. (eds.) SPIRE 2016. LNCS, vol. 9954, pp.
35–49. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-46049-9 4

16. I, T.: Longest common extensions with recompression. In: Proceedings of 28th
Annual Symposium on Combinatorial Pattern Matching (CPM). LIPIcs, vol. 78,
pp. 18:1–18:15 (2017)

17. Jez, A.: Approximation of grammar-based compression via recompression. Theor.

Comput. Sci. 592, 115–134 (2015)

18. Jez, A.: A really simple approximation of smallest grammar. Theor. Comput. Sci.

616, 141–150 (2016)

19. K¨arkk¨ainen, J., Sanders, P., Burkhardt, S.: Linear work suﬃx array construction.

J. ACM 53(6), 918–936 (2006)

20. Kieﬀer, J.C., Yang, E.-H.: Grammar-based codes: a new class of universal lossless

source codes. IEEE Trans. Inf. Theory 46(3), 737–754 (2000)

21. Kolmogorov, A.N.: Three approaches to the quantitative deﬁnition of information.

Prob. Inf. Transm. 1(1), 1–7 (1965)

22. Kreft, S., Navarro, G.: On compressing and indexing repetitive sequences. Theor.

Comput. Sci. 483, 115–133 (2013)

23. Lempel, A., Ziv, J.: On the complexity of ﬁnite sequences. IEEE Trans. Inf. Theory

22(1), 75–81 (1976)

24. M¨akinen, V., Navarro, G.: Succinct suﬃx arrays based on run-length encoding.

Nord. J. Comput. 12(1), 40–66 (2005)

25. Manber, U., Myers, G.: Suﬃx arrays: a new method for on-line string searches.

SIAM J. Comput. 22(5), 935–948 (1993)

On the Approximation Ratio of Lempel-Ziv Parsing

503

26. Mantaci, S., Restivo, A., Sciortino, M.: Burrows-Wheeler transform and Sturmian

words. Inf. Process. Lett. 86(5), 241–246 (2003)

27. Navarro, G.: Compact Data Structures - A Practical Approach. Cambridge University 
Press, Cambridge (2016)

28. Nishimoto, T., I, T., Inenaga, S., Bannai, H., Takeda, M.: Fully dynamic data
structure for LCE queries in compressed space. In: Proceedings of 41st International 
Symposium on Mathematical Foundations of Computer Science (MFCS),
pp. 72:1–72:15 (2016)

29. Prezza, N.: Compressed Computation for Text Indexing. Ph.D thesis. University

of Udine (2016)

30. Rodeh, M., Pratt, V.R., Even, S.: Linear algorithm for data compression via string

matching. J. ACM 28(1), 16–24 (1981)

31. Rytter, W.: Application of Lempel-Ziv factorization to the approximation of

grammar-based compression. Theor. Comput. Sci. 302(1–3), 211–222 (2003)

32. Sakamoto, H.: A fully linear-time approximation algorithm for grammar-based

compression. J. Discrete Algorithms 3(24), 416–430 (2005)

33. Shannon, C.E.: A mathematical theory of communication. Bell Syst. Tech. J. 27,

398–403 (1948)

34. Sthephens, Z.D., Lee, S.Y., Faghri, F., Campbell, R.H., Chenxiang, Z., Efron,
M.J., Iyer, R., Sinha, S., Robinson, G.E.: Big data: astronomical or genomical?
PLoS Biol. 17(7), e1002195 (2015)

35. Storer, J.A., Szymanski, T.G.: Data compression via textual substitution. J. ACM

29(4), 928–951 (1982)

