Algorithmica (2017) 78:379–393
DOI 10.1007/s00453-016-0167-2

Top-k Term-Proximity in Succinct Space
J. Ian Munro1 · Gonzalo Navarro2 ·
Jesper Sindahl Nielsen3 · Rahul Shah4 ·
Sharma V. Thankachan5

Received: 14 January 2015 / Accepted: 14 May 2016 / Published online: 27 May 2016
© Springer Science+Business Media New York 2016

Abstract Let D = {T1, T2, . . . , TD} be a collection of D string documents of n
characters in total, that are drawn from an alphabet set Σ = [σ]. The top-k document 
retrieval problem is to preprocess D into a data structure that, given a query
(P[1 . . . p], k), can return the k documents of D most relevant to the pattern P. The
relevance is captured using a predeﬁned ranking function, which depends on the set of
occurrences of P in Td. For example, it can be the term frequency (i.e., the number of

Funded in part by NSERC of Canada and the Canada Research Chairs program, Fondecyt Grant
1-140796, Chile, and NSF Grants CCF–1017623, CCF–1218904
MADALGO, Center for Massive Data Algorithmics, a Center of the Danish National Research
Foundation, grant DNRF84. An early partial version of this paper appeared in Proc. ISAAC 2014 [18].

B Jesper Sindahl Nielsen

jasn@cs.au.dk

J. Ian Munro
imunro@uwaterloo.ca

Gonzalo Navarro
gnavarro@dcc.uchile.cl

Rahul Shah
rahul@csc.lsu.edu

Sharma V. Thankachan
sharma.thankachan@gatech.edu

1

Cheriton School of CS, University of Waterloo, Waterloo, Canada

2 Department of CS, University of Chile, Santiago, Chile
3 MADALGO, Aarhus University, Aarhus, Denmark
4

School of EECS, Louisiana State University, Baton Rouge, LA, USA

5

School of CSE, Georgia Institute of Technology, Atlanta, GA, USA

123

380

Algorithmica (2017) 78:379–393

occurrences of P in Td), or it can be the term proximity (i.e., the distance between the
closest pair of occurrences of P in Td), or a pattern-independent importance score of
Td such as PageRank. Linear space and optimal query time solutions already exist for
the general top-k document retrieval problem. Compressed and compact space solutions 
are also known, but only for a few ranking functions such as term frequency and
importance. However, space efﬁcient data structures for term proximity based retrieval
have been evasive. In this paper we present the ﬁrst sub-linear space data structure
for this relevance function, which uses only o(n) bits on top of any compressed sufﬁx 
array of D and solves queries in O(( p + k) polylog n) time. We also show that
scores that consist of a weighted combination of term proximity, term frequency, and
document importance, can be handled using twice the space required to represent the
text collection.
Keywords Document indexing · Top-k document retrieval · Ranked document
retrieval · Succinct data structures · Compressed data structures · Compact data
structures · Proximity search

1 Introduction

Ranked document retrieval, that is, returning the documents that are most relevant to
a query, is the fundamental task in Information Retrieval (IR) [1,6]. Muthukrishnan
[19] initiated the study of this family of problems in the general scenario where both
the documents and the queries are general strings over arbitrary alphabets, which
has applications in several areas [20]. In this scenario, we have a collection D =
{T1, T2, . . . , TD} of D string documents of total length n, drawn from an alphabet Σ =
[σ], and the query is a pattern P[1 . . . p] over Σ. Muthukrishnan considered a family
of problems called thresholded document listing: given an additional parameter K , list
only the documents where some function score(P, d) of the occurrences of P in Td
exceeded K . For example, the document mining problem aims to return the documents
where P appears at least K times, whereas the repeats problem aims to return the
documents where two occurrences of P appear at distance at most K . While document
mining has obvious connections with typical term-frequency measures of relevance
[1,6], the repeats problem is more connected to various problems in bioinformatics [4,
11]. Also notice that the repeats problem is closely related to the term proximity based
document retrieval in the Information Retrieval ﬁeld [5,30,33–35]. Muthukrishnan
achieved optimal time for both problems, with O(n) space (in words) if K is speciﬁed
at indexing time and O(n log n) if speciﬁed at query time.

A more natural version of the thresholded problems, as used in IR, is top-k retrieval:
Given P and k, return k documents with the best score(P, d) values. Hon et al. [14,15]
gave a general framework to solve top-k problems for a wide variety of score(P, d)
functions, which takes O(n) space, allows k to be speciﬁed at query time, and solves
queries in O( p+k log k) time. Navarro and Nekrich [22] reduced the time to O( p+k),
and ﬁnally Shah et al. [31] achieved time O(k) given the locus of P in the generalized
sufﬁx tree of D.

123

Algorithmica (2017) 78:379–393

381

The problem is far from closed, however. Even the O(n) space (i.e., O(n log n)
bits) is excessive compared to the size of the text collection itself (n log σ bits), and
in data-intensive scenarios it often renders all these solutions impractical by a wide
margin. Hon et al. [15] also introduced a general framework for succinct indexes,
which use o(n) bits 1 on top of a compressed sufﬁx array (CSA) [21], which represents 
D in a way that also provides pattern-matching functionalities on it, all within
space (|CSA| bits) close to that of the compressed collection. A CSA ﬁnds the
sufﬁx array interval of P[1 . . . p] in time ts( p) and retrieves any cell of the sufﬁx
array or its inverse in time tSA. Hon et al. achieved O(ts( p) + k tSA log3+ε n) query
time, using O(n/ logε n) bits. Subsequent work (see [20]) improved the initial result
up to O(ts( p) + k tSA log2 k logε n) [24,26], and also considered compact indexes,
which may use up to o(n log n) bits on top of the CSA. For example, these achieve
O(ts( p) + k tSA log k logε n) query time using n log σ + o(n) further bits [13], or
O(ts( p) + k log
However, all these succinct and compact indexes work exclusively for the term
frequency (or closely related, e.g., tf-idf) measure of relevance. For the simpler case
where documents have a ﬁxed relevance independent of P, succinct indexes achieve
O(ts( p) + k tSA log k logε n) query time [3], and compact indexes using n log D +
o(n log D) bits achieve O(ts( p)+k log(D/k)) time [10]. On the other hand, there have
been no succinct nor compact indexes for the term proximity measure of relevance,
tp(P, d)=min{{|i− j| > 0, Td[i . . . i + p − 1]=Td[ j . . . j + p − 1] = P} ∪ {∞}}.

k) query time using n log D + o(n log n) further bits [25,26].

∗

In this paper we introduce the ﬁrst such results. Theorem 1 gives a succinct structure
that is competitive with the original succinct term-frequency results [15]. For example,
on a recent CSA [2], this time is O( p+(k log k+log n) log3+ε n), whereas the original
succinct term-frequency solution [15] would require O( p + k log4+ε n) time.
Theorem 1 Using a CSA plus o(n) bits data structure, one can answer top-k term
proximity queries in O(ts( p)+ (log2 n + k(tSA + log k log n)) log2+ε n) time, for any
constant ε > 0.
At the end, we show how to extend our results to a scenario where score(·,·) is a
weighted sum of document ranking, term-frequency, and term-proximity with predeﬁned 
non-negative weights [34].

2 Basic Concepts
Let T[1 . . . n] = T1◦T2◦. . . TD be the text (from an alphabet Σ = [σ]∪{$}) obtained
by concatenating all the documents in D. Each document is terminated with a special
symbol $, which does not appear anywhere else. A sufﬁx T[i . . . n] of T belongs to Td
iff i is in the region corresponding to Td in T. Thus, it holds d = 1 + rankB (i − 1),
where B[1 . . . n] is a bitmap deﬁned as B[ j] = 1 iff T[ j] = $ and rankB (i − 1)
1 If D = o(n), which we assume for simplicity in this paper. Otherwise it is D log(n/D)+ O(D)+ o(n) =
O(n) bits.

123

382

Algorithmica (2017) 78:379–393

is the number of 1s in B[1 . . . i − 1]. This operation is computed in O(1) time on a
representation of B that uses D log(n/D) + O(D) + o(n) bits [29]. For simplicity,
we assume D = o(n), and thus B uses o(n) bits.

2.1 Sufﬁx Trees

The sufﬁx tree [32] of T is a compact trie containing all of its sufﬁxes, where the ith
leftmost leaf, (cid:5)i , represents the ith lexicographically smallest sufﬁx. It is also called
the generalized sufﬁx tree of D, GST. Each edge in GST is labeled by a string, and
path(x) is the concatenation of the edge labels along the path from the GST root to
node x. Then path((cid:5)i ) is the ith lexicographically smallest sufﬁx of T. The highest
node x with path(x) preﬁxed by P[1 . . . p] is the locus of P, and is found in time
O( p) from the GST root. The GST uses O(n) words of space.

2.2 Sufﬁx Arrays
The sufﬁx array [16] of T, SA[1 . . . n], is deﬁned as SA[i] = n + 1 − | path((cid:5)i )|,
the starting position in T of the ith lexicographically smallest sufﬁx of T. The sufﬁx 
range of P is the range SA[sp, ep] pointing to the sufﬁxes that start with P,
T[SA[i] . . . SA[i] + p − 1] = P for all i ∈ [sp, ep]. Also, (cid:5)sp (resp., (cid:5)ep) are the
leftmost (resp., rightmost) leaf in the subtree of the locus of P.

2.3 Compressed Sufﬁx Arrays

The compressed sufﬁx array [21] of T , CSA, is a compressed representation of SA,
and usually also of T. Its size in bits, |CSA|, is O(n log σ ) and usually much less.
The CSA ﬁnds the interval [sp, ep] of P in time ts( p). It can output any value SA[i],
−1[i], in time tSA. For example, a CSA using
and even of its inverse permutation, SA
n Hh (T) + o(n log σ ) bits [2] gives ts( p) = O( p) and tSA = O(log1+ n) for any
constant  > 0, where Hh is the hth order empirical entropy [17].

2.4 Compressed Sufﬁx Trees

The compressed sufﬁx tree of T, CST, is a compressed representation of GST, where
node identiﬁers are their corresponding sufﬁx array ranges. The CST can be implemented 
using o(n) bits on top of a CSA [23] and compute (among others) the lowest
common ancestor (LCA) of two leaves (cid:5)i and (cid:5) j , in time O(tSA log n), and the Weiner
link Wlink(a, v), which leads to the node with path label a◦ path(v), in time O(tSA).2

2.5 Orthogonal Range Successors/Predecessors
Given n points in [n] × [n], an O(n log n)-bit data structure can retrieve the point in
a given rectangle with lowest y-coordinate value, in time O(log n) for any constant

2 Using O(n/ log n) bits and no special implementation for operations SA

−1[SA[i] ± 1].

123

Algorithmica (2017) 78:379–393

383

 > 0 [27]. Combined with standard range tree partitioning [7], the following result
easily follows.

(cid:7)
log2 n) bits can
Lemma 1 Given n
support the following query in O(log1+ n) time, for any constant  > 0: ﬁnd the
point in a region [x, x

points in [n] × [n] × [n], a structure using O(n
(cid:7)] × [y, y

(cid:7)

(cid:7)] × [z, z

(cid:7)] with the lowest/highest x-coordinate.

3 An Overview of Our Data Structure

The top-k term proximity is related to a problem called range restricted searching,
where one must report all the occurrences of P that are within a text range T[i . . . j]. It
is known that succinct data structures for that problem are unlikely to exist in general,
whereas indexes of size |CSA| + O(n/ log n) bits do exist for patterns longer than
Δ = log2+ n [12]. Therefore, our basic strategy will be to have a separate data
structure to solve queries of length p = π, for each π ∈ {1, . . . , Δ}. Patterns with
length p > Δ can be handled with a single succinct data structure. More precisely,
we design two different data structures that operate on top of a CSA:

– An O(n log log n/(π logγ n))-bits structure for handling queries of ﬁxed length
p = π, in time O(ts( p) + k(tSA + log log n + log k) π logγ n). This is described
in Sect. 4 and the result is summarized in Lemma 3.
– An O(n/ log n+ (n/Δ) log2 n)-bits structure for handling queries with p > Δ in
time O(ts( p)+ Δ(Δ+tSA)+k log k log2 n(tSA+ Δ log1+ n)). This is described
in Sect. 5 and the result is summarized in Lemma 5.
By building the ﬁrst structure for every π ∈ {1, . . . , Δ}, any query can be handled
using the appropriate structure. The Δ structures for ﬁxed pattern length add up to
O(n(log log n)2/ logγ n) = o(n/ logγ /2 n) bits, whereas that for long patterns uses
O(n/ log n) bits. By choosing ε = 4 = 2γ , the space is O(n/ logε/4 n) bits. As for
the time, the structures for ﬁxed p = π are most costly for π = Δ, where the time is

O(ts( p) + k(tSA + log log n + log k) Δ logγ n).

Adding up the time of the second structure, we get

O(ts( p) + Δ(Δ + k(tSA + log k log1+ n) log2 n)),

which is upper bounded by

O(ts( p) + (log2 n + k(tSA + log k log n)) log2+ε n).

This yields Theorem 1.

Now we introduce some formalization to convey the key intuition. The term proximity 
tp(P, d) can be determined by just two occurrences of P in Td, which are
the closest up to ties. We call them critical occurrences, and a pair of two closest
occurrences is a critical pair. Note that one document can have multiple critical pairs.

123

384

Algorithmica (2017) 78:379–393

Deﬁnition 1 An integer i ∈ [1, n] is an occurrence of P in Td if the sufﬁx T[i . . . n]
belongs to Td and T[i . . . i + p − 1] = P[1 . . . p]. The set of all occurrences of P in
T is denoted Occ(P).

Deﬁnition 2 An occurrence id of P in Td is a critical occurrence if there exists another
) is called a
occurrence i
critical pair of Td with respect to P.

d of P in Td such that |id − i
(cid:7)

| = tp(P, d). The pair (id , i

(cid:7)
d

(cid:7)
d

A key concept in our solution is that of candidate sets of occurrences, which contain
sufﬁcient information to solve the top-k query (note that, due to ties, a top-k query
may have multiple valid answers).
Deﬁnition 3 Let Topk(P, k) be a valid answer for the top-k query (P, k). A set
Cand(P, k) ⊆ Occ(P) is a candidate set of Topk(P, k) if, for each document identiﬁer 
d ∈ Topk(P, k), there exists a critical pair (id , i
) of Td with respect to P such
that id , i
Lemma 2 Given a CSA on D, a valid answer to query (P, k) can be computed from
Cand(P, k) in O(z log z) time, where z = |Cand(P, k)|.
Proof Sort the set Cand(P, k) and traverse it sequentially. From the occurrences
within each document Td, retain the closest consecutive pair (id , i
), and ﬁnally report
k documents with minimum values |id − i

|. This takes O(z log z) time.

∈ Cand(P, k).

(cid:7)
d

(cid:7)
d

(cid:7)
d

(cid:7)
d

We show that this returns a valid answer set. Since Cand(P, k) is a candidate set, it
) for each d ∈ Topk(P, k), so this critical pair (or another
contains a critical pair (id , i
with the same |id − i
| value) is chosen for each d ∈ Topk(P, k). If the algorithm
returns an answer other than Topk(P, k), it is because some document d ∈ Topk(P, k)
d(cid:7)| =
(cid:7) /∈ Topk(P, k) with the same score tp(P, d
(cid:7)
is replaced by another d
|id − i
(cid:9)(cid:10)

(cid:7)) = |id(cid:7) − i

| = tp(d).

(cid:7)
d

(cid:7)
d

(cid:7)
d

Our data structures aim to return a small candidate set (as close to size k as possible),

from which a valid answer is efﬁciently computed using Lemma 2.

4 Data Structure for Queries with Fixed p = π ≤ Δ
We build an o(n/π )-bits structure for handling queries with pattern length p = π.
Lemma 3 For any 1 ≤ π ≤ Δ = O(polylog n) and any constant γ > 0, there is
an O(n log log n/(π logγ n))-bits data structure solving queries (P[1 . . . p], k) with
p = π in O(ts( p) + k(tSA + log log n + log k)π logγ n) time.
The idea is to build an array F[1, n] such that a candidate set of size O(k), for any
query (P, k) with p = π, is given by {SA[i], i ∈ [sp, ep]∧ F[i] ≤ k}, [sp, ep] being
the sufﬁx range of P. The key property to achieve this is that the ranges [sp, ep] are
disjoint for all the patterns of a ﬁxed length π. We build F as follows.
1. Initialize F[1 . . . n] = n + 1.
2. For each pattern Q of length π,

123

Algorithmica (2017) 78:379–393

385

(a) Find the sufﬁx range [α, β] of Q.
(b) Find the list Tr1

, Tr2

, Tr3

values (ties broken arbitrarily).

, . . . of documents in the ascending order of tp(Q,·)

(c) For each document Trκ containing Q at least twice, choose a unique critical
(cid:7) ∈ [α, β], such that
(cid:7)]) is a critical pair of Trκ with respect to Q. Then

pair with respect to Q, that is, choose two elements j, j
(irκ , i
assign F[ j] = F[ j

) = (SA[ j], SA[ j
(cid:7)] = κ.

(cid:7)
rκ

The following observation is immediate.
Lemma 4 For a query (P[1 . . . p], k) with p = π and sufﬁx array range [sp, ep] for
P, the set {SA[ j], j ∈ [sp, ep] ∧ F[ j] ≤ k} is a candidate set of size at most 2k.
Proof A valid answer for (P, k) are the document identiﬁers r1, . . . , rk considered
at construction time for Q = P. For each such document Trκ , 1 ≤ κ ≤ k, we
(cid:7) ∈ [sp, ep], and set
have found a critical pair (irκ , i
(cid:7)] = κ ≤ k. All the other values of F[sp, ep] are larger than k. The size of
F[ j] = F[ j
the candidate set is thus 2k (or less, if there are less than k documents where P occurs
(cid:9)(cid:10)
twice).

) = (SA[ j], SA[ j

(cid:7)]), for j, j

(cid:7)
rκ

However, we cannot afford to maintain F explicitly within the desired space bounds.
Therefore, we replace F by a sampled array F(cid:7)
. The sampled array is built by cutting
F into blocks of size π(cid:7) = π logγ n and storing the logarithm of the minimum value
for each block. This will increase the size of the candidate sets by a factor of O(π(cid:7)).
More precisely, F(cid:7)[1, n/π(cid:7)] is deﬁned as

(cid:7)[ j] = (cid:2)

log min F[( j − 1)π(cid:7) + 1 . . . j π(cid:7)](cid:3)

F

.

Since F(cid:7)[ j] ∈ [0 . . . log n], the array can be represented in n log log n/(π logγ n)
bits. We represent F(cid:7)
with a multiary wavelet tree [9], which maintains the space in
O(n log log n/(π logγ n)) bits and, since the alphabet size is logarithmic, supports
in constant time operations rank and select on F(cid:7)
. Operation rank( j, κ) counts the
number of occurrences of κ in F(cid:7)[1 . . . j], whereas select ( j, κ) gives the position of
the jth occurrence of κ in F(cid:7)

.

4.1 Query Algorithm
To answer a query (P[1 . . . p], k) with p = π using a CSA and F(cid:7)
, we compute the
sufﬁx range [sp, ep] of P in time ts( p), and then do as follows.
1. Among all the blocks of F overlapping the range[sp, ep], identify those containing

an element ≤ 2

(cid:13)log k(cid:14)

, that is, compute the set

Sblocks = { j,(cid:13)sp/π(cid:7)(cid:14) ≤ j ≤ (cid:13)ep/π(cid:7)(cid:14) ∧ F

(cid:7)[ j] ≤ (cid:13)log k(cid:14)}.

2. Generate Cand(P, k) = {SA[ j
(cid:7) ∈ [( j − 1)π(cid:7) + 1, j π(cid:7)]}.
3. Find the query output from the candidate set Cand(P, k), using Lemma 2.

(cid:7)], j ∈ Sblocks ∧ j

123

386

Algorithmica (2017) 78:379–393

For step 1, the wavelet tree representation of F(cid:7)

generates Sblocks in time O(1 +
|Sblocks|): All the 2t positions3 j ∈ [sp, ep] with F(cid:7)[ j] = t are j = select (rank(sp−
1, t ) + i, t ) for i ∈ [1, 2t]. We notice if there are no sufﬁcient documents if we obtain
a j > ep, in which case we stop.
The set Cand(P, k) is a candidate set of (P, k), since any j ∈ [sp, ep] with
F[ j] ≤ k belongs to some block of Sblocks. Also the number of j ∈ [sp, ep] with
F[ j] ≤ 2
Now, Cand(P, k) is of size |Sblocks|π(cid:7) = O(kπ(cid:7)), and it is generated in step 2
in time O(k tSA π(cid:7)). Finally, the time for generating the ﬁnal output using Lemma 2
log(kπ(cid:7)))) = O(kπ logγ n(log k + log log n + log π )). By considering that
is O(kπ(cid:7)
π ≤ Δ = O(polylog n), we obtain Lemma 3.

(cid:13)log k(cid:14)

is at most 2 · 2

(cid:13)log k(cid:14) ≤ 4k, therefore |Sblocks| ≤ 4k.

5 Data Structure for Queries with p > Δ

We prove the following result in this section.
Lemma 5 For any Δ = O(polylog n) and any constant  > 0, there is an
O(n/ log n + (n/Δ) log2 n)-bits structure solving queries (P[1 . . . p], k), with p >
Δ, in O(ts( p) + Δ(Δ + tSA) + k log k log2 n(tSA + Δ log1+ n)) time.

We start with a concept similar to that of a candidate set, but weaker in the sense

that it is required to contain only one element of each critical pair.

Deﬁnition 4 Let Topk(P, k) be a valid answer for the top-k query (P, k). A set
Semi(P, k) ⊆ [n] is a semi-candidate set of Topk(P, k) if it contains at least one
critical occurrence id of P in Td for each document identiﬁer d ∈ Topk(P, k).

Our structure in this section generates a semi-candidate set Semi(P, k). Then, a
candidate set Cand(P, k) is generated as the union of Semi(P, k) and the set of
occurrences of P that are immediately before and immediately after every position
i ∈ Semi(P, k). This is obviously a valid candidate set. Finally, we apply Lemma 2
on Cand(P, k) to compute the ﬁnal output.

5.1 Generating a Semi-candidate Set

This section proves the following result.

Lemma 6 For any constant δ > 0, a structure of O(n(log log n)2/ logδ n) bits
plus a CSA can generate a semi-candidate set of size O(k log k logδ n) in time
O(tSA k log k logδ n).

Let node x be an ancestor of node y in GST. Let Leaf(x) (resp., Leaf(y)) be the set
of leaves in the subtree of node x (resp., y), and let Leaf(x\y) = Leaf(x) \ Leaf(y).
Then the following lemma holds.

3 Except for t = 0, which has 2 positions.

123

Algorithmica (2017) 78:379–393

387

Lemma 7 The set Semi(path(y), k)∪{SA[ j], (cid:5) j ∈ L(x\y)} is a semi-candidate set
of Topk(path(x), k).
Proof Let d ∈ Topk(path(x), k), then our semi-candidate set should contain id or
(cid:7)
(cid:7)
d are
d for some critical pair (id , i
i
occurrences of path(x) but not of path(y), then (cid:5) j or (cid:5) j(cid:7) are in L(x\y), for SA[ j] = id
and SA[ j
(cid:7)
d, and thus our set contains it. If, on the other hand, both id and
), then tp(path(y), d) =
(cid:7)
d are occurrences of path(y) for all critical pairs (id , i
i
tp(path(x), d), and the critical pairs of path(x) are the critical pairs of path(y). Thus
(cid:9)(cid:10)
Semi(y, k) contains id or i

). If there is some such critical pair where id or i

(cid:7)
d for some such critical pair.

(cid:7)] = i

(cid:7)
d

(cid:7)
d

Our approach is to precompute and store Semi(path(y), k) for carefully selected
nodes y ∈ GST and k values, so that any arbitrary Semi(path(x), k) set can be
computed efﬁciently. The succinct framework of Hon et al. [15] is adequate for this.

5.1.1 Node Marking Scheme

The idea [15] is to mark a set Markg of nodes in GST based on a grouping factor g:
Every gth leaf is marked, and the LCA of any two consecutive marked leaves is also
marked. Then the following properties hold.
1. |Markg| ≤ 2n/g.
2. If there exists no marked node in the subtree of x, then |Leaf(x)| < 2g.
3. If it exists, then the highest marked descendant node y of any unmarked node x is

unique, and |Leaf(x\y)| < 2g.
We use this idea, and a later reﬁnement [13]. Let us ﬁrst consider a variant of
Lemma 6 where k = κ is ﬁxed at construction time. We use a CSA and an o(n)-bit
CST on it, see Sect. 2. We choose g = κ log κ log1+δ n and, for each node y ∈ Markg,
we explicitly store a candidate set Semi(path(y), κ) of size κ. The space required is
O(|Markg|κ log n) = O(n/(log κ logδ n)) bits.
To solve a query (P, κ), we ﬁnd the sufﬁx range [sp, ep], then the locus node of
P is x = LCA((cid:5)sp, (cid:5)ep) (but we do not need to compute x). The node we compute
is y = LCA((cid:5)g(cid:13)sp/g(cid:14), (cid:5)g(cid:15)ep/g(cid:16)), the highest marked node in the subtree of x, as it has
associated the set Semi(path(y), κ). This takes time O(tSA log n) for any constant
 > 0 (see Sect. 2). Then, by the given properties of the marking scheme, combined
with Lemma 7, a semi-candidate set of size O(g + κ) = O(κ log κ log1+δ n) can be
generated in O(tSAκ log κ log1+δ n) time.
set Markg(cid:7) of nodes, for g
these prime nodes, not marked nodes. For each prime node y
compute a candidate set Semi(path(y
marked node in the subtree of y
of the κ nodes stored in Semi(path(y), κ) also belong to Semi(path(y
the same proof of Lemma 7, elements in Semi(path(y
must have a critical occurrence in Leaf(y
the critical positions id ∈ Semi(path(y

To reduce this time, we employ dual marking scheme [13]. We identify a larger
= κ log κ logδ n. To avoid confusion, we call
(cid:7) ∈ Markg(cid:7), we pre-
(cid:7)), κ) of size κ. Let y be the (unique) highest
(cid:7)
to indicate which
(cid:7)), κ). By
(cid:7)), κ) \ Semi(path(y), κ)
(cid:7)\y). Then, instead of explicitly storing
(cid:7)), κ) \ Semi(path(y), κ), we store their

. Then we store κ bits in y

(cid:7) = g

log n

(cid:7)

123

388

Algorithmica (2017) 78:379–393

Semi(path(y,κ))
7 9
1 2

· · ·

2
κ

(leaf ids)

root

P

Space: O(κ log n) bits

x

Leaf(x \ y )
Leaf(y \ y)

sp

y

y

Diﬀerence from Semi(path(y),κ)

0 1
1 2

1 0
3 4

· · ·

0
κ

(bit array)

And

≤ κ integers in interval of size O(g)

Space: O(κ + log g

κ ) = O(κ + κ log g

κ) bits

ep

≤ g

O(g)

O(g)

≤ g

Fig. 1 Scheme using marked and prime nodes. Set Semi(path(x), κ) is built from Semi(path(y
Leaf(x\y
of Leaf(y

(cid:7)), κ) and
(cid:7)), κ) is built from selected entries of Semi(path(y), κ) and selected elements

(cid:7)). Set Semi(path(y
(cid:7)\y)

Now we can compute CST prime node y

(cid:7)\y). Storing κ such positions in leaf order requires
left-to-right position in Leaf(y
O(κ log(g/κ)) = O(κ log log n) bits, using for example gamma codes. The total
space is O(|Markg(cid:7)|κ log log n) = O(n log log n/(log κ logδ n)) bits.
(cid:7) = LCA((cid:5)g(cid:7)(cid:13)sp/g(cid:7)(cid:14), (cid:5)g(cid:7)(cid:15)ep/g(cid:7)(cid:16)) and
(cid:7)), κ) with the help of the precomputed set
marked node y, compute Semi(path(y
Semi(path(y), κ) and the differential information stored at node y
, and apply the
same technique above to obtain a semi-candidate set from Markg(cid:7), yet of smaller size
(cid:7) + κ) = O(κ log κ logδ n), in O(tSAκ log κ logδ n) time. Figure 1 illustrates the
O(g
scheme.

(cid:7)

We are now ready to complete the proof of Lemma 6. We maintain structures as

described for all the values of κ that are powers of 2, in total

⎛
⎝(cid:6)

n log log n/ logδ n

O

(cid:7) · log D(cid:8)
i=1

⎞
⎠ = O(n(log log n)2/ logδ n)

1/i

bits of space. To solve a query (P, k), we compute κ = 2
semi-candidate set of (P, κ) using the corresponding structure.

(cid:13)log k(cid:14) < 2k and return the

5.2 Generating the Candidate Set

The problem of obtaining Cand(P, k) from Semi(P, k) boils down to the task of,
given P[1 . . . p] and an occurrence q, ﬁnding the occurrence of P closest to q. In other
words, ﬁnding the ﬁrst and the last occurrence of P in T[q + 1 . . . n] and T[1 . . . q +
p − 1], respectively. We employ sufﬁx sampling to obtain the desired space-efﬁcient
structure. The idea is to exploit the fact that, if p > Δ, then for every occurrence q

123

Algorithmica (2017) 78:379–393

389

of P there must be an integer j = Δ(cid:13)q/Δ(cid:14) (a multiple of Δ) and t ≤ Δ, such that
P[1 . . . t] is a sufﬁx of T[1 . . . j] and P[t + 1 . . . p] is a preﬁx of T[ j + 1 . . . n]. We
call q an offset-t occurrence of P. Then, Cand(P, k) can be computed as follows:

1. Find Semi(P, k) using Lemma 6.
2. For each q ∈ Semi(P, k) and t ∈ [1, Δ], ﬁnd the offset-t occurrences of P that

are immediately before and immediately after q.

3. The occurrences found in the previous step, along with the elements in Semi(P, k),

constitute Cand(P, k).

In order to perform step 2 efﬁciently, we maintain the following structures.
– Sparse Sufﬁx Tree (SST): A sufﬁx T[Δi+1 . . . n] is a sparse sufﬁx, and the trie of
all sparse sufﬁxes is a sparse sufﬁx tree. The sparse sufﬁx range of a pattern Q is the
range of the sparse sufﬁxes in SST that are preﬁxed by Q. Given the sufﬁx range
[sp, ep] of a pattern, its sparse sufﬁx range [ssp, sep] can be computed in constant
time by maintaining a bitmap B[1 . . . n], where B[ j] = 1 iff T[SA[ j] . . . n] is a
sparse sufﬁx. Then ssp = 1 + rankB (sp − 1) and sep = rankB (sp). Since B
has n/Δ 1s, it can be represented in O((n/Δ) log Δ) bits while supporting rankB
operation in constant time for any Δ = O(polylog n) [28].
– Sparse Preﬁx Tree (SPT): A preﬁx T[1 . . . Δi] is a sparse preﬁx, and the trie of
the reverses of all sparse preﬁxes is a sparse preﬁx tree. The sparse preﬁx range of
a pattern Q is the range of the sparse preﬁxes in SPT with Q as a sufﬁx. The SPT
can be represented as a blind trie [8] using O((n/Δ) log n) bits. Then the search
for the sparse preﬁx range of Q can be done in O(|Q|) time, by descending using
the reverse of Q4. Note that the blind trie may return a fake node when Q does
not exist in the SPT.
– Orthogonal Range Successor/Predecessor Search Structure over a set of(cid:13)n/Δ(cid:14)
points of the form (x, y, z), where the yth leaf in SST corresponds to T[x . . . n]
and the zth leaf in SPT corresponds to T[1 . . . (x − 1)]. The space needed is
O((n/Δ) log2 n) bits (recall Lemma 1).

(cid:7)
t

, sep

The total space of the structures is O((n/Δ) log2 n) bits. They allow computing
the ﬁrst offset-t occurrence of P in T[q + 1 . . . n] as follows: ﬁnd [sspt , sept] and
], the sparse sufﬁx range of P[t + 1 . . . p] and the sparse preﬁx range of
[ssp
(cid:7)
P[1 . . . t], respectively. Then, using an orthogonal range successor query, ﬁnd the point
t
(e,·,·) with the lowest x-coordinate value in[q+t+1, n]×[sspt , sept]×[ssp
].
(cid:7)
, sep
Then, e − t is the answer. Similarly, the last offset-t occurrence of P in T[1 . . . q − 1]
t
is f − t, where ( f,·,·) is the point in [1, q + t − 1]×[sspt , sept]×[ssp
] with
(cid:7)
t
the highest x-coordinate value.
First, we compute all the ranges [sspt , sept] using the SST. This requires knowing
the interval SA[spt , ept] of P[t+1 . . . p] for all 1 ≤ t ≤ Δ. We compute these by using
the CSA to search for P[Δ+ 1 . . . p] (in time at most ts( p)), which gives [spΔ, epΔ],
and then computing [spt−1, ept−1] = Wlink(P[t],[spt , ept]) for t = Δ − 1, . . . , 1.
Using an o(n)-bits CST (see Sect. 2), this takes O(Δ tSA) time. Then the SST ﬁnds

(cid:7)
t

(cid:7)
t

, sep

4 Using perfect hashing to move in constant time towards the children.

123

390

Algorithmica (2017) 78:379–393

(cid:7)
t

(cid:7)
t

, sep

all the [sspt , sept] values in time O(Δ). Thus the time spent on the SST searches is
O(ts( p) + Δ tSA).
Second, we search the SPT for reverse pattern preﬁxes of lengths 1 to Δ, and thus
they can all be searched for in time O(Δ2). Since the SPT is a blind trie, it might be
] it returns are the correct interval of P[1 . . . t], or
either that the intervals [ssp
that P[1 . . . t] does not terminate any sparse preﬁx. A simple way to determine which is
the case is to perform the orthogonal range search as explained, asking for the successor
e0 of position 1, and check whether the resulting position, e0 − t, is an occurrence of
−1[e0 − t] ∈ [sp, ep]. This takes O(tSA + log1+ n) time per
P, that is, whether SA
veriﬁcation. Considering the searches plus veriﬁcations, the time spent on the SPT
searches is O(Δ(Δ + tSA + log1+ n)).
], we
perform O(|Semi(P, k)|Δ) orthogonal range searches for positions q, in time
O(|Semi(P, k)|Δ log1+ n), and keep the closest one for each q.
Lemma 8 Given a semi-candidate set Semi(P, k), where p > Δ, a candidate set
Cand(P, k) of size O(|Semi(P, k)|) can be computed in time O(ts( p)+Δ(Δ+tSA+
|Semi(P, k)| log1+ n)) using a data structure of O((n/Δ) log2 n) bits.

the intervals [sspt , sept] and [ssp

Finally, after determining all

, sep

(cid:7)
t

(cid:7)
t

Thus, by combining Lemma 6 using δ = 2 (so its space is o(n/ log n) bits) and

Lemma 8, we obtain Lemma 5.

6 Extension

Up to now we have considered only term proximity. In a more general scenario one
would like to use a scoring function that is a linear combination of term proximity,
term frequency, and a document score like PageRank (document score counts for d
only if P appears in d at least once). In this section we provide the ﬁrst result on
supporting such a combined scoring function in compact space.
Theorem 2 Using a 2n log σ + o(n log σ ) bits data structure, one can answer topk 
document retrieval queries, where score(·,·) is a weighted sum of a document
score, term-frequency and term-proximity with predeﬁned non-negative weights, in
time O( p + k log k log4+ n)
Proof The theorem can be obtained by combing our previous results as follows:
1. Lemma 6 with δ > 0 gives the following: using a |CSA| + o(n) bits structure,
we can generate a semi-candidate set Semi(P, k) of size O(k log k logδ n) in time
O(tSAk log k logδ n). Although the ranking function assumed in Lemma 6 is termproximity,
 it is easy to see that the result holds true for our new ranking function
score(·,·) as well: we precompute Semi(path(y), κ) for score(·,·) rather than
for tp(·,·). Any document d that is not top-k on node y and does not appear further
in Leaf(x\y), cannot be top-k on node x, because its score cannot increase.
2. We wish to compute tf(P, d) = epd − spd + 1 for each entry of Td in Semi(P, k),
where [spd , epd] is the range in the sufﬁx array SAd of Td for the pattern P.
However, we do not wish to spend time ts( p), since that could potentially be

123

Algorithmica (2017) 78:379–393

391

expensive. Note we have already computed sp and ep. By using these and the
compressed sufﬁx array CSAd, which we will store for each document, we can
compute spd and epd more efﬁciently as follows. The position in T where Td
begins is selectB (d − 1), and |Td| = selectB (d) − selectB (d − 1). Note that we
are already given one position id in the region of Td in T by the entry in Semi(P, k),
= id − selectB (d − 1).
and we compute the corresponding entry solely in Td as i
]. We can
We compute the corresponding point to i
now deﬁne

d in [spd , epd] as q = SA
(cid:7)

−1
d

(cid:7)
d

[i

(cid:7)
d

epd = max{i | i ≥ q ∧ i ≤ |Td| ∧ SA

−1[selectB (d − 1) + SAd[i]] ≤ ep}

which is computed by an exponential search starting from q. A similar equation
holds for spd [15]. Computing q costs O(tSA) and the two exponential searches
require O(tSA log n) time each.
3. We need to be able to compute the term proximity distance for each Td in
Semi(P, k). This can be computed in time O((tSA + log2 n) log2+ε n) once we
know [spd , epd] of P in CSAd by applying Theorem 1: For each document Td
we store the o(|Td|) extra bits required by the theorem so we can answer queries
for k = 1. That query will then return the single tp value for the query pattern.

4. The document rank for each d is easily obtained, as it does not depend on P.

Finally the k documents with the highest score(P, d) are reported.

By maintaining structures of overall space |CSA| + (cid:11)
|CSAd| + o(n) bits, any
(P, k) query can be answered in O(ts( p) + k log k logδ n(tSA + log2 n) log2+ε n).
Using the version of the compressed sufﬁx array by Belazzougui and Navarro [2],
where |CSA| = n log σ + o(n log σ ), ts( p) = O( p) and tSA = O(log n log log n),
the space becomes 2n log σ + o(n log σ ) bits and the query time becomes O( p +
(cid:9)(cid:10)
k log k log4+ n). The proof is completed by choosing 0 < δ, ε < /2.

d

7 Conclusions

We have presented the ﬁrst compressed data structures for answering top-k termproximity 
queries, achieving the asymptotically optimal |CSA|+ o(n) bits, and query
times in O(( p + k) polylog n). This closes the gap that separated this relevance
model from term frequency and document ranking, for which optimal-space solutions
(as well as other intermediate-space tradeoffs) had existed for several years. The plausible 
hypothesis that term proximity was inherently harder than the other relevance
measures, due to its close relation with range restricted searching [12], has then been
settled on the negative.

For the case where the ranking function is a weighted average of document rank,
term-frequency, and term-proximity, we have introduced a compact-space solution
that requires twice the minimum space required to represent the text collection. An
interesting challenge is to ﬁnd an efﬁcient space-optimal data structure that solves this
more general problem.

123

392

References

Algorithmica (2017) 78:379–393

1. Baeza-Yates, R., Ribeiro-Neto, B.: Modern Information Retrieval, 2nd edn. Addison-Wesley, Reading

(2011)

2. Belazzougui, D., Navarro, G.: Alphabet-independent compressed text indexing. In: Proceedings of the

19th ESA, pp. 748–759 (2011)

3. Belazzougui, D., Navarro, G., Valenzuela, D.: Improved compressed indexes for full-text document

retrieval. J. Discrete Algorithms 18, 3–13 (2013)

4. Benson, G., Waterman, M.: A fast method for fast database search for all k-nucleotide repeats. Nucleic

Acids Res. 22(22), 4828–4836 (1994)

5. Broschart, A., Schenkel, R.: Index tuning for efﬁcient proximity-enhanced query processing. In: INEX,

pp. 213–217 (2009)

6. Büttcher, S., Clarke, C.L.A., Cormack, G.: Information Retrieval: Implementing and Evaluating Search

Engines. MIT Press, Cambridge (2010)

7. de Berg, M., van Kreveld, M., Overmars, M., Schwarzkopf, O.: Computational Geometry: Algorithms

and Applications, 3rd edn. Springer, Berlin (2008)

8. Ferragina, P., Grossi, R.: The string B-tree: a new data structure for string search in external memory

and its applications. J. ACM. 46(2), 236–280 (1999)

9. Ferragina, P., Manzini, G., Mäkinen, V., Navarro, G.: Compressed representations of sequences and

full-text indexes. ACM Trans. Algorithms 3(2), Art. No. 20 (2007)

10. Gagie, T., Navarro, G., Puglisi, S.J.: New algorithms on wavelet trees and applications to information

retrieval. Theor. Comput. Sci. 426–427, 25–41 (2012)

11. Gusﬁeld, D.: Algorithms on Strings, Trees and Sequences: Computer Science and Computational

Biology. Cambridge University Press, Cambridge (1997)

12. Hon, W.-K., Shah, R., Thankachan, S.V., Vitter, J.S.: On position restricted substring searching in

succinct space. J. Discrete Algorithms 17, 109–114 (2012)

13. Hon, W.-K., Shah, R., Thankachan, S.V., Vitter, J.S.: Faster compressed top-k document retrieval. In:

Proceedings of the 23rd DCC, pp. 341–350 (2013)

14. Hon, W.-K., Shah, R., Thankachan, S.V., Vitter, J.S.: Space-efﬁcient frameworks for top-k string

retrieval. J. ACM. 61(2), 9 (2014)

15. Hon, W.-K., Shah, R., Vitter, J.S.: Space-efﬁcient framework for top-k string retrieval problems. In:

Proceedings of the 50th FOCS, pp. 713–722 (2009)

16. Manber, U., Myers, G.: Sufﬁx arrays: a new method for on-line string searches. SIAM J. Comput.

22(5), 935–948 (1993)

17. Manzini, G.: An analysis of the Burrows–Wheeler transform. J. ACM. 48(3), 407–430 (2001)
18. Munro, J.I., Navarro, G., Nielsen, J.S., Shah, R., Thankachan, S.V.: Top-k term-proximity in succinct

space. In: Proceedings of the 25th ISAAC, pp. 169–180 (2014)

19. Muthukrishnan, S.: Efﬁcient algorithms for document retrieval problems. In: Proceedings of the 13th

SODA, pp. 657–666 (2002)

20. Navarro, G.: Spaces, trees and colors: the algorithmic landscape of document retrieval on sequences.

ACM Comput. Surv. 46(4), Art. No. 52 (2014)

21. Navarro, G., Mäkinen, V.: Compressed full-text indexes. ACM Comput. Surv. 39(1), Art. No. 2 (2007)
22. Navarro, G., Nekrich, Y.: Top-k document retrieval in optimal time and linear space. In: Proceedings

of the 23rd SODA, pp. 1066–1078 (2012)

23. Navarro, G., Russo, L.: Fast fully-compressed sufﬁx trees. In: Proceedings of the 24th DCC, pp.

283–291 (2014)

24. Navarro, G., Thankachan, S.V.: Faster top-k document retrieval in optimal space. In: Proceedings of

the 20th SPIRE, LNCS 8214, pp. 255–262 (2013)

25. Navarro, G., Thankachan, S.V.: Top-k document retrieval in compact space and near-optimal time. In:

Proceedings of the 24th ISAAC, LNCS 8283, pp. 394–404 (2013)

26. Navarro, G., Thankachan, S.V.: New space/time tradeoffs for top-k document retrieval on sequences.

Theor. Comput. Sci. 542, 83–97 (2014)

27. Nekrich, Y., Navarro, G.: Sorted range reporting. In: Proceedings of the 13th SWAT, LNCS 7357, pp.

271–282 (2012)

28. Pˇatra¸scu, M.: Succincter. In: Proceedings of the 49th FOCS, pp. 305–313 (2008)
29. Raman, R., Raman, V., Srinivasa, S.R.: Succinct indexable dictionaries with applications to encoding

k-ary trees, preﬁx sums and multisets. ACM Trans. Algorithms 3(4), Art. No. 43 (2007)

123

Algorithmica (2017) 78:379–393

393

30. Schenkel, R., Broschart, A., Hwang, S.-W., Theobald, M., Weikum, G.: Efﬁcient text proximity search.

In: SPIRE, pp. 287–299 (2007)

31. Shah, R., Sheng, C., Thankachan, S.V., Vitter, J.S.: Top-k document retrieval in external memory. In:

Proceedings of the 21st ESA, LNCS 8125, pp. 803–814 (2013)

32. Weiner, P.: Linear pattern matching algorithm. In: Proceedings of the 14th Annual IEEE Symposium

on Switching and Automata Theory, pp. 1–11 (1973)

33. Yan, H., Shi, S., Zhang, F., Suel, T., Wen, J.-R.: Efﬁcient term proximity search with term-pair indexes.

In: CIKM, pp. 1229–1238 (2010)

34. Zhu, M., Shi, S., Li, M., Wen, J.-R.: Effective top-k computation in retrieving structured documents

with term-proximity support. In: CIKM, pp. 771–780 (2007)

35. Zhu, M., Shi, S., Yu, N., Wen, J.-R.: Can phrase indexing help to process non-phrase queries? In:

CIKM, pp. 679–688 (2008)

123

