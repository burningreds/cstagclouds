Compressing Semistructured Text Databases(cid:1)

Joaqu´ın Adiego1, Gonzalo Navarro2, and Pablo de la Fuente1

1 Departamento de Inform´atica, Universidad de Valladolid

Valladolid, Espa˜na

{jadiego,pfuente}@infor.uva.es

2 Departamento de Ciencias de la Computaci´on, Universidad de Chile

Santiago, Chile

gnavarro@dcc.uchile.cl

Abstract. We describe a compression model for semistructured documents,
 called Structural Contexts Model, which takes advantage of the
context information usually implicit in the structure of the text. The
idea is to use a separate semiadaptive model to compress the text that
lies inside each diﬀerent structure type (e.g., diﬀerent XML tag). The
intuition behind the idea is that the distribution of all the texts that belong 
to a given structure type should be similar, and diﬀerent from that
of other structure types. We test our idea using a word-based Huﬀman
coding, which is the standard for compressing large natural language
textual databases, and show that our compression method obtains signiﬁcant 
improvements in compression ratios. We also analyze the possibility 
that storing separate models may not pay oﬀ if the distribution of
diﬀerent structure types is not diﬀerent enough, and present a heuristic
to merge models with the aim of minimizing the total size of the compressed 
database. This technique gives an additional improvement over
the plain technique. The comparison against existing prototypes shows
that our method is a competitive choice for compressed text databases.

Keywords: Text Compression, Compression Model, Semistructured
Documents, Text Databases.

1 Introduction

The process of data compression can be split into two parts: an encoder that
generates the compressed bitstream and a modeler that feeds information to
it [10]. These two separate tasks are called coding and modeling, respectively.
Modeling assigns probabilities to symbols depending on the source data, while
coding translates these probabilities into a sequence of bits. In order to work
properly, the decoder must have access to the same model as the encoder.

Compression of large document collections not only reduces the amount of
disk space occupied by the data, but it also decreases the overall query processing 
time in text retrieval systems. Improvements in processing times are achieved

(cid:1) This work was partially supported by CYTED VII.19 RIBIDI project (all authors)

and Fondecyt Project 1-020831 (second author).

F. Sebastiani (Ed.): ECIR 2003, LNCS 2633, pp. 482–490, 2003.
c(cid:1) Springer-Verlag Berlin Heidelberg 2003

Compressing Semistructured Text Databases

483

thanks to the reduced disk transfer times necessary to access the text in compressed 
form. Also, recent research on “direct” compressed text searching, i.e.,
searching a compressed text without decompressing it, has led to a win-win situation 
where the compressed text takes less space and is searched faster than
the plain text [12, 13].

Compressed text databases pose some requirements that outrule some compression 
methods. The most deﬁnitive is the need for random access to the text
without the possibility of decompressing it from the beginning. This outrules
most adaptive compression methods such as Ziv-Lempel compression and arithmetic 
coding. On the other hand, semiadaptive models —which uses a diﬀerent
model for each text encoded, building it before performing the compression and
storing it in the compressed ﬁle— such as Huﬀman [5] yield poor compression. In
the case of compressing natural language texts, it has been shown that an excellent 
choice is to consider the words, not the characters, as the source symbols [7].
Finally, the fact that the alphabet and the vocabulary of the text collections coincide 
permits eﬃcient and highly sophisticated searching, both in the form of
sequential searching and in the form of compressed inverted indexes over the
text [12, 13, 9, 8].

Although the area of natural language compressed text databases has gone
a long way since the end of the eighties, it is interesting that little has been
done about considering the structure of the text in this picture. Thanks to the
widespread acceptance of SGML, HTML and XML as the standards for storing,
 exchanging and presenting documents, semistructured text databases are
becoming the standard.

Our goal in this paper is to explore the possibility of considering the text
structure in the context of a compressed text database. We aim at taking advantage 
of the structure, while still retaining all the desirable features of a wordbased 
Huﬀman compression over a semiadaptive model. The idea is then to use
separate semiadaptive models to compress the text that lies inside diﬀerent tags.
While the possible gain due to this idea is clear, the price is that we have
to store several models instead of just one. This may or may not pay oﬀ. Hence
we also design a technique to merge the models if we can predict that this is
convenient in terms of compressed ﬁle length. Although the problem of ﬁnding
the optimal merging looks as a hard combinatorial problem, we design a heuristic
to automatically obtain a reasonably good merging of an initially separate set
of models, one per tag.

This model, which we call Structural Contexts Model, is general and does not
depend on the coder. We plug it to a word-based Huﬀman coder to test it. Our
experimental results show signiﬁcant gains over the methods that are insensitive
to the structure and over the current methods that consider the structure. At
the same time, we retain all the features of the original model that makes it
suitable for compressed text databases.

