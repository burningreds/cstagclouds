8
1
0
2

 

b
e
F
8
1

 

 
 
]
S
D
.
s
c
[
 
 

7
v
0
3
1
0
0

.

0
1
6
1
:
v
i
X
r
a

Fast and Compact Planar Embeddings$

Leo Ferresa, Jos´e Fuentes-Sep´ulvedab,c, Travis Gagied,c,∗, Meng Hee, Gonzalo

Navarrob,c

aFaculty of Engineering, Universidad del Desarrollo & Telef´onica I+D, Santiago, Chile

bDepartment of Computer Science, University of Chile, Santiago, Chile.

cCenter of Biotechnology and Bioengineering, University of Chile, Santiago, Chile.
dSchool of Computer Science and Telecommunications, Diego Portales University,

eFaculty of Computer Science, Dalhousie University, Halifax, Canada

Santiago, Chile

Abstract

There are many representations of planar graphs, but few are as elegant as
Tur´an’s (1984): it is simple and practical, uses only 4 bits per edge, can handle
self-loops and multi-edges, and can store any speciﬁed embedding.
Its main
disadvantage has been that “it does not allow eﬃcient searching” (Jacobson,
1989). In this paper we show how to add a sublinear number of bits to Tur´an’s
representation such that it supports fast navigation while retaining simplicity.
As a consequence of the inherited simplicity, we oﬀer the ﬁrst eﬃcient parallel
construction of a compact encoding of a planar graph embedding. Our experimental 
results show that the resulting representation uses about 6 bits per edge
in practice, supports basic navigation operations within a few microseconds,
and can be built sequentially at a rate below 1 microsecond per edge, featuring
a linear speedup with a parallel eﬃciency around 50% for large datasets.

Keywords: Planar embedding, Compact data structures, Parallel construction

1. Introduction

The rate at which we store data is increasing even faster than the speed and
capacity of computing hardware. Thus, if we want to use eﬃciently what we
store, we need to represent it in better ways. The surge in the number and complexity 
of the maps we want to have available on mobile devices is particularly
pronounced and has resulted in a bewildering number of ways to store planar
graphs. Each of these representations has its disadvantages, however: some do

Symposium (WADS 2017)[1].

$A previous version of this paper appeared in the 15th Algorithms and Data Structures
∗Corresponding author
Email addresses: lferres@udd.cl (Leo Ferres), jfuentess@dcc.uchile.cl (Jos´e

Fuentes-Sep´ulveda), travis.gagie@mail.udp.cl (Travis Gagie), mhe@cs.dal.ca (Meng He),
gnavarro@dcc.uchile.cl (Gonzalo Navarro)

Preprint submitted to Computational Geometry

February 20, 2018

not support fast navigation, some are large, some cannot represent multi-edges
or certain embeddings, and some are costly to build. In this paper we introduce
a compact representation of planar graph embeddings that addresses all these
issues, and demonstrate its practicality.
More concretely, as described in Section 2, a planar embedding with n nodes
and m edges can be represented in m log 12 ≈ 3.58m bits [2], which has been
matched with o(m)-bit redundancy with a structure that in addition supports
eﬃcient navigation [3]. The structure is, however, complex and no implementation 
has been attempted. The much simpler representation of Tur´an [4] uses
4m bits, which is still close to the lower bound, but it does not support navigation.
 The other existing representations require more than 4m bits for general
planar embeddings, some restrict the embeddings where they apply, and most
have complicated construction algorithms. The majority of these constructions
cannot be parallelized, and the others require O(m log m) work.

Our contribution in this paper is threefold:

1. We show how to add o(m) bits to Tur´an’s representation such that it
supports fast navigation. We can list the edges incident to any vertex in
clockwise or counter-clockwise order using constant time per edge, including 
starting the enumeration at any desired neighbour. As a consequence,
we can also list the nodes on a face in constant time per node. We can
also ﬁnd a vertex’s degree in time O(f (m)) for any f (m) ∈ ω(1), and
determine whether two vertices are neighbours in O(f (m)) time for any
f (m) ∈ ω(log m).
2. We give a parallel algorithm that builds our data structure from any spanning 
tree of the planar embedding, in O(m) work and O(log m) span

(O(cid:0)log2 m(cid:1) span to support the neighbour query). This is the ﬁrst linearwork 
practical parallel algorithm for building compact representations of
planar graphs.

3. We implement and experimentally evaluate the space, query, and construction 
performance of our representation.
In practice, our structure
uses less than 6m bits, performs navigation operations within a few microseconds,
 and can be built sequentially at a rate below 1 microsecond
per edge. The parallel algorithm scales linearly, with an eﬃciency around
50% for large datasets, with up to 24 processors.

Summarizing, we oﬀer the ﬁrst simple compact representation of planar embeddings,
 which is easy to program, uses little space, and is eﬃciently built and
navigated. Our structure is thousands of times faster than the classical one
when compression makes our representation ﬁt in main memory. We leave the
code publicly available at http://www.dcc.uchile.cl/~jfuentess/pemb/.

Tur´an chooses an arbitrary spanning tree of the graph, roots it at a vertex on
the outer face and traverses it, writing its balanced-parentheses representation
as he goes and interleaving that sequence with another over a diﬀerent binary
alphabet, consisting of an occurrence of one character for the ﬁrst time he sees
each edge not in the tree and an occurrence of the other character for the second
time he sees that edge. These two sequences can be written as three sequences

2

over {0, 1}: one of length 2n − 2 encoding the balanced-parentheses representation 
of the tree; one of length 2m − 2n + 2 encoding the interleaved sequence;
and one of length 2m indicating how they are interleaved. Our extension of this
representation is based on the observation that the interleaved sequence encodes
the balanced-parentheses representation of the complementary spanning tree of
the dual of the graph. By adding a sublinear number of bits to each balancedparentheses 
representation, we can support fast navigation in the trees, and by
storing the sequence indicating the interleaving as a bitvector with support for
operations rank and select [5], we can support fast navigation in the graph.

Section 2 surveys the related work on compact representations of planar
embeddings. Section 3 describes bitvectors and the balanced-parentheses representation 
of trees, which are the building blocks of our extension of Tur´an’s
representation. We also describe the model of parallelism we use in our construction 
algorithms. In Section 4 we prove the observation mentioned above
on Tur´an’s interleaved sequence. In Section 5 we describe our data structure
and how we implement queries. Section 6 describes our parallel construction
algorithm and discusses some implementation issues. In Section 7 we describe
our experiments on space, query and construction performance, and discuss
the results. Finally, in Section 8 we present our conclusions and future work
directions.

2. Related work

Tutte [2] showed that representing a speciﬁed embedding of a connected planar 
multi-graph with n vertices and m edges requires m log 12 ≈ 3.58m bits in
the worst case. Tur´an [4] gave a very simple representation that uses 4m bits.
Jacobson [5] argued that this representation “does not allow fast searching” and
(introducing techniques that we will apply to Tur´an’s representation) proposed
one that instead uses O(m) bits and supports fast navigation, based on book
embeddings [6]. Munro and Raman [7] estimated that Jacobson’s representation 
uses 64n bits and proposed one using 2m + 8n + o(m) bits that retains fast
navigation, still based on the same book embeddings (but this does not handle 
self-loops). Keeler and Westbrook [8] also noted that “the constant factor
in [Jacobson’s] space bound is relatively large” and gave a representation that
uses m log 12 + O(1) bits for planar graphs (not embeddings), as well as for
planar embeddings containing either no self-loops or no vertices with degree 1;
however, they again gave up fast navigation. Chiang, Lin and Lu [9], improving 
previous work by Chuang et al. [10], gave a representation (without allowing
self-loops) that uses 2m+3n+o(m) bits with fast navigation, based on so-called
orderly spanning trees. However, although all planar graphs can be represented
with orderly spanning trees, some planar embeddings cannot. For simple planar
embeddings (i.e., no self-loops nor multiple edges, thus m ≤ 3n), their space decreases 
to 2n + 2m + o(m) ≤ 4m + o(m) on connected graphs. Barbay et al. [11]
gave a data structure that uses O(m) bits to represent simple planar graphs
with fast navigation, based on realizers of planar triangulations [12]. Still, their

3

constant is relatively large, 18n+o(m). Finally, Blelloch and Farzan [3], extending 
the work of Blandford et al.. [13], matched for the ﬁrst time Tutte’s lower
bound on general planar embeddings, with a structure that uses m log 12 + o(m)
bits and supports fast navigation. Their structure is based on small vertex separators 
[14]. They can also represent any planar graph within its lower-bound
space plus a sublinear redundancy, even when the exact lower bound is unknown
for general planar graphs [15]. While Blelloch and Farzan closed the problem
in theoretical terms, their representation is complicated and has not been implemented.
 Other authors [16, 17, 18, 19, 20] have considered special kinds of
planar graphs, notably tri-connected planar graphs and triangulations. We refer
the reader to Munro and Nicholson’s [21] and Navarro’s [22, Chapter 9] recent
surveys for further discussion of compact data structures for graphs.

Most of the navigable representations we have mentioned require complicated 
construction algorithms, generally defying a parallel implementation. It
is not known how to compute a book embedding [6] in parallel, which is necessary 
to build the representations of Jacobson and of Munro and Raman. There
are also no parallel algorithms to build orderly spanning trees [9], necessary for
the representation of Chiang et al. Its predecessor [10] uses instead a triangulation 
and a canonical ordering; for the latter there is only a CREW construction

running in O(cid:0)log4 n(cid:1) time with n2 processors [23]. As for the vertex separators 
[14] required to build the representation of Blandford et al. and of Blelloch
and Farzan, Kao et al. [24] designed a linear-work, logarithmic-span algorithm
for computing a cycle separator of a planar graph. However, the construction
of these representations of planar embeddings decompose the input graph by
repeatedly computing separators until each piece is suﬃciently small. This increases 
the total work to O(n log n) even if this optimal parallel algorithm is
used. The best linear-work parallel algorithms [25] for building the realizers
[12] used in the construction of Barbay et al.’s representation have O(log n)
span in the expected case but O(log n log log n) deterministic span.

3. Preliminaries

3.1. Bitvectors and parentheses

A bitvector is a binary string that supports the queries rank and select in
addition to random access, where rankb(i) returns the number of bits set to b in
the preﬁx of length i of the string and selectb(j) returns the position of the jth
bit set to b. For convenience, we deﬁne selectb(0) = 0.

the bitvector has k 1s, it can be represented in log(cid:0)(cid:96)

It is possible to represent a bitvector of length (cid:96) in (cid:96) + o((cid:96)) bits and support
random access, rank and select in constant time [5, 26, 27]. Furthermore, if
(cid:96)H(k/(cid:96))+o((cid:96)) = k log((cid:96)/k)+O(k)+o((cid:96)), with H(x) = −x log x−(1−x) log(1−
x). All our logarithms are to the base 2 unless otherwise stated.

(cid:1) + o((cid:96)) bits [28], which is

k

By adding some further operations on the bitvectors, we can represent an
ordered tree or forest of t vertices using 2t + o(t) bits and support natural navigation 
queries in constant time. One of the most popular such representations is

4

a string of balanced parentheses: we traverse each tree from left to right, writing
an opening parenthesis when we ﬁrst visit a vertex (starting at the root) and
a closing parenthesis when we leave it for the last time (or, in the case of the
root, when we ﬁnish the traversal). We can encode the string of parentheses as
a bitvector of length 2t, with 0s encoding opening parentheses and 1s encoding
closing parentheses. By adding o(t) further bits, we can support in constant
time, among others, the following queries used by our solution [7, 29, 30]:

• match(i) locates the position of the parenthesis matching the ith parenthesis 
in the bitvector (i.e., ﬁnds the other parenthesis referring to the
same node);

• parent(v) returns the parent of v, or 0 if v is the root of its tree. Nodes v

and parent(v) are represented as their pre-order rank in the traversal.

3.2. Parallel computation model

As we focus on practical parallel algorithms, we describe and analyze our
construction using the Dynamic Multithreading (DyM) Model [31] (we nevertheless 
express our ﬁnal results in terms of the PRAM model as well). In the
DyM model, a multithreaded computation is modelled as a directed acyclic
graph (DAG) where vertices are instructions and edge (u, v) represents precedence 
between instructions u and v. The model is based on two parameters
of the multithreaded computation: its work T1 and its span T∞. The work is
the running time on a single thread, that is, the number of nodes (i.e., instruc-
tions) in the DAG, assuming each instruction takes constant time. The span is
the length of the longest path in the DAG, that is, the intrinsically sequential
part of the computation. The time Tp needed to execute the computation on p
threads then has complexity Θ(T1/p + T∞), which can be reached with a greedy
scheduler. The improvement of a multithreaded computation using p threads is
called speedup, T1/Tp. The upper bound on the achievable speedup, T1/T∞, is
called parallelism. Finally, the eﬃciency is deﬁned as T1/pTp and can be interpreted 
as the percentage of improvement achieved by using p cores or how close
we are to the linear speedup. In the DyM model, the workload of the threads
is balanced by using the work-stealing algorithm [32].

To describe parallel algorithms in the DyM model, we augment sequential
pseudocode with three keywords. The spawn keyword, followed by a procedure
call, indicates that the procedure should run in its own thread and may thus
be executed in parallel to the thread that spawned it. The sync keyword indicates 
that the current thread must wait for the termination of all threads it has
spawned. Finally, parfor is “syntactic sugar” for spawning one thread per iteration 
in a for loop, thereby allowing these iterations to run in parallel, followed
by a sync operation that waits for all iterations to complete. In practice, the
parfor keyword is implemented by halving the range of loop iterations, spawning 
one half and using the current procedure to process the other half recursively
until reaching one iteration per range. After that, the iterations are executed
in parallel. Therefore, this implementation adds an overhead bounded above by

5

the logarithm of the number of loop iterations. We include such overheads in
our complexities.

4. Spanning trees of planar graphs

It is well known [33, 34, 35] that for any spanning tree T of a connected
planar graph G, the edges dual to T are a spanning tree T ∗ of the dual of G,
with T and T ∗ interdigitating; see Figure 1 for an illustration (including multiedges 
and a self-loop). If we choose T as the spanning tree of G for Tur´an’s
representation, then we store a 0 and a 1, in that order, for each edge in T ∗.
We now show that these bits encode a traversal of T ∗.

Lemma 1. Consider any planar embedding of a planar graph G, any spanning
tree T of G and the complementary spanning tree T ∗ of the dual of G. If we
perform a depth-ﬁrst traversal of T starting from any vertex on the outer face
of G and always process the edges incident to the vertex v we are visiting in
counter-clockwise order (starting from the edge immediately after the one to v’s
parent or, if v is the root of T , from immediately after any incidence of the
outer face), then each edge not in T corresponds to the next edge we cross in a
depth-ﬁrst traversal of T ∗.
Proof. Suppose the traversal of T ∗ starts at the vertex of the dual of G corresponding 
to the outer face of G. We now prove by induction that the vertex we
are visiting in T ∗ always corresponds to the face of G incident to the vertex we
are visiting in T and to the previous and next edges in counter-clockwise order.
Our claim is true before we process any edges, since we order the edges
starting from an incidence of the outer face to the root of T . Assume it is still
true after we have processed i < m edges, and that at this time we are visiting
v in T and v∗ in T ∗. First suppose that the (i + 1)th edge (v, w) we process is in
T . We note that w (cid:54)= v, since otherwise (v, w) could not be in T . We cross from
v to w in T , which is also incident to the face corresponding to v∗. Now (v, w)
is the previous edge — considering their counter-clockwise order at w, starting
from (v, w) — and the next edge (which is (v, w) again if w has degree 1) is also
incident to v∗. This is illustrated on the left side of Figure 2. In fact, the next
edge is the one after (v, w) in a clockwise traversal of the edges incident to the
face corresponding to v∗.
Now suppose (v, w) is not in T and let w∗ be the vertex in T ∗ corresponding
to the face on the opposite side of (v, w), which is also incident to v. We note
that w∗ (cid:54)= v∗, since otherwise (v, w) would have to be in T . We cross from v∗
to w∗ in T ∗. Now (v, w) is the previous edge — this time still considering their
counter-clockwise order at v — and the next edge (which may be (v, w) again
if it is a self-loop) is also incident to w∗. This is illustrated on the right side
of Figure 2. In fact, the next edge is the one that follows (v, w) in a clockwise
traversal of the edges incident to the face corresponding to w∗.

Since our claim remains true in both cases after we have processed i + 1
In other words, whenever we should

edges, by induction it is always true.

6

T

G − T

T ∗

(1, 3)

(A, B)

(1, 2)
(2, 3)

(2, 3)
(2, 4)

(2, 4)

(1, 2)
(1, 5)
(5, 6)

(5, 6)

(1, 5)
(1, 7)

(7, 8)

(7, 8)

(1, 7)

(1, 3)

(A, B)

(4, 8)

(A, C)

(2, 6)

(C, D)

(2, 6)
(6, 8)

(C, D)
(C, E)

(5, 7)

(E, F)

(5, 7)

(E, F)

(6, 8)
(4, 8)
(7, 8)

(C, E)
(A, C)
(A, G)

(7, 8)

(A, G)

(1, 1)
(1, 1)

(A, H)
(A, H)

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28

Figure 1: Top left: A planar embedding of a planar graph G, with a spanning tree T of G
shown in red and the complementary spanning tree T ∗ of the dual of G shown in blue with
dashed lines. Bottom left: The two spanning trees, with T rooted at the vertex 1 on the
outer face and T ∗ rooted at the vertex A corresponding to the outer face. Right: The list of
edges we process while traversing T starting at 1 and processing edges in counter-clockwise
order, with the edges in T shown in red and the ones in G − T shown in black; the edges of
T ∗ corresponding to the edges in G − T are shown in blue.

process next an edge e in G that is not in T , we are visiting in T ∗ one of the
vertices corresponding to the faces incident to e (i.e., one of the endpoints of
the edge in the dual of G that corresponds to e). Since we process each edge
in G twice, once at each of its endpoints or twice at its unique endpoint if it
is a self-loop, it follows that the list of edges we process that are not in T ,
(cid:3)
corresponds to the list of edges we cross in a traversal of T ∗.

7

145678ABCDEF23GHHGCBEDFA34625178Figure 2: Left: If we process an edge (v, w) in T , then we move to w in our traversal of T
and the next edge, (w, x) in this case, is also incident to the vertex v∗ we are visiting in our
traversal of T ∗. Right: If (v, w) is not in T , then in T ∗ we move from v∗ to the vertex w∗
corresponding to the face on the opposite side of (v, w) in G. The next edge, (v, y) in this
case, is also incident to w∗.

We process the edges in counter-clockwise order so that the traversals of T
and T ∗ are from left to right and from right to left, respectively; processing
them in clockwise order would reverse those directions. For example, for the
embedding in Figure 1, if we start the traversal of the red tree T at vertex 1 and
start processing the edges at (1, 3), then we process them in the order shown at
the right of the ﬁgure.

5. Data structure

Our extension of Tur´an’s representation of a planar embedding of a connected 
planar graph G with n vertices and m edges consists of the following
components, which take 4m + o(m) bits:

• a bitvector A[1..2m] in which A[i] = 1 if and only if the ith edge we process

in the traversal of T described in Lemma 1, is in T ;

• a bitvector B[1..2(n − 1)] in which B[i] = 0 if and only if the ith time we
process an edge in T during the traversal, is the ﬁrst time we process that
edge;

• a bitvector B∗[1..2(m − n + 1)] in which B∗[i] = 0 if and only if the ith
time we process an edge not in T during the traversal, is the ﬁrst time we
process that edge.

Notice B encodes the balanced-parentheses representation of T , except that it
lacks the leading 0 and trailing 1 encoding the parentheses for the root. By
Lemma 1, B∗ encodes the balanced-parentheses representation of a traversal of
the spanning tree T ∗ of the dual of G complementary to T (the right-to-left
traversal of T ∗, in fact), except that it also lacks the leading 0 and trailing 1
encoding the parentheses for the root. Therefore, since B and B∗ encode forests,
we can support match and parent with them.
To build A, B and B∗ given the embedding of G and T , we traverse T as
in Lemma 1. Whenever we process an edge, if it is in T then we append a 1 to
A and append the edge to a list L; otherwise, we append a 0 to A and append
the edge to another list L∗. When we have ﬁnished the traversal, we replace
each edge in L or L∗ by a 0 if it is the ﬁrst occurrence of that edge in that list,

8

vwv∗vwv∗w∗xyuuand by a 1 if it is the second occurrence; this turns L and L∗ into B and B∗,
respectively. For the example shown in Figure 1, L and L∗ eventually contain
the edges shown in the columns labelled T and G− T , respectively, in the table
on the on the right side of the ﬁgure, and

A[1..28] = 0110110101110010110100010100

B[1..14] = 00101100110011
B∗[1..14] = 01001001110101 .

We identify each vertex v in G by its pre-order rank in our traversal of T .
We say that, while we visit v, we process all the edges that lead from v to other
nodes w. Note that each edge (v, w) is processed twice, while visiting v and
while visiting w, but these correspond to two distinct positions in our traversal.
Consider the following queries:

ﬁrst(v): return i such that the ﬁrst edge we process while visiting v is the ith

we process during our traversal;

last(v): return i such that the last edge we process while visiting v is the ith we

process during our traversal;

next(i): return j such that if we are visiting v when we process the ith edge
during our traversal, then the next edge we process when visiting v, in
counter-clockwise order, is the one we process jth;

prev(i): return j such that if we are visiting v when we process the ith edge
during our traversal, then the previous edge we processed when visiting
v, in counter-clockwise order, is the one we process jth;

mate(i): return j such that we process the same edge ith and jth during our

traversal;

vertex(i): return the vertex v such that we are visiting v when we process the

ith edge during our traversal.

With these it is straightforward to reenact our traversal of T and recover the
embedding of G. For example, with the following queries we can list the edges
incident to the root of T in Figure 1 and determine whether they are in T :

mate(1) = 4
ﬁrst(1) = 1
mate(2) = 10
next(1) = 2
mate(11) = 17
next(2) = 11
next(11) = 18 mate(18) = 26

vertex(4) = 3
A[1] = 0
vertex(10) = 2 A[2] = 1
vertex(17) = 5 A[11] = 1
vertex(26) = 7 A[18] = 1 .

To see why we can recover the embedding from the traversal, consider that if
we have already correctly embedded the ﬁrst i edges processed in the traversal,
then we can embed the (i + 1)th correctly given its endpoints and its rank in the
counter-clockwise order at those vertices. Queries last and prev are superﬂuous
for this task, but they allow traversing the neighbours of a node in clockwise
order.

9

5.1. Implementing the basic queries

We now explain our constant-time implementations of ﬁrst, next, prev, mate

and vertex.

Query ﬁrst. If m = 0 then ﬁrst(v) is undeﬁned, which we indicate by returning
0. Otherwise, we ﬁrst process an edge at v immediately after ﬁrst arriving at
v. Since we identify v with its pre-order rank in our traversal of T and B lacks
the opening parenthesis for the root, while ﬁrst arriving at any vertex v other
than the root we write the (v − 1)th 0 in B and, thus, the B.select0(v − 1)th 1
in A. If v is the root then ﬁrst(v) = 1 and so, since selectx(0) = 0, this case is
also handled by the formula below:

(cid:26) A.select1(B.select0(v − 1)) + 1 if m ≥ 1

0

otherwise.

ﬁrst(v) =

In our example,

ﬁrst(5) = A.select1(B.select0(4)) + 1 = A.select1(7) + 1 = 12

and indeed the twelfth edge we process, (5, 6), is the ﬁrst one we process at
vertex 5. Note that the formula works for nodes with only one edge too.

Query last. The logic of last is similar to that of ﬁrst; we must locate the closing
parenthesis that represents v in T .

(cid:26) A.select1(B.match(B.select0(v − 1))) if m ≥ 1

last(v) =

0

otherwise.

Query next. If the ith edge we process is the last edge we process at a vertex
v then next(i) is undeﬁned, which we again indicate by returning 0. This is the
case when i = 2m, or A[i] = 1 and B[A.rank1(i)] = 1. Otherwise, if the ith edge
we process is not in T , then A[i] = 0, and we process the next edge at v one
time step later. Finally, if the ith edge e we process is in T and not the last one
we process at v, then we next process an edge at v immediately after returning
to v by processing e again at time mate(i). This is the case when A[i] = 1 and
B[A.rank1(i)] = 0. In other words,

 i + 1

next(i) =

mate(i) + 1 if i < 2m and A[i] = 1 and B[A.rank1(i)] = 0
0

otherwise.

if i < 2m and A[i] = 0

In our example, since A[12] = 1, B[A.rank1(12)] = B[8] = 0, the twelfth edge
we process is (5, 6) and it is also the ﬁfteenth edge we process,

next(12) = mate(12) + 1 = 16 ,

and indeed the second edge we process at vertex 5 is (5, 7).

10

Query prev. The logic for prev is similar to that of next; we only need to consider 
that, once we move one position backwards, we might arrive at a closing
parenthesis. The formula follows.

prev(i) =

mate(i − 1) if i > 1 and A[i − 1] = 1 and B[A.rank1(i − 1)] = 1
0

otherwise.

if i > 1 and A[i − 1] = 0

 i − 1

Query mate. To implement mate(i), we check A[i] to determine whether we
wrote a bit in B or in B∗ while processing the ith edge, and use rank on A to
ﬁnd that bit in the corresponding sequence. We then use match to ﬁnd the bit
encoding the matching parenthesis, and ﬁnally use select on A to ﬁnd where we
wrote in A that matching bit. Therefore,

(cid:26) A.select0(B∗.match(A.rank0(i))) if A[i] = 0

mate(i) =

A.select1(B.match(A.rank1(i)))

otherwise.

To compute mate(12) for our example, since A[12] = 1,

mate(12)

= A.select1(B.match(A.rank1(12)))
= A.select1(B.match(8))
= A.select1(9)
= 15 .

Query vertex. Suppose the ith edge e we process is not in T and we process it at
vertex v. If the preceding time we processed an edge in T was the ﬁrst time we
processed that edge, we then wrote a 0 in B, encoding the opening parenthesis
for v; otherwise, we then wrote a 1 in B, encoding the closing parenthesis for
one of v’s children. Now suppose e is in T . If that is the ﬁrst time we process
e, we move to the other endpoint w of e — which is a child of v — and write
a 0 in B, encoding the opening parenthesis for w. If it is the second time we
process e, then we write a 1 in B, encoding the closing parenthesis for v itself.
Therefore,



vertex(i) =

B.rank0(A.rank1(i)) + 1

if A[i] = 0 and B[A.rank1(i)] = 0

B.parent(B.rank0(B.match(A.rank1(i)))) + 1

if A[i] = 0 and B[A.rank1(i)] = 1

B.parent(B.rank0(A.rank1(i))) + 1

if A[i] = 1 and B[A.rank1(i)] = 0

B.rank0(B.match(A.rank1(i))) + 1

otherwise.

In our example, since A[16] = 0 and B[A.rank1(16)] = B[9] = 1,

vertex(16)

11

Function degree

Function listing

Input: node v

Input: node v

Function face
Input: edge e

1 d = 0
2 edg = first(v)
3 while edg (cid:54)= 0 do

4

5

edg = next(edg)
d = d + 1

6 return d

1 edg = first(v)
2 while edg (cid:54)= 0 do
mt = mate(edg)
output vertex(mt)
edg = next(edg)

3

4

5

1 edg = e, fst = true
2 while edg (cid:54)= e or fst do

3

4

5

6

fst = false
mt = mate(edg)
output vertex(mt)
edg = next(mt)

= B.parent(B.rank0(B.match(A.rank1(16)))) + 1
= B.parent(B.rank0(B.match(9))) + 1
= B.parent(B.rank0(8)) + 1
= B.parent(5) + 1
= 5 ,

and indeed we process the sixteenth edge (5, 7) while visiting 5.

We remind the reader that since B lacks parentheses for the root of T ,
B.parent(5) refers to the parent of the ﬁfth vertex in an in-order traversal of T
not including the root, i.e., the parent vertex 5 of vertex 6. Adding 1 includes
the root in the traversal, so the ﬁnal answer correctly refers to vertex 5. The
lack of parentheses for the root also means that, e.g., B.parent(4) refers to the
parent of vertex 5 and returns 0 because vertex 5 is the root of its own tree in
the forest encoded by B, without vertex 1. Adding 1 to that 0 also correctly
turns the ﬁnal value into 1, the in-order rank of the root. Of course, we have the
option of prepending and appending bits to A, B and B∗ to represent the roots
of T and T ∗, but that slightly confuses the relationship between the positions
of the bits and the time steps at which we process edges.

We also note that, if we do not require that node identiﬁers are precisely
preorder ranks in T , then we can use the positions of their 0 in B as their
identiﬁers. This removes the need for using B.rank0 and B.select0 in all the
formulas that convert between node identiﬁers and positions in T .

5.2. More complex queries

We can deﬁne more complex queries on top of the basic ones. For example,
 we give the pseudocode of three queries: degree(v) returns the number of
neighbours of vertex v; listing(v) returns the list of neighbours of vertex v, in
counter-clockwise order; face(e) returns the list of vertices, in clockwise order,
of one of the face where the edge e belongs. We also support the other order
(clockwise or counter-clockwise, or the other face where e belongs) by using last
and prev instead of ﬁrst and next.
Queries listing(v) and face(e) are implemented in optimal time, that is,
Instead, degree(v) requires time O(degree(v)).
O(1) per returned element.
We can also determine neighbour(u, v), that is, whether two vertices u and v
are neighbours, by listing the neighbours of each in interleaved form, in time

12

O(min(degree(u), degree(v))). These times are not so satisfactory compared
with the O(1) achieved by other representations [7, 9] to compute neighbour(u, v)
and degree(v).

For degree(v), we can get arbitrarily close to constant time by adding
o(m) further bits to our representation, that is, we can solve the query in time
O(f (m)) for any given function f (m) ∈ ω(1). To do this, we store a bitvector
D[1..n] marking with 1s the (at most) m/f (m) = o(m) vertices with degree at
least f (m), which takes nH(m/(nf (m)))+o(n) = O((m/f (m)) log(nf (m)/m))+
o(n) = o(m) bits by using a sparse bitvector representation [28] (recall that
G is connected, so m ≥ n − 1). We also store a second bitvector E[1..2m]
where we append, for each D[v] = 1, degree(v) − 1 copies of 0s followed by a
1. Since E has m/f (m) 1s, it can also be stored as a sparse bitvector using
O((m/f (m)) log f (m)) + o(m) = o(m) bits. Therefore, if D[v] = 1, its degree 
is obtained in constant time with select1(E, r) − select1(E, r − 1), where
r = rank1(D, v). If, instead, D[v] = 0, then we know that degree(v) < f (m)
and thus we apply the procedure that sequentially counts the neighbours, in
time O(f (m)).
We can use a similar idea, albeit more complex, to answer neighbour(u, v)
queries in time O(f (m)), for any f (m) = ω(log m). We consider the graph
induced by the O(m/f (m)) = o(m/ log m) nodes with degree f (m) or higher
and eliminate multi-edges and self-loops. The resulting graph G(cid:48) is simple and
still planar, so it has average degree less than 6 and thus o(m/ log m) edges.
We represent G(cid:48) in classical adjacency-list form, with the nodes inside each
list sorted by increasing node identiﬁer. This requires o(m) bits in total. To
solve neighbour(u, v) in G(cid:48), we can use binary search for v in the list of u in
time O(log m) = o(f (m)). To answer neighbour(u, v) on G, we check whether
either u or v is low-degree (assuming we mark low-degree nodes in a bitvector
D(cid:48) analogous to D) and, if so, list its neighbours in O(f (m)) time. If not, we
translate nodes u and v to their corresponding nodes u(cid:48) = rank1(D(cid:48), u) and
v(cid:48) = rank1(D(cid:48), v) in G(cid:48) and query G(cid:48) in time o(f (m)).

The following theorem summarizes the results of this section.

Theorem 1. We can store a given planar embedding of a connected planar
graph G with m edges in 4m + o(m) bits such that later, given a vertex v, we
can list the edges incident to v in clockwise or counter-clockwise order, even if
we are given a particular starting edge incident to v, using constant time per
edge. We can also traverse the edges limiting a face in constant time per edge.
Further, we can ﬁnd a vertex’s degree in O(f (m)) time for any given function
f (m) ∈ ω(1), and determine whether two vertices are neighbours in O(f (m))
time for any given function f (m) ∈ ω(log m).

5.3. Reducing space on simple planar graphs

Chiang et al. [9] use 2m + 3n + o(m) bits to represent planar graphs without
self-loops, which can be more than the 4m+o(m) bits used in our representation.
However, if G is simple (i.e., has no loops nor multiple edges), their representation 
requires only 2m + 2n + o(m) ≤ 4m + o(m) bits. We remind the reader that

13

this representation can handle any simple planar graph, but does not always
respect the given embedding, so they cannot represent arbitrary embeddings.

We show that, if there are no self-loops, our representation can use less
than 4m + o(m) bits, by exploiting some redundancy in our representation and
without changing the main scheme. Assume we represent a single sequence
S[1..2m] over an alphabet of four symbols, Σ = {(, ), [, ]}, that replaces A, B,
and B∗. That is, the parentheses are the 0s and 1s in B, the brackets are the
0s and 1s and B∗, and A corresponds to whether the symbols are parentheses
or brackets. In our running example, the sequence is

C∈Σk

c∈Σ

mc
2m log 2m
mc

S[1..2m] = [ ( ( ] ) ( [ ) [ ) ( ( ] [ ) [ ) ( ] ( ] ] [ ) ] ) [ ].

The zeroth-order entropy of S is deﬁned as H0(S) = (cid:80)
Hk(S) = (cid:80)

, where
c occurs mc times in S. The kth-order entropy, for any k > 0, is deﬁned as
|SC|
2m H0(SC), where SC is the string formed by the symbols
that follow the context C in S (assume S is circular for simplicity, so that S[1]
follows S[2m]).
Ferragina and Venturini [36] show how to store a string S within |S|Hk(S) +
o(|S| log |Σ|) bits, for any k = o(log|Σ| |S|), so that any substring of length
O(log |S|) can be extracted in constant time. We use their result to store S
Instead of a structure on parentheses on bitvector
in 2mH1(S) + o(m) bits.
B and another on bitvector B∗, we build both parentheses structures on top
of sequence S. Both are similar to the original o(m)-bit structure of Navarro
and Sadakane [30], only that the structure built to navigate parentheses ignores
the bracket symbols, and vice versa (a similar arrangement is described by
Navarro [22, pp. 311–315]). The only changes are that each symbol uses 2 bits
instead of 1, that there are two symbols that do not change the “excess” count
(number of opening minus closing parentheses up to some position), and that
in order to extract a chunk of Θ(log m) symbols, we use the extraction method
of Ferragina and Venturini [36]. A rank/select functionality on top of A is also
easily provided on top of S, by using the same o(m)-bit structures [26, 27] and
interpreting both parentheses as 1s and both brackets as 0s. Therefore, with
o(m) further bits, we provide the necessary functionality on top of the H1(S)
bits needed to encode S.

This entropy gives precisely 2 bits per symbol (and thus 4m bits in total)
for general planar embeddings, but if there are no self-loops, then the substring
“[ ]” cannot appear in S (other longer strings cannot appear either, but we
would need a higher-entropy model to capture them). An upper bound to
the ﬁrst-order entropy when this substring is forbidden is obtained by noticing
that we can have only 3 symbols, instead of 4, following an opening bracket;
therefore we can encode S using n log 4 + n log 4 + (m− n) log 3 + (m− n) log 4 =
m log 12 + n log(4/3) ≈ 3.58m + 0.42n. This is still 4m in the worst case.
To obtain a nontrivial bound in terms of m, we calculate the exact ﬁrst-order
entropy of S when substring “[ ]” is forbidden.

Let us use the names op = (, cp =), ob = [, and cb =]. Let us call xy number
of symbols y following a symbol x in S; for example opob is the number of opening

14

brackets following opening parentheses, that is, the number of occurrences of

substring “( [” in S. It must then hold that(cid:80) op∗ =(cid:80) cp∗ = n and(cid:80) ob∗ =
(cid:80) cb∗ = m − n. It also holds(cid:80)∗op =(cid:80)∗cp = n and(cid:80)∗ob =(cid:80)∗cb = m − n.
and x =(cid:80) xi. Forbidding self-loops implies
where H(x1, . . . , x4) =(cid:80) xi

2mH1(S) = nH(op∗) + nH(cp∗) + (m − n)H(ob∗) + (m − n)H(cb∗),

The system of restrictions must be satisﬁed while maximizing

x log x
xi
the additional restriction obcb = 0.

We solve the optimization problem with a combination of algebraic and numeric 
computation, using Maple and C, up to 4 signiﬁcant digits. We ﬁnd that
the entropy is maximized at a value slightly below 3.8m.1 Therefore, the resulting 
space with no self-loops and using the described compressed representation
can be bounded by 3.8m + o(m) bits. Simple graphs have no self-loops and
no multiple-edges, but this second restriction translates into longer forbidden
substrings, whose eﬀect is harder to analyze.
We remind the reader that the representation of Keeler and Westbrook [8],
on the other hand, achieves m log 12 ≈ 3.58m bits when no self-loops (or, alternatively,
 no degree-one nodes) are permitted, yet it does not support queries.
When neither self-loops nor degree-one nodes are permitted, they reach 3m bits.
In this case, both “[ ]” and “( )” are forbidden strings. While we have not been
able to compute the exact ﬁrst-order entropy in this case, this must be at most
n log 3 + n log 4 + (m − n) log 3 + (m − n) log 4 = m log 12 ≈ 3.58m, which is obtained 
by using log 4 bits to encode the symbol that follows a closing bracket or
parenthesis, and log 3 bits to encode the symbol that follows an opening bracket
or parenthesis.

We note that these space improvements can also be applied on top of the
representation of Chiang et al. [9] since, when encoding a simple graph, the
diﬀerence between both representations is that they use a particular spanning
tree (which may also force a particular embedding).

5.4. Unconnected planar graphs

Our representation can be easily extended to unconnected planar graphs,
because our parentheses representations can immediately be extended to handle
forests instead of just individual trees. To handle an unconnected planar graph,
we ﬁrst ﬁnd all the connected components of the graph and then compute an
arbitrary spanning tree for each connected component. Then, we construct
the binary sequences: the sequence B will represent the forest of the spanning
trees, concatenating all the balanced-parentheses representations; the sequence
B∗ will represent the complementary spanning tree of the dual of the graph.
Finally, sequence A indicates the interleaving of the sequences B and B∗. We
visit the connected components in arbitrary order.

1The maximum is 3.7999m, found for m = 1.731n, opop = cpop = 0.2683n, obop = 0.2679n,
cbop = 0.1955n, opcp = cpcp = 0.2677n, obcp = 0.2673n, cbcp = 0.1974n, opob = cpob =
0.1961n, obob = 0.1958n, cbob = 0.1429n, opcb = cpcb = 0.2679n, obcb = 0, cbcb = 0.1952n.

15

Note that, in the case of connected planar graphs, our navigation queries and
the fact that the ﬁrst edge we list is adjacent to the external face, are suﬃcient
to recover the embedding. This is not the case if the graph has k > 1 connected
components. Concretely, some components may be embedded inside faces of
other components, whereas our arrangement assumes that all the connected
components lie on the outer face (our navigation queries cannot distinguish
between those cases).
To recover the embedding we might add k − 1 edges to the spanning tree, so
that all the connected components lying in a single face are threaded through
a node in their frontier, and the ﬁrst one is linked to a node on the containing
face. Therefore the total length of A will be 2(m + k − 1) and the length of B
will be 2(n + k− 2). The fake edges will be marked in a bitvector K[1..n + k− 2]
indexed by preorder value. Since K contains k − 1 1s, it can be encoded in
k log(n/k) + O(k) bits [37]. Since n ≤ m + k, the space of the whole structure
can be written in terms of m and k as 4m + k log(m/k) + O(k) + o(m) bits.
The k log(m/k) + O(k) or k log(n/k) + O(k) bits to describe the embedding
are asymptotically optimal: consider a chain of t triangles (delimited with m =
2t + 1 edges) and k − 1 isolated nodes (so there are k connected components in
total) to represent all the ways to distribute k− 1 balls into t bins. This requires
This is also k log(n/k) + O(k) bits, since this graph has n = 2t + k nodes.

(cid:1) = k log(t/k) + O(k) = k log(m/k) + O(k) bits with any encoding.

log(cid:0)k+t−2

k−1

We use K to avoid listing fake edges in any of the traversal operations.
The fake edges increase the degree of a node by a constant factor: a node may
have one fake edge per face it participates in, which at most doubles its degree.
Further, a node in the frontier of its component may have two extra fake edges
threading it with other connected components. Therefore, the time complexity
of the navigation operations is not aﬀected.

The fake edges may, in addition, be useful for a more ambitious face operation 
that takes into account the actual embedding, where a face is surrounded
by a sequence of edges but is also limited by the frontier edges of the connected
components it has inside. To ﬁnd all those edges, we also traverse the fake edges
in the face traversal, yet without listing them. The fake edges will lead us to
the other connected components that are contained and/or surround the face
we are listing.

6. Parallel construction

In this section we discuss the parallel construction of our extension of Tur´an’s
representation. Since the representation is based on spanning trees and tree
traversals, we can borrow ideas of well-known parallel algorithms, such as parallel 
Euler Tour traversal or parallel computation of spanning trees.
We assume that a tree T is represented with adjacency lists. Such representation 
consists of an array of nodes VT [1..n], and an array of edges ET [1..2n−2].
Each node v ∈ VT stores two indices in ET , v.ﬁrst and v.last, delimiting the adjacency 
list of v, which starts with v’s parent edge (except the root) and is sorted

16

counter-clockwise around v. The number of children of v is then v.last − v.ﬁrst
(plus 1 for the root). Each edge e ∈ ET has three ﬁelds: e.src and e.tgt are
the positions in VT of its source and target vertices, and e.mat is the position
in ET of the mate edge e(cid:48) of e, where e(cid:48).src = e.tgt and e(cid:48).tgt = e.src. Our
representation of graphs is similar, with the exception that the concept of parent
of a vertex is not valid in graphs; therefore the ﬁrst edge in the adjacency list
of a vertex v cannot be interpreted as v’s parent edge.

6.1. Parallel construction of compact planar embeddings

We will ﬁrst assume that the input consists of a connected planar graph
embedding G = (VG, EG) and a spanning tree T = (VT , ET ) of G, together with
an array C that stores the number of edges of G\T between any two consecutive
edges in T , in counter-clockwise order. In Section 6.3 we will explain how to
obtain T and C in parallel.
With the spanning tree, we construct the bitvectors A, B, and B∗ by performing 
an Euler Tour over T . During the tour, by writing a 0 for each forward
(parent to child) edge and a 1 for each backward (child to parent) edge, we
obtain the bitvector B. By reading in C the number of edges of G \ T between
two consecutive edges of T , representing these edges with 0s and the edges of T
with 1s, we obtain the bitvector A. Finally, by using the previous Euler Tour
and the array C we can obtain the bitvector B∗, by ﬁnding out which is the
ﬁrst (0) and which is the second (1) occurrence of each edge.

Algorithm 1 gives the detailed pseudocode. It works in the following steps:
1. In lines 1–4, it initializes the output bitvectors (A and B∗ are set to 0s)
and creates an auxiliar array LE that is used to store the traversal of the
tree following the Euler Tour. Each entry of LE represents one traversed
edge of T and stores four ﬁelds: value is 0 or 1 depending on whether the
edge is a forward or a backward edge, respectively; succ is the index in
LE of the next edge in the Euler tour; rankA is the rank of the edge in
A; and rankB is the rank of the edge in B.
2. In lines 5–19, the algorithm traverses T to create the Euler Tour. For
each edge ej ∈ ET , rankA is set to C[ET [j].mat] + 1 and rankB to 1 (lines
6–7). Those ranks will be used later to compute the ﬁnal positions of
the edges in A, B, and B∗. For each forward edge, a 0 is written in the
corresponding value ﬁeld and the succ ﬁeld is connected to the next edge
in the Euler Tour. For backward edges the procedure is similar. Note that
all the edges in the adjacency list of a node of T are forward edges, except
(for non-root nodes) the ﬁrst one, which is the parent edge.

3. Line 20 computes the ﬁnal ranks in A and B using a parallel list ranking
algorithm that adds up the weights from the beginning of the list to each
element. The weights are stored in the ﬁelds rankA and rankB of LE. We
use the list ranking algorithm of Helman and J´aj´a [38].

4. Bitvectors A and B are written in lines 21–23. Since initially all the
elements of A are 0s, it is enough to set to 1 all the elements in the rankA

17

Algorithm 1: Parallel compact planar embedding algorithm.

Input : A planar graph embedding G = (VG, EG), a spanning tree T = (VT , ET ) of
Output: Bitvectors A, B and B∗ induced by G and T .

G, an array C of size |ET |, and the starting vertex init.

1 A = a bitvector of length |EG| initialized with 0s
2 B = a bitvector of length |ET | − 2
3 B∗ = a bitvector of length |EG| − |ET | + 2 initialized with 0s
4 LE = an array of length |ET |
5 parfor j = 1 to |ET | do

6

7

8

9

10

11

12

13

14

15

16

17

18

19

LE [j].rankA = C[ET [j].mat] + 1
LE [j].rankB = 1
if ET [j].src = init or ET [j].src.ﬁrst (cid:54)= j then // forward edge

LE [j].value = 0 // opening parenthesis
if ET [j].tgt.ﬁrst = ET [j].tgt.last then // target is a leaf

LE [j].succ = ET [j].mat

else // target has children

LE [j].succ = ET [j].tgt.ﬁrst + 1

else // backward edge

LE [j].value = 1 // closing parenthesis
if ET [j].mat = ET [j].tgt.last then // j was the last edge of target, return

LE [j].succ = ET [j].tgt.ﬁrst

else // continue with next edge from target

LE [j].succ = ET [j].mat + 1

20 parallelListRanking(LE )
21 parfor j = 1 to |ET | do
A[LE [j].rankA] = 1
B[LE [j].rankB ] = LE [j].value

22

24 Dpos, Dedge = arrays of length |EG| and |EG| − |ET | + 2, respectively
25 parfor j = 1 to |ET | do

p = LE [j].rankA − LE [j].rankB
base = ref (ET [j].mat)
delta = p − base − 1
parfor k = base + 1 to base + C[ET [j].mat] do

23

26

27

28

29

30

31

Dpos[k] = k + delta
Dedge[k + delta] = k

32 parfor j = 1 to |EG| − |ET | + 2 do

33

34

35

mat = EG[Dedge[j]].mat
if j > Dpos[mat] then

B∗[j] = 1

36 createRankSelect(A), createBP(B ), createBP(B∗)

ﬁelds. For B, the algorithm copies the content of ﬁeld value at position
rankB, for all the elements in LE.
5. The algorithm now computes the position of each edge of G \ T in B∗.
That information is implicit in the ﬁelds rankA and rankB of LE (line 26),
once the list ranking of step 3 is carried out. For each edge e ∈ ET , the
algorithm computes the positions in B∗ of the edges of G\T that follow, in
counter-clockwise order, the mate edge of e (lines 27–31). The algorithm
uses two auxiliary arrays, Dpos and Dedge. Let edge EG[j] belong to G\ T .
Then Dpos[j] stores the position of the edge in B∗. The array Dedge is

18

the inverse of Dpos: Dedge[i] is the position of the i-th edge of B∗ in EG.
This step uses function ref (e), which maps the position e of an edge in
ET to its position in EG. This is naturally returned by the spanning tree
construction, which gives the identity in G of the edges selected for T .
6. In lines 32–35, the algorithm computes whether the edges stored in Dpos
are forward or backward edges. For each edge e in G \ T , it compares the
positions in B∗ of e and its mate. If the position of e is greater, then e is
a backward edge and, therefore, is represented with a 1.

7. Finally, in line 36 the structures to support operations rank, select, match,
and parent are constructed. For the bitvector A, the parallel algorithm
of Labeit et al. [39] (createRankSelect) is used. For B and B∗ the
parallel algorithm of Ferres et al. [40] for balanced parenthesis sequences
(createBP) is used.

We have omitted some implementation details for simplicity. For example,
the pseudocode uses parfor throughout, whereas the implementation uses the
threads in a more controlled manner. Line 29, in particular, is more eﬃciently
done in sequential form. We have also omitted some space optimizations, such
as the reuse of some ﬁelds instead of allocating new arrays.
Analysis. Step 1 initializes the arrays, which requires T1 = O(m) work and
T∞ = O(log m) span (due to the overhead of the implicit parfor). In step 2,
the algorithm traverses the edges of T , performing an independent computation
on each edge. Therefore, with the overhead of the parfor loop, we obtain T1 =
O(n) and T∞ = O(log n) time. Step 3 uses a parallel list ranking algorithm [38]
over n elements, which has complexities T1 = O(n) and T∞ = O(log n). Step 4
assigns the values to A and B independently for each entry, thus we have again
T1 = O(n) and T∞ = O(log n). In step 5, the algorithm traverses all the edges
in G \ T . Since the loop in line 29 is also processed in parallel, we obtain T1 =
O(m − n) and T∞ = O(log(m − n)). Similarly to step 4, in step 6 the algorithm
sets the entries of bitvector B∗, which can be done independently for each entry,
obtaining times T1 = O(m − n) and T∞ = O(log(m − n)). Finally, step 7
builds the rank/select structures in times T1 = O(m) and T∞ = O(log m) [39].
The construction of the structures supporting match and parent over balanced
parentheses is constructed in times T1 = O(m) and T∞ = O(log m) [40].
In addition to the size of the compact data structure, our algorithm uses
O(m log m) bits for the arrays LE, Dpos and Dedge. As said, the constant is
kept low in practice by reusing ﬁelds. Notice that the memory consumption is
independent of the number of threads.

6.2. Structures for degree and neighbour queries

Before discussing how to construct the structures to speed up degree(v)
and neighbour(u, v) queries, let us discuss the parallel construction of the sparse
bitvector of Raman et al. [28]. Let (cid:96) be the length of the sparse bitvector. Their
representation divides the bitvector into blocks of length b = (log (cid:96))/2. The ith
block is described as a pair (ci, oi), where ci corresponds to the number of 1s

19

inside the block, also known as the class of the block, and oi corresponds to its
oﬀset, an identiﬁer among all the diﬀerent blocks sharing the same class. Thus,
the bitvector is represented as two arrays, C[1..(cid:100)(cid:96)/b(cid:101)] and O[1..(cid:100)(cid:96)/b(cid:101)], where
C[i] = ci and O[i] = oi. We can compute in parallel each entry of the arrays
C and O independently, using linear time on each block [22, Sec. 4.1]. Thus,
we have O((cid:96)) work and O(log((cid:96)/b) + b) = O(log (cid:96)) span. In order to reduce the
space consumption of the arrays C and O, the entries of the arrays are packed
into the bits of consecutive machine words. Notice that the size of the elements
of C is ﬁxed, (cid:100)log(b + 1)(cid:101) bits, whereas the size of those of O, (cid:100)log oi(cid:101) bits, is
variable. To pack the entries of O in parallel, we need to compute an array
P [1..(cid:100)(cid:96)/b(cid:101)] pointing to the starting position of each element in O. Array P is
computed with a parallel parallel preﬁx sum over the values (cid:100)log oi(cid:101). This takes
linear work and logarithmic span [39], and then we can write each value oi to its
packed position in parallel. The array P is retained to provide eﬃcient access
to O. To reduce its space to o((cid:96)) bits, only the entries of the form P [i· log n] are
stored in absolute form, whereas the others are stored as diﬀerences from the
preceding multiple of log n, using O(log log n) bits. This space reduction is easily
computed in parallel within the same time bounds. Once the data structures
C, O, and P , using (cid:96)H + o((cid:96)) bits, are built, we can access in constant time
any chunk of O(log (cid:96)) bits from the bitvector by using tables [28]. Therefore, we
can provide rank and select functionality by building the classical o((cid:96))-bit data
structures on top of the bitvector, in parallel [39]. In total, we use O((cid:96)) work
and O(log (cid:96)) span.

The structures to support degree(v) can then be constructed in parallel as
follows: First, we construct the bitvector D by checking all the vertices with
degree at least f (m). Remember that the degree of a vertex v can be computed
in constant time with v.last − v.ﬁrst. Since the degree of each vertex can be
obtained independently, we can do this in parallel with O(m) work and O(log m)
span. Then, we construct the bitvector E by writing in unary the degree of each
high-degree vertex. To do that, we perform a parallel preﬁx sum over all the
degrees of high-degree vertices. The preﬁx sum returns the positions where we
have to write a 1 in E. Thus, we construct E with O(m) work and O(log m)
span. Finally, we construct the compact representation of D and E in O(m)
work and O(log m) span, using the sparse bitvectors of Raman et al. [28].
For the neighbour(u, v) query, we must contract the original graph G into
a smaller graph G(cid:48) = (V (cid:48), E(cid:48)), induced by all the vertices with degree at least
f (m). To build G(cid:48) eﬃciently in parallel we do as follows. We ﬁrst compute
D(cid:48)[1..n] similarly to D. We then ﬁll two arrays X[1..n] and Y [1..2m], so that
X[i] = D(cid:48)[i]; and Y [j] = 1 if D(cid:48)[EG[j].src] = 1 and D(cid:48)[EG[j].tgt] = 1, and
Y [j] = 0 otherwise. Next, we perform a parallel preﬁx sum over X, so that X[i]
is the name of node i in G(cid:48) (if D(cid:48)[i] = 1). We also perform a parallel preﬁx
sum on Y , so as to write contiguously in array E(cid:48) the mapped edge targets,
k=1 Y [k].
For each such edge, we also check if it is the ﬁrst with this X[EG[j].src] value,
and if so, we record that j(cid:48) is the start of the adjacency list of node X[EG[j].src],
in an array V (cid:48)[X[EG[j].src]] = j(cid:48).

E(cid:48)[j(cid:48)] = X[EG[j].tgt] for those entries j where Y [j] = 1, where j(cid:48) =(cid:80)j

20

Thus V (cid:48) and E(cid:48) are an adjacency list representation of G(cid:48), built with O(m)
work and O(log m) span. Instead of sorting the adjacency lists, however, we
build a wavelet tree representation on E(cid:48) [39]. This supports the operation rank
generalized to sequences, and therefore we use that high-degree nodes u and
v of G are connected if and only if X[v] is mentioned in the adjacency list of
X[u], that is, E(cid:48).rankX[v](V (cid:48)[X[u] + 1]− 1)− E(cid:48).rankX[v](V (cid:48)[X[u]]− 1) > 0. The
generalized rank operation takes time O(log |V (cid:48)|) and the wavelet tree is built

with O(|E(cid:48)|) = o(m/ log m) work and O(cid:0)log2 |E(cid:48)|(cid:1) = O(cid:0)log2 m(cid:1) span.
in Section 5, in O(m) work and O(log m) span (O(cid:0)log2 m(cid:1) span if operation

Lemma 2. Given a connected planar graph embedding G with m edges and
a spanning tree of G, we can compute in parallel a compact representation of
G, using 4m + o(m) bits and supporting the navigational operations described
neighbour is supported), using O(m log m) bits of additional memory.

6.3. Parallel computation of spanning trees

In this section we discuss the parallel computation of the spanning tree

T = (VT , ET ) and the array C used in Section 6.1.

Generating a rooted (or a directed) spanning tree turns out to be a diﬃcult
to parallelize problem. Even if it seems to be easier on planar embeddings, we
do not know of good worst-case results on the DyM model. We discuss practical
solutions later.

Such a spanning tree algorithm returns an array of parent references for
each vertex. With this array of references, we can construct the corresponding
adjacency list representation of the spanning tree. To do that, we mark with a
1 each edge EG that belongs to ET and with a 0 the rest of the edges. Using a
parallel preﬁx sum algorithm over EG, we compute the position of all the marked
edges of EG in ET . The ﬁrst and last ﬁelds of each node in the spanning tree are
computed similarly. As a byproduct of the computation of ET , we can compute
the array C, which stores the number of edges of G\ T between two consecutive
edges in T , in counter-clockwise order. This can be done by using the marks in
the edges, counting the number of 0s between two consecutive 1s. Note that the
starting vertex for the spanning tree must be in the outer face of G, to meet the
description of the compact data structure for planar embeddings. Overall, we
require times T1 = O(m) and T∞ = O(log m) once the spanning tree is built,
which is the complexity of the variants of the parallel preﬁx sum algorithm we
employ. By combining the results with Lemma 2, we have the main result on
construction.

Theorem 2. The compact representation introduced in Theorem 1 of a connected 
planar graph embedding G with m edges can be constructed under the Dynamic 
Multithreaded parallel model with O(m + spw) work and O(log m + sps)

span (O(cid:0)log2 m + sps(cid:1) span if operation neighbour is supported), where spw and

sps are the work and span, respectively, of any rooted spanning tree algorithm
on planar embeddings.

21

In practice. The generation of a spanning tree is also diﬃcult to parallelize in
practice. Bader and Cong [41] mention that “the spanning tree problem is notoriously 
hard for any parallel implementation to achieve reasonable speedup”,
and propose an algorithm that is shown to perform well in practice. This is the
one we use in our implementation.

Their algorithm works as follows. Given a starting vertex of the graph G
with n vertices and m edges, the algorithm computes sequentially a spanning
tree of size O(p), called stub spanning tree, where p is the number of available
threads. Then, the leaves of the stub spanning tree are evenly assigned to the p
threads as starting vertices. Each thread traverses G, using its starting vertices,
constructing spanning trees with a DFS traversal using a stack. For each vertex,
a reference to its parent is assigned. Since a vertex can be visited by several
threads, the assigment of the parent of the vertex may genarate a race condition.
However, since the parent assigned by any thread already belongs to a spanning
tree, any assignment will generate a correct tree. Thus, the race condition is
benign. Once a thread has no more vertices on its stack, it tries to steal vertices
from the stack of another thread by using the work-stealing algorithm. Since the
spanning trees generated by all the threads are connected to the stub spanning
tree, the union of all the spanning tree generates a spanning tree of G.
They analyze their algorithm in expectation on random graphs, obtaining
O(m/p) time when p (cid:28) m, but general random graphs have a very small diameter.
 The diameter seems to be a lower bound for the span of their algorithm,
√
and this is Θ(n1/4) on random planar graphs [42]. Also, their best possible
time is O(
m processors. Despite its analysis,
 the algorithm of Bader and Cong has a good practical behavior and its
implementation is simple.
To handle unconnected planar graphs, we can ﬁrst use the algorithm of
Shun et al. [43], which ﬁnds the connected components within O(n) work and

O(cid:0)log3 n(cid:1) span with high probability, and is shown to perform well in practice.

m), achieved when using p =

√

PRAM model. We can also analyze our algorithm under the PRAM model.
Algorithm 1 is easily translated into the EREW model, reaching O(m/ log m)
processors and O(log m) time, dominated by the parallel list ranking of line
20, the expansion from n to m processors in line 29, and the construction of
succinct structures in line 36. The construction in Section 6.2, of the structures
that speed up degree and neighbour queries, is also easily carried out in the
EREW model within those bounds, except for the sorting of the edges of G(cid:48).
This can be done in O(log m) time with O(m) processors in the EREW model

[44], and in O(cid:0)log2 m(cid:1) time with O(m/ log m) processors in the CREW model
spanning tree. The best PRAM results we know of are O(cid:0)log2 m log
m(cid:1) time
and O(m) processors in the EREW model [46], O(cid:0)log2 m(cid:1) time and O(m/ log m)
processors in the arbitrary CRCW model [47], and O(log m) time and O(cid:0)m3(cid:1)

[45]. The postprocessing we have described in this section, once the spanning
tree is built, also runs in O(log m) time and O(m/ log m) EREW processors.

The most costly part of the process is likely to be the construction of the

∗

processors in the same model [48].

22

Theorem 3. The compact representation introduced in Theorem 1 of a conncected 
planar graph embedding G with m edges can be constructed under the
under the PRAM arbitrary CRCW model with O(m/ log m) processors and

PRAM EREW model with O(m) processors and O(cid:0)log2 m log
O(cid:0)log2 m(cid:1) time, or O(cid:0)m3(cid:1) processors and O(log m) time.

m(cid:1) time, and

∗

7. Experiments

We implemented the data structure construction and queries in C and compiled 
it using GCC 5.4. For the parallel construction we used Cilk Plus extension,
 an implementation of the DyM model. We build only the basic structures,
excluding those to speed up operations degree and neighbour. The code and
data needed to replicate our results are available at http://www.dcc.uchile.
cl/~jfuentess/pemb/.
The experiments were carried out on a NUMA machine with two NUMA
nodes. Each NUMA node includes a 14-core Intel R(cid:13) Xeon R(cid:13) CPU (E5-2695)
processor clocked at 2.3GHz. The machine runs Linux 4.4.0-83-generic, in 64bit 
mode. The machine has per-core L1 and L2 caches of sizes 64KB and 256KB,
respectively and a per-processor shared L3 cache of 35MB, with a 768GB DDR3
RAM memory (384GB per NUMA node), clocked at 1867MHz. Hyperthreading
was enabled, giving a total of 28 logical cores per NUMA node.

7.1. Datasets

Our experiments ran on real and artiﬁcial datasets with diﬀerent numbers
of nodes. The datasets are shown in Table 1. For the artiﬁcial datasets we
generated points (x, y) with the function rnorm of R.2 The real dataset, wc,
corresponds to the coordinates of 2, 243, 467 unique cities in the world.3 From
those real or generated points, we obtained a Delaunay Triangulation using
Triangle, a software for the generation of meshes and triangulations4. Finally,
we generated planar embeddings from the Delaunay triangulations with the
Edge Addition Planarity Suite5. The minimum and maximum degree of the
dataset wc was 3 and 36, respectively. For the rest of the datasets, the minimum
degree was 3 and the maximum degree was 16.

2The rnorm function generates random numbers with normal distribution given a mean
and a standard deviation. In our case, the x and y components were generated using mean 0
and standard deviation 10000. For more information about the rnorm function, visit https:
//stat.ethz.ch/R-manual/R-devel/library/stats/html/Normal.html

3The dataset containing the coordinates was created by MaxMind, available from https://
www.maxmind.com/en/free-world-cities-database. The original dataset contains 3, 173, 959
cities, but some of them have the same coordinates. We selected the 2, 243, 467 cities with
unique coordinates to build our dataset wc.

4Available at http://www.cs.cmu.edu/~quake/triangle.html. Our triangulations were

generated using the options -cezCBVPNE.

5Available at https://github.com/graph-algorithms/edge-addition-planarity-suite.

Our embeddings were generated using the options -s -q -p.

23

Dataset Vertices (n) Edges (m)

1 wc
2
3
4
5
6

pe5M
pe10M
pe15M
pe20M
pe25M

2,243,467
5,000,000
10,000,000
15,000,000
20,000,000
25,000,000

6,730,395
14,999,983
29,999,979
44,999,983
59,999,975
74,999,979

Table 1: Datasets used in our experiments.

7.2. Space usage

There are no other implemented compact representations of planar embeddings.

In this subsection we aim to show that representations designed for
other kinds of graphs are indeed not competitive for this graph family. We compare 
our compact representation with four solutions designed to compress Web
graphs, social networks and planar graphs [49, 50, 51, 52], and with one parallel 
framework for processing general graphs in compressed form [53]. The three
solutions for Web graphs and social networks require reordering the vertices
of the graph. The solution of Apostolico and Drovandi [49] (AD) enumerates
the vertices through a BFS traversal of the graph. The reordering induces two
useful properties: locality (a vertex with index i will have neighbours with indexes 
close to i), and similarity (vertices with similar index will have similar
adjacency lists). Thus, the vertices and their adjacency lists are compressed
following the ordering induced by the BFS traversal. The solution of Boldi
et al. [50] (BRSV) reorders the nodes based on a clustering algorithm called
Layered Label Propagation (LLP). The LLP algorithm is used in combination
with the WebGraph framework [54]. Brisaboa et al. [51] proposed the k2-tree
structure for graph compression. The k2-tree is a compact tree representation
of the adjacency matrix of a graph. The structure exploits the clustering of
the edges in the adjacency matrix, representing large empty areas of the matrix
eﬃciently. The clustering is dependent on the ordering of the vertices of the
graph. In our comparison, we used the k2-tree structure combined with the BFS
traversal of [49], as suggested by Hern´andez and Navarro [55]. Blandford et al.
[52] proposed a compact representation based on graph separators (GS). To
construct the compact representation, the vertices of the graph must be renumbered.
 The new numbering is computed recursively, decomposing the graph
by the computation of graph separators. The sequence of computed separators
generate the new numbering. After the renumbering step, adjacent vertices tend
to be close in the numbering. The representation takes advantage of that and
reorders the adjacency list of each vertex, storing the diﬀerence between consecutive 
neighbours. Finally, the adjacency lists are encoded space-eﬃciently. In
our experiments, we use the child-ﬂipping heuristic [52] to compute the numbering 
of the vertices and snip code to encode the adjacency lists, which was

24

Dataset Plain Ligra+ BRSV AD k2-tree GS

Pemb

wc
pe5M
pe10M
pe15M
pe20M
pe25M

74.67
74.67
74.67
74.67
74.67
74.67

52.50
52.99
53.15
53.20
53.24
53.32

14.57
14.97
15.03
15.04
15.07
15.11

14.73
14.14
14.33
14.38
14.43
14.50

16.40
15.33
14.73
14.38
14.15
13.96

14.88
15.12
15.12
15.12
15.12
15.14

6.00
5.93
5.93
5.72
5.93
5.80

Table 2: Bits per edge (bpe) of the plain representation, alternative compressed graph representations,
 and ours.

the best among the choices we tested. Shun et al.
[53] introduced Ligra+, a
lightweight graph processing framework for shared-memory multicore machines.
In Ligra+, the graph is stored in compressed form, by compressing the adjacency
list of each vertex. The adjacency list of each vertex is sorted in increasing order
and then the consecutive diﬀerences are run-length encoded. Finally, we also
consider a plain representation (Plain) composed by an array of length 2m,
representing the concatenation of the adjacency lists, and an array of length n,
representing the beginning of the adjacency list of each vertex.

Table 2 shows the bits per edge (bpe) of all the representations, where our
solution is called Pemb, for planar embedding. In the table, we consider four
bytes for each vertex and edge in the plain representation, equivalent to an integer 
number in common programming languages. Our compact representation
reaches the best results, using less than half of the space of its closest competitor.
 Note that the other results, using widely diﬀerent techniques, obtain very
close results, around 15 bpe. This seems to suggest that exploiting planarity
is the key to obtain a drastic reduction in space. Our results, with at most 6
bpe, are in accordance with the 4m + o(m) bits of Theorem 1.6 Notice that due
to the reordering needed by the other representations, they are not suitable for
representing a particular planar embedding.

7.3. Query times

We test the time to carry out the three basic queries introduced in Section 5:
degree, listing and face. Additionally, we test a more complex operation:
a depth-ﬁrst search traveral, dfs, starting from an arbitrary vertex and using
a stack. We solve degree by sequentially traversing the edges, as we have not
built the extra data structures to speed up this query. Observe that, given an
adjacency list representation, answering degree and listing queries is straightforward.
 We measured the time of queries degree and listing 10 times per
vertex, face 10 times per edge, and dfs 10 times for 30 random vertices. Table 3

6We can get closer to 4 bpe by sparsifying the sublinear-size structures used to query

bitvectors and parentheses, thus trading space for query time.

25

Dataset

Plain

Compact

degree listing

face

dfs degree listing face

dfs

0.01
wc
0.02
pe5M
pe10M 0.03
pe15M 0.03
pe20M 0.03
pe25M 0.03

0.12
0.14
0.14
0.15
0.15
0.15

0.35
0.51
0.60
0.62
0.64
0.64

0.51s
1.39s
2.65s
4.51s
5.66s
7.46s

4.04
4.24
4.41
4.51
4.60
4.64

20.01
20.65
21.24
21.61
21.77
22.15

8.28
8.55
8.82
8.98
9.17
9.40

14.87s
34.61s
70.05s
106.50s
142.20s
181.09s

lim25M 9.31ms 42.62ms 12.79ms

-

2.04µs

8.47µs 5.26µs 150.20s

Table 3: Median times of degree, listing and face queries, and the DFS traversal. All the
values are in microseconds (µs), except the dfs columns and the lim25M row, which explicitly
indicate µs, ms or s (seconds).

shows the median time per query, both for the plain representation and for our
compact representation. The plain representation answers degree and listing
queries 200 and 150 times faster than the compact representation, respectively.
This result was expected, since the plain representation we use already has
the list of neighbours in counter-clockwise order. For the face query and the
dfs traversal, the adjacency list representation is only 16 and 26 times faster,
respectively.

This slowdown is the price of a representation that uses about 13 times
less space, that is, it could hold graphs 13 times larger in main memory. To
illustrate the eﬀect of holding the compressed graph representation in main
memory versus having to handle it on disk, we replicate the experiments in a
machine with artiﬁcially limited memory. For these new experiments we use the
pe25M dataset, whose plain representation requires 668MB, whereas its compact
representation needs only 52MB. The machine was set to use at most 600MB
of RAM memory7, just slightly less than the necessary to hold the whole input
representation. The results are shown in the last row of Table 3. For degree
query, the compact representation is around 4,500 times faster than the plain
representation. For the listing query, the diﬀerence is around 5,000 times. For
the face query, the compact representation is around 2,400 times faster than
the plain representation. We aborted the experiment on dfs for the adjacency
list representation after two hours; a projection of the other results suggests
that more than a day would have been needed.

Thus, the compact representation pays oﬀ when it is the key to allow holding

7The computer tested is a Intel R(cid:13) CoreTM i7-7500U CPU, with four physical cores running
at 2.70GHz. The computer runs Linux 4.8.0-53-generic, in 64-bit mode. This machine has
per-core L1 and L2 caches of sizes 64KB and 256KB, respectively, and a shared L3 cache of
4MB, with a 8GB DDR4 RAM. To reduce the size of the available physical memory, we set
the mem parameter of the Linux Kernel to mem=600MB.

26

p

wc pe5M pe10M pe15M pe20M pe25M

seq 2.93
3.56
2.24
1.33
0.76
0.54
0.43
0.36
0.31
0.27
0.27
0.27
0.24
0.23
0.22
0.22
0.22

1
2
4
8
12
16
20
24
28
32
36
40
44
48
52
56

7.36
8.93
5.15
2.98
1.73
1.22
1.00
0.80
0.72
0.65
0.60
0.59
0.54
0.52
0.49
0.48
0.47

15.46
18.77
10.74
5.94
3.43
2.43
1.86
1.63
1.41
1.23
1.21
1.12
1.05
0.99
0.95
0.92
0.91

23.61
28.78
16.24
8.94
5.03
3.59
2.80
2.37
2.05
1.97
1.77
1.69
1.57
1.49
1.42
1.38
1.34

31.76
39.33
21.88
12.31
6.53
4.70
3.66
3.13
2.83
2.50
2.30
2.25
2.07
1.96
1.91
1.82
1.78

40.01
49.20
27.26
15.25
8.14
5.84
4.54
3.88
3.44
3.08
2.88
2.74
2.60
2.46
2.35
2.28
2.23

Table 4: Running times of the parallel construction algorithm in seconds.

the graph in main memory.

7.4. Parallel construction

We now evaluate the performance of our parallel construction. In our implementation 
of the parallel spanning tree algorithm of Bader and Cong [41],
to limit the worst case, we included a treshold of O(m/p) elements in the stack
size of each thread. Each time a thread has more nodes that the threshold, it
creates a new parallel thread with half of its stack. Additionally, we also return,
for each node, the reference to its parent. This yields better performance than
forcing the ﬁrst edge of each node to lead to its parent.

Additionally, we implemented a sequential algorithm called seq, which corresponds 
to a sequential DFS algorithm to build the spanning tree, followed
by the serialization of the parallel algorithm. To serialize a parallel algorithm
in the DyM model, we replaced each parfor keyword for the for keyword and
deleted the spawn and sync keywords. Each data point is the median of 15
measurements.

Table 4 shows the running times obtained in our experiments, and Figure 3
shows the speedups compared with the seq algorithm. On average, the seq
algorithm took about 82% of the time obtained by the parallel algorithm running
with 1 thread. With p ≥ 2, the parallel algorithm shows better times than
the seq algorithm. We observe an almost linear speedup up to p = 24, with
an eﬃciency of at least 40% for the smaller datasets and almost 50% for the
bigger ones. With p = 28 the speedup has a slowdown, due to the topology
of our machine. Up to 24 cores, all the threads were running in the same

27

Figure 3: Speedup of the parallel algorithm.

NUMA node. With p ≥ 28, both NUMA nodes are used, which implies higher
communication costs. The communication costs intra NUMA nodes are lower
than the communication costs inter NUMA nodes [56]. In particular, the case of
p = 28 also uses both NUMA nodes, since at least one core on our machine was
available to OS processes. For p = 56, the wc dataset exhibits an eﬃciency of
only 24%, as it is the smallest one. For the bigger datasets, the lowest eﬃciency
is 32%.

The running times and speedups reported in Table 4 and Figure 3 include the
construction of bitvectors and balanced parentheses sequences, to support rank,
select, parent, and match operations. To measure the eﬃciency of our algorithm,
without the inﬂuence of the construction of those additional data structures,
we repeated all the construction experiments, excluding the additional data
structures. In the new experiments, we observed that the speedup increases on
average 2.7% for p ≤ 24 and 3.2% for p ≥ 28, reaching a maximum speedup of
18.8, compared to the values reported in Figure 3.

Table 5 shows the running time for diﬀerent edge densities of the dataset
pe25M, and Figure 4 shows the corresponding speedups compared with the
algorithm seq. The diﬀerent densities are generated by deleting x million edges
from the dataset pe25M, with x ∈ {5, 10, 15, 20, 25, 30}. If several components
are generated, we reconnect them by restoring one edge between two components
and then choosing new edges to be deleted. Thus, we report results for 45 to
75 (45Me to 75Me) million edges. The dataset 75Me corresponds to the original
dataset pe25M. We observe a decrease in the running time for all values of p,
according to the decrease in the number of edges. With respect to 75Me, the
rest of the datasets show a greater decrease in the running time for increasing
values of p, reaching speedups of up to 19.5 for 45Me. In the case of datasets
with the same number of edges (see columns pe15M and pe20M in Table 4, and
columns 45Me and 60Me in Table 5), the datasets with higher number of vertices

28

Number of threadsSpeedup148121620242832364044485256024681012141618llwcpe5Mpe10Mpe15Mpe20Mpe25Mllllllllllllllllllllllllllllllllp

45Me

50Me

55Me

60Me

65Me

70Me

75Me

seq 33.20 34.63 36.16 37.17 38.05 38.99 40.01
38.35 40.74 43.06 44.87 46.38 47.85 49.20
21.42 22.86 24.30 25.17 26.35 26.85 27.26
12.10 12.96 13.50 14.31 14.50 15.16 15.25
8.14
5.84
4.54
3.88
3.44
3.08
2.88
2.74
2.60
2.46
2.35
2.28
2.23

6.45
4.62
3.59
3.05
2.71
2.45
2.30
2.19
2.04
1.94
1.83
1.77
1.71

6.89
4.91
3.85
3.28
2.86
2.62
2.49
2.35
2.16
2.04
1.93
1.86
1.82

7.88
5.69
4.45
3.80
3.30
2.94
2.81
2.64
2.48
2.34
2.24
2.16
2.14

1
2
4
8
12
16
20
24
28
32
36
40
44
48
52
56

7.23
5.12
4.05
3.41
3.01
2.68
2.57
2.38
2.25
2.13
2.04
1.95
1.90

7.51
5.46
4.20
3.59
3.08
2.77
2.69
2.50
2.33
2.21
2.12
2.05
2.00

7.76
5.64
4.28
3.66
3.23
2.90
2.75
2.58
2.43
2.28
2.18
2.10
2.06

Table 5: Running times of the parallel construction algorithm varying the edge density for
the dataset pe25M. The running times are measured in seconds.

show higher running times. Comparing Figures 3 and 4, we observe that our
algorithm scales similarly for triangulated and non-triangulated graphs.

Figure 5 shows the memory consumption of our algorithm. Speciﬁcally, the
ﬁgure shows for each dataset the space used by its adjacency list representation
(inputGraph), the peak consumption of our construction (peakMem) in addition
to the input and the output, the space of its plain representation (plainGraph),
and the size of its compact representation (compGraph). The plain representation,
 consisting of an array of edges of length 2m and an array of vertices of
length n, is enough to navigate a graph, but for the construction we need more
information about the embedding of the input graph. This richer adjacency list
representation is what we call inputGraph. To measure the peak consumption,
we use malloc count8, which monitors the memory allocated and released with
malloc and free, respectively, and reports the peak usage. The observed peak
consumption equals the size of the arrays LE, Dpos and Dedge. Compared with
the space consumption of the input adjacency list representation, our implementation 
uses 73% of extra space. The ﬁnal compact representation uses about
8% of the plain representation, as we have seen.

8Timo Bingmann. Malloc count - Tools for runtime memory usage analysis and proﬁling.

URL:https://panthema.net/2013/malloc_count/. Last accessed: August 08, 2017.

29

Figure 4: Speedup of the parallel algorithm varying the edge density for the dataset pe25M.

Figure 5: Memory consumption of the parallel algorithm and the ﬁnal compact structure.

8. Conclusions and future work

Tur´an’s representation of planar embeddings [4] is much simpler than the
known alternatives and encodes any planar embedding of m edges in just 4m
bits, close to the lower bound of 3.58m bits.
In this paper we have shown
how to add o(m) bits to this encoding in order to support fast nagivation and
queries of the graph, in constant time for the most fundamental operations.
While there are asymptotically optimal representations [3], the simplicity of
Tur´an’s encoding enabled us to introduce the ﬁrst actual implementation of
such a compact data structure, where the basic navigation operations are solved
within microseconds. Further, the structure can be built at a rate of about
one microsecond per edge, and the construction can be parallelized with linear
speedup and an eﬃciency near 50%. Our parallel construction algorithm has
linear work and logarithmic span on the dynamic multithreaded model once a
spanning tree of the embedding is computed.

30

Number of threadsSpeedup14812162024283236404448525602468101214161820ll45Me50Me55Me60Me65Me70Me75MellllllllllllllllllllllllllllllllinputGraphpeakMemplainGraphcompGraphDatasetsMemory consumption(MB)wcpe5Mpe10Mpe15Mpe20Mpe25M030060090012001600One intriguing question is about the queries we do not support in constant
time. Some previous representations [7, 9, 3] can compute the degree of a
node in O(1) time, whereas we can handle any superconstant time. Similarly,
they can answer neighbour queries in O(1) time, whereas our structure needs
superlogarithmic time. The representation closest to ours [9] uses the same
technique of two types of parentheses, but the arrangement of the parentheses
follows a so-called orderly spanning tree. While much more complex to build
and unable to represent some embeddings, such spanning tree induces a certain
regularity on the representation of the edges leaving each node, which allows
determining in constant time the number of such edges, and whether two nodes
are connected.
It is an interesting question whether we can ﬁnd a simpler
arrangement that retains those properties.

Another future research line is how to make our data structure dynamic.
We can use a scheme inspired by Munro et al. [57]. Suppose we store our static
data structure and a dynamic buﬀer that contains information about edges that
have been added or deleted.
If we want to know if an edge is present, we
check our static data structure and then check the buﬀer to see if its status
has changed. Once the buﬀer becomes too large — e.g., more than m/ log m
bits — we rebuild our static structure. Even when updates arrive sequentially,
there are some issues to consider, such as how to quickly report the neighbours
of a node that originally had many edges but has had most of them deleted
(perhaps by moving all the information about a node into the buﬀer when half
its incident edges have been updated) and how to detect if the graph has become
non-planar. There are more issues when the updates can be made in parallel,
since then we may need locks for nodes and ﬁnding a practical design becomes
challenging.

Finally, we believe we can generalize our data structure to store eﬃciently
graphs that are almost planar, using for example generalizations of the technique
of Fischer and Peters [58] to store graphs that are almost trees. Of course, it is
NP-hard to ﬁnd the maximum planar subgraph of an arbitrary graph [59], but
there have been recent advances in approximating it and in practice bridges and
tunnels, for example, might already be identiﬁed anyway.

Acknowledgments

The ﬁrst author received funding from CORFO 13CEE2-21592 (2013-21592-
1-INNOVA PRODUCCION2013-21592-1). The second author received funding
from Conicyt Fondecyt grant 3170534. The second, third and ﬁfth authors
received travel funding from EU grant H2020-MSCA-RISE-2015 BIRDS GA
No. 690941, and funding from Basal Funds FB0001, Conicyt, Chile. The third
author received funding from Academy of Finland grant 268324. The fourth
author received funding from NSERC of Canada. The ﬁfth author received
funding from Millennium Nucleus Information and Coordination in Networks,
ICM/FIC RC130003. Early parts of this work were done while the third author
was at the University of Helsinki and while the third and ﬁfth authors were
visiting the University of A Coru˜na.

31

Many thanks to J´er´emy Barbay, Luca Castelli Aleardi, Guojing Cong, Arash
Farzan, Cecilia Hern´andez, Ian Munro, Pat Nicholson, Romeo Rizzi and Julian
Shun for fruitful discussions. We thank Susana Ladra and Guy Blelloch for
sharing their k2-tree and graph separators code with us. We also thank Telefonica 
I+D, in particular, Pablo Garc´ıa, for sharing their computing equipment
with us. The third author is grateful to the late David Gregory for his course
on graph theory.

References

References

[1] L. Ferres, J. Fuentes, T. Gagie, M. He, G. Navarro, Fast and compact
planar embeddings, in: Proceedings of the 15th International Symposium,
Algorithms and Data Structures (WADS), Springer International Publishing,
 2017, pp. 385–396.

[2] W. T. Tutte, A census of planar maps, Canadian Journal of Mathematics

15 (1963) 249–271.

[3] G. E. Blelloch, A. Farzan, Succinct representations of separable graphs,
in: Proceedings of the 21st Annual Conference on Combinatorial Pattern
Matching (CPM), Springer-Verlag, 2010, pp. 138–150.

[4] G. Tur´an, On the succinct representation of graphs, Discrete Applied Mathematics 
8 (3) (1984) 289 – 294.

[5] G. Jacobson, Space-eﬃcient static trees and graphs, in: Proceedings of the
30th Annual Symposium on Foundations of Computer Science (FOCS),
IEEE Computer Society, 1989, pp. 549–554.

[6] M. Yannakakis, Embedding planar graphs in four pages, Journal of Computer 
and System Sciences 38 (1) (1989) 36–67.

[7] J. I. Munro, V. Raman, Succinct representation of balanced parentheses

and static trees, SIAM Journal on Computing 31 (3) (2001) 762–776.

[8] K. Keeler, J. Westbrook, Short encodings of planar graphs and maps, Discrete 
Applied Mathematics 58 (1995) 239–252.

[9] Y.-T. Chiang, C.-C. Lin, H.-I. Lu, Orderly spanning trees with applications,

SIAM Journal on Computing 34 (2005) 924–945.

[10] R. C.-N. Chuang, A. Garg, X. He, M.-Y. Kao, H.-I. Lu, Compact encodings 
of planar graphs via canonical orderings and multiple parentheses, in:
Proceedings of the 25th International Colloquium on Automata, Languages
and Programming (ICALP), LNCS 1443, 1998, pp. 118–129.

[11] J. Barbay, L. C. Aleardi, M. He, J. I. Munro, Succinct representation of

labeled graphs, Algorithmica 62 (2012) 224–257.

32

[12] W. Schnyder, Embedding planar graphs on the grid, in: Proceedings of
the 1st Annual ACM-SIAM Symposium on Discrete Algorithms (SODA),
Society for Industrial and Applied Mathematics, 1990, pp. 138–148.

[13] D. K. Blandford, G. E. Blelloch, I. A. Kash, Compact representations of
separable graphs, in: Proceedings of the 14th Annual ACM-SIAM Symposium 
on Discrete Algorithms (SODA), Society for Industrial and Applied
Mathematics, 2003, pp. 679–688.

[14] R. J. Lipton, R. E. Tarjan, A separator theorem for planar graphs, SIAM

Journal on Applied Mathematics 36 (1979) 177–189.

[15] N. Bonichon, C. Gavoille, N. Hanusse, D. Poulalhon, G. Schaeﬀer, Planar
graphs, via well-orderly maps and trees, Graphs and Combinatorics 22 (2)
(2006) 185–202.

[16] X. He, M. Y. Kao, H.-I. Lu, A fast general methodology for informationtheoretically 
optimal encodings of graphs, SIAM Journal on Computing 30
(2000) 838–846.

[17] L. C. Aleardi, O. Devillers, G. Schaeﬀer, Succinct representation of triangulations 
with a boundary, in: Proceedings of the 9th International Conference 
on Algorithms and Data Structures (WADS), Springer-Verlag, 2005,
pp. 134–145.

[18] L. Castelli Aleardi, O. Devillers, G. Schaeﬀer, Succinct representations of

planar maps, Theoretical Computer Science 408 (2-3) (2008) 174–187.

[19] E. Fusy, G. Schaeﬀer, D. Poulalhon, Dissections, orientations, and trees
with applications to optimal mesh encoding and random sampling, ACM
Transactions on Algorithms 4 (2) (2008) 19:1–19:48.

[20] K. Yamanaka, S.-I. Nakano, A compact encoding of plane triangulations
with eﬃcient query supports, Information Processing Letters 110 (18-19)
(2010) 803–809.

[21] J. I. Munro, P. K. Nicholson, Compressed representations of graphs, in:

Encyclopedia of Algorithms, Springer, 2016, pp. 382–386.

[22] G. Navarro, Compact Data Structures: A Practical Approach, Cambridge

University Press, 2016.

[23] X. He, M.-Y. Kao, Parallel construction of canonical ordering and convex
drawing of triconnected planar graphs, in: Proceedings of the 4th International 
Symposium on Algorithms and Computation (ISAAC), 1993, pp.
303–312.

[24] M. Kao, S. Teng, K. Toyama, An optimal parallel algorithm for planar

cycle separators, Algorithmica 14 (1995) 398–408.

33

[25] M. Kao, M. F¨urer, X. He, B. Raghavachari, Optimal parallel algorithms for
straight-line grid embeddings of planar graphs, SIAM Journal on Discrete
Mathematics 7 (4) (1994) 632–646.

[26] D. R. Clark, Compact PAT trees, Ph.D. thesis, University of Waterloo,

Canada (1996).

[27] J. I. Munro, Tables, in: Proceedings of the 16th Conference on Foundations
of Software Technology and Theoretical Computer Science (FSTTCS),
LNCS 1180, 1996, pp. 37–42.

[28] R. Raman, V. Raman, S. R. Satti, Succinct indexable dictionaries with applications 
to encoding k-ary trees, preﬁx sums and multisets, ACM Transactions 
on Algorithms 3 (4).

[29] R. F. Geary, N. Rahman, R. Raman, V. Raman, A simple optimal representation 
for balanced parentheses, Theoretical Computer Science 368 (3)
(2006) 231–246.

[30] G. Navarro, K. Sadakane, Fully functional static and dynamic succinct

trees, ACM Trans. Algorithms 10 (3) (2014) 16:1–16:39.

[31] T. H. Cormen, C. E. Leiserson, R. L. Rivest, C. Stein, Multithreaded algorithms,
 in: Introduction to Algorithms, 3rd Edition, The MIT Press, 2009,
pp. 772–812.

[32] R. D. Blumofe, C. E. Leiserson, Scheduling multithreaded computations

by work stealing, Journal of the ACM 46 (5) (1999) 720–748.

[33] N. Biggs, Spanning trees of dual graphs, Journal of Combinatorial Theory,

Series B 11 (2) (1971) 127–131.

[34] D. Eppstein, Dynamic generators of topologically embedded graphs, in:
Proceedings of the 14th Annual ACM-SIAM Symposium on Discrete Algorithms 
(SODA), Society for Industrial and Applied Mathematics, 2003,
pp. 599–608.

[35] T. R. Riley, W. P. Thurston, The absence of eﬃcient dual pairs of spanning

trees in planar graphs, Electronic Journal of Combinatorics 13 (1).

[36] P. Ferragina, R. Venturini, A simple storage scheme for strings achieving

entropy bounds, Theoretical Computer Science 371 (1) (2007) 115–121.

[37] D. Okanohara, K. Sadakane, Practical entropy-compressed rank/select dictionary,
 in: Proceedings of the 9th Workshop on Algorithm Engineering
and Experiments (ALENEX), 2007, pp. 60–70.

[38] D. R. Helman, J. J´aJ´a, Preﬁx computations on symmetric multiprocessors,

Journal of Parallel and Distributed Computing 61 (2001) 265–278.

34

[39] J. Labeit, J. Shun, G. E. Blelloch, Parallel lightweight wavelet tree, suﬃx
array and fm-index construction, Journal of Discrete Algorithms 43 (2017)
2–17.

[40] J. Fuentes-Sep´ulveda, L. Ferres, M. He, N. Zeh, Parallel construction of

succinct trees, Theoretical Computer Science. To appear.

[41] D. A. Bader, G. Cong, A fast, parallel spanning tree algorithm for symmetric 
multiprocessors (SMPs), Journal of Parallel and Distributed Computing
65 (2005) 994–1006.

[42] G. Chapuy, E. Fusy, O. Gim´enez, M. Noy, On the diameter of random
planar graphs, Combinatorics, Probability & Computing 24 (1) (2015) 145–
178.

[43] J. Shun, L. Dhulipala, G. Blelloch, A simple and practical linear-work
parallel algorithm for connectivity, in: Proceedings of the 26th ACM Symposium 
on Parallelism in Algorithms and Architectures (SPAA), 2014, pp.
143–153.

[44] R. Cole, Parallel merge sort, SIAM Journal on Computing 17 (4) (1988)

770–785.

[45] G. Bilardi, A. Nicolau, Adaptive bitonic sorting: An optimal parallel algorithm 
for shared-memory machines, SIAM Journal on Computing 18 (2)
(1989) 216–228.

[46] G. E. Shannon, A linear-processor algorithm for depth-ﬁrst search in planar

graphs, Information Processing Letters 29 (3) (1988) 119–123.

[47] M. Kao, S. Teng, K. Toyama, An optimal parallel algorithm for planar

cycle separators, Algorithmica 14 (5) (1995) 398–408.

[48] T. Hagerup, Planar depth-ﬁrst search in O(log n) parallel time, SIAM Journal 
on Computing 19 (4) (1990) 678–704.

[49] A. Apostolico, G. Drovandi, Graph compression by BFS, Algorithms 2 (3)

(2009) 1031–1044.

[50] P. Boldi, M. Rosa, M. Santini, S. Vigna, Layered label propagation: A
multiresolution coordinate-free ordering for compressing social networks,
in: Proceedings of the 20th International Conference on World Wide Web
(WWW), ACM, 2011, pp. 587–596.

[51] N. Brisaboa, S. Ladra, G. Navarro, Compact representation of web graphs

with extended functionality, Information Systems 39 (1) (2014) 152–174.

[52] D. K. Blandford, G. E. Blelloch, I. A. Kash, Compact representations of
separable graphs, in: Proceedings of the 14th Annual ACM-SIAM Symposium 
on Discrete Algorithms (SODA), 2003, pp. 679–688.

35

[53] J. Shun, L. Dhulipala, G. E. Blelloch, Smaller and faster: Parallel processing 
of compressed graphs with Ligra+, in: Proceedings of the 25th Data
Compression Conference (DCC), 2015, pp. 403–412.

[54] P. Boldi, S. Vigna, The webgraph framework I: Compression techniques,
in: Proceedings of the 13th International Conference on World Wide Web
(WWW), ACM, 2004, pp. 595–602.

[55] C. Hern´andez, G. Navarro, Compressed representations for web and social

graphs, Knowledge and Information Systems 40 (2) (2014) 279–313.

[56] U. Drepper, What every programmer should know about memory (2007).

URL http://people.redhat.com/drepper/cpumemory.pdf

[57] J. I. Munro, Y. Nekrich, J. S. Vitter, Dynamic data structures for document
collections and graphs, in: Proceedings of the 34th ACM Symposium on
Principles of Database Systems (PODS), 2015, pp. 277–289.

[58] J. Fischer, D. Peters, GLOUDS: Representing tree-like graphs, Journal of

Discrete Algorithms 36 (2016) 39 – 49.

[59] M. Yannakakis, The eect of a connectivity requirement on the complexity
of maximum subgraph problems, Journal of the ACM 26 (1979) 618–630.

36

