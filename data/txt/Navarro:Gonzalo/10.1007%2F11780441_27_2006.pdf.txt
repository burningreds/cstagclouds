Statistical Encoding of Succinct Data Structures

Rodrigo Gonz´alez(cid:2) and Gonzalo Navarro(cid:2)(cid:2)

Department of Computer Science, University of Chile

{rgonzale, gnavarro}@dcc.uchile.cl

Abstract. In recent work, Sadakane and Grossi [SODA 2006] introduced 
a scheme to represent any sequence S = s1s2 . . . sn, over an alphabet 
of size σ, using nHk(S) + O( n
logσ n (k log σ + log log n)) bits of space,
where Hk(S) is the k-th order empirical entropy of S. The representation
permits extracting any substring of size Θ(logσ n) in constant time, and
thus it completely replaces S under the RAM model. This is extremely
important because it permits converting any succinct data structure requiring 
o(|S|) = o(n log σ) bits in addition to S, into another requiring
nHk(S) + o(n log σ) (overall) for any k = o(logσ n). They achieve this
result by using Ziv-Lempel compression, and conjecture that the result
can in particular be useful to implement compressed full-text indexes.

In this paper we extend their result, by obtaining the same space and
time complexities using a simpler scheme based on statistical encoding.
We show that the scheme supports appending symbols in constant amortized 
time. In addition, we prove some results on the applicability of the
scheme for full-text self-indexing.

1 Introduction

Recent years have witnessed an increasing interest on succinct data structures,
motivated mainly by the growth over time on the size of textual information.
This has triggered a search for less space-demanding data structures bounded
by the entropy of the original text. Their aim is to represent the data using as
little space as possible, yet eﬃciently answering queries on the represented data.
Several results exist on the representation of sequences [11, 16], trees [13, 3, 4],
graphs [13], permutations and functions [12, 14], texts [5, 7, 6, 9], etc.
Several of those succinct data structures are built over a sequence of symbols
S[1, n] = s1s2 . . . sn, from an alphabet A of size σ, and require only o(|S|) =
o(n log σ) additional bits in addition to S itself (S requires n log σ bits1). A
more ambitious goal is a compressed data structure, which takes overall space
proportional to the compressed size of S and still is able to recover any substring
of S and manipulate the data structure.

A very recent result by Sadakane and Grossi [18] gives a tool to convert any
succinct data structure on sequences into a compressed data structure. More

(cid:3) Work supported by Mecesup Grant UCH 0109, Chile.
(cid:3)(cid:3) Work supported by a grant from Yahoo! Research Latin America.
1 In this paper log stands for log2.

M. Lewenstein and G. Valiente (Eds.): CPM 2006, LNCS 4009, pp. 294–305, 2006.
c(cid:2) Springer-Verlag Berlin Heidelberg 2006

Statistical Encoding of Succinct Data Structures

295

precisely, they show that S can be encoded using nHk(S) + O( n
logσ n(k log σ +
log log n)) bits of space2, where Hk(S) is the k-th order empirical entropy of
S [10]. (Hk(S) is a lower bound to the space achieved by any statistical compressor 
based on k-th order modeling.) Their structure permits retrieving any
substring of S of Θ(logσ n) symbols in constant time. Under the RAM model of
computation this is equivalent to having S in explicit form.

In particular, for suﬃciently small k = o(logσ n), the space is Hk(S) +
o(n log σ). Any succinct data structure that requires o(n log σ) bits in addition
to S can thus be replaced by a compressed data structure requiring nHk(S) +
o(n log σ) bits overall, where any access to S is replaced by an access to the novel
structure. Their scheme is based on Ziv-Lempel encoding.

In this paper we show how the same result can be achieved by much simpler
means. We present an alternative scheme based on semi-static k-th order modeling 
plus statistical encoding, just as a normal semi-static statistical compressor
would process S. By adding some extra structures, we are able of retrieving any
substring of S of Θ(logσ n) symbols in constant time. Although any statistical
encoder works, we obtain the best results (matching exactly those of [18]) using
Arithmetic encoding [1]. Furthermore, we show that we can append symbols to S
without changing the space complexity, in constant amortized time per symbol.
In addition, we study the applicability of this technique to full-text
self-indexes. Compressed self-indexes replace a text T [1, n] by a structure requiring 
O(nH0(T )) or O(nHk(T )) bits of space. In order to provide eﬃcient
pattern matching over T , many of those structures [5, 15, 6] achieve space proportional 
to nHk(T ) by ﬁrst applying the Burrows-Wheeler Transform [2] over
T , S[1, n] = bwt(T ), and then struggling to represent S in eﬃcient form. An
additional structure of o(|S|) bits gives the necessary functionality to implement
the search. One could thus apply the new structure over S, so that the overall
structure requires nHk(S) + o(|S|) bits. Yet, the relation between Hk(S) and
Hk(T ) remains unknown. In this paper we move a step forward by proving a
positive result: H1(S) ≤ Hk(T ) log σ + o(1) for small k = o(logσ n). Thus we can,
for example, achieve essentially the same result of the Run-Length FM-Index [9]
just by using the new structure on S, without the involved techniques they use.
Several indexes, however, compress S = bwt(T ) by means of a wavelet tree
[7] on S, wt(S). This is a balanced tree storing several binary sequences. Each
such sequence B can be represented using |B|H0(B) bits of space. If we call
nH0(wt(S)) the overall resulting space, it turns out that nH0(wt(S)) = nH0(S).
A natural idea advocated in [18] is to use a k-th order representation for the
binary sequences B, yielding space nHk(wt(S)). Thus the question about the
relationship between Hk(wt(S)) and Hk(S) is raised. In this paper we exhibit
examples where either is larger than the other. In particular, we show that when
moving from wt(S) to S, the k-th order entropy grows at least by a factor of
Θ(log k).

2 The term k log σ appears as k in [18], but this is a mistake [17]. The reason is that
they take from [8] an extra space of the form Θ(kt+t) as stated in Lema 2.3, whereas
the proof in Theorem A.4 gives a term of the form kt log σ + Θ(t).

296

R. Gonz´alez and G. Navarro

2 Background and Notation

Hereafter we assume that S[1, n] = S1,n = s1s2 . . . sn is the sequence we wish
to encode and query. The symbols of S are drawn from an alphabet A =
{a1, . . . , aσ} of size σ. We write |w| to denote the length of sequence w.

Let B[1, n] be a binary sequence. Function rankb(B, i) returns the number of
times b appears in the preﬁx B[1, i]. Function selectb(B, i) returns the position
of the i-th appearance of b within sequence B. Both rank and select can be
computed in constant time using o(n) bits of space in addition to B [11].

2.1 The k-th Order Empirical Entropy
The empirical entropy resembles the entropy deﬁned in the probabilistic setting
(for example, when the input comes from a Markov source). However, the empirical 
entropy is deﬁned for any string and can be used to measure the performance
of compression algorithms without any assumption on the input [10].

The empirical entropy of k-th order is deﬁned using that of zero-order. This

is deﬁned as

H0(S) = − (cid:2)
a∈A

na
S
n

log2( na
S
n

)

(1)

with na
S the number of occurrences of symbol a in sequence S. This deﬁnition
extends to k > 0 as follows. Let Ak be the set of all sequences of length k
over A. For any string w ∈ Ak, called a context of size k, let wS be the string
consisting of the concatenation of characters following w in S. Then, the k-th
order empirical entropy of S is

Hk(S) =

1
n

|wS|H0 (wS) .

(2)

(cid:2)

w∈Ak

The k-th order empirical entropy captures the dependence of symbols upon
their context. For k ≥ 0, nHk(S) provides a lower bound to the output of
any compressor that considers a context of size k to encode every symbol of
S. Note that the uncompressed representation of S takes n log σ bits, and that
0 ≤ Hk(S) ≤ Hk−1(S) ≤ . . . ≤ H1(S) ≤ H0(S) ≤ log σ.
Note that a semi-static k-th order modeler that yields the probabilities p1, p2,
. . . , pn for the symbols s1, s2, . . . , sn, will actually determine pi ≈ P (si|si−k . . .
si−1) using the formula pi = nsi
wS|wS| , where w = si−k . . . si−1. It is not hard to see,
by grouping all the terms with the same w in the summation [10, 7], that

− n(cid:2)

pi log pi = nHk(S).

(3)

i=k+1

2.2 Statistical Encoding
We are interested in the use of semi-static statistical encoders in this paper.
Thus, we are given a k-th order modeler as described above, which will yield
the probabilities p1, p2, . . . , pn for each symbol in S, and we will encode the

Statistical Encoding of Succinct Data Structures

297
successive symbols of S trying to use −pi log pi bits for si. If we reach exactly
−pi log pi bits, the overall number of bits produced will be nHk(S) + O(k log n),
according to Eq. (3).
Diﬀerent encoders provide diﬀerent approximations to the ideal −pi log pi bits.
The simplest encoder is probably Huﬀman coding [1], while the best one, from
the point of view of the number of bits generated, is Arithmetic coding [1].

Given a statistical encoder E and a semi-static modeler over sequence S[1, n]
yielding probabilities p1, p2, . . . , pn, we call E(S) the bitwise output of E for
those probabilities, and |E(S)| its bit length. We call fk(E, S) = |E(S)| −
(− (cid:3)
1≤i≤n pi log pi) the extra space in bits needed to encode S using E, on
top of the entropy of the model. For example, the wasted space of Huﬀman encoding 
is bounded by 1 bit per symbol, and thus fk(Huﬀman, S) < |S| (tighter
bounds exist but are not useful for this paper [1]). On the other hand, Arithmetic
encoding approaches −pi log pi as closely as desired, requiring only at most two
extra bits to terminate the whole sequence [1, Section 5.2.6 and 5.4.1]. Thus
fk(Arithmetic, S) ≤ 2. Again, we can relate the model entropy of p1, p2, . . . , pn
with the empirical entropy of S using Eq. (3), achieving that, say, Arithmetic
coding encodes S using at most nHk(S) + O(k log n) + 2 bits.
Arithmetic coding essentially expresses S using a number in [0, 1) which lies
within a range of size P = p1 · p2 ··· pn. We need − log P = − (cid:3)
log pi bits
to distinguish a number within that range (plus two extra bits for technical
reasons). Thus each new symbol si, which appears within its context npi times,
requires − log pi bits to be encoded. This totalizes −n

pi log pi + 2 bits.

(cid:3)

There are usually some limitations to the near-optimality achieved by Arithmetic 
coding in practice [1]. One is that many bits are required to manipulate
P , which can be cumbersome. This is mainly alleviated by emitting the most
signiﬁcant bits of the ﬁnal number as soon as they are known, and thus scaling
the remainder of the number again to the range [0, 1) (that is, dropping the
emitted bits from our number). Still, some symbols with very low probability
may require many bits. To simplify matters, ﬁxed precision arithmetic is used
to approximate the real values, and this introduces a very small (yet linear) ineﬃciency 
in the coding. In our case, we never run into this problem because, as
seen later, we do not encode any sequence that requires more than log n
2 bits. As
soon as those bits are not precise enough to represent the encoding, we switch
to plain symbol-wise encoding.

Another limitation applies to adaptive encoding, where some kind of aging
technique is used to let the model forget symbols that have appeared many
positions away in the sequence. In our case this does not apply, as we use semistatic 
encoding. Finally, we notice that we run into no eﬃciency problems at all
at decoding time, as we will use the log n
2 -bit compressed stream as an index to
a precomputed table that will directly yield the uncompressed symbols.

2.3 Implementing Succinct Full-Text Self-indexes

A succinct full-text index provides fast search functionality using a space proportional 
to that of the text itself. A less space-demanding index, in particular,

298

R. Gonz´alez and G. Navarro

using space proportional to that of the compressed text is known as a compressed
full-text index.Those indexes that contain suﬃcient the information to recreate
the original text are known as self-indexes. An example of the latter is the FMindex 
family [5, 6, 9] based on the Burrows-Wheeler Transform (BWT) [2]. The
BWT of a text T , T bwt = bwt(T ), is a reversible transformation from strings
to strings. For this paper, it is enough to say that T bwt is a permutation of the
characters of T which is easier to compress by local optimization methods [10].
Full-text indexes need essentially to perform symbol rank queries over T bwt:
Occc(T bwt, i) is the number of occurrences of character c in T bwt[1, i]. This can be
done in constant time for very small alphabets [5], but to handle larger alphabets
[6] a tool called the wavelet tree [7] of S = T bwt is used.
Given a sequence S[1, n] the wavelet tree wt(S) [7] built on S is a perfect binary 
tree of height (cid:6)log σ(cid:7), built on the alphabet symbols, such that the root represents 
the whole alphabet and each leaf represents a distinct alphabet symbol.
If a node v represents alphabet symbols in the range Av = [i, j], then its left child
vl represents Avl = [i, i+j
2 + 1, j].
We associate to each node v the subsequence Sv of S formed by the characters
in Av. However, sequence Sv is not really stored at the node. Instead, we store a
bit sequence Bv telling whether characters in Sv go left or right, that is, Bv
i = 1
iﬀ Sv
i
The wavelet tree of S requires nH0(S) + O(n log log n/ logσ n) bits of space.

2 ] and its right child vr represents Avr = [ i+j

∈ Avr .

3 A New Entropy-Bound Succinct Data Structure

Given a sequence S[1, n] over an alphabet A of size σ, we encode S into a
(cid:5) within entropy bounds. To perform all the original
compressed data structure S
operations over S under the RAM model, it is enough to allow extracting any
b = 1

2 logσ n consecutive symbols of S, using S

(cid:5), in constant time.

3.1 Data Structures for Substring Decoding

We describe our data structure to represent S in essentially nHk(S) bits, and to
permit the access of any substring of size b = (cid:8) 1
2 logσ n(cid:9) in constant time. This
structure is built using any statistical encoder E as described in Section 2.2.
Structure. We divide S into blocks of length b = (cid:8) 1
block will be represented using at most b
We deﬁne the following sequences indexed by block number i = 0, . . . ,(cid:8)n/b(cid:9):
– Si = S[bi + 1, b(i + 1)] is the sequence of symbols forming the i-th block
– Ci = S[bi − k + 1, bi] is the sequence of symbols forming the k-th order

2 logσ n(cid:9) symbols. Each
2 log n(cid:9) bits (and hopefully less).

(cid:5) = (cid:8) 1

of S.

context of the i-th block (a dummy value is used for C0).

– Ei = E(Si) is the encoded sequence for the i-th block of S, initializing the
– (cid:4)i = |Ei| is the size in bits of Ei.

k-th order modeler with context Ci.

Statistical Encoding of Succinct Data Structures

299

(cid:4)

(cid:5)

Si if (cid:4)i > b
– ˜Ei =
Ei otherwise , is the shortest sequence among Ei and Si.
– ˜(cid:4)i = | ˜Ei| ≤ min(b
(cid:5) bits
The idea behind ˜Ei is to ensure that no encoded block is longer than b
(which could happen if a block contains many infrequent symbols). These special
blocks are encoded explicitly.

, (cid:4)i) is the size in bits of ˜Ei.

(cid:5)

Our compressed representation of S stores the following information:
– W [0,(cid:8)n/b(cid:9)]: A bit array such that

(cid:4)

W [i] =

(cid:5)
0 if (cid:4)i > b
1 otherwise ,

(cid:5), with 1 ≤ i ≤ (cid:8)n/b(cid:9).

with the additional o(n/b) bits to answer rank queries over W in constant
time [11].
– C[1, rank(W,(cid:8)n/b(cid:9))]: C[rank(W, i)] = Ci, that is, the k-th order context for
the i-th block of S iﬀ (cid:4)i ≤ b
– U = ˜E0 ˜E1 . . . ˜E(cid:6)n/b(cid:7): A bit sequence obtained by concatenating all the
variable-length ˜Ei.
(cid:2) −→ 2b: A table deﬁned as T [α, β] = γ, where α is any context
– T : Ak × 2b
(cid:5) bits at most, and γ represents
of size k, β represents any encoded block of b
(cid:5)
the decoded form of β, truncated to the ﬁrst b symbols (as less than the b
bits will be usually necessary to obtain the b symbols of the block).
– Information to answer where each ˜Ei starts within U. We group together
every c = (cid:6)log n(cid:7) consecutive blocks to form superblocks of size Θ(log2 n)
and store two tables:
• Rg[0,(cid:8)n/(bc)(cid:9)] contains the absolute position of each superblock.
• Rl[0,(cid:8)n/b(cid:9)] contains the relative position of each block with respect to
the beginning of its superblock.

3.2 Substring Decoding Algorithm
We want to retrieve q = S[i, i + b − 1] in constant time. To achieve this, we take
the following steps:

(cid:5) = (j +1) div c and u = U[Rg[h]+ Rl[j], Rg[h

(cid:5)]+

(cid:5) = (i + b − 1) div b.

1. We calculate j = i div b and j
2. We calculate h = j div c, h

Rl[j + 1] − 1], then
– if W [j] = 0 then we have Sj = u.
– if W [j] = 1 then we have Sj = T [C[rank(W, j)], u
We note that |u| ≤ b
Sj[i − jb + 1, b] Sj(cid:2)[1, i − jb] is the solution.

with b
(cid:5) (cid:11)= j then we repeat Step 2 for j

(cid:5) − |u| dummy bits.

3. If j

(cid:5)], where u

(cid:5) is u padded

(cid:5) and thus it can be manipulated in constant time.

(cid:5) = j + 1 and obtain Sj(cid:2). Then, q =

Lemma 1. For a given sequence S[1, n] over an alphabet A of size σ, we can
access any substring of S of b symbols in O(1) time using the data structures
presented in Section 3.1.

300

R. Gonz´alez and G. Navarro

3.3 Space Requirement

Let us now consider the storage size of our structures.

– We use the constant-time solution to answer the rank queries [11] over W ,

totalizing

2n
logσ n(1 + o(1)) bits.
2n
logσ n k log σ bits.
(cid:3)(cid:6)n/b(cid:7)

– Table C requires at most
– Let us consider table U. |U| =

i=0

| ˜Ei| ≤ (cid:3)(cid:6)n/b(cid:7)

i=0

(cid:3)(cid:6)n/b(cid:7)

|Ei| = nHk(S) +
i=0 fk(E, Si), which depends on the statistical encoder E
O(k log n) +
used. For example, in the case of Huﬀman coding, we have fk(Huﬀman, Si) <
b, and thus we achieve nHk(S) + O(k log n) + n bits. For the case of Arithmetic 
coding, we have fk(Arithmetic, Si) ≤ 2, and thus we have nHk(S) +
O(k log n) + 4n

logσ n bits, as described in Section 2.2.

(cid:2)
– The size of T is σk2b
– Finally, let us consider tables Rg and Rl. Table Rg has (cid:6)n/(bc)(cid:7) entries of size
c)(cid:7),

b log σ = σk n1/2 log n
logσ n bits. Table Rl has (cid:6)n/b(cid:7) entries of size (cid:6)log(b
bits.

(cid:6)log n(cid:7), totalizing
totalizing 4n log log n

2 bits.

(cid:5)

2n

logσ n

By considering that any substring of Θ(logσ n) symbols can be extracted in
constant time by applying O(1) times the procedure of Section 3.2, we have the
ﬁnal theorem.

Theorem 1. Let S[1, n] be a sequence over an alphabet A of size σ. Our data
structure uses nHk(S) + O( n
logσ n(k log σ + log log n)) bits of space for any k <
(1− ) logσ n and any constant 0 <  < 1, and it supports access to any substring
of S of size Θ(logσ n) symbols in O(1) time.
−) logσ n,
Note that, in our scheme, the size of T can be neglected only if k < ( 1
2
but this can be pushed as close to 1 as desired by choosing b = 1
s logσ n for
constant s ≥ 2.
Corollary 1. The previous structure takes space nHk(S) + o(n log σ) if k =
o(logσ n).
These results match exactly those of [18], once one corrects their k to k log σ
as explained. Note that we are storing some redundant information that can
be eliminated. The last characters of block Si are stored both within ˜Ei and
as Ci+1. Instead, we can choose to explicitly store the ﬁrst k characters of all
blocks Si, and encode only the remaining b − k symbols, Si[k + 1, b], either in
explicit or compressed form. This improves the space in practice, but in theory
it cannot be proved to be better than the scheme we have given.

4 Supporting Appends

We can extend our scheme to support appending symbols, maintaining the
same space and query complexity, with each appended symbol having constant

Statistical Encoding of Succinct Data Structures

301

amortized cost. Assume our current static structure holds n symbols. We use
(cid:5) = n/ log n symbols where we store symbols explicitly. When the
a buﬀer of n
buﬀer is full we use our entropy-bound data structure (EBDS, Section 3) to
(cid:5) symbols and then we empty the buﬀer. We repeat this until
represent those n
we have log n EBDS. At this moment we reencode all the structures plus our
original n symbols, generating a new single EBDS, and restart the process with
2n symbols.

Data structures. We describe the additional structures needed to append
symbols to the EBDS.

(cid:5)] is the sequence of at most n

(cid:5) = n/ log n uncompressed symbols.
– BF [1, n
– APi is the i-th EBDS, with 0 ≤ i ≤ N. N ≤ log n is the number of EBDS we
currently have. We call ASi the sequence APi represents. AP0 is the original
EBDS. So |AS0| = n and |ASi| = n/ log n, i > 0.

Substring decoding algorithm. We want to retrieve q = S[i, i + b − 1]. To
achieve this, we take the following steps:
– We algebraically calculate the indexes 0 ≤ t ≤ t

(cid:5) ≤ N +1 where the positions
i (for t) and i + b − 1 (for t
(cid:5)) belong; N + 1 represents BF . The case when
part of q belongs to BF is trivially solved because the symbols are explicitly
represented in BF .
(cid:5) we obtain q as in Section 3.2. Otherwise, we calculate the local
(cid:5)
of f where q starts in structure APt and ﬁnishes in APt(cid:2),
(cid:5) − tof f + 1 ≤ b symbols of APt and
≤ b symbols of APt(cid:2). Finally, we obtain q = q1q2.

indexes tof f and t
respectively. We decode q1 as the last n
q2 as the ﬁrst t

– If t = t

(cid:5)
of f

Construction time. after we reencode everything we have that n/2 symbols
have been reencoded once, n/4 symbols twice, n/8 symbols 3 times and so on.
2i = 2n. On the other hand, we are
The total number of reencodings is
using a semi-static statistical encoder, which takes O(1) time to encode each
symbol. Thus each symbol has a worst-case amortized appending cost of O(1).

i≥1 n i

(cid:3)

Space requirement. Let us now consider the storage of the appended structures.


– Table BF requires n/ logσ n bits
– Each APi is an EBDS, using |ASi|Hk(ASi) + O(

|ASi|)) bits of space.

|ASi|

|ASi| (k log σ + log log

logσ

i=0

|APi| ≤ |S AS1 . . . ASN|Hk(S AS1 . . . ASN )+O( n

Lemma 2. The space requirement of all APi,
(cid:3)log n
+O(σk+1 log2 n) + O(k log2 n) bits, where n = |S| ≤ |S AS1 . . . ASN|/2.
Proof. Consider summing any two entropies (recall Eqs. (1) and (2)).

for 0 ≤ i ≤ N,
is
logσ n(k log σ+log log n))

302

R. Gonz´alez and G. Navarro

(cid:5)

|AS1|Hk(AS1) + |AS2|Hk(AS2) =
(cid:3)
(cid:3)
|H0(wAS1) +
w∈Ak |wAS1
w∈Ak |wAS2
=
(cid:6)
≤ (cid:3)
|
|wAS1
+ log
log
w∈Ak
|,...,|naσ
|na1
O(σk+1 log n)
(cid:6)
≤ (cid:3)
|+|naσ
|
|na1
≤ |AS1AS2|Hk(AS1AS2) + O(σk+1 log n) + O(k log n)

|H0(wAS2)
(cid:6)
|wAS2
|,|na2
|na1

w∈Ak log

|+|na1

|,|na2

|wAS1

|+|wAS2
|+|na2

|
|,...,|naσ

|,|na2

AS2

AS2

AS2

AS1

AS1

AS1

AS1

AS2

(cid:7)
|

AS1

AS1

AS2

(cid:7)(cid:8)

|

+

|
|,...,|naσ
(cid:7)

AS2

+ O(σk+1 log n)

where O(σk+1 log n) comes from the relationship between the zero-order entropy
and the combinatorials, and O(k log n) comes from considering the symbols in the
border between AS1 and AS2. Note that σk+1 log n = o(n) if k < (1 − ) logσ n.
Then the lemma follows by adding up the N ≤ log n EBDSs.
Theorem 2. The structure of Theorem 1 supports appending symbols in constant 
amortized time and retains the same space and query time complexities,
being n the current length of the sequence.

5 Application to Full-Text Indexing

In this section we give some positive and negative results about the application
of the technique to full-text indexing, as explained in the Introduction. We have
a text T [1, n] over alphabet A and wish to compress a transformed version X of
T with our technique. Then, the question is how does Hk(X) relate to Hk(T ).

5.1 The Burrows-Wheeler Transform

The Burrows-Wheeler Transform, S = bwt(T ), is used by many compressed
full-text self-indexes [5, 6, 9]. We have introduced it in Section 2.3.

We show that there is a relationship between the k-th order entropy of a text
T and the ﬁrst order entropy of S = bwt(T ). For this sake, we will compress S
with a ﬁrst-order compressor, whose output size is an upper bound to nH1(S).
A run in S is a maximal substring formed by a single letter. Let rl(S) be the
number of runs in S. In [9] they prove that rl(S) ≤ nHk(T ) + σk for any k. Our
ﬁrst-order encoder exploits this property, as follows:

– If i > 1 and si = si−1 then we output bit 0.
– Otherwise we output bit 1 followed by si in plain form (log σ bits).

Thus we encode each symbol of S by considering only its preceding symbol.
The total number of bits is n + rl(S) log σ ≤ n(1 + Hk(S) log σ + σk log σ
). The
latter term is negligible for k < (1 − ) logσ n, for any 0 <  < 1. On the other
hand, the total space obtained by our ﬁrst-order encoder cannot be less than
nH1(S). Thus we get our result:
Lemma 3. Let S = bwt(T ), where T [1, n] is a text over an alphabet of size σ.
Then H1(S) ≤ 1 + Hk(T ) log σ + o(1) for any k < (1− ) logσ n and any constant
0 <  < 1.

n

Statistical Encoding of Succinct Data Structures

303

We can improve this upper bound if we use Arithmetic encoding to encode the
0 and 1 bits that distinguish run heads. Their zero-order probability is p =
n , thus the 1 becomes −p log p − (1 − p) log(1 − p) ≤ 1. Likewise, we
Hk(T ) + σk
can encode the run heads si up to their zero-order entropy. These improvements,
however, do not translate into clean formulas.

This shows, for example, that we can get (at least) about the same results of

the Run-Length FM-Index [9] by compressing bwt(T ) using our structure.

5.2 The Wavelet Tree

Several FM-Index variants [9, 6] use wavelet trees to represent S = bwt(T ),
while others [7] use them for other purposes. As explained in Section 2.3, wt(S)
is composed of several binary sequences. By compressing each such sequence B
to |B|H0(B) bits, one achieves nH0(S) bits overall. The natural question is, thus,
whether we can prove any bound on the overall space if we encode sequences B
to |B|Hk(B) bits. We present next two negative examples.
– First we show a case where Hk(S) < Hk(wt(S)). We choose S =

(ak

3ak

1ak

0ak

2ak

0)n, then

wt(S) =

ν0 = (1k0k0k1k0k)n

0

................................................................................................................................................................

a0a1

................................................................................................................................................................

1

a2a3
ν2 = (1k0k)n

ν1 = (1k0k0k)n

Let us compute Hk(S) according to Section 2.1. Note that H0(wS) = 0 for
0, where wS = a2(a3a2)n−1$, being “$” a sequence
all contexts except w = ak
terminator. Thus |wS| = 2n and H0(wS) = − n
−
2n log 1
1
On the other hand, Hk(wt(S)) =

n ). Therefore Hk(S) (cid:12) 2
5k .

2n = 1 + O( log n

(cid:3)2
i=0 Hk(νi) (cid:12) 2
log k
5k
(cid:9) (cid:10)(cid:11) (cid:12)

2n log n−1
log k
1
+
, as
3k
3k
(cid:12)
(cid:10)(cid:11)
(cid:9)
ν1

2n log n
2n

− n−1

+

2n

ν0

Therefore, in this case, Hk(S) < Hk(wt(S)), by a Θ(log k) factor.

– Second, we show a case where Hk(S) > Hk(wt(S)). Now we choose S =

Hk(ν2) (cid:12) 0.

(ak

0ak

3ak

0ak

2)n, then

wt(S) =

ν0 = (0k1k0k1k)n

0

................................................................................................................................................................

a0a1

................................................................................................................................................................

1

a2a3
ν2 = (1k0k)n
(cid:3)2
In this case, Hk(S) (cid:12) 2
i=0 Hk(νi) = O( log n
Hk(S) > Hk(wt(S)) by a factor of Θ(n/(k log n)).

ν1 = (0k0k)n
4k and Hk(wt(S)) =

n ). Thus

304

R. Gonz´alez and G. Navarro

Lemma 4. The ratio between the k-th order entropy of
tree
representation of a sequence S, Hk(wt(S)), and that of S itself, Hk(S), can
be at least Ω(log k). More precisely, Hk(wt(S))/Hk(S) can be Ω(log k) and
Hk(S)/Hk(wt(S)) can be Ω(n/(k log n)).

the wavelet

What is most interesting is that Hk(wt(S)) can be Θ(log k) times larger than
Hk(S). We have not been able to produce a larger gap. Whether Hk(wt(S)) =
O(Hk(S) log k) remains open.

6 Conclusions

We have presented a scheme based on k-th order modeling plus statistical encoding 
to convert any succinct data structure on sequences into a compressed
data structure. This structure permits retrieving any string of S of Θ(logσ n)
symbols in constant time. This is an alternative to the ﬁrst work achieving the
same result [18], which is based on Ziv-Lempel compression. We also show how
to append symbols to the original sequence within the same space complexity
and with constant amortized cost per appended symbol. This method also works
on the structure presented in [18].

We also analyze the behavior of this technique when applied to full-text selfindexes,
 as advocated in [18]. Many compressed self-indexes achieve space proportional 
to nHk(T ) by ﬁrst applying the Burrows-Wheeler Transform [2] over
T , S[1, n] = bwt(T ). In this paper, we show a relationship between the entropies 
of H1(S) and Hk(T ). More precisely, H1(S) ≤ Hk(T ) log σ + o(1) for
small k = o(logσ n). On the other hand, several indexes represent S = bwt(T )
as a wavelet tree [7] on S, wt(S). We show in this paper that Hk(wt(S)) can
be at least Θ(log k) times larger than Hk(S). This means that, by applying the
new technique to compress wavelet trees, we have no guarantee of compressing
the original sequence more than n min(H0(S), O(Hk(T ) log k)). Yet, we do have
guarantees if we compress S directly.

There are several future challenges on k-th order entropy-bound data struc-
tures: (i) making them fully dynamic (we have shown how to append symbols);
(ii) better understanding how the entropies evolve upon transformations such
bwt or wt; (iii) testing them in practice.

Acknowledgment. We thank K. Sadakane and R. Grossi for providing us
article [18] and for conﬁrming the correctness of Footnote 2.

References

1. T. Bell, J. Cleary, and I. Witten. Text compression. Prentice Hall, 1990.
2. M. Burrows and D. Wheeler. A block sorting lossless data compression algorithm.

Technical Report 124, Digital Equipment Corporation, 1994.

3. P. Ferragina, F. Luccio, G. Manzini, and S. Muthukrishnan. Structuring labeled

trees for optimal succinctness, and beyond. In Proc. 46st FOCS, 2005.

Statistical Encoding of Succinct Data Structures

305

4. P. Ferragina, F. Luccio, G. Manzini, and S. Muthukrishnan. Compressing and

searching XML data via two zips. In Proc. 15th WWW’06, 2006.

5. P. Ferragina and G. Manzini. Opportunistic data structures with applications. In

Proc. 41st FOCS, pages 390–398, 2000.

6. P. Ferragina, G. Manzini, V. M¨akinen, and G. Navarro. An alphabet-friendly FMindex.
 In Proc. 11th SPIRE, LNCS 3246, pages 150–160. Springer, 2004. Extended
version to appear in ACM TALG.

7. R. Grossi, A. Gupta, and J. Vitter. High-order entropy-compressed text indexes.

In Proc. 14th SODA, pages 841–850, 2003.

8. R. Kosaraju and G. Manzini. Compression of low entropy strings with Lempel-Ziv

algorithms. SIAM Journal on Computing, 29(3):893–911, 1999.

9. V. M¨akinen and G. Navarro. Succinct suﬃx arrays based on run-length encoding.

Nordic Journal of Computing, 12(1):40–66, 2005.

10. G. Manzini. An analysis of the Burrows-Wheeler transform. Journal of the ACM,

48(3):407–430, 2001.

11. I. Munro. Tables. In Proc. 16th FSTTCS, LNCS v. 1180, pages 37–42, 1996.
12. I. Munro, R. Raman, V. Raman, and S. Rao. Succinct representations of permutations.
 In Proc. 30th ICALP, pages 345–356, 2003.

13. I. Munro and V. Raman. Succinct representation of balanced parentheses, static

trees and planar graphs. In Proc. 38th FOCS, pages 118–126, 1997.

14. I. Munro and S. S. Rao. Succinct representations of functions.

In Proc. 31th

15. G. Navarro. Indexing text using the Ziv-Lempel trie. Journal of Discrete AlgoICALP,
 pages 1006–1015, 2004.

rithms (JDA), 2(1):87–114, 2004.

16. R. Raman, V. Raman, and S. Rao. Succinct indexable dictionaries with applications 
to encoding k-ary trees and multisets. In Proc. 13th SODA, pages 233–242,
2002.

17. K. Sadakane and R. Grossi. Personal communication, 2005.
18. K. Sadakane and R. Grossi. Squeezing succinct data structures into entropy

bounds. In Proc. 17th SODA, pages 1230–1239, 2006.

