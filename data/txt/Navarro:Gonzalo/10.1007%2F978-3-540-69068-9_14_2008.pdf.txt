On-Line Approximate String Matching with

Bounded Errors

Marcos Kiwi1,(cid:2), Gonzalo Navarro2,(cid:2)(cid:2), and Claudio Telha3,(cid:2)(cid:2)(cid:2)

1 Departamento de Ingenier´ıa Matem´atica, Centro de Modelamiento Matem´atico

UMI 2807 CNRS-UChile
www.dim.uchile.cl/∼mkiwi

2 Department of Computer Science, University of Chile

gnavarro@dcc.uchile.cl

3 Operations Research Center, MIT

ctelha@mit.edu

Abstract. We introduce a new dimension to the widely studied on-line
approximate string matching problem, by introducing an error threshold
parameter  so that the algorithm is allowed to miss occurrences with
probability . This is particularly appropriate for this problem, as approximate 
searching is used to model many cases where exact answers are not
mandatory. We show that the relaxed version of the problem allows us
breaking the average-case optimal lower bound of the classical problem,
achieving average case O(n logσ m/m) time with any  = poly(k/m),
where n is the text size, m the pattern length, k the number of errors
for edit distance, and σ the alphabet size. Our experimental results show
the practicality of this novel and promising research direction.

1 Introduction

∗ × Σ

In string matching one is interested in determining the positions (sometimes just
deciding the occurrence) of a given pattern P on a text T , where both pattern
and text are strings over some ﬁxed ﬁnite alphabet Σ of size σ. The lengths
of P and T are typically denoted by m and n respectively. In approximate
string matching there is also a notion of distance between strings, given say by
∗ → R. One is given an additional non-negative input parameter
d : Σ
k and is interested in listing all positions (or just deciding the occurrence) of
substrings S of T such that S and P are at distance at most k. In the “on-line”
or “sequential” version of the problem, one is not allowed to preprocess the text.
Since the 60’s several approaches were proposed for addressing the approximate 
matching problem, see for example the survey by Navarro [5]. Most of the
work focused on the edit or Levenshtein distance d, which counts the number

(cid:3) Gratefully acknowledges the support of CONICYT via FONDAP in Applied Mathematics 
and Anillo en Redes ACT08.

(cid:3)(cid:3) Funded in part by Fondecyt Grant 1-050493, Chile.
(cid:3)(cid:3)(cid:3) Gratefully acknowledges the support of CONICYT via Anillo en Redes ACT08 and

Yahoo! Research Grant ”Compact Data Structures”.

P. Ferragina and G. Landau (Eds.): CPM 2008, LNCS 5029, pp. 130–142, 2008.
c(cid:3) Springer-Verlag Berlin Heidelberg 2008

On-Line Approximate String Matching with Bounded Errors

131

of character insertions, deletions, and substitutions needed to make two strings
equal. This distance turns out to be suﬃciently powerful to model many relevant
applications (e.g., text searching, information retrieval, computational biology,
transmission over noisy channels, etc.), and at the same time suﬃciently simple
to admit eﬃcient solutions (e.g., O(mn) and even O(kn) time).

A lower bound to the (worst-case) problem complexity is obviously Ω(n) for
the meaningful cases, k < m. This bound can be reached by using automata,
which introduce an extra additive term in the time complexity which is exponential 
in m or k. If one is restricted to polynomially-bounded time complexities
on m and k, however, the worst-case problem complexity is unknown.

Interestingly, the average-case complexity of the problem is well understood.
If the characters in P and T are chosen uniformly and independently, the average
problem complexity is Θ(n(k + logσ m)/m). This was proved in 1994 by Chang
and Marr [2], who gave an algorithm reaching the lower bound for k/m < 1/3−
−1/2). In 2004, Fredriksson and Navarro [3] gave an improved algorithm
O(σ
achieving the lower bound for k/m < 1/2 − O(σ
−1/2). In addition to covering
the range of interesting k values for virtually all applications, the algorithm was
shown to be highly practical.

It would seem that, except for determining the worst-case problem complexity
(which is mainly of theoretical interest), the on-line approximate string matching 
problem is closed. In this paper, however, we reopen the problem under a
relaxed scenario that is still useful for most applications and admits solutions
that beat the lower bound. More precisely, we relax the goal of listing all positions 
where pattern P occurs in the text T to that of listing each such position
with probability 1 − , where  is a new input parameter.

There are several relevant scenarios where fast algorithms that make errors
(with a user-controlled probability) are appropriate. Obvious cases are those
where approximate string matching is used to increase recall when searching
data that is intrinsically error-prone. Consider for example an optical character 
recognition application, where errors will inevitably arise from inaccurate
scanning or printing imperfections, or a handwriting recognition application, or
a search on a text with typos and misspells. In those cases, there is no hope
to ﬁnd exactly all the correct occurrences of a word. Here, uncertainty of the
input translates into approximate pattern matching and approximate searching
is used to increase the chance of ﬁnding relevant occurrences, hopefully without
introducing too many false matches. As the output of the system, even using a
correct approximate string matching technique, is an approximation to the ideal
answer, a second approximation might be perfectly tolerable, and even welcome
if allows for faster searches.

A less obvious application arises in contexts where we might have a priori
knowledge that some pattern is either approximately present in the text many
times, or does not even approximately occur. Some examples are genetic markers
that might often appear or not at all, some modisms that might appear in several
forms in certain type of texts, some typical pattern variants that might appear
in the denomination of certain drugs, people names, or places, of which typically

132

M. Kiwi, G. Navarro, and C. Telha

several instances occur in the same text. Further, we might only be interested
in determining whether the pattern occurs or not. (A feature actually available
in the well known grep string searching utility as options -l and -L, and also
the approximate string searching utilities agrep and ngrep.) In this context, a
text with N approximate pattern occurrences will be misclassiﬁed by the inexact
algorithm with very low probability, N .

Another interesting scenario is that of processing data streams which ﬂow so
fast that there is no hope for scanning them exhaustively (e.g. radar derived
meteorological data, browser clicks, user queries, IP traﬃc logs, peer-to-peer
downloads, ﬁnancial data, etc.). Hence even an exact approximate search over
part of the data would give only partial results. A faster inexact algorithm could
even give better quality answers as it could scan a larger portion of the data,
even if making mistakes on it with controlled probability.

The new framework proposed in this work comes from the so-called testing and
property testing literature where the aim is to devise sublinear time algorithms
obtained by avoiding having to read all of the input of a problem instance. These
procedures typically read a very small fraction of the input. For most natural
problems the algorithm must use randomization and provide answers which in
some sense are approximate, or wrong with some probability. See the many
surveys on the topic, e.g. [7,8].

1.1 Main Contributions
We focus in particular on the so-called ﬁltering algorithms [5, § 8]. These algorithms 
quickly discard areas of the text that cannot approximately match
the pattern, and then verify the remaining areas with a classical algorithm. In
practice, ﬁltering algorithms are also the fastest approximate string matching
algorithms. They also turn out to be natural candidates to design probabilistic
variants in this paper.

In Section 3.1 we describe a procedure based on sampling q-grams motivated
by the ﬁltering algorithm of Ukkonen [9]. For a ﬁxed constant t > 0 and k <
m/ logσ m, the derived algorithm has an average case complexity of O(tn logσ m/
m) and misses pattern occurrences with probability = O((k logσ m/m)t). Note
that the time equals Yao’s lower bound for exact string matching (k = 0). In
contrast, Ukkonen’s original algorithm takes O(n) time. In Section 3.2 we describe 
an algorithm based on Chang and Marr’s [2] average-optimal algorithm.
For ﬁxed t > 0, we derive an O(tn logσ m/m) average-time approximate matching 
algorithm with error  = O((k/m)t). Note that the latter achieves the same
time complexity for a smaller error, and that it works for any k < m, whereas
the former needs k < m/ logσ m.

The discrepancy between both algorithms inherits from that of the original
classical algorithms they derive from, where the original diﬀerences in time complexities 
has now translated into their error probabilities. It is important to stress
that both algorithms beat the average-complexity lower bound of the problem
when errors are not allowed, Ω(n(k + logσ m)/m), as they remove the Ω(kn/m)
term in the complexity (the k/m term now shows up in the error probability).

On-Line Approximate String Matching with Bounded Errors

133

The aforementioned average case complexity results are for random text, but
hold even for ﬁxed patterns. Our analyzes focus exclusively on Levenshtein distance 
d, but should be easily adapted to other metrics.

In Section 4 we present some experimental results that corroborate the theoretical 
results of Section 3 and give supporting evidence for the practicality of
our proposals. In particular, the experiments favor the technique of Section 3.1
over that of Section 3.2, despite the theoretical superiority of the latter.

2 Model for Approximate Searching Allowing Errors

In this section we formalize the main concepts concerning the notion of approximate 
matching algorithms with errors. We adopt the standard convention
of denoting the substring Si . . . Sj of S = S1 . . . Sn by Si..j and refer to the
number of characters of S by the length of S which we also denote by |S|.
We start by recalling the formal deﬁnition of the approximate string matching
problem when the underlying distance function is d. Henceforth, we abbreviate
d-Approximate String Matching as d-ASM.
Problem d-Approximate String Matching
Output S = S(T, P, k) ⊆ {1, . . . , n} such that j ∈ S if and only if there is

∗ and parameter k ∈ N.

Input Text T ∈ Σ

∗, pattern P ∈ Σ
an i such that d(Ti..j, P ) ≤ k.

When the text T and pattern P are both in Σ

∗, and the parameter k is
in N we say that (T, P, k) is an instance of the d-ASM problem, or simply
an instance for short. We henceforth refer to S(T, P, k) as the solution set of
instance (T, P, k). We say that algorithm A solves the d-ASM problem if on
instance (T, P, k) it outputs the solution set S(T, P, k). Note that A might be a
probabilistic algorithm, however its output is fully determined by (T, P, k).
For a randomized algorithm A that takes as input an instance (T, P, k), let
A(T, P, k) be the distribution over sets S ⊆ {1, . . . , n} that it returns.
Henceforth we denote by X ← D the fact that the random variable X is chosen
according to distribution D. For a set C we denote the probability that X ∈ C
when X is chosen according to the distribution D by Pr [X ∈ C; X ← D] or
PrX←D [X ∈ C]. Also, we might simply write PrX [X ∈ C] or Pr [X ∈ C] when
it is clear from context that X ← D. The notation generalizes in the obvious way
to the case where X is a random vector, and/or when instead of a probability
one is interested in taking expectation.
(cid:5))-

We say that randomized algorithm A solves the d-ASM problem with (, 

error provided that on any instance (T, P, k) the following holds:

Completeness: if i ∈ S(T, P, k), then Pr [i ∈ S
Soundness: if i (cid:8)∈ S(T, P, k), then Pr [i ∈ S
; S

(cid:5)

(cid:5)

(cid:5) ← A(T, P, k)] ≥ 1 − ,

(cid:5); S
(cid:5) ← A(T, P, k)] ≤ 

,

where the two probabilities above are taken only over the source of randomness
of A.

134

M. Kiwi, G. Navarro, and C. Telha

short. When  = 

(cid:5) = 0 we say that A is an errorless or exact algorithm.

(cid:5) = 0 we say that A has one–sided -error or that it is one–sided for
When 
We say that randomized algorithm F is a d-ASM probabilistic ﬁlter with αerror 
or simply is an α-ﬁlter for short, provided that on any instance (W, P, k)
the following holds: if d(Pi..j , W ) ≤ k for some pattern substring Pi..j, then
Pr [F(W, P, k) = Check] ≥ 1 − α, where the probability is taken over the source
of randomness of F. If a ﬁlter does not return Check we assume without loss of
generality that it returns Discard.
The notion of an α-ﬁlter is crucial to the ensuing discussion. Roughly said, a
ﬁlter F will allow us to process a text T by considering non-overlapping consecutive 
substrings W of T , running the ﬁlter on instance (W, P, k) and either: (1)
in case the ﬁlter returns Check, perform a costly approximate string matching
procedure to determine whether P approximately occurs in T in the surroundings 
of window W , or (2) in case the ﬁlter does not return Check, discard the
current window from further consideration and move forward in the text and
process the next text window. The previously outlined general mechanism is the
basis of the generic algorithm we illustrate in Fig. 1 and describe below. The

Text T

Current window W

Filter F

Discard

Apply ﬁlter to next window

Check

Approximate search

Fig. 1. Generic algorithm d-Approximate String Matching algorithm

attentive reader would have noticed that when deﬁning probabilistic ﬁlters we
substituted the notation T for texts by W . This is done in order to stress that
the probabilistic ﬁlters that we will talk about access the text T by sequentially
examining substrings of T which we will refer to as windows. These windows will
typically have a length which is independent of n, more precisely they will be of
length O(m).
We now precisely describe the central role played by probabilistic ﬁlters in the
design of d-ASM algorithms with errors. First, from now on, let w denote (cid:9)(m−
k)/2(cid:10). Henceforth let W1, . . . , Ws be such that T = W1 . . . Ws and |Wp| = w (pad
T with an additional character not in Σ as necessary). Note that s = (cid:11)n/w(cid:12) and
Wp = T(p−1)w+1..pw. Given any probabilistic ﬁlter F and an exact algorithm E
we can devise a generic d-ASM algorithm with errors such as the one speciﬁed
in Algorithm 1.1
We will shortly show that the generic algorithm G is correct. We also would like
to analyze its complexity in terms of the eﬃciencies of both the probabilistic ﬁlter
1 For A ⊆ Z we use the standard convention of denoting {a + x : x ∈ A} by a + A.

On-Line Approximate String Matching with Bounded Errors

135

Algorithm 1. Generic d-Approximate String Matching with Errors
1: procedure G(T, P, k)
S ← ∅
2:
w ← (cid:7)(m − k)/2(cid:8)
3:
s ← (cid:9)n/w(cid:10)
4:
for p ∈ {1, . . . , s} do
5:
6:
7:
8:

if F(Wp, P, k) = Check then
S ← S ∪ (cid:2)

(pw − m − k + 1) + E(Tpw−m−k+1..(p−1)w+m+k−1, P, k)

return S

(cid:4) T ∈ Σn, P ∈ Σm, k ∈ N

(cid:4) Where Wp = T(p−1)w+1..pw

(cid:3)

F and the exact algorithm E. However, we ﬁrst need to introduce the complexity
measures that we will be looking at. Let TimeA(T, P, k) ∈ N ∪ {+∞} be the
expected time complexity of A on the instance (T, P, k), where the expectation
is taken over the random choices of A. We also associate to A the following
average time complexity measures:

AvgA(n, P, k) = ExT [TimeA(T, P, k)] ,
AvgA(n, m, k) = ExT,P [TimeA(T, P, k)] .

Let MemA(T, P, k) ∈ N ∪ {+∞} be the maximum amount of memory required
by A on instance (T, P, k), where the maximum is taken over all possible sequences 
of random bits on which A may act, and let

MemA(n, P, k) = max
T∈Σn
T∈Σn,P∈Σm

MemA(n, m, k) =

max

MemA(T, P, k),

MemA(T, P, k).

We similarly deﬁne RndA(T, P, k), RndA(n, P, k), and RndA(n, m, k), but with
respect to the maximum number of random bits used by A. Also, the same
complexity measures can be deﬁned for probabilistic ﬁlters and exact algorithms.
Theorem 1. Suppose m > k. Let F be an α-ﬁlter and let E be the standard
deterministic O(kn) dynamic programming algorithm for the d-ASM problem.
Let w = (cid:9)(m − k)/2(cid:10), s = (cid:11)n/w(cid:12), and W ⊆ Σw. Then, the generic algorithm G
is a d-ASM algorithm with one-sided α-error such that
AvgG(n, P, k) ≤ s · AvgF(w, P, k)
+s · O (mk) · (PrW←Σw [W ∈ W] + maxW(cid:7)∈W Pr [F(W, P, k)= Check]) + O(s).
Also, MemG(n, P, k) = MemE(3w + 4k + 2, P, k) (ignoring the space required
to output the result), and RndG(n, P, k) = O( n
Proof. First, let us establish completeness of G. Assume i ∈ S(T, P, k). Let p + 1
be the index of the window to which the character Ti belongs. As any occurrence
has length at least m− k, Wp is completely contained in the occurrence ﬁnishing
at i, and thus Wp must be at distance at most k of a substring of P . It follows
that F(Wp, P, k) = Check with probability at least 1− α, in which case line 7 of

m−k ) · RndF(w, P, k).

M. Kiwi, G. Navarro, and C. Telha

136
the algorithm will run an exact veriﬁcation with E over a text area comprising
any substring of length m + k that contains Wp. Since m + k is the maximum
length of an occurrence, it follows that i will be included in the output returned
by G. Hence, with probability at least 1− α we have that i is in the output of G.
To establish soundness, assume i (cid:8)∈ S(T, P, k). In this case, i will never be
included in the output of G in line 7 of the algorithm.
We now determine G’s complexity. By linearity of expectation and since

TimeE(O(m), m, k) = O(mk), we have
AvgG(n, P, k) =
s(cid:4)

(ExT [TimeF(Wp, P, k)] + O(mk) · PrT [F(Wp, P, k) = Check] + O(1))

p=1

= s · AvgF (w, P, k) + O(mk) ·

s(cid:4)

PrT [F(Wp, P, k) = Check] + O(s) .

p=1

Conditioning according to whether Wp belongs to W, we get for any W that
PrT [F(Wp, P, k)= Check] ≤ PrW←Σw [W ∈ W]+ max
W(cid:7)∈W Pr [F(W, P, k)= Check] .
The stated bound on AvgG(n, P, k) follows immediately. The memory and ran-
(cid:15)(cid:16)
domized complexity bounds are obvious.
The intuition behind the preceding theorem is that, given any class W of “in-
teresting” windows, if we have a ﬁlter that discards the uninteresting windows
with high probability, then the probability that the algorithm has to verify a
given text window can be bounded by the sum of two probabilities: (i) that of
the window being interesting, (ii) the maximum probability that the ﬁlter fails
to discard a noninteresting window. As such, the theorem gives a general framework 
to analyze probabilistic ﬁltration algorithms. An immediate consequence
of the result is the following:
Corollary 1. Under the same conditions as in Theorem 1, if in addition

PrW←Σw [W ∈ W] = max

W(cid:7)∈W Pr [F(W, P, k) = Check] = O

(cid:3)

(cid:2)
1/m2

,

then AvgG(n, P, k) = O(s · AvgF (w, P, k)). This also holds if E is the classical
O(m2) time algorithm.
The previous results suggests an obvious strategy for the design of d-ASM algorithms 
with errors. Indeed, it suﬃces to identify a small subset of windows
W ⊆ Σw that contain all windows of length w that are at distance at most k
of a pattern substring, and then design a ﬁlter F such that: (1) the probability
that F(W, P, k) = Check is high when W ∈ W (in order not to miss pattern oc-
currences), and (2) the probability that F(W, P, k) = Check is low when W (cid:8)∈ W
(in order to avoid running an expensive procedure over regions of the text where
there are no pattern occurrences).
by standard methods (running A repeatedly).

The next result is a simple observation whose proof we omit since it follows

On-Line Approximate String Matching with Bounded Errors

137
Proposition 1. Let A be a randomized algorithm that solves the d-ASM prob-
(cid:5))-error.
lem with (, 
– Let α ≤  = 
(cid:5)

< 1/2 and N = O(log(1/α)/(1−2)2). Then, there is
a randomized algorithm A(cid:5) that
solves the d-ASM problem with (α, α)-
error such that AvgA(cid:2)(n, P, k) = N · AvgA(n, P, k), MemA(cid:2)(n, P, k) =
MemA(n, P, k)+ O(log N), and where RndA(cid:2)(n, P, k) = N ·RndA(n, P, k).
– If A is one-sided, then there is a randomized algorithm A(cid:5)solving the d-ASM
problem with (N , 0)-error such that AvgA(cid:2)(n, P, k) = N · AvgA(n, P, k),
MemA(cid:2)(n, P, k) = MemA(n, P, k) +O(log N), and RndA(cid:2)(n, P, k) = N ·
RndA(n, P, k).

3 Algorithms for Approximate Searching with Errors

In this section we derive two probabilistic ﬁlters inspired on existing (errorless)
ﬁltration algorithms. Note that, according to the previous section, we focus on
the design of the window ﬁlters, and the rest follows from the general framework.

3.1 Algorithm Based on q-Gram Sampling
A q-gram is a substring of length q. Thus, a pattern of length m has (m− q + 1)
overlapping q-grams. Each error can alter at most q of the q-grams of the pattern,
and therefore (m− q + 1− kq) pattern q-grams must appear in any approximate
occurrence of the pattern in the text. Ukkonen’s idea [9] is to sequentially scan
the text while keeping count of the last q-grams seen. The counting is done
using a suﬃx tree of P and keeping the relevant information attached to the
m− q + 1 important nodes at depth q in the suﬃx tree. The key intuition behind
the algorithms design is that in random text it is diﬃcult to ﬁnd substrings of the
pattern of length q > logσ m. The opposite is true in zones of the text where the
pattern approximately occurs. Hence, by keeping count of the last q-grams seen
one may quickly ﬁlter out many bad pattern alignments.

We now show how to adapt the ideas mentioned so far in order to design
a probabilistic ﬁlter. The ﬁltering procedure randomly chooses several indices
i ∈ {1, . . . ,|W| − q + 1} and checks whether the q-gram Wi..i+q−1 is a pattern
substring. Depending on the number of q-grams that are present in the pattern
the ﬁlter decides whether or not to discard the window. See Algorithm 2 for a
formal description of the derived probabilistic ﬁlter Q-PE-F c,ρ,q, where c and ρ
are parameters to be tuned later. Using the ﬁlter as a subroutine for the generic
algorithm with errors described in Algorithm 1 gives rise to a procedure to which
we will henceforth refer to as Q-PE.
Let W be the collection of all windows in Σw for which at least β of its
(cid:5) = w − q + 1 be the number of
q-grams are substrings of the pattern. Let w
q-grams (counting repetitions) in a window of length w. Finally, let p denote the
probability that a randomly chosen q-gram is a substring of the pattern P , i.e.

p =

1
σq

· |{Pi..i+q−1 : i = 1, . . . , m − q + 1}| .

138

M. Kiwi, G. Navarro, and C. Telha

Algorithm 2. Probabilistic ﬁlter based on q-grams
1: procedure Q-PE-F c,ρ,q(W, P, k)
2: ctr ← 0
3: for i ∈ {1, . . . , c} do
Choose ji uniformly at random in {1, . . . , |W| − q + 1}
4:
if Wji..ji+q−1 is a substring of P then ctr ← ctr + 1
5:
6: if ctr > ρ · c then return Check else return Discard

(cid:4) W ∈ Σw, P ∈ Σm, k ∈ N

The following result shows that a window chosen randomly in Σw is unlikely

to be in W.
Lemma 1. Let β ≥ pw

(cid:5)

− 24(β − pw
(cid:5))2
25q(β + 2pw(cid:5))

(cid:6)
(cid:5). Then, PrW←Σw [W ∈ W] ≤ exp
.
(cid:5) let Yi be the indicator variable of the event “Wi..i+q−1
Proof. For i = 1, . . . , w
is a substring of P ” when W is randomly chosen in Σw. Clearly, Ex [Yi] =
(cid:7)
i=1 Yi ≥ β. Unfortunately, a standard
p. Moreover, W ∈ W if and only if
w
Chernoﬀ type bound cannot be directly applied given that the Yi’s are not
independent. Nevertheless, the collection {Y1, . . . , Yw(cid:2)} can be partitioned into q
families according to i mod q, each one an independent family of variables. The
desired result follows applying a Chernoﬀ type bound for so called q-independent
(cid:15)(cid:16)
families [4, Corollary 2.4].
Lemma 2. If W (cid:8)∈ W, then

(cid:2)

Pr [Q-PE-F ρ,c,q(W, P, k) = Check] ≤ exp

(cid:5)
ρc − cβ
w(cid:5)

(cid:6) (cid:5)

(cid:6)ρc

β
ρw(cid:5)

.

Proof. Let Xji denote the indicator of whether Wji..ji+1−1 turns out to be a
substring of the pattern P in line 5 of the description of Q-PE-F ρ,c,q. Note that
(cid:5) when W (cid:8)∈ W.
the Xji’s are independent, each with expectation at most β/w
The claim follows by a standard Chernoﬀ type bound from the fact that:

PrW←Σw [Q-PE-F ρ,c,q(W, P, k) = Check] = Pr

(cid:8)
c(cid:4)

(cid:9)

≥ ρ · c

Xji

,

where the probabilities are taken exclusively over the sequence of random bits
(cid:15)(cid:16)
of the probabilistic ﬁlter.
Lemma 3. If kq ≤ w

(cid:5)(1 − ρ), then Q-PE-F ρ,c,q is an α-ﬁlter for

i=1

(cid:5)

α ≤ exp

(1−ρ)c − ckq
w(cid:5)

(cid:6) (cid:5)

(cid:6)

c(1−ρ)

.

kq

w(cid:5)(1−ρ)

Proof. Let W ∈ Σw. Assume d(Pi..j, W ) ≤ k for some pattern substring Pi..j.
(cid:5) − kq of W ’s q-grams are substrings of P . Deﬁning Xji as in
Then, at least w
Lemma 2 we still have that the Xji’s are independent but now their expectation

On-Line Approximate String Matching with Bounded Errors

139
(cid:5). The claim follows by a standard Chernoﬀ type bound from

is at least 1− kq/w
the fact that:

Pr [Q-PE-F ρ,c,q(W, P, k) = Discard] = Pr

≤ ρ · c

Xji

(cid:9)

,

(cid:8)
c(cid:4)

i=1

where the probabilities are taken exclusively over the sequence of random bits
(cid:15)(cid:16)
of the probabilistic ﬁlter.
Theorem 2. If k < (m − 2 logσ m)/(1 + 4 logσ m), then Q-PE is a d-ASM
algorithm with one-sided error  = O((k logσ m/m)t) for any constant t > 0,
running in average time AvgQ-PE(n, P, k) = O(tn logσ m/m).
Proof. The result follows from Theorem 1 and Corollary 1.
Choose q = 2(cid:11)logσ m(cid:12), so p ≤ m/σq ≤ 1/m. Taking β = Θ(log2 m) where the
hidden constant is suﬃciently large, we have by Lemma 1 that PrW←W [W ∈ W]
= O(1/m2). By Lemma 2 and taking ρ = 1/2 and c a suﬃciently large constant,
we get that Pr [Q-PE-F ρ,c,q(W, P, k) = Check] = O(1/m2) when W (cid:8)∈ W.
∗ satisﬁes the hypothesis of
(cid:5)) = 4k logσ m/(m−k−
Lemma 3. Choose c(1−ρ) ≥ t and note that kq/((1−ρ)w
2 logσ m). Lemma 3 thus implies that Q-PE-F ρ,c,q has O((k logσ m/m)t)-error.
(W, P, k) = O(cq) =
(cid:15)(cid:16)

(cid:5)(1 − ρ)/q and observe that k < k

(w, P, k) = TimeQ-PE-F ρ,c,q

Clearly AvgQ-PE-F ρ,c,q

Now, let k

∗ = w

O(t logσ m).

3.2 Algorithm Based on Covering by Pattern Substrings

In 1994 Chang and Marr [2] proposed a variant of SET [1] with running time
O(n(k + logσ m)/m) for k/m ≤ 1/3 − O(σ
−1/2). As in SET, Chang and Marr
consider blocks of text of size (m− k)/2, and pinpoint occurrences of the pattern
by identifying blocks that approximately match a substring of the pattern. This
identiﬁcation is based on splitting the text into contiguous substrings of length
(cid:10) = t logσ m and sequentially searching the text substrings of length (cid:10) in the
pattern allowing errors. The sequential search continues until the total number
of errors accumulated exceeds k. If k errors occur before (m−k)/2 text characters
are covered, then the rest of the window can be safely skipped.

The adaptation of Chang and Marr’s approach to the design of probabilistic
ﬁlters is quite natural. Indeed, instead of looking at (cid:10)-grams sequentially we just
randomly choose suﬃciently many non-overlapping (cid:10)-substrings in each block.
We then determine the fraction of them that approximately appear in the pattern.
 If this fraction is small enough, then the block is discarded. See Algorithm 3
for a formal description of the derived probabilistic ﬁlter CM-PE-F c,ρ,(cid:6),g. Using 
the ﬁlter as a subroutine for the generic algorithm with errors described in
Algorithm 1 gives rise to a procedure to which we will henceforth refer to as
CM-PE.

Remark 1. Note that asm(S, P ) of Algorithm 3 can be precomputed for all values 
of S ∈ Σ(cid:6).

140

M. Kiwi, G. Navarro, and C. Telha

ctr ← 0
for i ∈ {1, . . . , c} do

Algorithm 3. Probabilistic ﬁlter based on covering by pattern substrings
1: procedure CM-PE-F c,ρ,(cid:5),g(W, P, k)
2:
3:
4:
5:
6:
7:

Choose ji uniformly at random in {1, . . . , (cid:7)w/(cid:7)(cid:8)}
if asm(W(ji−1)(cid:5)+1..ji(cid:5), P ) ≤ g
then ctr ← ctr + 1

if ctr > ρ · c then return Check else return Discard

(cid:4) W ∈ Σw, P ∈ Σm, k ∈ N

(cid:4) asm(S, P ) = mina≤b d(S, Pa..b)

The analysis of Algorithm 3 establishes results such as Lemmas 1-3, but concerning 
CM-PE-F ρ,c,(cid:6),g. We can derive the following (proof omitted due to lack
of space):
Theorem 3. If k < m/5, then CM-PE is a d-ASM algorithm with one sided
error  = (4k/(m − k))t, for any constant t > 0. Its average running time is
AvgQ-PE(n, P, k) = O(tn logσ m/m).

4 Experimental Results

We implemented the algorithms of Sections 3.1 and 3.2. We extracted three reallife 
texts of 50MB from Pizza&Chili (http://pizzachili.dcc.uchile.cl): English
text, DNA, and MIDI pitches. We used patterns of length 50 and 100, randomly
extracted from the text, and some meaningful k values. Each data point is the
average over 50 such search patterns, repeating each search 15 times in the case

 50

 40

 30

 20

 10

s
r
e

t
c
a
r
a
h
C

 
.

m
u
N

 0

 0

 50

 40

 30

 20

 10

s
r
e
t
c
a
r
a
h
C

 
.

m
u
N

 0

 0

m= 50, k= 5

m= 50, k= 8

DNA
English
MIDI

English
MIDI

 50

 40

 30

 20

 10

s
r
e

t
c
a
r
a
h
C

 
.

m
u
N

 0

 0

 50

 40

 30

 20

 10

s
r
e
t
c
a
r
a
h
C

 
.

m
u
N

 0

 0

English
MIDI

English
MIDI

 1

 2

 3

 4

 5

 6

Error (%)

m= 100, k= 16

 1

 2

 3

 4

 5

 6

Error (%)

 1

 2

 3

 4

 5

 6

Error (%)

m= 100, k= 10

 1

 2

 3

 4

 5

 6

Error (%)

Fig. 2. Experimental results for Q-PE. Straight horizontal lines correspond to the
errorless version. The y axis represents the number of character inspections times 1024.

On-Line Approximate String Matching with Bounded Errors

141

m= 50, k= 5

m= 50, k= 8

 50

 40

 30

 20

 10

s
r
e

t
c
a
r
a
h
C

 
.

m
u
N

 0

 0

 50

 40

 30

 20

 10

s
r
e
t
c
a
r
a
h
C

 
.

m
u
N

 0

 0

English
MIDI
DNA

 10

 20

 30

 40

 50

 60

Error (%)

m= 100, k= 10

English
MIDI
DNA

 10

 20

 30

 40

 50

 60

Error (%)

 50

 40

 30

 20

 10

s
r
e

t
c
a
r
a
h
C

 
.

m
u
N

 0

 0

 50

 40

 30

 20

 10

s
r
e
t
c
a
r
a
h
C

 
.

m
u
N

 0

 0

English
MIDI
DNA

 10

 20

 30

 40

 50

 60

Error (%)

m= 100, k= 16

English
MIDI
DNA

 10

 20

 30

 40

 50

 60

Error (%)

Fig. 3. Experimental results for CM-PE. Straight horizontal lines correspond to the
errorless version. The y axis represents the number of character inspections times 1024.

of the probabilistic algorithms. We measured the average number of character
inspections and the average percentage of missed occurrences.

We used the following setup for the algorithms. For q-gram algorithms (Section 
3.1), we used q = 4. Our preliminary results show that ρ = 0.7 is a good
choice. For covering by pattern substrings (Section 3.2), we used  = 0.2 and
ρ = 0.3. In our algorithms, we only moved parameter c in order to change the
accuracy/time trade-oﬀ. We compared our algorithms with the corresponding
errorless ﬁltering algorithms.

Figure 2 shows the experimental results for the q-gram based procedure, and
Fig. 3 for the covering by pattern substrings process. The errorless version of
the q-grams algorithm inspects all text characters. In contrast, our q-gram based
procedure achieves less than 1% error rate and looks at up to 6 times less characters 
on English and MIDI corpora. For our second algorithmic proposal, the
result of the comparison against the errorless version is not as good. Nevertheless,
 we emphasize that it beats the average-optimal (errorless) algorithm by a
wide margin, speciﬁcally it inspects about half the characters with 15% errors
on the English corpus.

5 Final Comments

In this paper we have advocated considering a new dimension of the approximate
string matching problem, namely the probability of missing an approximate occurrence.
 This relaxation is particularly natural for a problem that usually arises
when modeling processes where errors have to be tolerated, and it opens the door

142

M. Kiwi, G. Navarro, and C. Telha

to novel approaches to approximate string matching which break the averagecase 
lower bound of the original problem. In particular, we have shown that
much faster text scanning is possible if one allows a small probability of missing
occurrences. We achieved O(n logσ m/m) time (which is the complexity of exact
string matching, k = 0) with error probability bounded by any polynomial in
k/m. Empirically, we have shown that our algorithms inspect a fraction of the
text with virtually no mistakes.

We have just scratched the surface of this new area. In particular, we have not
considered ﬁltration algorithms that use sliding instead of ﬁxed windows. Slidingwindow 
algorithms have the potential of being more eﬃcient (cf. Fredriksson
and Navarro’s variant [3] with the original Chang and Marr’s average-optimal
algorithms [2]). It is not hard to design those variants, yet analyzing them is more
challenging. On the other hand, it is rather simple to extend our techniques to
multiple ASM. We also applied the techniques to indexed algorithms, where
the text can be preprocessed [6]. Several indexes build on sequential ﬁltration
algorithms, and thus adapting them is rather natural.

Finally, it is interesting to determine the average complexity of this relaxed
problem, considering the error probability  in the formula. This would give
an idea of how much can one gain by allowing errors in the outcome of the
search. For example, our algorithms break the Ω(nk/m) term in the problem
complexity, yet a term poly(k/m) appears in the error probability. Which are
the best tradeoﬀs one can achieve?

References

1. Chang, W., Lawler, E.: Sublinear approximate string matching and biological applications.
 Algorithmica 12(4-5), 327–344 (1994)

2. Chang, W., Marr, T.: Approximate string matching and local similarity. In: Proceedings 
of the 5th Annual Symposium on Combinatorial Pattern Matching, pp.
259–273. Springer, Heidelberg (1994)

3. Fredriksson, K., Navarro, G.: Average-optimal single and multiple approximate
string matching. ACM Journal of Experimental Algorithmics (article 1.4) 9 (2004)
4. Janson, S.: Large deviations for sums of partly dependent random variables. Random

Structure & Algorithms 24(3), 234–248 (2004)

5. Navarro, G.: A guided tour to approximate string matching. ACM Computing Surveys 
33(1), 31–88 (2001)

6. Navarro, G., Baeza-Yates, R., Sutinen, E., Tarhio, J.: Indexing methods for approximate 
string matching. IEEE Data Engineering Bulletin 24(4), 19–27 (2001)

7. Ron, D.: Property Testing. In: Handbook of Randomized Computing, volume II of

Combinatorial Optimization, vol. 9. Springer, Heidelberg (2001)

8. Rubinfeld, R., Kumar, R.: Algorithms column: Sublinear time algorithms. SIGACT

News 34(4), 57–67 (2003)

9. Ukkonen, E.: Approximate string-matching with q-grams and maximal matches.

Theoretical Computer Science 92, 191–211 (1992)

