8
1
0
2

 

y
a
M
3
2

 

 
 
]
S
D
.
s
c
[
 
 

2
v
5
3
8
1
0

.

1
1
6
1
:
v
i
X
r
a

Compressed Dynamic Range Majority and

Minority Data Structures ⋆

Travis Gagie1,2, Meng He3, and Gonzalo Navarro1,4,5

1 CeBiB — Center for Biotechnology and Bioengineering, Chile

2 School of Computer Science and Telecommunications, Diego Portales University, Chile,

3 Faculty of Computer Science, Dalhousie University, Canada, mhe@cs.dal.ca

4 Millenium Institute for Foundational Research on Data, Chile

5 Department of Computer Science, University of Chile, Chile, gnavarro@dcc.uchile.cl

travis.gagie@gmail.com

Abstract. In the range α-majority query problem, we are given a sequence
S[1..n] and a ﬁxed threshold α ∈ (0, 1), and are asked to preprocess S such
that, given a query range [i..j], we can eﬃciently report the symbols that
occur more than α(j − i + 1) times in S[i..j], which are called the range
α-majorities. In this article we ﬁrst describe a dynamic data structure
that represents S in compressed space — nHk + o(n lg σ) bits for any
k = o(lgσ n), where σ is the alphabet size and Hk ≤ H0 ≤ lg σ is the k-th
α lg lg n(cid:17) time
order empirical entropy of S — and answers queries in O(cid:16) lg n
while supporting insertions and deletions in S in O(cid:0) lg n
α (cid:1) amortized time.
We then show how to modify our data structure to receive some β ≥ α at
β lg lg n(cid:17) time, without
query time and report the range β-majorities in O(cid:16) lg n

increasing the asymptotic space or update-time bounds. The best previous
dynamic solution has the same query and update times as ours, but it
occupies O(n) words and cannot take advantage of being given a larger
threshold β at query time.
Not even static data structures have previously achieved compression in
terms of higher-order entropy. The smallest ones take nH0 + o(n)(H0 + 1)
bits and answer queries in O(f (n)/α) time or take (1 + ǫ)nH0 + o(n) bits
and answer queries in optimal O(1/α) time, where f (n) is any function
in ω(1) and ǫ is any constant greater than 0. By giving up updates, we
can improve our query time to O((1/α) lg lgw σ) on a RAM with word size
w = Ω(lg n) bits, which is only slightly suboptimal, without increasing
our space bound. Finally, we design the ﬁrst dynamic data structure for
range α-minority — i.e., ﬁnd a non-α-majority that occurs in a range —
and obtain space and time bounds similar to those for α-majorities. A
static version of this structure is also the ﬁrst α-minority data structure
achieving compression in terms of Hk.

⋆ Partially supported by Fondecyt grant 1-171058, Chile; NSERC, Canada; basal funds FB0001, Conicyt,
Chile; and the Millenium Institute for Foundational Research on Data, Chile. A preliminary partial version
of this article appeared in Proc. DCC 2017 [14].

1

Introduction

An α-majority in a sequence S[1..n] is a character that occurs more than αn times in
S, where the threshold α ∈ (0, 1). Misra and Gries [21] proposed a two-pass algorithm
for ﬁnding all α-majorities that runs in O(1/α) space and can be made to run in
linear time [9]. In contrast, any algorithm that makes only a constant number of
passes over S needs nearly linear space even to estimate the frequency of the mode
well [2], where the mode of S is deﬁned as its most frequent element. Thus, ﬁnding
α-majorities is often considered a practical way to ﬁnd frequent characters in large
ﬁles and is important in data mining [12,9,19], for example.

For the range α-majority query problem, we are asked to preprocess S such that,
given a query range [i..j], we can eﬃciently report the α-majorities of S[i..j], i.e.,
the symbols that occur more than α(j − i + 1) times in S[i..j]. Not surprisingly, this
problem seems easier than the range mode query problem [16,7], in which the query
asks for the most frequent element in the query range. Karpinski and Nekrich [20]
ﬁrst considered the range α-majority query problem and proposed a solution that
uses O(n/α) words to support queries in O((lg lg n)2/α) time. Durocher et al. [10]
presented the ﬁrst solution that achieves optimal O(1/α) query time, and their structure 
also occupies O(n/α) words. Subsequent researchers have worked to make the
space usage independent of α [15,8,5] and even to achieve compression [15,5]. Among
all these works, the most recent one is that of Belazzougui et al. [3,5], who showed
how to represent S using (1 + ǫ)nH0 + o(n) bits for any constant ǫ > 0 to answer
range α-majority queries in O(1/α) time, where H0 is the 0-th order empirical entropy 
of S. When more compression is desired, they also showed how to represent S
in nH0 + o(n)(H0 + 1) bits to support range α-majority in O(f (n)/α) time, for any
f (n) = ω(1). Their solutions work for variable α, that is, α is not known at construction 
time; the value of α is given together with the range [i, j] in each query. We refer
readers to their most recent paper [5] for a more thorough survey.

In the dynamic setting, we wish to maintain support for range α-majority queries
under the following update operations on S: i) insert(c, i), which inserts symbol c
between A[i−1] and A[i], shifting the symbols in positions i through n to positions i+1
through n+ 1, respectively; ii) delete(c, i), which deletes A[i], shifting the symbols in
positions i through n to positions i− 1 through n− 1, respectively. Elmasry et al. [11]
considered this setting, and designed an O(n)-word structure that can answer range
α-majority queries in O( lg n
α lg lg n ) time, supporting insertions and deletions in O( lg n
α )
amortized time. Before their work, Karpinski and Nekrich [20] also considered the
dynamic case, though they deﬁned the dataset as a set of colored points in 1D. With
a proper reduction [11], the solutions by Karpinski and Nekrich can also be used to
encode dynamic sequences, although the results are inferior to those of Elmasry et
al. [11]. More precisely, their data structures, when combined with the reduction [11],
can represent S in O(n/α) words of space, answer queries in time O( lg2 n
α ), and support
insertions and deletions in O( lg2 n
α ) amortized time. Alternatively, they can increase
the space cost to O( n lg n
α ), while decreasing the query and update times to O( lg n
α )

2

worst-case and amortized time, respectively. All the previous work for the dynamic
case requires α to be a ﬁxed value given at construction time.

A closely related problem is the range α-minority query problem, in which we
preprocess a sequence S such that, given a query range [i..j], we can eﬃciently report
one α-minority of S[i..j], i.e., a symbol that occurs at least once but not more than
α(j − i + 1) times in S[i..j], if such a symbol exists, and otherwise return that there
is no α-minority in the range. Chan et al. [8] studied this problem and designed
an O(n)-word data structure that answers range α-minority queries in O(1/α) time.
Belazzougui et al. [3,5] further designed succinct data structures for range α-minority.
They again presented two tradeoﬀs: they either represent S using (1 + ǫ)nH0 + o(n)
bits for any constant ǫ > 0 to answer range α-minority queries in O(1/α) time, or use
nH0 + o(n)(H0 + 1) bits and support range α-minority queries in O(f (n)/α) time, for
any f (n) = ω(1). The solutions of both Chan et al. [8] and Belazzougui et al. [3,5]
work for variable α. No work has been done for dynamic range α-minority queries.

Our results. In this article we ﬁrst consider the dynamic range α-majority problem for
ﬁxed α and improve the result of Elmasry et al. [11] in two key performance aspects:
we compress their space requirements while reducing their time on some more general
queries. We describe a data structure that uses even less space than Belazzougui et
al.’s static representation: nHk + o(n lg σ) bits for any k = o(lgσ n), where σ is the
alphabet size and Hk ≤ H0 ≤ lg σ is the k-th order empirical entropy of S. At the same
time, while still supporting updates in O( lg n
α ) amortized time, we can reduce query
times. Speciﬁcally, although we still answer range α-majority queries in O( lg n
α lg lg n )
time, like Elmasry et al., our data structure can receive a threshold β ≥ α at query
time and report the range β-majorities in O( lg n
α lg lg n ) time.
This type of queries is called range β-majority queries. Gagie et al. [15] and Chan et
al. [8] investigated reporting β-majorities in the static setting (i.e., variable α) but
no one has previously investigated doing so in the dynamic setting. In summary, our
time bounds are at least as good as those by Elmasry et al., our space bound is better
to a surprising degree, and our data structure can take advantage of being given a
larger threshold at query time in order to answer queries more quickly.

β lg lg n) time, rather than O( lg n

We also design the ﬁrst solution to the dynamic range α-minority query problem,
for ﬁxed α. We can represent S using nHk + 2n + o(n lg σ) bits for any k = o(lgσ n)
to answer range α-minority queries in O( lg n
α lg lg n ) time, supporting symbol insertions
and deletions in O( lg n

α lg lg n ) amortized time.

As a byproduct of our main contributions, static versions of our dynamic data
structures turn out to be the ﬁrst using as little as nHk + o(n lg σ) bits of space (+2n
bits in the case of α-minorities), for any k = o(lgσ n). They support range α-majority
queries for variable α, or α-minority queries for ﬁxed α, in time O((1/α) lg lgw σ).
This time is not far from the O(1/α) achieved by Gagie et al. [5] using (1 + ǫ)nH0 +
o(n) bits, for any constant ǫ > 0, or the times in (1/α) · ω(1) they achieve within
nH0 + o(n)(H0 + 1) bits of space. The time O(1/α) is optimal for α-majority queries.
A preliminary partial version of this article appeared in Proc. DCC 2017 [14].
Apart from a more complete and detailed presentation, this version includes the

3

support for β-majority queries, the static data structure for α-majority queries, and
the dynamic data structure for α-minority queries.

2 Preliminaries

In this section, we summarize some existing data structures that will be used in our
solution. One such data structure is designed for the problem of maintaining a string S
under insert and delete operations to support the following operations: access(i),
which returns S[i]; rank(c, i), which returns the number of occurrences of character
c in S[1..i]; and select(c, i), which returns the position of the i-th occurrence of c in
S. The following lemma summarizes the currently best compressed solution to this
problem, which also supports the extraction of an arbitrary substring in optimal time:

Lemma 1 ([22]). A string of length n over an alphabet of size σ can be represented
using nHk + o(n lg σ) bits for any k = o(lgσ n) to support access, rank, select,
insert and delete in O(lg n/ lg lg n) time. It also supports the extraction of a substring 
of length l in O(lg n/ lg lg n + l/ lgσ n) time.

Raman et al. [24] considered the problem of representing a dynamic integer sequence 
Q to support the following operations: sum(Q, i), which computes Pi
j=1 Q[j];
search(Q, x), which returns the smallest i with sum(Q, i) ≥ x; and update(Q, i, δ),
which sets Q[i] to Q[i]+δ. One building component of their solution is a data structure
for small sequences, which will also be used in our data structures:

Lemma 2 ([24]). A sequence, Q, of O(lgǫ n) nonnegative integers of O(lg n) bits
each, where 0 ≤ ǫ < 1, can be represented using O(lg1+ǫ n) bits to support sum,
search, and update(Q, i, δ) where |δ| ≤ lg n, in O(1) time. This data structure can
be constructed in O(lgǫ n) time, and requires a precomputed universal table occupying
O(nǫ′) bits for any ﬁxed ǫ′ > 0.

3 Compressed Dynamic Range Majority Data Structures

α(⌈ lg n

α⌈ lg n

In this section we design compressed dynamic data structures for range α-majority
queries. We deﬁne three diﬀerent types of queries as follows. Given an α-majority
query with range [i..j], we compute the size, r, of the query range as j − i + 1. If
r ≥ L, where L = ⌈ 1
lg lg n⌉)2⌉, then we say that this query is a large-sized query.
The query is called a medium-sized query if L′ < r < L, where L′ = ⌈ 1
lg lg n⌉⌉. If
r ≤ L′, then it is a small-sized query.
We represent the input sequence S using Lemma 1. This supports small-sized
queries immediately: By Lemma 1, we can compute the content of the subsequence
lgσ n ) = O( lg n
S[i..j], where [i..j] is the query range, in O( lg n
α lg lg n ) time. We
can then compute the α-majorities in S[i..j] in O(j − i + 1) = O( lg n
α lg lg n ) time using
the algorithm of Misra and Gries [21]. Thus it suﬃces to construct additional data
structures only for largeand 
medium-sized queries.

lg lg n + j−i+1

4

3.1 Supporting Large-Sized Range α-Majority Queries

1

To support large-sized queries, we construct a weight-balanced B-tree [1] T with
branching parameter 8 and leaf parameter L. We augment T by adding, for each
node, a pointer to the node immediately to its left at the same level, and another
pointer to the node immediately to its right. These pointers can be maintained easily
under updates, and will not aﬀect the space cost of T asymptotically. Each leaf of T
represents a contiguous subsequence, or block, of S, and the entire sequence S can be
obtained by concatenating all the blocks represented by the leaves of T from left to
right. Each internal node of T then represents a block that is the concatenation of all
the blocks represented by its leaf descendants. We number the levels of T by 0, 1, 2, . . .
from the leaf level to the root level. Thus level a is higher than level b if a > b. Let
v be a node at the l-th level of T , and let B(v) denote the block it represents. Then,
by the properties of weight-balanced B-trees, if v is a leaf, the length of its block,
denoted by |B(v)|, is at least L and at most 2L − 1. If v is an internal node, then
2 · 8l · L < |B(v)| < 2 · 8l · L. We also have that each internal node has at least 2 and
at most 32 children.
We do not store the actual content of a block in the corresponding node of T .
Instead, for each v, we store the size of the block that it represents, and in addition,
compute and store information in a structure C(v) called candidate list about symbols
that can possibly be the α-majorities of subsequences that meet certain conditions.
More precisely, let l be the level of v, u be the parent of v, and SB(v) be the concatenation 
of the blocks represented by the node immediately to the left of u at level
l + 1, the node u, and the node immediately to the right of u at level l + 1. Then C(v)
contains each symbol that appears more than αbl times in SB(v), where bl = 1
2 · 8l · L
is the minimum size of a block at level l. Since the maximum length of each block at
level l + 1 is 4bl+1 = 32bl, we have |SB(v)| ≤ 96bl, and thus |C(v)| = O(1/α). To show
the idea behind the candidate lists, we say that two subsequences touch each other if
their corresponding sets of indices in S are not disjoint. We then observe that, since
the size of any block at level l + 1 is greater than 8bl, any subsequence S[i..j] touching
B(v) is completely contained in SB(v) if r = j − i + 1 is within (bl, 8bl). Since each
α-majority in S[i..j] appears at least αr > αbl times, it is also contained in C(v).
Therefore, to ﬁnd the α-majority in S[i..j], it suﬃces to verify whether each element
in C(v) is indeed an answer; more details are to be given in our query algorithm later.
Even though it only requires O(|SB(v)|) time to construct C(v) [21], it would
be costly to reconstruct it every time an update operation is performed on SB(v).
To make the cost of maintaining C(v) acceptable, we only rebuild it periodically by
adopting a strategy by Karpinski and Nekrich [20]. More precisely, when we construct
C(v), we store symbols that occur more than αbl/2 times in SB(v). We also keep a
counter U(v) that we increment whenever we perform insert or delete in SB(v).
Only when U(v) reaches αbl/2 do we reconstruct CB, and then we reset U(v) to 0.
Since at most αbl/2 updates can be performed to |SB(v)| between two consecutive
reconstructions, any symbol that becomes an α-majority in |SB(v)| any time during
these updates must have at least αbl/2 occurrences in SB(v) before these updates
are performed. Thus we can guarantee that any symbol that appears more than αbl

5

times in SB(v) is always contained in C(v) during updates. The size of C(v) is still
O(bl/α), and, as will be shown later, it only requires O((lg n)/α) amortized time per
update to S to maintain all the candidate lists.

We also construct data structures to speed up a top-down traversal in T . These
data structures are deﬁned for the marked levels of T , where the k-th marked level is
level k⌈(1/6) lg lg n⌉ of T for k = 0, 1, . . .. Given a node v at the k-th marked level, the
number of its descendants at the (k − 1)-st marked level is at most 32⌈(1/6) lg lg n⌉−1 ≤
32(1/6) lg lg n = lg5/6 n. Thus, the sizes of the blocks represented by these descendants,
when listed from left to right, form an integer sequence, Q(v), of at most lg5/6 n
entries. We represent Q(v) using Lemma 2, and store a sequence of pointers P (v), in
which P (v)[i] points to the i-th leftmost descendant at the (k − 1)-st marked level.
We next prove the following key lemma regarding an arbitrary subsequence S[i..j]
of length greater than L, which will be used in our query algorithm:

3 lg 2r

L − 1⌉ whose block touches S[i..j].

Lemma 3. If r = j − i + 1 > L, then each α-majority element in S[i..j] is contained
in C(v) for any node v at level l = ⌈ 1
Proof. Let u be v’s parent. Then S[i..j] also touches u, and u is at level l + 1. Let u1
and u2 be the nodes immediately to the left and right of u at level l + 1, respectively.
Let bl and bl+1 denote the minimum block size represented by nodes at level l and
l + 1 of T , respectively. Then, by the properties of weight-balanced B-trees, if l > 0,
bl = 1
2 · 8l · L = 1
L · L = r. When l = 0, bl = L < r. Thus, we
always have bl < r. Therefore, any α-majority of S[i..j] occurs more than αr > αbl
times in S[i..j].

L −1⌉ · L < 1

2 · 8⌈ 1

2 · 8

3 lg 2r

3 lg 2r

1

1

3 lg 2r

3 lg 2r

2 · 8⌈ 1

L ⌉ · L ≥ 1

On the other hand, bl+1 = 1

L · L = r. Since S[i..j]
touches B(u), this inequality means that S[i..j] is entirely contained in either the
concatenation of B(u1) and B(u), or the concatenation of B(u) and B(u2). In either
case, S[i..j] is contained in SB(v). Since any α-majority of S[i..j] occurs more than
αbl times in S[i..j], it also occurs more than αbl times in SB(v). As C(v) includes
any symbol that appears more than αbl times in SB(v), any α-majority of S[i..j] is
contained in C(v).
⊓⊔

2 · 8

We now describe our query and update algorithms, and analyze space cost.

Lemma 4. Large-sized range α-majority queries can be supported in O( lg n

α lg lg n ) time.

3 lg 2r

Proof. Let [i..j] be the query range, r = j − i + 1 and l = ⌈ 1
L − 1⌉. We ﬁrst look
for a node v at level l whose block touches S[i..j]. The obvious approach is to perform
a top-down traversal of T to look for a node at level l whose block contains position i.
During the traversal, we make use of the information about the lengths of the blocks
represented by the nodes of T to decide which node at the next level to descend to,
and to keep track of the starting position in S of the block represented by the node
that is currently being visited. More precisely, suppose we visit node u at the current
level as we have determined previously that B(u) contains S[i]. We also know that
the ﬁrst element in B(u) is S[p]. Let u1, u2, . . . , ud denote the children of u, where
d ≤ 32. To decide which child of u represents a block that contains S[i], we retrieve

6

the block that each node represents, these steps use constant time.

the lengths of all |B(uk)|’s, and look for the smallest q such that p+Pq
position of its block in S is p + Pq−1

k=1 |B(uk)| > i.
Node uq is then the node at the level below whose block contains S[i], and the starting
k=1 |B(uk)|. As d ≤ 32 and we store the length of
However, if we follow the approach described in the previous paragraph, we would
use O(lg n) time in total, as T has O(lg n) levels. Thus we make use of the additional
data structures stored at marked levels to speed up this process. If there is no marked
level between the root level and l, then the top down traversal only descends O(lg lg n)
levels, requiring O(lg lg n) time only. Otherwise, we perform the top-down traversal
until we reach the highest marked level. Let x be the node we visit at the highest
marked level. As Q(x) stores the lengths of the blocks at the next marked level, we can
perform a search operation in Q(x) and then follow an appropriate pointer in P (x)
to look for the node y at the second highest level that contains S[i], and perform a sum
operation in Q(x) to determine the starting position of B(y) in S. These operations
require constant time. We repeat this process until we reach the lowest marked level
above level l, and then we descend level by level until we ﬁnd node v. As there are
O(lg n/ lg lg n) marked levels, the entire process requires O(lg n/ lg lg n) time.

By Lemma 3, we know that the α-majorities of S[i..j] are contained in C(v).
We then verify, for each symbol, c, in C(v), whether it is indeed an α-majority by
computing its number, m, of occurrences in S[i..j] and comparing m to αr. As m =
rank(c, j) − rank(c, i − 1), m can be computed in O(lg n/ lg lg n) time by Lemma 1.
As |C(v)| = O(1/α), it requires O( lg n
α lg lg n) time in total to ﬁnd out which of these
symbols should be included in the answer to the query. Therefore, the total query
time is O( lg n
⊓⊔
Lemma 5. The data structures described in Section 3.1 can be maintained in O( lg n
α )
amortized time under update operations.

α lg lg n ) = O( lg n

lg lg n + lg n

α lg lg n ).

Proof. We show only how to support insert; the support for delete is similar.

To perform insert(c, i), we ﬁrst perform a top down traversal to look for the node
v at level 0 whose block contains S[i]. During this traversal, we descend level by level
as in Lemma 4, but we do not use the marked levels to speed up the process. For
each node u that we visit, we increment the recorded length of B(u). In addition, we
update the counters U stored in the children of u and in the children of the two nodes
that surround u. There are a constant number of these nodes, and they can all be
located in constant time by following either the edges of T , or the pointers between
two nodes that are next to each other at the same level where we augment T .

When incrementing the counter U of each node, we ﬁnd out whether the candidate
list of this node has to be rebuilt. To reconstruct the candidate list of a node x at
level l, we ﬁrst compute the starting and ending positions of SB(x) in S. This can be
computed in constant time because, during the top down traversal, we have already
computed the starting and ending positions of B(v) in S, and the three nodes whose
blocks form SB(x), as well as the sizes of these three blocks, can be retrieved by
following a constant number of pointers starting from v. We then extract the content
of SB(x). As |SB(x)| ≤ 96bl (see discussions earlier in this section) and bl ≥ L, by

7

Lemma 1, SB(x) can be extracted from S in O(bl) time. We next compute all the
symbols that appear in SB(x) more than αbl/2 times in O(bl) time [21], and these
are the elements in the reconstructed C(x). Since the counter U(x) has to reach αbl/2
before C(x) has to be rebuilt, the amortized cost per update is O(1/α).

If u is at a marked level, we perform a search operation in O(1) time to locate
the entry of Q(u) that corresponds to the node at the next lower marked level whose
block contains i, and perform an update, again in O(1) time, to increment the value
stored in this entry. So far we have used O(1/α) amortized time for each node we
visit during the top-down traversal. Since T has O(lg n) levels, the overall cost we
have calculated up to this point is O((lg n)/α) amortized time.

When a node, z, at level l of T splits into two nodes z1 and z2, where z1 is to
the left of z2, we construct C(z1) and C(z2) in O(bl) time. In addition, for any node
y that is a child of z1 or z2, or a child of the node immediately to the left of z1 or
the right of z2 at the same level, we reconstruct C(y) in O(bl) time. As there are
a constant number of such nodes, all these structures can be reconstructed in O(bl)
time. If l is a marked level, but it is not the lowest marked level, we also build Q(z1),
Q(z2), P (z1), and P (z2). We also have to rebuild P (z′) and Q(z′), where z′ is the
lowest ancestor of z that is on a marked level. All this takes O(lg5/6 n) = o(bl) time.
By the properties of a weight-balanced B-tree, after a node at level l has been split, it
requires at least 1
2 · 8l · L = bl insertions before it can be split again. Therefore, we can
amortize the cost of reconstructing these data structures over the insertions between
reconstructions, and each insert is thus charged with O(1) amortized cost. As each
insert may cause one node at each level of T to split, the overall cost charged to an
insert operation is thus O(lg n).

Finally, update operations may cause the value of L to change. For this to happen,
 the value of ⌈ lg n
lg lg n⌉ must change, and this requires Ω(n) updates. When this
happens, we rebuild our data structure in O(n lg n) time: we can easily precompute
the structures for each level of T in linear time and there are O(lg n) levels. Thus,
such rebuilding incurs O(lg n) amortized time for each update. To summarize, insert
can be supported in O((lg n)/α) amortized time.
⊓⊔

Lemma 6. The data structures described in Section 3.1 occupy o(n lg σ) bits.

lg n

Proof. As T has O(n/L) nodes, the structure of T , pointers between nodes at the same
level, as well as counters and block lengths stored with the nodes, occupy O(n/L ×
lg n) = O( αn(lg lg n)2
) bits in total. Each candidate list can be stored in O((lg σ)/α) bits,
so the candidate lists stored in all the nodes use O(n/L × (lg σ)/α) = O( n lg σ(lg lg n)2

)
bits in total. The size of the structures Q(v) and P (v) can be charged to the pointed
nodes, so there are O(n/L) entries to store. As each entry of Q(v) uses O(lg n) bits,
) bits. The same analysis applies

lg2 n

all the Q(v)s occupy O(n/L × lg n) = O( αn(lg lg n)2
to P (v). Therefore, the data structures described in this section use O( αn(lg lg n)2
n lg σ(lg lg n)2

lg n

lg n

+
⊓⊔

lg2 n

) = o(n lg σ) bits.

8

3.2 Supporting Medium-Sized Range α-Majority Queries

We could use the same structures designed in Section 3.1 to support medium-sized
queries if we simply set the leaf parameter of T to be L′ instead of L, but then
the resulting data structures would not be succinct. To save space, we build a data
structure D(v) for each leaf node v of T . Our idea for supporting medium-sized queries
is similar to that for large-sized queries, but since the block represented by a leaf node
of T is small, we are able to simplify the idea and the data structures in Section 3.1.
Such simpliﬁcations allow us to maintain a multi-level decomposition of B(v) in a
hierarchy of lists instead of in a tree, which are further laid out in one contiguous
chunk of memory for each leaf node of T , to avoid using too much space for pointers.
We now describe this multi-level decomposition of B(v), which will be used to
deﬁne the data structure components of D(v). As we deﬁne one set of data structure
components in D(v) for each level of this decomposition, we use D(v) to refer to
both the data structure that we build for B(v) and the decomposition of B(v). To
distinguish a level of D(v) from a level of T , we number each level of D(v) using a nonpositive 
integer. At level −l, for l = 0, 1, 2, . . . ,⌈lg(L/L′)−1⌉, B(v) is partitioned into
miniblocks of length between L/2l and L/2l−1. Note that the level 0 decomposition
contains simply one miniblock, which is B(v) itself, as the length of any leaf block in
T is between L and 2L already. We deﬁne ml = L/2l, which is the minimum length
of a miniblock at level −l. As L′ < m⌈lg(L/L′)−1⌉ ≤ 2L′, the minimum length of a
miniblock at the lowest level, i.e., level −⌈lg(L/L′) − 1⌉, is between L′ and 2L′.
For each miniblock M at level −l of D(v), we deﬁne its predecessor, pred(M), as
follows: If M is not the leftmost miniblock at level −l of D(v), then pred(M) is the
miniblock immediately to its left at the same level. Otherwise, if v is not the leftmost
leaf (pred(M) is null otherwise), let v1 be the leaf immediately to the left of v in T ,
and pred(M) is deﬁned to be the rightmost miniblock at level −l of D(v1). Similarly,
we deﬁne the successor, succ(M), of M as the miniblock immediately to the right of
M at level −l of D(v) if such a miniblock exists. Otherwise, succ(M) is the leftmost
miniblock at level −l of D(v2) where v2 is the leaf immediately to the right of v in
T if v2 exists, or null otherwise. Then, the candidate list, C(M), of M contains each
symbol that occurs more than αml/2 times in the concatenation of M, pred(M) and
succ(M). To maintain C(M) during updates, we use the same strategy in Section 3.1
that is used to maintain C(v). More speciﬁcally, we store a counter U(M) so that
we can rebuild C(M) after exactly αml/4 update operations have been performed to
M, pred(M) and succ(M). Whenever we perform the reconstruction, we include in
C(M) each symbol that occurs more than αml/4 times in the concatenation of M,
pred(M) and succ(M). Since |pred(M)| + |M| + |succ(M)| ≤ 6ml, the number of
symbols included in C(M) is at most 24/α.
The precomputed information for each miniblock M includes |M|, C(M), and
U(M). These data for miniblocks at the same level, −l, of D(v) are chained together
in a doubly linked list Ll(v). D(v) then contains these O(lg(L/L′)) = O(lg lg n)
lists. We cannot, however, aﬀord storing each list in the standard way using pointers
of O(lg n) bits each, as this would use too much space. Instead, we lay them out
in a contiguous chunk of memory as follows: We ﬁrst observe that the number of

9

miniblocks at level −l of D(v) is less than 2L/(L/2l) = 2l+1. Thus, the total number
of miniblocks across all levels is less than 2 · 2⌈lg(L/L′)−1⌉+1 − 1 < 4L/L′. We then
use an array A(v) of ⌈4L/L′⌉ ﬁxed-size slots to store D(v), and each slot stores the
precomputed information of a miniblock.
To determine the size of a slot, we compute the maximum number of bits needed
to encode the precomputed information for each miniblock M. C(M) can be stored
in ⌈24/α⌉ · ⌈lg σ⌉ bits. As M has less than 2L elements, its length can be encoded
in ⌈lg(2L)⌉ bits. The counter U(M) can be encoded in ⌈lg(αml/4)⌉ < ⌈lg(αL/2)⌉ ≤
⌈lg(L/2)⌉ bits. The two pointers to the neighbours of M in the linked list can be
encoded as the indices of these miniblocks in the memory chunk. Since there are
⌈4L/L′⌉ slots, each pointer can be encoded in ⌈lg⌈4L/L′⌉⌉ bits. Therefore, we set the
size of each slot to be ⌈24/α⌉ · ⌈lg σ⌉ + 2⌈lg L⌉ + 2⌈lg⌈4L/L′⌉⌉ bits.
We prepend this memory chunk with a header. This header encodes the indices
of the slots that store the head of each Ll(v). As there are ⌈lg(L/L′)⌉ levels and each
index can be encoded in ⌈lg⌈4L/L′⌉⌉ bits, the header uses ⌈lg(L/L′)⌉ · ⌈lg⌈4L/L′⌉⌉
bits. Clearly our memory management scheme allows us to traverse each doubly
linked list Ll(v) easily. When miniblocks merge or split during updates, we need to
perform insertions and deletions in the doubly linked lists. To facilitate these updates,
we always store the precomputed information for all miniblocks in D(v) in a preﬁx
of A(v), and keep track of the number of used slots of A(v). When we perform an
insertion into a list Ll(v), we use the ﬁrst unused slot of A to store the new information,
and update the header if the newly inserted list element becomes the head. When we
perform a deletion, we copy the content of the last used slot (let M ′ be the miniblock
that corresponds to it) into the slot corresponding to the deleted element of Ll(v).
We also follow the pointers encoded in the slot for M ′ to locate the neighbours of
M ′ in its doubly linked list, and update pointers in these neighbours that point to
M ′. If M ′ is the head of a doubly linked list (we can determine which list it is using
|M ′|), we update the header as well. The following lemma shows that our memory
management strategy does, indeed, save space:

Lemma 7. The data structures described in Section 3.2 occupy o(n lg σ) bits.

Proof. We ﬁrst analyze the size of the memory chunk storing D(v) for each leaf v of
T . By our analysis in previous paragraphs, we observe that the header of this chunk
uses O((lg lg n)2) bits. Each slot of A(v) uses O( lg σ
α + lg lg n) bits, and A(v) has
O(lg n/ lg lg n) entries. Therefore, A(v) occupies O( lg σ lg n
α lg lg n + lg n) bits. Hence the total
size of the memory chunk of each leaf of T is O( lg σ lg n
α lg lg n +lg n) bits. As there are O(n/L)
leaves in T , the data structures described in this section use O( n lg σ lg lg n
) =
o(n lg σ) bits.
⊓⊔

lg n + αn(lg lg n)2

lg n

We now show how to support query and update operations.

Lemma 8. Medium-sized range α-majority queries can be supported in O( lg n
time.
Proof. Let [i..j] be the query range and let r = j − i + 1. We ﬁrst perform a top down
traversal in T to locate the leaf, v, that represents a block containing S[i] in O( lg n
lg lg n )

α lg lg n )

10

time using the approach described in the proof of Lemma 4. In this process, we can
also ﬁnd the starting position of B(v) in S.

We next make use of D(v) to answer the query as follows. Let l = ⌈lg(L/r) − 1⌉.
As ml = L/2⌈lg(L/r)−1⌉, we have ml/2 ≤ r < ml. We then scan the list Ll(v) to look
for a miniblock, M, that contains S[i] at level −l. This can be done by ﬁrst locating
the head of Ll(v) from the header of the memory chunk that stores D(v), and then
performing a linear scan, computing the starting position of each miniblock in Ll(v)
along the way. As Ll(v) has at most O(L/L′) = O( lg n
lg lg n ) entries, we can locate M in
O( lg n
lg lg n ) time. Since ml > r, S[i..j] is either entirely contained in the concatenation of
pred(M) and M, or in the concatenation of M and succ(M). Thus each α-majority
of S[i..j] must occur more than αr > αml/2 times in the concatenation of pred(M),
M and succ(M). Therefore, each α-majority of S[i..j] is contained in C(M). We
can then perform rank operations in S to verify whether each symbol in C(M) is
indeed an α-majority of S[i..j]. As C(M) has O(1/α) symbols, this process requires
O( lg n
⊓⊔

α lg lg n) time. The total query time is hence O( lg n

α lg lg n ).

Lemma 9. The data structures described in Section 3.2 can be maintained in O( lg n
lg lg n

lg lg n+

α ) amortized time under update operations.

Proof. We show only how to support insert; the support for delete is similar.

To perform insert(c, i), we ﬁrst perform a top down traversal in T to locate the
leaf, v, that represents a block containing S[i] in O( lg n
lg lg n) time. We then increment the
recorded lengths of all the miniblocks that contain S[i]. We also increment the counters
U of these miniblocks, as well as the counters of their predecessors and successors.
All the miniblocks whose counters should be incremented are located in D(v), D(v1)
and D(v2), where v1 and v2 are the leaves immediately to the left and right of v in
T . At each level −l, we scan each doubly linked list Ll(v), Ll(v1) and Ll(v2) to locate
these miniblocks. Since D(v), D(v1) and D(v2) have O( lg n
lg lg n ) miniblocks in total over
all levels, it requires O( lg n

lg lg n) to ﬁnd these miniblocks and update them.

The above process can ﬁnd all these miniblocks, as well as their starting and
ending positions in S. It may be necessary to reconstruct the candidate list of these
miniblocks. Similarly to the analysis in the proof of Lemma 5, the candidate list of
each of these miniblocks can be maintained in O(1/α) amortized time. Since there are
O(lg lg n) levels in D(v), D(v1) and D(v2), and only a constant number of miniblocks
needing rebuilding at each level, O((lg lg n)/α) amortized time will be required to
reconstruct all of them.

An insertion may also cause a miniblock M to split. As in the proof of Lemma 5, we
compute the candidate lists and other required information for the miniblocks created
as a result of the split in time linear in the length of M, and amortize the cost over the
insertions that lead to the split. As the number of these insertions is also proportional
to the length of M, the amortized cost is again O(1). As there can possibly be a split at
each level of D(v), it requires O(lg lg n) amortized time to handle them. Finally, when
the value of L′ changes, we rebuild all the data structures designed in this section.
Since these data structures are constructed for O(lg lg n) levels and the structures for

11

each level can be rebuilt in linear time, this process incurs O(lg lg n) amortized time.
Therefore, the total time required to support insert is O( lg n
⊓⊔
Combining Lemma 1 and Lemmas 4-9, we obtain our ﬁrst result, when the struclg 
lg n + lg lg n

α ).

ture is queried for α-majorities.

Theorem 1. For any 0 < α < 1, a sequence of length n over an alphabet of size σ
can be represented using nHk + o(n lg σ) bits for any k = o(lgσ n) to answer range
α-majority queries in O( lg n
α lg lg n) time, and to support symbol insertions and deletions
in O( lg n

α ) amortized time.

4 Supporting β-Majorities

Theorem 1 supports range α-majority queries, where α is chosen at construction time.
We now enhance our data structure to ﬁnd range β-majorities, for any β ≥ α given
at query time together with the interval [i..j]. While it is easy to answer those queries
in time O( lg n
β lg lg n). Updates are still carried out
in amortized time O( lg n

α lg lg n), our goal is to reach time O( lg n

α ).

Although we have not used this in previous sections, note that we can focus our
attention in the case β > 1/σ, since otherwise we can directly check the range of S
for each of the σ symbols c, reporting those where rank(c, j) − rank(c, i − 1) > βr,
all in time O( σ lg n
β lg lg n ). Thus, at construction time we can set α to 1/σ if α
turns out to be smaller. This implies, in particular, that all our 1
α in the complexities
can be replaced by min( 1
α, σ). We will also use the fact that lg 1
α = O(lg σ) ∩ O(lg n).
Similarly, it makes sense to consider α ≤ 1/2 only, as otherwise we use the solution
for α = 1/2 and report only the true α-majorities found, within the same complexity.

lg lg n ) = O( lg n

4.1 Large and Medium-Sized Intervals

For large and medium-sized intervals, it is not diﬃcult to answer β-majority queries
within the desired time. Note that, in those cases, the crux of the solution is to verify
a list of candidates, C(v) in the block v (for large intervals) or C(M) in the miniblock
M (for medium-sized intervals), both of size O(1/α). It is suﬃcient that those lists
are sorted by decreasing frequency of the elements and that we stop verifying them
when reaching an element with frequency below βr. Since r > bl in large intervals
and r ≥ ml/2 in medium-sized intervals, there can be only O(1/β) such candidates
and we solve the query in time O( lg n
We can maintain those list only approximately sorted, however. When the lists
are created, we sort them by decreasing frequency. However, the elements can later
change their frequency upon updates, but only by a maximum of γ = αbl/2 (for large
intervals) or γ = αml/4 (for medium-sized intervals), before we rebuild the lists. We
do not store symbol frequencies, just their order. This ordering is not modiﬁed upon
updates, only when the lists are rebuilt. Therefore, we can stop verifying safely only
when the frequency we compute on the ﬂy drops below βr − γ, since this guarantees
than the next element cannot have a current frequency over βr. Since r > bl for large

β lg lg n ).

12

intervals and r ≥ ml/2 for medium-sized intervals, it holds that βr − γ > βbl/2 for
large intervals and βr − γ ≥ βml/4 for medium-sized intervals, and therefore the
resulting complexity is in both cases O( lg n

β lg lg n ).

4.2 Small Intervals

α⌈ lg n

lg lg n⌉.

The small ranges, which were solved by brute force, pose a more diﬃcult problem,
because now we cannot aﬀord scanning a block of S of size O( lg n
α lg lg n ). To handle small
ranges, we add further structures to our tree leaves, which contain L′ to 2L′ elements
for L′ = ⌈ 1
lg lg n⌉⌉. The leaves will be further partitioned into halves repeatedly in
lg(1/α) levels, until reaching size between L∗ and 2L∗, for L∗ = ⌈ lg n
These additional levels, numbered −l∗ for l = 0, . . . ,⌊lg(L′/L∗)⌋, are organized
much as the miniblocks of Section 3.2. Indeed, our highest level, −0∗, is the same
level, −⌈lg(L/L′)⌉, as the deepest one of Section 3.2. The main diﬀerence is that, in
the new levels, not only the sizes ml∗ are halved as we descend, but also the majority
thresholds are doubled: we use the value αl∗ = α · 2l∗ to deﬁne the candidate lists
C(M) at level −l∗. In our last level, −l∗ = −⌊lg(L′/L∗)⌋, it holds that αl∗ = Θ(1)
(precisely, αl∗ > 1/2) and ml∗ = O( lg n
Because, at each level −l∗, C(M) can store at most 24/αl∗ = 24/(α 2l∗) elements,
we do not store together the information of all the miniblocks descending from a leaf
block, as done in Section 3.2. Rather, we stratify it per level −l∗. For each leaf block,
we have an array of O(lg 1
α ) entries, one per level −l∗, to memory areas of miniblocks
of that level descending from the leaf block. Within each memory area, the slots are
of the same size, as in Section 3.2.

lg lg n ) (precisely, L∗ ≤ ml∗ ≤ 2L∗).

We also impose further structure to the linked lists of miniblocks inside each
memory area: the list nodes are not anymore linked, but they are the leaves of a
B-tree of arity B to 2B, for B = √lg n. Since the list at level −l∗ has 2l∗ < 1
α + 1
elements, the B-tree is of height O(lg(1/α)/ lg lg n). Each B-tree node stores the up to
2√lg n subtree sizes (measured in terms of number of positions of S stored in all the
subtree leaves) using Lemma 2, which allows routing the search for a given position
in S in constant time per B-tree node. To facilitate memory management, we have
one memory area for the B-tree nodes and another for the list nodes, so that memory
slots are of the same size within each area.

Finally, we use a new arrangement to store the lists C(M) in these miniblocks.
Instead of representing the candidate symbols directly, we store one position of
pred(M) · M · succ(M) where the symbol appears. The actual symbol can then
be obtained with an access to S in time O( lg n
lg lg n ). Further, we sort all the O(1/αl∗)
positions of the candidates as follows: The primary criterion for the sort is ⌈lg(1/f )⌉,
where f is the relative frequency of the element in pred(M) · M · succ(M). The secondary 
criterion, when the ﬁrst produces ties, is the increasing order of the positions
in pred(M) · M · succ(M) we use to represent the symbols.
Therefore, the list C(M) is partitioned into O(lg n) chunks of symbols with the
same quantized frequency, qf = ⌈lg(1/f )⌉, and the positions stored are increasing
within each chunk. Those chunks are then represented as the diﬀerences between
consecutive positions using γ-codes [6], and a diﬀerence of zero is used to signal the

13

end of a chunk. By Jensen’s Inequality, the number of bits required to represent k
diﬀerences that add up to m is O(k lg(m/k)).6 Since there are at most 2qf elements
with quantized frequency qf , their chunk is represented with O(2qf (lg(m) − qf )) bits.
Adding up to relative frequency f ∗ = αl∗/24 (i.e., the minimum for a candidate stored
in C(M)), the total space adds up to the order of

qf =lg⌈24/αl∗ ⌉

Xqf =0

2qf (lg m − qf ) = 2(cid:24) 24

αl∗(cid:25)(cid:18)lg m − lg(cid:24) 24

αl∗(cid:25) + 1(cid:19) − 3 = O(cid:18) lg(αl∗m)

αl∗ (cid:19) ,

and since in our case m = O(ml∗) = O(
represent C(M) is O((1/αl∗) lg lg n).

lg n

α2l∗ lg lg n ) = O(

lg n

αl∗ lg lg n), the total space to

Lemma 10. The data structures described in Section 4 occupy o(n lg σ) bits.

Proof. The analysis is analogous to that of Lemma 7. The number of miniblocks at
level −l∗ of each array A(v) is at most 2L/(L′/2l∗) = O( lg n
lg lg n · 2l∗+1). The size of the
miniblocks includes the space to store the list of candidates, O((1/αl∗) lg lg n), plus
a constant number of (lg L)-bit counters and pointers, which require O(lg lg n + lg 1
α)
further bits. The B-tree nodes, stored in another memory area, require O(B lg L) bits
per node, but have O(1/B) nodes per miniblock M, thus their space is already covered
in our formula. All this adds up to O((1/αl∗) lg lg n + lg 1
α) bits per miniblock, which
α + lg n lg 1
multiplied by the number of miniblocks at level −l∗ of A(v) yields O( lg n
lg lg n ·
α levels −l∗, we obtain O( lg n lg 1
2l∗) bits. Summing up this space over all the lg 1
α +
lg n lg 1
α lg lg n ) = O( lg n lg 1
) bits. Finally, multiplying this space by the O(n/L) leaves, we
α
obtain O( n lg 1
α (lg lg n)2
lg n

) = o(n lg σ) bits.
α) global pointers for the memory areas of each level −l∗, which
⊓⊔

) = o(n lg σ) bits in total.

multiplied by the O(n/L) leaves yields O( αn lg σ(lg lg n)2

We also have O(lg 1

lg n

α

α

α

α

Now we show how to support range β-majority queries with this structure.

Lemma 11. Small-sized range β-majority queries, for any β ≥ α, can be supported
in O( lg n

β lg lg n ) time.

lg n

2r lg lg n ≤ αl∗ < lg n

Proof. After we arrive at the corresponding leaf block v in time O( lg n
lg lg n ) as in the
proof of Lemma 8, we choose the level −l∗ according to r = j−i+1: it must hold that
r lg lg n. This level is appropriate to apply the
ml∗/2 ≤ r < ml∗, i.e.,
same reasoning of Lemma 8, and it exists whenever 2L∗ ≤ r < L′. On the other hand,
we need that αl∗ ≤ β in order to ensure that the candidates stored in C(M) (which
include all the possible αl∗-majorities) include all the possible β-majorities. Since
αl∗ < lg n
r lg lg n ≤ β to have αl∗ ≤ β. This condition is equivalent
to r ≥ lg n
β lg lg n . We can always assume this condition to be true, since otherwise we

r lg lg n , it suﬃces that

lg n

6 Since γ-codes can only represent positive numbers and we want to use zero to signal end of chunks, we

will always use the code for x + 1 to represent the number x. This adds only O(k) extra bits.

14

can use Lemma 1 to extract S[i..j] and ﬁnd its majorities in time O( lg n
the help of any other data structure.

β lg lg n ) without

lg lg n ) = O( lg n

We then traverse the B-tree of level −l∗ so as to ﬁnd the appropriate miniblock
M. The traversal takes time O( lg(1/α)
lg lg n ). Once we arrive at the proper
miniblock M, we scan the successive chunks of C(M). Since the frequencies in the
next chunk (at the moment of list construction) could not be more than those in the
current chunk, and since some frequency may have increased by at most αl∗ml∗/4
since the last reconstruction, we can safely stop the scan when the highest frequency
seen in the current chunk does not exceed βr − αl∗ml∗/4.
Let us analyze the cost we incur to scan up to this threshold. Since frequencies can
also decrease by up to ml∗/4 until the next reconstruction, an element with current
frequency over βr − αl∗ml∗/4 must have had frequency over βr − αl∗ml∗/2 ≥ (β −
αl∗)ml∗/2 when the list was built, and thus its relative frequency was f ≥ (β−αl∗)/12.
Its quantized frequency was therefore qf ≤ ⌈lg(12/(β − αl∗))⌉, and thus we might
have to process up to 2qf +1 = O(
β−αl∗ ) elements before covering its chunk. This is
O(1/β) if β ≥ 2αl∗; otherwise we might use the argument that the whole list is of
size O(1/αl∗) = O(1/β) anyway. Therefore, we try out each candidate using rank on
S in time O( lg n
⊓⊔
Finally, we show that we can still maintain the structure within the original time.

β lg lg n) and complete the query.

1

Lemma 12. The data structures described in Section 4 can be maintained in amortized 
time O( lg2(1/α)

lg lg n ) under update operations.

lg lg n + 1

α + lg n

Proof. For large and medium blocks, the only diﬀerence is that we must sort the
candidate lists C(v) and C(M) by decreasing frequency. This is not diﬃcult because
we already spend time O(bl) (for large ranges) and O(ml) (for medium-sized ranges)
in building the lists. The frequencies range over a universe of the same size, thus we
can sort them within the same times, O(bl) or O(ml), using radix sort.

The maintenance procedure for the levels −l∗ is very similar to that of miniblocks
described in Lemma 9. One diﬀerence is that, when changes in a miniblock M occurs,
we must update its size upwards in its B-tree. This adds O( lg(1/α)
lg lg n ) time, because
Lemma 2 allows us update each B-tree node in constant time. Node splits and merges
require O(B) time, but these amortize to O(1). Finally, a single update requires
modifying the B-trees in all the lg(1/α) levels −l∗, for a total update cost of O( lg2(1/α)
lg lg n ).
The amortized cost to reconstruct a list C(M) at level −l∗ is O(1/αl∗), including
the special sorting and encoding we use. Since a single update is reﬂected in all the
levels, we must add up this cost for all −l∗, yielding O(1/α). Updates also need time
O( lg n
⊓⊔
α ) encomlg 
lg n ) to reach the desired miniblock.
We now have all the elements to prove our main result. Note that O( lg n

passes all the update costs for the three range sizes.

Theorem 2. For any 0 < α < 1, a sequence of length n over an alphabet of size σ
can be represented using nHk + o(n lg σ) bits for any k = o(lgσ n) to answer range
β-majority queries for any β ≥ α in O( lg n
β lg lg n ) time, and to support symbol insertions
and deletions in O( lg n

α ) amortized time.

15

4.3 A Static Variant

Since none of the current static data structures supporting range α-majority queries
reaches the high-order entropy space we obtain in Theorem 2, we now consider a
static version of our data structure.

A static variant of our solutions uses blocks and miniblocks of ﬁxed size, so one
can access in constant time the desired block at the corresponding level l, l, or −l∗,
and then try out the preﬁx of O(1/β) stored candidates that covers all the possible
β-majorities. All that precomputed data amounts to o(n lg σ) bits of space, even
when built for the minimum α of interest, max(1/n, 1/σ), as shown in Lemmas 6, 7,
and 10. Using a sequence representation that uses nHk + o(n lg σ) bits [4, Thm. 11]
and answers access queries in time O(1) and rank queries in time O(lg lgw σ), where
w = Ω(lg n) is the RAM word size in bits, we can solve β-majority queries in time
O((1/β) lg lgw σ).

Since the structure has O(lg n) levels and each level is built in linear time as
described in Lemmas 5, 9, and 12, the construction time is O(n lg n). The sequence
representation we use [4, Thm. 11] is built in linear time.

Interestingly, since update times are irrelevant in this case, the asymptotic time
and space complexities of our data structure do not depend on α. Thus, our structure
can be built directly for the minimum relevant value of α, max(1/n, 1/σ), and then
it can be queried for any value of β (if β ≤ max(1/n, 1/σ), we just try out all the
σ possible candidates using rank on S[l..r]). Thus, this data structure can be used
to answer range α-majorities for variable α, which is even more powerful than the
range β-majority query. The following theorem presents our result, which is stated as
a solution to the range α-majority problem for variable α.

Theorem 3. On a RAM machine of w = Ω(lg n) bits, a sequence of length n over an
alphabet of size σ can be represented using nHk + o(n lg σ) bits for any k = o(lgσ n)
to answer range α-majority queries for any 0 < α < 1 deﬁned at query time, in
O((1/α) lg lgw σ) time. The structure is built in O(n lg n) time.

5 Finding α-Minorities

We now introduce the ﬁrst dynamic structure to ﬁnd α-minorities in array ranges.
We build on the idea of Chan et al. [8], who ﬁnd A = 1 + ⌊1/α⌋ distinct elements
in S[l..r] and try them out one by one, since one of those must be a minority (there
may be no minority if there are less than A distinct elements in S[l..r]). A succinct
static structure based on this idea [3] uses an O(n)-bit range minimum query data
structure [13], of which no dynamic succinct version exists.

We use a diﬀerent dynamic arrangement that can be implemented in succinct
space. We partition S into pieces, which contain A to 3A distinct elements, except
when S contains a single piece with less than A distinct elements. The following
property is the key to ﬁnd an α-minority in time O( lg n

α lg lg n).

Lemma 13. If S[l..r] overlaps one or two pieces only and it has an α-minority, then
this minority element is one of the distinct elements in those pieces. If S[l..r] contains

16

a piece that is not the last one, then one of the distinct elements in that contained
piece is a minority in S[l..r].

Proof. If S[l..r] overlaps one or two pieces only, then all of its distinct elements are
also distinct elements in some of those overlapped pieces, so the result holds. If S[l..r]
contains a piece with A distinct elements, then one of those must be an α-minority
of S[l..r], since not all of them can occur more than α· (j − i + 1) times in S[l..r]. ⊓⊔
Our data structure is formed by a compressed dynamic representation of S using
nHk + o(n lg σ) bits (Lemma 1) plus two dynamic bitvectors that add 2n + o(n) bits:

1. P [1..n], where P [i] = 1 iﬀ a new piece starts at S[i].
2. C[1..n], where each distinct element in each piece has one arbitrary occurrence at

position j (within the piece) marked with C[j] = 1.

The dynamic bitvectors support the operations access, rank, select, insert, and
delete, in time O(lg n/ lg lg n) (see [23, Lem. 8.1] or [17]).

To ﬁnd an α-minority in S[l..r], we use rank and select on P to determine the ﬁrst
and the last piece overlapped by [l..r]. More precisely, we compute the starting position
of the ﬁrst of these pieces as x = select1(P, rank1(P, l)) and the ending position of
the last as y = select1(P, rank1(P, r)+1)−1. If there are one or two pieces overlapped
by [l..r], that is, rank1(P, y) − rank1(P, x − 1) ≤ 2, we try out all their at most 6A
distinct elements as follows: For k = 1, 2, . . ., we ﬁnd their k-th distinct element,
c, in S[l..r] using the formula c = S[p] for p = select1(C, rank1(C, x − 1) + k)].
We then compute rankc(S, r) − rankc(S, l − 1) to count how many times c occurs
in S[l..r] and thus determine whether c is an α-minority. We repeat this process
until we ﬁnd and return an α-minority, or until p > y, in which case we report that
there is no α-majority in the query range. If S[l..r] overlaps 3 pieces or more, we
choose its leftmost contained piece (the left and right endpoints of this piece are
select1(P, rank1(P, l− 1) + 1) and select1(P, rank1(P, l− 1) + 2)− 1, respectively),
and do as before to obtain its A to 3A candidates and count their occurrences in S.
This process yields, in time O( lg n

α lg lg n ), an α-minority of S[l..r], if there is one.

5.1 Handling Updates

To insert a new symbol c at position i in S, we ﬁrst do the insertion in S, and
also insert a 0 in P [i] and C[i]. We then ﬁnd the piece P [x..y] where P [i] belongs,
using x = select1(P, rank1(P, i)) and y = select1(rank1(P, i) + 1) − 1. Finally, if
rankc(S, y) − rankc(S, x − 1) = 1, then c is a new distinct symbol in the piece and
we must mark it, with C[i] ← 1.
This completes the insertion process unless we exceed the maximum number of
distinct element in the piece, that is, rank1(C, y)− rank1(C, x− 1) > 3A. In this case,
we repartition the piece into pieces of size A to 3A.
The repartitioning proceeds as follows. We locate the ﬁrst occurrences of the
distinct elements in the piece, using rank and select on C and S. We unmark (i.e.,
set to 0) their piece positions in C (which may not be their ﬁrst occurrences). We then
use the classic algorithm that computes order statistics in linear time (i.e., O(A)) to

17

ﬁnd the (A + 1)th of those ﬁrst occurrence positions, p. Then the ﬁrst piece, with
A distinct elements, goes from x to p − 1, so we set P [p] ← 1 and mark in C the
A positions we had located. We then use rank and select on S to ﬁnd the ﬁrst
occurrence of those A positions in S[p..y], continue with the second piece, and so on.
This process generates a number of pieces with A distinct elements, except the
last one, which may have fewer. In this case, we merge the last two pieces built into
one, which will have less than 2A distinct elements. Those are found by repeating the
generation of the penultimate piece, this time not stopping at the (A + 1)th smallest
position, but rather including all the distinct elements. Overall, each new piece is
built in time O( lg n

α lg lg n).

To delete c = S[i], we remove the position i from sequences S, P , and C. If
it holds that P [i] = 1 before removing it, then we had deleted the mark of the
beginning of the piece, so we reset P [i] ← 1 again after removing P [i]. If it holds
that C[i] = 1 before removing it, we must see if there is another occurrence of c in
its piece. We compute the piece endpoints x and y as for the insertion, and then see
if rankc(S, y) − rankc(S, x − 1) ≥ 1. If so, we set another occurrence of c in C, for
example, C[selectc(S, rankc(S, x − 1) + 1)] ← 1. Otherwise, we have lost a distinct
element in the piece and must see if we still have suﬃciently many distinct elements,
that is, if rank1(C, y) − rank1(C, x − 1) ≥ A. If this is true, we ﬁnish.

Otherwise, we have less than A distinct elements in the piece, so we merge it with
the previous or next piece (if none exists, then S has only one piece, which can have
less than A distinct elements). The merged piece has at least A distinct elements, but
it might have up to 4A− 1 and thus overﬂow. The merging then consists of removing
the intermediate 1 in P that separates the two pieces and running our repartitioning
process described above. The cost will be, again, O( lg n

α lg lg n ) per piece generated.

5.2 Analysis

Our partitioning process may require time proportional to the length of the piece,
which can be arbitrarily longer than 3A. Consider for example A = 2 and the piece
(abc)ndef . Inserting a g at the end produces 3
2 n + 2 pieces of length 2. We can show,
however, that the amortized cost of a repartitioning is within the desired time bounds.
We will measure the cost in terms of number of operations over the sequences, each
of which costs O( lg n

lg lg n ).

Let us deﬁne a potential function φ = n − A · (m − 1), where m is the number
of pieces at the present moment. It always holds φ ≥ 0, even when we start with
n = 1 element and m = 1 piece. Then an insertion or a deletion without overﬂow
or underﬂow modiﬁes φ by ∆φ = ±1, which does not aﬀect the asymptotic cost. A
repartitioning, instead, takes a piece and produces t > 1 pieces out of it. The actual
cost of generating each piece, ignoring constants, is A operations on the sequences;
therefore the total cost of the operation is A · t. On the other hand, the diﬀerence in
potential is ∆φ = A · (1 − t) (n does not change while repartitioning). Therefore, the
amortized cost of the repartitioning is A.

18

The case of an underﬂow is similar: we ﬁrst join two pieces, which increases φ by
A. We then repartition the resulting piece into t, which costs A · t and changes the
potential by ∆φ = A · (1 − t). In total, the amortized cost of an underﬂow is 2A.
lg lg n ), the amortized
cost of the operations insert and delete is O( lg n

Since A = O(1/α) and the operations we are counting cost O( lg n

α lg lg n ).

Theorem 4. For any 0 < α < 1, a sequence of length n over an alphabet of size σ
can be represented using nHk +2n+o(n lg σ) bits for any k = o(lgσ n) to answer range
α-minority queries in O( lg n
α lg lg n ) time, and to support symbol insertions and deletions
in O( lg n
α lg lg n ) amortized time.

By using the idea in static form, we also obtain the ﬁrst solution for α-minority
queries using high-order entropy space. Here the bitvectors take constant time to
answer access, rank and select queries, and the static sequence representation [4,
Thm. 11] yields time O(lg lgw σ) per operation. The construction is easily done in
linear time.

Theorem 5. On a RAM machine of w = Ω(lg n) bits, a sequence of length n over
an alphabet of size σ can be represented using nHk + 2n + o(n lg σ) bits for any k =
o(lgσ n), to answer range α-minority queries in O((1/α) lg lgw σ) time. The structure
is built in O(n) time.

6 Conclusions

In this article, we have designed the ﬁrst compressed data structure for dynamic
range α-majority. To achieve this result, our key strategy is to perform a multi-level
decomposition of the sequence S and, for each block of S, precompute a candidate set
that includes all the α-majorities of any query range of the right size that touches this
block. Thus, when answering a query, we need not ﬁnd a set of blocks whose union
forms the query range, as is required in the solution of Elmasry et al. [11]. Instead,
we only look for a single block that touches the query range. This simpler strategy
allows us to achieve compressed space.

Furthermore, we generalize our solution to design the ﬁrst dynamic data structure
that can maintain S in the same space and update time, to support the computation
of the β-majorities in a given query range for any β ∈ [α, 1) in O( lg n
β lg lg n ) time. Note
that here β is given with the queries, and only α is ﬁxed and given beforehand. This
type of query is more general than range α-majority queries and was only studied in
the static case before [15,5].

Finally, we design the ﬁrst dynamic data structure for the range α-minority query
problem, and this data structure is also compressed. Even simple static solutions [8]
based on range minimum queries are diﬃcult to dynamize. We ﬁnd a new, simple
data structure that is easy to maintain upon updates and gives suﬃcient information
to ﬁnd α-minorities in time O( lg n

α lg lg n ).

For constant α, our query time O( lg n

lg lg n ) for α-majority is optimal within polylogarithmic 
update time, as it matches the lower bound of the simpler operation majority

19

[18, Prop. 3], which considers the particular case of binary alphabets, ranges of the
form S[1..i], and α = 1/2. Another obvious lower bound is O(1/α), as it is the output 
size in the worst case. It is not clear whether a dynamic structure can achieve
O( lg n
α) query time with polylogarithmic update time, even without compression.
In the case of α-minorities, there is no clear lower bound.

lg lg n + 1

Interestingly, our dynamic data structures for α-majority and α-minority use
nHk + o(n lg σ) bits of space (+2n bits in the case of minorities), which is less than
the space achieved so far in the static case. We thus describe static variants of our
structures, which answer queries in time O((1/α) lg lgw σ). The best static solutions
answer queries in time O(1/α) (which is optimal at least for α-majorities), but use
more than nH0 ≥ nHk bits of space [5]. An interesting question is whether the optimal
times can be achieved within k-th order entropy space.

References

1. Arge, L., Vitter, J.S.: Optimal external memory interval management. SIAM Journal on Computing

32(6), 1488–1508 (2003)

2. Beame, P., Jayram, T.S., Rudra, A.: Lower bounds for randomized read/write stream algorithms. In:

Proc. 39th Annual ACM Symposium on Theory of Computing (STOC). pp. 689–698 (2007)

3. Belazzougui, D., Gagie, T., Navarro, G.: Better space bounds for parameterized range majority and
minority. In: Proc. 12th Annual Workshop on Algorithms and Data Structures (WADS). pp. 121–132
(2013)

4. Belazzougui, D., Navarro, G.: Optimal lower and upper bounds for representing sequences. ACM Transactions 
on Algorithms 11(4), article 31 (2015)

5. Belazzougui, D., Gagie, T., Munro, J.I., Navarro, G., Nekrich, Y.: Range majorities and minorities in

arrays. CoRR abs/1606.04495 (2016)

6. Bell, T.C., Cleary, J., Witten, I.H.: Text Compression. Prentice Hall (1990)
7. Chan, T.M., Durocher, S., Larsen, K.G., Morrison, J., Wilkinson, B.T.: Linear-space data structures for

range mode query in arrays. Theory of Computing Systems 55(4), 719–741 (2014)

8. Chan, T.M., Durocher, S., Skala, M., Wilkinson, B.T.: Linear-space data structures for range minority

query in arrays. Algorithmica 72(4), 901–913 (2015)

9. Demaine, E.D., L´opez-Ortiz, A., Munro, J.I.: Frequency estimation of internet packet streams with
limited space. In: Proc. 10th Annual European Symposium on Algorithms (ESA). pp. 348–360 (2002)
10. Durocher, S., He, M., Munro, J.I., Nicholson, P.K., Skala, M.: Range majority in constant time and

linear space. Information and Computation 222, 169–179 (2013)

11. Elmasry, A., He, M., Munro, J.I., Nicholson, P.K.: Dynamic range majority data structures. Theoretical

Computer science 647, 59–73 (2016)

12. Fang, M., Shivakumar, N., Garcia-Molina, H., Motwani, R., Ullman, J.D.: Computing iceberg queries
eﬃciently. In: Proc. 24rd International Conference on Very Large Data Bases (VLDB). pp. 299–310
(1998)

13. Fischer, J., Heun, V.: Space-eﬃcient preprocessing schemes for range minimum queries on static arrays.

SIAM Journal on Computing 40(2), 465–492 (2011)

14. Gagie, T., He, M., Navarro, G.: Compressed dynamic range majority data structures. In: Proc. 27th

Data Compression Conference (DCC). pp. 260–269 (2017)

15. Gagie, T., He, M., Munro, J.I., Nicholson, P.K.: Finding frequent elements in compressed 2d arrays
and strings. In: Proc. 18th International Symposium on String Processing and Information Retrieval
(SPIRE). pp. 295–300 (2011)

16. Greve, M., Jørgensen, A.G., Larsen, K.D., Truelsen, J.: Cell probe lower bounds and approximations
for range mode. In: Proc. 37th International Colloquium on Automata, Languages and Programming
(ICALP). pp. 605–616 (2010)

17. He, M., Munro, J.I.: Succinct representations of dynamic strings. In: Proc. 17th International Symposium

on String Processing and Information Retrieval (SPIRE). pp. 334–346. LNCS 6393 (2010)

18. Husfeldt, T., Rauhe, T.: New lower bound techniques for dynamic partial sums and related problems.

SIAM Journal on Computing 32(3), 736–753 (2003)

20

19. Karp, R.M., Shenker, S., Papadimitriou, C.H.: A simple algorithm for ﬁnding frequent elements in

streams and bags. ACM Transactions on Database Systems 28, 51–55 (2003)

20. Karpinski, M., Nekrich, Y.: Searching for frequent colors in rectangles. In: Proc. 20th Canadian Conference 
on Computational Geometry (CCCG). pp. 11–14 (2008)

21. Misra, J., Gries, D.: Finding repeated elements. Science of Computer Programming 2(2), 143–152 (1982)
22. Munro, J.I., Nekrich, Y.: Compressed data structures for dynamic sequences. In: Proc. 23rd Annual

European Symposium on Algorithms (ESA). pp. 891–902 (2015)

23. Navarro, G., Sadakane, K.: Fully-functional static and dynamic succinct trees. ACM Transactions on

Algorithms 10(3), article 16 (2014)

24. Raman, R., Raman, V., Rao, S.S.: Succinct dynamic data structures. In: Proc. 7th International Workshop 
on Algorithms and Data Structures (WADS). pp. 426–437 (2001)

21

