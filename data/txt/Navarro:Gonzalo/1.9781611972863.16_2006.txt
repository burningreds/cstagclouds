Optimal Incremental Sorting ∗

Rodrigo Paredes †

Gonzalo Navarro †

Abstract
Let A be a set of size m. Obtaining the ﬁrst k ≤ m
elements of A in ascending order can be done in optimal
O(m+k log k) time. We present an algorithm (online on
k) which incrementally gives the next smallest element
of the set, so that the ﬁrst k elements are obtained in
optimal time for any k. We also give a practical version
of the algorithm, with the same complexity on average,
which performs better in practice than the best existing
online algorithm. As a direct application, we use our
technique to implement Kruskal’s Minimum Spanning
Tree algorithm, where our solution is competitive with
the best current implementations. We ﬁnally show that
our technique can be applied to several other problems,
such as obtaining an interval of the sorted sequence and
implementing heaps.

1 Introduction

There are cases where we need to obtain the smallest
elements from a ﬁxed set without knowing how many
elements we will end up needing. Prominent examples 
are Kruskal’s Minimum Spanning Tree (MST) algorithm 
[16] or ranking by Web search engines [1]. Given
a graph, Kruskal’s MST algorithm processes the edges
one by one, from smallest to largest, until it forms the
MST. At this point, remaining edges are not considered.
Web search engines display a very small sorted subset
of the most relevant documents among all those satisfying 
the query. Later, if the user wants more results, the
search engine displays the next group of most relevant
documents, and so on. In both cases, we could ﬁrst sort
the whole set and later return the desired objects, but
obviously this is more work than necessary.

This problem can be called Incremental Sorting. It
can be stated as follows: Given a set A of m numbers,
output the elements of A from smallest to largest, so
that the process can be stopped after k elements have
been output, for any k that is unknown to the algorithm.
Therefore, Incremental Sorting is the online version of

∗Supported in part by the Millennium Nucleus Center for Web

Research, Grant P04-067-F, Mideplan, Chile.

†Center for Web Research, Dept. of Computer Science, UniBlanco 
Encalada 2120, Santiago, Chile.

versity of Chile.
{raparede,gnavarro}@dcc.uchile.cl

an instance of the Partial Sorting problem: Given a
set A of m numbers and an integer k ≤ m, output the
smallest k elements of A in ascending order.
In 1971, J. Chambers introduced the general notion
of Partial Sorting [3]: given an array A of m numbers,
and a ﬁxed, sorted set of indices I = i0 < i1 < . . . <
ik−1 of size k ≤ m, arrange in place the elements of A
so that A[0, i0 − 1] ≤ A[i0] ≤ A[i0 + 1, i1 − 1] ≤ A[i1] ≤
. . . ≤ A[ik−2 +1, ik−1−1] ≤ A[ik−1] ≤ A[ik−1 +1, m−1].
This property is equivalent to the statement that A[i] is
the i-th order statistic of A for all i ∈ I.
We are interested in the particular case of ﬁnding
the ﬁrst k order statistics of a given set A of size m > k.
This can be easily solved by ﬁrst ﬁnding p, the k-th
smallest element of A, using O(m) time Select algorithm 
[2], and then collecting and sorting the elements
smaller than p. We call this algorithm SelectSort. Its
complexity, O(m + k log k), is optimal under the comparison 
model, as there are mk = m!/(m − k)! possible
answers and log(mk) = Ω(m + k log k).
A practical version of the above, QuickSelectSort 
(QSS), uses QuickSelect [10] and QuickSort
[11] as the selection and sorting algorithms, obtaining
O(m+k log k) average complexity. Recently, it has been
shown that selection and sorting can be interleaved. The
result, PartialQuickSort (PQS), has the same average 
complexity but smaller constant terms [17].

To solve the online problem, we must select the
smallest element, then the second one, and so on until
the process ﬁnishes at some unknown value k ∈ [0, m −
1]. One can do this by using Select to ﬁnd each of
the ﬁrst k elements, which costs O(km) overall. We
can improve this by transforming A into a min-heap
in time O(m) [6], and then performing k extractions.
This has O(m + k log m) worst-case complexity. Note
that m + k log m = O(m + k log k), as they can diﬀer
only if k = o(mc) for any c > 0, and then m dominates
k log m. However, according to experiments this scheme
is much slower than the oﬄine practical algorithm PQS
if a classical heap is used.

In [22], P. Sanders proposes sequence heaps, a cacheaware 
priority queue to solve the online problem, which
is optimized to insert and extract all the elements in the
priority queue at small amortized cost. Even though
the total CPU time used for this algorithm in the whole

Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpprocess of inserting and extracting all the m elements
is pretty close to the time of running QuickSort, our
experiments show that this scheme is not so eﬃcient
when we want to sort just a small fraction of the set.
Then the quest for a practical online algorithm for
partial sorting is raised.

In this paper we present IncrementalSelect
(IS), which is yet another algorithm that solves the
online problem in optimal O(m + k log k) time. But
our main contribution is IncrementalQuickSelect
(IQS), a practical variant of IS, which is O(m + k log k)
time on average. Our experimental results show that
IQS is almost as eﬃcient as its oﬄine version PQS,
and is faster in practice than alternative solutions.

As an application, we show how to use our algorithm 
to boost the performance of Kruskal’s MST algorithm 
[16]. Given a graph G(V, E), we compute its
MST in O(|E| + |V | log2 |V |) average time, which is optimal 
in medium or high density graphs. In practice,
by using IQS we obtain an eﬃcient MST implementation.
 Our implementation is much faster than any other
Kruskal’s implementation we could program or ﬁnd for
any graph density. As a matter of fact, our Kruskal’s
version is faster than Prim’s algorithm [20], even as optimized 
by B. Moret and H. Shapiro [18], and also competitive 
with the best alternative implementations we
could ﬁnd [14, 15].

We ﬁnally show that our algorithm can be used to
solve other basic problems, such as obtaining an incremental 
segment of the sorted sequence, and implementing 
a priority queue. The algorithm can obviously be
used to ﬁnd the largest elements instead of the smallest.

2 Incremental sorting

In this section we describe IQS algorithm. At the
end we show how it can be converted into its worstcase 
version IS. Essentially, to output the k smallest
elements, IQS calls QuickSelect to ﬁnd the smallest
element on arrays A[0, m − 1], A[1, m − 1], . . . , A[k −
1, m − 1]. This naturally leaves the k smallest elements
sorted in A[0, k− 1]. IQS avoids the O(km) complexity
by reusing the work among calls to QuickSelect.
Let us recall how QuickSelect works. Given an
integer k, QuickSelect aims to ﬁnd the k-th smallest
element from a set A of m numbers. For this sake it
chooses an object p (the pivot), and partitions A so
that the elements lower than p are allocated to the leftside 
partition, and the others to the right side. After
the partitioning, p is placed in its correct position ip.
So, if ip = k, QuickSelect returns p and ﬁnishes.
Otherwise, if k < ip it recursively processes the left
partition, else the right partition (with a new k ←
k − ip − 1).

51 81 74 12 58 92 86 25 67 33 18 41 49 63 29 37   S = {16}

33 37 29 12 49 41 18 25 51 67 86 92 58 63 74 81   S = {16, 8}

18 25 29 12 33 41 49 37                           S = {16, 8, 4}

12 18 29 25                                       S = {16, 8, 4, 1}

12                                                S = {16, 8, 4, 1, 0}

   18 29 25 33 41 49 37 51 67 86 92 58 63 74 81   S = {16, 8, 4, 1}

Figure 1: Example of how IQS ﬁnds the ﬁrst element of
an array. Each line corresponds to a new partition of a
sub-array. Note that all the pivot positions are stored in
stack S. In the example we use the ﬁrst element as the
pivot but it could be any other element. The bottom
line shows the array with the three partitions generated
by the ﬁrst call to IQS, and the pivot positions stored
in S.

Note that it is possible to reuse the work made
by previous calls to QuickSelect. When we call
QuickSelect on A[1, m − 1], a decreasing sequence of
pivots has already been used to partially sort A since the
previous invocation on A[0, m − 1]. IQS manages this
sequence of pivots to reuse previous work. Speciﬁcally,
it uses a stack S of decreasing pivot positions that are
relevant for the next calls to QuickSelect.

Fig. 1 shows how IQS searches for the smallest
element of an array by using a stack initialized with
a single value m = 16. To ﬁnd the next minimum, we
ﬁrst check whether p, the top value in S, is the index of
the element sought, in which case we pop and return it.
Otherwise, because of previous partitionings, it holds
that elements in A[1, p− 1] are smaller than all the rest,
so we run QuickSelect on that portion of the array,
pushing new pivots into S.

As can be seen in Fig. 1, the second minimum is the
pivot on the top of S, so we pop and return it. Fig. 2
shows how IQS ﬁnds the third minimum using the pivot
information stored in S. Notice that IQS just works on
the current ﬁrst chunk ({29, 25}), where it adds one
pivot position to S and returns the third element in the
next recursive call.

Now, retrieving the fourth and ﬁfth elements is
easy since both of them are pivots. Fig. 3 shows
how IQS ﬁnds the sixth minimum. The current ﬁrst
chunk contains three elements: {41, 49, 37}. So, IQS
obtains the next minimum by selecting 41 as pivot,
partitioning its chunk and returning the element 37.
The incremental sorting process will continue as long
as needed, and it can be stopped in any time.

Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php      29 25 33 41 49 37 51 67 86 92 58 63 74 81   S = {16, 8, 4}

      25 29                                       S = {16, 8, 4, 3}

      25                                          S = {16, 8, 4, 3, 2}

         29 33 41 49 37 52 67 86 92 58 63 74 81   S = {16, 8, 4, 3}   

Figure 2: Example of how IQS ﬁnds the third element of
the array. Since it starts with pivot information stored
in S, it just works on the current ﬁrst chunk ({29, 25}).

               41 49 37 52 67 86 92 58 63 74 81   S = {16, 8}

               37 41 49                           S = {16, 8, 6}

                  41 49 52 67 86 92 58 63 74 81   S = {16, 8, 6}       

Figure 3: Example of how IQS ﬁnds the sixth element of
an array. Since it starts with pivot information stored in
S, it just works on the current ﬁrst chunk ({41, 49, 37}).
We omit the line where element 37 becomes a pivot and
is popped from S.

The algorithm is given in Fig. 4.

Stack S is
initialized as S = {|A|}. IQS receives the set A, the
index idx1 of the element sought (that is, we seek the
smallest element in A[idx, m − 1]), and the current
stack S (with former pivot positions). First it checks
whether the top element of S is the desired index
idx,
in which case it pops idx and returns A[idx].
Otherwise it chooses a random pivot index pidx from
[idx, S.top()−1]. Pivot A[pidx]
is used to partition
A[idx, S.top()−1]. After the partitioning, the pivot has
reached its ﬁnal position pidx′, which is pushed in S.
Finally, a recursive invocation continues the work on
the left hand of the partition.

Recall that partition(A, A[pidx], i, j) rearranges
A[i, j] and returns the new position pidx′ of the original
element in A[pidx], so that, in the rearranged array,
all the elements smaller/larger than A[pidx′] appear
before/after pidx′. Thus, pivot A[pidx′] is left at the
correct position it would have in the sorted array A[i, j].
The next lemma shows that it is correct to search for
the minimum just within A[i, S.top() − 1], from which
the correctness of IQS immediately follows.

Lemma 2.1. After i minima have been obtained in
A[0, i − 1], (1) the pivot indices in S are decreasing
bottom to top, (2) for each pivot position p 6= m in

1Since we start counting array positions from 0, the place of

the k-th element is k − 1, so idx = k − 1.

IQS (Set A, Int idx, Stack S)

// Precondition: idx ≤ S.top()
If idx = S.top() Then S.pop(), Return A[idx]
pidx ← random[idx, S.top()−1]
pidx′ ← partition(A, A[pidx], idx, S.top()−1)
// Invariant: A[0] ≤ . . . ≤ A[idx − 1]
// ≤ A[idx, pidx′ − 1] ≤ A[pidx′]
// ≤ A[pidx′ + 1, S.top()−1] ≤ A[S.top(), m − 1]
S.push(pidx′)
Return IQS(A, idx, S)

1.
2.

3.

4.
5.

Figure 4:
IncrementalQuickSelect (IQS) algorithm.
 Stack S is initialized as S ← {|A|}. Both S and
A are modiﬁed and rearranged during the algorithm.
Note that the search range is limited to the array segment 
A[idx, S.top()−1]. Procedure partition returns
the position of pivot A[pidx] after the partition completes.
 Note that the tail recursion can be easily removed.


S, A[p] is not smaller than any element in A[i, p − 1]
and not larger than any element in A[p + 1, m − 1].
Proof. Initially this holds since i = 0 and S = {m}.
Assume this is valid before pushing p, when p′ was
the top of the stack. Since the pivot was chosen from
A[i, p′ − 1] and left at some position i ≤ p ≤ p′ − 1 after
partitioning, property (1) is guaranteed. As for property
(2), after the partitioning it still holds for any pivot
other than p, as the partitioning rearranged elements at
the left of it. With respect to p, the partitioning ensures
that elements smaller than p are left at A[i, p− 1], while
larger elements are left at A[p + 1, p′ − 1]. Since A[p]
was already not larger than elements in A[p′, m− 1], the
lemma holds. It obviously remains true after removing
elements from S.
(cid:4)

The worst-case complexity of IQS is O(m2), but it
is easy to derive worst-case optimal IS from it. The only
change is in line 2 of Fig. 4, where the random selection
of the next pivot position must be changed to choosing
the median of A[idx, S.top() − 1], using the linear-time
selection algorithm [2]. Section 3 analyzes the worstcase 
of IS and Section 4 considers the average-case of
IQS, both of which are O(m + k log k).

3 IS worst-case complexity

In this section we analyze IS, which is not as eﬃcient in
practice as IQS, but has good worst-case performance.
In particular, the analysis serves as a basis for the
average-case analysis of IQS in Section 4. In IS, the

Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phph = j = 5

h =

h =

h =

h =

4

3

2

1

Figure 5: IS partition tree for incremental sorting when
k = 5, m = 31, j = 5.

partition is perfectly balanced since each pivot position
is chosen as the median of its array segment.

In this analysis we assume that m is of the form
2j − 1. We recall that array indices are in the range
[0, m − 1]. Fig. 5 illustrates the incremental sorting
process when k = 5 in a perfect balanced tree of m = 31
elements, j = 5. Black nodes are the elements already
reported, grey nodes are the pivots that remain in stack
S, and white nodes and trees are the other elements of
A.

2 ] and A[ m+1

The pivot at the tree root is the ﬁrst to be obtained
(the median of A), at cost linear in m (both to obtain
the median and to partition the array). The two root
children are the medians of A[0, m−3
2 , m −
1]. Obtaining those pivots and partitioning with them
will cost time linear in m/2. The left child of the root
will actually be the second pivot to be processed. The
right child, on the other hand, will be processed only
if k > m−1
2 , that is, at the moment we ask IS to
output the m+1
In general, processing
the pivots at level h will cost O(2h), but only some of
these will be required for a given k. The maximum level
is j = log2(m + 1).

2 -th minimum.

It is not hard to see that, in order to obtain the

level h. Adding up their processing cost we get Eq. (3.1),

k smallest elements of A, we will require (cid:6) k
2h(cid:7) pivots of
where we split the sum into the cases (cid:6) k
2h(cid:7) > 1 and
(cid:6) k
2h(cid:7) = 1. Only then, in Eq. (3.3), we use k + 2h to

bound the terms of the ﬁrst sum, and redistribute terms
to obtain that IS is O(m+k log k) worst-case time. The
extra space used by IS is O(log m) pivot positions.

(3.1)

(3.2)

(3.3)

(3.4)

log2(m+1)

=

⌊log2 k⌋

T (m, k) =

(cid:24) k
2h(cid:25) 2h
Xh=1
2h(cid:25) 2h +
Xh=1 (cid:24) k
Xh=1
Xh=1
T (m, k) = k⌊log2 k⌋ + 2m + 1

⌊log2 k⌋

k +

≤

log2(m+1)

2h

log2(m+1)

Xh=⌊log2 k⌋+1

2h

4 IQS average-case complexity

In this case the ﬁnal pivot position p after the partitioning 
of A[0, m− 1] distributes uniformly in [0, m− 1].
Consider again Fig. 5, where the root is not anymore
the middle of A but a random position. We call T (m, k)
the average number of key comparisons needed to obtain 
the k smallest elements of A[0, m − 1]. After the
m − 1 comparisons used in the partitioning, there are
three cases depending on p: (1) k ≤ p, in which case the
right subtree will never be expanded, and the total extra 
cost will be T (p, k) to solve A[0, p− 1]; (2) k = p + 1,
in which case the left subtree will be fully expanded to
obtain its p elements at cost T (p, p); and (3) k > p + 1,
in which case we pay T (p, p) on the left subtree, whereas
the right subtree, of size m− 1 − p, will be expanded so
as to obtain the remaining k − p − 1 elements.
Thus IQS average cost follows Eq. (4.5), which is
rearranged as Eq. (4.6). It is easy to check that this is
exactly the same as Eq. (3.1) in [17], which shows that
IQS makes exactly the same number of comparisons of
the oﬄine version, PQS. This is 2m + 2(m + 1)Hm −
2(m + 3 − k)Hm+1−k − 6k + 6. That analysis [17] is
rather sophisticated, resorting to bivariate generating
functions.
In which follows we give a simple development 
arriving at a solution of the form O(m + k log k).

(4.5)

T (m, k) = m − 1 +

1

m  m−1
Xp=k

T (p, k) + T (k − 1, k − 1)

+

k−2

Xp=0(cid:16)T (p, p) + T (m − 1 − p, k − p − 1)(cid:17)!

(4.6)

= m − 1 +

1

m  k−1
Xp=0

T (p, p)

+

k−2

Xp=0

T (m − 1 − p, k − p − 1) +

T (p, k)!

m−1

Xp=k

Eq. (4.6) simpliﬁes to Eq. (4.7) by noticing that
T (p, p) behaves exactly like QuickSort, 2(p+1)Hp−4p
[9] (this can also be seen by writing down T (p) = T (p, p)
and noting that the very same QuickSort recurrence is
obtained), so thatPk−1
2 (5k −
1). We also write p′ for k − p− 1 and rewrite the second
sum as Pk−1

p=0 T (p, p) = k(k + 1)Hk − k

p′=1 T (m − k + p′, p′).

(4.7)

T (m, k) = m − 1 +

k
2

1

m k(k + 1)Hk −
Xp=k

m−1

T (m − k + p, p) +

(5k − 1)
T (p, k)!

+

k−1

Xp=1

Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpWe make some pessimistic simpliﬁcations now. The
ﬁrst sum governs the dependence on k of the recurrence.
To avoid such dependence, we bound the second argument 
to k and the ﬁrst to m, as T (m, k) is monotonic on
both its arguments. The new recurrence, Eq. (4.8), depends 
only on parameter m and treats k as a constant.

(4.8)

T (m) = m − 1 +

1

k
2

m k(k + 1)Hk −
(5k − 1)
T (p)!
Xp=k
+(k − 1)T (m) +

m−1

We subtract m T (m) − (m − 1)T (m − 1) using
Eq. (4.8), to obtain Eq. (4.9) and Eq. (4.10). Since T (k)
is actually T (k, k), we use again QuickSort formula in
Eq. (4.11). We bound the ﬁrst part by 2m + 2kHm−k
and the second part by 2kHk to obtain Eq. (4.12).

(4.9)

(4.10)

(4.11)

(4.12)

T (m) = 2

m

= 2

m − 1
+ T (m − 1)
m − k + 1
i − k + 1(cid:19) + T (k)
Xi=k+1(cid:18)1 +
k − 2
= 2(m − k) + 2(k − 2)(Hm−k+1 − 1)
+ (2(k + 1)Hk − 4k)
< 2(m + kHm−k + kHk)

This result does not yet look good enough, but we
plug it again into Eq. (4.7). In this case, we bound the
sum Pk−1
p=1 2(m − k + p +
pHm−k + pHp) = (k − 1)(cid:0)2m + k(cid:0)Hm−k + Hk − 3
2(cid:1)(cid:1).

p=1 T (m − k + p, p) with Pk−1

Upper bounding again and multiplying by m we get a
new recurrence in Eq. (4.13). Note that this recurrence
only depends on m.

+ T (m − 1)

i − k −

1

i(cid:19)(cid:19)

(4.14)

T (m) = 2

(4.15)

(4.16)

i

m

<

+

m

+ T (k)

(k − 1)k
(m − k)m
k − 2
+ (k − 1)(cid:18) 1

m + k − 2
Xi=k+1(cid:18)2 + 2
= 2(m − k) + 2(k − 2)(Hm − Hk)
+ (k − 1)(Hm−k − Hm + Hk)
+ (2(k + 1)Hk − 4k)
Note that Hm − Hk < m−k

k+1 and thus (k − 2)(Hm −
Hk) < m−k. Also, Hm−k ≤ Hm, so collecting terms we
obtain Eq. (4.17). Therefore, IQS is also O(m + k log k)
in the average-case when we choose pivots at random.

(4.17) T (m, k) < 4m−8k+(3k+1)Hk < 4m+3kHk
As a ﬁnal remark, note that when we use QSS a
portion of the QuickSort partitioning work repeats
the work made in the previous QuickSelect calling.
Fig. 6 illustrates this, showing that upon ﬁnding the kth 
element, the QuickSelect stage has produced partitions 
A1 and A2, however the QuickSort that follows
processes the left partition as a whole ([A1p1A2]), ignoring 
the previous partitioning work done over it. On
the other hand, IQS sorts the left segment by processing 
each partition independently, because it knows their
limits (as they are stored in the stack S). This also applies 
to PQS and it explains the ﬁnding of C. Mart´ınez
that PQS, and thus IQS, makes 2k− 4Hk + 2 less comparisons 
than QSS [17].

QSS QuickSelect stage

QSS QuickSort stage

k

p2

p1

A
1

A1

p1

A2

(4.13)
m T (m) = m(m − 1) + k(k + 1)Hk −
+ (k − 1)(cid:18)2m + k(cid:18)Hm−k + Hk −

k
2

m−1

3

(5k − 1)
2(cid:19)(cid:19) +
Xp=k

T (p)

A2

Subtracting again m T (m)−(m−1)T (m−1) we get
m(cid:17),
m−k − 1

Eq. (4.14). Noting that (k−1)k
we get Eq. (4.15), which is solved in Eq. (4.16).

(m−k)m = (k − 1)(cid:16) 1

Figure 6: Partition work performed by QSS. First, QSS
uses QuickSelect for ﬁnding the k-th element (left).
Then it uses QuickSort on the left array segment as a
whole ([A1 p1 A2]) neglecting the previous partitioning
work (right).

Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpKruskal1 (Graph G(V, E))
1.

UnionFind C ← {v ∈ V,{v}}
// the set of all connected components
mst ← ∅ // the growing minimum spanning tree
ascendingSort(E), k ← 0

4. While |C| > 1 Do

// select an edge in ascending order
(e = {u, v}) ← E[k], k ← k + 1
If C.ﬁnd(u) 6= C.ﬁnd(v) Then

mst ← mst ∪ {e}, C.union(u, v)

Return mst

2.

3.

5.

6.
7.

8.

5 IQS and the minimum spanning tree

In this section we explore a practical application of
IQS: improving the performance of Kruskal’s Minimum
Spanning Tree (MST) algorithm.

Let us recall the MST problem. Let G(V, E) be a
connected graph with a nonnegative cost function d(e)
assigned to its edges e ∈ E. A minimum spanning tree
mst of the graph G(V, E) is a tree composed of edges of
E that connect all the vertices of V at the lowest total

not necessarily unique.

cost Pe∈mst d(e). Note that, given a graph, its MST is
Let n = |V |, m = |E|. The most popular algorithms
to solve the MST problem are Kruskal’s [16] and Prim’s
[20], whose basic versions have complexity O(m log m)
and O(n2), respectively. We call these basic versions
Kruskal1 and Prim1, respectively. In sparse graphs,
with |E| = O(n), it is recommended to use Kruskal1,
whereas in dense graphs, with |E| = O(n2), Prim1
is recommended [5, 25]. Alternatively, Prim can be
implemented using Fibonacci Heaps [8] to obtain O(m+
n log n) complexity.

There are other MST algorithms compiled by Tarjan 
[23]. Recently, B. Chazelle [4] gave an O(mα(m, n))
algorithm, where α ∈ ω(1) is the very slowly-growing inverse 
Ackermann’s function. Later, S. Pettie and V. Ramachandran 
[19] proposed an algorithm that runs in
optimal time O(T ∗(m, n)), where T ∗(m, n) is the minimum 
number of edge-weight comparisons needed to determine 
the MST of any graph G(V, E) with m edges
and n vertices. The best known upper bound of this algorithm 
is also O(mα(m, n)). These algorithms almost
reach the lower bound Ω(m), yet they are so complicated 
that their interest is mainly theoretical. Furthermore,
 there is a randomized algorithm [13] that ﬁnds
the MST in O(m) time with high probability in the
restricted RAM model, but it is also considered impractical 
as it is complicated to implement and the O(m)
complexity hides a big constant factor.

Experimental studies on MST are given in [18, 14,
15]. In [18], they compare several versions of Kruskal’s,
Prim’s and Tarjan’s algorithms, concluding that the
best in practice (albeit not in theory) is Prim using
pairing heaps [7]. We call this algorithm Prim2.
Their experiments show that neither Cheriton and
Tarjan’s [23] nor Fredman and Tarjan’s algorithm [8]
ever approach the speed of Prim2. On the other hand,
they show that Kruskal1 can run very fast when it uses
an array of edges that can be overwritten during sorting,
instead of an adjacency list. Moreover, they show
that it is possible to use heaps to improve Kruskal’s
algorithm. They call this variant Kruskal’s with demand
sorting, and we will refer to it as Kruskal2. The
result is a rather eﬃcient MST version with complexity

Figure 7: The basic version of Kruskal’s MST algorithm
(Kruskal1). To carry out the heap-based optimization
(Kruskal2), we change line 3 to heapify(E) and line
5 to (e = {u, v}) ← E.extractMin().

O(m + k log m), being k ≤ m the number of edges
reviewed by Kruskal technique.
In [14, 15], they give an algorithm whose complexity
It generates a subgraph G′ by
is O(m + n log n).
selecting √mn edges from G at random. Then, it builds
the minimum spanning forest T ′ of G′. Then, it ﬁlters
each edge e ∈ E using the cycle property: discard e if
it is the heaviest edge on a cycle in T ′ ∪ {e}. Finally, it
builds the MST of the subgraph that contains the edges
of T ′ and the edges that were not ﬁltered out. We call
this algorithm iMax.

5.1 Kruskal’s MST algorithm. Kruskal’s algorithm 
starts with n single-node components, and it
merges them until it produces a sole connected component.
 To do this, Kruskal1 begins by setting the mst
to (V,∅), that is, n single-node trees. Later, in each iteration,
 it adds to the mst the cheapest edge of E that
does not produce a cycle on the mst, that is, it only
adds edges whose vertices belong to diﬀerent connected
components. Once the edge is added, both components
are merged. The process ends when the mst becomes a
single connected component. At this point the mst is a
minimum spanning tree of G(V, E).

To manage the component operations, we use the
Union-Find data structure C with path compression,
see [5, 25] for a comprehensive explanation. Given
two vertices u and v, we use the ﬁnd(u) operation
to compute which component u belongs to, and use
union(u, v) to merge the components of u and v. The
amortized cost of ﬁnd(u) is O(α(m, n)) and the cost of
union(u, v) is constant.

Fig. 7 depicts the basic Kruskal’s MST algorithm.

Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpKruskal3 (Graph G(V, E))
1.

UnionFind C ← {v ∈ V,{v}}
// the set of all connected components
mst ← ∅ // the growing minimum spanning tree
Stack S, S.push(|E|), k ← 0

4. While |C| > 1 Do

// select the lowest edge incrementally
(e = {u, v}) ← IQS(E, k, S), k ← k + 1
If C.ﬁnd(u) 6= C.ﬁnd(v) Then

mst ← mst ∪ {e}, C.union(u, v)

Return mst

2.

3.

5.

6.
7.

8.

We need O(n) time to initialize both C and mst, and
O(m log m) time to sort the edge set E. Then we make
at most m O(α(m, n))-time iterations of the While
loop. Therefore, Kruskal1 complexity is O(m log m).

2 γn + 1

4 + O( 1

2 n ln n + 1

Assuming we are using either full or random graphs
whose cost edges are assigned at random independently
of the rest (using any continuous distribution), the
subgraph composed by V with the edges reviewed by the
algorithm is a random graph [12]. Therefore, based on
[12, pp. 349], we expect to ﬁnish the MST construction
upon reviewing 1
n ) edges, which
can be much lower than m. So, it is not necessary
to sort the whole set E, but it is enough to select
and extract one by one the minimum-cost edges until
we complete the MST. The common solution of this
type consists in min-heapifying the set E, and later
performing as many min-extractions of the lowest cost
edge as needed (in [18], they do this in their Kruskal’s
demand sorting version). This is an implementation of
Incremental Sort. For this sake we modify lines 3 and
5 of Fig. 7: line 3 changes to heapify(E) and line 5 to
(e = {u, v}) ← E.extractMin().
Kruskal2 needs O(n) time to initialize both C and
mst, and O(m) time to heapify E. We expect to review
1
2 n ln n + O(n) edges in the While loop. For each of
these edges, we use O(log m) time to select and extract
the minimum element of the heap, and O(α(m, n)) time
to perform the union and ﬁnd operations. Therefore,
Kruskal2 average complexity is O(m + n log n log m).
As n − 1 ≤ m ≤ n2, Kruskal2 average complexity can
also be written as O(m + n log2 n).

5.2 IQS-based implementation of the Kruskal’s
MST algorithm. We can use IQS in order to incrementally 
sort E. After initializing C and mst, we create
the stack S, and push m into S. Later, inside the While
loop, we call IQS in order to obtain the k-th edge of E
incrementally. Fig. 8 shows our Kruskal’s MST variant,
that we call Kruskal3. Note that the expected number
of pivoting edges that we store in S is O(log m).

We need O(n) time to initialize both C and mst,
and constant time for S. We expect to review 1
2 n ln n +
O(n) edges within the While loop, thus we need
O(m + n log2 n) overall expected time for IQS and
O(nα(m, n) log n) time for all the union and ﬁnd
operations. Therefore, Kruskal3 average complexity
is O(m + n log2 n), just as Kruskal2.

Figure 8: Our Kruskal’s MST variant (Kruskal3).
Note the changes in lines 3 and 5 with respect to
Kruskal1.

3 GHz, 4 GB of RAM and local disk.
For each
experimental condition we show averages computed over
50 repetitions, for all competing implementations. The
weighted least square ﬁttings were performed with R
[21]. In order to illustrate the precision of our ﬁttings,
we also show the average percent error of residuals with
respect to real values (| y−ˆy
y |100%) for ﬁttings belonging
around to the 45% of the largest values2.

[22]

6.1 Evaluating IQS. We compared IQS against
PQS, QSS, and two online approach:
the ﬁrst
(called HEx), and
based on classical heaps [26]
the second based on sequence heaps
(called
SH, obtained from www.mpi-inf.mpg.de/~sanders/-
programs/spq/). The idea is to verify that IQS is in
practice a competitive algorithm for the Partial Sorting
problem for ﬁnding the smallest elements in ascending
order. For this sake, we use random permutations in
[0, m − 1], for m ∈ [105, 108], and we select the k ﬁrst
elements with k = 2j < m, for j ≥ 10. The selection is
incremental for IQS, HEx and SH, and in one shot for
PQS and QSS. We measure CPU time and the number 
of key comparisons, except for SH where we only
measure CPU time.

As it turned out to be more eﬃcient, we implement
HEx by using the bottom up heuristic [24] for extract-
Min: when the minimum is extracted, we lift up ele6 
Experimental results

We ran two experimental series with IQS. In the
ﬁrst series we compare IQS against other alternatives.
In the second we evaluate our Kruskal3 algorithm.
The experiments were run on an Intel Pentium 4 of

2Our ﬁttings are too pessimistic for small permutations or edge
sets, so we intend to show that they are asymptotically good. In
the ﬁrst series we compute the percent error for permutations of
length m ∈ [107, 108] for all the k values, approximately 45.4% of
the measures. In the second series we compute the percent error
for edge density in [16%, 100%] for all values of |V |, approximately
44.4% of the measures. Both turn out to be around 45%.

Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpIQS, PQS, QSS, HEx and SH CPU time m = 100e+06

IQS, PQS and QSS CPU time m = 100e+06

IQS
PQS
QSS
HEx
SH

 256

 128

 64

 32

 16

 8

 4

]
c
e
s
[
 

e
m

i
t
 

U
P
C

 2
 1000

IQS
PQS
QSS
HEx

 2.7

 2.65

 2.6

 2.55

 2.5

 2.45

 2.4

 2.35

]
c
e
s
[
 

e
m

i
t
 

U
P
C

 2.3

 1000

 10000

 100000

 1e+06

 1e+07

 1e+08

amount of selected elements k

 10000

 100000

 1e+06

amount of selected elements k

(a) CPU time for the ﬁve algorithms.

(b) Detail of CPU time for IQS, PQS, QSS and HEx.

Figure 9: Performance comparison between IQS, PQS, QSS, HEx and SH as a function of the amount of
searched elements k for diﬀerent values of set size m. Note the logscales in the plots.

PQScpu
PQScmp
IQScpu
PQScmp
QSScpu
QSScmp
HExcpu
HExcmp
SHcpu

Fitting

Error
25.79m + 16.87k log2 k
6.77%
2.138m + 1.232k log2 k
5.54%
25.81m + 17.44k log2 k
6.82%
2.138m + 1.232k log2 k
5.54%
25.82m + 17.20k log2 k
6.81%
2.140m + 1.292k log2 k
5.53%
23.85m + 67.89k log2 m 6.11%
1.904m + 0.967k log2 m 1.20%
9.165m log2 m + 66.16k
2.20%

Table 1: IQS, PQS, QSS, HEx and SH weighted least
square ﬁttings. For SH we only compute the CPU time
ﬁtting. CPU time is measured in nanoseconds.

ments on a min-path from the root to a leaf in the bottom 
level. Then, we place the rightmost element (the
last of the heap) into the free leaf, and bubble it up to
restore the min-heap condition. Using this heuristic we
perform only log2 m + O(1) key comparisons for each
extraction on average (saving up to half of the comparisons 
used by a straightforward implementation taken
from textbooks [5, 25]).

We summarize the experimental results in Figs. 9,
10 and 11, and Table 1. As can be seen from the least
square ﬁttings of Table 1, IQS CPU time performance
is 2.99% slower than that of its oﬄine version PQS.
The number of key comparisons is exactly the same,
as we expected from Section 4. This is an extremely
small price for permitting incremental sorting without
knowing in advance how many elements we wish to

retrieve, and shows that IQS is practical. Moreover,
as the pivots in the stack help us reuse the partitioning
work, our online IQS is 1.33% slower in CPU time and
uses 4.20% less key comparisons than the oﬄine QSS.
On the other hand, we obtain large improvements
with respect to online alternatives. According to the
insertion and deletion strategy of sequence heaps, we
compute its CPU time least square ﬁtting by noticing
that we can split the experiment in two stages. The
ﬁrst inserts m random elements into the priority queue,
and the second extracts the smallest k elements from it.
Then, we obtain a simpliﬁed O(m log m + k) complexity
model that shows that most of the work performed by
SH comes from the insertion process. Note that, if we
want a small fraction of the sorted sequence, we prefer
to pay a lower insertion and a higher extraction cost
(just like IQS) than to perform most of the work in
the insertions and a little in the extractions. Finally,
even when the online HEx with the bottom-up heuristic
uses at most 2m key comparisons to heapify the array,
and log m + O(1) key comparisons on average to extract
elements, numerous cache faults slow down its performance.
 As a matter of fact, HEx takes 3.88 times more
CPU time and 18.76% less key comparisons than IQS.
Fig. 9(a) compares the ﬁve algorithms. As can be
seen, even though SH is the best implementation to
sort the whole set, it is not so eﬃcient to sort just a
small fraction of it. We suspect that this is because SH
is cleverly optimized for the whole process of insertions
and extractions, but not for a small fraction. As we
have already said, its CPU time depends only mildly on
the number of extracted elements, as most of the work
performed by SH comes from the insertion process.

Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpIQS, PQS, QSS and HEx key comparisons m = 100e+06

IQS
PQS
QSS
HEx

 500

 450

 400

 350

 300

 250

 200

6
0
+
e
0
1

 
x
 
s
n
o
s
i
r
a
p
m
o
c
 
y
e
k

 150

 100000

 1e+06

 1e+07

amount of selected elements k

Figure 10: Detail of key comparisons for IQS, PQS,
QSS and HEx for m = 108 varying k. Note the logscale
in the plot.

 64

 32

 16

 8

 4

 2

 1

]
c
e
s
[
 

e
m

i
t
 

U
P
C

 0.5

 0.25

 0.125

 1000

IQS CPU time

IQS, m=100+e06
IQS, m=  70+e06
IQS, m=  40+e06
IQS, m=  10+e06

 10000

 100000

 1e+06

 1e+07

 1e+08

amount of selected elements k

Figure 11: IQS CPU time as a function of k and m.
Note the logscale in the plot.

HEx has the second worst CPU performance for k ≤
0.1m and the worst for k ∈ [0.1m, m], despite that it
makes less key comparisons than others when extracting
few objects, see Fig. 10. The reason is that classic
heaps (even with the bottom-up heuristic) do not take
advantage of the cache because of their poor locality of
reference, which slows down the performance of HEx.
Fig. 9(b) shows that PQS is the fastest algorithm when
sorting a small fraction of the set, but IQS and QSS
have rather similar behavior, and HEx follow them by
far, conﬁrming the results of our ﬁttings of Table 1.

Finally, Fig. 11 shows that, as k grows, IQS
behavior changes as follows. When k ≤ 0.01m, there
is no diﬀerence in the ﬁrst k element incremental
sorting, namely, the term m dominates the cost. When

Error
1.69%
Sorted edges
1.74%
Kruskal1cpu
Kruskal2cpu 39.99m + 46.30n log2 n log2 m 6.96%
4.17%
Kruskal3cpu

19.26m + 10.93n log2

12.85m log2 m

Fitting

0.532n ln n

2 n

Table 2: Weighted least square ﬁttings for Kruskal’s
MST versions (n = |V |, m = |E|). CPU time is
measured in nanoseconds.

0.01m < k ≤ 0.04m, there is a slight increase of both
CPU time and key comparisons, that is, both terms
m and k log k take part in the cost. Finally, when
0.04m < k ≤ m, term k log k leads the cost.
6.2 Evaluating Kruskal3. We now evaluate how
IQS improves Kruskal’s MST algorithm, so we compare
its three versions against state of the art alternatives.
We use synthetic graphs with edges chosen at random,
and with edge costs uniformly distributed in [0, 1]. We
consider graphs with |V | ∈ [2000, 26000], and graph
edge densities ρ ∈ [0.5%, 100%], where ρ = 2m
n(n−1) 100%.
According to the experiments of Section 6.1, we
preferred classical heaps using the bottom-up heuristic 
(HEx) over sequence heaps (SE) to implement
Kruskal2 in these experiments (as we expect to extract
1
2 n ln n + O(n) ≪ m edges). We also show results for
iMax and Prim2 implementations from [14], as well
as Prim1 in complete graphs3. We obtained both the
iMax and the optimized Prim2 implementations from
www.mpi-sb.mpg.de/~sanders/dfg/iMax.tgz.

For Kruskal’s versions we measure the CPU time,
memory requirements and the size of the edge subset
reviewed during the MST construction. Note that those
edges are the ones we incrementally sort. As the three
versions run over the same graphs, they review the same
subset of edges and use almost the same memory. For
Prim1 we only measure CPU time. For Prim2 and
iMax we measure CPU time and memory requirements.
We summarize the experimental results in Figs. 12,
14 and 13, and Table 2. Table 2 shows our least
squares ﬁttings for the MST experiments. First of all,
we compute the ﬁtting for the number of lowest-cost
edges Kruskal’s MST algorithm reviews to build the
tree. We obtain 0.532 |V | ln|V |, which is very close
to the theoretically expected value 1
2|V | ln|V |. Second,
we compute ﬁttings for the CPU cost of the three
versions of Kruskal’s using their theoretical complexity

3Note that we use the plain Prim1, that is, without priority
queues. This is the best choice to implement Prim in complete
graphs.

Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpKruskal1
Kruskal2
Kruskal3
Prim2
iMax

 16

 8

 4

 2

 1

MST CPU time, n = 20,000

MST Memory requirements, n = 20,000

Kruskal3
Prim2
iMax

 3

 2.5

 2

 1.5

 1

 0.5

]

B
G

[
 
s
t

n
e
m
e
r
i
u
q
e
r
 
y
r
o
m
e
M

]
c
e
s
[
 

e
m

i
t
 

U
P
C

 0.5

 0.25

 0.125

 0.0625

 0.03125

 0.5

 1

 2

 4

 8

 16

 32

 64

 100

graph edge density [%]

 0

 0.5

 1

 2

 4

 16
graph edge density [%]

 8

 32

 64

 100

Figure 12: Evaluating Kruskal3. MST CPU time,
dependence on ρ. For ρ = 100% Kruskal1 reaches 70.1
seconds. Note the logscale.

models. Note that, in terms of CPU time, Kruskal1
is 18.27 times and Kruskal2 is 2.14 times slower than
Kruskal3.

Fig. 12 compares the Kruskal’s versions, Prim2
and iMax for n = 20000 and graph edge density
ρ ∈ [0.5%, 100%]. As can be seen, Kruskal1 is, by
far, the slowest version, and, Kruskal3 is systematically
the best for all ρ. We also notice that, as ρ increases,
the advantage of our MST variant is more remarkable
against basic Kruskal’s MST algorithm. We could not
complete the series for Prim2 and iMax, as their
structures require too much space. For 20000 vertices
and ρ ≥ 32% these algorithms reach the 3 GB out-ofmemory 
threshold of our machine.
the memory requirements of
Kruskal3, iMax and Prim2 for n = 20000. Since our
Kruskal’s implementation sort edges in place, we require
a bit of extra memory to manage the edge incremental 
sorting. On the other hand, the additional structures 
of Prim2 and iMax increase heavily the memory 
consumption of the process. We suspect that these
high memory requirements trigger many cache faults
and slow down the CPU performance. As a result, for
large graphs, Prim2 and iMax become slower than
Kruskal3, despite their better complexity.

Fig. 13 shows

Figs. 14(a), 14(b), 14(c) and 14(d) show the comparison 
for four edge densities ρ = 2%, 8%, 32% and
100%, respectively.
In the four plots Kruskal3 is always 
the best Kruskal’s version for all sizes of set V and
all edge densities ρ. Moreover, Fig. 14(d) shows that
Kruskal3 is also better than Prim1, even in complete
graphs. On the other hand, Kruskal3 is better than
iMax in the four plots, and very competitive against

Figure 13: Memory used by Kruskal3, iMax and
Prim2 for |V | = 20000 nodes, dependence on ρ. As
can be seen, iMax and Prim2 exhausts the memory
for ρ ≥ 16% and ρ ≥ 32%, respectively.

Prim2, beating Prim2 in some cases (for |V | ≥ 18000
and 22000 vertices in ρ = 2% and 8%, respectively).
We suspect that this is due to the high memory usage 
of Prim2, which aﬀects cache eﬃciency. Note that
with ρ = 32% and 100% we could not ﬁnish the series 
with Prim2 and iMax because of their memory
requirements.

7 Conclusions

have

presented

We
IncrementalQuickSelect
(IQS), an algorithm to incrementally retrieve the next
smallest element from a set. IQS has the same average
complexity as existing solutions, but it is considerably
faster in practice. It is nearly as fast as the best algorithm 
that knows beforehand the number of elements
to retrieve. We have applied IQS to solve the graph
MST problem, showing that the IQS-based Kruskal’s
version is competitive against the best state-of-the-art
alternatives.

One trend of further work considers studying the
behaviour of our IQS-based Kruskal on diﬀerent graph
classes, and also research variants tuned for secondary
memory. Another trend is to look for other applications
of IQS. We ﬁnish by detailing two of them.

The ﬁrst is that we can use the IQS stack-ofpivots 
underlying idea to partially sort in increas-
ing/decreasing order starting from any place of the array.
 For instance, if we want to perform an incremental 
sorting in increasing order with a stack initialized
as the set size, we ﬁrst use QuickSelect to ﬁnd the
ﬁrst element we want, storing in the stack all the pivots 
larger than the ﬁrst element, and later we use IQS

Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpMST CPU time, density = 2%

MST CPU time, density = 8%

]
c
e
s
[
 

e
m

i
t
 

U
P
C

 0.3

 0.25

 0.2

 0.15

 0.1

 0.05

 0

Kruskal1
Kruskal2
Kruskal3
Prim2
iMax

 2

 4

 6

 8  10  12  14  16  18  20  22  24  26

]
c
e
s
[
 

e
m

i
t
 

U
P
C

 0.7

 0.6

 0.5

 0.4

 0.3

 0.2

 0.1

 0

Kruskal1
Kruskal2
Kruskal3
Prim2
iMax

 2

 4

 6

 8  10  12  14  16  18  20  22  24  26

nodes x 1,000

nodes x 1,000

(a) MST CPU time, dependence on n.

(b) MST CPU time, dependence on n.

MST CPU time, density = 32%

MST CPU time, density = 100%

]
c
e
s
[
 

e
m

i
t
 

U
P
C

 2.5

 2

 1.5

 1

 0.5

 0

]
c
e
s
[
 

e
m

i
t
 

U
P
C

Kruskal1
Kruskal2
Kruskal3
Prim2
iMax

 2

 4

 6

 8  10  12  14  16  18  20  22  24  26

 8

 7

 6

 5

 4

 3

 2

 1

 0

Kruskal1
Kruskal2
Kruskal3
Prim1
Prim2
iMax

 2

 4

 6

 8

 10  12  14  16  18  20  22  24  26

nodes x 1,000

nodes x 1,000

(c) MST CPU time, dependence on n.

(d) MST CPU time, dependence on n.

Figure 14: Evaluating Kruskal3. MST CPU time, dependence on n = |V | in (a), (b), (c) and (d) for ρ = 2%,
8%, 32% and 100%, respectively. For n = 26000, in (a) Kruskal1, Kruskal2 and iMax reach 2.67, 0.76 and
0.62 seconds; in (b) Kruskal1, Kruskal2 and iMax reach 9.08, 1.56 and 1.53 seconds; in (c) Kruskal1 and
Kruskal2 reach 37.02 and 4.82 seconds; in (d) Kruskal1, Kruskal2 and Prim1 reach 121.14, 13.84 and 25.96
seconds, respectively.

with the stack to search for the next elements (the other
pivots would be useful for decreasing order, initializing
the stack with −1). Moreover, with two stacks we can
make centered searching, namely, ﬁnding the k-th element,
 the (k + 1)-th and (k − 1)-th, the (k + 2)-th and
(k − 2)-th, and so on.
The second remarkable application is that we can
use IQS as an underlying implementation of the Heap
data structure [5, 25]. (Naturally, this allow us to speed
up HeapSort [26].)
In this application, we heapify
the set A by using IQS to search for the ﬁrst element,
paying on average O(m) CPU time, and then we extract
elements by using IQS incrementally, paying average
amortized time O(log k) for the k-th extraction. To

insert a new element x, we need to know which is the
array segment that corresponds to x (see Fig. 1). To
do this it is enough with reviewing the pivot stack S.
Assume S = {|A|, p1, p2, ..., pj}. From Lemma 2.1, we
know that A[p1] > A[p2] > . . . > A[pj]. So, to insert x
we need to ﬁnd the ﬁrst pivot pi such that A[pi] < x,
so as to place x at A[pi−1]. Then, we put A[pi−1] at
position pi−1 + 1 (and increment pi−1 in S). Then,
we move the old A[pi−1 + 1] value to A[pi−2], and so
on. Note that, as pivot closer to the bottom cover
exponentially larger areas, the insertion takes O(1) time
on average. With this IQS-based heap we can reach
the O(m+n log n) performance of Fibonacci-heap-based
Prim’s algorithm [8], yet using a rather simple heap.

Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpIn Proc. 2nd Workshop Algorithms and Data Structures
(WADS’91), LNCS 519, pages 400–411, 1991.

[19] S. Pettie and V. Ramachandran. An optimal minimum 
spanning tree algorithm. Journal of the ACM,
49(1):16–34, 2002.

[20] R. Prim. Shortest connection networks and some generalizations.
 Bell System Technical Journal, 36:1389–
1401, 1957.

[21] R Development Core Team. R: A language and
environment for statistical computing. R Foundation
for Statistical Computing, Vienna, Austria, 2004.

[22] P. Sanders. Fast priority queues for cached memory.

J. Exp. Algorithmics, 5:7, 2000.

[24] I. Wegener.

[23] R. E. Tarjan. Data structures and network algorithms.
Society for Industrial and Applied Mathematics, 1983.
bottom-up-heapsort, a new variant
of heapsort beating, on an average, quicksort (if
n is not very small). Theoretical Computer Science,
118(1):81–98, 1993.

[25] M. Weiss. Data structures & algorithm analysis in

Javatm. Addison-Wesley, 1999.

[26] J. Williams. Algorithm 232 (heapsort). Communications 
of the ACM, 7(6):347–348, 1964.

Acknowledgment

We wish to thank the very valuable comments from the
anonymous referees.

References

[1] R. Baeza-Yates and B. Ribeiro-Neto. Modern Information 
Retrieval. Addison-Wesley, 1999.

[2] M. Blum, R. W. Floyd, V. Pratt, R. L. Rivest, and
R. E. Tarjan. Time bounds for selection. Journal of
Computer and System Sciences, 7(4):448–461, 1973.

[3] J. M. Chambers. Algorithm 410 (partial sorting).

Communications of the ACM, 14(5):357–358, 1971.

[4] B. Chazelle. A minimum spanning tree algorithm with
Journal of the

inverse-ackermann type complexity.
ACM, 47(6):1028–1047, 2000.

[5] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and
C. Stein. Introduction to Algorithms. The MIT Press,
2nd edition, 2001.

[6] R. W. Floyd. Algorithm 245 (treesort). Communications 
of the ACM, 7:701, 1964.

[7] M. L. Fredman, R. Sedgewick, D. D. Sleator, and R. E.
Tarjan. The pairing heap: a new form of self-adjusting
heap. Algorithmica, 1(1):111–129, 1986.

[8] M. L. Fredman and R. E. Tarjan. Fibonacci heaps
and their uses in improved network optimization algorithms.
 Journal of the ACM, 34(3):596–615, 1987.

[9] G. Gonnet and R. Baeza-Yates. Handbook of Algorithms 
and Data Structures. Addison-Wesley, 2nd edition,
 1991.

[10] C. A. R. Hoare. Algorithm 65 (find). Communications

of the ACM, 4(7):321–322, 1961.

[11] C. A. R. Hoare. Quicksort. Computer Journal,

5(1):10–15, 1962.

[12] S. Janson, D. Knuth, T.  Luczak, and B. Pittel. The
birth of the giant component. Random Structures &
Algorithms, 4(3):233–358, 1993.

[13] D. Karger, P. N. Klein, and R. E. Tarjan. A randomized 
linear-time algorithm to ﬁnd minimum spanning
trees. J. ACM, 42(2):321–328, 1995.

[14] I. Katriel, P. Sanders, and J. Tr¨aﬀ. A practical
minimum spanning tree algorithm using the cycle
property. Research Report MPI-I-2002-1-003, Max-
Planck-Institut f¨ur Informatik, October 2002.

[15] I. Katriel, P. Sanders, and J. Tr¨aﬀ. A practical
minimum spanning tree algorithm using the cycle
property. In 11th European Symposium on Algorithms
(ESA’03), LNCS 2832, pages 679–690, 2003.

[16] J. Kruskal. On the shortest spanning subtree of a graph
and the traveling salesman problem. Proceedings of the
American Mathematical Society, 7:48–50, 1956.

[17] C. Mart´ınez. Partial quicksort.

In Proc. 6th ACMSIAM 
Workshop on Algorithm Engineering and Experiments 
and 1st ACM-SIAM Workshop on Analytic
Algorithmics and Combinatorics, pages 224–228, 2004.
[18] B. Moret and H. Shapiro. An empirical analysis of
algorithms for constructing a minimum spanning tree.

Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php