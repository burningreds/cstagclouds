0
1
0
2

 

v
o
N
9
1

 

 
 
]
S
D
.
s
c
[
 
 

1
v
2
3
5
4

.

1
1
0
1
:
v
i
X
r
a

New Algorithms on Wavelet Trees

and Applications to Information Retrieval ⋆

Travis Gagie1, Gonzalo Navarro2⋆⋆, and Simon J. Puglisi3⋆ ⋆ ⋆

1 Department of Computer Science, Aalto University, Finland. travis.gagie@gmail.com
2 Department of Computer Science, University of Chile, Chile. gnavarro@dcc.uchile.cl

3 School of Computer Science and Information Technology, Royal Melbourne Institute of Technology, Australia.

{simon.puglisi}@rmit.edu.au

Abstract. Wavelet trees are widely used in the representation of sequences, permutations, text collections,
 binary relations, discrete points, and other succinct data structures. We show, however, that this
still falls short of exploiting all of the virtues of this versatile data structure. In particular we show how
to use wavelet trees to solve fundamental algorithmic problems such as range quantile queries, range
next value queries, and range intersection queries. We explore several applications of these queries in
Information Retrieval, in particular document retrieval in hierarchical and temporal documents, and in
the representation of inverted lists.

1

Introduction

The wavelet tree [34] is a versatile data structure that stores a sequence S[1, n] of elements from
a symbol universe [1, σ] within asymptotically the same space required by a plain representation
of the sequence, n log σ (1 + o(1)) bits.4 Within that space, the wavelet tree is able to return any
sequence element S[i], and also to answer two queries on S that are fundamental in compressed
data structures for text retrieval:

rankc(S, i) = number of occurrences of symbol c in S[1, i],
selectc(S, j) = position of the jth occurrence of symbol c in S.

The time for these three queries is O(log σ).5 Originally designed for compressing suﬃx arrays
[34], the usefulness of the wavelet tree for many other scenarios was quickly realized. It was soon
adopted as a fundamental component of a large class of compressed text indexes, the FM-index
family, giving birth to most of its modern variants [27, 43, 28, 45].

The connection between the wavelet tree and an old geometric structure by Chazelle [19] made
it evident that wavelet trees could be used for range counting and reporting points in the plane.
More formally, given a set of t points P = {(xi, yi), 1 ≤ i ≤ t} on a discrete grid [1, n] × [1, σ],
wavelet trees answer the following basic queries:

range count (P, xs, xe, ys, ye) = number of pairs (xi, yi) such that xs ≤ xi ≤ xe, ys ≤ yi ≤ ye,
range report (P, xs, xe, ys, ye) = list of those pairs (xi, yi) in some order,

⋆ Early parts of this work appeared in SPIRE 2009 [32] and SPIRE 2010 [53].
⋆⋆ Partially supported by Fondecyt Grant 1-080019, Chile.
⋆ ⋆ ⋆ Partially supported by the Australian Research Council.

4 Our logarithms are in base 2 unless otherwise stated. Moreover, within a time complexity, log x should be understood 
as max(1, log x).

5 This can be reduced to O(cid:0)1 + log σ

algorithms we develop in this article.

log log n(cid:1) [28] using multiary wavelet trees, but these do not merge well with the new

both in O(log σ) time [44].6 These new capabilities were subsequently used to design powerful
succinct representations of two-dimensional point grids [44, 14, 16], permutations [12], and binary
relations [7], with applications to other compressed text indexes [50, 20, 21], document retrieval
problems [66] and many others.

In this paper we show, by uncovering new capabilities, that the full potential of wavelet trees
is far from realized. We show that the wavelet tree allows us to solve the following fundamental
queries:

range quantile(S, i, j, k) = kth smallest value in S[i, j],

range next value(S, i, j, x) = smallest S[r] ≥ x such that i ≤ r ≤ j,

range intersect(S, i1, j1, . . . , ik, jk) = distinct common values in S[i1, j1], S[i2, j2], . . . , S[ik, jk].

The ﬁrst two are solved in time O(log σ), whereas the cost of the latter is O(log σ) per delivered
value plus the size of the intersection of the tries that describe the diﬀerent values in S[i1, j1] and
S[i2, j2]. A crude upper bound for the latter is O(min(σ, j1 − i1 + 1, j2 − i2 + 1)), however, we give
an adaptive analysis of our method, showing it requires O(α log σ
α ) time, where α is the so-called
alternation complexity of the problem [8].

All these algorithmic problems are well known. Har-Peled and Muthukrishnan [35] describe
applications of range median queries (a particular case of range quantile) to the analysis of Web
advertizing logs. Stolinski et al. [64] use them for noise reduction in grey scale images. Similarly,
Crochemore et al. [23] use range next value queries for interval-restricted pattern matching, and
Keller et al. [40] and Crochemore et al. [22] use them for many other sophisticated pattern matching
problems. Hon et al. [37] use range intersect queries for generalized document retrieval, and in a
simpliﬁed form the problem also appears when processing conjunctive queries in inverted indexes.
We further illustrate the importance of these fundamental algorithmic problems by uncovering
new applications in several Information Retrieval (IR) activities. We ﬁrst consider document retrieval 
problems on general sequences. This generalizes the classical IR problems usually dealt with
on Natural Language (NL), and deﬁnes them in a more general setting where one has a collection
C of strings (i.e., the documents), and queries are strings as well. Then one is interested in any substring 
of the collection that matches the query, and the following IR problems are deﬁned (among
several others):

doc listing(q) = distinct documents where query q appears,

doc frequency(q, d) = number of occurrences of query q in document d,

doc intersect (q1, . . . , qk) = distinct documents where all queries q1, . . . , qk appear.

These generalized IR problems have applications in text databases where the concept of words
does not exist or is diﬃcult to deﬁne, such as in Oriental languages, DNA and protein sequences,
program code, music and other multimedia sequences, and numeric streams in general. The interest
in carrying out IR tasks on, say, Chinese or Korean is obvious despite the diﬃculty of automatically
delimiting the words. In those cases one resorts to a model where the text is seen as a sequence
of symbols and must be able to retrieve any substring. Agglutinating languages such as Finnish
or German present similar problems to a certain degree. While indexes for plain string matching
are well known, supporting more sophisticated IR tasks such as ranked document retrieval is a

6 Again, this can be reduced to O(cid:0)1 + log σ

log log n(cid:1) using multiary wavelet trees [14].

2

very recent research area. It is not hard to imagine that similar capabilities would be of interest
in other types of sequences: for example listing the functions where two given variables are used
simultaneously in a large software development system, or ranking a set of gene sequences by the
number of times a given substring marker occurs.

By constructing a suﬃx array A [47] on the text collection, one can obtain in time O(|q| log |C|)
the range of A where all the occurrence positions of q in C are listed. The classical solution to
document retrieval problems [49] starts by deﬁning a document array D giving the document to
which each suﬃx of A belongs. Then problems like document listing boil down to listing the distinct
values in a range of D, and intersection of documents becomes the intersection of values in a range of
D. Both are solved with our new fundamental algorithms (the former with range quantile queries).
Other queries such as computing frequencies reduce to a pair of rankd queries on D.

Second, we generalize document retrieval problems to other scenarios. The ﬁrst scenario is
temporal documents, where the document numbers are consistent with increasing version numbers
of the document set. Then one is interested in restricting the above queries to a given interval of
time (i.e., of document numbers). A similar case is that of hierarchical documents, which contain
each other as in the case of an XML collection or a ﬁle system. Here, restricting the query to a
range of document numbers is equivalent to restricting it to a subtree of the hierarchy. However,
one can consider more complex queries in the hierarchical case, such as marking a set of retrievable
nodes at query time and carrying out the operations with respect to those nodes. We show how to
generalize our algorithms to handle this case as well.

Finally, we show that variants of our new fundamental algorithms are useful to enhance the
functionality of inverted lists, the favorite data structures for both ranked and full-text retrieval in
NL. Each of these retrieval paradigms requires a diﬀerent variant of the inverted list, and one has
to maintain both in order to support all the activities usually required in an IR system. We show
that a wavelet tree representation of the inverted lists supports not only the basic functionality of
both representations within essentially the space of one, but also several enhanced functionalities
such as on-the-ﬂy stemming and restriction of documents, and most list intersection algorithms.

The article is structured as follows. In Section 2 we review the wavelet tree data structure and
its basic algorithmics. Section 3 reviews some basic IR concepts. Then Section 4 describes the new
solutions to fundamental algorithmic problems, whereas Sections 5 and 6 explore applications to
various IR problems. Finally we conclude in Section 7.

2 Wavelet Trees

A wavelet tree T [34] for a sequence S[1, n] over an ordered alphabet [1, σ] is an ordered, strictly
binary tree whose leaves are labeled with the distinct symbols in S in order from left to right, and
whose internal nodes Tv store binary strings Bv. The binary string at the root contains n bits and
each is set to 0 or 1 depending on whether the corresponding character of S is the label of a leaf in
T ’s left or right subtree. For each internal node v of T , the subtree Tv rooted at v is itself a wavelet
tree for the subsequence Sv of S consisting of the occurrences of its leaf labels in Tv. For example,
if S = abracadabra and the leaves in T ’s left subtree are labeled a, b and c, then the root stores
00100010010, the left subtree is a wavelet tree for abacaaba and the right subtree is a wavelet tree
for rdr.

In this article we consider balanced wavelet trees, where the number of leaves to the left and to
the right of each node diﬀer at most by 1. The important properties of such a wavelet tree for our
purposes are summarized in the following lemma.

3

Lemma 1. The wavelet tree T for a sequence S[1, n] on alphabet [1, σ] with u distinct symbols
requires at most n log σ + O(n) bits of space, and can be constructed in O(n log u) time.

Proof. By the description above the wavelet tree has height ⌈log u⌉ and can be easily built in time
O(n log u) (we need to determine the u ≤ min(n, σ) distinct values ﬁrst, but this is straightforward
within the same complexity).

As for the space, note that the wavelet tree stores only the bitmaps Bv for all the nodes. The
total length of the binary strings is at most n at each level of the wavelet tree, which adds up to
n⌈log u⌉. Apart from the bitmaps, there is the binary tree of O(u) nodes. Instead of storing the
nodes, one can concatenate all the bitmaps of the same depth and simulate the nodes [44], so this
requires just one pointer per level, O(log u log n) = o(n) bits.

The distinct values must be stored as well. Indeed, if σ ≤ n, we can just assume all the σ
values exist and the wavelet tree will have ⌈log σ⌉ levels and the theorem holds. Otherwise, we can
mark the unique values in a bitmap U [1, σ], which can be stored in compressed form [54] so that
it requires u log σ
u + O(u) bits and the ith distinct number is retrieved as select1(U, i) in constant
time7. Adding up all the spaces we get n log u + O(n) + u log σ
u + O(u) ≤ n log σ + O(n) bits, and
the construction time is O(u).

Finally, we can represent the bitmaps with data structures that support constant-time (binary)
rank and select operations [55]. The overall extra space stays within O(n) bits and the construction
time within O(n log u). Binary rank and select operations are essential to operate on the wavelet
⊓⊔
trees, as seen shortly.

The most basic operation of T is to replace S, by retrieving any S[i] value in O(log u) time.
The algorithm is as follows. We ﬁrst examine the ith bit of the root bitmap Broot. If Broot[i] = 0,
then symbol S[i] corresponds to a leaf descending by the left child of the root, and by the right
otherwise. In the ﬁrst case we continue recursively on the left child, Tl. However, position i must now
be mapped to the subsequence handled at Tl. Precisely, if the 0 at Broot[i] is the jth 0 in Broot, then
S[i] is mapped to Sv[j]. In other words, when we go left, we must recompute i ← rank0(Broot, i).
Similarly, when we go right we set i ← rank1(Broot, i).

When the tree nodes are not explicit, we ﬁnd out the intervals corresponding to Bv in the
levelwise bitmaps as follows. Broot is a single bitmap. If node v has depth d, and Bv corresponds
to interval Bd[l, r], then its left child corresponds to Bd+1[l, k] and its right child to Bd+1[k + 1, r],
where k = rank0(Bd, r) − rank0(Bd, l − 1) [44].

The wavelet tree can also answer rankc(S, i) queries on S with a mechanism similar to that for
retrieving S[i]. This time one decides whether to go left or right depending on which subtree of the
current node the leaf labeled c appears in, and not on the bit values of Bv. The ﬁnal i value when
one reaches the leaf is the answer. Again, the process requires O(log u) time.

Finally, selectc(S, j) is also supported in O(log u) time using the wavelet tree. This time we
start from position j at the leaf labeled c;8 this indeed corresponds to the jth occurrence of symbol
c in S. If the leaf is a left child of its parent v, then the position of that c in Sv is select0(Bv, j),
and select1(Bv, j) if the leaf is a right child of v. We continue recursively from this new j value
until reaching the root, where j is the answer.

7 For this, one has to use a constant-time data structure for select [48] in their internal bitmap H[1, 2u] [54].
8 If the tree nodes are not explicitly stored then we ﬁrst descend to the node labeled c in order to delimit the interval

corresponding to the leaf and to all of its ancestors in the levelwise bitmaps.

4

Algorithm 1 Basic wavelet tree algorithms: On the wavelet tree of sequence S, access(vroot, i)
returns S[i]; rank(vroot, c, i) returns rankc(S, i); and select(vroot, c, i) returns selectc(S, i).
access(v, i)

rank(v, c, i)

if v is a leaf then
return label(v)

else if Bv[i] = 0 then

if v is a leaf then

return i

else if c ∈ labels(vl) then

select(v, c, i)

if v is a leaf then

return i

return access(vl, rank0(Bv, i))

return rank(vl, c, rank0(Bv, i))

else if c ∈ labels(vl) then

else

else

return select0(Bv, select(vl, c, i))

return access(vr, rank1(Bv, i))

return rank(vr, c, rank1(Bv, i))

else

end if

end if

return select1(Bv, select(vr, c, i))

end if

Algorithm 2 Range algorithms: count(vroot, xs, xe, [ys, ye]) returns range count(P, xs, xe, ys, ye)
on the wavelet tree of sequence P ; and report(vroot, xs, xe, [ys, ye]) outputs all pairs (y, f ), where
ys ≤ y ≤ ye and y appears f > 0 times in P [xs, ys].
count(v, xs, xe, rng)

report(v, xs, xe, rng)

if xs > ys ∨ labels(v) ∩ rng = ∅ then

if xs > ys ∨ labels(v) ∩ rng = ∅ then

return 0

else if label(v) ⊆ rng then

return xe − xs + 1

return

else if v is a leaf then

output (label(v), xe − xs + 1)

else
xs
l ← rank0(Bv, xs − 1) + 1
l ← rank0(Bv, xe)
xe
r ← xs − xs
xs
return count(vl, xs
count(vr, xs

r ← xe − xe
l

l , xe

l , xe
r, xe

l , rng)+
r, rng)

else
xs
l ← rank0(Bv, xs − 1) + 1
l ← rank0(Bv, xe)
xe
r ← xs − xs
xs
report(vl, xs
report(vr, xs

r ← xe − xe
l
l , rng)
r, rng)

l , xe
l , xe
r, xe

end if

end if

Algorithm 1 gives pseudocode for the basic access, rank and select algorithms on wavelet trees.
For all the pseudocodes in this article we use the following notation: v is a wavelet tree node and
vroot is the root node. If v is a leaf then its symbol is labels(v) ∈ [1, σ]. Otherwise vl and vr are its
left and right children, respectively, and Bv is its bitmap. For all nodes, labels(v) is the range of
leaf labels that descend from v (a singleton in case of leaves).

As we make use of range count and a form of range report queries in this article, we give
pseudocode for them as well, in Algorithm 2. Indeed, range count is a kind of multi-symbol rank
and range report is a kind of multi-symbol access.

In Section 4 we develop new algorithms based on wavelet trees to solve fundamental algorithmic 
problems. We prove now a few simple lemmas that are useful for analyzing range count and
range report , as well as many other algorithms we introduce throughout the article. Most results
are folklore but we reprove them here for completeness.

Lemma 2. Any contiguous range of ℓ leaves in a wavelet tree is the set of descendants of O(log ℓ)
nodes.

Proof. Start with the ℓ leaves. For each consecutive pair that shares the same parent, replace the
pair by their parent. At most two leaves are not replaced, and at most ℓ/2 parents are created.
Repeat the operation at the parent level, and so on. After working on ⌈log ℓ⌉ levels, we have at
most two nodes per wavelet tree level, for a total of O(log ℓ) nodes covering the original interval.
Lemma 3. Any set of r nodes in a wavelet tree of u leaves has at most O(cid:0)r log u

r (cid:1) ancestors.

5

Proof. Consider the paths from the root to each of the r nodes. They cannot be all disjoint. They
share the least if they diverge from depth ⌈log r⌉. In this case, all the O(r) tree nodes of depth up
to ⌈log r⌉ belong to some path, and from that depth each of the r paths is disjoint, adding at most
⌈log u⌉ − ⌈log r⌉ distinct ancestors. The total is O(cid:0)r + r log u
r (cid:1).
Lemma 4. Any set of r nodes covering a contiguous range of leaves in a wavelet tree of u leaves
has at most O(r + log u) ancestors.

Proof. We ﬁrst count all the ancestors of the ℓ consecutive leaves covered and then subtract the
sizes of the subtrees rooted at the r nodes v1, v2, . . . , vr. Start with ℓ leaves. Mark all the parents of
the leaves. At most ⌈ℓ/2⌉ < 1 + ℓ/2 distinct parents are marked, as most pairs of consecutive leaves
will share the same parent. Mark the parents of the parents. At most ⌈(1 + ℓ/2)/2⌉ < 3/2 + ℓ/4
parents of parents are marked. At height h, the number of marked nodes is always less than 2+ℓ/2h.
Adding over all heights, we have that the total number of ancestors is at most 2ℓ + 2 log u. Now let
ℓi be the number of leaves covered by node vi, so that P1≤i≤r ℓi = ℓ. The subtree rooted at each
vi has 2ℓi − 1 nodes. By subtracting those subtree sizes and adding back the r root nodes we get
2ℓ + 2 log u − (2ℓ − r) + r = O(r + log u).

From the lemmas we conclude that count in Algorithm 2 (left) takes time O(log u): it ﬁnds the
O(log(ye − ys + 1)) nodes that cover the range [ys, ye] (Lemma 2), by working in time proportional
to the number of ancestors of those nodes, O(log(ye − ys + 1) + log u) = O(log u) (Lemma 4).
Interestingly, report in Algorithm 2 (right) can be analyzed in two ways. On one hand, it takes
time O(ye − ys + log u) as it arrives at most at the ye − ys + 1 consecutive leaves and thus it works
on all of their ancestors (Lemma 4). On the other hand, if it outputs r results (which are not
necessarily consecutive), it also works proportionally to the number of their ancestors, O(cid:0)r log u
r (cid:1)
(Lemma 3). The latter is an output-sensitive analysis. The following lemma shows that the cost is
indeed O(cid:16)log u + r log ye−ys+1

(cid:17).

r

Lemma 5. The number of ancestors of r wavelet tree leaves chosen from ℓ contiguous leaves, on
a wavelet tree of u leaves, is O(cid:16)log u + r log ℓ

r(cid:17).

Proof. By Lemma 2 those leaves are covered by c = O(log ℓ) nodes. Say that ri of the r searches
fall within the ith of those subtrees, then by Lemma 3 the number of nodes accessed within that
subtree is at most O(cid:16)ri log ℓ
r/c(cid:17). Given the limit
r(cid:17). The ancestors reached above those subtrees are O(c + log u) = O(log u) by
on c this is O(cid:16)r log ℓ
Lemma 4, for a total of O(cid:16)log u + r log ℓ

ri(cid:17), adding up by convexity to at most O(cid:16)r log ℓ

r(cid:17).

3

Information Retrieval Concepts

3.1 Suﬃx and Document Arrays

Let C be a collection of documents (which are actually strings over an alphabet [1, σ]) D1, D2, . . . , Dm.
Assume strings are terminated by a special character “$”, which does not occur elsewhere in the
collection. Now we identify C with the concatenation of all the documents, C[1, n] = D1D2 . . . Dm.
Each position i deﬁnes a suﬃx C[i, n]. A suﬃx array [47] of C is an array A[1, n] where the integers

6

[1, n] are ordered in such a way that the suﬃx starting at A[i] is lexicographically smaller than that
starting at A[i + 1], for all 1 ≤ i < n.

Put another way, the suﬃx array lists all the suﬃxes of the collection in lexicographic order.
Since any substring of C is the preﬁx of a suﬃx, ﬁnding the occurrences of a query string q in C
is equivalent to ﬁnding the suﬃxes that start with q. These form a lexicographic range of suﬃxes,
and thus can be found via two binary searches in A (accessing C for the string comparisons). As
each step in the binary search may require comparing up to |q| symbols, the total search time
is O(|q| log n). Once the interval A[sp, ep] is determined, all the occurrences of q start at A[i] for
sp ≤ i ≤ ep. Compressed full-text self-indexes permit representing both C and A within the space
required to represent C in compressed form, and for example determine the range [sp, ep] within
time O(|q| log σ) and list each A[i] in time O(cid:16)log1+ǫ n(cid:17) for any constant ǫ > 0 [28, 51].

For listing the distinct documents where q appears, one option is to ﬁnd out the document to
which each A[i] belongs and remove duplicates. This, however, requires Ω(ep − sp + 1) time; that
is, it is proportional to the total number of occurrences of q, occ = ep − sp + 1. This may be much
larger than the number of distinct documents where q appears, docc.

Muthukrishnan [49] solved this problem optimally by deﬁning a so-called document array D[1, n],
so that D[i] is the document suﬃx A[i] belongs to. Other required data structures in his solution are
an array C[1, n], so that C[i] = maxj<i D[j] = D[i], and a data structure to compute range minimum
queries on C, RM QC(i, j) = argmini≤k≤jC[k]. Muthukrishnan was able to list all the distinct
documents where q appears in time O(docc) once the interval A[sp, ep] was found. However, the data
structures occupied O(n log n) bits of space, which is too much if we consider the compressed selfindexes 
that solve the basic string search problem. Another problem is that the resulting documents
are not retrieved in ascending order, which is inconvenient for several purposes.

V¨alim¨aki and M¨akinen [66] were the ﬁrst to illustrate the power of wavelet trees for this problem.
By representing D with a wavelet tree, they simulated C[i] = selectD[i](D, rankD[i](D, i − 1))
without storing it. By using a 2n-bit data structure for RM Q [29], the total space was reduced to
n log m(1 + o(1)) + O(n) bits, and still Muthukrishnan’s algorithm was simulated within reasonable
time, O(docc log m).

Ranked document retrieval is usually built around two measures: term frequency, tf d,q =
doc frequency(q, d) is the number of times the query q appears in document d, and the document 
frequency df q, the number of diﬀerent documents where q appears. For example a typical 
weighting formula is wd,q = tf d,q × idf q, where idf q = log m
is called the inverse docudf 
q
ment frequency. Term frequencies are easily computed with wavelet trees as doc frequency(q, d) =
rankd(D, ep) − rankd(D, sp − 1). Document frequencies can be computed with just 2n + o(n) more
bits for the case of the D array [61], and on top of a wavelet tree for the C array for more general
scenarios [31].

In Section 5 we show how our new algorithms solve the document listing problem within the
same time complexity O(docc log m), without using any RM Q data structure, while reporting the
documents in increasing order. This is the basis for a novel algorithm to list the documents where
two (or more) queries appear simultaneously. We extend these solutions to temporal and hierarchical
document collections.

3.2

Inverted Indexes

The inverted index is a classical IR structure [5, 67], lying at the heart of most modern Web search
engines and applications handling natural-language text collections. By “natural language” texts

7

one refers to those that can be easily split into a sequence of words, and where queries are also
limited to words or sequences thereof (phrases). An inverted index is an array of lists. Each array
entry corresponds to a diﬀerent word of the collection, and its list points to the documents where
that word appears. The set of diﬀerent words is called the vocabulary. Compared to the document
retrieval problem for general strings described above, the restriction of word queries allows inverted
indexes to precompute the answer to each possible word query.

Two main variants of inverted indexes exist [4, 69]. Ranked retrieval is aimed at retrieving
documents that are most “relevant” to a query, under some criterion. As explained, a popular
relevant formula is wd,q = tf d,q × idf q, but others built on tf and df , as well as even more complex
ones, have been used. In inverted indexes for ranked retrieval, the lists point to the documents where
each word appears, storing also the weight of the word in that document (in the case of tf × idf ,
only tf values are stored, since idf depends only on the word and is stored with the vocabulary).
IR queries are usually formed by various words, so the relevance of the documents is obtained by
some form of combination of the various individual weights. Algorithms for this type of query have
been intensively studied, as well as diﬀerent data organizations for this particular task [57, 67, 69,
1, 65]. List entries are usually sorted by descending weights of the term in the documents.

Ranked retrieval algorithms try to avoid scanning all the involved inverted lists. A typical
scheme is Persin’s [57]. It ﬁrst retrieves the shortest list (i.e., with highest idf ), which becomes
the candidate set, and then considers progressively longer lists. Only a preﬁx of the subsequent
lists is considered, where the weights are above a threshold. Those documents are merged with
the candidate set, accumulating relevance values for the documents that contain both terms. The
longer the list, the least relevant is the term (as the tf s are multiplied by a lower idf ), and thus
the shorter the considered preﬁx of its list. The threshold provides a time/quality tradeoﬀ.

The second variant is the inverted indexes for so-called full-text retrieval (also known as boolean
retrieval). These simply ﬁnd all the documents where the query appears. In this case the lists
point to the documents where each term appears, usually in increasing document order. Queries
can be single words, in which case the retrieval consists simply of fetching the list of the word; or
disjunctive queries, where one has to fetch the sorted lists of all the query words and merge them;
or conjunctive queries, where one has to intersect the lists. Intersection queries are nowadays more
popular, as this is Google’s default policy to treat queries of several words. Another important
query where intersection is essential is the phrase query, where intersecting the documents where
the words appear is the ﬁrst step.

While intersection can be achieved by scanning all the lists in synchronization, faster approaches
aim to exploit the the phenomenon that some lists are much shorter than others [68]. This general
idea is particularly important when the lists for many terms need to be intersected. The amount
of recent research on intersection of inverted lists witnesses the importance of the problem [26,
8, 3, 6, 10, 63, 24, 9] (see Barbay et al. [11] for a comprehensive survey). In particular, in-memory
algorithms have received much attention lately, as large main memories and distributed systems
make it feasible to hold the inverted index entirely in RAM.

Needless to say, space is an issue in inverted indexes, especially when combined with the goal of
operating in main memory. Much research has been carried out on compressing inverted lists [67,
52, 69, 24], and on the interaction of compression with query algorithms, including list intersections.
Most of the list compression algorithms for full-text indexes rely on the fact that the document
identiﬁers are increasing, and that the diﬀerences between consecutive entries are smaller on the
longer lists. The diﬀerences are thus represented with encodings that favor small numbers [67].

8

Random access is supported by storing sampled absolute values. For lists sorted by decreasing
weights, these techniques can still be adapted: most documents in a list have small weight values,
and within the same weight one can still sort the documents by increasing identiﬁer.

A serious problem of the current state of the art is that an IR system usually must support both
types of retrieval: ranked and full-text. For example, this is necessary in order to provide ranked
retrieval on phrases. Yet, to maintain reasonable space eﬃciency, the list must be ordered either
by decreasing weights or by increasing document number, but not both. Hence one type of search
will be signiﬁcantly slower than the other, if aﬀordable at all.

In Section 6 we show that wavelet trees allow one to build a data structure that permits, within
the same space required for a single compressed inverted index, retrieving the list of documents
of any term in either decreasing-weight or increasing-identiﬁer order, thus supporting both types
of retrieval. Moreover, we can eﬃciently support the operations needed to implement any of the
intersection algorithms, namely: retrieve the ith element of a list, retrieve the ﬁrst element larger
than x, retrieve the next element, and several more complex ones. In addition, our structure oﬀers
novel ways of carrying out several operations of interest. These include, among others, the support
for stemming and for structured document retrieval without any extra space cost.

4 New Algorithms

4.1 Range Quantile

Two n¨aive ways of solving query range quantile(i, j, k) are by sequentially scanning the range in
time O(j − i + 1) [13], and storing the answers to the O(cid:0)n3(cid:1) possible queries in a table and returning
answers in O(() 1) time. Neither of these solutions is really satisfactory.

Until recently there was no work on range quantile queries, but several authors wrote about
range median queries, the special case in which k is half the length of the interval between i and j.
Krizanc et al. [41] introduced the problem of preprocessing for range median queries and gave four
solutions, three of which require time superlogarithmic in n. Their fourth solution requires almost
quadratic space, storing O(cid:0)n2 log log n/ log n(cid:1) words to answer queries in constant time (a word holds
log σ bits). Bose et al. [15] considered approximate queries, and Har-Peled and Muthukrishnan [35]
and Gfeller and Sanders [33] considered batched queries. Recently, Krizanc et al.’s fourth solution
was superseded by one due to Petersen and Grabowski [58, 59], who slightly reduced the space
bound to O(cid:16)n2(log log n)2/ log2 n(cid:17) words.

At about the same time we presented the early version of our work [32], Gfeller and Sanders [33]
gave a similar O(n)-word data structure that supports range median queries in O(log n) time and
observed in a footnote that “a generalization to arbitrary ranks will be straightforward”. A few
months later, Brodal and Jørgensen [18] gave a more involved data structure that still takes O(n)
words but only O(log n/ log log n) time for queries. These two papers have now been merged [17].
Very recently, Jørgensen and Larsen [39] proved a matching lower bound for any data structure
that takes n logO(1) n space.

In the sequel we show that, if S is represented using a wavelet tree, we can answer general range
quantile queries in O(log u) time, where u ≤ min(σ, n) is the number of distinct symbols in S. As
explained in Section 2, within these n log σ + O(n) bits of space we can also retrieve any element
S[i] in time O(log u), so our data structure actually replaces S (requiring only O(n) extra bits). The
latest alternative structure [39] may achieve slightly better time but it requires O(n log n) extra
bits of space, apart from being signiﬁcantly more involved.

9

Algorithm 3 New wavelet tree algorithms: rqq(vroot, i, j, k) returns (range quantile(S, i, j, k), f )
on the wavelet tree of sequence S, assuming k ≤ j − i + 1, and where f is the frequency of the
returned element in S[i, j]; rnv(vroot, i, j, 0, x) returns (range next value(S, i, j, x), f, p), where f
is the frequency and p is the smallest rank of the returned element in the multiset S[i, j] (the
element is ⊥ if no answer exists); and rint(vroot, i1, j1, i2, j2, [ys, ye]) solves an extension of query
range intersect (S, i1, j1, i2, j2) outputting triples (y, f1, f2), where y are the common elements, f1
is their frequency in S[i1, j1], and f2 is their frequency, in S[i2, j2], and moreover ys ≤ y ≤ ye.
rqq(v, i, j, k)

rnv(v, i, j, p, x)

rint(v, i1, j1, i2, j2, rng)

if v is a leaf then

if i > j then

return (label(v), j − i + 1)

return (⊥, 0, 0)

else

il ← rank0(Bv, i − 1) + 1
jl ← rank0(Bv, j)
ir ← i − il, jr ← j − jr
nl ← jl − il + 1
if k ≤ nl then

return rqq(vl, il, jl, k)

else

else if v is a leaf then

return (x, j − i + 1, p)

else

il ← rank0(Bv, i − 1) + 1
jl ← rank0(Bv, j)
ir ← i − il, jr ← j − jr
nl ← jl − il + 1
if x ∈ labels(vr) then

return rqq(vr, ir, jr, k − nl)

return rnv(vr, ir, jr, p + nl, x)

end if

end if

else

(y, f ) ← rnv(vl, il, jl, p, x)
if y 6=⊥ then

return (y, f )

else

return rnv(vr, ir, jr, p + nl,
min labels(vr))

end if

end if

end if

if i1 > j1 ∨ i2 > j2 then

return

else if labels(v) ∩ rng = ∅ then

return

else if v is a leaf then

output (label(v),

j1 − i1 + 1, j2 − i2 + 1)

else

i1l ← rank0(Bv, i1 − 1) + 1
j1l ← rank0(Bv, j1)
i1r ← i1 − i1l, j1r ← j1 − j1r
i2l ← rank0(Bv, i2 − 1) + 1
j2l ← rank0(Bv, j2)
i2r ← i2 − i2l, j2r ← j2 − j2r
rint(vl, i1l, j1l, i2l, j2l, rng)
rint(vr, i1r, j1r , i2r, j2r, rng)

end if

Theorem 1 Given a sequence S[1, n] storing u distinct values over alphabet [1, σ], we can represent
S within n log σ + O(n) bits, so that range quantile queries are solved in time O(log u). Within that
time we can also know the number of times the returned value appears in the range.

Proof. We represent S using a wavelet tree T , as in Lemma 1. Query range quantile(i, j, k) is
then solved as follows. We start at the root of T and consider its bitmap Broot. We compute
nl = rank0(Broot, j) − rank0(Broot, i − 1), the number of 0s in Broot[i, j]. If nl ≥ k, then there
are at least k symbols in S[i, j] that label leaves descending from the left child Tl of T , and thus
we must ﬁnd the kth symbol on Tl. Therefore we continue recursively on Tl with the new values
i ← rank0(Broot, i − 1) + 1, j ← rank0(Broot, j), and k unchanged. Otherwise, we must descend
to the right child, mapping the range to i ← rank1(Broot, i − 1) + 1 and j ← rank1(Broot, j). In
this case, since we have discarded nl numbers that are already to the left of the kth value, we set
k ← k − nl. When we reach a leaf, we just return its label. Furthermore, we have that the value
occurs j − i + 1 times in the original range. Since T is balanced and we spend constant time at each
node as we descend, our search takes O(log u) time.
⊓⊔

Algorithm 3 (left) gives pseudocode. Note that, if u is constant, then so is our query time.

10

4.2 Range Next Value

Again, two naive ways of solving query range next value(i, j, x) on sequence S[1, n] are scanning in
O(j − i + 1) worst-case time, and precomputing all the possible answers in O(cid:0)n3(cid:1) space to achieve
constant time queries. Crochemore et al. [23] reduced the space to O(cid:0)n2(cid:1) words while preserving the
constant query time. Later, Crochemore et al. [22] further improved the space to O(cid:0)n1+ǫ(cid:1) words.
Alternatively, M¨akinen et al. [46, Lemma 4] give a simple O(n)-words space solution based on an
augmented binary search tree. This yields time O(log u), where once again u ≤ min(n, σ) is the
number of distinct symbols in S and [1, σ] the domain of values. For the particular case of semiinﬁnite 
queries (i.e., i = 1 or j = n) one can use an O(n)-words and O(log log n) time solution by
Gabow et al. [30].

By using wavelet trees, we also solve the general problem in time O(log u). Our space is better 
than the simple linear-space solution, n + O(n/ log σ) words (n of which actually replace the
sequence).

Theorem 2 Given a sequence S[1, n] storing u distinct values over alphabet [1, σ], we can represent
S within n log σ + O(n) bits, so that range next value queries are solved in time O(log u). Within
the same time we can return the position of the ﬁrst occurrence of the value in the range.

Proof. We represent S using a wavelet tree T , as in Lemma 1. Query range next value(i, j, x) is
then solved as follows. We start at the root of T and consider its bitmap Broot. If x labels a leaf
descending by the right child Tr, then the left subtree is irrelevant and we continue recursively on
Tr, with the new values i ← rank1(Broot, i − 1) + 1 and j ← rank1(Broot, j). Otherwise, we must
descend to the left child Tl, mapping the range to i ← rank0(Broot, i−1)+1 and j ← rank0(Broot, j).
If our interval [i, j] becomes empty at any point, we return with no value.

When the recursion returns from Tr with no value, we return no value as well. When it returns
from Tl with no value, however, there is still a chance that a number ≥ x appears on the right in
the interval [i, j]. Indeed, if we descend to Tr and map i and j accordingly, and the interval is not
empty, then we want the minimum value of that interval, that is, the minimum value in Sl[i, j].
This is a particular case of a range quantile query carried out on a wavelet (sub)tree Tr. The overall
time is O(log u).
⊓⊔

Algorithm 3 (middle) gives pseudocode. While our space gain may not appear very impressive,
we point out that our solution requires only O(n) extra bits on top of the sequence (if we accept
the logarithmic slowdown in accessing S via the wavelet tree). Moreover, we can use the same
wavelet tree to carry out the other algorithms, instead of requiring a diﬀerent data structure for
each. This will be relevant for the applications, which need support for several of the operations
simultaneously.

4.3 Range Intersection

The query range intersect(i1, j1, i2, j2), which ﬁnds the common symbols in two ranges of a sequence
S[1, n] over alphabet [1, σ], appears naturally in many cases. In particular, a simpliﬁed variant where
the two ranges to intersect are sorted in increasing order arises when intersecting full-text inverted
lists, when solving intersection, phrase, or proximity queries.

11

Worst-case complexity measures depending only on the range sizes are of little interest for this
problem, as an adversary can always force us to completely traverse both ranges, and time complexity 
O(j1 − i1 + j2 − i2 + 1) is easily achieved through merging9. More interesting are adaptive
complexity measures, which deﬁne a ﬁner diﬃculty measure for problem instances. For example, in
the case of sorted ranges, an instance where the ﬁrst element of the second range is larger than the
last element of the ﬁrst range is easier (one can establish the emptiness of the result with just one
well-chosen comparison) than another where elements are mixed.

A popular measure for this case is called alternation and noted α [8]. For two sorted sequences
without repetitions, α can be deﬁned as the number of switches from one sequence to the other in
the sorted union of the two ranges, or equivalently, as the time complexity of a nondeterministic
program that guesses which comparisons to carry out, or equivalently as the length of a certiﬁcate
that, through the results of comparing elements of both sequences, is suﬃcient to prove what the
result is. This deﬁnition can be extended to intersecting k ranges. Formally, the measure α is deﬁned
through a function C : [1, σ] → [0, k], where C[c] gives the number of any range where symbol c
does not appear, and C[c] = 0 if c appears in all ranges. Then α is the number of zeros in C plus
the minimum possible number of switches (i.e., C[c] 6= C[c + 1]) in such a function. A lower bound
α (cid:17), where
in terms of alternation (still holding for randomized algorithms) [8] is Ω (cid:16)α · P1≤r≤k log nr
nr is the length of the rth range. There exist adaptive algorithms matching this lower bound [26,
8, 9].

We show now that the wavelet tree representation of S[1, n] allows a rather simple intersection
algorithm that approaches the lower bound, even if one starts from ranges of disordered values,
possibly with repetitions. For k = 2, we start from both ranges [i1, j1] and [i2, j2] at the root of the
wavelet tree. If either range is empty, we stop. Otherwise we map both ranges to the left child of the
root using rank0, and to the right child using rank1. We continue recursively on the branches where
both intervals are nonempty. If we reach a leaf, then its corresponding symbol is in the intersection,
and we know that there are j1 − i1 + 1 copies of the symbol in the ﬁrst range, and j2 − i2 + 1 in
the second. For k ranges [ir, jr], we maintain them all at each step, and abandon a path as soon as
any of the k ranges becomes empty. Algorithm 3 (right) gives pseudocode for the case k = 2.

Lemma 6. The algorithm just described requires time O(cid:0)αk log u
tinct values in the sequence and α is the alternation complexity of the problem.

α(cid:1), where u is the number of disProof.
 Consider the function p : Σ → {0, 1}∗, so that p(c) is a bit stream of length equal to the
depth of the leaf representing symbol c in the wavelet tree. More precisely, p[i] is 0 if the leaf
descends from the left child of its ancestor at depth i, and 1 otherwise. That is, p(c) describes the
path from the root to the wavelet tree leaf labeled c.

Now let Tr be the trie (or digital tree) formed by the strings p(c) for all those c appearing in
S[ir, jr], and let T∩ be the trie formed by the branches present in all Tr, 1 ≤ r ≤ k. It is easy to see
that T∩ contains precisely the wavelet tree nodes traversed by our intersection algorithm, so the
complexity of our algorithm is O(|T∩|).

We show now that |T∩| has at most α leaves. The leaves of T∩ that are wavelet tree leaves
correspond to the symbols that belong to the intersection, and thus to the number of 0s in any
function C. This is accounted for in measure α. So let us focus on the other leaves of T∩. Consider
two consecutive leaves of T∩ that are not wavelet tree leaves u1 and u2, and any symbols c1 < c2

9 If the ranges are already ordered; otherwise a previous sorting is necessary.

12

whose wavelet tree leaves v1 and v2 descend from u1 and u2, respectively. If there were a single
range S[ir, jr] where c1 and c2 would not belong, then the lowest common ancestor of v1 and v2
would not belong to T∩, and thus there could not be two leaves u1 and u2 in T∩. Therefore, for
each pair of consecutive leaves in T∩ there is at least one switch in C, and thus there are at most
α leaves in T∩. Thus, by Lemma 3, T∩ has O(cid:0)α log u
α(cid:1) nodes. To obtain the ﬁnal cost we multiply
⊓⊔
by k, which is the cost of maintaining the k ranges throughout the traversal.

In the case where all the lists are sorted and without repetitions (so nr ≤ u), our algorithm
complexity is pretty close to the lower bound, matched when all nr = u. Note also that our algorithm
is easily extended to handle the so-called (t, k)-thresholded problem [8], where we return any symbol
appearing in at least t of the k ranges. It is simply a matter of abandoning a range only when more
than k − t ranges have become empty.

A diﬀerent form of carrying out the intersection is via the query range next value(S, i, j, x):
Start with x1 ← range next value(S, i1, j1, 1) and x2 ← range next value(S, i2, j2, x1). If x2 > x1
then continue with x1 ← range next value(S, i1, j1, x2); if now x1 > x2 then continue with x2 ←
range next value(S, i2, j2, x1); and so on. If at any moment x1 = x2 then output it as part of the
intersection and continue with x1 ← range next value(S, i1, j1, x2 + 1). It is not hard to see that
there must be a switch in C for each step we carry out, and therefore the cost is O(α log u).

To reduce the cost to O(cid:0)α log u

α(cid:1), we carry out a ﬁngered search in range next value queries,
that is, we remember the path traversed from the last time we called range next value(S, i, j, x)
and only retraverse the necessary part upon calling range next value(S, i, j, x′) for x′ > x. For this
reason we move upwards from the leaf where the query for x was solved until reaching the ﬁrst
node v such that x′ ∈ labels(v), and complete the rnv procedure from that node. Since the total
work done by this point is proportional to the number of distinct ancestors of the α leaves arrived
at, the complexity is O(cid:0)α log u
This second procedure is the basis of most algorithms for intersecting two or more lists [11].
The rint method we have presented is simpler, potentially faster, and more ﬂexible (e.g., it is easily
adapted to t-thresholded queries). Moreover, it is speciﬁc to the wavelet tree.

α(cid:1) by Lemma 3.

5 Document Listing and Intersections

The algorithm for range report (P, xs, xe, ys, ye) queries described in Section 2 can be used to solve
problem doc listing(q), as follows. As explained in Section 3.1, use a (compressed) suﬃx array A
to ﬁnd the range A[sp, ep] corresponding to query q, and use a wavelet tree on the document array
D[1, n] on alphabet [1, m], so that the answer is the set of distinct document numbers d1 < d2 <
. . . < ddocc in D[sp, ep]. Then range report (D, sp, ep, 1, m) returns the docc document numbers,
in order, in total time O(cid:0)docc log m
docc(cid:1). Moreover, procedure report in Algorithm 2 also retrieves
the frequencies of each di in D[sp, ep], outputting the pairs (di, tf q,di) within the same cost. (As
explained, arbitrary frequencies tf d,q = doc frequency(q, d) can also be obtained in time O(log m)
by two rankd queries on D.) Alternative solutions using range quantile or range next value queries
are possible, and will be explored later for other applications.

As explained in Section 3.1, this is simpler and requires less space than various previous solutions10,
 and has the additional beneﬁt of delivering the documents in increasing document identiﬁer
order. This enables us to extend the algorithm to more complex scenarios, as shown in Section 6.

10 It is even better than our previous solution based on range quantile queries [32], which takes time O(docc log m).

13

Now consider k queries q1, q2, . . . , qk, and the problem of listing the documents where all those
queries appear (i.e., problem doc intersect (q1, . . . , qk)). With the suﬃx array we can map the queries
to ranges [spr, epr], and then the problem is that of ﬁnding the distinct document numbers that appear 
in all those ranges. This corresponds exactly to query range intersect(D, sp1, ep1, . . . , spk, epk),
which we have solved in Section 4.3. We have indeed solved a more general variant where we list the
documents (and their tf d,qr values) where at least t of the k terms appear. Note this corresponds
to the disjunctive query for the case t = 1.

5.1 Temporal and Hierarchical Documents

The simplest extension when we have versioned or hierarchical documents is to restrict queries
doc listing(q) and doc intersect(q1, . . . , qk) to a range of documents [dmin, dmax], which represents
a temporal interval or a subtree of the hierarchy in which we are interested. Such a restricted document 
listing and intersection is easily supported by setting rng = [dmin, dmax] in procedures report
(cid:17)
(Algorithm 2) and rint (Algorithm 3), respectively. The complexities are O(cid:16)docc log dmax−dmin+1
for listing and O(cid:16)α log dmax−dmin+1

(cid:17) for intersections, due to Lemma 5.

docc

α

When the hierarchical documents represent nodes in an XML collection, other queries of interest
become obvious. Indeed, how to carry out ranking on XML collections is an unresolved issue, with
very complex ranking proposals counterweighted by others advocating simple measures. Rather
than trying to cover such a broad topic, we refer the reader to comprehensive surveys and discussions
in the article by Hiemstra and Mihajlovi´c [36], the PhD thesis of Pehcevski [56, Ch. 2], and the
recent book by Lalmas [42, Ch. 6].

In most models, the frequency of a term within a subtree, and the size of such subtree, are central
to the deﬁnition of ranking strategies. The latter is usually easy to compute from the sequence
representation. The former, a generalization of doc frequency to ranges, can actually be computed
with query range count(D, sp, ep, dl, dr), deﬁned in the Introduction (see also Algorithm 2), where
[sp, ep] is the suﬃx array range corresponding to query q, and [dl, dr] is the range of documents
corresponding to our structural element. This query also takes time O(log m).

5.2 Restricting to Retrievable Units

We focus now on a more complex issue that is also essential for XML ranked retrieval. Query
languages such as XPath and XQuery deﬁne structural constraints together with terms of interest.
For example, one might wish to retrieve books about the term “cryptography”, or rather book
sections about that term, in each case ranked by the relevance of the term. Thus the deﬁnition of
the retrievable unit (books, sections) comes in the query together with the terms (cryptography)
whose relevance is to be computed with respect to the retrievable units that contain it. We show
now how to support a simple model where the retrievable units are deﬁned by an XML tag name,
and consider other models at the end. We report the smallest retrievable unit containing the query
occurrences.

Following common models of XML data (e.g., [2]), we consider that text data can appear only
at the leaves of the XML structure, so that we create extra leaves if text data appears between
consecutive structural elements (a bitmap may be used to mark leaves that do not contain any text
data, but we omit this detail here for clarity). Thus, each leaf of the XML tree will be associated
with a document number, 1 to m, so that di will be the document associated to the ith leaf. The

14

XML tree, containing n nodes, will be represented using a sequence P [1, 2n] of parentheses [38].
These are obtained through a preorder traversal, by appending an opening parenthesis when we
reach a node and a closing one when we leave it. A tree node will be identiﬁed with the position of
its opening parenthesis in P . Several succinct data structures can represent the parentheses within
2n + o(n) bits and simulate a wealth of tree operations in constant time (e.g., [62]).

In addition we represent a sequence Tag [1, 2n] giving the tag name associated to each parenthesis
in P . Sequence Tag is represented using a wavelet tree in 2n log τ + O(n) bits of space, where τ
is the number of distinct tags in the collection. Finally, for each distinct tag name t we store a
parenthesis representation Pt of the nodes of the XML tree that are tagged t. The total space for
P , Tag, and the Pt trees is 2n log τ + O(n) bits.

A ﬁrst task we can carry out is, given an occurrence in document number (i.e., leaf) i, ﬁnd
expand (t, i), the range of documents (i.e., leaves) corresponding to its lowest ancestor tagged t. This
allows us to ﬁnd the closest retrievable unit to which the occurrence at leaf i must be assigned.
We use operation j = selectLeaf (P, i) to ﬁnd the ith leaf of P . Then r = rankt(Tag, j) ﬁnds the
rank of the last occurrence of t in Tag preceding j. If Pt[r] = ′(′, then r is the lowest ancestor of
i tagged t, otherwise it is r ← parent(Pt, r), the node tagged t that encloses position r. Finally,
position r is mapped back to the global tree P with p = selectt(Tag , r), and we return the range
of leaves corresponding to p, expand (t, i) = leaf range(p) = [rankLeaf (P, p) + 1, rankLeaf (P, p + 2 ·
subtreeSize(P, p))], where rankLeaf and subtreeSize are self-explanatory tree operations. The process
takes O(log τ ) time, dominated by the costs to operate on Tag. Algorithm 4 (left) gives pseudocode.
If we now want to count the number of occurrences of our query q in a retrievable node p, we
need to count the number of occurrences of the range of leaves (i.e., document numbers) below p
within the interval D[sp, ep] corresponding to query q. Such a range is easily obtained in constant
time as [dl, dr] = leaf range(p). Then the result is range count (D, sp, ep, dl, dr), as explained.

To carry out document listing restricted to structural elements tagged t, we build on range
next value queries. We start with d1 = range next value(D, sp, ep, 1), which gives us the smallest 
(leaf) document number in D[sp, ep]. Now we compute [dl1, dr1] = expand (t, d1), the range
of the lowest node tagged t that contains d1. Then we ﬁnd the next leaf document using d2 =
range next value(D, sp, ep, dr1 + 1), and so on. In general, di+1 = range next value(D, sp, ep, dri +
1). Algorithm 4 (left) gives pseudocode. The cost per document retrieved is O(log τ + log m).
However, using the ﬁngered search on rnv outlined in Section 4.3, the overall cost reduces to
O(cid:0)docc(cid:0)log τ + log m
docc(cid:1)(cid:1). If we wish to additionally restrict the retrieval to documents in the range
[dmin, dmax], we simply start with d1 = range next value(D, sp, ep, dmin) and stop when we retrieve 
a document larger than dmax. The cost improves to O(cid:16)docc(cid:16)log τ + log dmax−dmin+1
(cid:17)(cid:17) due
to Lemma 5. Complexity returns to O(docc log m) if we compute also the frequency in each retrievable 
unit using hdfreq.

docc

Finally, to carry out intersections restricted to retrievable units, we follow in principle the
same algorithm outlined in Section 4.3. The diﬀerence is that we must not split retrievable ranges.
Therefore, when we are at any wavelet tree node, before going to the left child that represents the
range of symbols [da, db] and/or to the right child representing range [db + 1, dc], we ﬁrst ﬁnd out
whether there is a retrievable unit covering [db, db+1]. To do this we compute expand (t, db) = [dl, dr]
and expand (t, db + 1) = [dl′, dr′]. Since the leaves are consecutive, there are only two possibilities:
either [dl, dr] = [dl′, dr′] or they are disjoint (and dr = db and dl′ = db + 1). In the latter case
we proceed with the recursion as in Section 4.3. In the former case, we descend to the left child
with document range restricted to [da, dl − 1], then report document [dl, dr] if it belongs to the

15

the node in P for expand (t, i) and leafRange(P, p) computes

Algorithm 4 Algorithms for hierarchical document listing and intersections: exp(Tag , P, Pt, t, i)
leaf range(p);
computes
hdfreq(P, D, sp, ep, p) computes the frequency of p in D[sp, ep]; hdlist(A, D, Tag , P, Pt, t, q) lists
the retrievable units where q appears; and hdint(Tag , P, Pt, t, vroot, i1, j1, i2, j2, rng) lists the retrievable 
units with leaves in both D[i1, j1] and D[i2, j2], with their frequencies in both ranges, and
subject to belonging to document range rng (which is assumed not to split any retrievable unit).
exp(Tag, P, Pt, t, i)

hdint(Tag, P, Pt, t, v, i1, j1, i2, j2, rng)

j ← selectLeaf (P, i)
r ← rank(Tag, t, j)
if Pt[r] = ′)′ then

r ← parent(Pt, r)

end if
return select(Tag, t, r)

leafRange(P, p)

return [rankLeaf (P, p) + 1,
rankLeaf (P, p + 2 · subtreeSize (P, p))]

hdfreq(P, D, sp, ep, p)

[dl, dr] ← leafRange(P, p)
return count(D, sp, ep, [dl, dr])

hdlist(A, D, Tag , P, Pt, t, q, [dmin, dmax])

[sp, ep] ← pattern search(A, q)
v ← root(D)
(d, f, r) ← rnv(v, sp, ep, 0, dmin)
while d 6=⊥ ∧ d ≤ dmax do

p ← exp(Tag, P, Pt, t, d)
output p
[dl, dr] ← leafRange(P, p)
(d, f, r) ← rnv(v, sp, ep, 0, dr + 1)

if i1 > j1 ∨ i2 > j2 ∨ rng = ∅ then

return

else if v is a leaf then

output (label(v), j1 − i1 + 1, j2 − i2 + 1)

else

[dl, dr] ← ∅, f1 ← 0, f2 ← 0
i1l ← rank0(Bv, i1 − 1) + 1, j1l ← rank0(Bv, j1)
i1r ← i1 − i1l, j1r ← j1 − j1r
i2l ← rank0(Bv, i2 − 1) + 1, j2l ← rank0(Bv, j2)
i2r ← i2 − i2l, j2r ← j2 − j2r
if i1l ≤ j1l ∧ i2l ≤ j2l ∧ i1r ≤ j1r ∧ i2r ≤ j2r ∧

labels(vl) ∩ rng 6= ∅ ∧ labels(vr) ∩ rng 6= ∅ then
pl ← exp(Tag, P, Pt, t, max labels(vl))
pr ← exp(Tag , P, Pt, t, min labels(vr))
if pl = pr then

[dl, dr] ← leafRange(P, pl)
f1 ← count(v, i1, j1, [dl, dr])
f2 ← count(v, i2, j2, [dl, dr])

end if

end if
hdint(Tag, P, Pt, t, vl, i1l, j1l, i2l, j2l, (labels(vl) ∩ rng) − [dl, dr])
if f1 > 0 ∧ f2 > 0 then

output (pl, f1, f2)

end if
hdint(Tag, P, Pt, t, vr, i1r, j1r , i2r, j2r , (labels(vr) ∩ rng) − [dl, dr])

end while

end if

intersection, and ﬁnally descend to the right child with document range restricted to [dr + 1, dc].
By “descending with document range restricted to [x, y]” we mean we abandon branches whose
document range has no intersection with [x, y], and such restrictions are inherited as we descend.
Algorithm 4 (right) gives pseudocode. The complexity is the same as if the retrievable units were
materialized into consecutive document numbers, that is, O(cid:0)α log m
α (cid:1) under this interpretation. The
only extra cost is the computation of f1 and f2. Note, however, that these are computed with a
range count query restricted to the local subtree, and thus the cost at height h is O(h). Moreover,
this is computed only for nodes of trie T∩ (recall Lemma 6) having two children, that is, at most
α times. The most expensive case is thus when all those α nodes are as high as possible in the
wavelet tree, in which case the O(h) costs add up to O(cid:0)α log m
α (cid:1) and do not aﬀect the complexity.
Once again, we can restrict the results to a range [dmin, dmax] with the usual time improvement.

Other possibilities for marking the retrievable documents can be supported, as long as one is
able to ﬁnd the lowest retrievable ancestor of any leaf. For example we could mark retrievable
nodes in a bitmap B[1, 2n] aligned with P , where we set to 1 the opening and closing parentheses
of retrievable nodes. Then we can compute expand (B, i) via rank and select operations on B in
constant as follows. We start with j = selectLeaf (P, i), then p = select1(rank1(B, j)), then if
P [p] = ′)′ we recompute p = parent(P, p), and ﬁnally expand (B, i) = leaf range(p). In cases where

16

the retrievable units are deﬁned dynamically, say from previous parts of the query processing, we can
store them in a balanced tree, so that query select1(rank1(B, j)) (which is actually a predecessor
query) can be answered in O(log n) time.

6

Inverted Lists

Recall m is the total number of documents in the collection and let ν be the number of diﬀerent
terms. Let Lt[1, df t] be the list of document identiﬁers where term t appears, in decreasing weight
order (for concreteness we will assume we store tf values in the lists as weights, but any weight
will do). Let n = Pt df t be the total number of occurrences of distinct terms in the documents,
and N = Pt,d tf t,d the total length, in words, of the text collection (thus m ≤ n ≤ min(mν, N )).
Finally, let |q| be the number of terms in query q.

We propose to concatenate all the lists Lt into a unique sequence L[1, n], and store for each
term t the starting position st of list Lt within L. The sequence L of document identiﬁers is then
represented with a wavelet tree.

The tf values themselves are stored in diﬀerential and run-length compressed form in a separate
sequence. More precisely, we mark the vt diﬀerent tf t,d values of each list in a bitmap Tt[1, mt], where
mt = maxd tf t,d, and the vt points in Lt[1, df t] where value tf d,t changes, in a bitmap Rt[1, df t].
Thus one can obtain tf t,Lt[i] = select1(Tt, vt − rank1(Rt, i) + 1). The st sequence is also represented
using a bitmap S[1, n] providing rank/select operations. Thus we can recover st = select1(S, t),
and also rank1(S, i) tells us which list L[i] belongs to.

Let us analyze the space required by our representation. According to Lemma 1, the wavelet tree
of L occupies is n log m + O(n) bits. The classical encoding of inverted ﬁles, when documents are
sorted by increasing document identiﬁer, records the consecutive diﬀerences using δ-codes [67]. This
needs at most Pt df t log m
n bits plus lower-order terms, which is asymptotically less
than our space. If, however, the lists are sorted by decreasing tf values, then diﬀerential encoding
can only be used on some parts of the lists. Yet, n log m (plus lower-order terms) is still an upper
bound to the space required to list the documents. As can be seen, no inverted index representation
takes more space than our wavelet tree. However, it must be remembered that our wavelet tree will
oﬀer the combined functionality of both inverted indexes, and more.

≤ n log mν

df t

+ O(vt) + vt log df t
vt

We also store the tf and the st values. The former is encoded with Tt and Rt. We use Okanohara
and Sadakane’s representation [54] for Tt and Pˇatra¸scu’s [55] for Rt (see Section 2), to achieve total
space vt log mt
+ o(df t) bits and retain constant time access to tf values. This
vt
space is similar to that needed to represent, in a traditional tf -sorted index, each new tf t,d value and
the number of entries that share it. The st values require ν log n
ν + o(n) bits using again Pˇatra¸scu
[55], which gives constant-time access to st and requires less space than the usual pointers from
the vocabulary to the list of each term. Overall our data structure takes at most n log(mν) + O(n)
bits.

We will now consider the classical and extended operations that can be carried out with our
data structure. In particular we will show how to give some support for hierarchical document
retrieval (as already seen for general documents) and for stemmed searches without using any extra
space. One common way to support stemming is by coalescing terms having the same root at index
construction time. However, the index is then unable to provide non-stemmed searching. One can
of course index the stemmed and non-stemmed occurrence of each term, but this costs space. Our
method can provide both types of search without using any extra space provided all the variants

17

of the same stemmed word be contiguous in the vocabulary (this is in many cases automatic as
stemmed terms share the same root, or preﬁx).

6.1 Full-Text Retrieval

The full-text index, rather than Lt, requires a list Ft, where the same documents are sorted by
increasing document identiﬁer. Diﬀerent kinds of access operations need to be carried out on Ft.
We now show how all these can be supported in O(log m) time or less.

Direct retrieval First, with our wavelet tree representation of L we can compute any speciﬁc
value Ft[k] in time O(log m). This is equivalent to ﬁnding the kth smallest value in L[st, st+1 − 1],
that is, query range quantile(L, st, st+1 − 1, k) described in Section 4.1.

We can also extract any segment Ft[k, k′], in order, in time O(cid:16)(k′ − k + 1) log m

k′−k+1(cid:17), that
is, faster per document as we extract more documents. The algorithm is as for range quantile on
quantiles k to k′ simultaneously, going just by one branch when both k and k′ choose the same
branch, and splitting the interval into two separate searches when they do not. We arrive at k′−k+1
leaves of the wavelet tree, thus the cost follows from Lemma 3.

Another useful operation is ﬁngered search, that is, to ﬁnd Ft[k′] after having visited Ft[k], for
some k′ > k. This is slightly more complex than for consecutive range next value queries. We need
to store log m values mδ, eδ and vδ, where m0 = ∞ and e1 = 0, and the others are computed as
follows when we obtain Ft[k]: at wavelet tree node v of depth δ (the root being depth 1) we set
vδ ← v and, if we must go to the left child, then we set mδ ← eδ + nl and eδ+1 ← eδ; else we set
mδ ← mδ−1 and eδ+1 ← eδ + nl. Here nl is the value local to the node (recall rqq in Algorithm 3).
Therefore eδ counts the values skipped to the left, and mδ is the maximum k′ value such that the
downward paths to compute Ft[k] and Ft[k′] coincide up to depth δ. Now, to compute Ft[k′], we
consider all the δ values, from largest to smallest, until ﬁnding the ﬁrst one such that k′ ≤ mδ.
From there on we recompute the downward path, resetting mδ, eδ, and vδ accordingly.

If we carry out this operation r times, across a range [k, k′], the cost is O(cid:16)log m + r log k′−k+1

r

(cid:17)

by Lemma 5. Algorithm 5 depicts the new extended variants of rqq.

Intersection algorithms The most important operation in the various list intersection algorithms
described in the literature is to ﬁnd the ﬁrst k such that Ft[k] ≥ d, given d. This is usually solved
with a combination of sampling and linear, exponential, or binary search. In our case, this operation
takes time O(log m) with query range next value(L, st, st+1−1, d) described in Section 4.2. Our time
complexity is not far from the O(log(st+1 − st)) of traditional approaches. Moreover, as explained in
Section 4.3, we can use ﬁngered searches on rnv to achieve time O(cid:0)log m + r log m
r (cid:1) for r accesses.
Furthermore, if all the accesses are for documents in a range [d, d′] then, by Lemma 5, the cost will
be O(cid:16)log m + r log d′−d+1
(cid:17) time. This is indeed the time required by r successive searches using
exponential search.

r

Finally, we can intersect the lists Ft and Ft′ using range intersect(L, st, st+1 − 1, st′, st′+1 − 1), in
adaptive time O(cid:0)α log m
α (cid:1) — recall Section 4.3. As explained, this can be extended to intersecting
k terms simultaneously, and to report documents where a minimum number of the terms appear.

Other operations of interest If the range of terms [t, t′] represent the derivatives of a single
stemmed root, we might wish to act as if we had a single list Ft,t′ containing all the documents

18

Algorithm 5 Extended variants of range quantile algorithms: mrqq(vroot, i, j, k, k′) outputs all
the (distinct) values range quantile(S, i, j, k) to range quantile(S, i, j, k′), with their frequencies, on
the wavelet tree of sequence S, assuming k′ ≤ j − i + 1; frqq1(vroot, i, j, k) returns the same as
rqq(vroot, i, j, k) but prepares the iterator for subsequent ﬁngered searches; those are carried out
by calling frqq1(vroot, k), where it is assumed that the k values increase at each call; frqq′ is the
recursive procedure that reprocesses the needed part of the path.
mrqq(v, i, j, k, k′)

frqq′(v, i, j, k, δ)

frqq1(v, i, j, k)

if v is a leaf then

output (label(v), j − i + 1)

else

il ← rank0(Bv, i − 1) + 1
jl ← rank0(Bv, j)
ir ← i − il, jr ← j − jr
nl ← jl − il + 1
if k ≤ nl then

mrqq(vl, il, jl, k, min(nl, k′))

end if
if k′ > nl then

mrqq(vr, ir, jr, max(k − nl, 1), k′)

end if

end if

m0 ← ∞
e1 ← v
i∗ ← i
j ∗ ← j
return frrq′(v, i, j, k, 1)

frqq(v, k)

δ ← height of v
while k > mδ−1 do

δ ← δ − 1

end while
return frqq′(vδ, i∗, j ∗, k, δ)

if v is a leaf then

output (label(v), j − i + 1)

else

vδ ← v
il ← rank0(Bv, i − 1) + 1
jl ← rank0(Bv, j)
ir ← i−il, jr ← j −jr, nl ← jl −il +1
if k ≤ nl then

mδ ← eδ + nl
eδ+1 ← eδ
return frqq′(vl, il, jl, k, δ + 1)

else

mδ ← mδ−1
eδ+1 ← eδ + nl
return frrq′(vr, ir, jr, k, δ + 1)

end if

end if

where they occur. Indeed, if we apply our previous algorithm to obtain Ft[k] from L[st, st+1 − 1],
on the range L[st, st′+1 − 1], we obtain precisely Ft,t′ [k], if we understand that a document d may
repeat several times in the list if diﬀerent terms in [t, t′] appear in d. Still we can obtain the list
of docc distinct documents for a range of terms [t, t′] with exactly the same method as for the D
array, described at the beginning of Section 5, in time O(cid:0)docc log m
Furthermore, the algorithms to ﬁnd the ﬁrst k such that Ft[k] ≥ d, can be applied verbatim to
obtain the same result for Ft,t′ [k] ≥ d. All the variants of these queries are directly supported as
well. Our intersection algorithm can also be applied verbatim in order to intersect stemmed terms.
Additionally, note that we can compute some summarization information. More precisely, we
can obtain the local vocabulary of a document d, that is, the set of diﬀerent terms that appear in
d. By executing rank1(S, selectd(L, i)) for successive i values, we obtain all the local vocabulary, in
order, and in time O(log m) per term. This allows, for example, merging the vocabularies of diﬀerent
documents, or binary searching for a particular term in a particular document (yet, the latter is
easier via two rank operations on L: rankd(L, st+1 − 1) − rankd(L, st − 1); then the corresponding
position can be obtained by selectd(L, 1 + rankd(L, st − 1))).

docc(cid:1).

Finally, the data structure provides some basic support for temporal and hierarchical documents,
by restricting the inverted lists Ft to a range of document values [dmin, dmax] (recall Section 5.1). A
simple way to proceed is to ﬁrst carry out a query range next value(L, st, st+1 − 1, dmin) with rnv
(Algorithm 3), which will also give us the rank p of the ﬁrst document ≥ d. Then any subsequent
range quantile query on Ft must increase its argument by p − 1, and discard answers larger than
dmax. On the other hand, functions hdlist and hdint (Algorithm 4) will work without changes,
and support inverted list algorithms on XML retrievable units, just as in Section 5.2.

19

6.2 Ranked Retrieval

We focus now on the operations of interest for ranked retrieval, which are also simulated in O(log m)
time or less.

Direct access and Persin’s algorithm The Lt lists used for ranked retrieval are directly concatenated 
in L, so Lt[i] is obtained by accessing symbol L[st + i − 1] using the wavelet tree. Recall
that the term frequencies tf are available in constant time. A range Lt[i, i′] is obtained in time
O(cid:16)(i′ − i + 1) log m

i′−i+1(cid:17) by using query range report (L, st + i, st + i′, [1, m]) (Algorithm 2).

This algorithm has the problem of retrieving the documents in document order, not in tf order as
they are in Lt. Note, however, that retrieving the highest-tf documents in document order is indeed
beneﬁcial for Persin’s algorithm [57] (recall Section 3.2), where a problem is how to accumulate
results across unordered document sets. More precisely, assume we have the current candidate set
as an array ordered by increasing document identiﬁer. Persin’s algorithm computes a threshold
term frequency f , so that the next list to consider, Lt, should be processed only for tf values that
are at least p. Instead of traversing Lt by decreasing tf values and stopping when these fall below
f , we can compute p = select1(Rt, vt − rank1(Tt, f ) + 1) − 1, so that Lt[1, p] is precisely the preﬁx
where the term frequencies are at least f . Now we extract all the values as explained. As they are
obtained in increasing document identiﬁer order, they are easily merged with the current candidate
set, in order to accumulate frequencies in common documents.

Other operations of interest Any candidate document d in Persin’s algorithm can be directly
evaluated, obtaining its tf d,t values, by ﬁnding d within Lt for each t ∈ q (with rankd and selectd
on L, as explained), and its tf obtained from Rt and Tt, all in O(|q| log m) time.

If we use stemming, we might want to retrieve preﬁxes of several lists Lt to Lt′. We may carry
out the previous algorithm to deliver all the distinct documents in these preﬁxes, now carrying on
the t′ − t + 1 intervals as we descend in the wavelet tree. When we arrive at the relevant leaves
labeled d, the corresponding positions will be contiguous, thus we can naturally return just one
occurrence of each d in the union. If we wish to obtain the sum of the tf values for all the stemmed
terms in d, we can traverse the wavelet tree upwards for each interval element at leaf d, and obtain
its tf upon ﬁnding its position in L. Alternatively, we could store the tf values aligned to the leaves
and mark their cumulative values on a compressed bitmap, so as to obtain the sum in constant time
as the diﬀerence of two select1 operations on that bitmap. The space for tf , however, becomes now
n log N
n + O(n) bits, which is higher than in our current representation. This method also delivers
the results in document order.

Maintaining the tf values aligned to the leaf order yields some support for hierarchical queries.
Assume a retrievable unit (recall Section 5.2) spans the document range [dl, dr], and thus we wish
to compute the total tf of t in range [dl, df ]. Any such range is exactly covered by O(log m) wavelet
tree nodes (Lemma 2). We can descend, projecting the range of Lt in L, until those nodes, and
then add up the accumulated tf values of those O(log m) nodes, in overall time O(log m).

We can also support temporal and hierarchical documents by restricting our accesses in Lt only
to documents within a range [dmin, dmax] (recall Section 5.1). It is suﬃcient to use [dmin, dmax] as
the last argument when we use the range report query that underlies our support for accessing Lt.
This automatically yields, for example, Persin’s algorithm restricted to a range of documents.

20

7 Conclusions

The wavelet tree data structure [34] has had an enormous impact on the implementation of reducedspace 
text databases. In this article we have shown that it has several other under-explored capabilities.
 We have proposed three new algorithms on wavelet trees that solve fundamental problems,
improving upon the state of the art in some aspects. For range intersections we achieve an adaptive
complexity that matches the one achieved for sorted ranges. For range quantile and range next value
problems, we match or get close to the best known time complexities while using less space: basically 
that needed to represent the sequence S[1, n] plus O(n) extra bits, versus the O(n log n) extra
bits required by previous solutions. Furthermore, if we use compressed bitmap representations [60]
in our wavelet trees, we retain the time complexities and achieve zero-order compression in the representation 
of S [34], that is, our overall space including the sequence becomes nH0(S) + O(n + σ),
where [1, σ] is the alphabet of S and H0(S) is its empirical zero-order entropy.

We have also explored a number of applications of those novel algorithms to two areas of
Information Retrieval (IR): document retrieval on general string databases, and inverted indexes.
In both cases we obtained support for a number of powerful operations without further increasing
the space required to support basic ones.

The algorithms are elegant and simple to implement, so they have the potential to be useful
in practice. Future work involves implementing them within an IR framework and evaluating their
practical performance. Although we have used some theoretical data structures for handling bitmaps
within convenient space bounds, practical variants of rank/select-capable plain and compressed
bitmaps, as well as various wavelet tree implementations, are publicly available11. Some preliminary
experiments [25] show that an early version of our results [32] do improve signiﬁcantly in practice
upon the previous state of the art on document retrieval for general strings. Our improved versions
presented in this article should widen the gap. In the case of inverted indexes we do not expect our
representation to be faster for the basic operations, yet it is likely that it requires less space than
that of a full-text plus a ranked-retrieval inverted index, and that it is more eﬃcient on sophisticated
operations.

Aknowledgements. We thank J´er´emy Barbay for his help in understanding the adaptive complexity
measures for intersections, and Meg Gagie for righting our grammar.

References

1. V. Anh and A. Moﬀat. Pruned query evaluation using pre-computed impacts. In Proc. 29th Annual International
ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pages 372–379, 2006.
2. D. Arroyuelo, F. Claude, S. Maneth, V. M¨akinen, G. Navarro, K. Nguy˜ˆen, J. Sir´en, and N. V¨alim¨aki. Fast
in-memory XPath search over compressed text and tree indexes. In Proc. 26th IEEE International Conference
on Data Engineering (ICDE), pages 417–428, 2010.

3. R. Baeza-Yates. A fast set intersection algorithm for sorted sequences. In Proc. 15th Annual Symposium on

Combinatorial Pattern Matching (CPM), LNCS 3109, pages 400–408, 2004.

4. R. Baeza-Yates, A. Moﬀat, and G. Navarro. Searching Large Text Collections, pages 195–244. Kluwer Academic

Publishers, 2002.

5. R. Baeza-Yates and B. Ribeiro. Modern Information Retrieval. Addison-Wesley, 1999.
6. R. Baeza-Yates and A. Salinger. Experimental analysis of a fast intersection algorithm for sorted sequences. In
Proc. 12th International Symposium on String Processing and Information Retrieval (SPIRE), LNCS 3772, pages
13–24, 2005.

11 See for example http://www.recoded.cl.

21

7. J. Barbay, F. Claude, and G. Navarro. Compact rich-functional binary relation representations. In Proc. 9th

Latin American Symposium on Theoretical Informatics (LATIN), LNCS 6034, pages 172–185, 2010.

8. J. Barbay and C. Kenyon. Adaptive intersection and t-threshold problems. In Proc. 13th Annual ACM-SIAM

Symposium on Discrete Algorithms (SODA), pages 390–399, 2002.

9. J. Barbay and C. Kenyon. Alternation and redundancy analysis of the intersection problem. ACM Transactions

on Algorithms, 4(1), 2008.

10. J. Barbay, A. L´opez-Ortiz, and T. Lu. Faster adaptive set intersections for text searching. In Proc. 5th International 
Workshop on Experimental Algorithms (WEA), LNCS 4007, pages 146–157, 2006.

11. J. Barbay, A. L´opez-Ortiz, T. Lu, and A. Salinger. An experimental investigation of set intersection algorithms

for text searching. ACM Journal of Experimental Algorithmics, 14(3):article 7, 2009.

12. J. Barbay and G. Navarro. Compressed representations of permutations, and applications. In Proc. 26th International 
Symposium on Theoretical Aspects of Computer Science (STACS), pages 111–122, 2009.

13. M. Blum, R. W. Floyd, V. R. Pratt, R. L. Rivest, and R. E. Tarjan. Time bounds for selection. Journal of

Computer and System Sciences, 7(4):448–461, 1973.

14. P. Bose, M. He, A. Maheshwari, and P. Morin. Succinct orthogonal range search structures on a grid with applications 
to text indexing. In Proc. 11th International Symposium on Algorithms and Data Structures (WADS),
pages 98–109, 2009.

15. P. Bose, E. Kranakis, P. Morin, and Y. Tang. Approximate range mode and range median queries. In Proc. 22nd

Symposium on Theoretical Aspects of Computer Science (STACS), pages 377–388, 2005.

16. N. Brisaboa, M. Luaces, G. Navarro, and D. Seco. A fun application of compact data structures to indexing
geographic data. In Proc. 5th International Conference on Fun with Algorithms (FUN), LNCS 6099, pages 77–88,
2010.

17. G. S. Brodal, B. Gfeller, A. G. Jørgensen, and P. Sanders. Towards optimal range medians. Theoretical Computer

Science, to appear.

18. G. S. Brodal and A. G. Jørgensen. Data structures for range median queries.

In Proc. 20th International

Symposium on Algorithms and Computation (ISAAC), LNCS 5878, pages 822–831, 2009.

19. B. Chazelle. A functional approach to data structures and its use in multidimensional searching. SIAM Journal

on Computing, 17(3):427–462, 1988.

20. Y.-F. Chien, W.-K. Hon, R. Shah, and J. S. Vitter. Geometric Burrows-Wheeler transform: Linking range

searching and text indexing. In Proc. Data Compression Conference (DCC), pages 252–261, 2008.

21. F. Claude and G. Navarro. Self-indexed text compression using straight-line programs. In Proc. 34th International

Symposium on Mathematical Foundations of Computer Science (MFCS), LNCS 5734, pages 235–246, 2009.

22. M. Crochemore, C. S. Iliopoulos, M. Kubica, M. Rahman, and T. Walen. Improved algorithms for the range next
value problem and applications. In Proc. 25th Symposium on Theoretical Aspects of Computer Science (STACS),
pages 205–216, 2008.

23. M. Crochemore, C. S. Iliopoulos, and M. Rahman. Finding patterns in given intervals. In Proc. 32nd International

Symposium on Mathematical Foundations of Computer Science (MFCS), LNCS 4708, pages 645–656, 2007.

24. J. S. Culpepper and A. Moﬀat. Compact set representation for information retrieval. In Proc. 14th International

Symposium on String Processing and Information Retrieval (SPIRE), LNCS 4726, pages 137–148, 2007.

25. J. S. Culpepper, G. Navarro, S. J. Puglisi, and A. Turpin. Top-k ranked document search in general text
databases. In Proc. 18th Annual European Symposium on Algorithms (ESA), LNCS 6347, pages 194–205 (part
II), 2010.

26. E. Demaine and I. Munro. Adaptive set intersections, unions, and diﬀerences. In Proc. 11th Annual ACM-SIAM

Symposium on Discrete Algorithms (SODA), pages 743–752, 2000.

27. P. Ferragina, G. Manzini, V. M¨akinen, and G. Navarro. An alphabet-friendly FM-index.

In Proc. 11th International 
Symposium on String Processing and Information Retrieval (SPIRE), LNCS 3246, pages 150–160,
2004.

28. P. Ferragina, G. Manzini, V. M¨akinen, and G. Navarro. Compressed representations of sequences and full-text

indexes. ACM Transactions on Algorithms, 3(2):article 20, 2007.

29. J. Fischer and V. Heun. A new succinct representation of RMQ-information and improvements in the enhanced

suﬃx array. In Proc. 1st ESCAPE, LNCS 4614, pages 459–470, 2007.

30. H. Gabow, J. Bentley, and R. Tarjan. Scaling and related techniques for geometry problems. In Proc. 16 ACM

Symposium on Theory of Computing (STOC), pages 135–143, 1984.

31. T. Gagie, G. Navarro, and S. J. Puglisi. Colored range queries and document retrieval. In Proc. 17th International

Symposium on String Processing and Information Retrieval (SPIRE), LNCS 6393, pages 67–81, 2010.

32. T. Gagie, S.J. Puglisi, and A. Turpin. Range quantile queries: another virtue of wavelet trees. In Proc. 16th
International Symposium on String Processing and Information Retrieval (SPIRE), LNCS 5721, pages 1–6, 2009.

22

33. B. Gfeller and P. Sanders. Towards optimal range medians. In Proc. 36th International Colloquium on Automata,

Languages and Programming, (ICALP), LNCS 5555, pages 475–486, 2009.

34. R. Grossi, A. Gupta, and J. S. Vitter. High-order entropy-compressed text indexes. In Proc. 14th Symposium on

Discrete Algorithms (SODA), pages 841–850, 2003.

35. S. Har-Peled and S. Muthukrishnan. Range medians. In Proc. 16th European Symposium on Algorithms (ESA),

LNCS 5193, pages 503–514, 2008.

36. D. Hiemstra and V. Mihajlovi´c. The simplest evaluation measures for XML information retrieval that could

possibly work. In Proc. INEX Workshop on Element Retrieval Methodology, 2005.

37. W.-K. Hon, R. Shah, S. Thankachan, and J. S. Vitter. String retrieval for multi-pattern queries. In Proc. 17th
International Symposium on String Processing and Information Retrieval (SPIRE), LNCS 6393, pages 55–66,
2010.

38. G. Jacobson. Space-eﬃcient static trees and graphs.

In Proc. 30th Symposium on Foundations of Computer

Science (FOCS), pages 549–554, 1989.

39. A. G. Jørgensen and K. D. Larsen. Range selection and median: Tight cell probe lower bounds and adaptive

data structures. In Proc. 22nd Symposium on Discrete Algorithms (SODA), 2011. To appear.

40. O. Keller, T. Kopelowitz, and M. Lewenstein. Range non-overlapping indexing and successive list indexing. In
Proc. 10th International Workshop on Algorithms and Data Structures (WADS), LNCS 4619, pages 625–636,
2007.

41. D. Krizanc, P. Morin, and M. H. M. Smid. Range mode and range median queries on lists and trees. Nordic

Journal of Computing, 12(1):1–17, 2005.

42. M. Lalmas. XML Retrieval, volume 1. Morgan & Claypool Publishers, 2009.
43. V. M¨akinen and G. Navarro. Succinct suﬃx arrays based on run-length encoding. Nordic Journal of Computing,

12(1):40–66, 2005.

44. V. M¨akinen and G. Navarro. Position-restricted substring searching. In Proc. 7th Latin American Symposium

on Theoretical Informatics (LATIN), LNCS 3887, pages 703–714, 2006.

45. V. M¨akinen and G. Navarro. Implicit compression boosting with applications to self-indexing. In Proc. 14th
International Symposium on String Processing and Information Retrieval (SPIRE), LNCS 4726, pages 214–226,
2007.

46. V. M¨akinen, G. Navarro, and E. Ukkonen. Transposition invariant string matching. Journal of Algorithms,

56(2):124–153, 2005.

47. U. Manber and G. Myers. Suﬃx arrays: a new method for on-line string searches. SIAM Journal on Computing,

22(5):935–948, 1993.

48. I. Munro. Tables. In Proc. 16th Conference on Foundations of Software Technology and Theoretical Computer

Science (FSTTCS), LNCS 1180, pages 37–42, 1996.

49. S. Muthukrishnan. Eﬃcient algorithms for document retrieval problems.

In Proc 13th Annual ACM-SIAM

Symposium on Discrete Algorithms (SODA), pages 657–666, 2002.

50. G. Navarro. Indexing text using the ziv-lempel trie. Journal of Discrete Algorithms, 2(1):87–114, 2004.
51. G. Navarro and V. M¨akinen. Compressed full text indexes. ACM Computing Surveys, 39(1):article 2, 2007.
52. G. Navarro, E. Moura, M. Neubert, N. Ziviani, and R. Baeza-Yates. Adding compression to block addressing

inverted indexes. Information Retrieval, 3(1):49–77, 2000.

53. G. Navarro and S. J. Puglisi. Dual-sorted inverted lists.

In Proc. 17th International Symposium on String

Processing and Information Retrieval (SPIRE), LNCS 6393, pages 310–322, 2010.

54. D. Okanohara and K. Sadakane. Practical entropy-compressed rank/select dictionary. In Proc. 9th Workshop on

Algorithm Engineering and Experiments (ALENEX), 2007.

55. M. Pˇatra¸scu. Succincter. In Proc. 49th IEEE Annual Symposium on Foundations of Computer Science (FOCS),

pages 305–313, 2008.

56. J. Pehcevski. Evaluation of Eﬀective XML Information Retrieval. PhD thesis, RMIT University, Australia, 2006.
57. M. Persin, J. Zobel, and R. Sacks-Davis. Filtered document retrieval with frequency-sorted indexes. Journal of

the American Society for Information Sicence, 47(10):749–764, 1996.

58. H. Petersen. Improved bounds for range mode and range median queries. In Proc. 34th Conference on Current

Trends in Theory and Practice of Computer Science (SOFSEM), LNCS 4910, pages 418–423, 2008.

59. H. Petersen and S. Grabowski. Range mode and range median queries in constant time and sub-quadratic space.

Information Processing Letters, 109(4):225–228, 2009.

60. R. Raman, V. Raman, and S. Srinivasa Rao. Succinct indexable dictionaries with applications to encoding kary 
trees and multisets. In Proc. 13th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages
233–242, 2002.

23

61. K. Sadakane. Succinct data structures for ﬂexible text retrieval systems. Journal of Discrete Algorithms, 5(1):12–

22, 2007.

62. K. Sadakane and G. Navarro. Fully-functional succinct trees. In Proc. 21st Annual ACM-SIAM Symposium on

Discrete Algorithms (SODA), pages 134–149, 2010.

63. P. Sanders and F. Transier.

Intersection in integer inverted indices.

In Proc. 9th Workshop on Algorithm

Engineering and Experiments (ALENEX), 2007.

64. S. Stolinski, Sz. Grabowski, and W. Bieniecki. On eﬃcient implementations of median ﬁlters in theory and

practice. Unpublished manuscript, 2010.

65. T. Strohman and B. Croft. Eﬃcient document retrieval in main memory. In Proc. 30th Annual International
ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pages 175–182, 2007.
66. N. V¨alim¨aki and V. M¨akinen. Space-eﬃcient algorithms for document retrieval. In Proc. 18th Annual Symposium

on Combinatorial Pattern Matching (CPM), LNCS 4580, pages 205–215, 2007.

67. I. Witten, A. Moﬀat, and T. Bell. Managing Gigabytes. Morgan Kaufmann Publishers, 2nd edition, 1999.
68. G. Zipf. Human Behaviour and the Principle of Least Eﬀort. Addison-Wesley, 1949.
69. J. Zobel and A. Moﬀat. Inverted ﬁles for text search engines. ACM Computing Surveys, 38(2):art. 6, 2006.

24

