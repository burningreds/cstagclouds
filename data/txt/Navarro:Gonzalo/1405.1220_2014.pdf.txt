4
1
0
2

 

y
a
M
6

 

 
 
]
S
D
.
s
c
[
 
 

1
v
0
2
2
1

.

5
0
4
1
:
v
i
X
r
a

Eﬃcient Compressed Wavelet Trees over Large Alphabets ∗

Francisco Claude
Esc. Inf. & Tel.

Gonzalo Navarro†

Dept. of Computer Science

Alberto Ord´o˜nez‡
Database Laboratory

Univ. Diego Portales, Chile

Univ. of Chile, Chile

Univ. da Coru˜na, Spain

fclaude@recoded.cl

gnavarro@dcc.uchile.cl

alberto.ordonez@udc.es

Abstract

The wavelet tree is a ﬂexible data structure that permits representing sequences S[1, n] of
symbols over an alphabet of size σ, within compressed space and supporting a wide range of
operations on S. When σ is signiﬁcant compared to n, current wavelet tree representations
incur in noticeable space or time overheads. In this article we introduce the wavelet matrix, an
alternative representation for large alphabets that retains all the properties of wavelet trees but
is signiﬁcantly faster. We also show how the wavelet matrix can be compressed up to the zeroorder 
entropy of the sequence without sacriﬁcing, and actually improving, its time performance.
Our experimental results show that the wavelet matrix outperforms all the wavelet tree variants
along the space/time tradeoﬀ map.

1

Introduction

In many applications related to text indexing and succinct data structures, it is necessary to represent 
a sequence S[1, n] over an integer alphabet [0, σ) so as to support the following functionality:

• access(S, i) returns S[i].
• ranka(S, i) returns the number of occurrences of symbol a in S[1, i].
• selecta(S, j) returns the position in S of the j-th occurrence of symbol a.

Some examples where this problem arises are indexes for supporting indexed pattern matching
on strings [35, 36, 28, 29, 51], indexes for solving computational biology problems on sequences
[60, 11], simulation of inverted indexes over natural language text collections [14, 2], representation
of labeled trees and XML structures [12, 3, 27, 1, 8], representation of binary relations and graphs
[7, 21, 5, 8], solving document retrieval problems [63, 31], and many more.

An elegant data structure to solve this problem is the wavelet tree [35]. In its most basic form,
this is a balanced tree of O(σ) nodes storing bitmaps.
It requires n lg σ + o(n lg σ) + O(σ lg n)
bits to represent S and solves the three queries in time O(lg σ). The wavelet tree supports not

∗An early partial versions of this article appeared in Proc. SPIRE 2012 [23].
†Funded in part by Millennium Nucleus Information and Coordination in Networks ICM/FIC P10-024F, Chile.
‡Xunta de Galicia (co-funded with FEDER), ref. 2010/17 and CN 2012/211 (strategic group), and by the Spanish

MICINN ref. AP2010-6038 (FPU Program)

1

only the three queries we have mentioned, but more general range search operations that ﬁnd
applications in representing geometric grids [17, 13, 15, 5, 54] and text indexes based on them
[49, 28, 44, 18, 22, 41, 52], complex queries on numeric sequences [32, 41, 30], and many others.
Various recent surveys [51, 26, 37, 46, 50] are dedicated, partially or totally, to the number of
applications of this versatile data structure.

In various applications, the alphabet size σ is signiﬁcant compared to the length n of the
sequence. Some examples are sequences of words (seen as integer tokens) when simulating inverted
indexes, sequences of XML tags, and sequences of document numbers in document retrieval. When
using wavelet trees to represent grids, the sequence length n becomes the width of the grid and the
alphabet size becomes the height of the grid, and both are equal in most cases.

A large value of σ aﬀects the space usage of wavelet trees. A pointerless wavelet tree [44]
concatenates all the bitmaps levelwise and removes the O(σ lg n) bits from the space. It retains the
time complexity of pointer-based wavelet trees, albeit it is slower in practice. This representation
can be made to use nH0(S)+o(n lg σ) bits, where H0(S) ≤ lg σ is the per-symbol zero-order entropy
of S, by using compressed bitmaps [58, 35]. This makes the wavelet tree traversal even slower in
practice, however.

A pointer-based wavelet tree, instead, can achieve zero-order compression by replacing the
balanced tree by the Huﬀman tree [39]. Then, even without compressing the bitmaps, the storage
space becomes n(H0(S)+1)+o(n(H0(S)+1))+O(σ lg n) bits. Adding bitmap compression removes
the n bits of the Huﬀman redundancy. In addition, this technique is faster than the basic one, as
the average access time is O(H0(S)). However, it still requires the O(σ lg n) extra bits.

Other than wavelet trees, Golynski et al. [33] proposed a sequence representation for large alphabets,
 which uses n lg σ +o(n lg σ) bits (no compression) and oﬀers much faster time complexities
to support the three operations, O(lg lg σ). Later, Barbay et al. [6] built on this idea to obtain
zero-order compression, nH0(S) + o(n(H0(S) + 1)) bits, while retaining the times. This so-called
“alphabet-partitioned” representation does not, however, oﬀer the richer functionality of wavelet
trees. Moreover, as shown in their experiments [4], its sublinear space terms are higher in practice
than those of a zero-order compressed wavelet tree (yet their better complexity does show up in
practice). There are recent theoretical developments slightly improving those complexities [10], but
their sublinear space terms would be even higher in practice.

Our contribution.
In this article we introduce the wavelet matrix. This is an alternative representation 
of the balanced pointerless wavelet tree that reorders the nodes in each level, in a way
that retains all the wavelet tree functionality while the traversals needed to carry out the operations
are simpliﬁed and sped up. The wavelet matrix then retains all the capabilities of wavelet trees, is
resistant to large alphabets, and its speed gets close to that of pointer-based wavelet trees. It can
also obtain zero-order compression by compressing the bitmaps (which slows it down).

We then consider how to give Huﬀman shape to the wavelet trees without the burden of storing
the tree pointers. This is achieved by combining canonical Huﬀman codes [61] with pointerless
wavelet trees (now unbalanced). Finally, we aim at combining both improvements, that is, obtaining
Huﬀman shaped wavelet matrices. These yield simultaneously zero-order compression and fast
operations. It turns out, however, that the canonical Huﬀman codes cannot be directly combined
with the node numbering induced by the wavelet matrix, so we derive an alternative code assignment
scheme that is also optimal and compatible with the wavelet matrix.

We implement all the variants and test them over various real-life sequences, showing that a few

2

Figure 1: On the left, the standard wavelet tree over a sequence. The subsequences Sv are not
stored. The bitmaps Bv, in bold, are stored, as well as the tree topology. On the right, its pointerless
version. The divisions into nodes are not stored but computed on the ﬂy.

versions of the wavelet matrix dominate all the wavelet tree variants across the space/time tradeoﬀ
map, on diverse sequences over large alphabets and point grids.

2 Basic Concepts

2.1 Wavelet Trees

A wavelet tree [35] for sequence S[1, n] over alphabet [0..σ) is a complete balanced binary tree,
where each node handles a range of symbols. The root handles [0..σ) and each leaf handles one
symbol. Each node v handling the range [αv, ωv) represents the subsequence Sv[1, nv] of S formed
by the symbols in [αv, ωv), but it does not explicitly store Sv. Rather, internal nodes v store
a bitmap Bv[1, nv], so that Bv[i] = 0 if Sv[i] < αv + 2(cid:100)lg(ωv−αv)(cid:101)−1 and Bv[i] = 1 otherwise.
That is, we partition the alphabet interval [αv, ωv) into two roughly equal parts: a “left” one,
[αv, αv + 2(cid:100)lg(ωv−αv)(cid:101)−1) and a “right” one, [αv + 2(cid:100)lg(ωv−αv)(cid:101)−1, ωv). These are handled by the left
and right children of v. No bitmaps are stored for the leaves. Figure 1 (left) gives an example.
The tree has height (cid:100)lg σ(cid:101), and it has exactly σ leaves and σ − 1 internal nodes. If we regard it
level by level, we can see that it holds, in the Bv bitmaps, exactly n bits per level (the lowest one
may hold fewer bits). Thus it stores at most n(cid:100)lg σ(cid:101) bits. Storing the tree pointers, and pointers
to the bitmaps, requires O(σ lg n) further bits, if we use the minimum of lg n bits for the pointers.
To access S[i], we start from the root node ν, setting iν = i. If Bν[iν] = 0, this means that
S[i] = Sν[iν] < 2(cid:100)lg σ(cid:101)−1 and that the symbol is represented in the subsequence Sνl of the left child
νl of the root. Otherwise, Sν[iν] ≥ 2(cid:100)lg σ(cid:101)−1 and it is represented in the subsequence Sνr of the right
child νr of the root. In the ﬁrst case, the position of Sν[iν] in Sνl is iνl = rank0(Bν, iν), whereas
in the second, the position in Sνr is iνr = rank1(Bν, iν). We continue recursively, extracting Sv[iv]
from node v = νl or v = νr, until we arrive at a leaf representing the alphabet interval [a, a], where
we can ﬁnally report S[i] = a.
Therefore, the cost of operation access is that of (cid:100)lg σ(cid:101) binary rank operations on bitmaps Bv.
Binary rank and select operations can be carried out in constant time using only o(nv) bits on
top of Bv [40, 48, 19].
The process to compute ranka(S, i) is similar. The diﬀerence is that we do not descend according
to whether Bv[i] equals 0 or 1, but rather according to the bits of a ∈ [0, σ): the highest bit of a
tells us whether to go left or right, and the lower bits are used in the next levels. When moving to a

3

4  7  6  5  3  2  1  0  2  1  4  1  71  1  1  1  0  0  0  0  0  0  1  0  13  2  1  0  2  1  11  1  0  0  1  0  04  7  6  5  4  70  1  1  0  0  11  0  1  1 1  0  1  13  2  21  0  04 5 40 1 07 6 71 0 11 1 102 234 4567 74  7  6  5  3  2  1  0  2  1  4  1  71  1  1  1  0  0  0  0  0  0  1  0  13  2  1  0  2  1  1  4  7  6  5  4  71  1  0  0  1  0  0  0  1  1  0  0  11  0  1  1  3  2  2  4  5  4  7  6  71  0  1  1  1  0  0  0  1  0  1  0  10  0  0  1  2  2  3  4  4  5  6  7  7Algorithm 1 Standard wavelet tree algorithms: On the wavelet tree of sequence S rooted at ν,
acc(ν, i) returns S[i]; rnk(ν, a, i) returns ranka(S, i); and sel(ν, a, j) returns selecta(S, j). The
left/right children of v are called vl/vr.
acc(v, i)

rnk(v, a, i)

if ωv − αv = 1 then

sel(v, a, j)

if ωv − αv = 1 then

if ωv − αv = 1 then

return αv

end if
if Bv[i] = 0 then
i ← rank0(Bv, i)
return acc(vl, i)
i ← rank1(Bv, i)
return acc(vr, i)

else

end if

return i

return j

end if
if a < 2(cid:100)lg(ωv−αv)(cid:101)−1 then

i ← rank0(Bv, i)
return rnk(vl, a, i)
i ← rank1(Bv, i)
return rnk(vr, a, i)

else

end if
if a < 2(cid:100)lg(ωv−αv)(cid:101)−1 then

j ← sel(vl, a, j)
return select0(Bv, j)
j ← sel(vr, a, j)
return select1(Bv, j)

else

end if

end if

child u of v, we compute iu = rank0/1(Bv, iv) to be the number of times the current bit of a appears
in Bv[1, iv]. When we arrive at the leaf u handling the range [a, a], the answer to ranka(S, i) is iu.
Finally, to compute selecta(S, j) we must proceed upwards. We start at the leaf u that handles
the alphabet range [a, a]. So we want to track the position of Su[ju], ju = j, towards the root. If
u is the left child of its parent v, then the corresponding position at the parent is Sv[jv], where
jv = select0(Bv, ju). Else, the corresponding position is jv = select1(Bv, ju). When we ﬁnally
arrive at the root ν, the answer to the query is jν.
Thus the cost of query ranka(S, i) is (cid:100)lg σ(cid:101) binary rank operations (just like access(S, i)),
and the cost of query selecta(S, i) is (cid:100)lg σ(cid:101) binary select operations. Algorithm 1 gives the
pseudocode (the recursive form is cleaner, but recursion can be easily removed).

2.2 Pointerless Wavelet Trees

Since the wavelet tree is a complete balanced binary tree, it is possible to concatenate all the
bitmaps at each level and still retain the same functionality [44]. Instead of a bitmap per node v,
there will be a single bitmap per level (cid:96), ˜B(cid:96)[1, n]. Figure 1 (right) illustrates this arrangement. The
main problem is how to keep track of the range ˜B(cid:96)[sv, ev] corresponding to a node v of depth (cid:96).
The strict variant. The strict variant [44] stores no data apart from the (cid:100)lg σ(cid:101) pointers to the
level bitmaps. Keeping track of the node ranges is not hard if we start at the root (as in access
and rank). Initially, we know that [sν, eν] = [1, n], that is, the whole bitmap ˜B0 is equal to the
bitmap of the root, Bν. Now, imagine that we have navigated towards a node v at depth (cid:96), and
know [sv, ev]. The two children of v share the same interval [sv, ev] at ˜B(cid:96)+1. The split point is
m = rank0( ˜B(cid:96), ev) − rank0( ˜B(cid:96), sv − 1), the number of 0s in ˜B(cid:96)[sv, ev]. Then, if we descend to the
left child vl, we will have [svl, evl] = [sv, sv + m − 1]. If we descend to the right child vr, we will
have [svr , evr ] = [sv + m, ev].

Things are a little bit harder for select, because we must proceed upwards.

In the strict
variant, the way to carry out selecta(S, j) is to ﬁrst descend to the leaf corresponding to symbol
a, and then track the leaf position j up to the root as we return from the recursion.
Algorithm 2 gives the pseudocode (we use p = s − 1 instead of s = sv). Note that, compared

4

Algorithm 2 Pointerless wavelet tree algorithms (strict variant): On the wavelet tree of sequence 
S, acc(0, i, 0, n) returns S[i]; rnk(0, a, i, 0, n) returns ranka(S, i); and sel(0, a, j, 0, n) returns 
selecta(S, j). For simplicity we have omitted the computation of [αv, ωv).

acc((cid:96), i, p, e)

if ωv − αv = 1 then

return αv

rnk((cid:96), a, i, p, e)

if ωv − αv = 1 then

return i

end if
l ← rank0( ˜B(cid:96), p)
r ← rank0( ˜B(cid:96), e)
if ˜B(cid:96)[i] = 0 then

z ← rank0( ˜B(cid:96), p + i)
return acc((cid:96)+1,
z−l, p, p+r−l)
z ← rank1( ˜B(cid:96), p + i)
return acc((cid:96)+1,
z−(p−l), p+r−l, e)

else

end if
l ← rank0( ˜B(cid:96), p)
r ← rank0( ˜B(cid:96), e)
if a < 2(cid:100)lg(ωv−αv)(cid:101)−1 then

z ← rank0( ˜B(cid:96), p + i)
return rnk((cid:96)+1, a,
z−l, p, p+r−l)
z ← rank1( ˜B(cid:96), p + i)
return rnk((cid:96)+1, a,
z−(p−l), p+r−l, e)

else

sel((cid:96), a, j, p, e)

if ωv − αv = 1 then

return j

end if
l ← rank0( ˜B(cid:96), p)
r ← rank0( ˜B(cid:96), e)
if a < 2(cid:100)lg(ωv−αv)(cid:101)−1 then

j←sel((cid:96)+1, a, j, p, p+r−l)
return select0( ˜B(cid:96), l + j)−p
j←sel((cid:96)+1, a, j, p+r−l, e)
return select1( ˜B(cid:96), (p−l)+j)−p

else

end if

end if

end if

to the standard version, the strict variant requires two extra binary rank operations per original
binary rank, on the top-down traversals (i.e., for queries access and rank). For query select,
the strict variant requires two extra binary rank operations per original binary select. Thus the
times may up to triple for these traversals.1

The extended variant. The extended variant [20], instead, stores an array C[0, σ−1] of pointers
to the σ starting positions of the symbols in the (virtual) array of the leaves, or said another way,
C[a] is the number of occurrences of symbols smaller than a in S. Note this array requires O(σ lg n)
bits (or at best O(σ lg(n/σ)) + o(n) if represented as a compressed bitmap [58]), but the constant
is much lower than on a pointer-based tree (which stores the left child, the right child, the parent,
the value nv, the pointer to bitmap Bv, pointers to the leaves, etc.).

With the help of array C, the number of operations becomes closer to the standard version,
since C lets us compute the ranges: The range of any node v is simply [sv, ev] = [C[αv] + 1, C[ωv]].
In the algorithms for queries access and rank, where we descend from the root, the values αv and
ωv are easily maintained. Thus we do not need to compute r in Algorithm 2, as it is used only to
compute e = ev = C[ωv]. Thus we require only one extra binary rank operation per level.

This is slightly more complicated when solving query selecta(S, j). We start at oﬀset j in the
interval [C[αu] + 1, C[ωu]] for (αu, ωu) = (a, a + 1) and track this position upwards: If the leaf u is
a left child of its parent v (i.e., if αu is even), then the parent’s range (in the deepest bitmap ˜B(cid:96)) is
(αv, ωv) = (αu, ωu + 1). Instead, if the leaf is a right child of its parent, then the parent’s range is
(αv, ωv) = (αu − 1, ωu). We use binary select on the range [C[αv] + 1, C[ωv]] to map the position
j to the parent’s range. Now we proceed similarly at the parent w of v. If αv = 0 mod 4, then v is

1In practice the eﬀect is not so large because of cache eﬀects when sv is close to ev. In addition, binary select is

more expensive than rank in practice, thus the impact on query select is lower.

5

Algorithm 3 Pointerless wavelet tree algorithms (extended variant): On the wavelet tree of sequence 
S, acc(0, i) returns S[i]; rnk(0, a, i) returns ranka(S, i); and sel(a, j) returns selecta(S, j).
For simplicity we have omitted the computation of [αv, ωv), except on sel(a, j), where for simplicity
we assume C[a] refers to level (cid:96) = (cid:100)lg σ(cid:101), where in fact it could refer to level (cid:96) = (cid:100)lg σ(cid:101) − 1.
acc((cid:96), i)

rnk((cid:96), a, i)

if ωv − αv = 1 then

return αv

end if
l ← rank0( ˜B(cid:96), C[αv])
z ← rank0( ˜B(cid:96), C[αv]+i)
if ˜B(cid:96)[i] = 0 then
return acc((cid:96)+1, z−l)
return acc((cid:96)+1, i−(z−l))

else

if ωv − αv = 1 then

return i

end if
l ← rank0( ˜B(cid:96), C[αv])
z ← rank0( ˜B(cid:96), C[αv] + i)
if a < 2(cid:100)lg(ωv−αv)(cid:101)−1 then
return rnk((cid:96)+1, a, z−l)
return rnk((cid:96)+1, a, i−(z−l))

else

end if

end if

sel(a, j)

(cid:96) ← (cid:100)lg σ(cid:101), d ← 1
while (cid:96) ≥ 0 do

if a mod 2d = 0 then
l ← rank0( ˜B(cid:96), C[αv])
j ← select0( ˜B(cid:96), l+j)
αv ← αv − 2d−1
l ← rank1( ˜B(cid:96), C[αv])
j ← select1( ˜B(cid:96), l+j)

else

end if
j ← j − C[αv]
(cid:96) ← (cid:96) − 1, d ← d + 1

end while
return j

the left child of w, otherwise it is the right child. In the ﬁrst case, the range of w in bitmap ˜B(cid:96)−1 is
(αw, ωw) = (αv, ωv + 2), otherwise it is (αw, ωw) = (αv − 2, ωv). We continue until the root, where
j is the answer. In this case we need only one extra binary rank operation per level. Algorithm 3
details the algorithms.

2.3 Huﬀman Shaped Wavelet Trees

Given the frequencies of the σ symbols in S[1, n], the Huﬀman algorithm [39] produces an optimal
variable-length encoding so that (1) it is preﬁx-free, that is, no code is a preﬁx of another; (2) the
size of the compressed sequence is minimized. If symbol a ∈ [0, σ) appears na times in S, then
a na(cid:96)a is
minimized. Then the ﬁle is compressed to L bits by replacing each symbol S[i] = a by its code
≤ lg σ, and no
statistical compressor based on individual symbol probabilities can output less than nH0(S) bits.
a na(cid:96)a < n(H0(S) + 1) bits, which

the Huﬀman algorithm will assign it a codeword of length (cid:96)a so that the sum L = (cid:80)
of length (cid:96)a. The empirical zero-order entropy [25] of S is H0(S) = (cid:80)
The output size of Huﬀman compression can be bounded by(cid:80)

na
n lg n
na

a

is oﬀ the optimum by less than 1 bit per symbol.

Huﬀman [39] showed how to build a so-called Huﬀman tree to obtain these codes. The tree
leaves will contain the symbols, whose codes are obtained by following the tree path from the root
to their leaves. Each branch of the tree is labeled with 0 (say, the left child) or 1 (say, the right
child), and the code associated with a symbol a is obtained by concatenating the labels found in
the path from the tree root to the leaf that contains symbol a.
Building a balanced wavelet tree is equivalent to using a ﬁxed-length encoding of (cid:100)lg σ(cid:101) or
(cid:98)lg σ(cid:99) bits per symbol. Instead, by giving the wavelet tree the shape of the Huﬀman tree, the total
number of bits stored is exactly the output size of the Huﬀman compressor [35, 50]: The leaf of

6

Figure 2: On the left, the same wavelet tree of Figure 1. On the right, its Huﬀman shaped version.

a is at depth (cid:96)a, and each of the na occurrences induces one bit in the bitmap of each of the (cid:96)a
ancestors of the leaf. The size of this tree, plus rank/select overheads, is thus upper bounded by
n(H0(S) + 1) + o(n(H0(S) + 1)) + O(σ lg n) bits. Figure 2 depicts a Huﬀman shaped wavelet tree.
The wavelet tree operations are performed verbatim on Huﬀman shaped wavelet trees. Moreover,
 they become faster on average: If i ∈ [1, n] is chosen at random for access(S, i), or a is chosen
with probability na/n in operations ranka(S, i) and selecta(S, j) (which is the typical case in most
applications), then the average time is O(H0(S) + 1). By rebalancing deep leaves, the space and
average time are maintained and the worst-case time of the operations is limited to O(lg σ) [9].

Zero-order compression can also be achieved on the balanced wavelet tree, by using a compressed
representation of the bitmaps [58]. The time remains the same and the space decreases to nH0(S)+
o(n lg σ) bits [35]. Combining the compressed bitmap representation with Huﬀman shape, we obtain
nH0(S) + o(n(H0(S) + 1)) + O(σ lg n) bits. This combination works well in practice [20], although
the compressed bitmap representation is in practice slower than the plain one.

2.4 Wavelet Trees on Point Grids

As mentioned in the Introduction, wavelet trees are not only useful to support access, rank and
select operations on sequences. They are also frequently used to represent point grids [17, 50],
where they can for example count or list the points that lie in a rectangular area. Typically the
grid is square, of n × n cells, and contains n points, exactly one per point and per column (other
arrangements are routinely mapped to this simpliﬁed case). Then it can be regarded as a sequence
S[1, n] over a large alphabet of size σ = n. In this case, pointer-based wavelet trees perform poorly,
as the space for the pointers is dominant. Similarly, zero-order compression is ineﬀective. The
balanced wavelet trees without pointers [44] are the most successful representation.

The pseudocode for range searches using standard wavelet trees is easily available, see for
example Gagie et al. [31]. Algorithm 4 shows the algorithms adapted to pointerless wavelet trees.
We consider the two basic operations count(P, x1, x2, y1, y2) and report(P, x1, x2, y1, y2), which
count and list, respectively, the points within the rectangle [x1, x2] × [y1, y2] from the point set
represented in sequence P [1, n]. The time complexities can be shown to be O(lg n) for count
and O(k lg(n/k)) for a report operation that lists k points. In practical terms, compared to the
standard versions, the pointerless algorithms requires twice the number of rank operations.

7

4  7  6  5  3  2  1  0  2  1  4  1  71  1  1  1  0  0  0  0  0  0  1  0  13  2  1  0  2  1  11  1  0  0  1  0  04  7  6  5  4  70  1  1  0  0  11  0  1  1 1  0  1  13  2  21  0  04 5 40 1 07 6 71 0 11 1 102 234 4567 74  7  6  5  3  2  1  0  2  1  4  1  71  0  1  1  1  1  0  1  1  0  1  0  0 7  1  1  1  70  1  1  1  04  6  5  3  2  0  2  4 1  0  0  0  1  0  1  17 71 1 16  5  3  01  1  0  04  2  2  41  0  0  13  01  06  50  12  24  40365Algorithm 4 Range search algorithms on pointerless wavelet trees: count(0, x1, x2, y1, y2, 0, n)
returns count(P, x1, x2, y1, y2) on the wavelet tree of sequence P ; and report(0, x1, x2, y1, y2, 0, n)
outputs all those y, where a point with coordinate y1 ≤ y ≤ y2 appears in P [x1, x2]. For simplicity
we have omitted the computation of [αv, ωv).
count((cid:96), x1, x2, y1, y2, p, e)

report(v, x1, x2, y1, y2, p, e)

if x1 > x2 ∨ [αv, ωv] ∩ [y1, y2] = ∅ then
else if [αv, ωv] ⊆ [y1, y2] then

else

return 0
return x2 − x1 + 1
l ← rank0( ˜B(cid:96), p)
r ← rank0( ˜B(cid:96), e)
1 ← rank0( ˜B(cid:96), x1 − 1) − l + 1
xl
2 ← rank0( ˜B(cid:96), x2) − l
xl
2 ← x2 − xl
1 ← x1 − xl
xr
1 + 1, xr
1, xl
return count((cid:96)+1, xl
+count((cid:96)+1, xr
1, xr

2

2, y1, y2, p, p+r−l)
2, y1, y2, p+r−l, e)

if x1 > x2 ∨ [αv, ωv] ∩ [y1, y2] = ∅ then
else if ωv − αv = 1 then

return

else

output αv
l ← rank0( ˜B(cid:96), p)
r ← rank0( ˜B(cid:96), e)
1 ← rank0( ˜B(cid:96), x1 − 1) − l + 1
xl
2 ← rank0( ˜B(cid:96), x2) − l
xl
2 ← x2 − xl
1 ← x1 − xl
xr
1 + 1, xr
report((cid:96)+1, xl
report((cid:96)+1, xr

2, y1, y2, p, p+r−l)
2, y1, y2, p+r−l, e)

1, xl
1, xr

2

end if

end if

3 Pointerless Huﬀman Shaped Wavelet Trees

In this section we show how to use canonical Huﬀman codes [61] to represent Huﬀman shaped
wavelet trees without pointers, this way removing the main component of the O(σ lg n) extra bits
and retaining the advantages of reduced space and O(H0(S) + 1) average traversal time.

The problem that arises when storing a standard Huﬀman shaped wavelet tree in levelwise form
is that a leaf that appears in the middle of a level leaves a “hole” that ruins the calculations done at
the nodes to the right of it to ﬁnd their position in the next level. Canonical Huﬀman codes choose
one of the many optimal Huﬀman trees that, among other interesting beneﬁts [61, 59], yields a set
of codes in which longer codes appear to the left of shorter codes.2 As a consequence, all the leaves
of a level appear grouped to the right, and therefore do not alter the navigation calculations for
the other nodes. The levelwise deployment of the tree can be seen as a sequence of “contiguous”
bitmaps of varying length.

The navigation procedures of Algorithm 2 can then be used verbatim, except for a few alphabet
mappings that must be carried out: For access(S, i), we need to maintain the Huﬀman tree so that,
given the 0/1 labels of the traversed path, we determine the alphabet symbol corresponding to that
leaf of the Huﬀman tree. For ranka(S, i), we need to convert the symbol a to its variable-length
code, in order to follow the corresponding path in the wavelet tree. Finally, for selecta(S, i), we
need the same as for rank for the strict variant, or a pointer to the corresponding leaf area in some
bitmap ˜B(cid:96), for the extended variant. The mappings are also used to determine when to stop a
top-down traversal. The mapping information amounts to O(σ lg n) bits as well, but it is much
less in practice than what is stored for pointer-based wavelet trees, as explained. Moreover, in the
case of canonical codes, σ lg σ + O(σ) bits are suﬃcient to represent the mappings. It has also been

2It is usually to the right, but this way is more convenient for us.

8

Figure 3: On the left, the pointer-based canonical Huﬀman code for our running example. On the
right, its levelwise representation. Note that from now on we interpret the bit 0 as going right and
the bit 1 as going left.

shown that they can be represented within O(σ lg lg n) bits as well [55].

The maximum number of levels in a Huﬀman tree is O(lg n), and as explained it can be made
O(lg σ) without aﬀecting the asymptotic performance. Thus the pointers to the levels add up to
a negligible O(lg2 n) bits. The rest of the space is as for standard Huﬀman shaped wavelet trees:
n(H0(S) + 1) + o(n(H0(S) + 1)) bits. Moreover, by using compressed bitmaps [58], the space is
reduced to nH0(S) + o(n(H0(S) + 1)) bits, albeit in practice the navigation is slowed down.

The algorithm to compute a canonical Huﬀman code [61] starts from the code length assignments
(cid:96)a produced by the standard Huﬀman algorithm, and produces a particular Huﬀman tree with the
same code lengths. First, it computes (cid:96)min and (cid:96)max, the minimum and maximum code lengths, and
array nCodes[(cid:96)min, (cid:96)max], where nCodes[(cid:96)] is the number of codes of length (cid:96). Then, the algorithm
assigns the codes as follows:

1. f st[(cid:96)min] = 0(cid:96)min (i.e., (cid:96)min 0s) is the ﬁrst code of length (cid:96)min.

2. All the codes of a given length (cid:96) are consecutive numbers, from f st[(cid:96)] to last[(cid:96)] = f st[(cid:96)] +

nCodes[(cid:96)] − 1.

3. The ﬁrst code of the next length (cid:96)(cid:48) > (cid:96) that has nCodes[(cid:96)(cid:48)] > 0 is f st[(cid:96)(cid:48)] = 2(cid:96)(cid:48)−(cid:96)(last[(cid:96)] + 1).

Note that rule 2 ensures that all codes of a given level are consecutive numbers and the ﬁrst of
their length, whereas rule 3 guarantees that the set of produced codes is preﬁx-free. By interpreting
the bit 0 as the right child and the bit 1 as the left child, we have that all the leaves at any level are
the rightmost nodes. Figure 3 illustrates the standard and the levelwise deployment of a canonical
Huﬀman code.

4 The Wavelet Matrix

The idea of the wavelet matrix is to break the assumption that the children of a node v, at interval
˜B(cid:96)[sv, ev], must be aligned to it and occupy the interval ˜B(cid:96)+1[sv, ev]. Freeing the structure from this
assumption allows us to design a much simpler mapping mechanism from one level to the next: all
the zeros of the level go left, and all the ones go right. For each level, we will store a single integer

9

4  7  6  5  3  2  1  0  2  1  4  1  71  0  1  1  1  1  0  1  1  0  1  0  0 7  1  1  1  70  1  1  1  04  6  5  3  2  0  2  4 0  1  1  1  0  1  0  07 71 1 16  5  3  01  1  0  03 01 06 50 103654  2  2  41  0  0  12 24 44  7  6  5  3  2  1  0  2  1  4  1  71  0  1  1  1  1  0  1  1  0  1  0  0 4  6  5  3  2  0  2  4  7  1  1  1  70  1  1  1  0  1  0  0  0  1  1  1  06  5  3  0  4  2  2  4  1  1  1  7  7 1  1  0  0  1  0  0  16  5  3  0  4  4  2  20  1  1  05  6  3  0Figure 4: On the left, the pointerless wavelet tree of Figure 1. On the right, the wavelet matrix
over the same sequence. One vertical line per level represents the position stored in the z(cid:96) values.

z(cid:96) that tells the number of 0s in level (cid:96). This requires just O(lg n lg σ) bits, which is insigniﬁcant,
and allows us to implement the pointerless mechanisms in a simpler and faster way.

More precisely, if ˜B(cid:96)[i] = 0, then the corresponding position at level (cid:96) + 1 will be rank0( ˜B(cid:96), i).
If ˜B(cid:96)[i] = 1, the position at level (cid:96) + 1 will be z(cid:96) + rank1( ˜B(cid:96), i). Note that we can map the position
without knowledge of the boundaries of the node the position belongs. Still, every node v at level
(cid:96) occupies a contiguous range in ˜B(cid:96), as proved next.
Lemma 1. All the bits in any bitmap ˜B(cid:48)
tree node v are also contiguous in the bitmap ˜B(cid:96) of the the wavelet matrix.
Proof. This is obviously true for the root v = ν, as it corresponds to the whole ˜B(cid:48)
0 = ˜B0. Now,
assuming it is true for a node v, with interval ˜B(cid:96)[sv, ev], all the positions with ˜B(cid:96)[i] = 0 for
sv ≤ i ≤ ev will be mapped to consecutive positions ˜B(cid:96)+1[rank0( ˜B(cid:96), i)], and similarly with positions
˜B(cid:96)[i] = 1.

(cid:96) of the pointerless wavelet tree that correspond to a wavelet

Figure 4 illustrates the wavelet matrix, where it can be seen that the blocks of the wavelet tree
are maintained, albeit in diﬀerent order. We now describe how to carry out the operations under
the strict and the extended variants.

The strict variant. To carry out access(S, i), we ﬁrst set i0 to i. Then, if ˜B0[i0] = 0, we set i1
to rank0( ˜B0, i0). Else we set i1 to z0 + rank1( ˜B0, i0). Now we descend to level 1, and continue until
reaching a leaf. The sequence of bits ˜B(cid:96)[i(cid:96)] read along the way form the value S[i] (or, said another
way, we maintain the interval [αv, ωv) and upon reaching the leaf it holds S[i] = αv). Note that we
have carried out only one binary rank operation per level, just as the standard wavelet tree.

Consider now the computation of ranka(S, i). This time we need to keep track of the position
i, and also of the position preceding the range, initially p0 = 0. At each node v of depth (cid:96), if
a < 2(cid:100)lg(ωv−αv)(cid:101)−1, then we go “left” by mapping p(cid:96)+1 to rank0( ˜B(cid:96), p(cid:96)) and i(cid:96)+1 to rank0( ˜B(cid:96), i(cid:96)).
Otherwise, we go “right” by mapping p(cid:96)+1 to z(cid:96) + rank1( ˜B(cid:96), p(cid:96)) and i(cid:96)+1 to z(cid:96) + rank1( ˜B(cid:96), i(cid:96)).
When we arrive at the leaf level, the answer is i(cid:96) − p(cid:96). Note that we have needed one extra binary
rank operation per original rank operation of the standard wavelet tree, instead of the two extra
operations required by the (strict) pointerless variant.

Finally, consider operation selecta(S, j). We ﬁrst descend towards the leaf of a just as done
for ranka(S, i), keeping track only of p(cid:96). When we arrive at the last level, p(cid:96) precedes the range
corresponding to the leaf of a, and thus we wish to track upwards position j(cid:96) = p(cid:96) + j. The upward

10

4  7  6  5  3  2  1  0  2  1  4  1  71  1  1  1  0  0  0  0  0  0  1  0  13  2  1  0  2  1  1  4  7  6  5  4  71  1  0  0  1  0  0  0  1  1  0  0  11  0  1  1  3  2  2  4  5  4  7  6  71  0  1  1  1  0  0  0  1  0  1  0  10  0  0  1  2  2  3  4  4  5  6  7  74  7  6  5  3  2  1  0  2  1  4  1  71  1  1  1  0  0  0  0  0  0  1  0  13  2  1  0  2  1  1  4  7  6  5  4  71  1  0  0  1  0  0  0  1  1  0  0  11  0  1  1  4  5  4  3  2  2  7  6  71  0  1  1  0  1  0  1  0  0  1  0  1 0  4  4  2  2  6  1  1  1  5  3  7  7Algorithm 5 Wavelet matrix algorithms (strict variant): On the wavelet matrix of sequence S,
acc(0, i) returns S[i]; rnk(0, a, i, 0) returns ranka(S, i); and sel(0, a, j, 0) returns selecta(S, j).
For simplicity we have omitted the computation of [αv, ωv).
acc((cid:96), i)

rnk((cid:96), a, i, p)

sel((cid:96), a, j, p)

if ωv − αv = 1 then

return i − p

if ωv − αv = 1 then

return p + j

if ωv − αv = 1 then

return αv

end if
if ˜B(cid:96)[i] = 0 then
i ← rank0( ˜B(cid:96), i)
i ← rank1( ˜B(cid:96), i)

else

end if
return acc((cid:96)+1, i)

end if
if a < 2(cid:100)lg(ωv−αv)(cid:101)−1 then

p ← rank0( ˜B(cid:96), p)
i ← rank0( ˜B(cid:96), i)
p ← z(cid:96) + rank1( ˜B(cid:96), p)
i ← z(cid:96) + rank1( ˜B(cid:96), i)

else

end if
return rnk((cid:96)+1, a, i, p)

end if
if a < 2(cid:100)lg(ωv−αv)(cid:101)−1 then

p ← rank0( ˜B(cid:96), p)
j ← sel((cid:96)+1, a, j, p)
return select0( ˜B(cid:96), j)
p ← z(cid:96) + rank1( ˜B(cid:96), p)
j ← sel((cid:96)+1, a, j, p)
return select1( ˜B(cid:96), j−z(cid:96))

else

end if

tracking of a position ˜B(cid:96)[j(cid:96)] is simple: If we went left from level (cid:96)−1, then this position was mapped
from a 0 in ˜B(cid:96)−1, and therefore it came from j(cid:96)−1 = ˜B(cid:96)−1[select0( ˜B(cid:96), j(cid:96))]. Otherwise, position j(cid:96)
was mapped from a 1, and thus it came from j(cid:96)−1 = ˜B(cid:96)−1[select1( ˜B(cid:96), j(cid:96) − z(cid:96))]. When we arrive
at the root bitmap, j0 is the answer. Note that we have needed one extra binary rank per original
binary select required by the standard wavelet tree. We remind that in practice rank is much
less demanding, so this overhead is low. Algorithm 5 gives the pseudocode.

The extended variant. We can speed up rank and select operations if the array C that points
to the starting positions of each symbol in the last level bitmap is available. First, we note that
for ranka(S, i) we do not need anymore to keep track of p(cid:96), since all we need at the end is to
return i(cid:96) − C[a]. Thus the cost becomes similar to that of the standard wavelet tree, which was not
achieved with the extended variant of the pointerless wavelet tree.

For selecta(S, j) we can avoid the ﬁrst downward traversal, as in the pointerless wavelet tree,
and use the same technique to determine whether we came from the left or from the right in the
parent bitmap. Once again, the cost becomes the same as in a standard wavelet tree, with no extra
rank operations required. Algorithm 6 gives the detailed algorithm.
Range searches. Range searches for rectangles [x1, x2] × [y1, y2] require essentially that we are
able to track the points x1 and x2 downwards in the tree. Thus the same wavelet matrix mechanism
for rank can be used. Since we are only interested in the value x2−x1 at the traversed nodes, we do
not need to keep track of p, even in the strict variant (the extended variant requires too much space
in this scenario). As a result, we need the same number of rank operations as in a pointer-based
representation, and get rid of the two extra rank operations required by the pointerless wavelet
tree. Algorithm 7 gives the pseudocode.

Construction. Construction of the wavelet matrix is even simpler than that of the pointerless
wavelet tree, because we do not need to care about node boundaries. At the ﬁrst level we keep in
bitmap ˜B0 the highest bits of the symbols in S, and then stably sort S by those highest bits. Now

11

Algorithm 6 Wavelet matrix algorithms (extended variant): On the wavelet matrix of sequence
S, acc(0, i) returns S[i]; rnk(0, a, i) returns ranka(S, i); and sel(a, j) returns selecta(S, j). For
simplicity we have omitted the computation of [αv, ωv) For simplicity we have omitted the computation 
of [αv, ωv), and in sel(a, j) we assume C[a] refers to level (cid:96) = (cid:100)lg σ(cid:101), where in fact it could
refer to level (cid:96) = (cid:100)lg σ(cid:101) − 1.
acc((cid:96), i)

rnk((cid:96), a, i)

sel(a, j)

(cid:96) ← (cid:100)lg σ(cid:101), d ← 1
j ← C[a] + j
while (cid:96) ≥ 0 do

if a mod 2d = 0 then
j ← select0( ˜B(cid:96), j)
j ← select1( ˜B(cid:96), j−z(cid:96))

else

end if
(cid:96) ← (cid:96) − 1, d ← d + 1

end while
return j

if ωv − αv = 1 then

return αv

end if
if ˜B(cid:96)[i] = 0 then
i ← rank0( ˜B(cid:96), i)
i ← rank1( ˜B(cid:96), i)

else

end if
return acc((cid:96)+1, i)

if ωv − αv = 1 then
return i − C[a]

end if
if a < 2(cid:100)lg(ωv−αv)(cid:101)−1 then

i ← rank0( ˜B(cid:96), i)
i ← z(cid:96) + rank1( ˜B(cid:96), i)

else

end if
return rnk((cid:96)+1, a, i)

we keep in bitmap ˜B1 the next-to-highest bits, and stably sort S by those next-to-highest bits. We
continue until considering the lowest bit. This takes O(n lg σ) time.

Indeed, we can build the wavelet matrix almost in place, by removing the highest bits after
using them and packing the symbols of S. This frees n bits, where we can store the bitmap ˜B0 we
have just generated, and keep doing the same for the next levels. We generate the o(n lg σ)-space
indexes at the end. Thus the construction space is n(cid:100)lg σ(cid:101) + max(n, o(n lg σ)) bits. Other more
sophisticated techniques [24, 62] may use even less space.

5 The Compressed Wavelet Matrix

Just as on the pointerless wavelet tree, we can achieve zero-order entropy with the wavelet matrix
by replacing the plain representations of bitmaps ˜B(cid:96) by compressed ones [58], the space becoming
nH0(S) + o(n lg σ) bits. Compared to obtaining zero-order entropy using Huﬀman shaped trees,
this solution has several disadvantages, as explained: (1) the compressed bitmaps are slower to
operate than in a plain representation; (2) the number of operations on a Huﬀman shaped tree is
lower on average than on a balanced tree; (3) the Huﬀman shaped wavelet tree is more compact, as
it reduces the redundancy from o(n lg σ) to o(n(H0(S) + 1)) (albeit a small O(σ lg n)-bit space term
is added to hold the Huﬀman model); (4) the bitmap compression can be additionally combined
with the Huﬀman shape, obtaining further compression (yet higher time).

The idea is the same as in Section 3: Arrange the codes so that all the leaves are grouped to
the right of the bitmaps ˜B(cid:96). However, because of the reordering of nodes produced by the wavelet
matrix, the use of canonical Huﬀman codes does not guarantee that the leaves of the same level are
contiguous. In the wavelet matrix, the position of a code c in ˜B(cid:96)+1 depends only on the position of c
in ˜B(cid:96) and on the bit of c in that level, c[(cid:96)]. Figure 5 illustrates an example of a canonical set of codes
where the ﬁrst 16 shortest codewords take values from 00000 to 01111 and the remaining 32 from
100000 to 111110. The ﬁgure shows the relative positions of the codes at successive levels of the

12

Algorithm 7 Range search algorithms on the wavelet matrix: count(0, x1, x2, y1, y2) returns
count(P, x1, x2, y1, y2) on the wavelet tree of sequence P ; and report(0, x1, x2, y1, y2) outputs all
those y, where a point with coordinate y1 ≤ y ≤ y2 appears in P [x1, x2]. For simplicity we have
omitted the computation of [αv, ωv).

count((cid:96), x1, x2, y1, y2)

if x1 > x2 ∨ [αv, ωv] ∩ [y1, y2] = ∅ then
else if [αv, ωv] ⊆ [y1, y2] then

return 0
return x2 − x1 + 1
else
1 ← rank0( ˜B(cid:96), x1 − 1) + 1
xl
2 ← rank0( ˜B(cid:96), x2)
xl
1 ← x1 − xl
xr
1 + 1, xr
return count((cid:96)+1, xl
+count((cid:96)+1, xr

2 ← x2 − xl
2
1, xl
2, y1, y2)
1, xr
2, y1, y2)

end if

report(v, x1, x2, y1, y2)

if x1 > x2 ∨ [αv, ωv] ∩ [y1, y2] = ∅ then
else if ωv − αv = 1 then

return

output αv
else
1 ← rank0( ˜B(cid:96), x1 − 1) + 1
xl
2 ← rank0( ˜B(cid:96), x2)
xl
1 ← x1 − xl
xr
1 + 1, xr
report((cid:96)+1, xl
report((cid:96)+1, xr

2, y1, y2)
2, y1, y2)

1, xl
1, xr

2 ← x2 − xl

2

end if

(cid:96) = 1

. . . , c8, c12, c32, c48 . . .

(cid:96) = 2

(cid:96) = 3

. . . , c8, c12, . . .| . . . , c32, c48 . . .
. . . , c32 . . .| . . . , c8, c12, . . . , c48 . . .
. . . , c32, . . . , c8, . . . , c48| . . . , c12, . . .
(cid:96) = 4
(cid:96) = 5 . . . , c16, . . . , c8, . . . , c48, . . . , c12 . . .| . . .
(cid:96) = 6

. . . , c32, c48, . . .| . . .

Figure 5: Example of a sequence of canonical codes along wavelet matrix levels, showing that the
leaves do not span a contiguous area. The the vertical bar “|” marks the points z(cid:96).

wavelet matrix for a sequence . . . c8, c12, c32, c48 . . ., where c8 = 01000, c12 = 01100, c32 = 100000,
and c48 = 110000. As we can see, codes c8 and c12 ﬁnish at level 5 but they are not contiguous
since there is a c48 between them.

We require a distinct mechanism to design an optimal preﬁx-free code that guarantees that,
under the shuﬄing rules of the wavelet matrix, all the leaves at any level form a contiguous area
to the right of the bitmap.

We start by studying how the wavelet matrix sorts the codes at each level. Consider a pair of
codes c1[1, (cid:96)1] and c2[1, (cid:96)2]. Depending on their bits at a given level (cid:96) of the wavelet matrix, two
cases are possible: (a) c1[(cid:96)] = c2[(cid:96)] and then the relative positions of c1 and c2 stay the same at
level (cid:96) + 1, or (b) c1[(cid:96)] (cid:54)= c2[(cid:96)] and then their relative positions in level (cid:96) + 1 depend on the relation
between c1[(cid:96)] and c2[(cid:96)]. This yields the following lemma:

Lemma 2. In a wavelet matrix, given any pair of codes c1 and c2, c1 appears before(after) c2 in ˜B(cid:96)
if, for some 0 ≤ i < (cid:96), it holds c1[(cid:96)− i, (cid:96)− 1] = c2[(cid:96)− i, (cid:96)− 1] and c1[(cid:96)− i− 1] = 0(1) (cid:54)= c2[(cid:96)− i− 1].

13

Proof. If c1[(cid:96) − i, (cid:96) − 1] = c2[(cid:96) − i, (cid:96) − 1], then c1 and c2 transitively keep their relative positions
from level (cid:96) − i to level (cid:96). Instead, c1[(cid:96) − i − 1] (cid:54)= c2[(cid:96) − i − 1] makes their ordering in level (cid:96) − i
dependent only on how c1[(cid:96) − i − 1] and c2[(cid:96) − i − 1] compare to each other.

As a second step, assume we want to design a set of ﬁxed-length codes {ca, a ∈ [0, σ)} such
that ca < cb iﬀ the area of ca is before that of cb in ˜B(cid:100)lg σ(cid:101). That is, we want the codes to be listed
in order in the last level. Let inv : {0, 1}N+ × N+ → {0, 1}N+ be deﬁned as inv(c[1, (cid:96)], (cid:96)) = c−1[1, (cid:96)],
where c−1[i] = c[(cid:96) − i + 1] for all 1 ≤ i ≤ (cid:96). That is, inv(c, (cid:96)) takes number c as a codeword of (cid:96)
bits and returns the code obtained by reading c backwards. Then, the following lemma holds:
Lemma 3. Given any two values i, j ∈ [0, σ) where i < j, code inv(i,(cid:100)lg σ(cid:101)) is located to the left
of code inv(j,(cid:100)lg σ(cid:101)) in the bitmap ˜B(cid:100)lg σ(cid:101) of a wavelet matrix that uses such codes.
Proof. Let τi = inv(i,(cid:100)lg σ(cid:101)) and τj = inv(j,(cid:100)lg σ(cid:101)). If τi and τj do not share any common suﬃx,
then their relative positions in ˜B(cid:100)lg σ(cid:101) depend only on their last bit and the relation is given by
that bit. Otherwise, τi and τj share a common suﬃx of length (cid:100)lg σ(cid:101) − δ + 1 ∈ [1,(cid:100)lg σ(cid:101)], that
is, τi[δ,(cid:100)lg σ(cid:101)] = τj[δ,(cid:100)lg σ(cid:101)]. Then, according to Lemma 2, τi is before τj iﬀ τi[δ] < τj[δ].
In
both cases the relation is given by the last distinct bit of the codes, or the ﬁrst if they are read
backwards. Since the codes are of the same length, comparing by the ﬁrst distinct bit is equivalent
to comparing numerically. That is, τi is before τj iﬀ inv(τi,(cid:100)lg σ(cid:101)) < inv(τj,(cid:100)lg σ(cid:101)). In turn, since
inv(inv(c, (cid:96)), (cid:96)) = c, this is equivalent to i < j.

The lemma gives a way to force a desired order in a set of ﬁxed-length codes: Given symbols
a ∈ [0, σ), we can assign them codes ca = inv(a,(cid:100)lg σ(cid:101)) to ensure that the areas become ordered
in ˜B(cid:100)lg σ(cid:101). As a side note, we observe that we could have retained the symbol order natively in the
wavelet matrix if we had chosen to decompose the symbols from their least to their most signiﬁcant
bit, and not the other way (in this case the wavelet matrix is actually radix-sorting the values).
This brings problems in the extended variants, however, because the resulting range of codes has
unused entries if σ is not a power of 2. For example, consider alphabet 0, 1, 2, 3, 4 = 000, . . . , 100;
after reversing the bits we obtain numbers 0, 1, 2, 4, 6, so we need to allocate 7 cells for C instead of
5. The size of C can double in the worst case. We cannot either directly use the idea of reversing the
canonical Huﬀman codes, because the codes could not be preﬁx-free anymore. A more sophisticated
scheme, based on Lemma 3, is required.

Assume we have obtained the desired code lengths (cid:96)a, as well as the array nCodes from the
canonical Huﬀman construction. We generate the ﬁnal Huﬀman tree in levelwise order. The
simplest description is as follows. We start with a set of valid codes C = {0, 1} and level (cid:96) = 1. At
each level (cid:96), we remove from C the nCodes[(cid:96)] codes c with minimum inv(c, (cid:96)) value. The removed
nodes are assigned to the nCodes[(cid:96)] symbols that require codes of length (cid:96). Now we replace each
code c remaining in C, by two new codes, c : 0 and c : 1, and continue with level (cid:96) + 1. It is clear
that this procedure generates a preﬁx-free set of codes that, when reversed, satisfy that the codes
ﬁnishing at a level are smaller than those that continue.
It is not hard to see that the total cost of this algorithm is linear. There are two kind of codes
inserted in C: those that will be chosen for a code and those that will not. There are exactly σ
nodes of the ﬁrst class, whereas for each node of the second class we insert other two codes in C.
Therefore the total number of codes ever inserted in C adds up to O(σ). The codes to use at each
level (cid:96) can be obtained by linear-time selection over the set of codes just extended (sorting codes
by inv(c, (cid:96))), thus adding up to O(σ) time as well.

14

Figure 6: On the left, the Huﬀman tree resulting from our code reassignment algorithm on the
running example. On the right, the resulting Huﬀman shaped wavelet matrix.

Figure 6 gives an example of the construction.

6 Experimental Results

Our implementations build over the wavelet tree implementations of Libcds, a library implementing 
several space-eﬃcient data structures.3 For each wavelet tree/matrix variant we present two
versions, CM and RRR. The ﬁrst one corresponds to using the the rank/select enabled bitmap implementation 
[34] of the proposals of Clark [19] and Munro [48], choosing 5% space overhead over
the plain bitmap. The second version, RRR, corresponds to using the bitmap implementation [20] of
the compressed bitmaps of Raman, Raman and Rao [58]. The variants compared are the following:

• WT: standard pointer-based wavelet tree;
• WTNP: the (extended) pointerless wavelet tree (“No Pointers”);
• WM: the (extended) wavelet matrix (Section 4);
• HWT: the Huﬀman shaped standard pointer-based wavelet tree;
• HWTNP: the Huﬀman shaped extended levelwise wavelet tree (Section 3);
• HWM: the Huﬀman shaped (extended) wavelet matrix (Section 5);
• AP: the alphabet-partitioned data structure of Barbay et al. [4], which is the best state-of-

the-art alternative to wavelet trees.

These names are composed with the bitmap implementations by appending the bitmap representation 
name. For example, we call WT-RRR the standard pointer-based wavelet tree with all bitmaps
represented with Raman, Raman and Rao’s compressed bitmaps. AP uses always CM bitmaps, which
is the best choice for this structure.

Note that all the pointerless structures use the array C. The extended versions generally achieve

space very close to the strict ones and perform much faster.

3https://github.com/fclaude/libcds

15

17250634000000011111114  7  6  5  3  2  1  0  2  1  4  1  70  1  0  1  0  1  0  1  1  0  0  0  1  7  5  2  0  2  7  4  6  3  1  1  4  10  1  1  1  1  0  1  1  1  0  0  1  01  0  1  0  0  1  1  05  0  6  3  2  2  4  40  1  0  10  3  5  65  2  0  2  4  6  3  4  7  7  1  1  16.1 Datasets

In order to evaluate the performance of access, rank and select, we use four diﬀerent datasets:4
• ESWiki: Sequence of word identiﬁers generated by stemming the Spanish Wikipedia5 with the
Snowball algorithm. The sequence has length n = 200,000,000, alphabet size σ = 1,634,145,
and zero-order entropy H0 = 11.12. This sequence can be used to simulate a positional
inverted index [20, 2, 14].

• BWT: The Burrows-Wheeler transform (BWT) [16] of ESWiki. The length and size of the
alphabet, as well as the zero-order entropy, match those of ESWiki. However, BWT has a much
lower high-order entropy [47]. Many full-text compressed self-indexes [28, 29, 51] use the
BWT of the text they represent.

• Indochina: The concatenation of all adjacency lists of Web graph Indochina2004, available
at the WebGraph project.6 The length of the sequence is n = 100,000,000, the alphabet size
σ = 2,705,024, and the entropy is H0 = 15.69. This representation has been used to support
forward and backward traversals on the graph [20, 21].

• INV: Concatenation of inverted lists for a random sample of 2,961,510 documents from the
English Wikipedia.7 This sequence has length n = 338,027,430 and its alphabet size is
σ = 2,961,510. From this sequence we extract the ﬁrst n = 180,000,000 elements with an
alphabet of size σ = 1,590,398 and an entropy of H0 = 19.01. This sequence has been used
to simulate document inverted indexes [56, 31].

In order to evaluate the range search performance over discrete grids, we use the following
three datasets formed by synthetic and real collections of MBRs (Minimum Bounding Rectangles
of objects). We insert the two opposite corners of each MBR as points in our dataset.

• Zipf: A synthetic collection of 1,000,000 MBRs with a Zipﬁan distribution (world size =

1,000 × 1,000, ρ = 1).8

• Gauss: A synthetic collection of contains 1,000,000 MBRs with a Gaussian distribution (world

size = 1,000 × 1,000, µ = 500, σ = 200).8

• Tiger: A real collection of 2,249,727 MBRs from California roads, available at the U.S.

Census Bureau.9

For range searches we cannot use Huﬀman compression, because the order of the symbols is
not maintained at the leaves. AP also shuﬄes the alphabet, so it cannot be used in this scenario.
Extended variants are not a good option either, because in this case it holds σ = n. Thus we test
only the strict variants of WTNP and WM.

4Left at http://lbd.udc.es/research/ECWTLA
5http://es.wikipedia.org dated 03/02/2010.
6http://law.dsi.unimi.it
7http://en.wikipedia.org
8http://lbd.udc.es/research/serangequerying
9http://www.census.gov/geo/www/tiger

16

6.2 Measurements

To measure performance we generated 100,000 inputs for each query and averaged their execution
time, running each query 10 times. The access(S, i) queries were generated by choosing positions
i uniformly at random in [1, n]. Queries ranka(S, i) were generated by choosing i uniformly at random,
 and then setting a = S[i]. Each selecta(S, j) query was generated by ﬁrst choosing a position
i at random in [1, n], then setting a = S[i], and ﬁnally choosing j at random in [1, ranka(S, n)].
The resulting distribution is the most common in applications, and it obtains the O(H0(S) + 1)
average time performance in the Huﬀman shaped variants.

To measure the performance on point grids, for synthetic collections we generate sets of queries
covering 0.001%, 0.01%, 0.1%, and 1% of the grid area. The sets contain 1,000 queries, each with
a ratio between both axes varying uniformly at random between 0.25 and 2.25. For the real data
set Tiger, we use as queries the following four collections (also available for downloading at the
Web site of Tiger): Block (groups of buildings), BG (block groups), SD (elementary, secondary, and
uniﬁed school districts), and COUSUB (country subdivisions).

The machine used is an Intel(R) Xeon(R) E5620 running at 2.40GHz with 96GB of RAM
memory. The operating system is GNU/Linux, Ubuntu 10.04, with kernel 2.6.32-33-server.x86 64.
All our implementations use a single thread and are coded in C++. The compiler is gcc version
4.4.3, with -O9 optimization.

6.3 Results on Sequences

Figures 7 to 9 show the time and space for the diﬀerent data structures and conﬁgurations for
access, rank and select queries. The black vertical bar on the plots shows the value of H0. The
bitmaps are parametrized by setting their sampling values to 32, 64, and 128. In the case of AP,
these bitmap samplings are combined with permutation samplings 4, 16, and 64, respectively, and
all are run with (cid:96)min = 10, as in previous work [4].

Space. We start by discussing the space usage, which we measure in bits per symbol (bps). First
we note that the WM variants use always the same space as the corresponding WTNP variants (while
being faster, as we discuss soon). The space of WTNP-CM and WM-CM is obviously close to (cid:100)lg σ(cid:101) bps.
The extra space incurred by WT-CM is the overhead of the wavelet tree pointers, and is roughly
proportional to σ/n (times some implementation-dependent constant). This amounts to nearly 4
bps in ESWiki and BWT, but 3.5 times more (14 bps) in Indochina, as expected from its larger
alphabet size, and again 4 bps in INV. On the other hand, the space of HWTNP-CM and HWM-CM is
always close to H0 bits per symbol, plus a small extra to store the Huﬀman model. The space
overhead of HWT-CM on top of those corresponds, again, to the wavelet tree pointers.

The sampling parameter aﬀects more sharply the RRR variants, as they store more data per
sample. The diﬀerence between WTNP-RRR or WM-RRR and WT-RRR is also proportional to σ/n, but
this time the constant is higher because the RRR implementation needs more constants to be stored
per bitmap (i.e., per wavelet tree node). Thus the penalty is 6 bps on ESWiki and BWT, 21 bps
(3.5 times more) on Indochina, and 7 bps on INV. The same diﬀerences can be observed between
HWTNP-RRR or HWM-RRR and HWT-RRR. We return later to the fact that HWM-RRR takes more space
than HWTNP-RRR on Indo and INV.

Finally, how WTNP-RRR/WM-RRR and HWTNP-RRR/HWM-RRR compare to HWTNP-CM/WM-CM depends
strongly on the type of sequence. In general, RRR compression achieves the zero-order entropy as

17

an upper bound, but it can reach much less when the sequence has local regularities. On the
other hand, RRR representation poses an additive overhead of 27% of lg σ, which corresponds to the
o(n lg σ) overhead in this implementation [20]. When combining Huﬀman and bitmap compression,
this 27% overhead acts over H0 and not over lg σ, which brings it down, but on the other hand we
must add the overhead of storing the Huﬀman model. On ESWiki, which has no special properties,
the 27% overhead is around 5.7 bps, showing that RRR compression reaches around 8.3 bps, well
below H0. When combining with Huﬀman compression, this overhead becomes 14%, that is, nearly
3 bps. Added to the 8.3 bps and to the 1 bps of the Huﬀman model overhead, we still get slightly
more space than plain Huﬀman compression, which is the best choice and reaches only 10% overhead
over the zero-order entropy.

The picture changes when we consider BWT. The Burrows-Wheeler transform of ESWiki boosts
its higher-order compressibility [47], which is captured by RRR compression [43], making RRR compression 
reach the same space of Huﬀman compression, despite its 27% space overhead. When
combining both compressions, the result breaks the zero-order entropy barrier by more than 10%
and becomes the best choice in terms of space.

RRR gives another surprising result on Indochina and INV, where bitmap compression alone
is more space-eﬀective than in combination with Huﬀman compression, and breaks the zero-order
entropy by a large margin. This cannot be explained by high-order compressibility, as in this case
the combination with Huﬀman would not harm. This behavior corresponds to the special nature
of these sequences: the adjacency lists of the graph and the inverted lists are sorted in increasing
order. Long increasing sequences induce long runs of 0s and 1s in the bitmaps of the wavelet trees
and matrices. Those are retained in deeper levels when our partitioning by the most signiﬁcant bit
is used.10 The Huﬀman algorithm, instead, combines the nodes in unpredictable ways and destroys
those long runs. Still, our Huﬀman algorithm maintains the order between those symbols whose
codewords have the same length, and thus the impact of this reordering is not as high as it could be.
Instead, the Huﬀman wavelet matrix completely reshuﬄes the symbols. As a result, for example,
the space of HWM-RRR exceeds that of HWTNP-RRR by around 5 bps on Indo and 6–7 bps on INV.

Time. The time results are rather consistent across collections. Let us ﬁrst consider operation
access. If we start considering the variants that do not use Huﬀman compression, we have that
WT-RRR is about 10%–25% slower than WT-CM, which is explained by a more complex implementation
[20]. Instead, the pointerless variant, WTNP-CM, is 20%–25% slower (recall that, in their extended
variant, these require twice the number of rank operations, but locality of reference makes them
faster than twice the cost of one rank operation). However, WTNP-RRR is about 40% slower than
WT-RRR, as the rank operation is slower and its higher number impacts more on the total time (but
still locality of reference makes the percentage much less than 100%). The wavelet matrix, instead,
carries out the same number of rank operations than the pointer-based wavelet tree, so this time
penalty disappears. Actually, WM-CM is 8%–14% faster than WT-CM, and WM-RRR is up to 4% faster
than WT-RRR. This may be due to less memory usage, which increases locality of reference. Finally,
the use of Huﬀman compression improves times by about H0/ lg σ, as expected: times are reduced
to about 50%–60% on ESWiki and BWT, to about 65%–85% on Indochina, and there is almost no
reduction on INV.

The situation is basically the same for operation rank, as expected from the algorithms. The
times are usually slightly lower because it is not necessary to access the bitmaps as we descend.

10This is another advantage over using the least signiﬁcant bit, as this partitioning breaks the runs faster.

18

The use of the wavelet matrix still gives essentially the same time (and even slightly faster) than
a pointer-based wavelet tree, and the use of Huﬀman shaped trees reduces the times by the same
factors as for access, as expected.

The times of operation select show less diﬀerence between the standard and the pointerless
variants, because performing one extra rank operation is less relevant compared to the original
(slower) select operation on the bitmaps. One can see that WTNP-CM is 30%–40% slower than WT-CM
and that WTNP-RRR is 35%–50% slower than WT-RRR. The diﬀerence between plain and compressed
bitmaps does not vary much, on the other hand: WT-RRR is 25%–30% slower than WT-CM. What is
more surprising is that the wavelet matrix is clearly slower than the pointer-based wavelet trees:
WM-CM is 10%–15% slower than WT-CM and WM-RRR is 20%–30% slower than WT-RRR. The reason
is that the implementations of select [34, 20] proceed by binary search on the sampled values,
thus their cost has in practice a component that is logarithmic on the bitmap length. The bitmaps
on the wavelet tree nodes are shorter than n, whereas in the wavelet matrix (and the pointerless
wavelet tree) they are always of length n. Indeed, the wavelet matrix is faster than the pointerless
wavelet tree: WM-CM is 20%–25% faster than WTNP-CM and WM-RRR is 12%–15% faster than WTNP-RRR.
Once again, the use of Huﬀman reduces all the times by about the same space fraction obtained
by zero-order compression.

Bottom line. On ESWiki, where zero-order compression is the dominant space factor, our Huﬀman 
shaped wavelet matrix, HWM-CM, obtains the best space (only 10% oﬀ the zero-order entropy)
and the best time, by a good margin.

On BWT, where higher-order compression is exploited by RRR, the space-time tradeoﬀ map is
dominated by the combination of HWM-RRR (minimum space) and HWM-CM (minimum time), the two
variants of our Huﬀman shaped wavelet matrix. The former breaks the zero-order entropy barrier
by about 10%.

On Indochina and INV, where RRR achieves space gains that are only degraded by Huﬀman
compression, the dominant techniques are variants of the wavelet matrix: WM-RRR (least space) and
HWM-CM (least time). The former takes about 75% of the zero-order entropy.

Summarizing, the wavelet matrix variants obtain the same space of the pointerless wavelet
trees, but they operate in about 65% of their time, reaching basically the same performance of the
pointer-based variants but much less space. As a result, they are always the dominant technique.
Which variant is the best, HWM-CM, HWM-RRR or WM-RRR, depends on the nature of the collection.

The comparison with AP is interesting. In collections similar to ESWiki, Barbay et al. [4] show
that AP generally achieves the best space and time among the alternatives WTNP-RRR, WTNP-CM,
WT-CM, and WT-RRR, thus becoming an excellent choice in that group. The new alternatives we
have developed, however, clearly outperform AP in space: pointerless Huﬀman compression, and in
particular Huﬀman wavelet matrices, improve upon the old wavelet tree alternatives in both space
and time, using much less space than AP. Still, AP is a faster representation, only slightly faster
in operations access and rank, and deﬁnitely faster in operation select. The other collections
also demonstrate that wavelet trees and matrices can exploit other compressibility features of the
sequences apart from H0, whereas AP is blind to those (this is also apparent in their experiments
[4], even using the basic wavelet tree variants).

19

6.4 Results on Point Grids

Figures 10 and 11 show the performance of WTNP and WM for count and report queries, respectively.
It turns out that, in the ﬁrst level of each wavelet tree, the number of zeros and ones is highly
unbalanced when the grid size is far from the next power of 2. This makes the entropy of the ﬁrst
bitmap rather low, whereas the other bitmaps are more balanced. On the other hand, the range
search algorithms spend just a few rank operations on the ﬁrst bitmap. To take advantage of this
feature, we compress the bitmap of the ﬁrst level of both data structures, WTNP and WM, with RRR
and with a sampling of 32. The rest of bitmaps are represented using CM with sampling rates of 32,
64, and 128.
In both ﬁgures 10 and 11 we append to the name of the data structure the name of the query
set. This takes values in {Q0001, Q001, Q01, Q1} in case of synthetic collections. In case of the real
collection Tiger, it takes values in {BLock, BG, SD, COUSUB}.

The results for the counting queries shows that the time worsens as the query are less selective.
The wavelet matrix is always faster than the pointerless wavelet tree, while using the same space.
The diﬀerence in time is proportional to the cost for each selectivity, but additive with respect to
the sampling. For example, it becomes about 25% faster when using the most space. We note in
passing that the space is basically 21 bps for the synthetic spaces and 23 bps for the Tiger dataset,
which is essentially lg σ = lg n.

In the case of reporting queries, we show the time per reported item, which decreases as the
query is less selective. Once again the wavelet matrix is faster than the pointerless wavelet tree,
albeit this time by a smaller margin: usually below 10%.

7 Conclusions

The levelwise wavelet tree [42, 44], designed to avoid the O(σ lg n) space overhead of standard
wavelet trees [35], was unnecessarily slow in practice. We have redesigned this data structure
so that its time overhead over standard wavelet trees is signiﬁcantly lower. The result, dubbed
wavelet matrix, enjoys all the good properties of levelwise wavelet trees but performs signiﬁcantly
faster in practice. It requires n lg σ + o(n lg σ) bits of space, and can be built in O(n lg σ) time
and almost in-place. We have also shown how to represent Huﬀman shaped wavelet trees without
using pointers by means of canonical Huﬀman codes, and adapted the mechanism to Huﬀman
shaped wavelet matrices. This required a nontrivial redesign of the variable-length code assignment
mechanism. Our experimental results show that the compressed wavelet matrix dominates the
space/time tradeoﬀ map for all the real-life sequences we considered, also outperforming in most
cases other structures designed for large alphabets [4]. We also showed that the wavelet matrix is
the best choice to represent point grids that support orthogonal range queries.
An interesting future work is to adapt multiary wavelet trees [29] to wavelet matrices. The only
diﬀerence is that, instead of a single accumulator z(cid:96) per level, we have an array of ρ−1 accumulators
in a ρ-ary wavelet matrix. As the useful values for ρ are O(lg n), the overall space is still negligible,
O(lg2 n lg σ). The real challenge is to transalte the reduction in depth into a reduction of actual
execution times.

Dynamic wavelet trees [45, 38, 57] can immediately be translated into wavelet matrices.

It
would be interesting to consider newer, theoretically more eﬃcient dynamic versions [53], and
obtain practically eﬃcient implementations over wavelet matrices.

20

Figure 7: Running time per access query over the four datasets.

21

 2 4 6 8 10 12 14 16 14 16 18 20 22 24 26 28 30Time (microseconds)Bits per symbolINVHWM-RRRHWM-CMHWT-RRRHWT-CMWT-RRRWT-CMWTNP-RRRWTNP-CMWM-RRRWM-CMHWTNP-RRRHWTNP-CMAP 1 2 3 4 5 6 7 8 9 10 8 10 12 14 16 18 20 22 24 26 28Time (microseconds)Bits per symbolBWT 1 2 3 4 5 6 7 8 9 10 11 14 16 18 20 22 24 26 28 30Time (microseconds)Bits per symbolINV 1 2 3 4 5 6 7 8 9 10 12 14 16 18 20 22 24 26 28Time (microseconds)Bits per symbolESWiki 1 2 3 4 5 6 7 8 9 10 15 20 25 30 35 40Time (microseconds)Bits per symbolIndochinaFigure 8: Running time per rank query over the four datasets.

22

 2 4 6 8 10 12 14 16 14 16 18 20 22 24 26 28 30Time (microseconds)Bits per symbolINVHWM-RRRHWM-CMHWT-RRRHWT-CMWT-RRRWT-CMWTNP-RRRWTNP-CMWM-RRRWM-CMHWTNP-RRRHWTNP-CMAP 1 2 3 4 5 6 7 8 9 8 10 12 14 16 18 20 22 24 26 28Time (microseconds)Bits per symbolBWT 1 2 3 4 5 6 7 8 9 10 11 14 16 18 20 22 24 26 28 30Time (microseconds)Bits per symbolINV 1 2 3 4 5 6 7 8 9 10 12 14 16 18 20 22 24 26 28Time (microseconds)Bits per symbolESWiki 1 2 3 4 5 6 7 8 9 10 15 20 25 30 35 40Time (microseconds)Bits per symbolIndochinaFigure 9: Running time per select query over the four datasets.

23

 2 4 6 8 10 12 14 16 14 16 18 20 22 24 26 28 30Time (microseconds)Bits per symbolINVHWM-RRRHWM-CMHWT-RRRHWT-CMWT-RRRWT-CMWTNP-RRRWTNP-CMWM-RRRWM-CMHWTNP-RRRHWTNP-CMAP 2 4 6 8 10 12 14 16 18 20 22 8 10 12 14 16 18 20 22 24 26 28Time (microseconds)Bits per symbolBWT 2 4 6 8 10 12 14 16 18 20 22 14 16 18 20 22 24 26 28 30Time (microseconds)Bits per symbolINV 2 4 6 8 10 12 14 16 18 20 22 10 12 14 16 18 20 22 24 26 28Time (microseconds)Bits per symbolESWiki 2 4 6 8 10 12 14 16 18 20 10 15 20 25 30 35 40Time (microseconds)Bits per symbolIndochinaFigure 10: Running time per count query over the three datasets.

24

 0.2 0.4 0.6 0.8 1 1.2 1.4 20.45 20.5 20.55 20.6 20.65 20.7 20.75 20.8 20.85 20.9 20.95Time (microseconds) per queryBits per MBRGAUSSGAUSS & ZIPFWM-Q0001WTNP-Q0001WM-Q001WTNP-Q001WM-Q01WTNP-Q01WM-Q1WTNP-Q1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 22.4 22.5 22.6 22.7 22.8 22.9 23 23.1Time (microseconds) per queryBits per MBRTIGERTIGERWM-BlockWTNP-BlockWM-BGWTNP-BGWM-SDWTNP-SDWM-COUSUBWTNP-COUSUB 0.2 0.4 0.6 0.8 1 1.2 1.4 20.45 20.5 20.55 20.6 20.65 20.7 20.75 20.8 20.85 20.9 20.95Time (microseconds) per queryBits per pointGAUSS 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 20.45 20.5 20.55 20.6 20.65 20.7 20.75 20.8 20.85 20.9 20.95Time (microseconds) per queryBits per pointZIPF 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 22.4 22.5 22.6 22.7 22.8 22.9 23 23.1Time (microseconds) per queryBits per pointTIGERFigure 11: Running time of report query over the three datasets.

25

 0.2 0.4 0.6 0.8 1 1.2 1.4 20.45 20.5 20.55 20.6 20.65 20.7 20.75 20.8 20.85 20.9 20.95Time (microseconds) per queryBits per MBRGAUSSGAUSS & ZIPFWM-Q0001WTNP-Q0001WM-Q001WTNP-Q001WM-Q01WTNP-Q01WM-Q1WTNP-Q1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 22.4 22.5 22.6 22.7 22.8 22.9 23 23.1Time (microseconds) per queryBits per MBRTIGERTIGERWM-BlockWTNP-BlockWM-BGWTNP-BGWM-SDWTNP-SDWM-COUSUBWTNP-COUSUB 0.6 0.8 1 1.2 1.4 1.6 1.8 2 2.2 2.4 2.6 20.45 20.5 20.55 20.6 20.65 20.7 20.75 20.8 20.85 20.9 20.95Time (microseconds) per reported itemBits per pointGAUSS 0.5 1 1.5 2 2.5 3 3.5 4 20.45 20.5 20.55 20.6 20.65 20.7 20.75 20.8 20.85 20.9 20.95Time (microseconds) per reported itemBits per pointZIPF 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 2.2 2.4 2.6 22.4 22.5 22.6 22.7 22.8 22.9 23 23.1Time (microseconds) per reported itemBits per pointTIGERAcknowledgement. Thanks to Daisuke Okanohara for useful comments.

References

[1] D. Arroyuelo, F. Claude, S. Maneth, V. M¨akinen, G. Navarro, K. Nguy˜ˆen, J. Sir´en, and
N. V¨alim¨aki. Fast in-memory XPath search over compressed text and tree indexes. In Proc.
26th IEEE International Conference on Data Engineering (ICDE), pages 417–428, 2010.

[2] D. Arroyuelo, S. Gonz´alez, and M. Oyarz´un. Compressed self-indices supporting conjunctive
queries on document collections. In Proc. 17th International Symposium on String Processing
and Information Retrieval (SPIRE), pages 43–54, 2010.

[3] J. Barbay, L.C. Aleardi, M. He, and J.I. Munro. Succinct representation of labeled graphs.

Algorithmica, 62(1-2):224–257, 2012.

[4] J. Barbay, F. Claude, T. Gagie, G. Navarro, and Y. Nekrich. Eﬃcient fully-compressed sequence 
representations. Algorithmica, 2013. To appear, available as early view.

[5] J. Barbay, F. Claude, and G. Navarro. Compact rich-functional binary relation representations.
In Proc. 9th Latin American Symposium on Theoretical Informatics (LATIN), LNCS 6034,
pages 170–183, 2010.

[6] J. Barbay, T. Gagie, G. Navarro, and Y. Nekrich. Alphabet partitioning for compressed
rank/select and applications. In Proc. 21st Annual International Symposium on Algorithms
and Computation (ISAAC), LNCS 6507, pages 315–326 (part II), 2010.

[7] J. Barbay, A. Golynski, I. Munro, and S. Srinivasa Rao. Adaptive searching in succinctly
encoded binary relations and tree-structured documents. Theoretical Computer Science,
387(3):284–297, 2007.

[8] J. Barbay, M. He, I. Munro, and S. Srinivasa Rao. Succinct indexes for strings, binary relations

and multilabeled trees. ACM Transactions on Algorithms, 7(4):article 52, 2011.

[9] J. Barbay and G. Navarro. On compressing permutations and adaptive sorting. CoRR,

abs/1108.4408, 2011.

[10] D. Belazzougui and G. Navarro. New lower and upper bounds for representing sequences. In
Proc. 20th Annual European Symposium on Algorithms (ESA), LNCS 7501, pages 181–192,
2012.

[11] T. Beller, S. Gog, E. Ohlebusch, and T. Schnattinger. Computing the longest common preﬁx
In Proc 18th International Symposium on

array based on the Burrows-Wheeler transform.
String Processing and Information Retrieval (SPIRE), LNCS 7024, pages 197–208, 2011.

[12] D. Benoit, E. D. Demaine, J. I. Munro, R. Raman, V. Raman, and S. S. Rao. Representing

trees of higher degree. Algorithmica, 43(4):275–292, 2005.

[13] P. Bose, M. He, A. Maheshwari, and P. Morin. Succinct orthogonal range search structures on a
grid with applications to text indexing. In Proc. 11th International Symposium on Algorithms
and Data Structures (WADS), LNCS 5664, pages 98–109, 2009.

26

[14] N. Brisaboa, A. Fari˜na, S. Ladra, and G. Navarro. Implicit indexing of natural language text

by reorganizing bytecodes. Information Retrieval, 15(6):527–557, 2012.

[15] N. Brisaboa, M. Luaces, G. Navarro, and D. Seco. A fun application of compact data structures
to indexing geographic data. In Proc. 5th International Conference on Fun with Algorithms
(FUN), pages 77–88, 2010.

[16] M. Burrows and D. Wheeler. A block sorting lossless data compression algorithm. Tech. Rep.

124, Digital Equipment Corporation, 1994.

[17] B. Chazelle. A functional approach to data structures and its use in multidimensional searching.

SIAM Journal on Computing, 17(3):427–462, 1988.

[18] Y.-F. Chien, W.-K. Hon, R. Shah, and J. Vitter. Geometric Burrows-Wheeler transform:
Linking range searching and text indexing. In Proc. 18th Data Compression Conference (DCC),
pages 252–261, 2008.

[19] D. Clark. Compact Pat Trees. PhD thesis, Univ. of Waterloo, Canada, 1996.

[20] F. Claude and G. Navarro. Practical rank/select queries over arbitrary sequences. In Proc.
15th International Symposium on String Processing and Information Retrieval (SPIRE), LNCS
5280, pages 176–187, 2008.

[21] F. Claude and G. Navarro. Extended compact Web graph representations. In Algorithms and

Applications (Ukkonen Festschrift), LNCS 6060, pages 77–91, 2010.

[22] F. Claude and G. Navarro. Self-indexed grammar-based compression. Fundamenta Informaticae,
 111(3):313–337, 2010.

[23] F. Claude and G. Navarro. The wavelet matrix. In Proc. 19th International Symposium on

String Processing and Information Retrieval (SPIRE), LNCS 7608, pages 167–179, 2012.

[24] F. Claude, P. Nicholson, and D. Seco. Space eﬃcient wavelet tree construction. In Proc. 18th
International Symposium on String Processing and Information Retrieval (SPIRE), pages 185–
196, 2011.

[25] T. Cover and J. Thomas. Elements of Information Theory. Wiley, 1991.

[26] P. Ferragina, R. Giancarlo, and G. Manzini. The myriad virtues of wavelet trees. Information

and Computation, 207(8):849–866, 2009.

[27] P. Ferragina, F. Luccio, G. Manzini, and S. Muthukrishnan. Compressing and indexing labeled

trees, with applications. Journal of the ACM, 57(1):article 4, 2009.

[28] P. Ferragina and G. Manzini. Indexing compressed texts. Journal of the ACM, 52(4):552–581,

2005.

[29] P. Ferragina, G. Manzini, V. M¨akinen, and G. Navarro. Compressed representations of sequences 
and full-text indexes. ACM Transactions on Algorithms, 3(2):article 20, 2007.

[30] T. Gagie, J. K¨arkk¨ainen, G. Navarro, and S.J. Puglisi. Colored range queries and document

retrieval. Theoretical Computer Science, 483:36–50, 2013.

27

[31] T. Gagie, G. Navarro, and S. J. Puglisi. New algorithms on wavelet trees and applications to

information retrieval. Theoretical Computer Science, 426-427:25–41, 2012.

[32] T. Gagie, S. J. Puglisi, and A. Turpin. Range quantile queries: Another virtue of wavelet
trees. In Proc. 16th International Symposium on String Processing and Information Retrieval
(SPIRE), LNCS 5721, pages 1–6, 2009.

[33] A. Golynski, J. I. Munro, and S. S. Rao. Rank/select operations on large alphabets: a tool for
text indexing. In Proc. 17th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA),
pages 368–373, 2006.

[34] R. Gonz´alez, Sz. Grabowski, V. M¨akinen, and G. Navarro. Practical implementation of rank
and select queries. In Proc. 4th Workshop on Eﬃcient and Experimental Algorithms (WEA),
pages 27–38, 2005. Posters.

[35] R. Grossi, A. Gupta, and J. Vitter. High-order entropy-compressed text indexes. In Proc. 14th

Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 841–850, 2003.

[36] R. Grossi and J. Vitter. Compressed suﬃx arrays and suﬃx trees with applications to text

indexing and string matching. SIAM Journal on Computing, 35(2):378–407, 2006.

[37] R. Grossi, J. Vitter, and B. Xu. Wavelet trees: From theory to practice. In Proc. 1st International 
Conference on Data Compression, Communications and Processing (CCP), pages
210–221, 2011.

[38] M. He and I. Munro. Succinct representations of dynamic strings. In Proc. 17th International

Symposium on String Processing and Information Retrieval (SPIRE), pages 334–346, 2010.

[39] D. Huﬀman. A method for the construction of minimum-redundancy codes. Proceedings of

the I.R.E., 40(9):1090–1101, 1952.

[40] G. Jacobson. Space-eﬃcient static trees and graphs.

In Proc. 30th IEEE Symposium on

Foundations of Computer Science (FOCS), pages 549–554, 1989.

[41] S. Kreft and G. Navarro. On compressing and indexing repetitive sequences. Theoretical

Computer Science, 483:115–133, 2013.

[42] V. M¨akinen and G. Navarro. Position-restricted substring searching. In Proc. 7th Latin American 
Symposium on Theoretical Informatics (LATIN), pages 703–714, 2006.

[43] V. M¨akinen and G. Navarro.

Implicit compression boosting with applications to selfindexing.
 In Proc. 14th International Symposium on String Processing and Information Retrieval 
(SPIRE), LNCS 4726, pages 214–226, 2007.

[44] V. M¨akinen and G. Navarro. Rank and select revisited and extended. Theoretical Computer

Science, 387(3):332–347, 2007.

[45] V. M¨akinen and G. Navarro. Dynamic entropy-compressed sequences and full-text indexes.

ACM Transactions on Algorithms, 4(3):article 32, 2008.

28

[46] C. Makris. Wavelet trees: a survey. Computer Science and Information Systems, 9(2):585–625,

2012.

[47] G. Manzini. An analysis of the Burrows-Wheeler transform. Journal of the ACM, 48(3):407–

430, 2001.

[48] I. Munro. Tables.

In Proc. 16th Conference on Foundations of Software Technology and

Theoretical Computer Science (FSTTCS), LNCS 1180, pages 37–42, 1996.

[49] G. Navarro. Indexing text using the Ziv-Lempel trie. Journal of Discrete Algorithms, 2(1):87–

114, 2004.

[50] G. Navarro. Wavelet trees for all. In Proc. 23rd Annual Symposium on Combinatorial Pattern

Matching (CPM), LNCS 7354, pages 2–26, 2012.

[51] G. Navarro and V. M¨akinen. Compressed full-text indexes. ACM Computing Surveys, 39(1):article 
2, 2007.

[52] G. Navarro and Y. Nekrich. Top-k document retrieval in optimal time and linear space. In
Proc. 23rd Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1066–1078,
2012.

[53] G. Navarro and Y. Nekrich. Optimal dynamic sequence representations. In Proc. 24th Annual

ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 865–876, 2013.

[54] G. Navarro, Y. Nekrich, and L. Russo. Space-eﬃcient data-analysis queries on grids. Theoretical

Computer Science, 482:60–72, 2013.

[55] G. Navarro and A. Ord´o˜nez. Compressing Huﬀman models on large alphabets. In Proc. 23rd

Data Compression Conference (DCC), pages 381–390, 2013.

[56] G. Navarro and S. J. Puglisi. Dual-sorted inverted lists. In Proc. 17th International Symposium

on String Processing and Information Retrieval (SPIRE), pages 310–322, 2010.

[57] G. Navarro and K. Sadakane. Fully-functional static and dynamic succinct trees. CoRR,

abs/0905.0768v5, 2010.

[58] R. Raman, V. Raman, and S. Srinivasa Rao. Succinct indexable dictionaries with applications 
to encoding k-ary trees, preﬁx sums and multisets. ACM Transactions on Algorithms,
3(4):article 43, 2007.

[59] D. Salomon. Data Compression. Springer, 2007.

[60] T. Schnattinger, E. Ohlebusch, and S. Gog. Bidirectional search in a string with wavelet trees.
In Proc. 21st Annual Symposium on Combinatorial Pattern Matching (CPM), LNCS 6129,
pages 40–50, 2010.

[61] E. Schwartz and B. Kallick. Generating a canonical preﬁx encoding. Communications of the

ACM, 7(3):166–169, 1964.

[62] G. Tischler. On wavelet tree construction. In Proc. 22nd Annual Symposium on Combinatorial

Pattern Matching (CPM), pages 208–218, 2011.

29

[63] N. V¨alim¨aki and V. M¨akinen. Space-eﬃcient algorithms for document retrieval. In Proc. 18th
Annual Symposium on Combinatorial Pattern Matching (CPM), LNCS 4580, pages 205–215,
2007.

30

