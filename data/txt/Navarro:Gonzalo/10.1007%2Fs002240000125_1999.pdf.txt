Theory Comput. Systems 32, 435–452 (1999)

Theory of
Computing
Systems
© 1999 Springer-Verlag
New York Inc.

Bounding the Expected Length of Longest Common Subsequences
and Forests

∗

R. A. Baeza-Yates,1 R. Gavald`a,2 G. Navarro1, and R. Scheihing1

1Department of Computer Science, University of Chile,
Blanco Encalada 2120, Santiago, Chile
{rbaeza,gnavarro,rscheihi}@dcc.uchile.cl
2Department of LSI, Technical University of Catalunya,
Jordi Girona Salgado 1–3, 08034 Barcelona, Spain
gavalda@lsi.upc.es

Abstract. We present improvements to two techniques to ﬁnd lower and upper
bounds for the expected length of longest common subsequences and forests of
two random sequences of the same length, over a ﬁxed size, uniformly distributed
alphabet. We emphasize the power of the methods used, which are Markov chains
and Kolmogorov complexity. As a corollary, we obtain some new lower and upper
bounds for the problems addressed as well as some new exact results for short
sequences.

1.

Introduction

The longest common subsequence (LCS) of two strings is one of the main problems in
combinatorial pattern matching. The LCS problem is related to DNA or protein alignments,
 ﬁle comparison, speech recognition, etc. We say that s is a subsequence of u if
we can obtain s by deleting zero or more letters of u. The LCS of two strings u and v
of length n is deﬁned as the longest subsequence s common to u and v. For example,
the LCS of longest and large is lge. An open problem related to the LCS is its expected
length for two random strings of length n over a uniformly distributed alphabet of size

∗

This work has been partially supported by the ESPRIT Long Term Research Project 20244, ALCOM
IT. The ﬁrst author has also been supported by Direcci´on General de Investigaci´on y T´ecnica (Ministry of
Education and Science, Spain) and Fundaci´on Andes. The second author has also been supported by Fondecyt
Grants 1950569 and 1940520. The third author has also been supported by Fondecyt Grant 1960881.

436

R. A. Baeza-Yates, R. Gavald`a, G. Navarro, and R. Scheihing

k, denoted by EL(k)
n . In particular, if an alignment or common subsequence of two given
sequences is relatively larger than EL(k)
n , we may infer that it is more than a coincidence,
and that the similarity should be studied further. For example, two long random DNA
sequences will have on average an alignment comprising 65% of its length, and that
amount of similarity might be counterintuitive.

If (cid:96)cs(u, v) denotes the length of the LCS for two strings u and v, we have

(cid:88)

EL(k)
n

= 1
k2n

(cid:96)cs(u, v).

|u|=|v|=n

+

is superadditive (credited to Klarner and Rivest in [CS1]), that is, EL(k)
n

Because EL(k)
≤ EL(k)
n
n+m, it is possible to show [CS1] that
EL(k)
m
γk = lim
n→∞

= sup

EL(k)
n
n

EL(k)
n
n

n

exists. However, the exact values of γk are still not known. For that reason, several lower
and upper bounds have been devised for γk. Chv´atal and Sankoff [CS1], [CS2] obtained

0.727273 ≤ γ2 ≤ 0.866595,

which was improved by Deken [De1], [De2] to

0.7615 ≤ γ2 ≤ 0.8575.

These were improved by Dan˘c´ık and Paterson [Da1], [DP], [PD] to

0.77391 ≤ γ2 ≤ 0.83763.
Also, it is known that [CS1]

√
k ≤ e.

EL(2)
n

1 ≤ γk
First, we present exact results for small sequences, computing the exact values of
from n = 10 [CS1] to n = 16, showing that the convergence to γk is slow.
Second, we present new lower bounds for 2 < k < 6 for the LCS. These new results
are based on a new class of automata (following the work of Deken [De1] and Dan˘c´ık
and Paterson [Da1], [PD]) that simulates an algorithm that computes a long common
subsequence over two random inﬁnite strings. These automata are called CSS (common
subsequence) machines in [Da1].

Third, to obtain upper bounds, we reﬁne and extend the Kolmogorov complexity
approach due to Tao Jiang (mentioned in [LV]), which is simple and elegant. Kolmogorov
complexity has been very useful in many areas of computer science. The reader is referred
to the monograph of Li and Vat´anyi [LV] for a very complete treatment of the origins,
development, and applications of this concept.

We also apply both techniques to a generalization of the LCS problem, called the
longest common forest (LCF) by Pevzner and Waterman [PW], obtaining the ﬁrst known
lower and upper bounds for the expected size of the LCF of two random sequences. In
particular, we show that, for large alphabets, the fraction of the expected length of the
LCF is upper bounded by e/

k as in the LCS case.

√

The results included here were presented in preliminary form in [BYS] and [BYGN].

Bounding the Expected Length of Longest Common Subsequences and Forests

437

2. Longest Common Subsequences and Forests

The length of the LCS of two strings u and v can be computed using dynamic programming 
over a matrix L deﬁned by L[0, i] = L[i, 0] = 0 for 0 ≤ i ≤ n and

L[i, j] = max(L[i − 1, j], L[i, j − 1], L[i − 1, j − 1] + (u[i] =? v[ j])),

1 ≤ i,

j ≤ n,

where (u[i] =? v[ j]) is deﬁned as 1 if both letters are equal, or 0 otherwise. The length of
the LCS is given by L[n, n]. This algorithm can be implemented using 3n2 comparisons.
For faster algorithms which solve the LCS problem we refer the reader to [GBY], [PD],
and [Ri].

LCFs are deﬁned in [PW] as one particular case of general alignments between
strings, called the A-LCS problem. Basically, in an LCF we allow a letter to match more
than one letter of the other sequence, but if we look at every match as an edge between
the two sequences, then no edge crossings can exist. Hence, the alignment is a set of
trees or a forest. In [PW] a cn2 algorithm to compute the A-LCS problem is given, where
c is related to the determinant of a matrix deﬁning the generalized alignment rules. They
mention that c = 2 for the LCF problem, but a simple algorithm is not explicitly given.
In fact, the dynamic programming procedure for LCF is given by

L[i, j] = max(L[i − 1, j], L[i, j − 1]) + (u[i] =? v[ j]),

which requires only 2n2 comparisons. If (cid:96)cf (u, v) denotes the length of the LCF for two
strings u and v, in general we have

0 ≤ (cid:96)cf (u, v) ≤ 2(|u| + |v|) − 1,

where the upper bound can be seen as the longest path where we either advance in a row
or a column of the matrix L. Similarly to the LCS, the LCF is superadditive. We can
deﬁne

(cid:88)

(cid:96)cf (u, v)

|u|=|v|=n

EF(k)
n

= 1
k2n

and

fk = lim
n→∞

EF(k)
n
n

= sup

n

EF(k)
n
n

≤ 2.

Table 1 and Figure 2 show some exact values of EL(2)
n

Figure 1 shows some examples of LCFs as well as the corresponding LCS length (the
solutions shown in the examples are not necessarily unique).
/n for n ≤ 16.
For the LCS these results extend [CS1]. Figure 3 shows the probability distribution of
LCS and LCF for n = 15 normalized by the length n. We can see that in both cases
the distribution is centered but with signiﬁcative tails, which partially explains why it is
difﬁcult to bound their average value better.

/n and EF(2)
n

438

R. A. Baeza-Yates, R. Gavald`a, G. Navarro, and R. Scheihing

Fig. 1. Some extreme LCS and LCF examples for a binary alphabet.

3. Lower Bounds: Markov Chains

The lower bounds are based on the work by Deken [De1] and Dan˘c´ık and Paterson [Da1],
[PD]. They present a ﬁnite automaton that models an algorithm which ﬁnds a common
subsequence (CS) on two inﬁnite strings (tapes). By analyzing the associated Markov
chain, a bound on the expected length of the LCS is found. The same idea can be applied
to the LCF problem.

Dan˘c´ık and Paterson use an automaton that alternatively reads from each one of the
two unbounded tapes. We read at the same time from both strings, allowing the possibility
of applying some symmetry rules which reduce the number of states. Informally, when
reading a new pair of letters of an alphabet  of size k with letters {0, 1, . . . , k − 1}, the
automaton outputs some matches that increase the CS and computes a new state based on

Fig. 2. Exact values for ELn /n and EFn /n for n ≤ 16 and k = 2.

Bounding the Expected Length of Longest Common Subsequences and Forests

439

Table 1. Exact values of ELn /n
and EFn /n for n ≤ 16 and k = 2.

n

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

EL(2)

n /n

0.5
0.5625
0.604167
0.630859
0.649219
0.66333
0.674491
0.68364
0.691303
0.697844
0.703517
0.708493
0.712904
0.7168467
0.7203977
0.7236174

EF(2)

n /n

0.5
0.875
1.0625
1.16406
1.22734
1.27148
1.30399
1.32881
1.34828
1.36388
1.37662
1.38721
1.39613
1.403736
1.4103058
1.4160315

Fig. 3. Probability distribution of ELn /n and EFn /n for n = 16 and k = 2.

440

R. A. Baeza-Yates, R. Gavald`a, G. Navarro, and R. Scheihing

the letters not used yet (partial memory). Therefore, at this point, all information about
the past has been lost. So, we obtain a lower bound, because, potentially, a longer CS (the
LCS) could have been obtained looking at the complete strings. Nevertheless, the fact
that we only have to look at the current state and the future, simpliﬁes the problem by
applying the following rules. Consider that each state s ∈ S is identiﬁed by two strings
[u, v] which are the letters not yet used in each tape, then:

1. We force that |u| ≥ |v|. If it is not true, we just switch the two tapes and the
behavior of the automaton is the same. This is only valid because the contents of
the tape are random and the letters uniformly distributed.
2. We force that u < v lexicographically on their ﬁrst |v| letters (note that due to
the previous rule, |u| ≥ |v|). We do that by exchanging letters. If u[1] is not 0,
we exchange in u and v all the occurrences of u[1] with 0 and vice versa. The
same thing can be done with v. If v[1] > 1, then we exchange in u and v all the
occurrences of v[1] with 1 and vice versa. This is valid because the letters are
indistinguishable and uniformly distributed.

These two rules diminish approximately by a factor of 2k2 the possible number of states
that a machine like this can generate, by using classes of equivalence between states.
Rule 2 can be extended recursively to u[2], by permuting u[2] with 2 if u[2] > 2, etc. We
have done that for larger k, up to k − 1 letters, reducing for every exchange the number
of states by a factor of k. This symmetry is used in a similar way in [Da1].

Formally, our CSS machine is a tuple (S, δ, O) where S is a set of states, δ is
the transition function which, given a state s and a pair of letters, gives the new state
(cid:48) ← δ(s, [x, y])), and O is the output function which, given a state s and a pair of
(s
letters [x, y], returns the length of the chosen CS for that transition (this is explained
later). The expected behavior of a CSS machine can be modeled by a strongly connected
Markov chain (no absorbing states), where the probability of transition from one state to
another state is the probability of the input letter pair associated to that transition (1/k2).
In the limit, the probability of being in a given state converges to the solution of

T (cid:69)p = (cid:69)p,

pi = 1,

(cid:88)

i

(cid:88)

(cid:88)

where T is the probability transition matrix and (cid:69)p is the steady state probability vector
[CM]. After these probabilities are obtained, a lower bound on γk is given by the expected
length of the output

γk ≥

ps

s∈S

[x,y]∈×

O(s, [x, y])

k2

.

CSS machines can be produced automatically as shown in [Da1]. In our case we
have a different production algorithm. The idea is that given a CSS machine M(S, δ, O),
we select a subset of m states Um from S and we expand those states. Expanding a state s
means to concatenate all possible pairs of letters to s, obtaining k2 states. We normalize
each of those states by applying rules 1 and 2 deﬁned before. That is, all the transitions
be the set of new states.
of s go to these states. Of those, some of them are new. Let S
, we compute all the possible transitions as before, but we impose the
For each s
condition that the states generated by s will have at most the same number of letters of

(cid:48) ∈ S

(cid:48)

(cid:48)

Bounding the Expected Length of Longest Common Subsequences and Forests

441

Fig. 4. Production process.

(cid:48)

. If we have a larger number of letters, we drop one or two letters (we choose to delete
marking
have been
. All states that have been expanded plus

s
the letters with smaller frequency). If we produce new states, we add them to S
(cid:48)
(cid:48)
s
expanded, obtaining a new CSS machine M
the states of M, from M

as expanded. The condition above implies that at some point all states in S

(see Figure 4).

(cid:48)

(cid:48)

(cid:48)

We can repeat this process several times to obtain larger and larger CSS machines,

starting with the empty state [λ, λ], where λ denotes the empty string.

There are several possibilities to generate the next state in a transition. We tried
several ways to do it and the most successful one was the following. Given a state
s = [u, v], and a pair of letters [x, y], the next state is given by s
] such that
ux = w1u

(cid:48) = [u
where w1 and w2 are the strings that maximize

and vy = w2v(cid:48)

(cid:48), v(cid:48)

(cid:48)

(cid:96)cs(w1, w2)
|w1| + |w2|

if (cid:96)cs(u, v) > 0. If there is more than one candidate, we choose the one with smaller
|w1| + |w2|. Otherwise, if (cid:96)cs(u, v) = 0, we use w1 = u[1]. In this case, for w2, we use
w2 = λ if |u| > |v| or |v| = 0; otherwise w2 = v[1]. This can be seen as a heuristic that
locally maximizes γk by using the fewest possible number of letters. In practice, most
of the time the cut w1, w2 will happen on the “best” ﬁrst match from left to right. Note
that it may happen that u[1] = v[1] in opposition to [Da1] where they force the starting
letters to be different.
Figure 5 shows the basic CSS machine for general k for the LCS case when applying
the production algorithm once starting from the empty state and using m = 1. The output
function is shown between parentheses, if it is not null.
The transition probability matrix of this example is

 1/k

(k − 2)/k2
(k − 1)/k2

T =

((k − 1)2 − (k − 2))/k2

1 − 1/k
(k − 1)2/k2

0

(2k − 1)/k2

1/k



442

R. A. Baeza-Yates, R. Gavald`a, G. Navarro, and R. Scheihing

Fig. 5. CSS example for LCS.

and the steady state probabilities are

p0 = k2 − 1

,

p1 = k2(k − 1)

,

p2 = k(2k − 1)

,

D

D
with D = k3 + 2k2 − k − 1. For this automaton we have

D

γk ≥ p0
k

+ 3(k − 1) p1
For k = 2 we obtain γ2 ≥ 9

k2

+ (2k − 1) p2
k2
≈ 0.6923.

13

= 3k2 − k − 1
k3 + 2k2 − k − 1

+ O(k

−2).

= 3
k

In the production algorithm we have left open the question of how to select Um.
Here, the number of states m to be expanded and the selection procedure is not ﬁxed. In
[Da1] a next state is selected by “looking ahead” on the random input and choosing the
transition where on average a longer CS is lost. Although this might be the best selection
procedure, looking ahead can be computationally very expensive. They do it only for
k = 2 using the average of all possible strings of length 6. This is not practical for k > 2
as the number of look ahead strings grows very fast. For that reason, we tried different
heuristic cost functions associated with a state s. The one that gave the best results was
to expand the states with the largest expected output, that is,

(cid:88)

Cost(s) = ps

O(s, [x, y]).

[x,y]∈×

So the selection procedure chooses the m states with largest Cost to obtain Um. For small
k we used m between 2 and 10 to speed up the growing rate of the CSS machine. For
larger k, m = 1 was enough, as the number of states grows exponentially.
The CSS machine for the LCF problem is given in Figure 6 for the case m = 1. We
can further improve this automaton by noticing that in states 0 and 2, the previous event
is always a match. So, if one of the new letters is equal to the previous match, we can
increase the LCF by 1. This has been considered in the output by adding the adequate

Bounding the Expected Length of Longest Common Subsequences and Forests

443

Fig. 6. CSS example for LCF.

terms which are a function of k. So we have the following transition matrix:

1/k

1/k
1/2

T =

((k − 1)2 − (k − 2))/k2

1 − 1/k
(k − 1)2/k2

0

(2k − 3)/k2
(k − 1)/k2



and we obtain

fk ≥ (k + 1) p0

+ (3k − 1)
k2
which for k = 2 gives f2 ≥ 1.

k2

( p1 + p2) = 3k2 − 3k + 2

k3

,

+

The generation algorithm described has been implemented using the Maple symbolic
]. Table 2 shows the lower bounds obtained so far by using our
algebra system [CGG
CSS machines up to 2000 states for the LCS and LCF problem. We were not able to
obtain values for k > 6 due to the combinatorial explosion of the number of states.
Figures 7 and 9 show these results compared with our upper bounds and experimental
results.

Table 2. New lower bounds for LCS and LCF (new results in boldface), and

experimental results for n = 100,000.

Our γk

lower bound

Previous γk
[De1], [PD]

0.75875
0.63376
0.55282
0.50952
0.46695

0.77391
0.61538
0.54545
0.50615
0.47169

γk

(exper.)

0.8118
0.7172
0.6537
0.6069
0.5701

fk (new)

lower bound

1.41031
1.03554
0.83356
0.67948
0.56400

fk

(exper.)

1.4998
1.2969
1.1426
1.0281
0.9403

k

2
3
4
5
6

444

R. A. Baeza-Yates, R. Gavald`a, G. Navarro, and R. Scheihing

Fig. 7. The bold solid lines are our lower and upper bounds on γk for each alphabet size k. They are compared
with the results of Chv´atal and Sankoff [CS1] (bold dashed line is the lower bound, the upper bound matches
ours for k > 2), Deken [De1] (dashed line), and Dan˘c´ık [Da1] (solid line is the upper bound, and the bullet
for k = 2 is the lower bound). In between we show experimental results for n = 100,000 (dotted line).

4. Upper Bounds: Kolmogorov Complexity

The original goal of Kolmogorov complexity was to have a quantitative measure of
the complexity of a ﬁnite object. Kolmogorov and others had the following idea: the
regularities of an object can be used to give short descriptions of it; on the other hand, if
an object is highly nonregular, or random, there should be no way of describing it that is
much shorter than giving the full object itself. To formalize this notion, we ﬁrst encode
discrete objects as strings, as is customary in the theory of computation. Second, we want
to have descriptions that can be handled algorithmically, so we identify descriptions with
“programs for a sufﬁciently powerful model of computation.”
Fix a Universal Turing Machine U whose input alphabet is{0, 1} and output alphabet
is . The Kolmogorov complexity of a string s ∈ (cid:63) is the minimum length of a program
that makes U generate s and stops.

Observe that this deﬁnition seems to depend on the choice of the Universal Turing
Machine. However, it can be shown that changing the machine only affects this measure
of complexity by an additive constant.

Komogorov random. These are strings that cannot be compressed algorithmically.

Strings whose Kolmogorov complexity are equal, or close, to their lengths are called
As there are at most 2n − 1 binary “programs” of length n − 1 or less, clearly
there is some string of length n whose Kolmogorov complexity is at least n. A slight
generalization of this counting argument gives that, for every c and n, there are at most
2n−c+1 − 1 strings in n having Kolmogorov complexity ≤ n − c.

of 2

For c even a small constant, this amounts to say that most strings, all but a fraction
−c, are almost random: they cannot be compressed by more than c bits.
Many combinatorial properties have simple proofs via this prepackaged counting
argument. Suppose that we want to show that property P(u) holds for some string u.
Take a Kolmogorov-random string u. Assume that P(u) is false, and show that this gives
a way to describe u concisely. This is a contradiction. In fact, this argument usually gives

Bounding the Expected Length of Longest Common Subsequences and Forests

445

a proof that P(u) holds with high probability, as the majority of strings are Kolmogorov
random up to small constants.
For example, P(u) could be some static property of u, such as “the difference between 
zeros and ones in u is at most 2 log|u|”;1 or a dynamic property such as “algorithm
A takes time at most 5|u| on input u.” In fact, several lower bounds on the (worst-case and
expected) running time of algorithms have been proved using Kolmogorov complexity
[LV].

To apply this kind of argument to the case of LCS, observe that if two n-bit strings
have a very long LCS (that is, close to n bits), then these two strings are in some sense very
similar: knowing one of them gives away a lot of information about the other. Intuitively,
if two strings are mutually random, knowing one of them should give essentially zero
information to build the other. This must be true, in particular, if the two strings are
obtained by chopping a Kolmogorov-random string of 2n bits into two n-bit pieces. This
argument is given in [LV, Exercise 6.7.1, p. 420], where it is credited to Tao Jiang, though
in fact it is only done for k = 2.

identify strings of length n over k letters with binary strings of length n log k.

We formalize this argument for general alphabets : just bear in mind that we can
We will determine γ such that (cid:96)cs(u, v) ≤ γ n for Kolmogorov-random strings
≤ γkn + O(1/n). Indeed,
u and v. Then averaging over all strings we obtain EL(k)
let A be the set of words uv (u, v ∈ n) that have Kolmogorov complexity at least
n
(2n− 3 log n)· log k. See that all but a fraction O(1/n3) of the strings have this property.
Then

(cid:35)

(cid:96)cs(uv)

= 1/k2n

EL(k)
n

(cid:34)(cid:88)
(cid:34)(cid:88)

uv∈A

(cid:88)
(cid:35)

uv /∈A

(cid:96)cs(uv) +
(cid:88)

γ n +

≤ 1/k2n
≤ 1/k2n[k2n(1 − O(1/n3))γ n + k2n O(1/n3)n]
= (1 + O(1/n3))γ n.

uv /∈A

uv∈A

n

Assume (cid:96)cs(u, v) = γ n. Clearly we can obtain uv if we have the following information:

• The values of n and γ n.
• The LCS of u and v.
• A description of the letter positions of u and v that give the LCS.
• The sequence of letters of u that do not belong to the LCS.
• The sequence of letters of v that do not belong to the LCS.
Formally, there is a ﬁxed program (independent of n, u, and v) that, given this
information, makes the Universal Turing Machine produce uv. As uv is random, the
length of writing down this information in bits, plus the size of this program, must be at
least (2n − 3 log n) log k. Let us estimate the bit-length of each part.

The values of n and γ n can be given in 2 log n bits each. By assumption, the LCS
can be encoded in (γ n) log k bits. The bits necessary to specify the letter positions is

1 All logarithms in this paper are in base 2.

446

R. A. Baeza-Yates, R. Gavald`a, G. Navarro, and R. Scheihing

the log of the number of position sets that correspond to LCSs of two strings. Call this
number In,γ .

For the last item, we use the following. A pair of strings may have several LCSs.
We take as a representative that one with a lexicographically smallest set of positions:
that is, if there are two choices for matching a letter we match it with the lowest index.
Then, for every letter not in the LCS, we can discard one out of k possibilities: if adjacent
letters from positions i to j of u are not in the LCS, but letter j + 1 is, we know that
u[k] (cid:54)= u[ j + 1], for any i ≤ k ≤ j. Hence, the (1 − γ )n letters of u not in the LCS can
be encoded given as a string of length (1 − γ )n over an alphabet with k − 1 letters, and
similarly for v. In particular, for k = 2, this information is empty.

Adding up, we obtain the equation
4 log n + γkn log k + log In,γk

+ 2(1 − γk )n log(k − 1) ≥ (2n − 3 log n) log k.
Dividing the equation by n, all sublinear terms vanish asymptotically, so we obtain

+ 2(1 − γk ) log(k − 1) ≥ (2 − γk ) log k.

(1)
(cid:161)
A ﬁrst upper bound on In,γk is the number of all subsets of {1··· n} with γkn elements,
squared (once for choosing in u, times the choice for v). By Stirling’s approximation,
log
binary entropy function. So we obtain the equation

(cid:162) = n H (γk )(1 + o(1)), where H (x) = −x log(x) − (1 − x) log(1 − x) is the

log In,γk

n

n
γk n

2H (γk ) + 2(1 − γk ) log(k − 1) ≥ (2 − γk ) log k.

For every k, solving this equation numerically gives a feasible range for γk. For example,
for k = 2 it gives 0.282 ≤ γ2 ≤ 0.867. Figure 7 plots the values of γk up to k = 18,
as well as experimental results for n = 100,000 (average taken over ten trials). Table 3
gives some exact values. By taking the limit on k, we obtain the already known result
γk ≤ e/
For k = 2 this is the result obtained by Tao Jiang using a technique from [JL] as
mentioned in [LV]. Although this result is also equivalent to the note added in proof

√
k.

Table 3. Upper bounds for LCS and LCF (new results

in boldface).

Our γk

upper bound

Previous γk
[DP], [Da1]

fk (new)

upper bound

0.86019
0.78647
0.72971
0.68612
0.65098
0.62172
0.59676
0.57507
0.55597
0.48538

0.83763
0.76581
0.70824
0.66443
0.62932
0.60019
0.57541
0.55394
0.53486
0.46462

2.00000
1.76704
1.56594
1.41289
1.29384
1.19855
1.12033
1.05478
0.99890
0.80753

k

2
3
4
5
6
7
8
9
10
15

Bounding the Expected Length of Longest Common Subsequences and Forests

447

Fig. 8. Forbidden case for an LCS with k = 2 (left), and counting variables used (right) .

in [CS1] (which is explained in [CS2]), using Kolmogorov complexity is much easier
to understand and simpler. We obtain a better bound for k = 2 by estimating more
accurately the number of positions In,γk .
Consider the example given in Figure 8. If the letters u[i + 1] and v[i + 1] are equal,
we can match them and obtain a longer common sequence. If they are different, one of
them equals u[i + 2] = v[i + 2], so we can match it with either u[i + 2] or v[i + 2] and
obtain a lexicographically smaller set of positions. So we have to count sets of positions
that do not leave gaps simultaneously on both strings.

As we will take the log of the number of strings divided by n for large n, we disregard
smaller terms such as leading polynomials, etc., without further notice. In particular, we
count only those strings that end with a match; it is not hard to see that this does not
affect the main term.

To count the number of LCS alignments, we use generating functions. Let G(x, y, z)

be

G(x, y, z) =

Gn1,n2,(cid:96)x n1 yn2 z(cid:96),

(cid:88)

n1,n2,(cid:96)

where Gn1,n2,(cid:96) is the number of alignments such that the upper string is of length n1, the
lower string is of length n2 and the LCS is of length (cid:96). That is, in the symbolic variable x
we accumulate the number of skipped letters between two consecutive positions of the
upper string which participate in the alignment. We also count the aligned letter, so at
the end we have in x the length of the upper string. We do the same for the lower string
on the variable y. Finally, z counts the number of edges in the alignment between both
strings. The counting model is depicted in Figure 8. So we are interested in Gn,n,nγ .

When using generating functions to describe combinatorial objects and count their
properties, multiplication represents object composition and addition represents the
union of different objects. In our case we have

(cid:181)

(cid:182)

G(x, y, z) =
=

yxzG(x, y, z) + 1

+ x
1 − x
1

1
1 − y
1 − (1/(1 − y) + x/(1 − x))x yz

.

This formula counts all the forms to build all the possible alignments, as follows: either
the alignment has an edge or not. The ﬁrst case is the main part of the formula: we put
zero or more initial letters either in y or in x (as we explained, there cannot be gaps in

(cid:80)
i≥0 x i +(cid:80)

448

R. A. Baeza-Yates, R. Gavald`a, G. Navarro, and R. Scheihing

(cid:181)

We now extract the coefﬁcients Gn1,n2,(cid:96) from the function G(x, y, z). We begin by

j≥0 y j = 1/(1− x)+ 1/(1− y). However, we subtract 1,
both strings), i.e.,
since otherwise the case of both empty strings would be counted twice (hence x/(1− x)
instead of 1/(1 − x)). After this, we have an edge x yz (to count the aligned letters in
x and in y, and the edge z). After this, we add more letters and edges by recursively
concatenating more elements of the same form, that is, G(x, y, z). The recursion ends
when we determine that no more edges are to be added. This corresponds to the leading
+1 of the formula (i.e., x 0 y0z0). This is because, as explained, we assume that the last
letters are aligned and hence if there are no alignments the strings must be empty.

extracting (cid:96). Since 1/(1 − az) =(cid:80)
+ x
(cid:161)
and, since xr /(1 − x)s+1 = (cid:80)
1 − x
x i for s ≥ 0 (and the same property is used
i−r+s
(cid:182)(cid:181)
n2 − i − 1
(cid:96) − i − 1
(cid:182)

where we have discarded some clumsy border conditions which do not affect the main
term. Finally,
Gn,n,(cid:96) =

i≥0
n1 − (cid:96) − 1
(cid:182)(cid:181)
n − i − 1
(cid:96) − i − 1

G (cid:96)(x, y) = (x y)(cid:96)
(cid:181)
(cid:88)

Gn1,n2,(cid:96) =
(cid:88)

1
1 − y
(cid:182)(cid:181)

(cid:182)(cid:96) =
(cid:162)

(1 − x)i (1 − y)(cid:96)−i

n − (cid:96) − 1
i − 1

(cid:96)≥0 a(cid:96)z(cid:96) we have

(cid:181)

(cid:88)

(cid:181)

(cid:182)(cid:181)

x i+(cid:96) y(cid:96)

(cid:96)
i

(cid:182)

,

i − 1

for y),

(cid:182)

s

(cid:96)
i

i

i

(cid:96)
i

i

We do not need the exact solution to the above sum, just its logarithm divided by n,

for large n. Call Mn,(cid:96) the maximum term of the summation. Then we have

.

(cid:181)

(cid:182)

Mn,(cid:96) ≤ Gn,n,(cid:96) ≤ (cid:96)Mn,(cid:96)

log Mn,(cid:96)

n

≤ log Gn,n,(cid:96)

n

≤ log Mn,(cid:96)

n

+ O

log n

n

(cid:112)

which shows that the larger term dominates the result. Moreover, we can maximize the
logarithm of the term and use Stirling as before. Let i = wn, take the logarithm of the
term i of the sum, divide by n, and maximize with respect to w. We obtain that the
maximum is reached for

5γ 2 − 8γ + 4
2

w(γ ) = 2 − γ −
(cid:162) = αn H (β/α) + O(log n), we have
(cid:161)
that satisﬁes the constraints of the sum, namely, 0 ≤ w(γ ) ≤ min(γ , 1 − γ ). By
(cid:182)
(cid:181)
using this maximum term instead of the whole sum, and using the asymptotic formula
log

(cid:181)

(cid:181)

(cid:182)

(cid:182)

αn
βn

γ H

w(γ )

γ

+ (1 − γ )H

w(γ )

1 − γ

+ (1 − w(γ ))H

γ − w(γ )
1 − w(γ )

≥ 2 − γ

whose numerical solution is

γ2 ≤ 0.86019,

Bounding the Expected Length of Longest Common Subsequences and Forests

449

which is still larger than what other more complicated theoretical models provide [Da1],
although quite close.

We now consider the LCF problem. The LCF allows a better letter representation,
since in this case not only each unconnected letter must be different than that of the next
alignment, but the letters corresponding to each tree of the forest must be different than
that of the next tree (otherwise we could join both trees). Hence, we need log(k − 1) bits
for all letters (connected and unconnected), except the ﬁrst one. For example, we need
only one bit for k = 2. Therefore, our inequality is

log In, fk

n

+ (2 − fk ) log(k − 1) ≥ 2 log k.
(cid:182)
(cid:181)

The next step is to obtain a bound for In, fk , the number of conﬁgurations for the
forest. In this case, a single letter can be matched to many, so we drop the requirement
for at least one gap between two edges. However, not both gaps can be zero. Hence,

(2)

(cid:96)
i

i

.

G(x, y, z) =
=

1

− 1

zG(x, y, z) + 1

(1 − x)(1 − y)
1
1 − (1/((1 − x)(1 − y)) − 1)

z,

where the base case (leading +1) is as before. The recursive case considers a number
of letters in the upper and lower strings (this time there can be letters in both of them
simultaneously). The z represents the edge, and this time it is not coupled with x y because
there may be many edges for a single letter. However, before putting a new edge we must
add at least one letter in the upper or lower strings (otherwise we are putting two edges
coupling the same pair of letters), and hence we subtract 1 = x 0 y0 to the formula in
parentheses. The recursive step multiplies G(x, y, z) at the end.
Computing the (cid:96)th coefﬁcient as before, rewriting 1/((1 − x)(1 − y)) − 1 = (x +
y − x y)/((1 − x)(1 − y)), and using the binomial expansion we have
G (cid:96)(x, y) = (x + y − x y)(cid:96)
(cid:181)
(cid:182)(cid:181)
(cid:88)
((1 − x)(1 − y))(cid:96)
(−1)(cid:96)−i− j
(cid:182)(cid:181)
(cid:181)
(cid:88)
(cid:182)(cid:181)
(cid:181)
(−1)(cid:96)−i− j
(cid:88)
(cid:181)
(cid:182)(cid:181)
(cid:88)
(−1)(cid:96)+i
(cid:181)
(cid:182)(cid:181)
(cid:88)

(cid:182)
(cid:96) − i
x i y j (x y)(cid:96)−i− j
((1 − x)(1 − y))(cid:96)
j
(cid:182)(cid:181)
(cid:182)(cid:181)
Now we use the same properties as before to invert in x and y and get
n1 + j − 1
n2 + i − 1
(cid:182)(cid:181)
(cid:181)
(cid:182)(cid:88)
(cid:96) − 1
(cid:96) − i
n1 + j − 1
(cid:182)
(cid:182)
(cid:181)
(−1) j
j
n2 + i − 1
n1 − 1
(cid:96) − 1
i − 1
(cid:182)(cid:181)
(cid:182)
n − 1
n + i − 1
i − 1
(cid:96) − 1

where we disregard again border conditions which do not affect the main term. Hence,

n2 + i − 1

Gn1,n2,(cid:96) =

Gn,n,(cid:96) =

(cid:96) − i
j

j

(−1)(cid:96)−i

(−1)(cid:96)+i

i, j

i

i

=

i, j

(cid:96) − 1

(cid:96)
i

(cid:96)
i

(cid:96) − 1

(cid:96) − 1

=

=

.

,

(cid:96)
i

(cid:96)
i

(cid:182)

(cid:182)

450

R. A. Baeza-Yates, R. Gavald`a, G. Navarro, and R. Scheihing

Fig. 9. The bold solid lines show the upper and lower bounds for fk, for each alphabet size k. In between
we show experimental results for n = 100,000 (dotted line).

1 + 4 f
2

.

Using the same maximizing technique as before (i = wn), we have
w( f ) = −1 + √
(cid:181)
(cid:182)
(cid:181)
w( f )
≤ 2 log k − (2 − f ) log(k − 1).

This maximum value for i = w( f )n is always in the bounds of the summation (i.e.,
(cid:182)
max( f − 1, 0) ≤ w( f ) ≤ min( f, 1)). Then we have

+ (1 + w( f ))H

f

1 + w( f )

+ H (w( f ))

f H

f

We can now numerically solve this inequality for each alphabet size k. Figure 9
plots the values of fk up to k = 18 as well as experimental results for n = 100,000
(average taken over ten trials), and Table 3 shows some exact values. These are the ﬁrst
theoretical upper bounds for the LCF problem. Taking the limit on k, we obtain

(cid:181)

(cid:182)

1
k

.

fk ≤ e√
k

+ O

5. Final Remarks

Algorithms to ﬁnd a long CS of two sequences can be considered as approximation
algorithms for this problem (they can also be seen as on-line algorithms with restricted
memory). The complexity of these algorithms is in general O(n), which compares favorably 
with O(n2).

The general case of ﬁnding the LCS of m sequences of length n can be solved in
time O(n(cid:96)) using dynamic programming. If (cid:96) is not ﬁxed, this problem is NP-complete
[Ma]. Jiang and Li [JL] show that it is difﬁcult to ﬁnd good approximation algorithms
in the worst case for the LCS, because if there is a polynomial time approximation
algorithm with performance ratio nδ (δ > 0), then P = NP. For that reason, it is better
to look at good approximation algorithms for random inputs. For the case of (cid:96) sequences

Bounding the Expected Length of Longest Common Subsequences and Forests
√
of length n, with (cid:96) a polynomial in n, simple greedy algorithms approximate the LCS
n1+ε) [JL]. The expected
of the sequences with an expected additive error of size O(
length of the sequence in this case is n/k for an alphabet of size k. That is, γk → 1/k
√
when (cid:96) = O(n). Note that, for (cid:96) = 2, γk = O(1/
k). Dan˘c´ık [Da1], [Da2] has proved
that in the case of (cid:96) sequences

451

1 ≤ γkk1−1/(cid:96) ≤ e.
The approximation ratio of our algorithms for two sequences in the worst case is
unbounded. On average, CSS machines are approximation algorithms with expected
additive error at most O(n/k), but the exact complexity is not known. One possible
measure is the ratio between the exact value of γk and the expected length of the CS
obtained by the algorithm. In our case, because the exact value is not known, we can use
an upper bound. For example, for k = 2 we have γ2 < 0.838 [DP]. So the automaton
given for the case m = 1 would be at most 1.22 suboptimal for random sequences. For
larger k, the ratio is at most e/3

k for the case m = 1.

√

We are also working on upper bounds, reﬁning the codiﬁcation methods to improve
the Kolmogorov bounds. For example, one can take pairs of letters and observe that
some conﬁgurations are in fact not possible, thus reducing the number of alternatives to
represent and hence improving the bound. However, the analysis becomes much more
complex.

Finally, we would like to say a few words about the experimental results. We tried to
ﬁt several curves using least squares on the last ten values (considering that we want to
√
obtain the asymptotic behavior), to see the real dependency on k of the expected value.
Sankoff and Mainville [SM] have conjectured that γk ≈ 2/
k. Fitting the curve a/kβ
we got a = 1.15 and β = 0.38 for γk and a = 2.42 and β = 0.52 for fk with relative
√
k + b/k we
errors of 0.08% and 0.03%, respectively. Doing the same for the curve a/
obtained a = 1.90 and b = −1.27 for γk and a = 2.19 and b = 0.35 for fk, both with
a relative error of 0.05%. This supports the same conjecture for the case of the LCF,
although gives no deﬁnitive answer for the case of the LCS.

Acknowledgments

Some ideas for this work originated while the second author was visiting the University of Chile in Santiago
during 1995 and attending the XV Conference of the Chilean Computer Science Society (SCCC) in Arica. He
is grateful to Eric Goles and Mart´ın Matamala for inviting him to the ﬁrst, and to the SCCC and particularly
Ricardo Baeza-Yates for inviting him to the second. This work continued thanks to the kind invitation of Josep
D´ıaz to the ﬁrst author to do a sabbatical at the Technical University of Barcelona and to the third author to
visit the same place during February 1996. Finally, we thank the referees for their helpful comments.

References

[BYGN] R. Baeza-Yates, R. Gavald´a, and G. Navarro. Bounding the expected length of longest common
subsequences and forests. In R. Baeza-Yates, N. Ziviani, and K. Guimar˜aes, editors, Proc. WSP
’96, pages 1–15, Recife, Brazil, August 1996.

[BYS] R. Baeza-Yates and R. Scheihing. New lower bounds for the expected length of longest common
subsequences and forests. In Proc. XV International Conference of the Chilean Computer Science
Society, pages 48–58, Arica, Chile, November 1995.

452

+

[CGG

R. A. Baeza-Yates, R. Gavald`a, G. Navarro, and R. Scheihing

] B. Char, G. Geddes, G. Gonnet, B. Leong, M. Monagan, and S. Watt. MAPLE V Language and

Library Reference Manual. Springer-Verlag, New York, 1991.

[CM] D. Cox and H. Miller. The Theory of Stochastic Processes. Chapman and Hall, London, 1965.
[CS1] V. Chv´atal and D. Sankoff. Longest common subsequences of two random sequences. Journal of

Applied Probability, 12:306–315, 1975.

[CS2] V. Chv´atal and D. Sankoff. An upper-bound technique for lengths of common subsequences. In
D. Sankoff and J. Kruskal, editors, Time Warps, String Edits, and Macromolecules: The Theory
and Practice of Sequence Comparison, pages 353–357. Addison-Wesley, Reading, MA, 1983.

[Da1] V. Dan˘c´ık. Expected Length of Longest Common Subsequences. Ph.D. Thesis, CS Dept., Univ. of

Warwick, Warwick, England, 1994.

[De1]

[Da2] V. Dan˘c´ık. Common subsequences and supersequences and their expected length. In Z. Galil and
E. Ukkonen, editors, Proc. 6th Annual Symposium on Combinatorial Pattern Matching, pages 55–
63, Espoo, Finland, July 1995. LNCS 937, Springer-Verlag, Berlin, 1995.
J. Deken. Some limit results for longest common subsequences. Discrete Mathematics, 26:17–31,
1979.
J. Deken. Probabilistic behavior of longest-common subsequence length. In D. Sankoff and
J. Kruskal, editors, Time Warps, String Edits, and Macromolecules: The Theory and Practice
of Sequence Comparison, pages 359–362. Addison-Wesley, Reading, MA, 1983.

[De2]

[DP] V. Dan˘c´ık and M. Paterson. Upper bounds for the expected length of a longest common subsequence

of two binary sequences. Random Structures & Algorithms, 6:449–458, 1995.

[GBY] G.H. Gonnet and R. Baeza-Yates. Handbook of Algorithms and Data Structures — In Pascal and

C, second edition. Addison-Wesley, Workingham, England, 1991.

[JL] T. Jiang and M. Li. On the approximation of shortest common supersequence and longest common

subsequences. SIAM Journal on Computing, 24(5):1112–1139, Oct 1995.

[LV] M. Li and P. Vit´anyi. An Introduction to Kolomogorov Complexity and Its Applications, second

edition. Springer-Verlag, New York, 1997.

[Ma] D. Maier. The complexity of some problems on subsequences and supersequences. Journal of the

ACM, 25:322–336, 1978.

[PD] M. Paterson and V. Dan˘c´ık. Longest common subsequences. In B. Rovan, I. Privara, and P. Ruzicka,
editors, Proc. 19th MFCS ’94, pages 127–142, Kosice, Slovakia, August 1994. LNCS 841, Springer
Verlag, Berlin, 1994.

[PW] P. Pevzner and M. Waterman. Generalized sequence alignment and duality. Advances in Applied

Mathematics, 14:139–171, 1993.

[Ri] C. Rick. A new ﬂexible algorithm for the longest common subsequence problem. In Proc. 6th
Annual Symposium on Combinatorial Pattern Matching, pages 340–351, Espoo, Finland, 1995.
LNCS 937, Springer-Verlag, Berlin, 1995.

[SM] D. Sankoff and S. Mainville. Common subsequences and monotone susequences. In D. Sankoff
and J. Kruskal, editors, Time Warps, String Edits, and Macromolecules: The Theory and Practice
of Sequence Comparison, pages 363–365. Addison-Wesley, Reading, MA, 1983.

Received November 1996, and in ﬁnal form October 1998.

