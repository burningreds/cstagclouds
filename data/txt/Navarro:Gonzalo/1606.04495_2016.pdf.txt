6
1
0
2

 

n
u
J
 

4
1

 
 
]
S
D
.
s
c
[
 
 

1
v
5
9
4
4
0

.

6
0
6
1
:
v
i
X
r
a

A

Range Majorities and Minorities in Arrays

DJAMAL BELAZZOUGUI, CERIST, Algeria
TRAVIS GAGIE, University of Helsinki, Finland
J. IAN MUNRO, University of Waterloo, Canada
GONZALO NAVARRO, University of Chile
YAKOV NEKRICH, University of Waterloo, Canada

Karpinski and Nekrich (2008) introduced the problem of parameterized range majority, which asks us to
preprocess a string of length n such that, given the endpoints of a range, one can quickly ﬁnd all the distinct 
elements whose relative frequencies in that range are more than a threshold τ . Subsequent authors
have reduced their time and space bounds such that, when τ is ﬁxed at preprocessing time, we need either
O(n lg(1/τ )) space and optimal O(1/τ ) query time or linear space and O((1/τ ) lg lg σ) query time, where σ
is the alphabet size. In this paper we give the ﬁrst linear-space solution with optimal O(1/τ ) query time,
even with variable τ (i.e., speciﬁed with the query). For the case when σ is polynomial on the computer word
size, our space is optimally compressed according to the symbol frequencies in the string. Otherwise, either
the compressed space is increased by an arbitrarily small constant factor or the time rises to any function
in (1/τ ) · ω(1). We obtain the same results on the complementary problem of parameterized range minority
introduced by Chan et al. (2015), who had achieved linear space and O(1/τ ) query time with variable τ .

Categories and Subject Descriptors: E.1 [Data structures]; E.4 [Coding and information theory]: Data
compaction and compression

General Terms: Arrays, Range Queries

Additional Key Words and Phrases: Compressed data structures, range majority and minority

1. INTRODUCTION
Finding frequent elements in a dataset is a fundamental operation in data mining.
The most frequent elements can be difﬁcult to spot when all the elements have nearly
equal frequencies. In some cases, however, we are interested in the most frequent elements 
only if they really are frequent. For example, Misra & Gries [1982] showed
how, given a string and a threshold 0 < τ ≤ 1, by scanning the string twice and
using O(1/τ ) space we can ﬁnd all the distinct elements whose relative frequencies
exceed τ . These elements are called the τ -majorities of the string. If the element universe 
is [1..σ], their algorithm can run in linear time and O(σ) space. Demaine et al.
[2002] rediscovered the algorithm and deamortized the cost per element; Karp et al.
[2003] rediscovered it again, obtaining O(1/τ ) space and linear randomized time. As

Funded in part by Academy of Finland grant 268324 and Millennium Nucleus Information and Coordination
in Networks ICM/FIC RC130003 (Chile). An early partial version of this article appeared in Proc. WADS
2013.
Authors’ addresses: D. Belazzougui, Research Center on Technical and Scientiﬁc Information, Algeria; T.
Gagie, Department of Computer Science, University of Helsinki; I. Munro and Y. Nekrich, David Cheriton
School of Computer Science, University of Waterloo, Canada; G. Navarro, Department of Computer Science,
University of Chile.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that
copies show this notice on the ﬁrst page or initial screen of a display along with the full citation. Copyrights
for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted.
 To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component
of this work in other works requires prior speciﬁc permission and/or a fee. Permissions may be requested
from Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212)
869-0481, or permissions@acm.org.
c(cid:13) YYYY ACM 1549-6325/YYYY/01-ARTA $15.00
DOI:http://dx.doi.org/10.1145/0000000.0000000

ACM Transactions on Algorithms, Vol. V, No. N, Article A, Publication date: January YYYY.

A:2

D. Belazzougui et al.

Cormode & Muthukrishnan [2003] put it, “papers on frequent items are a frequent
item!”.

Krizanc et al. [2005] introduced the problem of preprocessing the string such that
later, given the endpoints of a range, we can quickly return the mode of that range

(i.e., the most frequent element). They gave two solutions, one of which takes O(cid:0)n2−2ǫ(cid:1)
space for any ﬁxed positive ǫ ≤ 1/2, and answers queries in O(nǫ lg lg n) time; the other
takes O(cid:0)n2 lg lg n/ lg n(cid:1) space and answers queries in O(1) time. Petersen [2008] reduced 
Krizanc et al.’s ﬁrst time bound to O(nǫ) for any ﬁxed non-negative ǫ < 1/2, and
Petersen & Grabowski [2009] reduced the second space bound to O(cid:0)n2 lg lg n/ lg2 n(cid:1).
Chan et al. [2014] gave an O(n) space solution that answers queries in O(cid:16)pn/ lg n(cid:17)
time. They also gave evidence suggesting we cannot easily achieve query time substantially 
smaller than √n using linear space; however, the best known lower bound
[Greve et al. 2010] says only that we cannot achieve query time o(lg(n)/ lg(sw/n))
using s words of w bits each. Because of the difﬁculty of supporting range mode
queries, Bose et al. [2005] and Greve et al. [2010] considered the problem of approximate 
range mode, for which we are asked to return an element whose frequency is at
least a constant fraction of the mode’s frequency.

Karpinski & Nekrich [2008] took a different direction, analogous to Misra & Gries’
approach, when they introduced the problem of preprocessing the string such that
later, given the endpoints of a range, we can quickly return the τ -majorities of that
range. We refer to this problem as parameterized range majority. Assuming τ is ﬁxed
when we are preprocessing the string, they showed how we can store the string in

O(n(1/τ )) space and answer queries in O(cid:0)(1/τ )(lg lg n)2(cid:1) time. They also gave bounds
for dynamic and higher-dimensional versions. Durocher et al.
[2013] independently
posed the same problem and showed how we can store the string in O(n lg(1/τ + 1))
space and answer queries in O(1/τ ) time. Notice that, because there can be up to 1/τ
distinct elements to return, this time bound is worst-case optimal. Gagie et al. [2011]
showed how to store the string in compressed space — i.e., O(n(H + 1)) bits, where
H is the entropy of the distribution of elements in the string — such that we can answer 
queries in O((1/τ ) lg lg n) time. Note that H ≤ lg σ, thus O(n(H + 1)) bits is O(n)
space. They also showed how to handle a variable τ and still achieve optimal query
time, at the cost of increasing the space bound by a (lg n)-factor. That is, they gave a
data structure that stores the string in O(n(H + 1)) words such that later, given the
endpoints of a range and τ , we can return the τ -majorities of that range in O(1/τ ) time.
Chan et al. [2015] gave another solution for variable τ , which also has O(1/τ) query
time and uses O(n lg n) space. As far as we know, these are all the relevant bounds for
Karpinski & Nekrich’s original problem, both for ﬁxed and variable τ ; they are summarized 
in Table I together with our new results. Related work includes dynamic structures 
[Elmasry et al. 2011], approximate solutions [Lai et al. 2008; Wei & Yi 2011],
and encodings that do not access the string at query time [Navarro & Thankachan
2016].

In this paper we ﬁrst consider the complementary problem of parameterized range
minority, which was introduced by Chan et al. [2015] (and then generalized to trees
by Durocher et al.
[2016]). For this problem we are asked to preprocess the string
such that later, given the endpoints of a range, we can return (if one exists) a distinct
element that occurs in that range but is not one of its τ -majorities. Such an element
is called a τ -minority for the range. At ﬁrst, ﬁnding a τ -minority might seem harder
than ﬁnding a τ -majority because, for example, we are less likely to ﬁnd a τ -minority
by sampling. Nevertheless, Chan et al. gave an O(n) space solution with O(1/τ) query
time even for variable τ . In Section 3 we give two warm-up results, also for variable τ :

ACM Transactions on Algorithms, Vol. V, No. N, Article A, Publication date: January YYYY.

Range Majorities and Minorities in Arrays

A:3

Table I. Results for the problem of parameterized range majority on a string of length n over an
alphabet [1..σ] with σ ≤ n in which the distribution of the elements has entropy H ≤ lg σ. Note
that all the spaces given in bits are in O(n) words.
space

source

time

[Karpinski & Nekrich 2008]

O(n(1/τ )) words

[Durocher et al. 2013]

O(n lg(1/τ )) words

O(cid:0)(1/τ )(lg lg n)2(cid:1)

O(1/τ )

O(n(H + 1)) bits

O((1/τ ) lg lg σ)

[Gagie et al. 2011]

[Gagie et al. 2011]

[Chan et al. 2015]

τ is

ﬁxed

ﬁxed

ﬁxed

variable

variable

variable

variable

variable

O(n(H + 1)) words

O(n lg n) words

O(1/τ )

O(1/τ )

O(1/τ )

Theorem 4.5 (lg σ = O(lg w))

nH + o(n) bits

Theorem 5.2

Theorem 5.3

nH + o(n)(H + 1) bits

Any (1/τ ) · ω(1)

(1 + ǫ)nH + o(n) bits

O(1/τ )

(a) O(1/τ) query time using (1 + ǫ)nH + O(n) bits of space for any constant ǫ > 0; and
(b) any query time of the form (1/τ ) · ω(1) using nH + o(n)(H + 1) bits of space.

That is, our spaces are not only linear like Chan et al. ’s, but also compressed: we use
either nearly optimally compressed space with no slowdown, or optimally compressed
space with nearly no slowdown. Within this space, we can access S in constant time.
We reuse ideas from this section in our solutions for parameterized range majority.

In Section 4 we return to Karpinski & Nekrich’s original problem of parameterized
range majority, but with variable τ , and consider the case where the alphabet of the
string is polynomial on the computer word size, lg σ = O(lg w). We start with a simple
linear-space structure (i.e., O(n lg σ) bits, which is stricter than other linear-space solutions 
using O(n lg n) bits) with worst-case optimal O(1/τ ) query time, and then reﬁne it
to obtain optimally compressed space. In Section 5 we extend this solution to the more
challenging case of larger alphabets, lg σ = ω(lg w). We also start with a linear-space
structure and then work towards compressing it. While we obtain nearly optimally
compressed space and worst-case optimal query time, reaching optimally compressed
space imposes a slight slowdown in the query time. Summarizing, we obtain the following 
tradeoffs:
(1) O(1/τ ) query time using nH + o(n) bits of space, where lg σ = O(lg w);
(2) O(1/τ ) query time using (1 + ǫ)nH + o(n) bits of space for any constant ǫ > 0; and
(3) any query time of the form (1/τ ) · ω(1) using nH + o(n)(H + 1) bits of space.
Overall, we obtain for the ﬁrst time O(n)-word (i.e., linear space) and O(1/τ ) (i.e.,
worst-case optimal) query time, even for variable τ . In all cases, we preserve constanttime 
access to S within the compressed space we use, and our queries require O(1/τ )
extra working space. As a byproduct, we also show how to ﬁnd the range mode in a
time that depends on how frequent it actually is.

Finally, in Section 6 we return to τ -minorities. By exploiting the duality with τ -
majorities, we reuse the results obtained for the latter, so that the tradeoffs (1)–(3)
are also obtained for τ -minorities. Those results supersede the original tradeoff (a)
obtained in Section 3. Actually, a single data structure with the spaces given in points
(1)–(3) solves at the same time τ -minority and τ -majority queries.

2. PRELIMINARIES
We use the RAM model of computation with word size in bits w = Ω(lg n), allowing
multiplications. The input is an array S[1..n] of symbols (or “elements”) from [1..σ],
where for simplicity we assume σ ≤ n (otherwise we could remap the alphabet so that

ACM Transactions on Algorithms, Vol. V, No. N, Article A, Publication date: January YYYY.

A:4

D. Belazzougui et al.

every symbol actually appears in S, without changing the output of any τ -majority or
τ -minority query).

2.1. Access, select, rank, and partial rank
Let S[1..n] be a string over alphabet [1..σ], for σ ≤ n, and let H ≤ lg σ be the entropy
, where each element
of the distribution of elements in S, that is, H = Pa∈[1..σ]
a appears na times in S. An access query on S takes a position k and returns S[k]; a
rank query takes an alphabet element a and a position k and returns ranka(S, k), the
number of occurrences of a in S[1..k]; a select query takes an element a and a rank r
and returns selecta(S, r), the position of the rth occurrence of a in S. A partial rank
query, rankS[k](S, k), is a rank query with the restriction that the element a must occur
in the position k. These are among the most well-studied operations on strings, so we
state here only the results most relevant to this paper.

na
n lg n
na

For σ = 2 and any constant c, P ˇatras¸cu [2008] showed how we can store S in nH +
O(n/ lgc n) bits, supporting all the queries in time O(c). If S has m 1s, this space is
m + n/ lgc n(cid:1) bits. For lg σ = O(lg lg n), Ferragina et al. [2007] showed how we
O(cid:0)m lg n
can store S in nH + o(n) bits and support all the queries in O(1) time. This result
was later extended to the case lg σ = O(lg w) [Belazzougui & Navarro 2015, Thm. 7].
Barbay et al. [2014] showed how, for any positive constant ǫ, we can store S in (1 +
ǫ)nH + o(n) bits and support access and select in O(1) time and rank in O(lg lg σ) time.
Alternatively, they can store S in nH + o(n)(H + 1) bits and support either access
or select in time O(1), and the other operation, as well as rank, in time O(lg lg σ).
Belazzougui & Navarro [2015, Thm. 8] improved the time of rank to O(lg lgw σ), which
they proved optimal, and the time of the non-constant operation to any desired function
in ω(1). Belazzougui & Navarro [2014, Sec. 3] showed how to support O(1)-time partial
rank using o(n)(H + 1) further bits. Throughout this article we also use some simpler
variants of these results.

2.2. Colored range listing
Muthukrishnan [2002] showed how we can store S[1..n] such that, given the endpoints
of a range, we can quickly list the distinct elements in that range and the positions of
their leftmost occurrences therein. Let C[1..n] be the array in which C[k] is the position
of the rightmost occurrence of the element S[k] in S[1..k − 1] — i.e., the last occurrence
before S[k] itself — or 0 if there is no such occurrence. Notice S[k] is the ﬁrst occurrence
of that distinct element in a range S[i..j] if and only if i ≤ k ≤ j and C[k] < i. We store
C, implicitly or explicitly, and a data structure supporting O(1)-time range-minimum
queries on C that returns the position of the leftmost occurrence of the minimum in
the range.

To list the distinct elements in a range S[i..j], we ﬁnd the position m of the leftmost
occurrence of the minimum in the range C[i..j]; check whether C[m] < i; and, if so,
output S[m] and m and recurse on C[i..m − 1] and C[m + 1..j]. This procedure is online
— i.e., we can stop it early if we want only a certain number of distinct elements —
and the time it takes per distinct element is O(1) plus the time to access C.
Suppose we already have data structures supporting access, select and partial rank
queries on S, all in O(t) time. Notice C[k] = selectS[k](S, rankS[k](S, k) − 1), so we can
also support access to C in O(t) time. Sadakane [2007] and Fischer [2010] gave O(n)-
bit data structures supporting O(1)-time range-minimum queries. Therefore, we can
implement Muthukrishnan’s solution using O(n) extra bits such that it takes O(t) time
per distinct element listed.

ACM Transactions on Algorithms, Vol. V, No. N, Article A, Publication date: January YYYY.

Range Majorities and Minorities in Arrays

A:5

3. PARAMETERIZED RANGE MINORITY
Chan et al. [2015] gave a linear-space solution with O(1/τ) query time for parameterized 
range minority, even for the case of variable τ (i.e., chosen at query time). They
ﬁrst build a list of ⌈1/τ⌉ distinct elements that occur in the given range (or as many
as there are, if fewer) and then check those elements’ frequencies to see which are
τ -minorities. There cannot be as many as ⌈1/τ⌉ τ -majorities so, if there exists a τ -
minority for that range, then at least one must be in the list. In this section we use a
simple approach to implement this idea using compressed space; then we obtain more
reﬁned results in Section 6.

3.1. Compressed space
To support parameterized range minority on S[1..n] in O(1/τ ) time, we store a
data structure supporting O(1)-time access, select and partial rank queries on S
[Barbay et al. 2014] and a data structure supporting O(1)-time range-minimum
queries on the C array corresponding to S [Muthukrishnan 2002]. As seen in Section 
2, for any positive constant ǫ, we can store these data structures in a total
of (1 + ǫ)nH + O(n) bits. Given τ and endpoints i and j, in O(1/τ ) time we use
Muthukrishnan’s algorithm to build a list of ⌈1/τ⌉ distinct elements that occur in
S[i..j] (or as many as there are, if fewer) and the positions of their leftmost occurrences 
therein. We check whether these distinct elements are τ -minorities using the
following lemma:

LEMMA 3.1. Suppose we know the position of the leftmost occurrence of an element
in a range. Then we can check whether that element is a τ -minority or a τ -majority
using a partial rank query and a select query on S.

PROOF. Let k be the position of the ﬁrst occurrence of a in S[i..j]. If S[k] is the rth
occurrence of a in S, then a is a τ -minority for S[i..j] if and only if the (r+⌊τ (j−i+1)⌋)th
occurrence of a in S is strictly after S[j]; otherwise a is a τ -majority. That is, we can
check whether a is a τ -minority for S[i..j] by checking whether
selecta(S, ranka(S, k) + ⌊τ (j − i + 1)⌋) > j ;
since S[k] = a, computing ranka(S, k) is only a partial rank query.

This gives us the following theorem, which improves Chan et al. ’s solution to use

nearly optimally compressed space with no slowdown.

THEOREM 3.2. Let S[1..n] be a string whose distribution of symbols has entropy H.
For any constant ǫ > 0, we can store S in (1 + ǫ)nH +O(n) bits such that later, given the
endpoints of a range and τ , we can return a τ -minority for that range (if one exists) in
time O(1/τ ).
3.2. Optimally compressed space
By changing our string representation to that of Belazzougui & Navarro [2015, Thm.
8], we can store our data structures for access, select and partial rank on S and rangeminimum 
queries on C in a total of nH + o(nH) + O(n) bits at the cost of the select
queries taking O(g(n)) time, for any desired g(n) = ω(1); see again Section 2. Therefore
the range minority is found in time O((1/τ )g(n)).
To reduce the space bound to nH + o(n)(H + 1) bits, we must reduce the space of
the range-minimum data structure to o(n). Such a result was sketched by Hon et al.
[2009], but it lacks sufﬁcient detail to ensure correctness. We give these details next.

The technique is based on sparsiﬁcation. We cut the sequence into blocks of length
g(n), choose the n/g(n) minimum values of each block, and build the range-minimum

ACM Transactions on Algorithms, Vol. V, No. N, Article A, Publication date: January YYYY.

A:6

D. Belazzougui et al.

data structure on the new array C ′[1..n/g(n)] (i.e., C ′[i] stores the minimum of
C[(i − 1) · g(n) + 1..i · g(n)]). This requires O(n/g(n)) = o(n) bits. Muthukrishnan’s
algorithm is then run over C ′ as follows. We ﬁnd the minimum position in C ′, then
recursively process its left interval, then process the minimum of C ′ by considering
the g(n) corresponding cells in C, and ﬁnally process the right part of the interval. The
recursion stops when the interval becomes empty or when all the g(n) elements in the
block of C are already reported.

LEMMA 3.3. The procedure described identiﬁes the leftmost positions of all the distinct 
elements in a block-aligned interval S[i..j], working over at most g(n) cells per new
element discovered.

PROOF. We proceed by induction on the size of the current subinterval [ℓ..r], which
is always block-aligned. Let k′ be the position of the minimum in C ′[(ℓ − 1)g(n) +
1..r/g(n)] and let k be the position of the minimum in C[(k′ − 1)· g(n) + 1, k′ · g(n)]. Then
C[k] is clearly the minimum in C[ℓ..r] and S[k] is the leftmost occurrence in S[ℓ..r] of
the element a = S[k]. If C[k] ≥ i, then a already occurs in S[i..ℓ−1] and we have already
reported it. Since the minimum of C[ℓ..r] is within the block k′ of C, it is sufﬁcient that
C[k] ≥ i for all the positions k in that block to ensure that all the values in S[ℓ..r] have
already been reported, in which case we can stop the procedure. The g(n) scanned cells
can be charged to the function that recursively invoked the interval [ℓ..r].

Otherwise, we recursively process the interval to the left of block k′, which by inductive 
hypothesis reports the unique elements in that interval. Then we process the
current block of size g(n), ﬁnding at least the new occurrence of element S[k] (which
cannot appear to the left of k′). Finally, we process the interval to the right of k′, where
the inductive hypothesis again holds.

Note that the method is also correct if, instead of checking whether all the elements
in the block of C ′[k′] are ≥ i, we somehow check that all of them have already been
reported. We will use this variant later in the paper.

For general ranges S[i..j], we must include in the range of C ′ the two partially overlapped 
blocks on the extremes of the range. When it comes to process one of those
blocks, we only consider the cells that are inside [i..j]; the condition to report an element 
is still that C[k] < i.

We use this procedure to obtain any ⌈1/τ⌉ distinct elements. We perform
O((1/τ ) g(n)) accesses to C, each of which costs time O(g(n)) because it involves a
select query on S. Therefore the total time is O(cid:0)(1/τ ) g(n)2(cid:1). Testing each of the candidates 
with Lemma 3.1 takes time O((1/τ ) g(n)), because we also use select queries on
S. Therefore, for any desired time of the form O((1/τ ) f (n)), we use g(n) = pf (n).

THEOREM 3.4. Let S[1..n] be a string whose distribution of symbols has entropy H.
For any function f (n) = ω(1), we can store S in nH + o(n)(H + 1) bits such that later,
given the endpoints of a range and τ , we can return a τ -minority for that range (if one
exists) in time O((1/τ ) f (n)).

Note that this representation retains constant-time access to S.

4. PARAMETERIZED RANGE MAJORITY ON SMALL ALPHABETS
In this section we consider the case lg σ = O(lg w), where rank queries on S can be supported 
in constant time. Our strategy is to ﬁnd a set of O(1/τ ) candidates that contain
all the possible τ -majorities and then check them one by one, counting their occurrences 
in S[i..j] via rank queries on S. The time will be worst-case optimal, O(1/τ ). We
ﬁrst obtain O(n lg σ) bits of space and then work towards compressing it.

ACM Transactions on Algorithms, Vol. V, No. N, Article A, Publication date: January YYYY.

Range Majorities and Minorities in Arrays

A:7

First, note that if τ < 1/σ, we can simply assume that all the σ symbols are candidates 
for majority, and check them one by one; therefore we care only about how to
ﬁnd O(1/τ ) candidates in the case τ ≥ 1/σ.
4.1. Structure
We store an instance of the structure of Belazzougui & Navarro [2015, Thm. 5] supporting 
access, rank, and select on S in O(1) time, using n lg σ + o(n) bits. For every
0 ≤ t ≤ ⌈lg σ⌉ and t ≤ b ≤ ⌊lg n⌋, we divide S into blocks of length 2b−1 and store a
b[k] = 1 if, (1) the element S[k] occurs at least 2b−t
binary string Gt
times in S[k − 2b+1..k + 2b+1], and (2) S[k] is the leftmost or rightmost occurrence of
that element in its block.
At query time we will use t = ⌈lg(1/τ )⌉ and b = ⌊lg(j − i + 1)⌋. The following lemma
b[k] = 1.
LEMMA 4.1. For every τ -majority a of S[i..j] there exists some k ∈ [i..j] such that

shows that it is sufﬁcient to consider the candidates S[k] for i ≤ k ≤ j where Gt

b[1..n] in which Gt

S[k] = a and Gt

b[k] = 1.

PROOF. Since S[i..j] cannot be completely contained in a block of length 2b−1, if
S[i..j] overlaps a block then it includes one of that block’s endpoints. Therefore, if S[i..j]
contains an occurrence of an element a, then it includes the leftmost or rightmost
occurrence of a in some block. Suppose a is a τ -majority in S[i..j], and b ≥ t. For all
i ≤ k ≤ j, a occurs at least τ 2b ≥ 2b−t times in S[k − 2b+1..k + 2b+1], so since some
occurrence of a in S[i..j] is the leftmost or rightmost in its block, it is ﬂagged by a 1 in
Gt
b[i..j].

b

The number of distinct elements that occur at least 2b−t times in a range of size
O(cid:0)2b(cid:1) is O(2t), so in each block there are O(2t) positions ﬂagged by 1s in Gt
b, for a
total of m = O(cid:0)n 2t−b(cid:1) 1s. It follows that we can store an instance of the structure of
P ˇatras¸cu [2008] (recall Section 2.1) supporting O(1)-time access, rank and select on Gt
in O(cid:0)n2t−b(b − t) + n/ lg3 n(cid:1) bits in total. Summing over t from 0 to ⌈lg σ⌉ and over b
from t to ⌊lg n⌋, calculation shows we use a total of O(n lg σ) bits for the binary strings.
4.2. Queries
Given endpoints i and j and a threshold τ , if τ < 1/σ, we simply report every element
a ∈ [1..σ] such that ranka(S, j)−ranka(S, i−1) > τ (j−i+1), in total time O(σ) = O(1/τ ).
Otherwise, we compute b and t as explained and, if b < t, we run a sequential algorithm
on S[i..j] in O(j − i) = O(1/τ ) time [Misra & Gries 1982]. Otherwise, we use rank
and select on Gt
b[i..j]. Since S[i..j] overlaps at most 5 blocks of
length 2b−1, it contains O(1/τ) elements ﬂagged by 1s in Gt
b; therefore, we have O(1/τ )
candidates to evaluate, and these include all the possible τ -majorities. Each candidate
a is tested in constant time for the condition ranka(S, j) − ranka(S, i − 1) > τ (j − i + 1).
We aim to run the sequential algorithm in O(j − i) = O(1/τ ) worst-case time and
space, which we achieve by taking advantage of the rank and select operations on S.
We create a doubly-linked list with the positions i to j, plus an array T [1..j−i+1] where
T [k] points to the list node representing S[i + k− 1]. We take the element S[i] = a at the
head of the list and know that it is a τ -majority in S[i..j] if ranka(S, j)− ranka(S, i− 1) >
τ (j− i + 1). If it is, we immediately report it. In any case, we remove all the occurrences
of a from the doubly-linked list, that is, the list nodes T [selecta(S, ranka(S, i) + r)], r =
0, 1, 2, . . .. We proceed with the new header of the doubly-linked list, which points to a
different element S[i′] = a′, and so on. It is clear that we perform O(j − i) constant-time
rank and select operations on S, and that at the end we have found all the τ -majorities.

b to ﬁnd all the 1s in Gt

ACM Transactions on Algorithms, Vol. V, No. N, Article A, Publication date: January YYYY.

A:8

D. Belazzougui et al.

THEOREM 4.2. Let S[1..n] be a string over alphabet [1..σ], with lg σ = O(lg w). We
can store S in O(n lg σ) bits such that later, given the endpoints of a range and τ , we can
return the τ -majorities for that range in time O(1/τ ).
4.3. Succinct space
To reduce the space we will open the structure we are using to represent S
[Belazzougui & Navarro 2015, Thm. 5]. This is a multiary wavelet tree: it cuts the
alphabet range [1..σ] into wβ contiguous subranges of about the same size, for some
conveniently small constant 0 < β ≤ 1/4. The root node v of the wavelet tree stores
the sequence Sv[1..n] indicating the range to which each symbol of S belongs, roughly
Sv[i] = ⌈S[i]/(σ/wβ)⌉. This node has wβ children, where the pth child stores the subsequence 
of the symbols S[i] such that Sv[i] = p. The alphabet of each child has
been reduced to a range of size roughly σ/wβ. This range is split again into wβ
subranges, creating wβ children for each child, and so on. The process is repeated
recursively until the alphabet range is of size less than wβ. The wavelet tree has
height lgwβ σ = lg σ
β lg w = O(1), and at each level the strings Sv stored add up to
n lg(wβ ) = βn lg w bits, for a total of n lg σ bits of space. The other o(n) bits are needed
to provide constant-time rank and select support on the strings Sv.

We use this hierarchical structure to ﬁnd the τ -majorities as follows. Assume we
have the structures to ﬁnd τ -majorities in any of the strings Sv associated with
wavelet tree nodes v, in time O(1/τ ). Then, if a is a τ -majority in S[i..j], the symbol 
p = ⌈a/(σ/wβ)⌉ is also a τ -majority in Sv[i..j], where v is the wavelet tree root.
Therefore, we ﬁnd in time O(1/τ) the τ -majorities p in Sv[i..j]. We verify each such τ -
majority p recursively in the pth child of v. In this child u, the range Sv[i..j] is projected
to Su[iu..ju] = Su[rankp(Sv, i − 1) + 1, rankp(Sv, j)], and the corresponding threshold is
τu = τ (j − i + 1)/(ju − iu + 1) < 1. This process continues recursively until we ﬁnd the
majorities in the leaf nodes, which correspond to actual symbols that can be reported
as τ -majorities in S[i..j].

The time to ﬁnd the τu-majorities in each child u of the root v is O(1/τu) =
O((ju − iu + 1)/((j − i + 1)τ )). Added over all the children u, this gives Pu O(1/τu) =
Pu O((ju − iu + 1)/((j − i + 1)τ )) = O(1/τ). Adding this over all the levels, we obtain
O((1/β)(1/τ )) = O(1/τ ).

b is O(cid:16) nv lg σ′ lg w

wβ

+ nv

The price of using this higher lower bound for b is that it requires us to sequentially

Finding τ ′-majorities on tiny alphabets. The remaining problem is how to ﬁnd τ ′-
majorities on an alphabet of size σ′ = wβ, on each of the strings Sv of length nv. We
do almost as we did for Theorem 4.2, except that the range for b is slightly narrower:
⌊lg(2t · wβ /4)⌋ ≤ b ≤ ⌊lg nv⌋. Then calculation shows that the total space for the bitveclg 
nv(cid:17) = o(nv), so added over the whole wavelet tree is o(n).
tors Gt
ﬁnd τ ′-majorities in time O(1/τ ′) on ranges of length O(cid:0)(1/τ ′)wβ(cid:1). However, we can
take advantage of the small alphabet. First, if 1/τ ′ ≥ σ′, we just perform σ′ pairs of
constant-time rank queries on Sv. For 1/τ ′ < σ′, we will compute an array of σ′ counters
with the frequency of the symbols in the range, and then report those exceeding the
threshold. The maximum size of the range is (4/τ ′)wβ/4 ≤ σ′wβ = w2β, and thus 2β lg w
bits sufﬁce to represent each counter. The σ′ counters then require 2βwβ lg w bits and
can be maintained in a computer word (although we will store them somewhat spaced
for technical reasons). We can read the elements in Sv by chunks of wβ symbols, and
compute in constant time the corresponding counters for those symbols. Then we sum
the current counters and the counters for the chunk, all in constant time because they
are ﬁelds in a single computer word. The range is then processed in time O(1/τ ′).

ACM Transactions on Algorithms, Vol. V, No. N, Article A, Publication date: January YYYY.

Range Majorities and Minorities in Arrays

A:9

To compute the counters corresponding to wβ symbols, we extend the popcounting
algorithm of Belazzougui & Navarro [2015, Sec. 4.1]. Assume we extract them from Sv
and have them packed in the lowest kℓ bits of a computer word X, where k = wβ is the
number of symbols and ℓ = lg σ′ the number of bits used per symbol. We ﬁrst create
σ′ copies of the sequence at distance 2kℓ of each other: X ← X · (02kℓ−11)σ′
. In each
copy we will count the occurrences of a different symbol. To have the ith copy count
the occurrences of symbol i, for 0 ≤ i < σ′, we perform

X ← X XOR 0kℓ((σ′ − 1)ℓ)k . . . 0kℓ(2ℓ)k 0kℓ(1ℓ)k 0kℓ(0ℓ)k,

where iℓ is number i written in ℓ bits. Thus in the ith copy the symbols equal to i
become zero and the others nonzero. To set a 1 at the highest bit of the symbols equal
to i in the ith copy, we do

X ← (Y − (X AND NOT Y )) AND Y AND NOT X,

where Y = (0kℓ(10ℓ−1)k)σ′
.1 Now we add all the 1s in each copy with X ← X ·
0kℓ(2σ′
−1)(0ℓ−11)k. This spreads several sums across the 2kℓ bits of each copy, and in
particular the kth sum adds up all the 1s of the copy. Each sum requires lg k bits,
which is precisely the ℓ bits we have allocated per ﬁeld. Finally, we isolate the desired
counters using X ← X AND (0kℓ1ℓ0(k−1)ℓ)σ′
. The σ′ counters are not contiguous in the
computer word, but we still can afford to store them spaced: we use 2kℓσ′ = 2βw2β lg w
bits, which since β ≤ 1/4, is always less than w.
The cumulative counters, as said, need lg(σ′wβ ) = 2ℓ bits. We will store them in a
computer word C separated by 2kℓ bits so that we can directly add the resulting word
X after processing a chunk of wβ symbols of the range in Sv: C ← C + X. If the last
chunk is of length l < wβ, we complete it with zeros and then subtract those spurious
wβ − l occurrences from the ﬁrst counter, C ← C − (wβ − l) · 2(k−1)ℓ.
after processing the range. We use

The last challenge is to output the counters that are at least y = ⌊τ ′(j − i + 1)⌋ + 1

C ← C + (22ℓ − y) · (0kℓ+ℓ−110(k−1)ℓ)σ′

so that the counters reaching y will overﬂow to the next bit. We isolate those overﬂow
bits with C ← C AND (0(k−1)ℓ−110(k+1)ℓ)σ′
, so that we have to report the ith symbol if
and only if C AND 0(k(2σ′
−2i+1)−1)ℓ−110(k(2i−1)+1)ℓ 6= 0. We repeatedly isolate the lowest
bit of C with

D ← (C XOR (C − 1)) AND (0(k−1)ℓ−110(k+1)ℓ)σ′

,

and then remove it with C ← C AND (C − 1), until C = 0. Once we have a position
isolated in D, we ﬁnd the position in constant time by using a monotone minimum
perfect hash function over the set {2(k(2i−1)+1)ℓ, 1 ≤ i ≤ σ′}, which uses O(σ′ lg w) =
o(w) bits [Belazzougui et al. 2009]. Only one such data structure is needed for all the
sequences, and it takes less space than a single systemwide pointer.

THEOREM 4.3. Let S[1..n] be a string over alphabet [1..σ], with lg σ = O(lg w). We
can store S in n lg σ + o(n) bits such that later, given the endpoints of a range and τ , we
can return the τ -majorities for that range in time O(1/τ ).

1This could have been simply X ← (Y − X) AND Y if there was an unused highest bit set to zero in the
ﬁelds of X. Instead, we have to use this more complex formula that ﬁrst zeroes the highest bit of the ﬁelds
and later considers them separately.

ACM Transactions on Algorithms, Vol. V, No. N, Article A, Publication date: January YYYY.

A:10

D. Belazzougui et al.

4.4. Optimally compressed space
One choice to compress the space is to use a compressed representation of the strings
Sv [Ferragina et al. 2007]. This takes chunks of c = (lg n)/2 bits and assigns them a
code formed by a header of lg c bits and a variable-length remainder of at most c bits.
For decoding a chunk in constant time, they use a directory of O(nv lg c/c) bits, plus
a constant table of size 2c = O(√n) that receives any encoded string and returns the
original chunk. The compressed size of any string Sv with entropy Hv then becomes
nvHv + O(nv lg σ′ lg lg n/ lg n) bits, which added over the whole wavelet tree becomes
nH + O(cid:16) n lg σ lg lg n
(cid:17) bits. This can be used in replacement of the direct representation

of sequences Sv in Theorem 4.3, since we only change the way a chunk of Θ(lg n) bits
is read from any Sv. Note that we read chunks of wβ symbols from Sv, which could
be ω(lg n) if n is very small. To avoid this problem, we apply this method only when
lg σ = O(lg lg n), as in this case we can use computer words of w = lg n bits.

lg n

COROLLARY 4.4. Let S[1..n] be a string whose distribution of symbols has entropy
H, over alphabet [1..σ], with lg σ = O(lg lg n). We can store S in nH + o(n) bits such that
later, given the endpoints of a range and τ , we can return the τ -majorities for that range
in time O(1/τ).

For the case where lg σ = ω(lg lg n) but still lg σ = O(lg w), we use another technique.
We represent S using the optimally compressed structure of Barbay et al. [2014]. This
structure separates the alphabet symbols into lg2 n classes according to their frequencies.
 A sequence K[1..n], where K[i] is the class to which S[i] is assigned, is represented
using the structure of Corollary 4.4, which supports constant-time access, rank, and
select, since the alphabet of K is of polylogarithmic size, and also τ -majority queries in
time O(1/τ). For each class c, a sequence Sc[1..nc] contains the subsequence of S of the
symbols S[i] where K[i] = c. We will represent the subsequences Sc using Theorem 4.3.
Then the structure for K takes nHK + o(n) bits, where HK is the entropy of the distribution 
of the symbols in K, and the structures for the strings Sc take nc lg σc + o(nc)
bits, where Sc ranges over alphabet [1..σc]. Barbay et al. show that these space bounds
add up to nH + o(n) bits and that one can support access, rank and select on S via
access, rank and select on K and some Sc.

Our strategy to solve a τ -majority query on S[i..j] resembles the one used to prove
Theorem 4.3. We ﬁrst run a τ -majority query on string K. This will yield the at most
1/τ classes of symbols that, together, occur more than τ (j − i + 1) times in S[i..j]. The
classes excluded from this result cannot contain symbols that are τ -majorities. Now,
for each included class c, we map the interval S[i..j] to Sc[ic..jc] in the subsequence of
its class, where ic = rankc(K, i − 1) + 1 and jc = rankc(K, j), and then run a τc-majority
query on Sc[ic..jc], for τc = τ (j − i + 1)/(jc − ic + 1). The results obtained for each
considered class c are reported as τ -majorities in S[i..j]. The query time, added over all
the possible τc values, is Pc O(1/τc) = O(1/τ ) as before.
THEOREM 4.5. Let S[1..n] be a string whose distribution of symbols has entropy H,
over alphabet [1..σ], with lg σ = O(lg w). We can store S in nH + o(n) bits such that later,
given the endpoints of a range and τ , we can return the τ -majorities for that range in
time O(1/τ ).
5. PARAMETERIZED RANGE MAJORITY ON LARGE ALPHABETS
Rank queries
time on large alphabets
[Belazzougui & Navarro 2015]. To obtain optimal query time in this case, we resort
to the use of Lemma 3.1 instead of performing rank queries on S. For this purpose, we
must be able to ﬁnd the leftmost occurrence of each τ -majority in a range. This is done

cannot be performed in constant

ACM Transactions on Algorithms, Vol. V, No. N, Article A, Publication date: January YYYY.

Range Majorities and Minorities in Arrays

A:11

by adding further structures on top of the bitvectors Gt
b used in Theorem 4.2. Those
bitvectors Gt
b alone require O(n lg σ) bits of space, whereas our further structures add
only o(n) bits. Within these O(n lg σ) bits, we can store a simple representation of S
[Barbay et al. 2014], which supports both access and select queries in constant time.
We also add the structures to support partial rank in constant time, within o(n lg σ)
further bits. Therefore we can apply Lemma 3.1 in constant time and solve τ -majority
queries in time O(1/τ ). We consider compression later.
5.1. Structure
First, to cover the case τ < 1/σ, we build the structure of Muthukrishnan [2002] on S,
using O(n) extra bits as shown in Section 2.2, so that we can ﬁnd the O(σ) = O(1/τ )
leftmost occurrences of each distinct element in S[i..j]. On each leftmost occurrence we
can then apply Lemma 3.1 in constant time. Now we focus on the case τ ≥ 1/σ.
In addition to the bitvectors Gt
b of the previous section, we mark in a second bitvector
b each (lg4 n)-th occurrence in S of the alphabet symbols in the area where they mark
J t
b. More precisely, let a = S[k] occur at least 2b−t times in S[k − 2b+1..k + 2b+1],
bits in Gt
and let i1, i2, . . . be the positions of a in S. Then we mark in J t
b the positions {iq lg4 n, k −
2b+1 ≤ iq lg4 n ≤ k + 2b+1}.
For the subsequence St
b, we build an instance of
Muthukrishnan’s structure. That is, we build the structure on the array C t
b corresponding 
to the string St
b , k)]. This string need not be stored explicitly, but
instead we store C t

b of elements of S marked in J t

b[k] = S[select1(J t
b in explicit form.

Furthermore, if for any b and t it holds J t

b [iq lg4 n] = 1, being S[iq lg4 n] = a, we create a
succinct SB-tree [Grossi et al. 2009, Lem 3.3] successor structure2 associated with the
chunk of lg4 n consecutive positions of a: i1+(q−1) lg4 n, . . . , iq lg4 n. This structure is stored
associated with the 1 at J t
b[iq lg4 n] (all the 1s at the same position iq lg4 n, for different
b and t values, point to the same succinct SB-tree, as it does not depend on b or t).

The SB-tree operates in time O(cid:0)lg(lg4 n)/ lg lg n(cid:1) = O(1) and uses O(cid:0)lg4 n lg lg n(cid:1) bits.

It needs constant-time access to the positions ir+(q−1) lg4 n, as it does not store them.
We provide those positions using ik = selecta(S, k).

arrays C t

Added over all the symbols a, occurring na times in S, each bitvector J t

b contains
Pa⌊na/ lg4 n⌋ = O(cid:0)n/ lg4 n(cid:1) 1s. Thus, added over every b and t, the bitvectors J t
b,
b, and pointers to succinct SB-trees (using O(lg n) bits per pointer), require
O(n/ lg n) = o(n) bits. Each succinct SB-tree requires O(cid:0)lg4 n lg lg n(cid:1) bits, and they
may be built for O(cid:0)n/ lg4 n(cid:1) chunks, adding up to O(n lg lg n) bits. This is O(n lg σ) if we

assume lg σ = Ω(lg lg n).

5.2. Queries
Given i and j, we compute b = ⌊lg(j − i + 1)⌋ and t = ⌈lg(1/τ )⌉, and ﬁnd the O(1) blocks
of length 2b overlapping S[i..j]. As in the previous section, every Gt
b[i..j] is
a candidate to verify, but this time we need to ﬁnd its leftmost occurrence in S[i..j].

b[k] = 1 in Gt

To ﬁnd the leftmost position of a = S[k], we see if the positions k and i are in the
same chunk. That is, we compute the chunk index q = ⌈ranka(S, k)/ lg4 n⌉ of k (via a
partial rank on S) and its limits il = selecta(S, (q − 1) lg4 n) and ir = selecta(S, q lg4 n).
Then we see if il < i ≤ ir. In this case, we use the succinct SB-tree associated with
J t
b[ir] = 1 to ﬁnd the successor of i in time O(1). Then we use Lemma 3.1 from that
position to determine in O(1) time if a is a τ -majority in S[i..j].

2In that paper they ﬁnd predecessors, but the problem is analogous.

ACM Transactions on Algorithms, Vol. V, No. N, Article A, Publication date: January YYYY.

A:12

D. Belazzougui et al.

If k is not in the same chunk of i, we disregard it because, in this case, there is an
occurrence S[il] = a in S[i..j] that is marked in J t
b. We will instead ﬁnd separately the
leftmost occurrence in S[i..j] of any candidate a that is marked in J t
b[i..j], as follows.
We apply Muthukrishnan’s algorithm on the 1s of J t
b[i..j], to ﬁnd the distinct elements
of St
b, j)]. Thus we obtain the leftmost sampled occurrences
in S[i..j] of all the τ -majorities, among other candidates. For each leftmost occurrence
St
b[k′], it must be that k = select1(J t
b, k′) is in the same chunk of i, and therefore we can
ﬁnd the successor of i using the corresponding succinct SB-tree in constant time, and
then verify the candidate using Lemma 3.1.

b, i− 1) + 1, rank1(J t

b[rank1(J t

It follows from the construction of J t

b[i..j] gives us O(1/τ) candidates to verify, in time O(1/τ ).

b that the distinct elements sampled in any S[i..j]
must appear at least 2t times in an interval of size O(cid:0)2b(cid:1) containing S[i..j], and so there
can only be O(1/τ) distinct sampled elements. Therefore, Muthukrishnan’s algorithm
on J t
When b < t, we use our sequential algorithm of Section 4.2 with the only difference
that, since we always ﬁnd the leftmost occurrence of each candidate in S[i..j], we can
use Lemma 3.1 to verify the τ -majorities. Thus the algorithm uses only select and
partial rank queries on S, and therefore it runs in time O(1/τ ) as well.

THEOREM 5.1. Let S[1..n] be a string over alphabet [1..σ], with lg σ = Ω(lg lg n). We
can store S in O(n lg σ) bits such that later, given the endpoints of a range and τ , we can
return the τ -majorities for that range in time O(1/τ ).
5.3. Compressed space
To reduce the space, we use the same strategy used to prove Theorem 4.5: we represent 
S using the optimally compressed structure of Barbay et al. [2014]. This time,
however, closer to the original article, we use different representations for the strings
Sc with alphabets of size σc ≤ w and of size σc > w. For the former, we use the representation 
of Theorem 4.3, which uses nc lg σc + o(nc) bits and answers τc-majority queries
in time O(1/τc). For the larger alphabets, we use a slight variant of Theorem 5.1: we
use the same structures Gt
b, and pointers to succinct SB-trees, except that the
lower bound for b will be ⌊lg(2t · g(n, σ))⌋, for any function g(n, σ) = ω(1). The total
(cid:17) = o(nc lg σc), whereas
space for the bitvectors Gt
the other structures already used o(nc) bits (with a couple of exceptions we consider
soon).

b, J t
b of string Sc is thus O(cid:16) nc lg σc lg g(n,σ)

b, C t

g(n,σ)

Then, representing Sc with the structure of Belazzougui & Navarro [2015, Thm. 6],
so that it supports select in time O(g(n, σ)) and access in time O(1), the total space for
Sc is nc lg σc + o(nc lg σc), and the whole structure uses nH + o(n)(H + 1) bits.
The cases where b ≥ ⌊lg(2t · g(n, σ))⌋ are solved with O(1/τc) applications of select
on S, and therefore take time O((1/τc) g(n, σ)). Instead, the shorter ranges, of length
O((1/τc) g(n, σ)), must be processed sequentially, as in Section 4.2. The space of the
sequential algorithm can be maintained in O(1/τc) = O(1/τ ) words as follows. We
cut the interval Sc[ic..jc] into chunks of m = ⌈1/τc⌉ consecutive elements, and process
each chunk in turn as in Section 4.2. The difference is that we maintain an array with
the τc-majorities a we have reported and the last position pa we have deleted in the
lists. From the second chunk onwards, we remove all the positions of the known τcmajorities 
a before processing it, selecta(Sc, ranka(Sc, pa) + r), for r = 1, 2, . . .; note that
ranka(Sc, pa) is a partial rank query. Since select on Sc costs O(g(n, σ)) and we perform
O((1/τc) g(n, σ)) operations, the total time is O(cid:0)(1/τc) g(n, σ)2(cid:1). Then we can retain the
optimally compressed space and have any time of the form O((1/τ ) f (n, σ)) by choosing
g(n, σ) = pf (n, σ).

ACM Transactions on Algorithms, Vol. V, No. N, Article A, Publication date: January YYYY.

Range Majorities and Minorities in Arrays

A:13

c pieces of length σ2

c . For each symbol a we store a bitvector Ba[1, nc/σ2

There are, as anticipated, two ﬁnal obstacles related to the space. The ﬁrst are the
O(nc) bits of Muthukrishnan’s structure associated with Sc to handle the case τc <
1/σc. To reduce this space to o(nc), we sparsify the structure as in Section 3.2. The
case of small τc is then handled in time O(cid:0)σc g(n, σ)2(cid:1) = O((1/τc) f (n, σ)) and the space
for the sparsiﬁed structure is O(nc/g(n, σ)) = o(nc).
The second obstacle is the O(nc lg lg nc) bits used by the succinct SB-trees. Examination 
of the proof of Lemma 3.3 in Grossi et al. [2009] reveals that one can obtain
O(p lg lg u) bits of space and O(lg p/ lg lg n) time if we have p elements in a universe
[1..u] and can store a precomputed table of size o(n) that is shared among all the succinct 
SB-trees. We reduce the universe size as follows. We logically cut the string Sc
into nc/σ2
c ] where
Ba[i] = 1 if and only if a appears in the ith piece. These bitvectors require O(nc/σc)
bits in total, including support for rank and select. The succinct SB-trees are now local
to the pieces: a succinct SB-tree that spans several pieces is split into several succinct
SB-trees, one covering the positions in each piece. The 1s corresponding to these pieces
in bitvectors Ba point to the newly created succinct SB-trees. To ﬁnd the successor of
position i given that it is in the same chunk of ir > i, with J t
b[ir] = 1, we ﬁrst compute
the piece p = ⌈i/σ2
c⌉ of ir, and see if i and ir are in the
same piece, that is, if p = pr. If so, the answer is to be found in the succinct SB-tree
associated with the 1 at J t
b[ir]. Otherwise, that original structure has been split into
several, and the part that covers the piece of i is associated with the 1 at Ba[p]. It
is possible, however, that there are no elements in the piece p, that is, Ba[p] = 0, or
that there are elements but no one is after i, that is, the succinct SB-tree associated
with piece p ﬁnds no successor of i. In this case, we ﬁnd the next piece that follows p
where a has occurrences, p′ = select1(Ba, rank1(Ba, p) + 1), and if p′ < pr we query the
succinct SB-tree associated with Ba[p′] = 1 for its ﬁrst element (or the successor of i).
If, instead, p′ ≥ pr, we query instead the succinct SB-tree associated with J t
b[ir] = 1,
as its positions are to the left of those associated with Ba[p′] = 1. Therefore, successor
queries still take O(1) time. The total number of elements stored in succinct SB-trees
is still at most nc, because no duplicate elements are stored, but now each requires
only O(lg lg σc) bits, for a total space of O(nc lg lg σc) = o(nc lg σc) bits. There may be up
to σ · (nc/σ2) pointers to succinct SB-trees from bitvectors Ba, each requiring O(lg nc)
bits, for a total of O((nc lg nc)/σc) = O(nc) = o(nc lg σc), since σc > w.

c⌉ of i and the piece pr = ⌈ir/σ2

THEOREM 5.2. Let S[1..n] be a string whose distribution of symbols has entropy H,
over alphabet [1..σ]. For any f (n, σ) = ω(1), we can store S in nH + o(n)(H + 1) bits such
that later, given the endpoints of a range and τ , we can return the τ -majorities for that
range in time O((1/τ ) f (n, σ)).

associated with the O(cid:16) nc lg σc lg g(n,σ)

Note that accessing a position in S still requires constant time with this representation.
 Further, we can obtain a version using nearly compressed space, (1 + ǫ)nH + o(n)
bits for any constant ǫ > 0, with optimal query time, by setting g(n, σ) to a constant
value. First, use for Sc the structure of Barbay et al. [2014] that needs (1 + ǫ/3)nH +
o(n) bits and solves access and select in constant time. Second, let κ be the constant
b and the sparsiﬁed
Muthukrishnan’s structures. Then, choosing g(n, σ) = 6κ
ǫ ensures that the space
becomes (ǫ/3)nc lg σc bits, which add up to (ǫ/3)nH. All the other terms of the form
o(nH) are smaller than another (ǫ/3)nH + o(n). Therefore the total space adds up to
(1 + ǫ)nH + o(n) bits. The time to sequentially solve a range of length O((1/τc) g(n, σ))
is O(cid:0)(1/τc) g(n, σ)2(cid:1) = O(1/τc).

(cid:17) bits used by bitvectors Gt

ǫ lg 6κ

g(n,σ)

ACM Transactions on Algorithms, Vol. V, No. N, Article A, Publication date: January YYYY.

A:14

D. Belazzougui et al.

THEOREM 5.3. Let S[1..n] be a string whose distribution of symbols has entropy H,
over alphabet [1..σ]. For any constant ǫ > 0, we can store S in (1 + ǫ)nH + o(n) bits such
that later, given the endpoints of a range and τ , we can return the τ -majorities for that
range in time O(1/τ ).
5.4. Finding range modes
While ﬁnding range modes is a much harder problem in general, we note that we
can use our data structure from Theorem 5.2 to ﬁnd a range mode quickly when it
is actually reasonably frequent. Suppose we want to ﬁnd the mode of S[i..j], where it
occurs occ times (we do not know occ). We perform multiple range τ -majority queries
on S[i..j], starting with τ = 1 and repeatedly reducing it by a factor of 2 until we ﬁnd
at least one τ -majority. This takes time

O(cid:16)(cid:16)1 + 2 + 4 + . . . + 2⌈lg j−i+1

occ ⌉(cid:17) f (n, σ)(cid:17) = O(cid:18) (j − i + 1)f (n, σ)

occ

(cid:19)

and returns a list of O(cid:0) j−i+1

times in S[i..j]. We use rank queries to determine which of these elements is the mode.
For the fastest possible time on those rank queries, we use for S the representation of
Belazzougui & Navarro [2015, Thm. 8], and also set f (n, σ) = lg lgw σ, the same time

occ (cid:1) elements that includes all those that occur at least occ

of rank. The cost is then O(cid:16) (j−i+1) lg lgw σ

occ

(cid:17). The theorem holds for lg σ = O(lg w) too, as

in this case we can use Theorem 4.5 with constant-time rank queries.

THEOREM 5.4. Let S[1..n] be a string whose distribution of symbols has entropy H.
We can store S in nH + o(n)(H + 1) bits such that later, given endpoints i and j, we can

return the mode of S[i..j] in O(cid:16) (j−i+1) lg lgw σ

occ

mode occurs in S[i..j].

(cid:17) time, where occ is the number of times the

6. RANGE MINORITIES REVISITED
The results obtained for range majorities can be adapted to ﬁnd range minorities,
which in particular improves the result of Theorem 3.2. The main idea is again that, if
we test any ⌈1/τ⌉ distinct elements, we must ﬁnd a τ -minority because not all of those
can occur more than τ (j − i + 1) times in S[i..j]. Therefore, we can use mechanisms
similar to those we designed to ﬁnd O(1/τ ) distinct candidates to τ -majorities.
Let us ﬁrst consider the bitvectors Gt
b deﬁned in Section 4. We now deﬁne bitvectors
b, where we ﬂag the positions of the ﬁrst 2t and the last 2t distinct values in each
I t
block (we may ﬂag fewer positions if the block contains less than 2t distinct values).
Since we set O(2t) bits per block, the bitvectors I t
b use asymptotically the same space
of the bitvectors Gt
b.

Given a τ -minority query, we compute b and t as in Section 4 and use rank and select
b[i..j]. Those positions contain a τ -minority in S[i..j] if

to ﬁnd all the 1s in the range I t
there is one, as shown next.

LEMMA 6.1. The positions ﬂagged in I t

b[i..j] contain a τ -minority in S[i..j], if there

is one.

PROOF. If I t

b[i..j] overlaps a block where it does not ﬂag 2t = ⌈1/τ⌉ distinct elements
(in which case one is for sure a τ -minority), then it marks all the distinct block elements 
that fall inside [i..j]. This is obvious if [i..j] fully contains the block, and it also
holds if [i..j] intersects a preﬁx or a sufﬁx of the block, since the block marks its 2t ﬁrst
and last occurrences of distinct elements.

ACM Transactions on Algorithms, Vol. V, No. N, Article A, Publication date: January YYYY.

Range Majorities and Minorities in Arrays

A:15

Just as for τ -majorities, we use I t

b only if 1/τ ≤ σ, since otherwise we can test all the
alphabet elements one by one. The test proceeds using rank on S if σ is small, or using
Lemma 3.1 if σ is large. We now describe precisely how we proceed.

6.1. Small alphabets
If lg σ = O(lg w), we use a multiary wavelet tree as in Section 4. This time, we do not
run τ -majority queries on each wavelet tree node v to determine which of its children to
explore, but rather we explore every child having some symbol in the range Sv[iv..jv].
To efﬁciently ﬁnd the distinct symbols that appear the range, we store a sparsiﬁed
Muthukrishnan’s structure similar to the one described in Section 3.2; this time we
will have no slowdown thanks to the small alphabet of Sv.

v[k′] also holds C ′

Let Cv be the array corresponding to string Sv. We cut Sv into blocks of wβ bits, and
v[1..nv/wβ] the minimum value in the corresponding block of Cv.
record in an array C ′
Then, the leftmost occurrence S[k] = p of each distinct symbol p in Sv[iv..jv] has a value
Cv[k] < iv, and thus its corresponding block C ′
v[k′] < iv. We initialize a
word E ← 0 containing ﬂags for the σ′ = wβ symbols, separated as in the ﬁnal state of
the word C of Section 4.3. Each time the algorithm of Muthukrishnan on C ′
v gives us a
new block, we apply the algorithm of Section 4.3 to count in a word C the occurrences of
the distinct symbols in that block, we isolate the counters reaching the threshold y = 1,
and compare E with E OR C. If they are equal, then we stop the recursive algorithm,
since all the symbols in the range had already appeared before (see the ﬁnal comments
on the proof of Lemma 3.3). Otherwise, we process the subrange to the left of the
block, update E ← E OR C, and process the subrange to the right. When we ﬁnish, E
contains all the symbols that appear in Sv[iv..jv]. In the recursive process, we also stop
when we have considered ⌈1/τv⌉ blocks, since each includes at least one new element
and it is sufﬁcient to explore ⌈1/τv⌉ children to ﬁnd a τv-minority (because each child
contains at least one candidate). Finally, we extract the bits of E one by one as done
in Section 4.3 with the use of D. For each extracted bit, we enter the corresponding
child in the wavelet tree. The total time is thus O(1/τv) and the bitvectors Cv add up
to O(cid:0)n/wβ(cid:1) = o(n) bits in total.
The τu values to use in the children u of v are computed as in Section 4.3, so the
analysis leading to O(1/τ ) total time applies. When we arrive at the leaves u of the
wavelet tree, we obtain the distinct elements and compute using rank the number of
times they occur in Su[iu..ju], so we can immediately report the ﬁrst τ -minority we ﬁnd.
We still have to describe how we handle the intervals that are smaller than the
lower limit for b, ⌊2t · wβ/4⌋. We do the counting exactly as in Section 4.3. We must
then obtain the counters that are between 1 and y − 1. On one hand, we use the bound
y′ = 1 and repeat their computation to obtain in Cl ← C the counters that are at least
1. On the other, we compute C ← C + (22ℓ− y)· (0kℓ+ℓ−110(k−1)ℓ)σ′
as before, and isolate
the non-overﬂowed bits with Cr ← (NOT C) AND (0(k−1)ℓ−110(k+1)ℓ)σ′
. Then we extract
the ﬁrst of the bits marked in C ← Cl AND Cr and report it.
To obtain compressed space, we use the alphabet partitioning technique of Section 
4.4. Once again, we must identify at most ⌈1/τ⌉ nonempty ranges [ic..jc] from
K[i..j]. Those are obtained in the same way as on the multiary wavelet tree, since K
is represented in that way (albeit the strings Sv are compressed). We then look for
τc-minorities in the strings Sc[ic..jc] one by one, until we ﬁnd one or we exhaust them.
The total time is O(1/τ ).

THEOREM 6.2. Let S[1..n] be a string whose distribution of symbols has entropy H,
over alphabet [1..σ], with lg σ = O(lg w). We can store S in nH + o(n) bits such that later,
given the endpoints of a range and τ , we can return a τ -minority for that range (if one
exists) in time O(1/τ ).

ACM Transactions on Algorithms, Vol. V, No. N, Article A, Publication date: January YYYY.

A:16

D. Belazzougui et al.

Note that we can use a single representation using nH + o(n) bits solving both the

τ -majority queries of Theorem 4.5 and the τ -minority queries of Theorem 6.2.

6.2. Large alphabets
For large alphabets we must use Lemma 3.1 to check for τ -minorities, and thus we
must ﬁnd the leftmost positions in S[i..j] of the τ -minority candidates. We use the
same bitvectors J t
b of Section 5, so that they store sampled positions corresponding to
the 1s in I t

b, and proceed exactly as in that section, both if τ < 1/σ or if τ ≥ 1/σ.

To obtain compression, we also use alphabet partitioning. We use on the multiary 
wavelet tree of K the method described in Section 6.1, and then complete the
queries with τc-minority queries on the strings Sc over small or large alphabets, as required,
 until we ﬁnd one result or exhaust all the strings. The only novelty is that
we must now ﬁnd τc-minorities sequentially for the ranges that are shorter than

⌊lg(2t · g(n, σ))⌋ = O((1/τc) g(n, σ)). For this, we adapt the O(cid:0)(1/τ ) g(n, σ)2(cid:1)-time sequential 
algorithm described in Section 5.3. The only difference is that we stop as soon
as we test a candidate a that turns out not to be a τc-majority, then reporting the
τ -minority a.

Depending on whether we use Theorem 5.2 or 5.3 to represent S and how we choose

f (n, σ), we obtain Theorem 3.4 again or an improved version of Theorem 3.2:

THEOREM 6.3. Let S[1..n] be a string whose distribution of symbols has entropy H,
over alphabet [1..σ]. For any constant ǫ > 0, we can store S in (1 + ǫ)nH + o(n) bits such
that later, given the endpoints of a range and τ , we can return a τ -minority for that
range (if one exists) in time O(1/τ ).

In both cases, we can share the same structures to ﬁnd majorities and minorities.

7. CONCLUSIONS
We have given the ﬁrst linear-space data structure for parameterized range majority
with query time O(1/τ ), even in the more difﬁcult case of τ speciﬁed at query time.
This is worst-case optimal in terms of n and τ , since the output size may be up to 1/τ .
Moreover, we have improved the space bounds for parameterized range majority and
minority, reaching in many cases optimally compressed space with respect to the entropy 
H of the distribution of the symbols in the sequence. While we have almost closed
the problem in these terms, there are some loose ends that require further research:

— Our results for τ -majorities are worst-case time optimal, but they take O(1/τ ) time
even if the number of majorities is o(1/τ ). Is it possible to run in time O(occ + 1)
when there are occ τ -majorities? Can we use O(occ + 1) instead of O(1/τ) space?
— Our structure and previous ones for τ -minorities also take time O(1/τ), although we
are required to output only one τ -minority. Is it possible to run in O(1) time, or to
prove a lower bound? Can we use less than O(1/τ) space?
— On large alphabets, σ = wω(1), both for τ -majorities and τ -minorities we must use
(1+ǫ)nH+o(n) bits, for any constant ǫ > 0, to reach time O(1/τ ). With nH+o(n)(H+1)
bits we only have a time of the form (1/τ ) · ω(1). Is it possible to close this gap?
— Our results do not improve when τ is ﬁxed at indexing time, which is in principle an
easier scenario. Is it possible to obtain better results for ﬁxed τ ?

ACKNOWLEDGMENTS

Many thanks to Patrick Nicholson for helpful comments.

ACM Transactions on Algorithms, Vol. V, No. N, Article A, Publication date: January YYYY.

Range Majorities and Minorities in Arrays

A:17

REFERENCES

Barbay, J., Claude, F., Gagie, T., Navarro, G., & Nekrich, Y. 2014. Efﬁcient fullycompressed 
sequence representations. Algorithmica, 69(1), 232-268.

Belazzougui, D., & Navarro, G. 2014. Alphabet-independent compressed text indexing.

ACM Transactions on Algorithms, 10(4), article 23.

Belazzougui, D., & Navarro, G. 2015. Optimal Lower and Upper Bounds for Representing 
Sequences. ACM Transactions on Algorithms, 11(4), article 31.

Belazzougui, D., Boldi, P., Pagh, R., & Vigna, S.. 2009. Monotone minimal perfect
hashing: searching a sorted table with O (1) accesses. Proc. 20th Annual ACM-SIAM
Symposium on Discrete Algorithms (SODA), pp. 785–794.

Bose, P., Kranakis, E., Morin, P., & Tang, Y. 2005. Approximate range mode and range
median queries. Proc. 22nd Symposium on Theoretical Aspects of Computer Science
(STACS), pp. 377-388.

Chan, T. M., Durocher, S., Skala, M., & Wilkinson, B. T. 2015. Linear-space data structures 
for range minority query in arrays. Algorithmica, 72(4), 901–913.

Chan, T. M., Durocher, S., Larsen, K. G., Morrison, J., & Wilkinson, B. T. 2014. Linearspace 
data structures for range mode query in arrays. Theory of Computing Systems,
55(4), 719–741.

Cormode, G., & Muthukrishnan,

Data Stream Methods.
http://www.cs.rutgers.edu/∼muthu/198-3.pdf. Lecture 3 of Rutger’s 198:671
Seminar on Processing Massive Data Sets.

S.

2003.

Demaine, E. D., L´opez-Ortiz, A., & Munro, J. I. 2002. Frequency estimation of internet
packet streams with limited space. Proc. 10th European Symposium on Algorithms
(ESA), pp. 348–360.

Durocher, S., He, M., Munro, J. I., Nicholson, P. K., & Skala, Matthew. 2013. Range
Information and Computation, 222,

majority in constant time and linear space.
169–179.

Durocher, S., Shah, R., Skala, M., & Thankachan, S. V. 2016. Linear-Space Data Structures 
for Range Frequency Queries on Arrays and Trees. Algorithmica, 74(1), 344366.


Elmasry, A., Munro, J. I., & Nicholson, P. K. 2011. Dynamic range majority data
structures. Proc. 22nd International Symposium on Algorithms and Computation
(ISAAC), pp. 150–159.

Ferragina, P., Manzini, G., M ¨akinen, V., & Navarro, G. 2007. Compressed representations 
of sequences and full-text indexes. ACM Transactions on Algorithms, 3(2),
article 20.

Fischer, J. 2010. Optimal succinctness for range minimum queries. Proc. 9th Latin

American Symposium on Theoretical Informatics (LATIN), pp. 158–169.

Gagie, T., He, M., Munro, J. I., & Nicholson, P. K. 2011. Finding frequent elements in
compressed 2D arrays and strings. Proc. 18th Symposium on String Processing and
Information Retrieval (SPIRE), pp. 295–300.

Golynski, A., Munro, I., & Rao, S. 2006. Rank/select operations on large alphabets: a
tool for text indexing. Proc. 17th Annual ACM-SIAM Symposium on Discrete Algorithms 
(SODA), pp. 368–373.

Greve, M., Jørgensen, A. G., Larsen, K. D., & Truelsen, J. 2010. Cell probe lower
bounds and approximations for range mode. Proc. 37th International Colloquium on
Automata, Languages and Programming (ICALP), pp. 605–616.

Grossi, R., Orlandi, A., Raman, R., & Rao, S. S. 2009. More Haste, Less Waste: Lowering 
the Redundancy in Fully Indexable Dictionaries. Proc. 26th Symposium on
Theoretical Aspects of Computer Science (STACS), pp. 517–528.

ACM Transactions on Algorithms, Vol. V, No. N, Article A, Publication date: January YYYY.

A:18

D. Belazzougui et al.

Hon, W.-K., Shah, R., & Vitter, J. 2009. Space-Efﬁcient Framework for Top-k String Retrieval 
Problems. Proc. 50th IEEE Annual Symposium on Foundations of Computer
Science (FOCS), pp. 713–722.

Karp, R. M., Shenker, S., & Papadimitriou, C. H. 2003. A simple algorithm for ﬁnding
frequent elements in streams and bags. ACM Transactions on Database Systems,
28(1), 51–55.

Karpinski, M., & Nekrich, Y. 2008. Searching for frequent colors in rectangles. Proc.

20th Canadian Conference on Computational Geometry (CCCG), pp. 11–14.

Krizanc, D., Morin, P., & Smid, M. H. M. 2005. Range mode and range median queries

on lists and trees. Nordic Journal of Computing, 12(1), 1–17.

Lai, Y. K., Poon, C. K., & Shi, B. 2008. Approximate colored range and point enclosure

queries. Journal of Discrete Algorithms, 6(3), 420–432.

Misra, J., & Gries, D. 1982. Finding repeated elements. Science of Computer Programming,
 2(2), 143–152.

Muthukrishnan, S. 2002. Efﬁcient algorithms for document retrieval problems. Proc.

13th Symposium on Discrete Algorithms (SODA), pp. 657–666.

Navarro, G., & Thankachan, S. V. 2016. Optimal Encodings for Range Majority

Queries. Algorithmica, 74(3), 1082–1098.

Petersen, H. 2008. Improved bounds for range mode and range median queries. Proc.
34th Conference on Current Trends in Theory and Practice of Computer Science
(SOFSEM), pp. 418–423.

Petersen, H., & Grabowski, S. 2009. Range mode and range median queries in constant

time and sub-quadratic space. Information Processing Letters, 109(4), 225–228.

P ˇatras¸cu, M. 2008. Succincter. Proc. 49th Symposium on Foundations of Computer

Science (FOCS), pp. 305–313.

Sadakane, K. 2007. Succinct data structures for ﬂexible text retrieval systems. Journal

of Discrete Algorithms, 5(1), 12–22.

Wei, Z., & Yi, K. 2011. Beyond simple aggregates: indexing for summary queries. Proc.

30th Symposium on Principles of Database Systems (PODS), pp. 117–128.

ACM Transactions on Algorithms, Vol. V, No. N, Article A, Publication date: January YYYY.

