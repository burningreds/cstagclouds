Space-Efﬁcient Construction of LZ-Index(cid:1)

Diego Arroyuelo and Gonzalo Navarro

Dept. of Computer Science, University of Chile,
{darroyue, gnavarro}@dcc.uchile.cl

Blanco Encalada 2120, Santiago, Chile

Abstract. A compressed full-text self-index is a data structure that replaces a text
and in addition gives indexed access to it, while taking space proportional to the
compressed text size. The LZ-index, in particular, requires 4uHk(1 + o(1)) bits
of space, where u is the text length in characters and Hk is its k-th order empirical
entropy. Although in practice the LZ-index needs 1.0-1.5 times the text size, its
construction requires much more main memory (around 5 times the text size),
which limits its applicability to large texts. In this paper we present a practical
space-efﬁcient algorithm to construct LZ-index, requiring (4+)uHk +o(u) bits
of space, for any constant 0 <  < 1, and O(σu) time, being σ the alphabet size.
Our experimental results show that our method is efﬁcient in practice, needing an
amount of memory close to that of the ﬁnal index.

1 Introduction and Previous Work

A full-text database is a system providing fast access to a large mass of textual data. The
simplest (yet realistic and rather common) scenario is as follows. The text collection
is regarded as a unique sequence of characters T1...u over an alphabet Σ of size σ,
and the search pattern P1...m as another (short) sequence over Σ. Then the text search
problem consists of ﬁnding all the occ occurrences of P in T . To provide fast access,
data structures called indexes are built on the text. Typical text databases contain natural
language texts, DNA or protein sequences, MIDI pitch sequences, program code, etc.

Until a short time ago, the smallest indexes available in practice were the sufﬁx
arrays [21], requiring u log u bits (log means log2 in this paper). Since the text requires
u log σ bits to be represented, this index is usually much larger than the text (typically
4 times the text size). To handle huge texts like the Human Genome (about 3 × 109
base pairs), one solution is to store the indexes on secondary memory. However, this
has signiﬁcant inﬂuence on the running time of an application, as access to secondary
memory is considerably slower.

Several attempts to reduce the space of the sufﬁx trees [2] or arrays [13,17,19,1]
focused on reducing the size of the data structures but not the text, and did not relate
text compressibility with the size of its index.

A parallel track started at about the same time [15,14,10,28,4,5,6,8,9,20,7], with the
distinguishing feature of providing compressed indexes, whose sizes are proportional
to the compressed text size. Moreover, in most cases, those indexes replace the text by

(cid:1) Supported in part by CONICYT PhD Fellowship Program (ﬁrst author) and Fondecyt Grant

1-050493 (second author).

X. Deng and D. Du (Eds.): ISAAC 2005, LNCS 3827, pp. 1143–1152, 2005.
c(cid:1) Springer-Verlag Berlin Heidelberg 2005

1144

D. Arroyuelo and G. Navarro

being able to reproduce any text substring. This is called self-indexing. Taking space
proportional to the compressed text, replacing it, and providing efﬁcient indexed access
to it, is an unprecedented breakthrough in text indexing and compression.

The LZ-index [24,25,26] is a full-text self-index on these lines, based on the ZivLempel 
parsing of the text. If the text is parsed into n phrases by the LZ78 algorithm
[29], then the LZ-index takes 4n log n(1 + o(1)) bits of space, which is 4 times the size
of the compressed text and also 4 times the k-th order text entropy, i.e. 4uHk + o((1 +
Hk)u), for any k = o(log n/ log2 σ) [16,6]. See the original article for details on its
search algorithms, as we focus only in construction in this paper.

However, all these works do not consider the space-efﬁcient construction of the indexes.
 For example, construction of CS-array [28] and FM-index [4] involves building 
ﬁrst the sufﬁx array of the text. Similarly, the LZ-index is constructed over a noncompressed 
intermediate representation. In both cases, one needs about 5 times the text
size. For example, the Human Genome may ﬁt in 1 GB of main memory using these
indexes (and thus it can be operated entirely in RAM on a desktop computer), but 15
GB of main memory are needed to build them! Using secondary memory for the construction 
is usually rather inefﬁcient.

The works of T.-W. Lam et al. [18] and W.-K.Hon et al. [12] deal with the space
(and time) efﬁcient construction of CS-array. The former work presents an algorithm
that uses (2H0 + 1 + )u bits of space to build the CS-array, where H0 is the 0-th order
empirical entropy of the text, and  is any positive constant; the construction time is
O(σu log u), which is good enough if the alphabet is small (as in the case of DNA), but
may be impractical in the case of proteins and Oriental languages, such as Chinese or
Japanese. The second work [12] addresses this problem by requiring (H0 + 2 + )u bits
of space and O(u log u) time to build the CS-array. Also, they show how to build the
FM-index from CS-array in O(u) time.

Our work follows this line of research. We present a practical and efﬁcient algorithm
to construct the LZ-index using little space. Our idea is to replace the (non-compressed)
intermediate representations of the tries that conform the index by space-efﬁcient counterparts.
 Basically, we use the balanced parentheses representation of Munro and Raman
[23] as an intermediate representation for the tries, but we modify such representation
to allow fast incremental construction directly from the text. The resulting intermediate 
data structure consists of a tree whose nodes are small subsequences of balanced
parentheses, which are easier and cheaper to update. The idea is inspired in the work
of Clark and Munro [3], yet ours differs in numerous technical aspects and practical
considerations (structuring inside the nodes, overﬂow management policies, etc.).

Our algorithm requires (4+)uHk+o(u) bits to build the LZ-index, for any constant
0 <  < 1. This is very close to the space the ﬁnal LZ-index requires to operate.
This is the ﬁrst construction algorithm for a self-index requiring space proportional
to Hk instead of H0. In practice our algorithm also requires about the same memory
as the ﬁnal index. That is, wherever the LZ-index can be used, we can build it. The
indexing speed is approximately 5 sec/MB in a 2GHz machine, which is competitive
with the (non-space-efﬁcient) construction of FM-index and much faster than CS-array
construction [26]. We argue that our method outperforms (in time) previous work [12]
when indexing the Human Genome, using about the same indexing space.

2 The LZ-Index Data Structure

Space-Efﬁcient Construction of LZ-Index

1145

Assume that the text T1...u has been partitioned using the LZ78 [29] algorithm into
n+1 blocks T = B0 . . . Bn, such that B0 = ε; ∀k (cid:2)= (cid:5), Bk (cid:2)= B(cid:1); and ∀k (cid:1) 1, ∃(cid:5) < k,
c ∈ Σ, Bk = B(cid:1) · c. To ensure that Bn is not a preﬁx of another Bi, we append to T a
special character “$” (cid:2)∈ Σ. The data structures that conform LZ-index are [26,25]:
1. LZTrie: is the trie formed by all the blocks B0 . . . Bn. Given the properties of LZ78
compression, this trie has exactly n + 1 nodes, each one corresponding to a string.
n. In this trie there

2. RevTrie: is the trie formed by all the reverse strings Br
0

. . . Br

could be internal nodes not representing any block. We call these nodes empty.

3. Node: is a mapping from block identiﬁers to their node in LZTrie.
4. RNode: is a mapping from block identiﬁers to their node in RevTrie.

Each of these 4 structures requires n log n(1 + o(1)) = uHk(1 + o(1)) bits of space.

For the construction of LZTrie we traverse the text and at the same time build a
normal trie (using one pointer per parent-child relation) of the strings represented by
Ziv-Lempel blocks. At step k (assume Bk = Bi · c), we read the text that follows and
step down the trie until we cannot continue. At this point we create a new trie leaf (child
of the trie node of block i, by character c, and assigning the leaf block number k), go
to the root again, and go on with step k + 1 reading the rest of the text. The process
completes when the last block ﬁnishes with the text terminator “$”.

Then we compact the normal trie, essentially using the parentheses representation
of Munro and Raman [23]. We traverse the normal trie in preorder, writing an opening
parenthesis when going down to a node, and a closing parenthesis when going up.

The LZTrie structure consists of the above sequence of parentheses plus a sequence
lets of characters that label each trie edge and a sequence ids of block identiﬁers, both
in preorder. We identify a trie node x with its opening parenthesis in the representation.
 The subtree of x contains those nodes (parentheses) enclosed between the opening
parenthesis representing x and its matching closing parenthesis.
Once the LZTrie is built we free the space of the normal trie, and build Node. This
is just an array with the n nodes of LZTrie, using (cid:5)log n(cid:6) bits for each. It is built as the
inverse of permutation ids.

To construct RevTrie we traverse LZTrie in depth-ﬁrst order, generating each string
stored in LZTrie in constant time, and then inserting it into a normal trie of reversed
strings. We then traverse the trie and represent it using a sequence of parentheses and
block identiﬁers, rids. Empty unary nodes are removed only at this step. Finally, we
build the normal reverse trie and build RNode as the inverse permutation of rids.

In the experiments of the original LZ-index [26,25], the largest extra space needed to
build LZTrie is that of the normal trie, which is 1.7–2.0 times the text size. The indexing
space for the normal reverse trie is, in some cases, 4 times the text size. This is, mainly,
because of the empty unary nodes. This space dictates the maximum indexing space
of the algorithm (note that the text itself can be processed by buffers and hence does
not require signiﬁcant space). The overall indexing space was 4.8–5.8 times the text
size for English text, and 3.4–3.7 times the text size for DNA. As a comparison, the
construction of a plain sufﬁx array without any extra data structure requires 5 times the
text size.

1146

D. Arroyuelo and G. Navarro

3 Space-Efﬁcient Construction of LZTrie

The main memory requirement to build the LZ-index comes from the normal tries used
to build LZTrie and RevTrie. We focus on building those tries in little memory, by replacing 
them with space-efﬁcient data structures that support insertions. These can be
seen as hybrids between normal tries and their ﬁnal parentheses representations.

Let us start with LZTrie. In its ﬁnal representation as a linear sequence of balanced
parentheses [23], the insertion of a new node at any position of the sequence may force
rebuilding the sequence from scratch. To avoid that cost, we work on a hierarchical
representation of balanced parentheses (hrbp for short), such that we rebuild only a
small part of the entire sequence to insert a new node.

In a hrbp we cut the trie in pages, that is, in subsets of trie nodes such that if a node x
is stored in page q, then node y, the parent of x, is: (1) also stored in q (enclosing x), or
(2) stored in a page p, the parent page of q, and hence y is ancestor of all nodes stored
in q. We store in p information indicating that node y encloses all nodes in q. In a hrbp
we arrange the pages in a tree, thus the entire trie is represented by a tree of pages.

We represent a page as a contiguous block of memory. Let N1 < . . . < Nt be even
integers. We say that a page has size Ni if it can store Ni parentheses (Ni/2 nodes),
although its physical size is larger than Ni bits. Each page p of size Ni consists of: Ni
bits to represent a subsequence of balanced parentheses; Ni/2 bits (the ﬂags) indicating
which opening parentheses (nodes) in a page have their subtree stored in a child page;
(cid:5)log Ni/2(cid:6) bits to tell the number of nodes stored in the page; (Ni/2)(cid:5)log u(cid:6) bits to
store the block identiﬁers (ids) in the page (in preorder); (Ni/2)(cid:5)log σ(cid:6) bits to store the
characters (lets) in the page (in preorder); and a variable number of pointers to child
pages. The number of pointers varies from 0 to Ni/2, and it corresponds to the number
of ﬂags with value 1 in p. To maintain a constant physical page size, these pointers are
stored in a separately allocated array, and we store a pointer to them in the page.

As in the parentheses representation of LZTrie, in the hrbp a node encloses its subtree,
 but not necessarily a node and its subtree are stored both in the same page. If the
subtree of the j-th opening parenthesis of page p is stored in page q, then q is a child
page of p and the j-th ﬂag in p has the value 1. If the number of ﬂags in 1 before the j-th
ﬂag (not including it) is h, then the h-th pointer of p points to q. An important property
we enforce is that sibling nodes must be stored all in the same page.

Initially, the data structure consists of a sole empty page (the root page) of size N1.
The construction of LZTrie proceeds as explained in Section 2, but this time the nodes
are inserted in a hrbp of LZTrie, instead of a normal trie. The insertion of a new node
Bk = Bi · c in the hrbp implies to recompute the page in which the insertion is done.
If the new leaf must become j-th opening parenthesis in the page (counting from left to
right), then we insert “()” at the corresponding parentheses position and the j-th ﬂag
is set to 0. Also, c is inserted at position j within the characters, and k is inserted at the
same position within the identiﬁers.

We do not store information to traverse the parentheses sequence in the pages of the
hrbp. Instead, all the navigation inside each page is done sequentially, in a single O(Nt)
time pass: the ﬁrst child of an opening parenthesis starts at the next position (unless that
contains a closing parenthesis, in which case the node is a leaf in the page), and the next
sibling starts right after the closing parenthesis matching the current position, which is

Space-Efﬁcient Construction of LZ-Index

1147

found sequentially. As we traverse the page, we maintain the current position j in ﬂags,
ids and lets, as well as the count h of 1-bits seen in ﬂags.
A page overﬂow occurs when inserting a new node in a full page p. If the size of p
is Ni, 1 (cid:2) i < t, we perform a grow operation on p, which allocates a new page p(cid:1)
of size Ni+1, copies the content of p to p(cid:1)
, frees p, and replaces the pointer to p by a
pointer to p(cid:1)
in the parent of p. If the size of p is Nt, instead, we select a subset of nodes
to be copied to a new child page of p and then deleted from p.

is at least (cid:7)Nt/(2σ)(cid:8) − 1 nodes.

We only allow the selection of the whole subtree of a node in the page (without
selecting the node itself). This simple way ensures that sibling nodes are always stored
in the same page. As the maximum number of siblings is σ, we must have Nt (cid:1) 2σ so
that a page with children always has space for its top-level nodes at least. We choose the
subtree of maximum size not exceeding Nt/2 nodes. It is easy to see that this guarantees
that the size of the new leaf p(cid:1)
Assume we have selected in this way the subtree of the j-th opening parenthesis in
the page. The selected subtree is copied to a new page p(cid:1)
, whose size is the minimum
Ni suitable to hold the subtree. As p(cid:1)
is a new leaf page, all its ﬂags are initialized to
0. Next we add in p a pointer to p(cid:1)
, inserted at the current (j-th) position, and set to
1 the j-th bit in ﬂags. Finally, we delete from p the selected subtree. After that, if the
number of parentheses in p does not exceed Ni for some i < t, we perform a shrink
operation, which is the opposite of grow.
Once we solved the overﬂow, the insertion of the new node may have to be done in
p or in p(cid:1)
, but we are sure that there is room for the new pair of parentheses in either
page. The following lemma states the condition to achieve a minimum ﬁll ratio α in the
pages of the data structure, thus controlling the wasted space. The proof is obvious.

Lemma 1. Let 0 < α < 1 be a real number. If each page has the smallest posible size
Ni to hold its parentheses, and we deﬁne Ni = Ni−1/α, i = 2, . . . , t, and 2 (cid:2) N1 (cid:2)
2/α, then all pages of the data structure have a ﬁll ratio of at least α.

As the trie has n nodes, we need 2n + n + n log u + n log σ + n log u(2σ/Nt)
bits of storage to represent the parentheses, ﬂags, identiﬁers, characters and pointers
to child pages, respectively. The last bound holds because leaves are created with at
least Nt/(2σ) nodes and thus there is at worst one pointer for each Nt/(2σ) nodes
(except the root). If, in addition, we deﬁne the Nis as in Lemma 1, in the worst case the
construction algorithm needs n
α(3 + log σ + (1 + 2σ/Nt) log u) bits of storage. We can
relate this space requeriment to Hk: as n log u = uHk+O(kn log σ) for any k [16], and
since n (cid:2) u/ logσ u, the space is 1+2σ/Nt

uHk + o(u) for any k = o(log n/ log3 σ).

α

When constructing LZTrie, the navigational cost per character of the text is O(Nt)
in the worst case. Hence, the overall navigational cost is O(Ntu). On the other hand,
the cost of rebuilding pages after an insertion is O(Nt), with or without page overﬂows.
As there are n insertions, the total cost is O(Ntn). However, the constant involved with
page overﬂows is greater than that of simple insertions, thus in practice we expect that
larger values of α yield a greater construction time (and a smaller space requirement).
In general, choosing Nt = 2σ/γ for any constant 0 < γ < 1, we get 1+γ
α uHk + o(u)
bits of space, which can be made (1 + )uHk + o(u) for any constant 0 <  < 1 by
properly choosing γ and α. The construction time is O( 1

γ σu) = O(σu).

1148

D. Arroyuelo and G. Navarro

Once we construct the hrbp for LZTrie, we build the ﬁnal version of LZTrie in O(n)
time. We perform a preorder traversal on the hrbp, writing an opening parenthesis each
time we reach a node, then checking the corresponding ﬂag, traversing the subtree of
the node recursively in preorder (which, depending of the ﬂag, may be stored in the
same or in a child page), and then writing a closing parenthesis.

4 Space-Efﬁcient Construction of RevTrie

For the space-efﬁcient construction of RevTrie, we use a hrbp to represent not the original 
reverse trie but its PATRICIA tree [22], which compresses empty unary paths of
the reverse trie. This yields an important saving of space. We do not store the skips in
each node since they can be computed using the connection with LZTrie. We store, in
the nodes of the reverse trie, pointers to nodes of LZTrie, instead of the corresponding
block identiﬁers. Each pointer uses (cid:5)log 2n(cid:6) bits. This is done to avoid access to Node
when constructing the reverse trie, so we can build Node after both tries have been built
(thus reducing the indexing space). The empty non-unary nodes are marked by storing
in them the same pointer to LZTrie stored in their ﬁrst child.

The hrbp of the reverse trie requires at least 1

To construct the reverse trie we traverse LZTrie in depth-ﬁrst order, generating each
string Bi stored in LZTrie in constant time, and then inserting its reverse Br
i into the
reverse trie. As we compress empty unary paths, the edges of the trie are labeled with
strings instead of simple characters. The PATRICIA tree stores only the ﬁrst character
of the string that labels the edge. Given a node v in the reverse trie, the position of the
character in Br
i on which v discriminates is 1 plus the length of the string represented
by v. If v is not an empty node, then it stores a pointer to LZTrie node nv. The length of
the string is the same as the depth of nv in LZTrie, which can be computed in constant
time [26]. On the other hand, if v is an empty node, we we must use instead a procedure
similar to that used in [26] to compute the string that labels an edge of the trie.
α(2n(cid:1) + n(cid:1) + n(cid:1) log 2n + n(cid:1) log σ +
(2σ/Nt)n(cid:1) log n(cid:1)) bits of storage to represent the parentheses, ﬂags, pointers to LZTrie,
characters and pointers to child pages, respectively, where n(cid:1) (cid:1) n is the number of
nodes in the reverse trie. As we compress unary paths, n(cid:1) (cid:2) 2n, and thus the space is
upper bounded by 2(1+2σ/Nt)
will
be much less than 2n (see Section 5 for empirical results).
i to be inserted in the reverse trie, 1 (cid:2) i (cid:2) n, the navigational
cost is O(Nt|Br
i |2) in the worst case (when we work O(Nt) per character, and
i |2).
every traversed node is empty). The total construction cost is
i |2 is usually O(u logσ u), but in pathological cases it is O(u3/2).
The sum
To have a better theoretical bound, we can explicitly store the skips, using 2 log log u
extra bits per node (inserting empty unary nodes when the skip is exceeded). In this
way, one of each log2 u empty unary nodes could be explicitly represented. In the
worst case there are O(u) empty unary nodes, of which O( u
log u) are explicitly represented.
 This means o(u) extra bits in the hrbp, and the construction cost is reduced to
(cid:1)n
i=1 (Nt|Br
After we construct the hrbp for the reverse trie, we construct RevTrie directly from
the hrbp in O(n(cid:1)) time, replacing the pointers to LZTrie by the corresponding block

For each string Br
i | + |Br
|Br

uHk + o(u). However, in practice we expect that n(cid:1)

|Br

i | = u, the total cost is O(Ntu).

(cid:1)n
i=1 (Nt|Br

i | + |Br

(cid:1)n

i | + |Br

i |). As

(cid:1)n

i=1

α

i=1

Space-Efﬁcient Construction of LZ-Index

1149
identiﬁers (rids), and then we free the space of the hrbp. If we use n(cid:1) log n bits for the
rids array, RevTrie requires 2uHk + o(u) bits of storage, and the whole index requires
5uHk(1+ o(1)) bits. Instead, we can represent the rids array with n log n bits (i.e., only
the non-empty nodes), plus a bitmap of 2n(1 + o(1)) bits supporting rank queries in
O(1) time [27]. The j-th bit of the bitmap is 1 if the node represented by the j-th open
parenthesis is not an empty node, otherwise the bit is 0. The rids index corresponding
to the j-th opening parenthesis is rank(j). Using this representation, RevTrie requires
uHk + o(u) bits of storage. This was unclear in the original LZ-index paper [26,25].
We ﬁnally build Node mapping from ids array in time O(n) and n log n = uHk +
o(u) bits of space, and RNode from rids in O(n(cid:1)) time and n log n(cid:1) = uHk + o(u) bits.
Now we summarize the construction process, and show in parentheses the total space

requeriment and the time in each step. Then, we conclude with a theorem.

1. We build the hrbp of LZTrie from the text ((1 + )uHk + o(u) bits, O(σu) time).
2. We build LZTrie from its hrbp ((1 + )uHk + uHk + o(u) bits, O(n) time).
3. We free the hrbp of LZTrie and build the hrbp of the reverse trie from LZTrie ((2 +

)uHk + uHk + o(u) bits, O(σu) time).

4. We build RevTrie from its hrbp ((2 + )uHk + uHk + uHk + o(u) bits, O(n) time).
5. We free the hrbp of RevTrie and build Node from ids (uHk + uHk + uHk + o(u)

bits, O(n) time).

6. We build RNode from rids (uHk + uHk + uHk + uHk + o(u) bits, O(n) time).

Theorem 1. Our space-efﬁcient algorithm to construct LZ-index requires (4+)uHk+
o(u) bits of space, reached at step 4 above, and O(σu) time. This holds for any constant
0 <  < 1 and any k = o(log n/ log3 σ).

5 Experimental Results

For the experiments we use the ﬁle ohsumed.88-91 from the OHSUMED collection
[11], as a representative of other text collections we tested, such as DNA, music, and
others. We use incremental subsets of the ﬁle, ranging from 10MB to 100MB. We run
our experiments on an AMD Athlon processor at 2GHz, 1024MB of RAM and 512Kb
of cache, running version 2.6.7-gentoo-r11 of Linux kernel. We compiled the code with
gcc 3.3.4 using optimization option -O9. Times were obtained using 10 repetitions.
In Fig. 1 we show the construction space for LZTrie and RevTrie. As expected, the
construction space is smaller as we use a greater value of α. On average, the construction 
space of LZTrie ranges from approximately 0.5 (α = 0.95) to 0.64 (α = 0.5) times
the text size, and from approximately 1.00 (α = 0.95) to 1.27 (α = 0.5) times the size
of the ﬁnal version of LZTrie. For construction of RevTrie the space varies from 0.52
(α = 0.95) to 0.65 (α = 0.5) times the text size, and from 1.47 (α = 0.95) to 1.85
(α = 0.5) times the size of the ﬁnal RevTrie. The greater difference among RevTrie and
its hrbp is due to the fact that the ﬁnal version of the trie does not store the characters.
As a comparison, the original construction algorithm [26] (labeled “Original” in the
plots) needs on average 1.82 times the text size to hold the normal trie and 3.30 times
to hold the normal reverse trie. The sizes as a fraction of the ﬁnal tries are 3.62 and 9.87
times, respectively.

1150

D. Arroyuelo and G. Navarro

LZTrie construction space

RevTrie construction space

s
e
t
y
b
a
g
e
M
 
n
i
 
e
z
S

i

s
e

t
y
b
a
g
e
M
n

 

i
 

e
z
S

i

 180

 160

 140

 120

 100

 80

 60

 40

 20

 0

 10

 160

 140

 120

 100

 80

 60

 40

 20

 0

 10

alpha=0.5
alpha=0.6
alpha=0.7
alpha=0.8
alpha=0.9
alpha=0.95
Original

alpha=0.5
alpha=0.6
alpha=0.7
alpha=0.8
alpha=0.9
alpha=0.95
Original

 350

 300

 250

 200

 150

 100

 50

s
e
t
y
b
a
g
e
M
 
n
i
 
e
z
S

i

 20

 30

 40

 50

 60

 70

 80

 90

 100

Megabytes of text

 0

 10

 20

 30

 40

 50

 60

 70

 80

 90

 100

Megabytes of text

Space requirement of LZ-index construction, alpha=0.5

Space requirement of LZ-index construction, alpha=0.95

Step 1
Step 2
Step 3
Step 4
Step 5
Step 6

Step 1
Step 2
Step 3
Step 4
Step 5
Step 6

 160

 140

 120

 100

 80

 60

 40

 20

s
e

t
y
b
a
g
e
M
n

 

i
 

e
z
S

i

 20

 30

 40

 50

 60

 70

 80

 90

 100

Megabytes of text

 0

 10

 20

 30

 40

 50

 60

 70

 80

 90

 100

Megabytes of text

Fig. 1. Size of the hrbps of LZTrie and RevTrie, N1 = 2, Nt = 512

LZTrie construction time

RevTrie construction time

LZ-index construction time

alpha=0.5
alpha=0.6
alpha=0.7
alpha=0.8
alpha=0.9
alpha=0.95
Original

 140

 120

 100

 80

 60

 40

 20

e
m

i
t
 
r
e
s
u

 
f

o

 
s
d
n
o
c
e
S

alpha=0.5
alpha=0.6
alpha=0.7
alpha=0.8
alpha=0.9
alpha=0.95
Original

 350

 300

 250

 200

 150

 100

 50

e
m

i
t
 
r
e
s
u

 
f

o

 
s
d
n
o
c
e
S

 0

 10

 20

 30

 40

 50

 60

 70

 80

 90

 100

 0

 10

 20

 30

 40

 50

 60

 70

 80

 90

 100

Megabytes of text

Megabytes of text

e
m

i
t
 
r
e
s
u

 
f

o

 
s
d
n
o
c
e
S

 500
 450
 400
 350
 300
 250
 200
 150
 100
 50
 0

 10

alpha=0.5
alpha=0.6
alpha=0.7
alpha=0.8
alpha=0.9
alpha=0.95
Original

 20

 30

 40

 50

 60

 70

 80

 90

 100

Megabytes of text

Fig. 2. Average user time to build LZTrie, RevTrie and the whole LZ-index, N1 = 2, Nt = 512

In the same Fig. 1 (below) we show the space requirements in each step of the
construction. The space to construct LZ-index varies from 1.46 (α = 0.95) to 1.49
(α = 0.5) times the text size, and from 1.00 (α = 0.95) to 1.02 (α = 0.5) times the
size of the ﬁnal index (labeled “Step 6” in the plots). This conﬁrms that the indexing
space is about the same to that needed by the ﬁnal index. For α = 0.5 the maximum is
reached in step 4, as predicted in the analysis. However, for α = 0.95 the maximum is
reached in step 6, mainly because in the experiments the number of nodes of the reverse
trie is (on average) n(cid:1) ≈ 1.032n, which is much less than the pesimistic theoretic bound
n(cid:1) (cid:2) 2n we used in the space requirement analysis.

In Fig. 2 we show the indexing time for the tries and the whole index. The average indexing 
rate for LZTrie varies from 0.805MB/sec (α = 0.95) to 0.828MB/sec (α = 0.5).
For RevTrie it varies from 0.302MB/sec (α = 0.95) to 0.309MB/sec (α = 0.5). The
whole indexing rate varies from 0.217MB/sec (α = 0.95) to 0.223MB/sec (α = 0.5).
As we expected, the indexing rate is greater as α becomes smaller. The original construction 
has an average indexing rate of 2.745MB/sec for LZTrie, 2.752MB/sec for
RevTrie, and 1.310MB/sec for the whole indexing process. Thus the succinct construction 
is 6 times slower in practice, as the upper bound O(σu) is too pessimistic.

Space-Efﬁcient Construction of LZ-Index

1151

We also tested our algorithm on DNA data 1, where the indexing rate is about
0.197MB/sec (α = 0.95, N1 = 2, Nt = 192), using on average 1.19 times the text
size of main memory to index. Extrapolating these results we can argue that the human
genome can be indexed in approximately 4.23 hours and using less than 4 GB of main
memory. As a comparison, W.-K. Hon et al. [12] argued that they can index the human
genome in 20 hours (although they do not describe the CPU of the machine used).

6 Conclusions and Future Work

In this paper we proposed a practical space-efﬁcient algorithm to construct LZ-index.
The basic idea is to construct the tries of LZ-index using space-efﬁcient intermediate
representations that allow fast incremental insertion of nodes. The algorithm requires
at most (4 + )uHk + o(u) bits (0 <  < 1) to construct LZ-index for the text T1...u
in time O(σu), being σ the alphabet size. This is the ﬁrst construction algorithm of a
compressed full-text index whose space requirement is related to Hk (the k-th order empirical 
entropy of the text). In our experiments the construction required approximately
1.45 times the text size, or 1.02 times the ﬁnal index size, which is much better than
the original LZ-index construction algorithm (4–5 times the text size), and the indexing
speed was approximately of 5sec/MB.

We believe that our intermediate hrbp representation could be made searchable, so
that it could be taken as the ﬁnal index. The result would be a LZ-index supporting
efﬁcient insertion of new text. Those pages could also be handled in secondary memory,
 so as to have an efﬁcient disk-based LZ-index. Furthermore, the hrbp might have
independent interest as a practical technique to represent dynamic general trees in little
space, so we plan to work on making them fully dynamic. For the near future, we plan
to compare our method against previous work [12], both in time and space.

References

1. M. Abouelhoda, E. Ohlebusch, and S. Kurtz. Optimal exact string matching based on sufﬁx

arrays. In Proc. SPIRE’02, LNCS 2476, pages 31–43, 2002.

2. A Apostolico. The myriad virtues of subword trees. In Combinatorial Algorithms on Words,

NATO ISI Series, pages 85–96. Springer-Verlag, 1985.

3. D. Clark and J. I. Munro. Efﬁcient sufﬁx trees on secondary storage. In Proc. SODA’96,

pages 383–391, 1996.

4. P. Ferragina and G. Manzini. Opportunistic data structures with applications.

In Proc

FOCS’00, pages 390–398, 2000.

5. P. Ferragina and G. Manzini. An experimental study of an opportunistic index.

In Proc.

SODA’01, pages 269–278, 2001.

6. P. Ferragina and G. Manzini. On compressing and indexing data. Technical Report TR-0201,
 Dipartamento di Informatica, Univ. of Pisa, 2002.

7. P. Ferragina, G. Manzini, V. M¨akinen, and G. Navarro. An alphabet-friendly FM-index. In

Proc.SPIRE’04, LNCS 3246, pages 150–160. Springer, 2004.

8. R. Grossi, A. Gupta, and J. S. Vitter. High-order entropy-compressed text indexes. In Proc.

SODA’03, pages 841–850. SIAM, 2003.

1 52.71MB from GenBank (Homo Sapiens DNA, http://www.ncbi.nlm.nih.gov).

1152

D. Arroyuelo and G. Navarro

9. R. Grossi, A. Gupta, and J.S. Vitter. When indexing equals compression: experiments with
compressing sufﬁx arrays and applications. In Proc. SODA’04, pages 636–645. SIAM, 2004.
10. R. Grossi and J.S. Vitter. Compressed sufﬁx arrays and sufﬁx trees with applications to text

indexing and string matching. In Proc. STOC’00, pages 397–406, 2000.

11. W. Hersh, C. Buckley, T. Leone, and D. Hickam. Ohsumed: An interactive retrieval evaluation 
and new large test collection for research. In Proc. SIGIR’94, pages 192–201, 1994.

12. W. K. Hon, T. W. Lam, K. Sadakane, and W. K. Sung. Constructing compressed sufﬁx arrays

with large alphabets. In Proc. ISAAC’03, LNCS 2906, pages 240–249, 2003.

13. J. K¨arkk¨ainen. Sufﬁx cactus: a cross between sufﬁx tree and sufﬁx array. In Proc. CPM’95,

LNCS 937, pages 191–204, 1995.

14. J. K¨arkk¨ainen. Repetition-based text indexes. PhD thesis, Dept. of Computer Science, University 
of Helsinki, Finland, 1999.

15. J. K¨arkk¨ainen and E. Ukkonen. Lempel-Ziv parsing and sublinear-size index structures for

string matching. In Proc. WSP’96, pages 141–155. Carleton University Press, 1996.

16. R. Kosaraju and G. Manzini. Compression of low entropy strings with Lempel-Ziv algorithms.
 SIAM Journal on Computing, 29(3):893–911, 1999.

17. S. Kurtz. Reducing the space requeriments of sufﬁx trees. Technical Report 98-03, Technische 
Kakult¨at, Universit¨at Bielefeld, Germany, 1998.

18. T. W. Lam, K. Sadakane, W. K. Sung, and S. M. Yiu. A space and time efﬁcient algorithm
for constructing compressed sufﬁx arrays. In Proc. COCOON 2002, pages 401–410, 2002.
19. V. M¨akinen. Compact sufﬁx array - a space-efﬁcient full-text index. Fundamenta Informaticae,
 56(1–2):191–210, 2003.

20. V. M¨akinen and G. Navarro. Succinct sufﬁx arrays based on run-length encoding. In Proc.

CPM’05, LNCS 3537, pages 45–56, 2005.

21. U. Manber and G. Myers. Sufﬁx arrays: A new method for on–line string searches. SIAM

Journal on Computing, 22(5):935–948, 1993.

22. D. R. Morrison. Patricia – practical algorithm to retrieve information coded in alphanumeric.

Journal of the ACM, 15(4):514–534, 1968.

23. I. Munro and V. Raman. Succinct representation of balanced parentheses, static trees and

planar graphs. In Proc. FOCS’97, pages 118–126, 1997.

24. G. Navarro. Indexing text using the Ziv-Lempel trie. In Proc. SPIRE’04, LNCS 2476, pages

325–336, 2002.

25. G. Navarro.

Indexing text using the Ziv-Lempel trie. Technical Report TR/DCC-20022,
 Dept. of Computer Science, Univ. of Chile, 2002. ftp://ftp.dcc.uchile.cl/
pub/users/gnavarro/lzindex.ps.gz.

26. G. Navarro. Indexing text using the Ziv-Lempel trie. Journal of Discrete Algorithms (JDA),

2(1):87–114, 2004.

27. V. Raman and S. Rao. Static dictionaries supporting rank. In Proc. ISAAC ’99, LNCS 1741,

pages 18–26, 1999.

28. K. Sadakane. Compressed text databases with efﬁcient query algorithms based on the compressed 
sufﬁx array. In Proc. ISAAC’00, LNCS 1969, pages 410–421, 2000.

29. J. Ziv and A. Lempel. Compression of individual sequences via variable–rate coding. IEEE

Trans. Inform. Theory, 24(5):530–536, 1978.

