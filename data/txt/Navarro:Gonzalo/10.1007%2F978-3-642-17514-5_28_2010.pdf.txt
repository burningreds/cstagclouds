Entropy-Bounded Representation of Point Grids

Arash Farzan1, Travis Gagie2,(cid:2), and Gonzalo Navarro2,(cid:2)

1 Max-Planck-Institut f¨ur Informatik

afarzan@mpi-inf.mpg.de

2 Department of Computer Science, University of Chile

{tgagie, gnavarro}@dcc.uchile.cl

Abstract. We give the ﬁrst fully compressed representation of a set of m
(cid:3)
points on an n× n grid, taking H + o(H) bits of space, where H = lg
n2
m
is the entropy of the set. This representation supports range counting,
range reporting, and point selection queries, with a performance that
is comparable to that of uncompressed structures and that improves
upon the only previous compressed structure. Operating within entropybounded 
space opens a new line of research on an otherwise well-studied
area, and is becoming extremely important for handling large datasets.

(cid:2)

1 Introduction

A point grid is an extremely basic structure underlying the representation of
two-dimensional point sets, graphics, spatial databases, geographic data, binary
relations, graphs, images, and so on. It has been intensively studied from a
computational geometry viewpoint, where most of the focus has been on two
basic primitives: (orthogonal) range counting (how many points are there in
this rectangle?), and (orthogonal) range reporting (list the points falling within
this rectangle). More sophisticated queries are possible if points have associated
values, and also more general shapes than rectangles have been considered.
Consider a n × n grid containing m points. Currently the best results related 
to the focus of this paper are as follows. Range counting can be done in
(cid:5)
time O(cid:4)
and linear space, that is, O(m) integers [12]. The preprocessing
√
time is O(cid:2)
[5]. That counting time cannot be improved within space
lg m
O(m polylog(m)) [17]. Range reporting can be done in time O(lg lg m + k), where
k is the number of points reported, using O(m lg m) integers for any constant
 > 0 [1]. The time raises to O(lg lg m(lg lg m + k)) if the space is reduced to
O(m lg lg m) integers [1], and it reaches O(cid:4)
if the space is linear
[15]. There are also some bounds that may be relevant when many points are
to be reported: O(lg m + k lg lg(4m/k)) time using O(m lg lg m) integers, and
O(lg m + k lg(2m/k)) time using O(m) integers [6]. Some of these results have
been matched even in the dynamic scenario [14].

lg lg m + k lg m
lg m

(cid:5)

lg m
lg lg m
m

(cid:3)

Many of the application areas for this problem handle huge volumes of information,
 and thus even the “linear” space structures might be excessively large.
(cid:2) Partially funded by Fondecyt Grant 1-080019, Chile.

O. Cheong, K.-Y. Chwa, and K. Park (Eds.): ISAAC 2010, Part II, LNCS 6507, pp. 327–338, 2010.
c(cid:2) Springer-Verlag Berlin Heidelberg 2010

328

A. Farzan, T. Gagie, and G. Navarro

(cid:5)

(cid:5)

Since each point requires storing two coordinates, the space complexities above
should be multiplied by 2 lg n bits. Unless m is very small, even storing these
bare coordinates uses much more space than necessary, and moreover the constants 
hidden within the O(...) notation are not negligible. Some succinct data
structures have been designed using, for example, m lg m + o(m lg m) bits when
m = n, answering range counting queries in time O(cid:4)
and reporting in
time O(cid:4)

[4]. That space is not the best possible when m is larger
than n. A (worst-case) lower bound on the number of bits needed to represent
a grid is the logarithm of the number of possible grids, called the “entropy”
H = lg

m + O(m + lg n).

(k + 1) lg m
lg lg m

= m lg n2

To the best of our knowledge, the only previous work achieving this “com-
pressed” space is by Barbay et al. [2]. They propose a data structure using
H + o(H) + O(m) + o(n) bits (see our Thm. 2). Within this space they solve
many interesting range counting, range reporting, and point selection queries
(give the kth point in a rectangle, according to some order) in O(lg n) time
per delivered point. Contrarily to succinct indexes, they propose an integrated
encoding, where the data and the index are stored together.

lg m
lg lg m

(cid:2)
n2
m

(cid:3)

(cid:5)

lg n2

(cid:2)
n2
m

time, range reporting in time O(cid:4)

In this paper we push further in the direction of storing the grid data within
its entropy bound, improving simultaneously the space redundancy and the time
performance of queries. Most notably, we achieve a fully compressed representation 
taking H + o(H) bits of space, while supporting the operations in constant
time in some cases. Depending on m, we use diﬀerent data structures to achieve
this goal. The result is summarized in Thm. 1; see Section 2.2 for more details.
Theorem 1. An n×n grid with m points can be represented within H+o(H) bits
(cid:3)
, so that orthogonal range counting can be answered
of space, where H = lg
in O(cid:4)
per delivered datum, and
point selection queries in at most O(cid:2)
time. Depending on the density of
the matrix the times are reduced down to O(1) per delivered datum.
The paper is organized as follows. Section 2 gives basic concepts on bitmaps and
point grids, deﬁnes the problems we address, proves some technical results needed
later, and summarizes the results we achieve. Section 3 describes a “compressed”
representation taking H + o(n2) bits of space and achieving constant time for
range counting and reporting. Section 4 achieves the “fully compressed” space, in
exchange for higher query times, and ﬁnishes with the proof of Thm. 1. Section 5
concludes and gives further research directions.

lg2 n2

lg2 n

(cid:5)

(cid:3)

m

m

2 Basic Concepts

2.1 The One-Dimensional Case

The one-dimensional variant of the problem, i.e., on a bitmap B[1, n], has been
(cid:3)
long studied. Let B have m bits set, then the entropy of the bitmap is H = lg
.

(cid:2)
n
m

Entropy-Bounded Representation of Point Grids

329

(cid:5)

All the range counting, range reporting, and point locating queries can be solved
in terms of two primitives: rankb(B, i) is the number of occurrences of bit b in
B[1, i], and selectb(B, j) is the position in B of the jth occurrence of bit b.

lg n

m + n lg lg n

Clark [8] and Munro [13] showed that both rank and select can be solved
in constant time using n + o(n) bits of space, that is, B itself plus sublinear
space. Golynski et al. [9] showed that the o(n) term must be Ω( n lg lg n
lg n ) if B
is stored in plain form, and moreover achieved this bound. Raman et al. [19]
provided a compressed representation retaining constant query times and taking
H + O(cid:4)
bits. We prove now a technical lemma we will need later.
Lemma 1. Let 0 < α ≤ 1 be a constant and b = Θ(lgα n). Let bitmap B[1, n]
be stored in a way such that we can only access pieces B[(i − 1) · b + 1, i · b] at
a time, for any i. Then we can perform rank and select in constant time using
O(cid:4)
Proof. Let us take any algorithm achieving constant time and O(cid:4)

extra
space, say Golynski’s [9], and adapt it to this restriction. The algorithm builds
and uses several indexes and accesses B a constant number of times. Each such
time, it reads a “word” of w = O(lg n) consecutive bits of B, in order to either
(a) count the number of 1s in a part of the word or (b) ﬁnd the position of the
kth 1 or 0 in a part of the word, using universal tables.

bits of extra space, and this is optimal.

n lg lg n

n lg lg n

lg n

(cid:5)

(cid:5)

b

b

(cid:5)

We introduce an indirection when accessing such universal tables. Each word
is covered by w
b pieces. For each piece, we store the summary number of 1s in the
(cid:5)
piece. This requires lg(b + 1) bits, so the total space is O(cid:4)
.
Moreover, in a RAM machine with word size w we can read all the summary
numbers of the pieces covering any word in O(1) accesses, as they add up to
b = o(lg n) bits. With these summary numbers we can index a universal table
(cid:3)
of size O(cid:2)
2o(lg n)polylog(n)
, telling (a) the number of bits set up to any given
piece of the word, and (b) the piece where the kth 0/1 of the word occurs. A ﬁnal
(cid:3)
access to one b-bit piece, with another universal table of size O(cid:2)
,
completes the query in constant time.

2bpolylog(n)

= O(cid:4)

n lg lg n

w lg b

n lg b

The lower bound comes directly from Golynski [9], who states that if one
probes t bits and answers rank/select in constant time, then the index must be
of size Ω( m lg t
). In the worst case m = n and the algorithm can access at most
(cid:4)(cid:5)
t = O(b) bits in constant time.

b

t

2.2 Two Dimensions
We can identify a grid with a binary matrix. We will consider rectangular ranges
of the form (i1, i2)× (j1, j2), where i1 and i2 are rows and j1 and j2 are columns.
Over those ranges we deﬁne the queries
– rank(i1, i2, j1, j2) counts the number of points in the range; and
– select(i1, i2, j1, j2, k1, k2) gives the k1th to the k2th points in the range, in
row-major or column-major order (this generalizes from range reporting and
point selection queries).

330

A. Farzan, T. Gagie, and G. Navarro

The general case is called a 4-sided query. A particular case, a 3-sided query,
arises when one of the coordinates is always 1 or n. A 2-sided query arises when
two of the coordinates, one of row and one of column, is always 1 or n. A bandquery 
has 1 and n for either the row or the column coordinates. Finally, a 1-sided
query has only one coordinate diﬀerent from 1 or n.
Since rank(i1, i2, j1, j2) = rank(1, i2, 1, j2) − rank(1, i1 − 1, 1, j2) − rank(1, i2, 1,
j1 − 1) + rank(1, i1 − 1, 1, j1 − 1), we study only 2-sided queries for rank. For
compliance with the existing literature, we prefer to study the queries in terms
of selecting the kth point, select(i1, i2, j1, j2, k), and reporting (up to) k points in
a range, report(i1, i2, j1, j2, k). Our solutions, however, can actually be combined
to solve the general select(i1, i2, j1, j2, k1, k2) query within the time of selecting
the k1th point and then reporting the k2 − k1 + 1 points following it. Furthermore,
 our rank query is never slower than our select, and select(i1, i2, j1, j2, k) =
select(i1, i2, 1, n, k + x) with x = rank(i1, i2, 1, j1 − 1) if select delivers in columnmajor 
order, and analogously in row-major order. Finally, our sublinear-sized
indexes can be computed (or the algorithms trivially modiﬁed) for several rotations 
and reﬂections of the grid within the same asymptotic space. Therefore we
can, without loss of generality, focus our study on the following queries:
– rank(i, j) is the number of points in (1, i) × (1, j);
– select(i1, i2, k) gives the kth point in the range (i1, i2) × (1, n), in column-
– select(i, k) gives the kth point in the range (1, i) × (1, n), in column-major
– report(i1, i2, j1, k) gives the ﬁrst (up to) k points in the range (i1, i2)×(j1, n),

major order (as explained this allows one to emulate any 4-sided query);

order (this allows one to emulate any 3-sided query); and

in column-major order.

Barbay et al. [2] propose a number of primitives on binary matrices, yet several
can be reduced to others. Their maximal operations (in the sense that the others
reduce to a constant number of applications of these) are rel rnk (equivalent
to our rank), rel sel obj maj and rel sel lab maj (equivalent to our select),
and lab rnk and obj rnk (which count the number of nonempty rows/columns
within a range and have no equivalent in this paper). By using wavelet trees [10],
they achieve the following result (adapted and ﬁxed here):

Theorem 2 ([2]). A binary matrix of σ rows (“labels”) by n columns (“ob-
jects”) with t 1s can be represented within H + o(H) + O(cid:4)
bits,
so that queries rel rnk(i1, i2, j1, j2) (number of points in (i1, i2) × (j1, j2)),
rel sel lab maj(i, k, j1, j2) (kth point, in label-major order, in (i, σ)× (j1, j2)),
and rel min obj maj(i1, i2, j) and rel acc obj maj(i1, i2, j) (ﬁrst and successive 
points, in object-major order, in (i1, i2)× (j, n)), can be answered in O(lg σ)
time per delivered datum. Rel sel obj maj(i1, i2, j, k) (kth point, in object-major
order, in (i1, i2) × (j, n)), can be carried out in O(lg σ lg n) time.

t + n lg lg n

lg n

(cid:5)

Entropy-Bounded Representation of Point Grids

331

Table 1. Space and time complexities achieved by Barbay et al. [2] and in this paper.
The “or” case depends on using row-major or column-major order. The times for
; otherwise Thm. 3 takes over. The

Thm. 4 are simpliﬁed assuming m = O(cid:4)

(cid:5)

n2
1/4 n

lg

times to operate on 0s are the same for Thm. 3; for Thm. 4 we give them explicitly.

Source
Thm. 2 [2] H + o(H) + O(m) + o(n)
Thm. 3

H + O(cid:4)

Space

(cid:5)

lg lg n
1/4 n

n2
lg

H + o(H) + O(m + lg n)

Thm. 4
Thm. 4 (0s)
Thm. 1

rank time

lg n
1
lg n2
lg n2
lg n2

m

m

m

report time
(k + 1) lg n

k + 1

m

(k + 1) lg n2
(k + 1) lg2 n2
m
(k + 1) lg2 n

select time (4-sided)

select time (3-sided)

H + o(H)

lg n or lg2 n

lg n

lg n + lg2 n2

m

lg2 n

Source
Thm. 2 [2]
Thm. 3
Thm. 4
Thm. 4 (0s)
Thm. 1

lg n or lg n + lg2 n2

m lg n2

lg n
lg lg n

m or lg n + lg2 n2
lg n + lg2 n2

m

m

lg2 n

(cid:3)
min(t, n lg t
” should indeed be t lg σ + o(t) lg σ + lg
n)
(cid:5)
(n+t) lg lg(n+t)
. Since the last three terms are t lg n

Proof. This is in their Thm. 2 [2]. Their space formula “t lg σ + o(t) lg σ +
O(cid:2)
+ O(min(n, t)) +
(cid:5)
O(cid:4)
, we have
(cid:5)
. (cid:4)(cid:5)
the total t lg nσ

= H + o(H) + O(cid:4)

lg n
t + n lg lg n

t
t + n lg lg n

t +O(cid:4)

t + n lg lg n

t + O(cid:4)

t + o(t) lg nσ

(cid:3)
(cid:2)
n+t

lg(n+t)

(cid:5)

lg n

lg n

Table 1 compares the previous and new complexities achieved for our operations.
 The previous compressed representation [2] achieves H + o(H) bits only if
ω( n lg lg n
lg2 n ) = m = o(n2) (note our m is their t). Also, it always supports rank in
time O(lg n). This time is O(1) in our “compressed” solution, and in our “fully
compressed” solution it is O(cid:4)
. This is never
worse than the previous result [2], and is strictly better if m = n2−o(1). For report
and select we are faster or slower depending on the case.

in the range m = O(cid:4)

lg n2

lg1/4 n

n2

(cid:5)

(cid:5)

m

3 A Compressed Representation

We ﬁrst describe a solution using n2 + o(n2) bits, and then convert it into one
using H + o(n2) bits.

3.1 Constant-Time Rank
The matrix is ﬁrst subdivided into superblocks of size s × s, s = lg2 n. Each
superblock is in turn subdivided into blocks of size b × b, b =
lg n
2 . The n2 bits

(cid:6)

332

A. Farzan, T. Gagie, and G. Navarro

of the matrix will be stored block-wise, that is, the b2 = lg n
will be stored contiguously.

2 bits of each block

(cid:5)

n2
lg n
lg2 n

store all rank(i, s · j) and rank(s · i, j) values. This requires O(cid:4)

For each superblock in the matrix, we store the rank values at all the positions
of the rightmost column and bottom row of the superblock. In other words, we
= o(n2)
bits. For each block within each superblock, we store the local (i.e. within its
superblock) rank values at all the positions of the rightmost column and bottom
row of the block. If we call ranks those local rank values, what we store is all

ranks(i, b · j) and ranks(b · i, j) values. This requires O(cid:4)
= o(n2) bits.
This gives enough information to compute rank(i, j) in constant time. Let
i = s · is + irs and j = s · js + jrs, so that s · is and s · js are the projections
of i and j to the last superblock-aligned row and column, and 0 ≤ irs, jrs < s
are the local positions within their superblock. Similarly, let irs = b· ib + irb and
jrs = b · jb + jrb, with 0 ≤ irb, jrb < b the projections into, and local coordinates
within, the blocks. Then it is easy to verify that

n2
√
lg lg n
lg n

(cid:5)

rank(i, j) = rankb(i, j)

+ ranks(i, b · jb) + ranks(b · ib, j) − ranks(b · ib, b · jb)
+ rank(i, s · js) + rank(s · is, j) − rank(s · is, s · js),

where rankb(i, j) is the local rank value within its block. All the rank and ranks
√
values in the formula are stored. As for rankb(i, j), this is rank(irb, jrb) within its
block. As there are only 2b2 =
√
n diﬀerent blocks, we can store all the answers
to all possible (local) rank queries within O(
n polylog(n)) = o(n) bits. Since
we can read at once the b2 = O(lg n) bits of the block (stored contiguously as
explained), we can look up a table entry in constant time.

3.2 Constant-Time Report
We ﬁrst solve a subproblem that might have independent interest. Given a row
range [i1, i2] and a column j, nextCol(i1, i2, j) is the smallest column number
j(cid:4) > j that is nonempty (i.e., contains a 1) in the range [i1, i2]. We now show
how to support this query in constant time and o(n2) extra space.

The key idea is to keep signature bit vectors which represent the bitwiseor 
of various contiguous ranges of matrix rows. First we divide the rows into
batches of s = lg2 n rows. Akin to the classical solution to range minimum
queries (RMQs) [3], we explicitly store bit vectors of length n which are the or
of batches i to i + 2k − 1 for all 1 ≤ i ≤ n/s, 0 ≤ k ≤ lg(n/s). Furthermore,
structures. This requires O(cid:4)
we enhance these bit vectors with one-dimensional constant-time rank and select
= o(n2) bits and reduces the query
nextCol(s · i1, s · i2 − 1, j) to that of ﬁnding the next 1 after position j in either
of two bit vectors (the one or-ing batches i1 to i1 + 2k − 1 and the one oring 
batches i2 − 2k to i2 − 1, for k = (cid:6)lg(i2 − i1)(cid:7)). Finding the next 1 in
a bit vector is easily reduced to one-dimensional rank and select queries, j(cid:4) =
select1(B, rank1(B, j) + 1).

· n lg n

lg2 n

(cid:5)

n

Entropy-Bounded Representation of Point Grids

333

n

(cid:5)

lg1/4 n

= o(n2).

(cid:5)
· n lg(lg2 n)

A general range [i1, i2] may contain several batches, plus possibly two withinbatch 
areas at each extreme. Thus we have reduced the problem to within batches
of size lg2 n. We now repeat the partition similarly within each batch. We divide
the rows into chunks of d = lg1/4 n rows and again use the same machinery to
isolate the problem to within chunks. The extra space for all the chunk-level bit
vectors is O(cid:4)
Now, conﬁned within a chunk of d rows, we consider bit vectors B(i1, i2),
1 ≤ i1, i2 < d, such that B(i1, i2) is the or of rows from i1 to i2. We cannot
explicitly store all these vectors, as the space would be ω(n2). However, we do
explicitly store the rank and select indexes for each such bit vector. To simulate
access to the virtual bit vector B(i1, i2), we use our b × b matrix blocks stored
contiguously, in order to provide in constant time any O(cid:2)√
bits of any
horizontal strip of width i2 − i1 + 1. By Lemma 1, we can in this case achieve
constant time for rank and select using extra indexes of size O(cid:4)
As there are O(cid:4)
(lg1/4 n)2
B(i1, i2), the total space is O(cid:4)
(cid:5)

chunks, each storing O(cid:4)
lg n · n lg lg n√

= o(n2) bits. The
nextCol(i1, i2, j) query is thus solved by consulting at most 2 batch bitmaps, 4
chunk bitmaps, and 2 (virtual) B(i1, i2) bitmaps. With rank and select on each,
we easily ﬁnd the next 1 after position j across the 8 bit vectors, in constant
time.

indexes for

n lg lg n√
(cid:5)
lg n

· √

n

lg1/4 n

n

lg1/4 n

lg n

(cid:3)

lg n

(cid:5)

.

Once nextCol is solved, it is easy to address report(i1, i2, j1, j2, k) queries. We
store one-dimensional rank and select indexes for every column of the matrix. As
already explained, their extra space adds up to O(cid:4)
= o(n) per column
as we can access only O(cid:2)√
contiguous bits of any column. The ﬁrst points
to report are at column j = nextCol(i1, i2, j1 − 1). With one-dimensional rank
and select on column j, we can report the points at rows [i1, i2] of that column,
each in constant time. We go on with j = nextCol(i1, i2, j), and so on, until
either j > j2 or we have reported k points. Thus the query takes time O(k + 1).

n lg lg n√
lg n

lg n

(cid:5)

(cid:3)

3.3 Select Queries

For select(i1, i2, k) we binary search, using rank, the position of the kth point in
O(lg n) time. We can do better for the simpler select(i, k) query. We have already
stored the rank values at the rightmost columns of the superblocks. Assume these
values are organized row-wise, and moreover in a y-fast trie data structure [20].
This sums to O(cid:4)
= o(n) bits per row. The trie for row i permits ﬁnding the
superblock column containing the kth point in (1, i) × (1, n), in O(lg lg n) time
(by ﬁnding the successor of k). Now a binary search over lg2 n values gives, in
another O(lg lg n) time, the precise column, and one-dimensional rank and select
on the column give the position of the kth point. Thus the time is O(lg lg n).

n lg n
lg2 n

(cid:5)

334

A. Farzan, T. Gagie, and G. Navarro

3.4 Entropy-Bounded Space
We have assumed the b × b blocks are explicitly stored. Instead, we can replace
them by a (c, o) pair, just as Raman et al. [19] do for one-dimensional bit vectors.
Let a block contain m 1s. Then its class c is m and its oﬀset o is the index of this
particular b × b block among all the diﬀerent blocks of class m. A table indexed
√
by c and o storing the contents of all the possible bit vectors takes O(
n lg n)
bits, thus we can recover any block content in constant time.
Each c value is stored in lg(b2+1)=O(lg lg n) bits, adding up to O(cid:4)

=
o(n2) bits in total. The number of bits required for all the o ﬁelds, assuming the
[19]. Finally,
rth block contains mr bits set, is
we also need pointers to ﬁnd in constant time an o ﬁeld, as these have variablebits 
[19] with

length representations. This can also be done within O(cid:4)

(cid:3)(cid:9) ≤ lg

+ O(cid:4)

n2
lg n
(cid:5)

(cid:2)
b2
mr

lg lg n
lg n

r(cid:8)lg

(cid:2)
n2
m

(cid:7)

n2

n2

(cid:5)

(cid:5)

(cid:3)

lg lg n
lg n

(cid:3)

(cid:5)

(cid:2)
n2
m

n2
lg lg n
lg1/4 n

techniques akin to one-dimensional rank.
Theorem 3. A n × n matrix with m 1s and entropy H = lg
can be represented 
within H + O(cid:4)
bits, so that operation rank(i, j) is computed
in O(1) time, report(i1, i2, j1, j2, k) performs in time O(k + 1), select(i1, i2, k) is
supported in O(lg n) time, and select(i, k) is computed in O(lg lg n) time.
Note that we can deﬁne the complementary queries, where 0s are considered
instead of 1s. This is obvious for rank but not for report nor select. It is not hard
to see that we can support in addition these complementary queries, by adding
other similar o(n2) bits of space, that is, asymptotically for free. As explained,
we can also support the select variants where rows and columns are exchanged,
within o(n2) additional space.

4 A Fully-Compressed Representation

Our compressed representation achieves entropy-bounded space for the matrix
itself, but the extra space is o(n2). This may dominate the entropy bound H.
The key to achieving indexes sublinear in H is to adapt the partitioning into
superblocks and blocks to the number of bits set in the matrix. The price will
be superconstant time for all queries, due to our internal usage of Thm. 2.

4.1 Rank Query
We ﬁrst divide the matrix into superblocks of size s × s, where now s = n2
lg m
m
(assume for now m = Ω(n lg m); we consider the other case later in Section 4.4).
The superblocks are further divided into blocks of size b × b, for b = n2
lg lg m
m .
Just as for Section 3.1, we store absolute ranks at the borders of superblocks
and local ranks at the borders of blocks. As the former require lg m bits to be
represented, they add up to O(m) bits. The latter require lg s2 bits per datum,
adding up to O(cid:4)

= o(H) + O(m + lg n).

= O(cid:4)

b · lg s2

lg lg m · lg n2

lg m
m

n2

(cid:5)

(cid:5)

m

Entropy-Bounded Representation of Point Grids

335

As before, the problem is reduced to supporting local rank within a block of
size b2. We store each block using the wavelet tree of Thm. 2, which for the rth
(cid:3)
) + O(mr) + o(b).1 It answers
block with mr bits set requires lg
. Added over all the blocks, the space

rank in time O(lg b) = O(cid:4)

(cid:2)
b2
mr

(cid:2)
b2
mr

+ o(lg

m + lg lg lg m

lg n2

(cid:5)

(cid:3)

(cid:3)

(cid:2)
n2
m

is lg

+ o(lg

(cid:2)
n2
m

(cid:3)
) + O(m) + o(m) = H + o(H) + O(m).

(cid:3)

lg s
b

= O(lg lg m) to binary search for the block.

4.2 Select Queries
As we have stored all the values rank(i, s · j), rank(s · i, j), and ranks(i, b · j), we
can compute any rank(i1, i2, b · j1, b · j2) in constant time. Thus we can binary
search for the column of blocks where the kth point of (i1, i2) × (1, n) lies. This
takes time O(cid:2)
(cid:3)
lg n
. For 3-sided queries we can arrange the superblock ranks
of each row in a y-fast trie as before, so as to pay O(lg lg m) time to ﬁnd the
b
superblock, plus O(cid:2)
Let jb be the column of blocks found, then the local rank of the (globally)
kth point, within block-column jb, is k(cid:4) = k − rank(i1, i2, 1, b · (jb − 1)). Now we
reﬁne the search to ﬁnd the exact column where the answer lies. A general way
to do this is to carry out a binary search within columns [b · (jb − 1) + 1, b · jb]
using rank. This rank is not constant-time because we are not in borders of
blocks. Hence the time raises to O(cid:2)
. Once we
know the precise column j, we must ﬁnd the k(cid:4)(cid:4)th point in it, within rows
[i1, i2], for k(cid:4)(cid:4) = k − rank(i1, i2, 1, j − 1). We ﬁrst binary search the block verti-
(cid:3)
cally in time O(cid:2)
, since we can compute any rank(b · i, j) value in constant
time. Finally, conﬁned within a block, we report the correct point in O(cid:2)
(cid:3)
lg2 b
time using rel sel obj maj on the wavelet tree of the block (or O(lg b) using
rel sel lab maj, depending on the orientation).

m + (lg lg lg m)2

= O(cid:4)

(cid:3)
lg2 b

lg2 n2

lg n
b

(cid:5)

A smarter way, but one which applies only to one direction (that is, we cannot 
have simultaneously the improved result for queries select(i1, i2, 1, n, k) and
select(1, n, j1, j2, k)), is to arrange the block contents in a diﬀerent way. Instead
of using one wavelet tree structure per block, we pack a whole block column per
wavelet tree, taking the b columns as their σ “labels” and the n rows as their
“objects” (recall Thm. 2). So the rank times are still O(lg σ) = O(lg b), and the
within-block rank used in Section 4.1 can still be carried out within this time.
Furthermore, ﬁnding the k(cid:4)th point, in label-major order, between objects i1
and i2 (operation rel sel lab maj), also takes O(lg b) time.

Therefore, if the band of our query is horizontal and we have stored block
columns in wavelet trees (or vice versa), we ﬁnd the kth point within time

(cid:5)

O(lg n) (4-sided select) or O(cid:4)

lg lg m + lg n2

m

(3-sided select).

4.3 Range Reporting
Let us ﬁrst assume that the query band is horizontal and we have stored rows
of blocks in wavelet trees. To solve report(i1, i2, j1, j2, k), we ﬁrst need to ﬁnd

(cid:3)

(cid:2)

b2
mr

1 The o(lg

) term is asymptotic in b, which is ω(1) in terms of n, so it can be

safely added up over many blocks later.

336

A. Farzan, T. Gagie, and G. Navarro

n

n

b

(cid:3)

s

the next column after j1 − 1 that is nonempty in the range [i1, i2]. We use the
same RMQ-akin idea of Section 3.2. We form batches storing the or of ranges of
rows of superblocks, for a total space of O(cid:2)
= O(m) bits, and chunks
storing the or of ranges of rows of blocks within superblocks, for a total space of
O(cid:2)
= O(m) bits. Instead of the virtual B(i1, i2) bitmaps, to ﬁnd the
next 1 in the band (i1, i2) × (j + 1, n) we use the horizontally arranged wavelet
trees. They ﬁnd this point using a rel min obj maj query, in O(lg b) time.

s · n lg n

b · n lg s

(cid:3)

Now we must be able to ﬁnd the 1s in the current column j, before proceeding
to the next one. Those 1s within the ﬁrst block since global row i1 are easily
found with rel acc obj maj in the wavelet tree of the block. In order to ﬁnd
the next block downwards containing points in column j, we store a signature
bit vector Bj[1, n/b] for each column j, so that Bj[i] = 1 iﬀ there is a 1 in the
range (i · (b − 1) + 1, i · b) × (j, j) of the matrix. Using one-dimensional rank and
select on the Bj vectors, we can easily ﬁnd the next block downwards that has
a 1 in the current column, in constant time. All the points in column j mod b of
that block are then reported (unless they exceed the global row i2). Bit vectors
Bj require O(cid:4)

= o(m) bits of space in total.

(cid:5)

n2
b

If, instead, the band of the query is orthogonal to that of wavelet trees, we
proceed as follows. The wavelet tree covering column j1 can deliver all the
points within rows [i1, i2] since column j1 onwards, in O(lg b) time each, using 
rel sel lab maj. We must now ﬁnd the next nonempty block. We build a
reduced matrix of n × (n/b), so that its cell (i(cid:4), j(cid:4)) contains a 1 iﬀ the original
matrix contains a 1 in (i(cid:4), b· (j(cid:4) − 1) + 1)× (i(cid:4), b· j(cid:4)). Then the block column that
is nonempty in [i1, i2] in the original matrix corresponds to the next column that
is nonempty in [i1, i2] in the reduced matrix. We can then apply the technique
of Section 3.2, creating the batch and chunk bitmaps over the reduced matrix,
taking overall space O(cid:4)
= o(m). Once we arrive at the next nonempty block

(cid:5)

n2
b

column, its wavelet tree delivers its points in order, and so on.

4.4 The Final Result

lg2 n

n lg lg n

A missing piece is to cover the case m = o(n lg m) = o(n lg n). When the matrix
is so sparse, lg n2
m = Θ(lg n), and thus it is preferable to use the wavelet tree by
itself. The only problem is the o(n) extra space (see Thm. 2), which does not
(cid:5)
ﬁt in o(H) whenever m = O(cid:4)
. This is because Barbay et al. [2] chose
Raman et al.’s representation [19] to achieve constant-time rank and select on
a bit vector. By using instead a binary searchable representation [11], the o(n)
term becomes o(H)+O(m + lg n) and the O(lg σ) time becomes O(lg σ + lg m).
This is O(lg n) for us, which ﬁts perfectly within our general result.
(cid:3)
Theorem 4. A n × n matrix with m 1s and entropy H = lg
can be stored
in H + o(H) +O(m + lg n) bits, so that operation rank(i, j) is computed in O(τ)
time and report(i1, i2, j1, j2, k) in time O((k + 1)τ), where τ = lg n2
m + lg lg lg m.
In one direction (that can be chosen), select(i1, i2, k) is computed in O(lg n)

n2
m

(cid:2)

Entropy-Bounded Representation of Point Grids

337

m

(cid:3)

n2

time.

lg n + τ 2
(cid:5)

, then τ + lg lg m = O(cid:4)

time, and select(i, k) in time O(τ + lg lg m). In the other direction, both select
operations cost O(cid:2)
Note that if m = O(cid:4)

; otherwise Thm. 3 is
better in all aspects. Finally, if we want to report or select 0s, we can replicate all
the extra structures considering the complemented matrix (still deﬁning s and
b in terms of 1s), within o(H) + O(m + lg n) bits . The wavelet trees, instead,
must internally binary search on rank to simulate their local operations, all of
(cid:3)
which (except rank itself) would now cost O(cid:2)
. Thus this complexity must be
added to the times given for report (per delivered datum) and any select.

lg n2

lg1/4 n

τ 2

(cid:5)

We now have all the necessary pieces to prove our main result.

(cid:5)

Proof (of Thm. 1). Putting together our “compressed” (Thm. 3) and “fully
compressed” (Thm. 4) solutions, the claimed time complexities are obtained. We
achieve H + o(H) bit space for all cases: (a) The O(cid:4)
extra bits of our
“compressed” solution is o(H) as long as ω( n2
lg1/4 n). (b) The
“fully compressed” solution uses H + o(H)+O(lg n) space as long as m = o(n2).
(c) These two cover the entire range for m except m = n2 − O(cid:4)
; there,
we complement the matrix and use the fully-compressed solution with 0-queries
instead of 1-queries. (d) The ﬁnal O(lg n) is not o(H) only if m = O(1), in which
case we can encode the points in diﬀerential form (e.g. δ-encoding), and answer
(cid:4)(cid:5)
queries in constant time by scanning the points.

lg1/4 n) = m = n2−ω( n2

n2
lg lg n
lg1/4 n

lg1/4 n

(cid:5)

n

5 Conclusions

(cid:4)

Although the area of orthogonal range queries has received much attention, the
extremely interesting case where the structure achieves entropy-bounded space
is largely under-explored. This work completes a large portion of the picture,
and hopefully opens the door to much further research.

(cid:5)
A ﬁrst line of future work is to ﬁnd and reach the lower bounds on the time

complexity under this new scenario. Existing lower bounds, such as Ω
counting time when using M words of memory [7], or one-dimensional lower
bounds [18] might be useful. Succinct-space results [4] suggest we can do better.
 Particularly intriguing is that select takes constant time on dense onedimensional 
bitmaps, whereas we have not achieved that in two dimensions. Interestingly,
 schemes that achieve entropy-bounded extra space in one dimension
(cid:3)
[16], oﬀer rank time analogous to ours, O(cid:2)
, yet their select is constant-time.
is a crude worst-case lower bound that does not
account for regularities, such as clusters of points, that arise in real life. Our
actual space is indeed much better: the sum of local entropies of small blocks.
An interesting future work challenge is to improve the lower order term, o(H).
Other natural directions for future work are to consider further operations [2],
extending to d-dimensional spaces, achieving dynamic compressed structures,
and secondary-memory variants.

As for space, H = lg

(cid:2)
n2
m

lg n
m

lg m
2M
lg
m

(cid:3)

338

A. Farzan, T. Gagie, and G. Navarro

Acknowledgements. We thank J´er´emy Barbay for discussions and proofreading.

References

1. Alstrup, S., Brodal, G., Rauhe, T.: New data structures for orthogonal range

searching. In: Proc. 41st FOCS, pp. 198–207 (2000)

2. Barbay, J., Claude, F., Navarro, G.: Compact rich-functional binary relation representations.
 In: L´opez-Ortiz, A. (ed.) LATIN 2010. LNCS, vol. 6034, pp. 172–185.
Springer, Heidelberg (2010)

3. Bender, M., Farach-Colton, M.: The level ancestor problem simpliﬁed. Theoretical

Computer Science 321(1), 5–12 (2004)

4. Bose, P., He, M., Maheshwari, A., Morin, P.: Succinct orthogonal range search
structures on a grid with applications to text indexing. In: Proc. 11th WADS, pp.
98–109 (2009)

5. Chan, T., P˘atra¸scu, M.: Counting inversions, oﬄine orthogonal range counting,

and related problems. In: Proc. 21st SODA, pp. 161–173 (2010)

6. Chazelle, B.: Filtering search: A new approach to query-answering. SIAM Journal

of Computing 15, 703–724 (1986)

7. Chazelle, B.: Lower bounds for orthogonal range searching: II. The arithmetic

model. Journal of the ACM 37(3), 430–463 (1990)

8. Clark, D.: Compact Pat Trees. PhD thesis, University of Waterloo, Canada (1996)
9. Golynski, A.: Optimal lower bounds for rank and select indexes. Theoretical Computer 
Science 387(3), 348–359 (2007)

10. Grossi, R., Gupta, A., Vitter, J.: High-order entropy-compressed text indexes. In:

Proc. 14th SODA (2003)

11. Gupta, A., Hon, W.-K., Shah, R., Vitter, J.S.: Compressed data structures: Dictionaries 
and data-aware measures. In: Proc. 16th DCC, pp. 213–222 (2006)

12. J´a J´a, J., Mortensen, C.W., Shi, Q.: Space-eﬃcient and fast algorithms for multidimensional 
dominance reporting and counting. In: Fleischer, R., Trippen, G. (eds.)
ISAAC 2004. LNCS, vol. 3341, pp. 558–568. Springer, Heidelberg (2004)

13. Munro, I.: Tables. In: Chandru, V., Vinay, V. (eds.) FSTTCS 1996. LNCS,

vol. 1180, pp. 37–42. Springer, Heidelberg (1996)

14. Nekrich, Y.: Space eﬃcient dynamic orthogonal range reporting. In: Proc. 21st

SCG, pp. 306–313 (2005)

15. Nekrich, Y.: Orthogonal range searching in linear and almost-linear space. Computational 
Geometry: Theory and Applications 42(4), 342–351 (2009)

16. Okanohara, D., Sadakane, K.: Practical entropy-compressed rank/select dictionary.

In: Proc. 9th ALENEX (2007)

17. P˘atra¸scu, M.: Lower bounds for 2-dimensional range counting. In: Proc. 39th

STOC, pp. 40–46 (2007)

18. P˘atra¸scu, M., Thorup, M.: Time-space trade-oﬀs for predecessor search. In: Proc.

38th STOC, pp. 232–240 (2006)

19. Raman, R., Raman, V., Srinivasa Rao, S.: Succinct indexable dictionaries with
applications to encoding k-ary trees and multisets. In: Proc. 13th SODA, pp. 233–
242 (2002)

20. Willard, D.: Log-logarithmic worst-case range queries are possible in space θ(n).

Information Processing Letters 17(2), 81–84 (1983)

