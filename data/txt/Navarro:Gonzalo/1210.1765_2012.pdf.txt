4
1
0
2

 
l
u
J
 

3
1

 
 
]
S
D
.
s
c
[
 
 

2
v
5
6
7
1

.

0
1
2
1
:
v
i
X
r
a

Frequency-Sensitive Queries in Ranges

DJAMAL BELAZZOUGUI, Department of Computer Science, University of Helsinki
TRAVIS GAGIE, Department of Computer Science, University of Helsinki
GONZALO NAVARRO, Department of Computer Science, University of Chile

A

Karpinski and Nekrich (2008) introduced the problem of parameterized range majority, which asks to preprocess 
a string of length n such that, given the endpoints of a range, one can quickly ﬁnd all the distinct
elements whose relative frequencies in that range are more than a threshold τ . Subsequent authors have
reduced their time and space bounds such that, when τ is given at preprocessing time, we need either
O(n lg(1/τ )) space and optimal O(1/τ ) query time or linear space and O((1/τ ) lg lg σ) query time, where σ
is the alphabet size. In this paper we give the ﬁrst linear-space solution with optimal O(1/τ ) query time.
For the case when τ is given at query time, we signiﬁcantly improve previous bounds, achieving either

O(n lg lg σ) space and optimal O(1/τ ) query time or compressed space and O(cid:0)(1/τ ) lg lg(1/τ )

Along the way, we consider the complementary problem of parameterized range minority that was recently
introduced by Chan et al. (2012), who achieved linear space and O(1/τ ) query time even for variable τ . We
improve their solution to use either nearly optimally compressed space with no slowdown, or optimally compressed 
space with nearly no slowdown. Some of our intermediate results, such as density-sensitive query
time for one-dimensional range counting, may be of independent interest.

lg w (cid:1) query time.

Categories and Subject Descriptors: X.0.0 [Data Structures]: How is the 2012 classiﬁcation supposed to
work?

General Terms: Arrays, Range Queries

Additional Key Words and Phrases: Parameterized range majority and minority, ...

ACM Reference Format:
Djamal Belazzougui, Travis Gagie, and Gonzalo Navarro, 2014. Frequency-Sensitive Queries in Ranges.
ACM Trans. Algor. V, N, Article A (January YYYY), 23 pages.
DOI:http://dx.doi.org/10.1145/0000000.0000000

1. INTRODUCTION
Finding frequent elements in a dataset is a fundamental operation in data mining.
 Finding the most frequent elements can be challenging when all the distinct
elements have nearly equal frequencies and we do not have the resources to compute 
all their frequencies exactly. In some cases, however, we are interested in the
most frequent elements only if they really are frequent. For example, Misra and
Gries [Misra & Gries 1982] showed how, given a string and a threshold τ with 0 <
τ ≤ 1, with two passes and O(1/τ ) words of space we can ﬁnd all the distinct elements 
in a string whose relative frequencies are at least τ . These elements are called
the τ -majorities of the string. Misra and Gries’ algorithm was rediscovered by Demaine,
 L´opez-Ortiz and Munro [Demaine et al. 2002], who noted it can be made to

This work is supported by somebody.
Authors’ addresses: D. Belazzougui and T. Gagie, Department of Computer Science, University of Helsinki;
G. Navarro, Department of Computer Science, University of Chile.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that
copies show this notice on the ﬁrst page or initial screen of a display along with the full citation. Copyrights
for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted.
 To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component
of this work in other works requires prior speciﬁc permission and/or a fee. Permissions may be requested
from Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212)
869-0481, or permissions@acm.org.
c(cid:13) YYYY ACM 1549-6325/YYYY/01-ARTA $15.00
DOI:http://dx.doi.org/10.1145/0000000.0000000

ACM Transactions on Algorithms, Vol. V, No. N, Article A, Publication date: January YYYY.

A:2

D. Belazzougui, T. Gagie and G. Navarro

run in O(1) time per element on a word RAM with Ω(lg n)-bit words, where n is the
length of the string, which is the model we use; it was then rediscovered again by
Karp, Shenker and Papadimitriou [Karp et al. 2003]. As Cormode and Muthukrishnan 
[Cormode & Muthukrishnan 2003] put it, “papers on frequent items are a frequent
item!”

Krizanc, Morin and Smid [Krizanc et al. 2005] introduced the problem of preprocessing 
the string such that later, given the endpoints of a range, we can quickly return
the mode of that range (i.e., the most frequent element). They gave two solutions, one

of which takes O(cid:0)n2−2ǫ(cid:1) space for any ﬁxed positive ǫ ≤ 1/2, and answers queries in
O(nǫ lg lg n) time; the other takes O(cid:0)n2 lg lg n/ lg n(cid:1) space and answers queries in O(1)
time. Petersen [Petersen 2008] reduced Krizanc et al.’s ﬁrst time bound to O(nǫ) for
any ﬁxed non-negative ǫ < 1/2, and Petersen and Grabowski [Petersen & Grabowski
2009] reduced the second space bound to O(cid:0)n2 lg lg n/ lg n(cid:1). Chan et al. [Chan et al.
2012b] recently gave a linear-space solution that answers queries in O(cid:16)pn/ lg n(cid:17) time.
They also gave evidence suggesting we cannot easily achieve query time substantially
smaller than √n using linear space; however, the best known lower bound, by Greve et
al. [Greve et al. 2010], says only that we cannot achieve query time o(cid:16) lg(n)/ lg(sw/n)(cid:17)

using s words of w bits each. Because of the difﬁculty of supporting range mode queries,
Bose et al. [Bose et al. 2005] and Greve et al. [Greve et al. 2010] considered the problem 
of approximate range mode, for which we are asked to return an element whose
frequency is at least a constant fraction of the mode’s frequency.

Karpinski and Nekrich [Karpinski & Nekrich 2008] took a different direction, analogous 
to Misra and Gries’ approach, when they introduced the problem of preprocessing 
the string such that later, given the endpoints of a range, we can quickly return
the τ -majorities of that range. We refer to this problem as parameterized range majority.
 Assuming τ is given when we are preprocessing the string, they showed how

we can store the string in O(n(1/τ )) space and answer queries in O(cid:0)(1/τ )(lg lg n)2(cid:1)
time. They also gave bounds for dynamic and higher-dimensional versions. Durocher
et al. [Durocher et al. 2013a] independently posed the same problem and showed how
we can store the string in O(n lg(1/τ + 1)) space and answer queries in O(1/τ ) time.
Notice that, because there can be up to 1/τ distinct elements to return, this time bound
is worst-case optimal. Gagie et al. [Gagie et al. 2011] showed how to store the string in
compressed space — i.e., O(n(H + 1)) bits, where H is the entropy of the distribution of
elements in the string — such that we can answer queries in O((1/τ ) lg lg n) time. They
also showed how to drop the assumption that τ is ﬁxed and simultaneously achieve optimal 
query time, at the cost of increasing the space bound by a (lg n)-factor. That is,
they gave a data structure that stores the string in O(n(H + 1)) space such that later,
given the endpoints of a range and τ , we can return the τ -majorities of that range in
O(1/τ) time. Chan et al. [Chan et al. 2012a] recently gave another solution for variable 
τ , which also has O(1/τ ) query time but uses O(n lg n) space. As far as we know,
these are all the relevant bounds for Karpinski and Nekrich’s original exact, static,
one-dimensional problem, both for ﬁxed and variable τ ; they are summarized in Table I
together with our own results. Related work includes Elmasry et al.’s [Elmasry et al.
2011] solution for the dynamic version and Lai, Poon and Shi’s [Lai et al. 2008] and
Wei and Yi’s [Wei & Yi 2011] approximate solutions for the dynamic version.

In this paper we ﬁrst consider the complementary problem of parameterized range
minority, which was recently introduced by Chan et al. [Chan et al. 2012a] (and then
generalized to trees by Durocher et al [Durocher et al. 2013b]). For this problem we
are asked to preprocess the string such that later, given the endpoints of a range, we
can return (if one exists) a distinct element that occurs in that range but is not one of

ACM Transactions on Algorithms, Vol. V, No. N, Article A, Publication date: January YYYY.

Frequency-Sensitive Queries in Ranges

A:3

Table I. Results for the problem of parameterized range majority on a string of length n over an
alphabet of size σ in which the distribution of the elements has entropy H.
time

variable τ

source

space

[Karpinski & Nekrich 2008]

O(n(1/τ )) words

[Durocher et al. 2013a]

O(n lg(1/τ )) words

O(cid:0)(1/τ )(lg lg n)2(cid:1)

O(1/τ )

no

no

no

no

yes

yes

yes

yes

yes

[Gagie et al. 2011]

Theorem 8.1

[Gagie et al. 2011]

[Chan et al. 2012a]

Theorem 9.1

Theorem 9.3

Theorem 9.4

O(n(H + 1)) bits

O((1/τ ) lg lg σ)

O(n) words

O(n(H + 1)) words

O(n lg n) words

O(n lg lg σ) words

O(1/τ )

O(1/τ )

O(1/τ )

O(1/τ )

nH + o(n)(H + 1) bits

O((1/τ ) lg lg σ)

(1 + ǫ)nH + o(n) bits O(cid:0)(1/τ ) lg lg(1/τ )
lg w (cid:1)

its τ -majorities. Such an element is called a τ -minority for the range. At ﬁrst, ﬁnding
a τ -minority might seem harder than ﬁnding a τ -majority because, e.g., we are less
likely to ﬁnd a τ -minority by sampling. Nevertheless, Chan et al. gave a linear-space
solution with O(1/τ ) query time even when τ is given at query time. In Section 7 we
give two results, also for the case of variable τ :
(1) for any positive constant ǫ, a solution with O(1/τ ) query time that takes (1+ǫ)nH +
(2) for any function f (n) = ω(1), a solution with O((1/τ ) f (n)) query time that takes

O(n) bits;
nH + o(n)(H + 1) bits.

That is, we improve Chan et al.’s solution to use either nearly optimally compressed
space with no slowdown, or optimally compressed space with nearly no slowdown. We
reuse ideas from this section in our solutions for parameterized range majority.

of the alphabet;

In Section 8 we return to Karpinski and Nekrich’s original problem of parameterized
range majority with ﬁxed τ and give the ﬁrst linear-space solution with worst-case
optimal O(1/τ) query time. In Section 9 we adapt this solution to the more challenging
case of variable τ and give three results:
(1) a solution with O(1/τ ) query time that takes O(n lg lg σ) space, where σ is the size
(2) a solution with O((1/τ ) lg lg σ) query time that takes nH + o(n)(H + 1) bits;
lg w (cid:17) query time that takes
(3) for any positive constant ǫ, a solution with O(cid:16)(1/τ ) lg lg(1/τ )
With (2), we can support O(1)-time access to the string and O(lg lg σ)-time rank and
select (see deﬁnitions in Section 2.1); with (3), select also takes O(1) time. While proving 
(3) we introduce a compressed data structure with density-sensitive query time for
one-dimensional range counting, which may be of independent interest. We will also
show in the full version how to use our data structures for (2) or (3) to ﬁnd a range
mode quickly when it is actually reasonably frequent. We leave as an open problem
reducing the space bound in (1) or the time bound in (2) or (3), to obtain linear or
compressed space with optimal query time.

(1 + ǫ)nH + o(n) bits.

2. PRELIMINARIES
2.1. Access, select and (partial) rank
Let S[1..n] be a string over an alphabet of size σ and let H be the entropy of the distribution 
of elements in S. An access query on S takes a position k and returns S[k];

ACM Transactions on Algorithms, Vol. V, No. N, Article A, Publication date: January YYYY.

A:4

D. Belazzougui, T. Gagie and G. Navarro

a rank query takes a distinct element a and a position k and returns the number of
occurrences of a in S[1..k]; a select query takes a distinct element a and a rank r and
returns the position of the rth occurrence of a in S. A partial rank query is a rank query
with the restriction that the given distinct element must occur in the given position;
i.e., S[k] = a. These are among the most well-studied operations on strings, so we state
here only the results most relevant to this paper.

For σ = 2 and any constant c, P ˇatras¸cu [P ˇatras¸cu 2008] showed how we can store
S in nH + O(n/ lgc n) bits. For σ = lgO(1) n, Ferragina et al. [Ferragina et al. 2007]
showed how we can store S in nH + o(n) bits and support access, rank and select in
O(1) time. For σ < n, Barbay et al. [Barbay et al. 2013] showed how, for any positive
constant ǫ, we can store S in (1 + ǫ)nH + o(n) bits and support access and select in O(1)
time and rank in O(lg lg σ) time. Alternatively, they can store S in nH + o(n)(H + 1)
bits and support either access or select in time O(1), and the other operation, as well
as rank, in time O(lg lg σ). Belazzougui and Navarro [Belazzougui & Navarro 2011]
showed how to support O(1)-time partial rank using O(n(lg H + 1)) bits; in the full
version of their paper [Belazzougui & Navarro n.d.] they reduced that space bound to
o(n)(H + 1) bits. In another paper, Belazzougui and Navarro [Belazzougui & Navarro
2012] showed how, for any function f (n) = ω(1), we can store S in nH + o(n)(H + 1)
bits and support access in O(1) time, select in O(f (n)) time and rank in O(lg lg σ)
time. They also proved, via a reduction from the predecessor problem, that we cannot
support general rank queries in o(lg(lg σ/ lg lg n)) time while using n lgO(1) n space.

2.1.1. Alphabet partitioning. The sequence representation of Barbay et al [Barbay et al.
2013] uses a technique called alphabet partitioning. The alphabet [1, σ] is partitioned
into at most ⌈lg2 n⌉ sub-alphabets. A character c occuring nc times will belong to subalphabet 
⌈lg(n/nc) lg n⌉. Then the sub-alphabet mapping table m[1..σ] is such that m[c]
stores the sub-alphabet to which character c ∈ [1, σ] belongs. The sequence t[1..n] over
[1.. lg2 n] is built from S by replacing every S[i] by the value m[S[i]] (replace every character 
of S by the sub-alphabet it belongs to). Finally subsequences Si[1..ti] of characters 
over alphabet [1..σi] are built as follows. Let σi, be the number of occurrences
of character i in m. For every value i ∈ [1.. lg2 n] start with an empty sequence Si,
scan the sequence S in left-to-right order and for every character c = S[k] such that
m[c] = i append c at the end of Si. Answering rank, access and select queries on the
original sequence S is now achieved through combinations of rank, access and select
queries on sequences m, t and Si. The sequences m and t are represented using zeroorder 
compressed multi-ary wavelet tree supporting constant time rank access and
selet queries [Ferragina et al. 2007]. The subsequences Si are represented either using 
a variant of Golynski et al.’s structure [Golynski et al. 2006] or Grossi et al’s result 
[Grossi et al. 2010] achieving O(lg lg σ) time for rank queries and either constant
time select and O(lg lg σ) time access (for the former) or O(lg lg σ) select and constant
time access (for the latter).

2.2. Coloured range listing
Motivated by the problem of document listing, Muthukrishnan [Muthukrishnan 2002]
showed how we can store S[1..n] such that, given the endpoints of a range, we can
quickly list the distinct elements in that range and the positions of their leftmost occurrences 
therein. This is the special case of one-dimensional coloured range listing in
which the points’ coordinates are the integers from 1 to n. Let C[1..n] be the array in
which C[k] is the position of the last occurrence of the distinct element S[k] in S[1..k−1]
— i.e., the last occurrence before S[k] itself — or 0 if there is no such occurrence. Notice 
S[k] is the ﬁrst occurrence of that distinct element in a range S[i..j] if and only

ACM Transactions on Algorithms, Vol. V, No. N, Article A, Publication date: January YYYY.

Frequency-Sensitive Queries in Ranges

A:5

if i ≤ k ≤ j and C[k] < i. We store C, implicitly or explicitly, and a data structure
supporting O(1)-time range-minimum queries on C that returns the position of the
leftmost occurrence of the minimum in the range.
To list the distinct elements in a range S[i..j] given i and j, we ﬁnd the position m of
the leftmost occurrence of the minimum in the range C[i..j]; check whether C[m] < i;
and, if so, output S[m] and m and recurse on C[i..m−1] and C[m+1..j]. This procedure is
online — i.e., we can stop it early if we want only a certain number of distinct elements
— and the time it takes per distinct element is O(1) plus the time to access C.
queries on S, all in O(t) time. Notice C[k] = S.selectS[k](cid:16)S.rankS[k](k) − 1(cid:17), so we can
also support access to C in O(t) time. Sadakane [Sadakane 2007] and Fischer [Fischer
2010] gave O(n)-bit data structures supporting O(1)-time range-minimum queries.
Therefore, we can implement Muthukrishnan’s solution using O(n) extra bits such
that it takes O(t) time per distinct element listed.

Suppose we already have data structures supporting access, select and partial rank

2.3. Minimal perfect hashing
Given a set X ⊂ [1..U ] such that |X| = n, a minimal perfect hash function (mphf for
short) is a bijective function from X onto [1..n]. It is well-knwon result that any general
scheme capable of representing an mphf for any given subset X of U of size n requires
exactly n lg2 e + Θ(lg lg U ) bits [Fredman & Koml´os 1984] to represent such an mphf.
This bound is almost achieved in [Hagerup & Tholey 2001] with a randomized linear
time construction and space n lg2 e + Θ(lg lg U ) + o(n) bits.

2.4. Monotone minimal perfect hashing
A monotone minimal perfect hash function (mmphf) is a mphf which in addition to
being bijective, is also monotone. That is given a set X ⊂ [1..U ] such that |X| = n,
an mmphf f over the set X is a bijective function from X onto [1..n] and such for any
pair (x, y) ∈ X 2 we have that f (x) < f (y) if and only if x < y. In [Belazzougui et al.
2009] two general schemes for generating mmphf representations were proposes. The
ﬁrst one allows query time O(1) and representation space O(n lg lg U ) bits. The second
allows query time O(lg lg U ) and uses space O(n lg lg lg U ).

2.5. Preﬁx sum data structures
Given an array A of n values that sum up to U , a preﬁx-sum data structure uses
answers to the following queries: given index i, return the sum of all the values of
indices ranging from 1 to i. It is possible to get a preﬁx sum that uses (n + U )(1 + o(1))
bits of space and that answers to queries as follows. Create a bivector V that contains
n ones and U zeros by scanning the original array and for each value A[i] append A[i]
zeros followed by a one. Then the preﬁx sum up to position i can be answered by a
select1(i) query on the vector V , ﬁninding the position of the jth one in the bitvector.
The answer is then the number of zeros that precede that position which is then j − i.
It is possible to improve the space of the above scheme to use only n(2 + ⌈lg(U/n)⌉) +
o(n) bits of space as follows. Build a bitvector that contains n + U/⌈U/n⌉ ≤ 2n bits,
where the ith one in the bitvector is preceded by ⌊((P1≤j≤i A[j])/⌈U/n⌉)⌋ zeros We
then build another vector B of n values of ⌈lg(U/n)⌉ bits each, where B[i] stores the
value (P1≤j≤i A[j]) − ⌊((P1≤j≤i A[j])/⌈U/n⌉)⌋. In other words, the bitvector V stores
the preﬁx sums divided by ⌈U/n⌉ and the vector B stores the values modulo ⌈U/n⌉. A
query is ansered by using a select operaton on V followed by reading one cell from B.

ACM Transactions on Algorithms, Vol. V, No. N, Article A, Publication date: January YYYY.

A:6

D. Belazzougui, T. Gagie and G. Navarro

2.6. Indexable dictionaries
Given a set X ⊂ [1..U ] with |X| = n, an indexable dictionary [Raman et al. 2007] is
data structure that uses O(n lg(U/n)) bits of space and that answers to membership
queries in constant time. A membership query asks given any element x ∈ U , whether
x ∈ X or not. In addition the dictionary associates a unique number to each element
in [1..n] to each element of X.

2.7. Predecessor data structures
Given a set X ⊂ [1..U ] with |X| = n, a predecessor data structure answers to the
following query. Given an element y ∈ U return the greatest element x ∈ X such
that x ≤ y. The y-fast trie [Willard 1983] achieves linear space O(n lg U ) bits with
query time O(lg lg U ). The rank of y is deﬁned as the number of elements of X no
greated than y. The rank of y being the same as that of its predecessor x, it is easy to
modify a (static) predecessor data structure to return the rank of the queried element
by explicitly storing the rank of every element x ∈ X and return it whenever element
x is returned.
A short-distance sensitive predecessor data structure for the set of points that, given
an element x ∈ U , returns the predecessor of x in O(lg lg d) time, where d is the
minimum of the distances from x to its predecessor and to its successor in S. The
ﬁrst such data structure was proposed by Johnson [Johnson 1981]. Then Bose et al.
[Bose et al. 2010, 2012], improved the space to O(n lg U lg lg lg U ) bits. Both Johnson
and Bose et al. solutions support insetions and deletions. Recently by Belazzougui et
al. [Belazzougui et al. n.d.] proposed a more space-efﬁcient static variant that uses
space O(n lg U ) bits only.
bound in [Chan & Wilkinson 2013], where adaptive counting time O(cid:16) lg(occ+1)
lower bounds for 2D and 1D range counting, which are respectively Ω(cid:16) lg(n+1)
Ω(cid:16)q lg(n+1)

achieved for the two-dimensional problem. We notice that both bounds converge to the

This bound can be considered as the one dimensional counterpart of the counting

lg lg(occ+1)(cid:17) was
lg lg(n+1)(cid:17) and

lg lg(n+1)(cid:17).

3. PREDECESSORS IN A RANGE
Assume we have a vector X that contains n elements from universe [1, U ] in sorted
order. We consider the problem of, given an interval [i, j] of the universe that does
contain elements of X, ﬁnding the predecessor of j (the answer can be arbitrary, even
wrong, if the interval contains no point of X). Our aim is to perform better when the
range is smaller. We start with a basic solution that requires time O(lg lg(j − i + 1))
and O(cid:0)n(lg lg U )2(cid:1) bits of space. Then we use it as a building block to design a more
elaborate variant that improves on both time and space.

3.1. A Simple Data Structure
Our data structure has lg lg U levels. At a level ℓ, we divide the universe into ⌈U/22ℓ
⌉
overlapping intervals, so that interval k will be [k · 22ℓ
]. We consider
separately the intervals with even and odd k (we call them even and odd intervals,
 respectively). For each of the two categories, the set of intervals will be disjoint.
 For each category, we use a mmphf Fℓ that stores the values k corresponding
to nonempty intervals, and a preﬁx sum data structure Bℓ to store the number of elements 
in each nonempty interval k. With Fℓ and Bℓ we map in constant time from
a nonempty interval k to its corresponding area in X (say p = Fℓ(k), then the area is

+ 1, (k + 2)22ℓ

ACM Transactions on Algorithms, Vol. V, No. N, Article A, Publication date: January YYYY.

Frequency-Sensitive Queries in Ranges

A:7

+ 1, (k + 2)22ℓ

X[sum(Bℓ, p − 1) + 1 .. sum(Bℓ, p)]). Since there are at most n nonempty intervals of
each category, Fℓ uses O(n lg lg U ) bits. Structure Bℓ uses O(n) bits.
In addition, for each nonempty interval with more than 2ℓ elements, we store a local
predecessor search data structure (lpsds). The lpsds of an interval samples one every
2ℓ elements in the interval and stores them in a local y-fast trie. The y-fast trie of
elements x ∈ [k· 22ℓ
− 1, and thus will range over
a universe of size O(cid:16)22ℓ(cid:17). Since they store, in total, O(cid:0)n/2ℓ(cid:1) elements over a universe
of size O(cid:16)22ℓ(cid:17), the space of all the lpsds adds up to O(n) bits at level ℓ.

] will store keys x− k· 22ℓ

Since a lpsds storing mr elements uses at most cmr bits, for some constant c, we
store them one after the other, reserving cmr bits for each lpsds storing mr elements.
We store a partial sum data structure Pℓ on the mr values (if there are less than 2ℓ
elements, then mr = 0 and no lpsds is stored). Then we can ﬁnd in constant time,
using sum on Pℓ, the starting point of each lpsds. Structure Pℓ uses at most O(n) bits.

Then, to carry out a predecessor search on interval [i, j], we proceed as follows:

(1) We compute ℓ = ⌈lg lg(j − i + 1)⌉, so that the query is for sure contained in an (even
(2) We use Fℓ to map k to its position p in the nonempty intervals, and then Bℓ to ﬁnd

or odd) interval k of level ℓ. Number k is found algebraically in constant time.

the corresponding range X[ik..jk]. This takes constant time.

(3) If jk − ik + 1 ≤ 2ℓ, we complete the query with a binary search on X[ik..jk], in time
O(ℓ), and ﬁnish.
(4) We use the local predecessor search data structure of interval p, found using Pℓ,
to determine the subinterval [i′k..j′k] ⊆ [ik..jk] of size 2ℓ where the answer lies. This
takes time O(cid:16)lg lg 22ℓ(cid:17) = O(ℓ).

(5) We complete the query using binary search on X[i′k..j′k], in time O(ℓ).

Our data structures use in total O(n lg lg U ) bits for a given level ℓ, which adds up
to O(cid:0)n(lg lg U )2(cid:1) bits in total. They answer queries in time O(ℓ) = O(lg lg(j − i + 1))
on nonempty intervals. Note that on empty intervals our mmphf Fℓ could return an
arbitrary value.

3.2. A Faster and Smaller Data Structure
Now we divide X into ⌈n/b⌉ blocks, each containing b = lg U keys. We build a set S′ of
sampled keys by selecting the ﬁrst element of each block (X[1], X[b + 1], X[2b + 1] . . .).
Thus S′ will contain in total ⌈n/b⌉ keys. Now we use the scheme of Section 3.1, except
for the lpsds implementation, which differs in the choice of the keys it stores. This time
we will only have lg lg(U/n)+1 levels: we collapse all the levels ℓ such that 2ℓ ≥ lg(U/n).
At at each level we will have the same associated mmphf Fℓ and the same partial sums
Bℓ, but the lpsds are built differently and a smarter encoding yields a space usage of

for every interval that contains at least one sampled element (i.e., from S′) and to store
all the sampled elements in the interval in a fast predecessor search data structure

O(cid:0)n(lg lg(U/n))2(cid:1) bits, instead of O(cid:0)n(lg lg U )2(cid:1). The new strategy is to store an lpsds
that supports queries in time O(cid:16)lg 2ℓ
lg w(cid:17). Note that for levels where 2ℓ ≥ lg(U/n), it is
sufﬁcient to achieve time O(cid:16)lg lg(U/n)
lg w (cid:17). All these levels are collapsed, as anticipated,
lg w (cid:17)
O(n/b) sampled keys (each of length lg U ), which answers queries in time O(cid:16)lg lg(U/n)
and uses O((n/b) · lg U ) = O(n) bits (e.g., [Belazzougui & Navarro 2012]).

into a singe level where a predecessor search data structure is built on the on the

ACM Transactions on Algorithms, Vol. V, No. N, Article A, Publication date: January YYYY.

A:8

D. Belazzougui, T. Gagie and G. Navarro

bits.

Each of the non-collapsed levels ℓ ≤ lg lg(U/n) stores the same kind of predecessor
data structure [Belazzougui & Navarro 2012], over ⌈n/b⌉ keys of length 2ℓ. Each such
structure uses O(cid:0)(n/b) · 2ℓ(cid:1) = O((n/b) lg(U/n)) = O(n) bits, and answers queries in
lg w(cid:17). Added over all the non-collapsed levels, the space is O(n lg lg(U/n))
time O(cid:16)lg 2ℓ
Note that those lpsds are built on universes of size 22ℓ
≤ U/n, and hence the keys
require only lg(U/n) bits. Since the lpsds are only built on sampled keys, they can only
determine a predecessor among the sampled keys. The real answer will be inside the
block that separates two sampled keys. To complete the search inside a block of b keys,
we store a predecessor data structure [Grossi et al. 2009, Lem. 3.3] for each block. The
structure is an index that uses O(b lg lg(U/n)) bits per block (in addition to a global
precomputed table of O(U ǫ) bits, any constant ǫ > 0) and computes the predecessor in
constant time for any b polylogarithmic in U , with O(1) accesses to the data. Added
over all the levels, the space of these structures is O(cid:0)n(lg lg(U/n))2(cid:1) bits.
3.2.1. Queries. In a level ℓ ≤ lg lg(U/n), and given the range [i, j] determined by Fℓ and
Bℓ, we ﬁrst construct a range [i0, j0], which is the largest subinterval of [i, j] aligned to
block boundaries. We ﬁrst use the predecessor structure of the block [j0, j0 + b] to look
for a predecessor of j. If it exists, this is the answer. Otherwise, we carry out a query
on the interval [i0 − b, j0], which cannot be empty if [i, j] is nonempty and is handled
with the proper lpsds in time O(cid:16)lg 2ℓ

lg w(cid:17) = O(cid:16)lg lg(j−i+1)

A query on the collapsed level, on the other hand, simply uses the predecessor data

(cid:17).

lg w

structure for that level. This gives our result.

THEOREM 3.1. Given n points in the discrete universe [1, U ] stored in an array X,

there exists a data structure using O(cid:0)n(lg lg(U/n))2(cid:1) bits of space that and solves in
(cid:17), and with O(1) accesses to the array X, the following query: Given
time O(cid:16)lg lg(j−i+1)

a range [i, j] known to contain some element in X, return the predecessor of j.

lg w

4. NUMBER OF POINTS IN A RANGE
In this section we describe a one-dimensional range counting data structure that handles 
n points in [1, U ] and can count the number occ of points in any range [i, j], faster
when the range is shorter and when there are more points to count. We start with

a simple solution that takes time O(cid:16)lg lg j−i+1

occ+1(cid:17) and O(n lg U ) bits of space. Then we

improve upon it to obtain a faster and smaller data structure, which in particular requires 
sublinear space overhead on top of any representation of the array.

4.1. A Simple Data Structure
We use ⌈lg n⌉ − 1 levels. At each level ℓ ≥ 2 we build a data structure that efﬁciently
answers queries of length between 2ℓ−2 + 1 and 2ℓ−1. Our structure deﬁnes speciﬁc
intervals and subintervals. For clarity we will refer to ranges to denote any other range
of the universe.

Given a level ℓ, we divide the universe into ⌈U/2ℓ−1⌉ overlapping intervals of size 2ℓ,
so that interval number k will be [2ℓ−1k + 1, 2ℓ−1k + 2ℓ]. It is clear that any range of
size at most 2ℓ−1 will be included in at least one interval.

We only consider nonempty intervals. We can have at most 2n nonempty intervals,
as each point belongs to 2 intervals. We use a mphf fℓ that maps the n′ ≤ 2n nonempty
intervals into unique numbers in [1, n′]. The mphf uses O(n′ + lg lg U ) = O(n + lg lg U )

ACM Transactions on Algorithms, Vol. V, No. N, Article A, Publication date: January YYYY.

Frequency-Sensitive Queries in Ranges

A:9

bits of space and answers queries in constant time [Hagerup & Tholey 2001]. It gives
a correct answer only if we query it for a nonempty interval.

We consider how to solve queries on nonempty intervals. Suppose that an interval
k contains nk elements. We cut the interval into nk equally-sized subintervals, of size
⌈2ℓ/nk⌉ (the last subinterval can be shorter). We use a preﬁx sum data structure to
store the number of elements in each subinterval of the interval k. That preﬁx sum
structure uses O(nk) bits. The space usage over all the preﬁx-sum data structures for
all the intervals is O(n) bits. We concatenate the memory areas of the preﬁx-sum data
structures of the intervals (in the order given by the mphf) and store another bitmap
Dℓ that marks the beginning of the preﬁx-sum data structure of each interval. This
new bitmap also uses O(n) bits.
We store one instance of this data structure for levels ℓ = 2 to ℓ = ⌈lg n⌉. Each structure 
uses O(n + lg lg U ) bits of space, resulting in O(n lg n + lg n lg lg U ) bits overall. In
addition, we store one instance of the predecessor data structure of Section 3, and
a range-emptiness data structure, which tells in constant time whether a range [i, j]
contains any point, using O(n lg U ) bits [Alstrup et al. 2001].

4.1.1. Queries.. We ﬁrst perform a range-emptiness query to determine whether the
query range [i, j] contains at least one element. If not, we immediately return 0. Otherwise 
we compute ℓ = ⌈lg(j − i + 1)⌉ + 1 and algebraically determine the interval of
level ℓ that encloses [i, j]. We answer the query using that interval, which we denote
[I, J].

We ﬁrst use fℓ to ﬁnd the index k of the interval [I, J]. Because we the interval is
nonempty, the mphf gives a meaningful answer. Next we use Dℓ to recover the preﬁxsum 
data structure for the interval [I, J]. Then, we ﬁnd the subinterval [I0, J0] of [I, J]
that contains i and the subinterval [I1, J1] that contains j. The number of elements
in [i, j] equals the sum of the the number of elements in the three ranges [i, J0], [J0 +
1, I1 − 1] and [I1, j]. The count of the range [J0 + 1, I1 − 1] is found in constant time
using the preﬁx sum structure associated to interval [I, J], as the range [J0 + 1, I1 − 1]
is aligned to subinterval boundaries.
What remains is to determine the counts in the two tail ranges [i, J0] and [I1, j]. We
only show how to determine the count in range [i, J0]; the other case is symmetric. First
we query the range-emptyness data structure to determine whether the subinterval
[J0 − ⌈2ℓ/nk⌉ + 1, i − 1] is empty. If it is, then the count in [i, J0] is the same as in
[J0 − ⌈2ℓ/nk⌉ + 1, J0], and thus can be computed from the preﬁx sum data structure.
Otherwise, we can carry out two predecessor queries, using the structure of Section 3,
for the intervals [J0 − ⌈2ℓ/nk⌉ + 1, i − 1] and [J0 − ⌈2ℓ/nk⌉ + 1, J0], knowing that both
intervals are nonempty. We count the number of elements in [i, J0] by subtracting the
rank of the predecessor of i − 1 from the rank of the predecessor of J0.
The query time is dominated by that of the predecessor search, O(cid:16)lg lg⌈2ℓ/nk⌉
lg w (cid:17) according 
to Theorem 3.1. Now note that nk, the number of elements in [I, J], is at least
occ. On the other hand J − I + 1 = 2ℓ is at most 4(j − i + 1). We thus conclude that the
lg w (cid:19), as promised. The space, however, is still O(n lg U ) bits.
query time is O(cid:18)lg
Now we introduce an improved solution that reduces it to O(cid:16)nplg(U/n)(cid:17) bits.

lg j−i+1
occ+1

4.2. A Smaller Data Structure
We now modify the data structure of Section 4.1. We only build this structure up to

level ℓ0 = √lg δ + lg lg δ, where δ = ⌈U/n⌉, and assume δ ≥ lg n (the case δ < lg n

will be considered at the end of the section). That is, we only build data structures

ACM Transactions on Algorithms, Vol. V, No. N, Article A, Publication date: January YYYY.

A:10

D. Belazzougui, T. Gagie and G. Navarro

to handle intervals of sizes 2, 4, . . . , 2ℓ0 = 2√lg δ lg δ. The structure now uses O(cid:0)n√lg δ(cid:1)
bits of space, since at each level it uses O(n) bits. In Section 4.3, we also build a more
space-efﬁcient range-emptiness index using O(cid:0)n√lg δ(cid:1) bits (on top of a table of keys in
sorted order) and answering range emptiness queries in constant time.
We now describe how the upper levels ℓ > ℓ0 are handled. For any such upper level
we store only intervals that have density at least 1/2√lg δ. As every interval of an
upper level is of size at least 2√lg δ lg δ, we only store intervals that contain at least
2√lg δ lg δ
2√lg δ = lg δ elements. More generally, in level ℓ = ℓ0 + ℓ′, stored intervals contain 
at least 2ℓ′
lg δ elements. Thus, we can store these intervals in dictionaries Rℓ
(instead of weaker mmphfs Fℓ), which use O(lg(U/n)) = O(lg δ) bits per stored interval
and answer to membership queries in constant time. At level ℓ = ℓ0 + ℓ′, there are
lg δ(cid:17)(cid:17) stored intervals, so dictionary Rℓ uses O(cid:16)n/2ℓ′(cid:17) bits. This adds up to
O(cid:16)n/(cid:16)2ℓ′
O(n) bits over all the upper levels.
interval is of density less than 1/2√lg δ. Since [i, j] ⊆ [I, J], it follows that
1/2√lg δ, and thus lg lg δ = O(cid:16)lg lg j−i+1

At query time, if we do not ﬁnd the query interval [I, J] in Rℓ, we conclude that the
j−i+1 <
occ+1(cid:17). This means that we can answer the counting

query within the promised time using predecessor searches on X, in particular using
the structure of Belazzougui and Navarro [Belazzougui & Navarro 2012], which takes

occ

O(cid:16)lg lg δ

lg w(cid:17) time. To reduce its space, we partition the universe U into n pieces of length

δ, and divide the elements in each piece into slices of lg δ elements. If we have more
than one slice, we build the predecessor structure [Belazzougui & Navarro 2012] on
the ﬁrst elements of the slices. Inside each slice we will use another predecessor structure 
for small blocks [Grossi et al. 2009, Lem 3.3]. Then, upon a predecessor query, an
O(n)-bit partial sums structure leads us to the right piece, the predecessor structure
of the piece leads us to the right slice, and the predecessor structure of the slice gives

the ﬁnal predecessor. The time is dominated by the O(cid:16)lg lg δ

lg w(cid:17) complexity of the predecessor 
structure of the piece. As for the space, we have O(n) bits for the partial sums,
O(n lg δ/ lg δ) = O(n) for the predecessor structures on the pieces, and O(n lg lg δ) bits
for the predecessor structures on the slices.

We now describe how the dense upper intervals are handled to answer to queries

using only O(cid:0)n√lg δ(cid:1) bits. For every interval ci at level i with density d (where d is

rounded to the nearest smaller power of two) we do not necessarily store the bitmap
(the one that stores the cardinalities of the subintervals of the interval), but instead
point (using a pointer) to cj, which is the interval of highest level j such that (1) the
density (also rounded to the nearest smaller power of two) of cj is at least d, and (2)
cj fully encloses ci. If ci = cj then we store the bitmap of ci. Therefore, at query time,
we simply determine whether the query interval ci is stored explicitly or has a pointer
to another interval cj. In the second case, we can correctly solve the query using the
data of cj, within the same time complexity (as it depends only on the rounded density
of the interval, which is the same for ci and cj). The rest of the section is devoted to
analyze the space usage.

We note that the pointer from ci to cj can be encoded using just lg lg n + 1 bits: we
need only to store the level pointed to, which requires lg lg n bits, and then we know
that only two intervals at any level ℓ can enclose ci, thus the pointer can be uniquely
determined using one additional indicator bit (saying whether the interval is the left or

ACM Transactions on Algorithms, Vol. V, No. N, Article A, Publication date: January YYYY.

Frequency-Sensitive Queries in Ranges

A:11

the right one). Since there are only O(cid:16)n/(cid:16)2ℓ′
lg δ(cid:17)(cid:17) intervals stored at level ℓ = ℓ0 + ℓ′,
and in addition it holds δ ≥ lg n, it follows that there are O(cid:16)n/(cid:16)2ℓ′
lg lg n(cid:17)(cid:17) stored
intervals at upper level ℓ, and hence the pointers add up to O(n) bits over all the upper
levels.
Now we upper bound the space used by the explicitly stored intervals. The key issue
is to prove that a point appears in at most 2√lg δ bitmaps. To see why, we will ﬁrst
prove that a point appears in at most two bitmaps of a given rounded density d. In
order to prove this let us ﬁrst prove the following lemma.

LEMMA 4.1. There are no three distinct intervals (from any levels) such that the

three pairs of distinct intervals partially overlap each other.

PROOF. Assume otherwise. Let the 3 intervals cα, cβ, cγ, from levels α ≤ β ≤ γ,
respectively. First note that cβ overlapping cα means that cα starts at or ends before
the middle of cα. The reason is that cβ starts on or ends before multiples of 2β/2 ≥ 2α/2.
Thus in order to overlap with cα it must start on or end before a point somewhere
strictly inside cα and the only point that can be a multiple of 2α/2 (and thus possibly a
multiple of 2β/2) is the middle of the interval.

The same argument holds for cγ, which must start on or end before the middle of
cα. We now compare cβ and cγ. If cβ (respectively cγ) starts in the middle of cα and
cγ (respectively cβ) ends before the middle of of cα, then they are not overlapping. It
remains to consider the case that both cβ and cγ start on or end before the middle of cα.
In this case, clearly cβ is enclosed in cγ, simply because they start or end at the same
point and cβ is bigger than cγ.

From the lemma we can now prove our next goal.

LEMMA 4.2. There cannot be a point that participates in three distinct interval

bitmaps with the same rounded density.

PROOF. Assume otherwise. Let a point participate in distinct intervals ci, cj, ck at
levels i ≤ j ≤ k with the same rounded density d. We prove that if this was the case
then the three intervals ci, cj, ck should be partially overlapping, which is impossible
by Lemma 4.1. First of all, the three intervals must include the same point, so they
must clearly be overlapping. Also, no interval can be included in the other, as if this
was the case then the included interval would not be explicit but instead point to some
of the intervals that enclose it, as they have the same rounded density. Thus, each pair
of intervals is overlapping and no interval is enclosed in the other, which means that
the pairs of intervals are partially overlapping.

As we have exactly √lg δ distinct levels, we conclude that each point participates
in at most 2√lg δ explicit bitmaps, and thus the total space used by all those bitmaps
(which store O(1) bits per point included) is O(cid:0)n√lg δ(cid:1) bits.

The case δ < lg n.. If δ = U/n < lg n, then U < n lg n. In this case we use a different
solution. We split the universe into n intervals of length δ < lg n. A partial sums data
structure accumulates the number of points in each interval using O(n) bits. Inside
each interval, we store one predecessor data structure [Grossi et al. 2009, Lem. 3.3],
which will add up to O(n lg lg δ) bits (plus a global precomputed table of O(δǫ) bits),
and will solve predecessor queries in constant time within the intervals. Then the
range counting is easily done in constant time and using O(n lg lg(U/n)) bits.

ACM Transactions on Algorithms, Vol. V, No. N, Article A, Publication date: January YYYY.

A:12

D. Belazzougui, T. Gagie and G. Navarro

4.3. Space-Efﬁcient Range Emptiness

A range-emptiness index that uses O(cid:0)n√lg U(cid:1) bits already exists [Belazzougui et al.
2010]. Its space can be trivially improved to O(cid:16)nplg(U/n)(cid:17) = O(cid:0)√lg δ(cid:1) bits by dividing 
U into n intervals of size ⌈U/n⌉ and storing in a preﬁx sum data structure the
number of elements in each interval. Then we build a local range emptiness index on
the elements that belong to each interval. The index will thus use O(cid:16)plg(U/n)(cid:17) bits
per element, for a total of O(cid:16)nplg(U/n)(cid:17) bits over all the local indexes. Now, given

a query range, it fully contains zero or more consecutive intervals and partially overlaps 
one or two intervals. The emptiness of the fully contained intervals is established
using the preﬁx sum structure, while the partially overlapped intervals are queried
using the local range emptiness indexes. Thus a range emptiness query can be decided
in constant time, and our ﬁnal result is proved.

THEOREM 4.3. Given n points in the discrete universe [1, U ] stored in an array

X[1..n], there exists a data structure using O(cid:16)nplg(U/n)(cid:17) bits of space that returns
the number occ of points in any range [i, j] in time O(cid:18)lg
lg w (cid:19) and in O(1) accesses
to the array X. The data structure uses precomputed tables that occupy O(U ǫ) bits of
space (where 0 < ǫ < 1 is any constant), which are independent of the point set.

lg j−i+1
occ+1

We can slightly adapt this procedure to return some element when the range is
nonempty. The structure used within the intervals [Belazzougui et al. 2010] is a weak
preﬁx search data structure, so it will return some element when it ﬁnds that the interval 
is nonempty. In case the only elements are in the sequence of whole consecutive
intervals covered by the partial sum data structure, we can use the structure to ﬁnd
the ﬁrst nonempty interval in the sequence (by searching for the interval where the
sum reaches x + 1, being x the sum up to the ﬁrst interval in the sequence, not including 
it). Once we have identiﬁed a nonempty interval, the weak preﬁx search data
structure [Belazzougui et al. 2010] of this interval will give us one element in it. This
feature will be useful later in the paper.

5. NUMBER OF POINTS IN A RANGE, AGAIN
We propose a different range counting data structure, which performs better when

We

the

structure

of Andersson

and

use

ﬁnger-search

data

lg lg(occ+1)(cid:17) using

there are fewer points in the count. Namely, we count in time O(cid:16)q lg(occ+1)
O(cid:16)nplg(U/n)(cid:17) bits.

Thorup 
[Andersson & Thorup 2007]. Given n elements in the discrete universe [1, U ],
it uses O(n lg U ) bits and answers the following variant of the predecessor query:
Given a “ﬁnger” element k and a query for the predecessor of x, it answers in time
O(cid:16)q lg(rd(x,k)+1)
lg lg(rd(x,k)+1)(cid:17), where rd(x, k) is the number of points lying between k and x. In
First, we cut the universe into ⌈U/n⌉ intervals of equal size and store in a preﬁx sum
data structure the number of elements in each interval. Given a range counting query,
we use the preﬁx sums to count the number of elements in the intervals that are fully
contained in the query range. What remains is to count the number of elements in the
up to two intervals that are not fully contained in the query range.

addition, we will use the range emptiness data structure of Section 4.3.

ACM Transactions on Algorithms, Vol. V, No. N, Article A, Publication date: January YYYY.

Frequency-Sensitive Queries in Ranges

A:13

To that end, we sample one every b keys, with b = lg(U/n), inside each interval,
and store the sampled keys in a ﬁnger-search data structure for that interval. We do
not store the full keys, but only the least signiﬁcant lg(U/n) bits, since the lg n upper
bits of all the keys inside an interval are the same. If an interval contains less than b
keys, we do not store the ﬁnger-search data structure. Overall, the ﬁnger-search data
structures store up to n/b keys, each of b bits, for a total space usage of O(n) bits.
The elements between two sampled keys form a block, and we store one predecessor 
structure [Grossi et al. 2009, Lem. 3.3] for each block. These will add up to
O(n lg lg(U/n)) bits of space, plus a ﬁxed shared table of O((U/n)ǫ) bits, for some constant 
ǫ. The range-emptiness data structure of Section 4.3 uses other O(cid:16)nplg(U/n)(cid:17)

bits, which dominate the overall space.

Given a range [i, j] fully contained in an interval, we ﬁrst ask if the range is empty.
If it is, the count is zero. Otherwise, the range emptiness data structure returns some
element k ∈ [i, j]. Then we perform two queries on the ﬁnger-search data structure, for
the points i and j, using the ﬁnger k. Since rd(i, k), rd(j, k) ≤ occ, the queries take time
time at most O(cid:16)q lg(occ+1)
lg lg(occ+1)(cid:17), and give us the predecessor j0 of j and the successor

i0 of i among the sampled keys stored in the ﬁnger-search data structure. Finally, the
ranges [i, i0] and [j0, j] are contained in blocks, so a predecessor search on each takes
constant time using the predecessor structures.

THEOREM 5.1. Given n points in the discrete universe [1, U ] stored in an array

X[1..n], there exists a data structure using O(cid:16)nplg(U/n)(cid:17) bits of space that returns
the number occ of points in any range [i, j] in time O(cid:16)q lg(occ+1)
lg lg(occ+1)(cid:17) and in O(1) accesses
to the array X. The data structure uses precomputed tables that occupy O((U/n)ǫ) bits
of space (where 0 < ǫ < 1 is any constant), which are independent of the point set.

6. COUNTING ELEMENTS IN A RANGE
We now switch to another scenario, where instead of points in a universe we have a
sequence S[1..n] of elements over a discrete alphabet of symbols in [1, σ]. We use the
results of the previous sections to answer queries on S, on top of a representation of S
that can answer access, rank and select queries. In this section we show how to count
the number of occurrences of a given symbol in an array interval, in time that improves
with its frequency in the interval, and using compressed space.

The basic idea is to create, for each symbol 1 ≤ c ≤ σ, a point set Pc = {i, S[i] = c}
over universe [1, n], and reduce the counting for symbol c to range counting on Pc. We
call nc the number of occurrences of c in S, and δc = n/nc its inverse relative frequency.
We will use the range counting structure of Theorem 4.3 for each Pc, using δ = δc.

We represent S using alphabet partitioning , which distributes the alphabet into
subalphabets according to the value of ⌈lg δc · lg n⌉. Thus any pair of symbols c and
c′ belonging to the same subalphabet satisfy δc = Θ(δc′). In the alphabet partitioned
representation, the subalphabets of polylogarithmic size are represented so that access,
 rank and select take constant time, and thus we can solve the counting for those
symbols in constant time using rank queries. Note that if δc ≤ lg n, it follows that
nc ≥ n/ lg n, and thus there cannot be more than lg n symbols where that holds. Therefore 
all those subalphabets are of logarithmic size, and we can focus only on the case
δc > lg n.

Note that, on those symbols with larger δc, rank queries on the partitioned representation 
take time O(cid:16)lg lg δc

lg w(cid:17), and those can be used to solve the counting query.

ACM Transactions on Algorithms, Vol. V, No. N, Article A, Publication date: January YYYY.

A:14
D. Belazzougui, T. Gagie and G. Navarro
This time is good enough for intervals of density below 1/2√lg δc , so this replaces the

predecessor data structure used in Section 4. We also note that the range emptiness
data structure needs to access the array of “points” Pc. This is simulated with select
operations on S.

As for the space when δc > lg n, the data structure of Theorem 4.3 uses O(cid:0)nc√lg δc(cid:1)
bits. We let nH = n′H′ + n′′H′′, where n′H′ = P nc lg(δc) for all c with δc > lg n is the
contribution of characters c with δc > lg n to the total entropy of the sequence S and n′
is the total number of occurrences of such characters (n′′ and H′′ are similarly deﬁned
for the remaining symbols). It is easy to see that H′ ≥ lg lg n.
O(cid:16)n′√H′(cid:17) ≤ O(n′ H′√lg lg n

By convexity of the logarithm, the total space used for all c with δc > lg n adds up to

) ≤ O(nH/√lg lg n) = o(nH) bits.

We also build the precomputed tables for lg2 n different values of δ, thus all the precomputed 
tables for the predecessor structures [Grossi et al. 2009, Lem. 3.3] occupy

O(cid:0)nǫ lg2 n(cid:1) = o(n) bits of space.

Therefore, the space is dominated by the (alphabet partitioned) representation of S.
Apart from operation rank, it supports select and access in constant time if we let it
use (1 + ǫ)nH bits of space.

THEOREM 6.1. For any positive constant ǫ, we can store a sequence S[1..n] over
alphabet [1, σ] and with per-symbol entropy H, within (1 + ǫ)nH + o(n) bits, such that it
supports operations access and select in O(1) time and rank in time O(lg lg σ). Moreover,
given endpoints i and j and a symbol c ∈ [1, σ], it computes occ = occ(a, S[i..j]) in time
O(cid:18)lg

lg w (cid:19).

lg j−i+1
occ+1

7. PARAMETERIZED RANGE MINORITY
Recall from Section 1 that a τ -minority for a range is a distinct element that occurs
in that range but is not one of its τ -majorities. The problem of parameterized range
minority is to preprocess a string such that later, given the endpoints of a range and
τ , we can quickly return a τ -minority for that range if one exists. Chan et al. gave
a linear-space solution with O(1/τ) query time even for the case of variable τ . They
ﬁrst build a list of ⌊1/τ⌋ + 1 distinct elements that occur in the given range (or as
many as there are, if fewer) and then check those elements’ frequencies to see which
are τ -minorities. There cannot be more than ⌊1/τ⌋ τ -majorities so, if there exists a τ -
minority for that range, then at least one must be in the list. In this section we show
how to implement this idea using compressed space.

To support parameterized range minority on S[1..n] in O(1/τ ) time, we store data
structures supporting O(1)-time access, select and partial rank queries on S and a
data structure supporting O(1)-time range-minimum queries on C. For any positive
constant ǫ, we can store these data structures in a total of (1 + ǫ)nH + O(n) bits. Given
τ and endpoints i and j, in O(1/τ) time we use Muthukrishnan’s algorithm to build
a list of ⌊1/τ⌋ + 1 distinct elements that occur in S[i..j] (or as many as there are, if
fewer) and the positions of their leftmost occurrences therein. We check whether these
distinct elements are τ -minorities using the following lemma:

LEMMA 7.1. Suppose we know the position of the leftmost occurrence of a distinct
element in a range. We can check whether that distinct element is a τ -minority or a
τ -majority using a partial rank query and a select query on S.

PROOF. Let k be the position of the ﬁrst occurrence of a in S[i..j]. If S[k] is the rth
occurrence of a in S, then a is a τ -minority for S[i..j] if and only if the (r + ⌈τ (j − i +

ACM Transactions on Algorithms, Vol. V, No. N, Article A, Publication date: January YYYY.

Frequency-Sensitive Queries in Ranges

A:15

1)⌉ − 1)th occurrence of a in S is strictly after S[j]; otherwise a is a τ -majority. That is,
we can check whether a is a τ -minority for S[i..j] by checking whether

S.selecta(cid:16)S.ranka(k) + ⌈τ (j − i + 1)⌉ − 1(cid:17) > j ;

since S[k] = a, computing S.ranka(k) is only a partial rank query.

To avoid storing C, which is used in Muthukrishnan’s algorithm, we use Sadakane’s
variant [Sadakane 2007], which marks the values found in a bitmap of size σ, and
stops the recursion when the new symbol to consider is already marked (for this to
work he must ﬁrst process the left and then the right interval of the minimum).

This gives us the following theorem, which improves Chan et al.’s solution to use

nearly optimally compressed space with no slowdown.

THEOREM 7.2. For any positive constant ǫ, we can store S in (1 + ǫ)nH + O(n) bits
such that later, given the endpoints of a range and τ , we can return a τ -minority for
that range (if one exists) in O(1/τ ) time.

Alternatively, for any function f (n) = ω(1), we can store our data structures for
access, select and partial rank on S and range-minimum queries on C in a total of
nH + O(n) + o(nH) at the cost of select queries taking O(f (n)) time.

THEOREM 7.3. For any function f (n) = ω(1), we can store S in nH + O(n) + o(nH)
bits such that later, given the endpoints of a range and τ , we can return a τ -minority
for that range (if one exists) in O((1/τ ) f (n)) time.

To reduce the space bound of this t to nH + o(n(H + 1)) bits, improving Chan et al.’s
solution to use optimally compressed space with nearly no slowdown, we must reduce
the space of the range-minority data structure to o(n).

We do this via sparsiﬁcation. We cut the sequence into blocks of length f (n), choose
the n/f (n) minimum values of each block, and build the RMQ data structure on the
new array C′[1..n/f (n)]. This requires O(n/f (n)) = o(n) bits. Muthukrishnan’s algorithm 
is then run over C′ as follows. We ﬁnd the minimum position in C′, then recursively 
process its left interval, then process the minimum of C′ by considering the
f (n) corresponding cells in C, and ﬁnally process the right part of the interval. The
recursion stops when the interval becomes empty or when all the f (n) elements in the
block of C are marked. In addition we must sequentially process the 2f (n) cells of S
that only partially overlap blocks in C′. We note that a similar technique is proposed
by Hon et al. [Hon et al. 2009], but it lacks sufﬁcient detail to ensure correctness. We
prove such correctness next.

LEMMA 7.4. The procedure described correctly identiﬁes all the distinct points in

S[i..j], working over at most f (n) cells per new element discovered.

PROOF. We show by induction on the size of the current subinterval [ℓ..r] that, if we
start the procedure with the elements that already appear in [i..ℓ− 1] marked, then we
ﬁnd and mark the leftmost occurrence of each distinct symbol not yet marked, spotting
at least one new element per block scanned.

This is trivial for the empty interval. Now consider the minimum position in C′[k′],
which contains the leftmost occurrence of some element S[k], for some k within the
block of C′[k′]. If S[k] is already marked, it means it appears in [i..ℓ − 1], and thus
the leftward pointer C[k] ≥ i, and so holds for all the values in C[ℓ..r]. Thus if all the
elements in the block of C′[k′] are marked, we can safely stop the procedure.
Otherwise, before doing any marking, we recursively process the interval to the left
of block k′, which by inductive hypothesis marks the unique elements in that interval.

ACM Transactions on Algorithms, Vol. V, No. N, Article A, Publication date: January YYYY.

A:16

D. Belazzougui, T. Gagie and G. Navarro

Now we process the current block of size f (n), ﬁnding at least the new occurrence of
element S[k] (which cannot appear to the left of k′). Once we mark the new elements
of the current block, we process the interval to the right of k′, where the inductive
hypothesis again holds.

By using this procedure to obtain any ⌊1/τ⌋ + 1 distinct elements, we obtain the

improved result.

THEOREM 7.5. For any function f (n) = ω(1), we can store S in nH + o(n)(H + 1))
bits such that later, given the endpoints of a range and τ , we can return a τ -minority
for that range (if one exists) in O((1/τ ) f (n)) time.
8. PARAMETERIZED RANGE MAJORITY WITH FIXED τ
The standard approach to ﬁnding τ -majorities, going back to Misra and Gries’ work, is
to build a list of O(1/τ) candidate elements and then verify them. For parameterized
range majority, an obvious way to verify candidates is to use rank queries. The problem
with this approach is that, as noted in Section 2.1, we cannot support general rank
queries in o(lg(lg σ/ lg w)) time while using n lgO(1) n space; e.g., with only linear space,
we cannot support general rank queries in O(1) time when the alphabet is superpolylogarithmic.
 If we can ﬁnd the position of candidates’ ﬁrst occurrences in the range,
however, then by Lemma 7.1 we can check them using only partial rank and select
queries.

Suppose we want to support parameterized range majority on S[1..n] for a ﬁxed
threshold τ . We ﬁrst store data structures that support access, select and partial rank
on S in O(1) time, which takes O(n) space. For 0 ≤ b ≤ ⌊lg n⌋, let Fb[1..n] be the binary 
string in which Fb[k] = 1 if the distinct element S[k] occurs at least τ 2b times in
S[k..k + 2b+1 − 1]; and let Sb and Cb be the subsequences of S and C, respectively, consisting 
of those elements ﬂagged by 1s in Fb. We store Fb in O(n) bits such that we can
support access, rank and select queries on Fb in O(1) time. Notice we can implement
an access query on Sb or Cb as a select query on Fb and access queries on S or C, respectively.
 As described in Section 2.2, we can implement an access query to C as access,
select and partial rank queries on S. We also store an O(1)-time range-minimum data
structure for Cb, which takes O(|Sb|) bits.
With these data structures, given endpoints i and j with ⌊lg(j − i + 1)⌋ = b, we use
Muthukrishnan’s algorithm to list the distinct elements in Sb[Fb.rank1(i).. Fb.rank1(j)]
and the positions of their leftmost occurrences therein; we then use select queries on
Fb to ﬁnd the positions of those elements in S. That is, we list the distinct elements in
S[i..j] that are ﬂagged by 1s in Fb and the positions of their leftmost ﬂagged occurrences
therein. We then apply Lemma 7.1 to each of these elements, treating the positions
of their leftmost ﬂagged occurrences as the positions of their leftmost occurrences.
Since each distinct element in S[i..j] that is ﬂagged in Fb occurs at least τ 2b times in
S[i..j + 2b+1 − 1] ⊂ S[i..i + 2b+2], there are O(1/τ ) of them and we use a total of O(1/τ)
time.
Notice that the leftmost ﬂagged occurrences of a distinct element a in S[i..j] may not
necessarily be the leftmost occurrence therein. However, if a is a τ -majority in S[i..j]
then, by deﬁnition, a occurs at least τ (j − i + 1) ≥ τ 2b times in S[i..j] ⊂ S[i..i + 2b+1− 1],
so a’s leftmost occurrence in S[i..j] is ﬂagged by a 1 in Fb and, therefore, we apply
Lemma 7.1 to it. It follows that we return each τ -majority in S[i..j].

We store only one set of data structures supporting access, select and partial rank
on S. Summing over b from 0 to ⌊lg n⌋, the data structures for range-minimum queries
take a total of O(n lg n) bits, which is O(n) words. Therefore, we have the ﬁrst linearACM 
Transactions on Algorithms, Vol. V, No. N, Article A, Publication date: January YYYY.

Frequency-Sensitive Queries in Ranges

A:17

space data structure with worst-case optimal O(1/τ ) query time for Karpinski and
Nekrich’s original problem of parameterized range majority with ﬁxed τ .

THEOREM 8.1. Given a threshold τ , we can store a string in linear space and support 
parameterized range majority in O(1/τ ) time.
9. PARAMETERIZED RANGE MAJORITY WITH VARIABLE τ
9.1. Nearly linear space with optimal query time
Suppose we have an instance of the data structure from Theorem 8.1 for each threshold
1, 1/2, 1/4, . . . , 1/2⌈lg n⌉, which takes a total of O(n lg n) space. Given endpoints i and j
and a threshold τ , we can use the instance for threshold 1/2⌈lg(1/τ )⌉ to build a list of
O(1/τ ) candidate elements and then check them with Lemma 7.1; this takes a total
of O(1/τ) time and returns all the τ -majorities in S[i..j]. Gagie et al. used a variant
of this idea to obtain the ﬁrst data structure for variable τ . We can easily reduce our
space bound to O(n lg σ) because, if 1/τ ≥ σ, then we can simply use Muthukrishnan’s
algorithm with S and C to list in O(σ) = O(1/τ ) time all the distinct elements in
S[i..j] and the positions of their leftmost occurrences therein, then check them with
Lemma 7.1.

t=0

For 0 ≤ b ≤ ⌊lg n⌋ and 1 ≤ k ≤ n, we have F t

Notice that we need store only one set of data structures supporting access, select 
and partial rank on S. Also, if S[k] is a (1/2t)-majority in a range, then it is
also a (1/2t′
)-majority for all t′ ≥ t. It follows that if, instead of querying only the
instance for the threshold 1/2⌈lg(1/τ )⌉, we query the instances for all the thresholds
1, 1/2, 1/4, . . . , 1/2⌈lg(1/τ )⌉ — which still takes O(cid:16)P2⌈lg(1/τ )⌉
2t(cid:17) = O(1/τ ) time — then
we can modify the instances to reduce the total number of 1s in their binary strings.
Speciﬁcally, for 0 ≤ t ≤ ⌈lg σ⌉, let F t
b be the binary string Fb in the instance for threshb 
such that F t
old 1/2t; we modify F t
b [k] = 1 if and only if the number of occurrences of
the distinct element S[k] in S[k..k + 2b+1 − 1] is at least 2b−t times but less than 2b−t+1.
b [k] = 1 for at most one value of t.
Therefore, all the binary strings contain a total of at most n(⌊lg n⌋ + 1) copies of 1, so
all the range-minimum data structures take a total of O(n lg n) bits. Since the binary
strings have total length n⌈lg n⌉⌈lg σ⌉, we can use P ˇatras¸cu’s data structure to store
them in a total of O(n lg(n) lg lg σ) bits. A slightly neater approach is to represent all
the binary strings F 0
b [k] = 1,
and ∞ if there is no such value t. We can implement access, rank and select queries
on F 0
by access, rank and select queries on Tb. Since Tb is an alphabet of
size O(lg σ), we can use Ferragina et al.’s data structure to store it in O(n lg lg σ) bits
and support access, rank and select queries in O(1) time. Either way, in total we use
O(n lg lg σ) space.

as a single string Tb[1..n] in which Tb[k] = t if F t

b , . . . , F ⌈lg σ⌉

b

b , . . . , F ⌈lg σ⌉

b

THEOREM 9.1. We can store S in O(n lg lg σ) space such that later, given the endpoints 
of a range and τ , we can return the τ -majorities for that range in O(1/τ) time.
9.2. Optimally compressed space with nearly optimal query time
To be able to apply Lemma 7.1, we must be able to ﬁnd the leftmost occurrence of
each τ -majority in a range. For this reason, we may ﬂag many occurrences of the same
distinct element even when they appear in close succession, because we cannot know
in advance where the query range will start. As discussed in Section 8, however, if we
have a data structure that supports rank queries on S, then it is sufﬁcient for us to
build a list of O(1/τ) candidate elements that includes all the τ -majorities — without
any information about positions — and then verify them using rank queries. This lets

ACM Transactions on Algorithms, Vol. V, No. N, Article A, Publication date: January YYYY.

A:18

D. Belazzougui, T. Gagie and G. Navarro

us ﬂag fewer elements and so reduce our space bound, at the cost of using slightly
suboptimal query time.

We store an instance of Barbay et al.’s data structure [Barbay et al. 2013] supporting 
access on S in O(1) time and rank and select on S in O(lg lg σ) time, which takes
nH + o(n(H + 1)) bits. For 0 ≤ t ≤ ⌈lg σ⌉ and ⌊lg(2t lg lg σ)⌋ ≤ b ≤ ⌊lg n⌋, we divide S
into blocks of length 2b−1 and store data structures supporting access, rank and select 
on the binary string Gt
b[k] = 1 if, ﬁrst, the distinct element S[k]
occurs at least 2b−t times in S[k − 2b+1..k + 2b+1] and, second, S[k] is the leftmost or
rightmost occurrence of that distinct element in its block. We also store an O(1)-time
range-minimum data structure for the subsequence of C consisting of elements ﬂagged
by 1s in Gt
b.

b[1..n] in which Gt

The number of distinct elements that occur at least 2b−t times in a range of size
O(cid:0)2b(cid:1) is O(2t), so there are O(2t) elements in each block ﬂagged by 1s in Gt
b. It follows
that we can store an instance of P ˇatras¸cu’s data structure supporting O(1)-time acb 
in O(cid:0)n2t−b(b − t) + n/ lg3 n(cid:1) bits in total; we need O(cid:0)n2t−b(cid:1)
cess, rank and select on Gt
bits for the corresponding range-minimum data structure. Summing over t from 0
to ⌈lg σ⌉ and over b from ⌊lg(2t lg lg σ)⌋ to ⌊lg n⌋, calculation shows we use a total of
lg n(cid:17) = o(n lg σ) bits for the binary strings and range-minimum data
O(cid:16) n lg σ lg lg lg σ

structures. Therefore, including the instance of Barbay et al.’s data structure for S, we
use nH + o(n lg σ) bits altogether.

+ n

lg lg σ

Given endpoints i and j and a threshold τ , if

⌊lg(j − i + 1)⌋ < jlg(cid:16)2⌈lg(1/τ )⌉ lg lg σ(cid:17)k ,

then we simply run Misra and Gries’ algorithm on S[i..j] in O(j − i) = O((1/τ ) lg lg σ)
time. Otherwise, we use Muthukrishnan’s algorithm to list the distinct elements
b, where t = ⌈lg(1/τ )⌉ and b = ⌊lg(j − i + 1)⌋ ≥ ⌊lg(2t lg lg σ)⌋, and use
ﬂagged by 1s in Gt
rank queries on S to check whether each of them is a τ -majority in S[i..j]. Since S[i..j]
overlaps at most 5 blocks of length 2b−1, it contains O(1/τ ) distinct elements ﬂagged by
1s in Gt
b; therefore, Muthukrishnan’s algorithm takes O(1/τ ) time and we use a total
of O((1/τ ) lg lg σ) time for all the rank queries on S.
Since S[i..j] cannot be completely contained in a block of length 2b−1, if S[i..j] overlaps 
a block then it includes one of that block’s endpoints. Therefore, if S[i..j] contains
an occurrence of a distinct element a, then it includes the leftmost or rightmost occurrence 
of a in some block. Suppose a is a τ -majority in S[i..j]. For all i ≤ k ≤ j, a occurs
at least τ 2b ≥ 2b−t times in S[k − 2b+1..k + 2b+1], so some occurrence of a in S[i..j] is
ﬂagged by a 1 in Gt

b. Therefore, we return a.

THEOREM 9.2. We can store S in nH + o(n lg σ) bits such that later, given the endpoints 
of a range and τ , we can return the τ -majorities for that range in O((1/τ ) lg lg σ)
time.

In order to reduce the space further, we open the black-box of Barbay et al.’s data
structure. This separates the sequence symbols into lg2 n classes according to their
frequencies. A sequence K[1, n], where K[i] is the class to which S[i] is assigned, is
represented using a (multi-ary) wavelet tree [Ferragina et al. 2007], which supports
constant-time access, rank, and select, since the alphabet of K is of polylogarithmic
size. For each class c, a sequence Sc[1..nc] contains the subsequence of S of the symbols
S[i] where K[i] = c (note that if S[i] = S[j] then K[i] = K[j]). They represent K, and the
subsequences Sc where σc ≤ lg n, using wavelet trees. The subsequences Sc over larger
alphabets are represented using Golynski et al.’s structure [Golynski et al. 2006]. The
wavelet tree for K takes nHK + o(n) bits, where HK is the entropy of K, the wavelet

ACM Transactions on Algorithms, Vol. V, No. N, Article A, Publication date: January YYYY.

Frequency-Sensitive Queries in Ranges

A:19

trees for the strings Sc take nc lg σc + o(nc) bits, and Golynski et al.’s structures take
nc lg σc + o(nc lg σ) bits. Barbay et al. show that these spaces add up to nH + o(n)(H + 1)
and that one can support access, rank and select on S via access, rank and select on K
and some Sc.

We will solve a τ -majority query on S[i..j] as follows. We ﬁrst run a τ -majority query
on string K. This will yield the at most 1/τ classes of symbols that, together, occur at
least τ (j − i + 1) times in S[i..j]. The classes excluded from this result cannot contain
symbols that are τ -majorities. Now, for each included class c, we map the interval
S[i..j] to Sc[ic..jc] in the subsequence of its class, since ic = K.rankc(i − 1) + 1 and jc =
K.rankc(j), and then run a τc-majority query on Sc[ic..jc], for τc = τ (j−i+1)/(jc−ic +1).
The results obtained for each considered class c are reported as τ -majorities in S[i..j].
To run the τc-majority queries on the sequences Sc that are implemented with
Golynski et al.’s structure, we store our representation of Theorem 9.2. This will add
o(nc lg σc) bits, which does not change the asymptotic space of the data structure.
Therefore we will take O((1/τc) lg lg σ) time to solve those majority queries. Added
over all the possible τc values, we have Pc(1/τc)O(lg lg σ) = Pc(jc − ic + 1)/(τ (j − i +
1))O(lg lg σ) = O((1/τ ) lg lg σ) total time on those sequences.
Let us now consider the case of the query on K[i..j]. Since the alphabet size is lg2 n,
we will partition it into lg2/3 n classes of lg4/3 n consecutive symbols, and subpartition
these new classes into lg2/3 n classes of lg2/3 n symbols. This works just like the general
partitioning into classes: we perform a τ -majority query in the ﬁrst level, then several
queries adding up to cost 1/τ on the second level, and then several queries adding up to
cost 1/τ on the third level, and then go to the subsequences Sc. It is sufﬁcient to show
that we can perform a τ -majority query on any sequence with alphabet size lg2/3 n to
obtain the result. The entropies of the three sequences add up to nHK + o(n) (indeed,
this leveled partitioning is how the wavelet tree is actually organized).

lg n

(cid:17) = o(n).

lg lg σ lg n (cid:17) = O(cid:16) n(lg lg n)3

To solve a τ -majority query on a sequence with alphabet size σ′ = lg2/3 n, we will
use again Theorem 9.2, with a slightly larger block size, ⌊lg(2t lg lg σ lg n/ lg lg n)⌋ ≤ b ≤
⌊lg n⌋, and for 0 ≤ t ≤ ⌈lg σ′⌉. Thus the structures Gt
b and the range-minimum data
structures add up to O(cid:16) n lg σ′(lg lg n)2
Since the wavelet tree implements rank, select and access in constant time,
the τ -majority operation is solved in time O(1/τ), except on blocks of size b0 =
O((1/τ ) lg lg σ lg n/ lg lg n), which have to be solved sequentially in time O((1/τ ) lg lg σ).
As before, we can ﬁnd the majorities in time O(σ′) by using rank over all the symbols,
so if 1/τ ≥ σ′ we can simply do this to achieve O(1/τ ) time. Otherwise, we can maintain 
counters for all the σ′ distinct symbols, each using O(lg lg n) bits to distinguish
values from 0 to b0 = O(σ′ lg n), using o(lg n) bits overall. Therefore a universal table
lets us read chunks of (lgσ′ n)/2 = Θ(lg n/ lg lg n) symbols and increase the corresponding 
counters in constant time. Thus the block can be processed sequentially in time
O((1/τ ) lg lg σ).
Finally, the same technique used for string K can be used for the sequences Sc that
are represented with wavelet trees, since their alphabet size is just lg n. Overall, we
have managed to reduce the redundancy of our representation.

THEOREM 9.3. We can store S in nH +o(n)(H +1) bits such that later, given the endpoints 
of a range and τ , we can return the τ -majorities for that range in O((1/τ ) lg lg σ)
time.

Since our solution includes an instance of Barbay et al.’s data structure, we can also

support O(1)-time access to S and O(lg lg σ)-time rank and select.

ACM Transactions on Algorithms, Vol. V, No. N, Article A, Publication date: January YYYY.

A:20

D. Belazzougui, T. Gagie and G. Navarro

9.3. Faster query time with nearly optimally compressed space
Recall from Section 9.1 that, if 1/τ ≥ σ, then we can simply use Muthukrishnan’s algorithm 
to list all the distinct elements in a range and then check them with Lemma 7.1;
therefore, we can assume 1/τ < σ. In this subsection we use our new data structure
with density-sensitive query time for one-dimensional range counting of Theorem 6.1
to obtain a nearly optimally compressed data structure for parameterized range malg 
w (cid:17) query time.

To obtain a compressed data structure for parameterized range majority with

jority with O(cid:16)(1/τ ) lg lg(1/τ )
O(cid:16)(1/τ ) lg lg(1/τ )
lg w (cid:17) query time, we combine our solution from Theorem 9.3 with Theorem 
6.1. Instead of using O(lg lg σ)-time rank queries to check each of the O(1/τ)
candidate elements returned by Muthukrishnan’s algorithm, we use range-counting
queries. We can make all O(1/τ ) range-counting queries each take O(cid:16)lg lg(1/τ )
lg w (cid:17) time

because, if one starts taking too much time, then the distinct element we are checking
cannot be a τ -majority and we can stop the query early. (In fact, as we will show in
the full version of this paper, our data structure from Theorem 6.1 does not need such
intervention.) This gives us our ﬁnal result:

THEOREM 9.4. We can store S in (1 + ǫ)nH + o(n) bits such that later, given
the endpoints of a range and τ , we can return the τ -majorities for that range in

lg w (cid:17) time.

O(cid:16)(1/τ ) lg lg(1/τ )
Notice our solution in Theorem 9.4 takes optimal O(1/τ ) time when 1/τ = lgO(1) n.
Again, we can also support access and select in O(1) time and rank in O(lg lg σ) time.
10. FREQUENT RANGE MODES
We note that we can use our data structures from Theorem 9.3 to ﬁnd a range mode
quickly when it is actually reasonably frequent. Suppose we want to ﬁnd the mode x of
S[i..j]. To do this, we perform multiple range τ -majority queries on S[i..j], starting with
τ = 1 and repeatedly reducing it by a factor of 2 until we ﬁnd at least one τ -majority.
This takes

times in S[i..j]. We use rank queries to determine which of these elements is the mode

j−i+1

j−i+1

occ(x,S[i..j])(cid:7)

j−i+1

occ(x, S[i..j]) (cid:19)
occ(x,S[i..j])(cid:7)(cid:19) lg lg σ(cid:19) = O(cid:18) (j − i + 1) lg lg σ
occ(x,S[i..j])(cid:17) elements that occur at least

O(cid:18)(cid:18)1 + 2 + 4 + . . . + 2(cid:6)lg
time and returns a list of the O(cid:16)
occ(x,S[i..j]) (cid:17) time.
x, again in O(cid:16) (j−i+1) lg lg σ
endpoints i and j, we can return the mode x of S[i..j] in O(cid:16) (j−i+1) lg lg σ

occ(x,S[i..j]) (cid:17) time.

2(cid:6)lg

j−i+1

THEOREM 10.1. We can store S in nH + o(n)(H + 1) bits such that later, given

11. CONCLUSIONS
We have given the ﬁrst linear-space data structure for parameterized range majority
with query time O(1/τ ), which is worst-case optimal in terms of n and τ . Moreover,
we have improved the space bounds for parameterized range majority and minority in
the important case of variable τ . For parameterized range majority with variable τ , we
have achieved nearly linear space and worst-case optimal query time, or compressed
space with a slight slowdown. For parameterized range minority, we have improved
Chan et al.’s solution to use nearly compressed space with no slowdown or compressed

ACM Transactions on Algorithms, Vol. V, No. N, Article A, Publication date: January YYYY.

Frequency-Sensitive Queries in Ranges

A:21

space with nearly no slowdown. We leave as an open problem achieving linear or compressed 
space with O(1/τ) query time for variable τ , or showing that this is impossible.
ACKNOWLEDGMENTS

Many thanks to Patrick Nicholson for helpful comments.

REFERENCES

Alstrup, S., Brodal, G., & Rauhe, T. 2001. Optimal static range reporting in one dimension.
 Pages 476–482 of: Proceedings of the 33rd ACM Symposium on Theory of
Computing (STOC).

Andersson, Arne, & Thorup, Mikkel. 2007. Dynamic ordered sets with exponential

search trees. J. ACM, 54(3), 13.

Barbay, J´er´emy, Claude, Francisco, Gagie, Travis, Navarro, Gonzalo, & Nekrich, Yakov.

2013. Efﬁcient fully-compressed sequence representations. Algorithmica, 1–37.

Belazzougui, D., & Navarro, G. Alphabet-independent compressed text indexing. ACM

Transactions on Algorithms. To appear.

Belazzougui, D., & Navarro, G. 2011. Alphabet-independent compressed text indexing.
 Pages 748–759 of: Proceedings of the 19th European Symposium on Algorithms
(ESA).

Belazzougui, D., & Navarro, G. 2012. New lower and upper bounds for representing
sequences. Pages 181–192 of: Proceedings of the 20th European Symposium on Algorithms 
(ESA).

Belazzougui, D., Boldi, P., & Vigna, S. Succinct indexes for predecessor search with

distance-sensitive query times. Unpublished.

Belazzougui, Djamal, Boldi, Paolo, Pagh, Rasmus, & Vigna, Sebastiano. 2009. Monotone 
minimal perfect hashing: searching a sorted table with O (1) accesses. Pages
785–794 of: Proceedings of the twentieth Annual ACM-SIAM Symposium on Discrete
Algorithms. Society for Industrial and Applied Mathematics.

Belazzougui, Djamal, Boldi, Paolo, Pagh, Rasmus, & Vigna, Sebastiano. 2010. Fast

Preﬁx Search in Little Space, with Applications. Pages 427–438 of: ESA (1).

Bose, P., Kranakis, E., Morin, P., & Tang, Y. 2005. Approximate range mode and range
median queries. Pages 377–388 of: Proceedings of the 22nd Symposium on Theoretical 
Aspects of Computer Science (STACS).

Bose, P., Dou¨ıeb, K., Dujmovic, V., Howat, J., & Morin, P. 2010. Fast local searches and
updates in bounded universes. Pages 261–264 of: Proceedings of the 22nd Canadian
Conference on Computational Geometry (CCCG).

Bose, P., Dou¨ıeb, K., Dujmovic, V., Howat, J., & Morin, P. 2012. Fast local searches
and updates in bounded universes. Computational Geometry. Advance access, DOI
10.1016/j.comgeo.2012.01.002.

Chan, T. M., Durocher, S., Skala, M., & Wilkinson, B. T. 2012a. Linear-space data
structures for range minority query in arrays. Pages 295–306 of: Proceedings of the
13th Scandinavian Symposium and Workshops on Algorithm Theory (SWAT).

Chan, T. M., Durocher, S., Larsen, K. G., Morrison, J., & Wilkinson, B. T. 2012b. Linearspace 
data structures for range mode query in arrays. Pages 290–301 of: Proceedings
of the 29th Symposium on Theoretical Aspects of Computer Science (STACS).

Chan, Timothy M., & Wilkinson, Bryan T. 2013. Adaptive and Approximate Orthogonal 
Range Counting. Pages 241–251 of: Proceedings of the 24th Symposium on
Discrete Algorithms (SODA).

Cormode, G., & Muthukrishnan,

Data Stream Methods.
http://www.cs.rutgers.edu/∼muthu/198-3.pdf. Lecture 3 of Rutger’s 198:671
Seminar on Processing Massive Data Sets.

S.

2003.

ACM Transactions on Algorithms, Vol. V, No. N, Article A, Publication date: January YYYY.

A:22

D. Belazzougui, T. Gagie and G. Navarro

Demaine, E. D., L´opez-Ortiz, A., & Munro, J. I. 2002. Frequency estimation of internet 
packet streams with limited space. Pages 348–360 of: Proceedings of the 10th
European Symposium on Algorithms (ESA).

Durocher, S., He, M., Munro, J. I., Nicholson, P. K., & Skala, Matthew. 2013a. Range
Information and Computation, 222,

majority in constant time and linear space.
169–179.

Durocher, Stephane, Shah, Rahul, Skala, Matthew, & Thankachan, Sharma V. 2013b.
Linear-Space Data Structures for Range Frequency Queries on Arrays and Trees.
Pages 325–336 of: Proceedings of the 38th Symposium on Mathematical Foundations
of Computer Science (MFCS).

Elmasry, A., Munro, J. I., & Nicholson, P. K. 2011. Dynamic range majority data structures.
 Pages 150–159 of: Proceedings of the 22nd International Symposium on Algorithms 
and Computation (ISAAC).

Ferragina, P., Manzini, G., M ¨akinen, V., & Navarro, G. 2007. Compressed representations 
of sequences and full-text indexes. ACM Transactions on Algorithms, 3(2).

Fischer, J. 2010. Optimal succinctness for range minimum queries. Pages 158–169
of: Proceedings of the 9th Latin American Symposium on Theoretical Informatics
(LATIN).

Fredman, Michael L, & Koml´os, J ´anos. 1984. On the size of separating systems and
families of perfect hash functions. SIAM Journal on Algebraic Discrete Methods,
5(1), 61–68.

Gagie, T., He, M., Munro, J. I., & Nicholson, P. K. 2011. Finding frequent elements
in compressed 2D arrays and strings. Pages 295–300 of: Proceedings of the 18th
Symposium on String Processing and Information Retrieval (SPIRE).

Golynski, A., Munro, I., & Rao, S. 2006. Rank/select operations on large alphabets: a
tool for text indexing. Pages 368–373 of: Proc. 17th Annual ACM-SIAM Symposium
on Discrete Algorithms (SODA).

Greve, M., Jørgensen, A. G., Larsen, K. D., & Truelsen, J. 2010. Cell probe lower
bounds and approximations for range mode. Pages 605–616 of: Proceedings of the
37th International Colloquium on Automata, Languages and Programming (ICALP).
Grossi, Roberto, Orlandi, Alessio, Raman, Rajeev, & Rao, S. Srinivasa. 2009. More
Haste, Less Waste: Lowering the Redundancy in Fully Indexable Dictionaries. Pages
517–528 of: STACS.

Grossi, Roberto, Orlandi, Alessio, & Raman, Rajeev. 2010. Optimal trade-offs for succinct 
string indexes. Pages 678–689 of: Automata, Languages and Programming.
Springer.

Hagerup, T., & Tholey, T. 2001. Efﬁcient Minimal Perfect Hashing in Nearly Minimal 
Space. Pages 317–326 of: Proceedings of the 18th International Symposium on
Theoretical Aspects of Computer Science (STACS).

Hon, W.-K., Shah, R., & Vitter, J. 2009. Space-Efﬁcient Framework for Top-k String
Retrieval Problems. Pages 713–722 of: Proc. 50th IEEE Annual Symposium on Foundations 
of Computer Science (FOCS).

Johnson, Donald B. 1981. A priority queue in which initialization and queue operations

takeO (loglogD) time. Mathematical Systems Theory, 15(1), 295–309.

Karp, R. M., Shenker, S., & Papadimitriou, C. H. 2003. A simple algorithm for ﬁnding
frequent elements in streams and bags. ACM Transactions on Database Systems,
28(1), 51–55.

Karpinski, M., & Nekrich, Y. 2008. Searching for frequent colors in rectangles. Pages
11–14 of: Proceedings of the 20th Canadian Conference on Computational Geometry
(CCCG).

Krizanc, D., Morin, P., & Smid, M. H. M. 2005. Range mode and range median queries

on lists and trees. Nordic Journal of Computing, 12(1), 1–17.

ACM Transactions on Algorithms, Vol. V, No. N, Article A, Publication date: January YYYY.

Frequency-Sensitive Queries in Ranges

A:23

Lai, Y. K., Poon, C. K., & Shi, B. 2008. Approximate colored range and point enclosure

queries. Journal of Discrete Algorithms, 6(3), 420–432.

Misra, J., & Gries, D. 1982. Finding repeated elements. Science of Computer Programming,
 2(2), 143–152.

Muthukrishnan, S. 2002. Efﬁcient algorithms for document retrieval problems. Pages

657–666 of: Proceedings of the 13th Symposium on Discrete Algorithms (SODA).

Petersen, H. 2008. Improved bounds for range mode and range median queries. Pages
418–423 of: Proceedings of the 34th Conference on Current Trends in Theory and
Practice of Computer Science (SOFSEM).

Petersen, H., & Grabowski, S. 2009. Range mode and range median queries in constant

time and sub-quadratic space. Information Processing Letter, 109(4), 225–228.

P ˇatras¸cu, M. 2008. Succincter. Pages 305–313 of: Proceedings of the 49th Symposium

on Foundations of Computer Science (FOCS).

Raman, Rajeev, Raman, Venkatesh, & Satti, Srinivasa Rao. 2007. Succinct indexable
dictionaries with applications to encoding k-ary trees, preﬁx sums and multisets.
ACM Transactions on Algorithms (TALG), 3(4), 43.

Sadakane, K. 2007. Succinct data structures for ﬂexible text retrieval systems. Journal

of Discrete Algorithms, 5(1), 12–22.

Wei, Z., & Yi, K. 2011. Beyond simple aggregates: indexing for summary queries. Pages
117–128 of: Proceedings of the 30th Symposium on Principles of Database Systems
(PODS).

Willard, Dan E. 1983. Log-Logarithmic Worst-Case Range Queries are Possible in

Space Theta(N). Inf. Process. Lett., 17(2), 81–84.

ACM Transactions on Algorithms, Vol. V, No. N, Article A, Publication date: January YYYY.

