Advantages of Backward Searching — Eﬃcient

Secondary Memory and Distributed

Implementation of Compressed Suﬃx Arrays

Veli M¨akinen1, Gonzalo Navarro2,(cid:1), and Kunihiko Sadakane3,(cid:1)(cid:1)

1 Dept. of Computer Science, Univ. of Helsinki, Finland

2 Center for Web Research, Dept. of Computer Science, Univ. of Chile, Chile

vmakinen@cs.helsinki.fi

3 Dept. of Computer Science and Communication Engineering, Kyushu Univ., Japan

gnavarro@dcc.uchile.cl

sada@csce.kyushu-u.ac.jp

Abstract. One of the most relevant succinct suﬃx array proposals in
the literature is the Compressed Suﬃx Array (CSA) of Sadakane [ISAAC
2000]. The CSA needs n(H0 + O(log log σ)) bits of space, where n is
the text size, σ is the alphabet size, and H0 the zero-order entropy of
the text. The number of occurrences of a pattern of length m can be
computed in O(m log n) time. Most notably, the CSA does not need
the text separately available to operate. The CSA simulates a binary
search over the suﬃx array, where the query is compared against text
substrings. These are extracted from the same CSA by following irregular
access patterns over the structure. Sadakane [SODA 2002] has proposed
using backward searching on the CSA in similar fashion as the FM-index
of Ferragina and Manzini [FOCS 2000]. He has shown that the CSA can
be searched in O(m) time whenever σ = O(polylog(n)).

In this paper we consider some other consequences of backward searching 
applied to CSA. The most remarkable one is that we do not need,
unlike all previous proposals, any complicated sub-linear structures based
on the four-Russians technique (such as constant time rank and select
queries on bit arrays). We show that sampling and compression are
enough to achieve O(m log n) query time using less space than the original 
structure. It is also possible to trade structure space for search time.
Furthermore, the regular access pattern of backward searching permits
an eﬃcient secondary memory implementation, so that the search can
be done with O(m logB n) disk accesses, being B the disk block size.
Finally, it permits a distributed implementation with optimal speedup
and negligible communication eﬀort.

(cid:1) Funded by Millennium Nucleus Center for Web Research, Grant P01-029-F, Mideplan,
 Chile.

(cid:1)(cid:1) Partially funded by the Grant-in-Aid of the Ministry of Education, Science, Sports

and Culture of Japan.

R. Fleischer and G. Trippen (Eds.): ISAAC 2004, LNCS 3341, pp. 681–692, 2004.
c(cid:1) Springer-Verlag Berlin Heidelberg 2004

682

V. M¨akinen, G. Navarro, and K. Sadakane

1

Introduction

The classical problem in string matching is to determine the occurrences of a
short pattern P = p1p2 . . . pm in a large text T = t1t2 . . . tn. Text and pattern
are sequences of characters over an alphabet Σ of size σ. Usually the same text
is queried several times with diﬀerent patterns, and therefore it is worthwhile to
preprocess the text in order to speed up the searches. Preprocessing builds an
index structure for the text.

To allow fast searches for patterns of any size, the index must allow access to
all suﬃxes of the text (the ith suﬃx of T is titi+1 . . . tn). These kind of indexes
are called full-text indexes. Optimal query time, which is O(m) as every character
of P must be examined, can be achieved by using the suﬃx tree [25, 12, 23] as
the index.

The suﬃx tree takes much more memory than the text. In general, it takes
O(n log n) bits, while the text takes n log σ bits1. A smaller constant factor is
achieved by the suﬃx array [10]. Still, the space complexity does not change.
Moreover, the searches take O(m log n) time with the suﬃx array (this can be
improved to O(m + log n) using twice the original amount of space [10]).

The large space requirement of full-text indexes has raised the interest on
indexes that occupy the same amount of space as the text itself, or even less.
For example, the Compressed Suﬃx Array (CSA) of Sadakane [19] takes in
practice the same amount of space as the text compressed with a zero-order
model. Moreover, the CSA does not need the text at all, since the text is included 
in the index. Existence and counting queries on the CSA take O(m log n)
time.

There are also other so-called succinct full-text indexes that achieve good
tradeoﬀs between search time and space complexity [3, 9, 7, 22, 5, 14, 18, 16, 4].
Most of these are opportunistic as they take less space than the text itself, and
also self-indexes as they contain enough information to reproduce the text: A
self-index does not need the text to operate.

Recently, several space-optimal self-indexes have been proposed [5, 6, 4], whose
space requirement depends on the k-th order empirical entropy with constant
factor one (except for the sub-linear parts). These indexes achieve good query
performances in theory, but they are complex to implement as such.

2

Summary of Results

In this paper, we concentrate on simplifying and generalizing earlier work on
succinct self-indexes. We build on the Sadakane’s CSA [19], which we brieﬂy
review in the following.

The CSA searches the pattern by simulating a binary search over the suﬃx
array. The search string must be compared against some text substring at each
step of the binary search. Since the text is not stored, the CSA must be traversed

1 By log we mean log2 in this paper.

Advantages of Backward Searching

683

in irregular access patterns to extract each necessary text substring. This makes
it unappealing e.g. for a secondary memory implementation.

Sadakane [20] has proposed using backward searching on the CSA in similar
fashion as the FM-index of Ferragina and Manzini [3]. Sadakane has shown that
the CSA can be searched in O(m) time whenever σ = O(polylog(n)). The CSA
scales much better than the FM-index as the alphabet size grows.

Backward searching has also some other consequences than improved search
time, as we will show in this paper. We exploit the fact that the access pattern
becomes much more regular. The most important consequence of this is that a
simpler implementation of the CSA is possible: All previous proposals heavily
rely on sublinear structures based on the so-called four-Russians technique [1]
to support constant time rank and select queries on bit arrays [8, 13, 2] (rank(i)
to ﬁnd out how many bits are set before position i, and select(j) to ﬁnd out
the position of the jth bit from the beginning). We show that these structures
are not needed for an eﬃcient solution, but rather one can do with sampling
and traditional compression. This is a distinguishing feature of our proposal.
The absence of four-Russians tricks makes our index usable on weaker machine
models and also makes it easier to implement.

(cid:1)(cid:2)−1

Under this simpler implementation scenario, we are able to retain the original
O(m log n) search time, and improve the original n(H0 + O(log log σ)) space to
n(H0 + ε)(1 + o(1)), for any ε > 0. The search time can be reduced gradually,
to O((cid:1)m/(cid:4)(cid:2) log n) time, up to O(m log σ + log n). The price is that the space
requirement increases to n(
Hi + ε)(1 + o(1)), being Hi the order-i empirical 
entropy of T . We also give an alternative implementation where the space
requirement depends on Hk, for any k. Furthermore, the CSA becomes amenable
of a secondary memory implementation. If we call B the disk block size, then
we can search the CSA in secondary memory using O(m logB n) disk accesses.
Finally, we show that the structure permits a distributed implementation with
optimal speedup and negligible communication eﬀort. Table 1 compares the original 
and the new CSA.

i=0

Table 1. Space and time complexities of the original and new CSA implementations

Original CSA Our CSA, version 1

Our CSA, version 2

n(H0 + O(log log σ)) n(

i=0 Hi + ε) 2n(Hk(log σ + log log n) + ε)

Space (bits)
Search time
Disk search time
Remote messages

O(m log n)
O(m log n)

m log n

(cid:1)(cid:2)−1
O((cid:1)m/(cid:3)(cid:2) log n)
O((cid:1)m/(cid:3)(cid:2) logB n)

(cid:1)m/(cid:3)(cid:2)

O(m log n)
O(m logB n)

m

Our solution takes about the same amount of space as Sadakane’s improvement 
in [20]. Our search time is worse by a log n factor. However, our structure 
works on more general alphabets; we only assume that σ = o(n/ log n), as
Sadakane’s improvement in [20] assumes that σ = O(polylog(n)). The spaceoptimal 
solutions [5, 6, 4] have also similar restrictions on the alphabet size.

684

V. M¨akinen, G. Navarro, and K. Sadakane

3 The Compressed Suﬃx Array (CSA) Structure

Let us ﬁrst describe the basic suﬃx array data structure. Given a text T =
t1t2 . . . tn, we consider the n text suﬃxes, so that the j-th suﬃx of T is tjtj+1 . . . tn.
We assume that a special endmarker “$” has been appended to T , such that the
endmarker is smaller than any other text character. The suﬃx array A of T is
the set of suﬃxes 1 . . . n, arranged in lexicographic order. That is, the A[i]-th
suﬃx is lexicographically smaller than the A[i+1]-th suﬃx of T for all 1 ≤ i < n.
Given the suﬃx array, the search for the occurrences of the pattern P =
p1p2 ··· pm is trivial. The occurrences form an interval [sp, ep] in A such that
suﬃxes tA[i]tA[i]+1 ··· tn, sp ≤ i ≤ ep, contain the pattern as a preﬁx. This
interval can be searched for using two binary searches in time O(m log n).
The compressed suﬃx array (CSA) structure of Sadakane [19] is based on that
of Grossi and Vitter [7]. In the CSA, the suﬃx array A[1 . . . n] is represented
by a sequence of numbers Ψ(i), such that A[Ψ(i)] = A[i] + 1. Furthermore, the
sequence is diﬀerentially encoded, Ψ(i) − Ψ(i − 1). If there is a self-repetition,
that is A[j . . . j + (cid:4)] = A[i . . . i + (cid:4)] + 1, then Ψ(i . . . i + (cid:4)) = j . . . j + (cid:4), and
Ψ(i(cid:2)) − Ψ(i(cid:2) − 1) = 1 for i < i(cid:2) ≤ i + (cid:4). Hence the diﬀerential array is encoded
with a method that favors small numbers and permits constant time access to
Ψ. Note in particular that Ψ values are increasing in the areas of A where the
suﬃxes start with the same character a, because ax < ay iﬀ x < y.
Additionally, the CSA stores an array C[1 . . . σ], such that C[c] is the number
of occurrences of characters {$, 1, . . . , c − 1} in the text T . Notice that all the
suﬃxes A[C[c] + 1] . . .A[C[c + 1]] start with character c. The text is discarded.
A binary search over A is simulated by extracting from the CSA strings of the
form tA[i]tA[i]+1tA[i]+2 . . . for any index i required by the binary search. The ﬁrst
character tA[i] is easy to obtain because all the ﬁrst characters of suﬃxes appear
in order when pointed from A, so tA[i] is the character c such that C[c] < i ≤
C[c+1]. This is found in constant time by using small additional structures based
on the four-Russians technique [8, 13, 2]. Once the ﬁrst character is obtained, we
move to i(cid:2) ← Ψ(i) and go on with tA[i(cid:1)] = tA[i]+1. We continue until the result of
the lexicographical comparison against the pattern P is clear. The overall search
complexity is the same as with the original suﬃx array, O(m log n).

Note that each string comparison may require accessing up to m arbitrary
cells in the Ψ array (see Fig. 1). Hence using the CSA in secondary memory is
not attractive because of the scattered access pattern. Also, a complex part in
the implementation of the CSA is the compression of the Ψ array, since it must
support constant time direct access at any position. This is achieved in [19] by
using four-Russians techniques, in n(H0 + O(log log σ)) bits of space.

Notice that the above search only solves existence and counting queries: We
ﬁnd the interval of the suﬃx array that would contain suﬃxes of the text matching 
the pattern. The pointers to suﬃxes are not stored explicitly, and hence we
cannot report the occurrences or show the text context around them. The solution 
is to sample suﬃxes 1, log n, . . ., and use the Ψ function to retrieve the
unsampled ones [19]. We will only consider counting queries in the sequel, since
we can use the sampling technique as is to report occurrences.

Advantages of Backward Searching

685

A

C

G

T

Fig. 1. One step of binary search for pattern P = CCAGT A. The blocks correspond to
the areas of suﬃx array whose suﬃxes start with the corresponding letter. The straight
arrow on top indicates the suﬃx the pattern is compared against. Other arrows indicate
the extraction of the preﬁx of the compared suﬃx. The extraction ends at letter G,
and hence the suﬃx does not correspond to an occurrence, and the search is continued
to the left of the current point

4 Backward Search on CSA

Sadakane [20] has proposed using backward search on the CSA. Let us review
how this search proceeds. We use the notation R(X), for a string X, to denote
the range of suﬃx array positions corresponding to suﬃxes that start with X.
The search goal is therefore to determine R(P ). We start by computing R(pm)
simply as R(pm) = [C[pm] + 1, C[pm + 1]]. Now, in the general case, given
R(P [i + 1 . . . m]), it turns out that R(P [i . . . m]) consists exactly of the suﬃx
array positions in R(pi) containing values j such that j + 1 appears in suﬃx
array positions in R(P [i + 1 . . . m]). That is, the occurrences of P [i . . . m] are
the occurrences of pi followed by occurrences of P [i + 1 . . . m]. Since A[Ψ(i)] =
A[i] + 1, it turns out that

x ∈ R(P [i . . . m]) ⇔ x ∈ R(pi) ∧ Ψ(x) ∈ R(P [i + 1 . . . m])

Now, Ψ can be accessed in constant time, and its values are increasing inside
R(pi). Hence, the set of suﬃx array positions x such that Ψ(x) is inside some
range forms a continuous range of positions and can be binary searched inside
R(pi), at O(log n) cost. Therefore, by repeating this process m times we ﬁnd
R(P ) in O(m log n) time.

Fig. 2 gives the pseudocode of the algorithm, and Fig. 3 illustrates.
Note that the backward search (as explained here) does not improve the
original CSA search cost. However, it is interesting that the backward search
does not use the text at all, while the original CSA search algorithm is based on
the concept of extracting text strings to compare them against P . These string
extractions make the access pattern to array Ψ irregular and non-local.

In the backward search algorithm, the accesses to Ψ always follow the same
pattern: binary search inside R(c), for some character c. In the next sections
we study diﬀerent ways to take advantage of this feature. This is where our
exposition diﬀers from [20].

686

V. M¨akinen, G. Navarro, and K. Sadakane

Algorithm BackwardCSA(P, C, Ψ):

lef tm+1 := 1; rightm+1 := n;
for i := m downto 1 do begin

lef ti = min{j ∈ [C[pi] + 1, C[pi + 1]], Ψ(j) ∈ [lef ti+1, righti+1]};
righti = max{j ∈ [C[pi] + 1, C[pi + 1]], Ψ(j) ∈ [lef ti+1, righti+1]};
if lef ti > righti return “no occurrences found”;

return “right1 − lef t1 + 1 occurrences found”

Fig. 2. Backward search algorithm over the CSA. Functions “min” and “max” stand
for binary searches

A
A

C
C

G
G

T
T

A
A

C
C

G
G

T
T

A
A

C
C

G
G

T
T

A

C

G

T

.

.

.

A

C

G

T

A

C

G

T

Fig. 3. Searching for pattern P = CCAGT A backwards (right-to-left). The situation
after reading each character is plotted. The gray-shaded regions indicate the interval
of the suﬃx array that contain the current pattern suﬃx. The computation of the new
interval is illustrated in the second step (starting from right). The Ψ values from the
block of letter G point to consecutive positions in the suﬃx array. Hence it is easy to
binary search the top-most and bottom-most pointers that are included in the previous
interval

5

Improved Search Complexity

A ﬁrst improvement due to the regular access pattern is the possibility of reducing 
the search complexity from O(m log n) to O(m log σ + log n). Albeit in [20]
they obtain O(m) search time, the more modest improvement we obtain here
does not need any four-Russians technique.

The idea is that we can extend the C array so as to work over strings of
length (cid:4) ((cid:4)-grams) rather than over single characters. Given (cid:4)-gram x, C[x] is
the number of text (cid:4)-grams that are lexicographically smaller than x. The ﬁnal
(cid:4) − 1 suﬃxes of length less than (cid:4) are accounted as (cid:4)-grams by appending them
as many “$” characters as necessary.
With this C array, we can search for pattern P of length m in O((cid:1)m/(cid:4)(cid:2) log n)
time as follows. We ﬁrst assume that m is a multiple of (cid:4). Let us write P =

Advantages of Backward Searching

687

G1G2 . . . Gm/(cid:2), where all Gi are all of length (cid:4). We start by directly obtaining 
R(Gm/(cid:2)) = [C[Gm/(cid:2)] + 1, C[next(Gm/(cid:2))]], where next(x) is the string of
length |x| that lexicographically follows x (if no such string exists, then assume 
C[next(Gm/(cid:2))] = n). Once this is known, we binary search in R(Gm/(cid:2)−1)
the subinterval that points inside R(Gm/(cid:2)). This becomes R(Gm/(cid:2)−1Gm/(cid:2)). The
search continues until we obtain R(P ). The number of steps performed is m/(cid:4),
each being a binary search of cost O(log n).
Let us consider now the case where (cid:4) does not divide m. We extend P so
that its length is a multiple of (cid:4). Let e = m − (m mod (cid:4)). Then we build two
patterns out of P . The ﬁrst is Pl, used to obtain the left limit of R(P ). Pl is
the lexicographically smallest (cid:4)-gram that is not smaller than P , Pl = P $e,
that is, P followed by e occurrences of character “$”. The second is Pr, used
to obtain the right limit of R(P ). Pr is the smallest (cid:4)-gram that is lexicographically 
larger than any string preﬁxed by P , Pr = next(P )$e. Hence, we
search for Pl and Pr to obtain R(Pl) = [spl, epl] and R(Pr) = [spr, epr]. Then,
R(P ) = [spl, spr − 1].

case we do not need to search for Pr, as we use spr = n + 1.

Note that next(x) is not deﬁned if x is the largest string of its length. In this
We have obtained O((cid:1)m/(cid:4)(cid:2) log n) search time, at the cost of a C table with
σ(cid:2) entries. If we require that C can be stored in n bits, then σ(cid:2) log n = n, that is,
(cid:4) = logσ n − logσ log n. The search complexity becomes O((cid:1)m/ logσ n(cid:2) log n) =
O(m log σ + log n) as promised.
Moreover, we can reduce the C space complexity to O(n/ logt n) for any
constant t. The condition σ(cid:2) log n = n/ logt n translates to (cid:4) = logσ n − (t +
1) logσ log n, and the search cost remains O(m log σ + log n).

Notice that we cannot use the original Ψ function anymore to ﬁnd the subintervals,
 since we read (cid:4) characters at a time. Instead, we need to store values
Ψ (cid:2)[i] = Ψ[Ψ[··· Ψ[i]]···], where Ψ function is recursively applied (cid:4) times. Next
section considers how to represent the Ψ (cid:2) values succinctly.

6 A Simpler and Smaller CSA Structure

One of the diﬃculties in implementing the CSA is to provide constant time access 
to array Ψ (or Ψ (cid:2) using the search procedure from previous section). This is
obtained by storing absolute samples every O(log n) entries and diﬀerential encoding 
for the others, and hence several complex four-Russians-based structures
are needed to access between samples in constant time.

Binary Search on Absolute Samples. Our binary searches inside R(pi),
instead, could proceed over the absolute samples ﬁrst. When the correct interval
between samples has been found, we decode the O(log n) entries sequentially
until ﬁnding the appropriate entry. The complexity is still O(log n) per binary
search (that is, O(log n) accesses for the initial binary search plus O(log n) for
the ﬁnal sequential search), and no extra structure is needed to access Ψ (or Ψ (cid:2)).
The search is illustrated in Fig. 4.

688

V. M¨akinen, G. Navarro, and K. Sadakane

G

Fig. 4. Binary search followed by sequential search. The top-most sampled value closest
to the previous interval is found using binary search (indicated by the top-most solid
arrow). Then the next Ψ values are decoded until the ﬁrst value inside the interval (if
exists) is encountered (indicated by the top-most dashed arrow). The same is repeated
to ﬁnd the bottom-most sampled value and then the bottom-most encoded value

εn

We ﬁrst consider how Ψ can be encoded. We store

2 log n = O(n/ log n) absolute 
samples of Ψ. For each such sample, we need to store value Ψ in log n bits,
as well as a pointer to its corresponding position in the diﬀerentially encoded Ψ
sequence, in other log n bits. Overall, the absolute samples require εn bits, for
any ε > 0, and permit doing each binary search in log n + 2
ε log n = O(log n)
steps. On the other hand, array C needs o(n) bits by choosing any t > 1 for its
O(n/ logt n) entries.

The most important issue is how to eﬃciently encode the diﬀerences between
consecutive Ψ cells. The n(H0 + O(log log σ)) space complexity of the original
CSA is due to the need of constant time access inside absolute samples, which
forces the use of a zero-order compressor. In our case, we could use any compression 
mechanism between absolute samples, because they will be decoded
sequentially.

Compressing Ψ Using Elias Encoding. We now give a simple encoding that
slightly improves the space complexity of the original CSA.
The diﬀerences Ψ(i) − Ψ(i − 1) can be encoded eﬃciently using Elias delta
coding [26]. Let b(p) be the binary string representation of a number p. We
use 1|b(r)|0b(r)b(p) to encode p, where r = |b(p)|. The length of the encoding is
log(2 log p + 1) + 1 + log p = log p(1 + o(1)). The overall length of the codes for
all diﬀerences can be bounded using the following observation: The Ψ values are
increasing inside a suﬃx array region corresponding to a character c. In other
words,

To encode the diﬀerences for character c, we thus need

i,i−1∈R(c)(1 +
o(1)) log(Ψ(i) − Ψ(i − 1)) bits. This becomes |R(c)|(1 + o(1)) log(n/|R(c)|) in
the worst case, where |R(c)| = r − (cid:4) + 1 is the length of the range R(c) = [(cid:4), r].

(cid:1)

(cid:2)

i,i−1∈R(c)

|Ψ(i) − Ψ(i − 1)| =

Ψ(i) − Ψ(i − 1) ≤ n.

(1)

(cid:2)

i,i−1∈R(c)

Advantages of Backward Searching

689

Summing over all characters gives

(cid:2)

c∈Σ

|R(c)|(1 + o(1)) log(n/|R(c)|) = nH0(1 + o(1)).

(2)

Hence the overall structure occupies n(H0 + ε)(1 + o(1)) bits.
Also, the “small additional structures” mentioned in Section 3, used to ﬁnd
in constant time the character c such that C[c] < i ≤ C[c + 1], are not anymore
necessary. These also made use of four-Russians techniques.
Let us consider how to decode a number p coded with Elias. We can read
1|b(r)|0bitwise in O(|b(r)|) = O(log r) = O(log |b(p)|) = O(log log p) = O(log log n)
time. Then we get b(r) and b(p) in constant time. The complexity can be lowered
)0b(r(cid:2))b(r), where r(cid:2) = |b(r)|. Indeed, we
to O(log log log n) if we code r as 1b(r(cid:1)
∗ n).
can apply this until the number to code is zero, for a complexity of O(log
Alternatively, we can decode Elias in constant time by only small abuse of fourRussians 
technique: Precompute the ﬁrst zero of any sequence of length log log n
and search the table with the ﬁrst bits of 1|b(r)|0b(r)b(p). This table needs only
O(log n log log n) space.

Due the lack of space we omit the analysis for encoding Ψ (cid:2). In the full version 
we show that Ψ (cid:2) can be encoded to n
0≤i<(cid:2) Hi bits. We also give an
alternative encoding for Ψ combining run-length encoding and Elias encoding to
achieve a 2n(Hk(H +log log n)+ ε)(1+ o(1)) bits representation of the structure.
(Meanwhile, these analyses appear in a technical report [15–Chapter 5].)

(cid:1)

7 A Secondary Memory Implementation

We show now how the regular access pattern can be exploited to obtain an
eﬃcient implementation in secondary memory, where the disk blocks can accommodate 
B log n bits.

Let us consider again the absolute samples. We pack all the O(n/ log n) absolute 
samples together, using O(n/(B log n)) blocks. However, the samples are
stored in a level-by-level order of the induced binary search tree: For each character 
c, we store the root of the binary search hierarchy corresponding to the
area C[c] + 1 . . . C[c + 1], that is, Ψ((cid:8)((C[c] + 1) + C[c + 1])/2(cid:9)). Then we store
the roots of the left and right children, and so on. When the disk block is ﬁlled,
the subtrees are recursively packed into disk pages. Fig. 5 illustrates.

Using this arrangement, any binary search inside the area of a character
c can make the ﬁrst log B accesses by reading only the ﬁrst disk block. Each
new disk block read permits making other log B accesses. Overall, we ﬁnd the
interval between two consecutive samples in O(log(n)/ log(B)) = O(logB n) disk
accesses.

The compressed entries of Ψ are stored in contiguous disk pages. Once we
determine the interval between consecutive samples, we sequentially read all the
necessary disk pages. This requires reading O(log(n)/B) additional disk pages,
which contributes a lower order term to the cost.

690

V. M¨akinen, G. Navarro, and K. Sadakane

Fig. 5. Packing of array cells to optimize binary search in secondary memory. The
dashed boxes indicate cells mapped to a disk block. Any binary search on the array at
the bottom is carried out with 2 disk accesses

Overall, we can maintain the CSA on disk and search it in O(m logB n) disk
accesses. The original structure requires O(m log n) disk accesses. If we can hold
O(n) bits in main memory, then we can cache all the absolute samples and pay
only O(m log(n)/B) disk accesses.

This scheme can be extended to use a table C of (cid:4)-grams rather than of
individual characters. Each individual binary search takes still O(logB n) time,
but we perform only (cid:1)m/(cid:4)(cid:2) of them.

One obstacle to a secondary memory CSA implementation might be in building 
such a large CSA. This issue has been addressed satisfactorily [21].

8 A Distributed Implementation

Distributed implementations of suﬃx arrays face the problem that not only the
suﬃx array, but also the text, are distributed. Hence, even if we distribute suﬃx
array A according to lexicographical intervals, the processor performing the local
binary search will require access to remote text positions [17]. Although some
heuristics have been proposed, log n remote requests for m characters each are
necessary in the worst case.

The original CSA does not help solve this. If array Ψ is distributed, we will
need to request cells of Ψ to other processors for each character of each binary
search step, for a total of m log n remote requests. Actually this is worse than
log n requests for m characters each.

The backward search mechanism permits us to do better. Say that one processor 
is responsible for the interval corresponding to each character. Then, we
can process pattern characters pm, pm−1, . . ., p1 as follows: The processor responsible 
for pm sends R(pm) to the processor responsible for pm−1. That processor

Advantages of Backward Searching

691

binary searches in its local memory for the cells that point inside R(pm), without 
any communication need. Upon completing the search, it sends R(pm−1pm)
to the processor responsible for pm−2 and so on. After m communication steps
exchanging O(1) data, we have the answer.

In the BSP model [24], we need m supersteps of O(log n) CPU work and
O(1) communication each. In comparison, the CSA needs O(m log n) supersteps
of O(1) CPU and communication each, and the basic suﬃx array needs O(log n)
supersteps of O(m) CPU and communication each.

More or less processors can be accommodated by coarsening or reﬁning the
lexicographical intervals. Although the real number of processors, r, is irrelevant
to search for one pattern (note that the elapsed time is still O(m log n)), it
becomes important when performing a sequence of searches.

If N search patterns, each of length m, are entered into the system, and we
assume that the pattern characters distribute uniformly over Σ, then a pipelining
eﬀect occurs. That is, the processor responsible for pm becomes idle after the ﬁrst
superstep and it can work on subsequent patterns. On average the N answers
are obtained after N m/r supersteps and O(N m log(n)/r) total CPU work, with
O(N m/r) communication work.

Hence, we have obtained O(m log(n)/r) amortized time per pattern with r
processors, which is the optimal speedup over the sequential algorithm. The
communication eﬀort is O(m/r), of lower order than the computation eﬀort.
We can apply again the technique of Section 5 to reduce the CPU time to
O(((cid:1)m/(cid:4)(cid:2) log n)/r).

9 Conclusions

We have proposed a new implementation of the backward search algorithm for
the Compressed Suﬃx Array (CSA). The new method takes advantage of the
regular access pattern to the structure, which allows several improvements over
the CSA: (i) tradeoﬀ between search time and space requirement, (ii) simpler
and more compact structure implementation, (iii) eﬃcient secondary memory
implementation, and (iv) eﬃcient distributed implementation. In particular, ours
is the only succinct full-text index structure not relying on four-Russians techniques.


References

1. V. L. Arlazarov, E. A. Dinic, M. A. Kronrod, and I. A. Faradzev. On economic
construction of the transitive closure of a directed graph. Dokl. Acad. Nauk. SSSR
194, 487–488, 1970 (in Russian). English translation in Soviet Math. Dokl. 11,
1209–1210, 1975.

2. D. Clark. Compact Pat Trees. PhD thesis, University of Waterloo, 1996.
3. P. Ferragina and G. Manzini. Opportunistic data structures with applications. In

Proc. FOCS’00, pp. 390–398, 2000.

692

V. M¨akinen, G. Navarro, and K. Sadakane

4. P. Ferragina, G. Manzini, V. M¨akinen, and G. Navarro. An Alphabet-Friendly
FM-index. To appear in 11th Symposium on String Processing and Information
Retrieval (SPIRE 2004), Padova, Italy, October 5-8, 2004.

5. R. Grossi, A. Gupta, and J. Vitter. High-order entropy-compressed text indexes.

In Proc. SODA’03, pp. 841–850, 2003.

6. R. Grossi, A. Gupta, and J. Vitter. When indexing equals compression: Experiments 
with compressing suﬃx arrays and applications. In Proc. SODA’04, pp.
636-645, 2004.

7. R. Grossi and J. Vitter. Compressed suﬃx arrays and suﬃx trees with applications

to text indexing and string matching. In Proc. STOC’00, pp. 397–406, 2000.

8. G. Jacobson. Succinct Static Data Structures. PhD thesis, CMU-CS-89-112,

Carnegie Mellon University, 1989.

9. J. K¨arkk¨ainen. Repetition-Based Text Indexes, PhD Thesis, Report A-1999-4,

Department of Computer Science, University of Helsinki, Finland, 1999.

10. U. Manber and G. Myers. Suﬃx arrays: A new method for on-line string searches.

SIAM J. Comput., 22, pp. 935–948, 1993.

11. G. Manzini. An Analysis of the Burrows-Wheeler Transform. J. of the ACM

48(3):407–430, 2001.

12. E. M. McCreight. A space economical suﬃx tree construction algorithm. J. of the

ACM, 23, pp. 262–272, 1976.

13. I. Munro. Tables. In Proc. FSTTCS’96, pp. 37–42, 1996.
14. V. M¨akinen. Compact Suﬃx Array — A space-eﬃcient full-text index. Fundamenta

Informaticae 56(1-2), pp. 191–210, 2003.

15. V. M¨akinen and G. Navarro. New search algorithms and space/time tradeoﬀs for
succinct suﬃx arrays. Technical report, C-2004-20, Dept. CS, Univ. Helsinki, April
2004. [http://www.cs.helsinki.ﬁ/u/vmakinen/papers/ssa tech 2004.ps.gz]

16. V. M¨akinen and G. Navarro. Compressed compact suﬃx arrays. In Proc. CPM’04,

LNCS 3109, pp. 420–433, 2004.

17. M. Mar´ın and G. Navarro. Distributed query processing using suﬃx arrays. In

Proc. SPIRE’03, pages 311–325, LNCS 2857, 2003.

18. G. Navarro. Indexing text using the Ziv-Lempel trie. J. of Discrete Algorithms

2(1):87–114, 2004.

19. K. Sadakane. Compressed text databases with eﬃcient query algorithms based on
the compressed suﬃx array. In Proc. ISAAC’00, LNCS 1969, pp. 410–421, 2000.
20. K. Sadakane. Succinct representations of lcp information and improvements in the

compressed suﬃx arrays. In Proc. SODA 2002, ACM-SIAM, pp. 225–232, 2002.

21. K. Sadakane. Constructing compressed suﬃx arrays with large alphabets. In Proc.

ISAAC’03, LNCS 2906, pp. 240–249, 2003.

22. S. Srinivasa Rao. Time-space trade-oﬀs for compressed suﬃx arrays. Inf. Proc.

Lett., 82 (6), pp. 307-311, 2002.

23. E. Ukkonen. On-line construction of suﬃx-trees. Algorithmica, 14, pp. 249–260,

1995.

24. L. Valiant. A bridging model for parallel computation. Comm. ACM, 33:103–111,

Aug. 1990.

25. P. Weiner. Linear pattern matching algorithms. In Proc. IEEE 14th Ann. Symp.

on Switching and Automata Theory, pp. 1–11, 1973.

26. I. Witten, A. Moﬀat, and T. Bell. Managing Gigabytes. Morgan Kaufmann Publishers,
 New York, second edition, 1999.

