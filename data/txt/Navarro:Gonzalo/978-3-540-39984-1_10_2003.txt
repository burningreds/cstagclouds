(S,C)-Dense Coding: An Optimized Compression

Code for Natural Language Text Databases(cid:1)

Nieves R. Brisaboa1, Antonio Fari˜na1, Gonzalo Navarro2, and

Mar´ıa F. Esteller1

1 Database Lab., Univ. da Coru˜na, Facultade de Inform´atica, Campus de Elvi˜na s/n,

{brisaboa,fari}@udc.es, mfesteller@yahoo.es

15071 A Coru˜na, Spain.

2 Dept. of Computer Science, Univ. de Chile, Blanco Encalada 2120, Santiago, Chile.

gnavarro@dcc.uchile.cl

Abstract. This work presents (s, c)-Dense Code, a new method for
compressing natural language texts. This technique is a generalization
of a previous compression technique called End-Tagged Dense Code that
obtains better compression ratio as well as a simpler and faster encoding
than Tagged Huﬀman. At the same time, (s, c)-Dense Code is a preﬁx
code that maintains the most interesting features of Tagged Huﬀman
Code with respect to direct search on the compressed text. (s, c)-Dense
Coding retains all the eﬃciency and simplicity of Tagged Huﬀman, and
improves its compression ratios.
We formally describe the (s, c)-Dense Code and show how to compute the
parameters s and c that optimize the compression for a speciﬁc corpus.
Our empirical results show that (s, c)-Dense Code improves End-Tagged
Dense Code and Tagged Huﬀman Code, and reaches only 0.5% overhead
over plain Huﬀman Code.

1 Introduction

Text compression techniques are based on exploiting redundancies in the text to
represent it using less space [2]. The amount of text collections has grown in the
last years, mainly due to the widespread use of digital libraries, documental
databases, oﬃce automation systems and the Web. Current text databases
contain hundreds of gigabytes and the Web is measured in terabytes. Although
the capacity of new devices to store data grows fast and the associated costs
decrease, the size of text collections increases also faster. Moreover, cpu speed
grows much faster than that of secondary memory devices and networks, so
storing data in compressed form reduces i/o time, which is more and more
convenient even at the expense of for some extra cpu time.

Another advantage of text compression techniques is that all of them allow
(and improve) the use of block addressing indexes. These indexes are smaller

(cid:1) This work is partially supported by CICYT Grant (#TIC2002-04413-C04-04),
CYTED VII.19 RIBIDI Project, and (for the third author) Fondecyt Grant 1-020831.

M.A. Nascimento, E.S. de Moura, A.L. Oliveira (Eds.): SPIRE 2003, LNCS 2857, pp. 122–136, 2003.
c(cid:1) Springer-Verlag Berlin Heidelberg 2003

(S,C)-Dense Coding

123

than standard inverted indexes because their entries point to blocks instead of
exact word positions. Of course the price to pay is sequential text scanning of the
pointed blocks. However, if the text is compressed with a technique that allows
direct search of words in the compressed text, then not only the index and text
size are reduced, but also the search inside candidate text blocks is much faster.
Notice that using those two techniques together, as in [8], the index is used just
as a device to ﬁlter out some blocks that do not contain the word we are looking
for. This index schema was ﬁrst proposed in Glimpse [13], a widely known system
that uses a block addressing index. On the other hand compression techniques
can be used as well to compress the inverted indexes themselves, as suggested
in [8] or [10].

For these reasons, compression techniques have become attractive methods
to save space and transmission time. However, if the compression scheme does
not allow to search for words directly on the compressed text, the retrieval of
documents will be less eﬃcient, due to the need of decompression before the
search.

Classic compression techniques, like the well-known algorithms of Ziv and
Lempel [15, 16] or the character oriented code of Huﬀman [4], are not suitable for
large textual databases. One important disadvantage of these techniques is the
ineﬃciency of searching for words directly on the compressed text. Compression
schemes based on Huﬀman codes are not often used on natural language because
of the poor compression ratios achieved. On the other hand, Ziv and Lempel
algorithms obtain better compression ratios, but the search for a word on the
compressed text is ineﬃcient. Empirical results [7] showed that searching on a
Ziv-Lempel compressed text can take half the time of decompressing that text
and then searching it. However, the compressed search is twice as slow as just
searching the uncompressed version of the text.

In [12], they presented a compression scheme that uses a semi-static wordbased 
model and a Huﬀman code where the coding alphabet is byte-oriented.
This compression scheme allows the search for a word on the compressed text
without decompressing it in such a way that the search can be up to eight times
faster for certain queries. The key idea of this work (and others [6]) is to take
the words as the symbols that compose the text (and therefore the symbols that
should be compressed). Since in Information Retrieval (IR) words are the atoms
of the search, these compression schemes are particularly suitable for IR.

In [3] it is shown that, although plain Huﬀman Code is the preﬁx code that
gives the shortest possible output when a source symbol is always substituted by
the same code, Tagged Huﬀman Code largely underutilizes the representation.
In that paper it is shown that, by signaling the last byte instead of the
ﬁrst one, the rest of the bits can be used in all their combinations and the
code is still a preﬁx code. The resulting code, called End-Tagged Dense Code,
becomes much closer to the compression obtained by Plain Huﬀman Code. This
code not only retains the ability of being searchable with any string matching
algorithm, but it is also extremely simple to build (it is not based on Huﬀman
at all) and permits a more compact representation of the vocabulary. Thus, the

