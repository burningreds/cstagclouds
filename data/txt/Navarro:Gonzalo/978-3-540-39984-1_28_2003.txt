Memory-Adaptative Dynamic Spatial

Approximation Trees(cid:1)

Diego Arroyuelo1, Francisca Mu˜noz2, Gonzalo Navarro2, and Nora Reyes1

1 Depto. de Inform´atica, Univ. Nac. de San Luis, Argentina.

{darroy,nreyes}@unsl.edu.ar

2 Center for Web Research, Dept. of Computer Science, Univ. of Chile.

{franmuno,gnavarro}@dcc.uchile.cl

Abstract. Dynamic spatial approximation trees (dsa–trees) are eﬃcient
data structures for searching metric spaces. However, using enough storage,
 pivoting schemes beat dsa–trees in any metric space. In this paper
we combine both concepts in a data structure that enjoys the features
of dsa–trees and that improves query time by making the best use of
the available memory. We show experimentally that our data structure
is competitive for searching metric spaces.

1 Introduction

“Proximity” or “similarity” searching is the problem of looking for objects in a
set close enough to a query. This has applications in a vast number of ﬁelds. The
problem can be formalized with the metric space model [1]: There is a universe
U of objects, and a positive real-valued distance function d : U × U −→ (cid:0)+
deﬁned among them, which satisﬁes the metric properties: strict positiveness
(d(x, y) = 0 ⇔ x = y), symmetry (d(x, y) = d(y, x)), and triangle inequality
(d(x, z) (cid:1) d(x, y) + d(y, z)). The smaller the distance between two objects, the
more “similar” they are. We have a ﬁnite database S ⊆ U that can be preprocessed 
to build an index. Later, given a query q ∈ U, we must retrieve all similar
elements in the database. We are mainly interested in the range query: Retrieve
all elements in S within distance r to q, that is, {x ∈ S, d(x, q) (cid:1) r}.

Generally, the distance is expensive to compute, so one usually deﬁnes the
search complexity as the number of distance evaluations performed. Proximity
search algorithms build an index of the database to speed up queries, avoiding
the exhaustive search. Many of these indexes are based on pivots (Sec. 2).

In this paper we present a hybrid index for metric space searching built on
the dsa–tree, an index supporting insertions and deletions that is competitive in
spaces of medium diﬃculty, but unable of taking advantage of the available memory.
 This is enriched with a pivoting scheme. Pivots use the available memory to
improve query time, and in this way they can beat any other structure, but too

(cid:1) Supported in part by CYTED VII.19 RIBIDI Project and, the third author, Millenium 
Nucleus Center for Web Research, Grant P01-029-F, Mideplan, Chile.

M.A. Nascimento, E.S. de Moura, A.L. Oliveira (Eds.): SPIRE 2003, LNCS 2857, pp. 360–368, 2003.
c(cid:1) Springer-Verlag Berlin Heidelberg 2003

Memory-Adaptative Dynamic Spatial Approximation Trees

361

many pivots are needed in diﬃcult spaces. Our new structure is still dynamic
and makes better use of memory, beating both dsa-trees and basic pivots.

Unlike previous work [3], (1) we use local rather than global pivots, and

provide empirical evidence in favor of this decision, (2) we use pivots for free.

2 Pivoting Algorithms

Essentially, pivoting algorithms choose some elements pi from the database S,
and precompute and store all distances d(a, pi) for all a ∈ S. At query time,
they compute distances d(q, pi) against the pivots. Then the distance by pivots
between a ∈ S and q gets deﬁned as D(a, q) = maxpi |d(a, pi) − d(q, pi)|.
It can be seen that D(a, q) (cid:1) d(a, q) for all a ∈ S, q ∈ U. This is used to avoid
distance evaluations. Each a such that D(a, q) > r can be discarded because we
deduce d(a, q) > r without actually computing d(a, q). All the elements that
cannot be discarded this way are directly compared against q.

Usually pivoting schemes perform better as more pivots are used, this way
beating any other index. They are, however, better suited to “easy” metric spaces
[1]. In hard spaces they need too many pivots to beat other algorithms.

3 Dynamic Spatial Approximation Trees

In this section we brieﬂy describe dynamic sa–trees (dsa-trees for short), in
particular the version called timestamp with bounded arity [2], on top of which
we build. Deletion algorithms are omitted for lack of space.

3.1 Insertion Algorithm

The dsa–tree is built incrementally, via insertions. The tree has a maximum
arity. Each tree node a stores a timestamp of its insertion time, time(a), and
its covering radius, R(a), which is the maximum distance to any element in its
subtree. Its set of children is called N(a), the neighbors of a. To insert a new
element x, its point of insertion is sought starting at the tree root and moving to
the neighbor closest to x, updating R(a) in the way. We ﬁnally insert x as a new
(leaf) child of a if (1) x is closer to a than to any b ∈ N(a), and (2) the arity of
a, |N(a)|, is not already maximal. Neighbors are stored left to right in increasing
timestamp order. Note that the parent is always older than its children.

3.2 Range Search Algorithm

The idea is to replicate the insertion process of elements to retrieve. That is, we
act as if we wanted to insert q but keep in mind that relevant elements may be
at distance up to r from q, so in each decision for simulating the insertion of q
we permit a tolerance of ±r. So it may be that relevant elements were inserted
in diﬀerent children of the current node, and backtracking is necessary.

