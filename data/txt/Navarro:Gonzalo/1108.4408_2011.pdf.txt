1
1
0
2

 

g
u
A
2
2

 

 
 
]
S
D
.
s
c
[
 
 

1
v
8
0
4
4

.

8
0
1
1
:
v
i
X
r
a

On Compressing Permutations and Adaptive Sorting ∗

Jérémy Barbay Gonzalo Navarro

Dept. of Computer Science, University of Chile

Abstract

Previous compact representations of permutations have focused on adding a small index
on top of the plain data hπ(1), π(2), . . . π(n)i, in order to eﬃciently support the application of
the inverse or the iterated permutation. In this paper we initiate the study of techniques that
exploit the compressibility of the data itself, while retaining eﬃcient computation of π(i) and
its inverse. In particular, we focus on exploiting runs, which are subsets (contiguous or not) of
the domain where the permutation is monotonic. Several variants of those types of runs arise in
real applications such as inverted indexes and suﬃx arrays. Furthermore, our improved results
on compressed data structures for permutations also yield better adaptive sorting algorithms.

1 Introduction

Permutations of the integers [1..n] = {1, . . . , n} are not only a fundamental mathematical structure,
 but also a basic building block for the succinct encoding of integer functions [MR04],
strings [Kär99, GMR06, GV06, ANS06, MN07, CHSV08], binary relations [BHMR07], and geometric 
grids [BLNS09], among others. A permutation π can be trivially encoded in n⌈lg n⌉ bits,
which is within O(n) bits of the information theory lower bound of lg(n!) bits, where lg x = log2 x
denotes the logarithm in base two.
In most of those applications, eﬃcient computation is required for both the value π(i) at any
point i ∈ [1..n] of the permutation, and for the position π−1(j) of any value j ∈ [1..n] (i.e., the value
of the inverse permutation). The only alternative we are aware of to storing explicitly both π and
π−1 is by Munro et al. [MRRR03], who add a small structure over the plain representation of π so
that, by spending ǫ lg n extra bits, any π−1(j) can be computed in time O(1/ǫ). This is extended
to any positive or negative power of π, πk(i). They give another solution using O(n) extra bits and
computing any πk(j) in time O(lg n/ lg lg n).
The lower bound of lg(n!) bits yields a lower bound of Ω(n lg n) comparisons to sort such a
permutation in the comparison model, in the worst case over all permutations of n elements. Yet,
a large body of research has been dedicated to ﬁnding better sorting algorithms which can take advantage 
of speciﬁcities of each permutation to sort. Some examples are permutations composed of
a few sorted blocks [Man85] (e.g., (1 , 3 , 5 , 7 , 9 , 2, 4, 6, 8, 10) or (6 , 7 , 8 , 9 , 10 , 1, 2, 3, 4, 5)), or permutations 
containing few sorted subsequences [LP94] (e.g., (1 , 6, 2 , 7, 3 , 8, 4 , 9, 5 , 10)). Algorithms
performing possibly o(n lg n) comparisons on such permutations, yet still O(n lg n) comparisons in

∗Partially funded by Fondecyt Grant 1-110066, Chile. An early partial version of this paper appeared in STACS

[BN09].

1

the worst case, are achievable and preferable if those permutations arise with suﬃcient frequency.
Other examples are classes of permutations whose structure makes them interesting for applications:
see the seminal paper of Mannila [Man85], and the survey of Moﬀat and Petersson [MP92] for more
details.

Each sorting algorithm in the comparison model yields an encoding scheme for permutations: the
result of all comparisons performed uniquely identiﬁes the permutation sorted, and hence encodes it.
Since an adaptive sorting algorithm performs o(n lg n) comparisons on a class of “easy” permutations,
each adaptive algorithm yields a compression scheme for permutations, at the cost of losing a
constant factor on the complementary class of “hard” permutations. Yet such compression schemes
do not necessarily support eﬃciently the computation of value π−1(j) of the inverse permutation
for an arbitrary value j ∈ [1..n], or even the simple application of the permutation, π(i).
This is the topic of our study: the interplay between adaptive sorting algorithms and compressed
representation of permutations that support eﬃcient application of π(i) and π−1(j). In particular
we focus on classes of permutations that can be decomposed into a small number of runs, that is,
monotone subsequences of π, either contiguous or not.

Our results include compressed representations of permutations whose space and time to compute 
any π(i) and π−1(j) are proportional to the entropy of the distribution of the sizes of the
runs. As far as we know, this is the ﬁrst compressed representation of permutations with similar
capabilities.

We also develop the corresponding sorting algorithms, which in general reﬁne the known complexities 
to sort those classes of permutations: While there exist sorting algorithms taking advantage
of the number of runs of various kinds, ours take advantage of their size distribution and are strictly
better (or equal, at worst).

Finally, we obtain a representation for strings that improves upon the state of the art [FMMN07,
GRR08] in the average case, while retaining their space and worst-case performance for operations
access, rank, and select.

At the end of the article we describe some applications where the class of permutations compressible 
with the techniques we develop here naturally arise, and conclude with a more general
perspective on the meaning of those results and the research directions they suggest.

2 Basic Concepts and Previous Work

2.1 Entropy

We deﬁne the entropy of a distribution [CT91], a measure that will be useful to evaluate compressibility 
results.

Deﬁnition 1 The entropy of a sequence of positive integers X = hn1, n2, . . . , nri adding up to n is
H(X) = Pr
. By concavity of the logarithm, it holds that (r − 1) lg n ≤ nH(X) ≤ n lg r
and that H(hn1, n2, . . . nri) > H(hn1+n2, . . . , nri).

ni
n lg n
ni

i=1

Here hn1, n2, . . . , nri is a distribution of values adding up to n and H(X) measures how even is
n−r+1)

the distribution. H(X) is maximal (lg r) when all ni = n/r and minimal ( r−1
when they are most skewed (X = h1, 1, . . . , 1, n − r + 1i).
If a
random variable P takes the value i with probability ni/n, for 1 ≤ i ≤ r, then its entropy is

This measure is related to entropy of random variables and of sequences as follows.

lg

n lg n + n−r+1

n

n

2

H(hn1, n2, . . . , nri). Similarly, if a string S[1..n] contains ni occurrences of character ci, then its
empirical zero-order entropy is H0(S) = H(hn1, n2, . . . , nri).
H(X) is then a lower bound to the average number of bits needed to encode an instance of P ,
or to encode a character of S (if we model S statistically with a zero-order model, that is, ignoring
the context of characters).

2.2 Huﬀman Coding
The Huﬀman algorithm [Huf52] receives frequencies X = hn1, n2, . . . , nri adding up to n, and
outputs in O(r lg r) time a preﬁx-free code for the symbols [1..r]. If ℓi is the bit length of the code
assigned to the ith symbol, then L = P ℓini is minimal. Moreover, L < n(1 +H(X)). For example,
given S[1..n] over alphabet [1..r], with symbol frequencies X, one can compress S by concatenating
the codewords of the successive symbols S[i], achieving total length L < n(1 + H0(S)). (One also
has to encode the usually negligible codebook of O(r lg r) bits.)
Huﬀman’s algorithm starts with a forest of r leaves corresponding to the frequencies
{n1, n2, . . . , nr}, and outputs a binary trie with those leaves, in some order. This so-called Huﬀman
tree describes the optimal encoding as follows: The sequence of left/right choices (interpreted as
0/1) in the path from the root to each leaf is the preﬁx-free encoding of that leaf, of length ℓi equal
to the leaf depth.

A generalization of this encoding is multiary Huﬀman coding [Huf52], in which the tree is given
arity t, and then the Huﬀman codewords are sequences over an alphabet [1..t]. In this case the
algorithm also produces the optimal code, of length L < n(1 + H(X)/ lg t).

2.3 Succinct Data Structures for Sequences

Let S[1..n] be a sequence of symbols from the alphabet [1..r]. This includes bitmaps when r = 2
(where, for convenience, the alphabet will be {0, 1} rather than {1, 2}). We will make use of succinct
representations of S that support the rank and select operators over strings and over binary vectors:
rankc(S, i) gives the number of occurrences of c in S[1..i] and selectc(S, j) gives the position in S
of the jth occurrence of c.

When r = 2, S requires n bits and rank and select can be supported in constant time using

n−m = m lg n

O(n lg lg n/ lg n) = o(n) bits on top of S [Mun96, Gol06].
Raman et al. [RRR02] devised a bitmap representation that takes nH0(S) + o(n) bits, while
maintaining the constant time for supporting the operators. For the binary case H0(S) is just
m + (n − m) lg n
m lg n
m + O(m), where m is the number of bits set to 1 in S. Golynski
et al. [GGG+07] reduced the o(n)-bits redundancy in space to O(n lg lg n/ lg2 n).
When m is much smaller than n, the o(n)-bits term may dominate. Gupta et al. [GHSV06]
showed how to achieve space m lg n
m + lg n) bits, which largely reduces the dependence
on n, but now rank and select are supported in O(lg m) time via binary search [Gup07, Theorem
17 p. 153].
For larger alphabets, of size r = O(polylog(n)), Ferragina et al. [FMMN07] showed how to
represent the sequence within nH0(S) + o(n lg r) bits and support rank and select in constant
time. Golynski et al. [GRR08, Lemma 9] improved the space to nH0(S) + o(n lg r/ lg n) bits while
retaining constant times.
Grossi et al. [GGV03] introduced the so-called wavelet tree, which decomposes an arbitrary
sequence into several bitmaps. By representing the bitmaps in compressed form [GGG+07], the

m +O(m lg lg n

3

overall space is nH0(S) + o(n) and rank and select are supported in time O(lg r). Multiary
wavelet trees decompose the sequence into subsequences over a sublogarithmic-sized alphabet and
reduce the time to O(1 + lg r/ lg lg n) [FMMN07, GRR08].
even those including several variables, will be asymptotic in n.

In this article n will generally denote the length of the permutation. All of our o() expressions,

2.4 Measures of Presortedness in Permutations

The complexity of adaptive algorithms, for problems such as searching, sorting, merging sorted
arrays or convex hulls, is studied in the worst case over instances of ﬁxed size and diﬃculty, for a
deﬁnition of diﬃculty that is speciﬁc to each analysis. Even though sorting a permutation in the
comparison model requires Θ(n lg n) comparisons in the worst case over permutations of n elements,
better results can be achieved for some parameterized classes of permutations. We describe some
of those below, see the survey by Moﬀat and Petersson [MP92] for others.

Knuth [Knu98] considered runs (contiguous ascending subsequences) of a permutation π,
counted by nRuns = 1 + |{i : 1 ≤ i < n, π(i + 1) < π(i)}|. Levcopoulos and Petersson [LP94]
introduced Shuﬄed Up-Sequences and its generalization Shuﬄed Monotone Sequences, respectively
counted by nSUS = min{k : π is covered by k increasing subsequences}, and nSMS = min{k :
π is covered by k monotone subsequences}. By deﬁnition, nSMS ≤ nSUS ≤ nRuns.
Munro and Spira [MS76] took an orthogonal approach, considering the task of sorting multisets
through various algorithms such as MergeSort, showing that they can be adapted to perform in
time O(n(1 + H(hm1, . . . , mri))) where mi is the number of occurrences of i in the multiset (note
this is totally diﬀerent from our results, that depend on the distribution of the lengths of monotone
runs).

Each adaptive sorting algorithm in the comparison model yields a compression scheme for permutations,
 but the encoding thus deﬁned does not necessarily support the simple application of the
permutation to a single element without decompressing the whole permutation, nor the application
of the inverse permutation.

3 Contiguous Monotone Runs

Our most fundamental representation takes advantage of permutations that are formed by a few
monotone (ascending or descending) runs.

Deﬁnition 2 A down step of a permutation π over [1..n] is a position 1 ≤ i < n such that π(i+1) <
π(i). An ascending run in a permutation π is a maximal range of consecutive positions [i..j] that
does not contain any down step. Let d1, d2, . . . , dk be the list of consecutive down steps in π. Then the
number of ascending runs of π is noted nRuns = k+1, and the sequence of the lengths of the ascending
runs is noted vRuns = hn1, n2, . . . , nnRunsi, where n1 = d1, n2 = d2 − d1, . . . , nnRuns−1 = dk − dk−1,
and nnRuns = n − dk. (If k = 0 then nRuns = 1 and vRuns = hn1i = hni.) The notions of up step
and descending run are deﬁned similarly.

For example, the permutation (1 , 3 , 5 , 7 , 9 , 2, 4, 6, 8, 10) contains nRuns = 2 ascending runs, of

lengths forming the vector vRuns = h5, 5i.
runs, and is able to compute any π(i) and π−1(i).

We now describe a data structure that represents a permutation partitioned into nRuns ascending

4

3.1 Structure

Construction We ﬁnd the down-steps of π in linear time, obtaining nRuns runs of lengths vRuns =
hn1, . . . , nnRunsi, and then apply the Huﬀman algorithm to the vector vRuns. When we set up the
leaves v of the Huﬀman tree, we store their original index in vRuns, idx(v), and the starting
position in π of their corresponding run, pos(v). After the tree is built, we use idx(v) to compute
a permutation φ over [1..nRuns] so that φ(i) = j if the leaf corresponding to ni is placed at the jth
left-to-right leaf in the Huﬀman tree. We also compute φ−1. We also precompute a bitmap C[1..n]
that marks the beginning of runs in π and give constant-time support for rank and select. Since
C contains only nRuns bits set out of n, it is represented in compressed form [GGG+07] within
nRuns lg

nRuns + o(n) bits.

n

Now we set a new permutation π′ over [1..n] where the runs are written in the order given by
φ−1: We ﬁrst copy from π the run whose endpoints are those of the leftmost tree leaf, then the
run pointed by the second leftmost leaf, and so on. Simultaneously, we compute pos′(v) for the
leaves v, denoting the starting position of the area they cover in π′. After creating π′ the original
permutation π can be deleted. We say that an internal node covers the contiguous area of π′ formed
by concatenating the runs of all the leaves that descend from v. We compute, for all nodes v,
pos′(v), the starting position of the area covered by v in π′, length(v), the size of that area, and
leaves(v), the number of leaves that descend from v.

Now we enhance the Huﬀman tree into a wavelet-tree-like structure [GGV03] without altering
its shape, as follows. Starting from the root, ﬁrst process recursively each child. For the leaves we
do nothing. Once the left and right children, vl and vr, of an internal node v have been processed,
the invariant is that the areas they cover have already been sorted. We create a bitmap for v, of size
length(v). Now we merge the areas of vl and vr in time O(length(v)). As we do the merging, each
time we take an element from vl we append a bit 0 to the node bitmap, and a bit 1 when we take
an element from vr. When we ﬁnish, π′ has been sorted and we can delete it. The Huﬀman-shaped
wavelet tree (only with ﬁelds leaves and pos), φ, and C represent π.

m +(n−m) lg n

Space and construction cost Note that each of the ni elements of leaf i (at depth ℓi) is merged
ℓi times, contributing ℓi bits to the bitmaps of its ancestors, and thus the total number of bits in
all bitmaps is P niℓi. Thus the total number of bits in the Huﬀman-shaped wavelet tree is at most
n(1 + H(vRuns)). Those bitmaps, however, are represented in compressed form [GGG+07], which
allows us removing the n extra bits added by the Huﬀman encoding.
Let us call mj = nφ−1(j) the length of the run corresponding to the jth left-to-right leaf, and
mi,j = mi+. . .+mj. The compressed representation [GGG+07] takes, on a bitmap of length n and m
n−m bits, plus a redundancy of O(n lg lg n/ lg2 n) bits. We prove by induction
1s, m lg n
(see also Grossi et al. [GGV03]) that the compressed space allocated for all the bitmaps descending
from a node covering leaves [i..k] is Pi≤r≤k mr lg mi,k
(we consider the redundancy later). Consider
two sibling leaves merging two runs of mi and mi+1 elements. Their parent bitmap contains mi
+ mi+1 lg mi+mi+1
0s and mi+1 1s, and thus its compressed representation requires mi lg mi+mi+1
bits. Now consider a general Huﬀman tree node merging a left subtree covering leaves [i..j] and
a right subtree covering leaves [j + 1..k]. Then the bitmap of the node will be compressed to
mi,j lg mi,k
bits. By the inductive hypothesis, all the bitmaps on the left child and
mi,j
, and those on the right add up to Pj+1≤r≤k mr lg mj+1,k
its subtrees add up to Pi≤r≤j mr lg mi,j
.
Adding up the three formulas we get the inductive thesis.

+ mj+1,k lg mi,k
mj+1,k

mr

mi

mi+1

mr

5

mr

Therefore, a compressed representation of the bitmaps requires nH(vRuns) bits, plus the redundancy.
 The latter, added over all the bitmaps, is O(n(1 + H(vRuns)) lg lg n/ lg2 n) = o(n) because
H(vRuns) ≤ lg n.1 To this we must add the O(nRuns lg n) bits of the tree pointers and extra data
like pos and leaves, the O(nRuns lg nRuns) bits for φ, and the nRuns lg
The construction time is O(nRuns lg nRuns) for the Huﬀman algorithm, plus O(nRuns) for computing 
φ and ﬁlling the node ﬁelds like pos and leaves, plus O(n) for constructing π′ and C, plus the
total number of bits appended to all bitmaps, which includes the merging cost. The extra structures
for rank are built in linear time on those bitmaps.2 All this adds up to O(n(1+H(vRuns))), because
nRuns lg nRuns ≤ nH(vRuns) + lg n by concavity, recall Deﬁnition 1.

nRuns + o(n) bits for C.

n

3.2 Queries

Computing π and π−1 One can regard the wavelet tree as a device that tracks the evolution of
a merge-sorting of π′, so that in the bottom we have (conceptually) the sequence π′ (with one run
per leaf) and in the top we have (conceptually) the sorted permutation (1, 2, . . . , n).

To compute π−1(j) we start at the top and ﬁnd out where that position came from in π′. We
start at oﬀset j′ = j of the root bitmap B. If B[j′] = 0, then position j′ came from the left subtree
in the merging. Thus we go down to the left child with j′ ← rank0(B, j′), which is the position of
j′ in the array of the left child before the merging. Otherwise we go down to the right child with
j′ ← rank1(B, j′). We continue recursively until we reach a leaf v. At this point we know that j
came from the corresponding run, at oﬀset j′, that is, π−1(j) = pos(v) + j′ − 1.
To compute π(i) we do the reverse process, but we must ﬁrst determine the leaf v and oﬀset
i′ within v corresponding to position i: We compute l = φ(rank1(C, i)), so that i falls at the lth
left-to-right leaf. Then we traverse the Huﬀman tree down so as to ﬁnd the lth leaf. This is easily
done as we have leaves(v) stored at internal nodes. Upon arriving at leaf v, we know that the oﬀset
is i′ = i − pos(v) + 1. We now start an upward traversal from v using the nodes that are already
in the recursion stack. If v is a left child of its parent u, then we set i′ ← select0(B, i′) to locate
it in the merged array of the parent, else we set i′ ← select1(B, i′), where B is the bitmap of u.
Then we set v ← u and continue until reaching the root, where we answer π(i) = i′.
Query time In both queries the time is O(ℓ), where ℓ is the depth of the leaf arrived at. If i is
chosen uniformly at random in [1..n], then the average cost is 1
n P niℓi = O(1+H(vRuns)). However,
the worst case can be O(nRuns) in a fully skewed tree. We can ensure ℓ = O(lg nRuns) in the worst
case while maintaining the average case by slightly rebalancing the Huﬀman tree [ML01]. Given any
constant x > 0, the height of the Huﬀman tree can be bound to at most (1 + x) lg nRuns so that the
total number of bits added to the encoding is at most n· nRuns−x lg ϕ, where ϕ ≈ 1.618 is the golden
ratio. This is o(n) if nRuns = ω(1), and otherwise the cost was O(nRuns) = O(1) anyway. Similarly,
the average time stays O(1 + H(vRuns)), as it increases at most by O(nRuns−x lg ϕ) = O(1). This
rebalancing takes just O(nRuns) time if the frequencies are already sorted.
Note also that the space required by the query is O(lg nRuns). This can be made constant by
storing parent pointers in the wavelet tree, which does not change the asymptotic space.

1To make sure this is o(n) even if there are many short bitmaps, we can concatenate all the bitmaps into a single
one, and replace pointers to bitmaps by oﬀsets to this single bitmap. Operations rank and select translate easily
into a concatenated bitmap.

2While the linear construction time is not obvious from their article [GGG+07], a subsequent result [Pˇ08] achieved

even less redundancy and linear construction time.

6

Theorem 1 There is an encoding scheme using at most nH(vRuns) + O(nRuns lg n) + o(n) bits to
represent a permutation π over [1..n] covered by nRuns contiguous ascending runs of lengths forming
the vector vRuns. It can be built within time O(n(1 + H(vRuns))), and supports the computation
of π(i) and π−1(i) in time O(1 + lg nRuns) for any value of i ∈ [1..n]. If i is chosen uniformly at
random in [1..n] then the average computation time is O(1 + H(vRuns)).

We note that the space analysis leading to nH(vRuns) + o(n) bits works for any tree shape.
We could have used a balanced tree, yet we would not achieve O(1 + H(vRuns)) average time. On
the other hand, by using Hu-Tucker codes instead of Huﬀman, as in our previous work [BN09], we
would not need the permutation φ and, by using compact tree representations [SN10], we would
nRuns ) + o(n). This is interesting for large
be able to reduce the space to nH(vRuns) + O(nRuns lg
values of nRuns, as it is always nH(vRuns) + o(n(1 + H(vRuns)) even if nRuns = Θ(n).3

n

3.3 Mixing Ascending and Descending Runs

We can easily extend Theorem 1 to mix ascending and descending runs.

Corollary 2 Theorem 1 holds verbatim if π is partitioned into a sequence nRuns contiguous monotone 
(i.e., ascending or descending) runs of lengths forming the vector vRuns.

The values π(i) and π−1(j) are easily computed from πasc: If π−1

Proof. We mark in a bitmap of length nRuns whether each run is ascending or descending, and
then reverse descending runs in π, so as to obtain a new permutation πasc, which is represented
using Theorem 1 (some runs of π could now be merged in πasc, but this only reduces H(vRuns),
recall Deﬁnition 1).
asc(j) = i, we use C to determine
that i is within run πasc(ℓ..r), that is, ℓ = select1(rank1(C, i)) and r = select1(rank1(C, i)+1)−1.
If that run is reversed in π, then π−1(j) = ℓ + r − i, else π−1(j) = i. For π(i), we use C to
determine that i belongs to run π(ℓ..r). If the run is descending, then we return πasc(ℓ + r − i),
else we return πasc(i). The operations on C require only constant time. The extra construction
time is just O(n), and no extra space is needed apart from nRuns = o(nRuns lg n) bits.
(cid:3)

Note that, unlike the case of ascending runs, where there is an obviously optimal way of partitioning 
(that is, maximize the run lengths), we have some freedom when partitioning into ascending
or descending runs, at the endpoints of the runs: If an ascending (resp. descending) run is followed
by a descending (resp. ascending) run, the limiting element can be moved to either run; if two ascending 
(resp. descending) runs are consecutive, one can create a new descending (resp. ascending)
run with the two endpoint elements. While ﬁnding the optimal partitioning might not be easy, we
note that these decisions cannot aﬀect more than O(nRuns) elements, and thus the entropy of the
partition cannot be modiﬁed by more than O(nRuns lg n), which is absorbed by the redundancy of
our representation.

3We do not follow this path because we are more interested in multiary codes (see Section 3.5) and, to the best
of our knowledge, there is no eﬃcient (i.e., O(nRuns lg nRuns) time) algorithm for building multiary Hu-Tucker codes
[Knu98].

7

3.4 Improved Adaptive Sorting

One of the best known sorting algorithms is MergeSort, based on a simple linear procedure to merge
two already sorted arrays, and with a worst case complexity of n⌈lg n⌉ comparisons and O(n lg n)
running time. It had been already noted [Knu98] that ﬁnding the down-steps of the array in linear
time allows improving the time of MergeSort to O(n(1 + lg nRuns)) (the down-step concept can be
applied to general sequences, where consecutive equal values do not break runs).
We now show that the construction process of our data structure sorts the permutation and,
applied on a general sequence, it achieves a reﬁned sorting time of O(n(1 + H(vRuns)) ⊂ O(n(1 +
lg nRuns)) (since H(vRuns) ≤ lg nRuns).

Theorem 3 There is an algorithm sorting an array of length n covered by nRuns contiguous monotone 
runs of lengths forming the vector vRuns in time O(n(1 + H(vRuns))), which is worst-case
optimal in the comparison model.

n!

Proof. Our wavelet tree construction of Theorem 1 (and Corollary 2) indeed sorts π within this
time, and it also works if the array is not a permutation. This is optimal because, even con-
n1!n2!...nnRuns! diﬀerent permutations that can be covered
sidering just ascending runs, there are
n1!n2!...nnRuns! comwith 
runs of lengths forming the vector vRuns = hn1, n2, . . . , nnRunsi. Thus lg
parisons are necessary. Using Stirling’s approximation to the factorial we have lg
n1!n2!...nnRuns! =
(n + 1/2) lg n − Pi(ni + 1/2) lg ni − O(lg nRuns). Since P lg ni ≤ nRuns lg(n/nRuns), this is
nH(vRuns) − O(nRuns lg(n/nRuns)) = nH(vRuns) − O(n). The term Ω(n) is also necessary to
read the input, hence implying a lower bound of Ω(n(1 + H(vRuns))).
Note, however, that the set of permutations that can be covered with nRuns runs of lengths
vRuns, may contain permutations that can be covered with fewer runs (as two consecutive runs
could be merged), and thus they have entropy less than H(vRuns), recall Deﬁnition 1. We
have proved that the lower bound applies to the union of two classes: one (1) contains (some4)
permutations of entropy H(vRuns) and the other (2) contains (some) permutations of entropy less
than H(vRuns). Obviously the bound does not hold for class (2) alone, as we can sort it in less
time. Since we can tell the class of a permutation in O(n) time by counting the down-steps, it
follows that the bound also applies to class (1) alone (otherwise O(n) + o(nH(vRuns)) would be
achievable for (1)+(2)).

(cid:3)

n!

n!

3.5 Boosting Time Performance

The time performance achieved in Theorem 1 (and Corollary 2) can be boosted by an O(lg lg n)
time factor by using Huﬀman codes of higher arity.
Given the run lengths vRuns, we build the t-ary Huﬀman tree for vRuns, with t = √lg n. Since
now we merge t children to build the parent, the sequence stored in the parent to indicate the
child each element comes from is not binary, but over alphabet [1..t]. In addition, we set up nRuns
pointers to provide direct access to the leaves, and parent pointers.

The total

length of all the sequences stored at all the Huﬀman tree nodes is < n(1 +
H(vRuns)/ lg t) [Huf52]. To reduce the redundancy, we represent each sequence S[1..m] stored

4Other permutations with vectors distinct from vRuns could also have entropy H(vRuns).

8

mi

at a node using the compressed representation of Golynski et al. [GRR08, Lemma 9], which yields
space mH0(S) + O(m lg t lg lg m/ lg2 m) bits.
For the string S[1..m] corresponding to a leaf covering run lengths m1, . . . , mt, we have
mH0(S) = P mi lg m
. From there we can carry out exactly the same analysis done in Section 
3.1 for binary trees, to conclude that the sum of the mH0(S) bits for all the strings
S over all the tree nodes is nH(vRuns). On the other hand, the redundancies add up to
O(n(1 + H(vRuns)/ lg t) lg t lg lg n/ lg2 n) = o(n) bits.5
The advantage of the t-ary representation is that the average leaf depth is 1 + H(vRuns)/ lg t =
O(1 +H(vRuns)/ lg lg n). The algorithms to compute π(i) and π−1(i) are similar, except that rank
and select are carried out on sequences S over alphabets of size √lg n. Those operations can still
be carried out in constant time on the representation we have chosen [GRR08]. The only detail
is that, for π(i) we ﬁrst moved from the root to the leaf using the ﬁeld leaves(v). This does not
anymore allow us processing a node in constant time, and thus we have opted for storing an array
of pointers to the leaves and parent pointers.

For the worst case, if nRuns = ω(1), we can again limit the depth of the Huﬀman tree to
O(lg nRuns/ lg lg n) and maintain the same average time. The multiary case is far less understood
than the binary case. Recently, an algorithm to ﬁnd the optimal length-restricted t-ary code has been
presented whose running time is linear once the lengths are sorted [Bae07]. To analyze the increase
in redundancy, consider the sub-optimal method that simply takes any node v of depth more than
ℓ = 4 lg nRuns/ lg t and balances its subtree (so that height 5 lg nRuns/ lg t is guaranteed). Since any
node at depth ℓ covers a total length of at most n/t⌊ℓ/2⌋ (see next paragraph), the sum of all the
lengths covered by these nodes is at most nRuns· n/t⌊ℓ/2⌋. By forcing those subtrees to be balanced,
the average leaf depth increases by at most (lg nRuns/ lg t) nRuns/t⌊ℓ/2⌋ ≤ lg(nRuns)/(nRuns lg t) =
O(1). Hence the worst case is limited to O(1 + lg nRuns/ lg lg n) while the average case stays within
O(1+H(vRuns)/ lg lg n). For the space we need a ﬁner consideration: As nRuns = ω(1), the increase
in average leaf depth is o(1/ lg t). Since increasing by one the depth of a leaf covering m elements
costs m lg t further bits, the total increase in space redundancy is o(n).

The limit on the probability is obtained as follows. Consider a node v in the t-ary Huﬀman
tree. Then length(u) ≥ length(v) for any uncle u of v, as otherwise switching v and u improves
the already optimal Huﬀman tree. Hence w, the grandparent of v (i.e., the parent of u) must cover
an area of size length(w) ≥ t · length(v). Thus the covered length is multiplied at least by t when
moving from a node to its grandparent. Conversely, it is divided at least by t as we move from a
node to any grandchild. As the total length at the root is n, the length covered by any node v at
depth ℓ is at most length(v) ≤ n/t⌊ℓ/2⌋.

This yields our ﬁnal result for contiguous monotone runs.

Theorem 4 There is an encoding scheme using at most nH(vRuns) + O(nRuns lg n) + o(n) bits to
encode a permutation π over [1..n] covered by nRuns contiguous monotone runs of lengths forming the
vector vRuns. It can be built within time O(n(1 +H(vRuns)/ lg lg n)), and supports the computation
of π(i) and π−1(i) in time O(1+lg nRuns/ lg lg n) for any value of i ∈ [1..n]. If i is chosen uniformly
at random in [1..n] then the average computation time is O(1 + H(vRuns)/ lg lg n).

The only missing part is the construction time, since now we have to build strings S[1..m] by
merging t increasing runs. This can be done in O(m) time by using atomic heaps [FW94]. The
compressed sequence representations are built in linear time [GRR08]. Note this implies that we

5Again, we can concatenate all the sequences to make sure this redundancy is asymptotic in n.

9

can sort an array with nRuns contiguous monotone runs of lengths forming the vector vRuns in time
O(n(1 + H(vRuns)/ lg lg n)), yet we are not anymore within the comparison model.

3.6 An Improved Sequence Representation

Interestingly, the previous result yields almost directly a new representation of sequences that,
compared to the state of the art [FMMN07, GRR08], provides improved average time performance.

Theorem 5 Given a string S[1..n] over alphabet [1..σ] with zero-order entropy H0(S), there is an
encoding for S using at most nH0(S) +O(σ lg n) + o(n) bits and answering queries S[i], rankc(S, i)
and selectc(S, i) in time O(1 + lg σ/ lg lg n) for any c ∈ [1..σ] and i ∈ [1..n]. When i is chosen at
random in query S[i], or c is chosen with probability nc/n in queries rankc(S, i) and selectc(S, i),
where nc is the frequency of c in S, the average query time is O(1 + H0(S)/ lg lg n).
Proof. We build exactly the same t-ary Huﬀman tree used in Theorem 4, using the frequencies
nc instead of run lengths. The sequences at each internal node are formed so as to indicate how
the symbols in the child nodes are interleaved in S. This is precisely a multiary Huﬀman-shaped
wavelet tree [GGV03, FMMN07], and our previous analysis shows that the space used by the tree
. The three queries are
is exactly as in Theorem 4, where now the entropy is H0(S) = Pc
solved by going down or up the tree and using rank and select on the sequences stored at the
nodes [GGV03, FMMN07]. Under the conditions stated for the average case, one arrives at the leaf
of symbol c with probability nc/n, and then the average case complexities follow.
(cid:3)

nc
n lg n
nc

4 Strict Runs

Some classes of permutations can be covered by a small number of runs of a stricter type. We
present an encoding scheme that take advantage of them.

Deﬁnition 3 A strict ascending run in a permutation π is a maximal range of positions satisfying
π(i + k) = π(i) + k. The head of such run is its ﬁrst position. The number of strict ascending runs
of π is noted nSRuns, and the sequence of the lengths of the strict ascending runs is noted vSRuns.
We will call vHRuns the sequence of contiguous monotone run lengths of the sequence formed by the
strict run heads of π. Similarly, the notion of a strict descending run can be deﬁned, as well as that
of strict (monotone) run encompassing both.

For example, the permutation (6 , 7 , 8 , 9 , 10 , 1, 2, 3, 4, 5) contains nSRuns = 2 strict runs, of
lengths vSRuns = h5, 5i. The run heads are h6 , 1i, which form 1 monotone run, of lengths vHRuns =
h2i. Instead, the permutation (1 , 3 , 5 , 7 , 9 , 2, 4, 6, 8, 10) contains nSRuns = 10 strict runs, each of
length 1.

Theorem 6 Assume there is an encoding P for a permutation over [1..n] with nRuns contiguous 
monotone runs of lengths forming the vector vRuns, which requires s(n, nRuns, vRuns) bits of
space and can apply the permutation and its inverse in time t(n, nRuns, vRuns). Now consider a
permutation π over [1..n] covered by nSRuns strict runs and by nRuns ≤ nSRuns monotone runs,
and let vHRuns be the vector formed by the nRuns monotone run lengths in the permutation of
strict run heads. Then there is an encoding scheme using at most s(nSRuns, nRuns, vHRuns) +

10

n

nSRuns ) + o(n) bits for π. It can be computed in O(n) time on top of that for building P .
O(nSRuns lg
It supports the computation of π(i) and π−1(i) in time O(t(nSRuns, nRuns, vHRuns)) for any value
i ∈ [1..n].
Proof. We ﬁrst set up a bitmap R of length n marking with a 1 bit the beginning of the strict runs.
We set up a second bitmap Rinv such that Rinv[i] = R[π−1(i)]. Now we create a new permutation
π′ over [1..nSRuns] which collapses the strict runs of π, π′(i) = rank1(Rinv, π(select1(R, i))). All
nSRuns +O(nSRuns) + o(n) bits in compressed
this takes O(n) time and the bitmaps take 2nSRuns lg
form [GGG+07], where rank and select are supported in constant time.
Now we build the structure P for π′. The number of monotone runs in π is the same as for the
sequence of strict run heads in π, and in turn the same as the runs in π′. So the number of runs
in π′ is also nRuns and their lengths are vHRuns. Thus we require s(nSRuns, nRuns, vHRuns) further
bits.

n

To compute π(i), we ﬁnd i′ ← rank1(R, i) and then compute j′ ← π′(i′). The ﬁnal answer is
select1(Rinv, j′) + i − select1(R, i′). To compute π−1(j), we ﬁnd j′ ← rank1(Rinv, j) and then
compute i′ ← (π′)−1(j′). The ﬁnal answer is select1(R, i′) + j − select1(Rinv, j′). The structure 
requires only constant time on top of that to support the operator π′() and its inverse π′−1() . (cid:3)

The theorem can be combined with previous results, for example Theorem 4, in order to obtain
concrete data structures. This representation is interesting because its space could be much less
than n if nSRuns is small enough. However, it still retains an o(n) term that can be dominant.
The following corollary describes a compressed data structure where the o(n) term is signiﬁcantly
reduced.

Corollary 7 The o(n) term in the space of Theorem 6 can be replaced by O(nSRuns lg lg
at the cost of O(1 + lg nSRuns) extra time for the queries.
Proof. Replace the structure of Golynski et al. [GGG+07] by the binary searchable gap encoding of
Gupta et al. [GHSV06], which takes O(1 + lg nSRuns) time for rank and select (recall Section 2.3).

nSRuns +lg n)

n

(cid:3)

Other tradeoﬀs for the bitmap encodings are possible,

such as the one described by

Gupta [Gup07, Theorem 18 p. 155].

5 Shuﬄed Sequences

Up to now our runs have been contiguous in π. Levcopoulos and Petersson [LP94] introduced
the more sophisticated concept of partitions formed by interleaved runs, such as Shuﬄed UpSequences 
(SUS) and Shuﬄed Monotone Sequences (SMS). We now show how to take advantage of
permutations formed by shuﬄing (interleaving) a small number of runs.

Deﬁnition 4 A decomposition of a permutation π over [1..n] into Shuﬄed UpSequences is a set of,
not necessarily consecutive, subsequences of increasing numbers that have to be removed from π in
order to reduce it to the empty sequence. The number of shuﬄed upsequences in such a decomposition
of π is noted nSUS, and the vector formed by the lengths of the involved shuﬄed upsequences, in
arbitrary order, is noted vSUS. When the subsequences can be of increasing or decreasing numbers,

11

we call them Shuﬄed Monotone Sequences, call nSMS their number and vSMS the vector formed by
their lengths.

For example, the permutation (1 , 6, 2 , 7, 3 , 8, 4 , 9, 5 , 10) contains nSUS = 2 shuﬄed upsequences 
of lengths forming the vector vSUS = h5, 5i, but nRuns = 5 runs, all of length 2.
Interestingly,
 we can reduce the problem of representing shuﬄed sequences to that of representing
strings and contiguous runs.

5.1 Reduction to Strings and Contiguous Monotone Sequences

We ﬁrst show how a permutation with a small number of shuﬄed monotone sequences can be
represented using strings over a small alphabet and permutations with a small number of contiguous
monotone sequences.

Theorem 8 Assume there exists an encoding P for a permutation over [1..n] with nRuns contiguous
monotone runs of lengths forming the vector vRuns, which requires s(n, nRuns, vRuns) bits of space
and supports the application of the permutation and its inverse in time t(n, nRuns, vRuns). Assume
also that there is a data structure S for a string S[1..n] over an alphabet of size nSMS with symbol
frequencies vSMS, using s′(n, nSMS, vSMS) bits of space and supporting operators rank, select, and
access to values S[i], in time t′(n, nSMS, vSMS). Now consider a permutation π over [1..n] covered
by nSMS shuﬄed monotone sequences of lengths vSMS. Then there exists an encoding of π using
at most s(n, nSMS, vSMS) + s′(n, nSMS, vSMS) + O(nSMS lg n
nSMS ) + o(n) bits. Given the covering into
SMSs, the encoding can be built in time O(n), in addition to that of building P and S. It supports
the computation of π(i) and π−1(i) in time t(n, nSMS, vSMS) + t′(n, nSMS, vSMS) for any value of
i ∈ [1..n]. The result is also valid for shuﬄed upsequences, in which case P is just required to handle
ascending runs.

Proof. Given the partition of π into nSMS monotone subsequences, we create a string S[1..n] over
alphabet [1..nSMS] that indicates, for each element of π, the label of the monotone sequence it
belongs to. We encode S[1..n] using the data structure S. We also store an array A[1..nSMS] so that
A[ℓ] is the accumulated length of all the sequences with label less than ℓ.

Now consider the permutation π′ formed by the sequences taken in label order: π′ can be covered
with nSMS contiguous monotone runs vSMS, and hence can be encoded using s(n, nSMS, vSMS)
additional bits using P . This supports the operators π′() and π′−1() in time t(n, nSMS, vSMS)
(again, some of the runs could be merged in π′, which only improves time and space in P ). Thus
π(i) = π′(A[S[i]] + rankS[i](S, i)) can be computed in time t(n, nSMS, vSMS) + t′(n, nSMS, vSMS).
Similarly, π−1(i) = selectℓ(S, (π′)−1(i)−A[ℓ]), where ℓ is such that A[ℓ] < (π′)−1(i) ≤ A[ℓ+1], can
also be computed in time t(n, nSMS, vSMS) + t′(n, nSMS, vSMS), plus the time to ﬁnd ℓ. The latter is
reduced to constant by representing A with a bitmap A′[1..n] with the bits set at the values A[ℓ]+ 1,
so that A[ℓ] = select1(A′, ℓ) − 1, and the binary search is replaced by ℓ = rank1(A′, (π′)−1(i)).
With the structure of Golynski et al. [GGG+07], A′ uses O(nSMS lg n
nSMS ) + o(n) bits and operates
in constant time.
(cid:3)

We will now obtain concrete results by using speciﬁc representations for P and S, and speciﬁc

methods to ﬁnd the decomposition into shuﬄed sequences.

12

5.2 Shuﬄed UpSequences

Given an arbitrary permutation, one can decompose it in linear time into contiguous runs in order
to minimize H(vRuns), where vRuns is the vector of run lengths. However, decomposing the same
permutation into shuﬄed up (resp. monotone) sequences so as to minimize either nSUS or H(vSUS)
(resp. nSMS or H(vSMS)) is computationally harder.
Fredman [Fre75] gave an algorithm to compute a partition of minimum size nSUS, into upsequences,
 claiming a worst case complexity of O(n lg n). Even though he did not claim it at the
time, it is easy to observe that his algorithm is adaptive in nSUS and takes O(n(1 + lg nSUS))
time. We give here an improvement of his algorithm that computes the partition itself within time
O(n(1 + H(vSUS))), no worse than the time of his original algorithm, as H(vSUS) ≤ lg nSUS.
Theorem 9 If an array D[1..n] can be optimally covered by nSUS shuﬄed upsequences (equal values
do not break an upsequence), then there is an algorithm ﬁnding a covering of size nSUS in time
O(n(1 + H(vSUS))) ⊂ O(n(1 + lg nSUS)), where vSUS is the vector formed by the lengths of the
upsequences found.

Proof. Initialize a sequence S1 = (D[1]), and a splay tree T [ST85] with the node (S1), ordered by
the rightmost value of the sequence contained by each node. For each further array element D[i],
search for the sequence with the maximum ending point no larger than D[i]. If it exists, add D[i]
to this sequence, otherwise create a new sequence and add it to T .

Fredman [Fre75] already proved that this algorithm ﬁnds a partition of minimum size nSUS.
Note that, although the rightmost values of the splay tree nodes change when we insert a new
element in their sequence, their relative position with respect to the other nodes remains the same,
since all the nodes at the right hold larger values than the one inserted. This implies in particular
that only searches and insertions are performed in the splay tree.

A simple analysis, valid for both the plain sorted array in Fredman’s proof and the splay tree of
our own proof, yields an adaptive complexity of O(n(1+lg nSUS)) comparisons, since both structures
contain at most nSUS elements at any time. The additional linear term (relevant when nSUS = 1)
corresponds to the cost of reading each element once.

The analysis of the algorithm using the splay tree reﬁnes the complexity to O(n(1 +H(vSUS))),
where vSUS is the vector formed by the lengths of the upsequences found. These lengths correspond
to the frequencies of access to each node of the splay tree, which yields the total access time of
O(n(1 + H(vSUS))) [ST85, Theorem 2].
(cid:3)

The theorem obviously applies to the particular case where the array is a permutation. For permutations 
and, in general, integer arrays over a universe [1..m], we can deviate from the comparison
model and ﬁnd the partition within time O(n lg lg m), by using y-fast tries [Wil83] instead of splay
trees.
We can now give a concrete representation for shuﬄed upsequences. The complete description
of the permutation requires to encode the computation the partitioning and of the comparisons
performed by the sorting algorithm. This time the encoding cost of partitioning is as important as
that of merging.

Theorem 10 Let π be a permutation over [1..n] that can be optimally covered by nSUS shuﬄed upsequences,
 and let vSUS be the vector formed by the lengths of the decomposition found by the algorithm
of Theorem 9. Then there is an encoding scheme for π using at most 2nH(vSUS)+O(nSUS lg n)+o(n)

13

bits. It can be computed in time O(n(1+H(vSUS))), and supports the computation of π(i) and π−1(i)
in time O(1 + lg nSUS/ lg lg n) for any value of i ∈ [1..n]. If i is chosen uniformly at random in
[1..n] the average query time is O(1 + H(vSUS)/ lg lg n).
Proof. We ﬁrst use Theorem 9 to ﬁnd the SUS partition of optimal size nSUS, and the corresponding 
vector vSUS formed by the sizes of the subsequences of this partition. Then we apply
Theorem 8: For the data structure S we use Theorem 5, whereas for P we use Theorem 4. Note
H(vSUS) is both H0(S) and H(vRuns) for permutation π′. The result follows immediately.
(cid:3)

One would be tempted to consider the case of a permutation π covered by nSUS upsequences
which form strict runs, as a particular case. Yet, this is achieved by resorting directly to Theorem 4.
The corollary extends verbatim to shuﬄed monotone sequences.

Corollary 11 There is an encoding scheme using at most nH(vSUS) + O(nSUS lg n) + o(n) bits to
encode a permutation π over [1..n] optimally covered by nSUS shuﬄed upsequences, of lengths forming
the vector vSUS, and made up of strict runs. It can be built within time O(n(1 + H(vSUS)/ lg lg n)),
and supports the computation of π(i) and π−1(i) in time O(1 + lg nSUS/ lg lg n) for any value of
i ∈ [1..n].
If i is chosen uniformly at random in [1..n] then the average query time is O(1 +
H(vSUS)/ lg lg n).
Proof. It is suﬃcient to invert π and represent π−1 using Theorem 4, since in this case π−1 is
covered by nSUS ascending runs of lengths forming the vector vSUS: If i0 < i1 . . . < im forms a
strict upsequence, so that π(it) = π(i0) + t, then calling j0 = π(i0) we have the ascending run
π−1(j0 + t) = it for 0 ≤ t ≤ m.
(cid:3)

Once more, our construction translates into an improved sorting algorithm, improving on the

complexity O(n(1 + lg nSUS)) of the algorithm by Levcopoulos and Petersson [LP94].
Corollary 12 We can sort an array of length n, optimally covered by nSUS shuﬄed upsequences,
in time O(n(1 +H(vSUS))), where vSUS are the lengths of the decomposition found by the algorithm
of Theorem 9.

Proof. Our construction in Theorem 10 ﬁnds and separates the subsequences of π, and sorts them,
all within this time (we do not need to build the string S).
(cid:3)

Open problem Note that the algorithm of Theorem 9 ﬁnds a partition of minimal size nSUS (this
is what we refer to with “optimally covered”), but that the entropy H(vSUS) of this partition is not
necessarily minimal: There could be another partition, even of size larger than nSUS, with lower
entropy. Our results are only in function of the entropy of the partition of minimal size nSUS found.
This is unsatisfactory, as the ideal would be to speak in terms of the minimum possible H(vSUS),
just as we could do for H(vRuns).
(1, 2, . . . , n/2−1, n, n/2, n/2+1, . . . , n−1),
for
partition
{(1, 2, . . . , n/2−1, n), (n/2, n/2+1, . . . , n−1)} of entropy H(hn/2, n/2i) = n lg 2 = n. This is
suboptimal, as the partition {(1, 2, . . . , n/2−1, n/2, n/2+1, . . . , n−1), (n)} is of much smaller
entropy, H(hn−1, 1i) = (n − 1) lg n

n−1 + lg n = O(lg n).

consider
integer n.

permutation

The

algorithm of Theorem 9

yields

some

even

the

An

example,

the

14

On the other hand, a greedy online algorithm cannot minimize the entropy of a SUS partitioning.
 As an example consider the permutation (2, 3, . . . , n/2, 1, n, n/2+1, . . . , n−1), for some even
integer n. A greedy online algorithm that after processing a preﬁx of the sequence minimizes the
entropy of such preﬁx, produces the partition {(1, n/2+1, . . . , n−1), (2, 3, . . . , n/2, n)}, of size 2 and
entropy H(hn/2, n/2i) = n. However, a much better partition is {(1, n), (2, 3, . . . , n−1)}, of size 2
and entropy H(h2, n − 2i) = O(lg n).
We doubt that the SUS partition minimizing H(vSUS) can be found within time O(n(1 +
H(vSUS))) or even O(n(1 + lg nSUS)). Proving this right or wrong is an open challenge.

5.3 Shuﬄed Monotone Sequences

No eﬃcient algorithm is known to compute the minimum number nSMS of shuﬄed monotone sequences 
composing a permutation, let alone ﬁnding a partition minimizing the entropy H(vSMS) of
the lengths of the subsequences. The problem is NP-hard, by reduction to the computation of the
“cochromatic” number of the graph corresponding to the permutation [KSW96].

Yet, should such a partition into monotone subsequences be available, and be of smaller entropy
than the partitions considered in the previous sections, this would yield an improved encoding by
doing just as in Theorem 10 for SUS.

Note that it takes a diﬀerence by a superpolynomial margin between the values of nSUS and
nSMS to yield a noticeable diﬀerence between lg nSUS and lg nSMS, and hence between the values
of H(vSUS) and H(vSMS). It seems unlikely that such a diﬀerence would justify the diﬀerence of
computing time between the two types of partitions, also diﬀerent by a superpolynomial margin to
the best of current knowledge (i.e., if P 6= N P ).

6 Conclusions

Relation between space and time Bentley and Yao [BY76] introduced a family of search
algorithms adaptive to the position of the element sought (also known as the “unbounded search”
problem) through the deﬁnition of a family of adaptive codes for unbounded integers, hence proving
that the link between algorithms and encodings was not limited to the complexity lower bounds
suggested by information theory. Such a relation between “time” and “space” can be found in other
contexts: algorithms to merge two sets deﬁne an encoding for sets [AL09], and the binary results of
the comparisons of any deterministic sorting algorithm in the comparison model yields an encoding
of the permutation being sorted.

We have shown that some concepts originally deﬁned for adaptive variants of the algorithm
MergeSort, such as runs and shuﬄed sequences, are useful in terms of the compression of permutations,
 and conversely, that concepts originally deﬁned for data compression, such as the entropy of
the sets of run lengths, are a useful addition to the set of diﬃculty measures previously considered
in the study of adaptive sorting algorithms.

Much more work is required to explore the application to the compression of permutations
and strings of the many other measures of preorder introduced in the study of adaptive sorting
algorithms. Figure 1 represents graphically some of those measures of presortedness (adding to those
described by Moﬀat and Petersson [MP92], those described in this and other recent work [BFN11])
and a preorder on them based on optimality implication in terms of the number of comparison
performed. This is relevant for the space of the corresponding permutation encodings, and for the

15

Hist

Block

Rem

Exc = Ham

Reg

Loc

Osc

Inv=DS

Max=Par

nSMS

Enc

nSUS

nRuns

nSRuns

H(vSMS) H(vSUS) H(vLRM) H(vRuns) H(vSRuns)

Figure 1: Partial order on some measures of disorder for adaptive sorting. New results are on the
bottom line.

space used by the potential corresponding compressed data structures for permutations. Note that
the reductions in this graph do not represent reductions in terms of optimality of the running time
to ﬁnd the partitions. For instance, we saw that H(vSMS)-optimality implies H(vSUS)-optimality in
terms of the number of comparison performed, but not in terms of the running time. In terms of
data structures, this relates to the construction time of the compressed data structure (as opposed
to the space it takes).

It is worth noticing that, in many cases, the time to support the operators
Adaptive operators
on the compressed permutations is smaller as the permutation is more compressed, in opposition
with the traditional setting where one needs to decompress part or all of the data in order to support
the operators. This behavior, incidental in our study, is a very strong incentive to further develop
the study of diﬃculty or compressibility measures: measures such that “easy” instances can both
be compressed and manipulated in better time capture the essence of the data.

Interestingly enough, our encoding techniques for permutations compress
Compressed indices
both the permutation and its index (i.e., the extra data to speed up the operators). This is opposed
to previous work [MRRR03] on the encoding of permutations, whose index size varied with the size of
the cycles of the permutation, but whose data encoding was ﬁxed; and to previous work [BHMR07]
where the data itself can be compressed but not the index, to the point where the space used by the
index dominates that used by the data itself. This direction of research is promising, as in practice
it is more interesting to compress the whole succinct data structure or at least its index, rather than
just the data.

Applications Permutations are everywhere, so that compressing their representation helps compress 
many other forms of data, and supporting in reasonable time the operators on permutations
yield support for other operators.

As a ﬁrst example, consider a natural language text tokenized into word identiﬁers. Its wordbased 
inverted index stores for each distinct word the list of its occurrences in the tokenized text,
in increasing order. This is a popular data structure for text indexing [BYRN11, WMB99]. By
regarding the concatenation of the lists of occurrences of all the words, a permutation π is obtained
that is formed by ν contiguous ascending runs, where ν is the vocabulary size of the text. The lengths
of those runs corresponds to the frequencies of the words in the text. Therefore our representation
achieves the zero-order word-based entropy of the text, which in practice compresses the text to
about 25% of its original size [BCW90]. With π(i) we can access any position of any inverted

16

list, and with π−1(j) we can ﬁnd the word that is at any text position j. Thus the representation
contains the text and its inverted index within the space of the compressed text.

A second example is given by compressed suﬃx arrays (CSAs), which are data structures for
indexing general texts. A family of CSAs builds on a function called Ψ [GV06, Sad03, GGV03],
which is actually a permutation. Much eﬀort was spent in compressing Ψ to the zeroor 
higher-order
entropy of the text while supporting direct access to it. It turns out that Ψ contains σ contiguous
increasing runs, where σ is the alphabet size of the text, and that the run lengths correspond to the
symbol frequencies. Thus our representation of Ψ would reach the zero-order entropy of the text.
It supports not only access to Ψ but also to its inverse Ψ−1, which enables so-called bidirectional
indexes [RNOM09], which have several interesting properties. Furthermore, Ψ contains a number of
strict ascending runs that depends on the high-order entropy of the text, and this allows compressing
it further [NM07].

From a practical point of view, our encoding schemes are simple enough to be implemented.
Some preliminary results on inverted indexes and compressed suﬃx arrays show good performances
on practical data sets. As an external test, the techniques were successfully used to handle scalability
problems in MPI applications [KMW10].

Followup Our preliminary results [BN09] have stimulated further research. This is just a glimpse
of the work that lies ahead on this topic.

While developing, with J. Fischer, compressed indexes for Range Minimum Query indexes based
on Left-to-Right Minima (LRM) trees [Fis10, SN10], we realized that LRM trees yield a technique
to rearrange in linear time nRuns contiguous ascending runs of lengths forming vector vRuns, into a
partition of nLRM = nRuns ascending subsequences of lengths forming a new vector vLRM, of smaller
entropy H(vLRM) ≤ H(vRuns) [BFN11]. Compared to a SUS partition, the LRM partition can have
larger entropy, but it is much cheaper to compute and encode. We represent it on Figure 1 between
H(vRuns) and H(vSUS).
While developing, with T. Gagie and Y. Nekrich, an elegant combination of previously known
compressed string data structures to attain superior space/time trade-oﬀs [BGNN10], we realized
that this yields various compressed data structures for permutations π such that the times for π()
and π−1() are improved to log-logarithmic. While those results subsume our initial ﬁndings [BN09],
the improved results now presented in Theorem 4 are incomparable, and in particular superior when
the number of runs is polylogarithmic in n.

Acknowledgements

We thank Ian Munro, Ola Petersson and Alistair Moﬀat for interesting discussions.

References

[AL09]

Bruno T. Ávila and Eduardo S. Laber. Merge source coding. In ISIT’09: Proceedings of
the 2009 IEEE international conference on Symposium on Information Theory, pages
214–218, Piscataway, NJ, USA, 2009. IEEE Press.

17

[ANS06]

D. Arroyuelo, G. Navarro, and K. Sadakane. Reducing the space requirement of LZindex.
 In Proc. 17th Annual Symposium on Combinatorial Pattern Matching (CPM),
LNCS 4009, pages 319–330, 2006.

[Bae07]

M. Baer. D-ary bounded-length Huﬀman coding. CoRR, abs/cs/0701012, 2007.

[BCW90] T. Bell, J. Cleary, and I. Witten. Text compression. Prentice Hall, 1990.

[BFN11]

Jérémy Barbay, Johannes Fischer, and Gonzalo Navarro. LRM-Trees: Compressed indices,
 adaptive sorting, and compressed permutations. In Proc. 22th Annual Symposium
on Combinatorial Pattern Matching (CPM), LNCS 6661, pages 285–298, 2011.

[BGNN10] Jérémy Barbay, Travis Gagie, Gonzalo Navarro, and Yakov Nekrich. Alphabet partitioning 
for compressed rank/select and applications. In O. Cheong, K.-Y. Chwa, and
K.Parks, editors, Proceedings of ISAAC 2010, LNCS, volume 6507, pages 315–326, 2010.

[BHMR07] Jérémy Barbay, Meng He, J. Ian Munro, and S. Srinivasa Rao. Succinct indexes for
strings, binary relations and multi-labeled trees. In Proceedings of the 18th ACM-SIAM
Symposium on Discrete Algorithms (SODA), pages 680–689. ACM, 2007.

[BLNS09] N. Brisaboa, M. Luaces, G. Navarro, and D. Seco. A new point access method based on
wavelet trees. In Proc. 3rd International Workshop on Semantic and Conceptual Issues
in GIS (SeCoGIS), LNCS 5833, pages 297–306, 2009.

[BN09]

J. Barbay and G. Navarro. Compressed representations of permutations, and applications.

In Proc. 26th International Symposium on Theoretical Aspects of Computer
Science (STACS), pages 111–122. Schloss Dagstuhl, Leibnitz Zentrum fuer Informatik,
Germany, 2009.

[BY76]

Jon Louis Bentley and Andrew Chi-Chih Yao. An almost optimal algorithm for unbounded 
searching. Information processing letters, 5(3):82–87, 1976.

[BYRN11] R. Baeza-Yates and B. Ribeiro-Neto. Modern Information Retrieval. Addison-Wesley,

2nd edition, 2011.

[CHSV08] Y.-F. Chien, W.-K. Hon, R. Shah, and J. Vitter. Geometric Burrows-Wheeler transform:
In Proc. Data Compression Conference

Linking range searching and text indexing.
(DCC), pages 252–261, 2008.

[CT91]

T. Cover and J. Thomas. Elements of Information Theory. Wiley, 1991.

[Fis10]

J. Fischer. Optimal succinctness for range minimum queries. In Proc. 9th Symposium
on Latin American Theoretical Informatics (LATIN), LNCS 6034, pages 158–169, 2010.

[FMMN07] P. Ferragina, G. Manzini, V. Mäkinen, and G. Navarro. Compressed representations of
sequences and full-text indexes. ACM Transactions on Algorithms (TALG), 3(2):article
20, 2007.

[Fre75]

M. L. Fredman. On computing the length of longest increasing subsequences. Discrete
Math., 11:29–35, 1975.

18

[FW94]

M. Fredman and D. Willard. Trans-dichotomous algorithms for minimum spanning
trees and shortest paths. Journal of Computer and Systems Science, 48(3):533–551,
1994.

[GGG+07] A. Golynski, R. Grossi, A. Gupta, R. Raman, and S.S. Rao. On the size of succinct
indices. In Proc. 15th Annual European Symposium on Algorithms (ESA), LNCS 4698,
pages 371–382, 2007.

[GGV03] R. Grossi, A. Gupta, and J. Vitter. High-order entropy-compressed text indexes. In
Proc. 14th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 841–
850, 2003.

[GHSV06] A. Gupta, W.-K. Hon, R. Shah, and J.S. Vitter. Compressed data structures: Dictionaries 
and data-aware measures. In Proc. 16th Data Compression Conference (DCC),
pages 213–222, 2006.

[GMR06] Alexander Golynski, J. Ian Munro, and S. Srinivasa Rao. Rank/select operations on
large alphabets: a tool for text indexing. In Proceedings of the 17th Annual ACM-SIAM
Symposium on Discrete Algorithms (SODA), pages 368–373. ACM, 2006.

[Gol06]

A. Golynski. Optimal lower bounds for rank and select indexes. In Proc. 33th International 
Colloquium on Automata, Languages and Programming (ICALP), LNCS 4051,
pages 370–381, 2006.

[GRR08] A. Golynski, R. Raman, and S. Rao. On the redundancy of succinct data structures. In
Proc. 11th Scandinavian Workshop on Algorithm Theory (SWAT), LNCS 5124, pages
148–159, 2008.

[Gup07]

A. Gupta. Succinct Data Structures. PhD thesis, Dept. of Computer Science, Duke
University, 2007.

[GV06]

[Huf52]

[Kär99]

R. Grossi and J. Vitter. Compressed suﬃx arrays and suﬃx trees with applications to
text indexing and string matching. SIAM Journal on Computing, 35(2):378–407, 2006.

D. Huﬀman. A method for the construction of minimum-redundancy codes. Proceedings
of the I.R.E., 40(9):1090–1101, 1952.

J. Kärkkäinen. Repetition-based text indexes. PhD thesis, Dept. of Computer Science,
University of Helsinki, Finland, 1999. Also available as Report A-1999-4, Series A.

[KMW10] H. Kamal, S. Mirtaheri, and A. Wagner. Scalability of communicators and groups in
MPI. In Proc. 19th ACM International Symposium on High Performance Distributed
Computing (HPDC), pages 264–275, 2010.

[Knu98]

Donald E. Knuth. Art of Computer Programming, Volume 3: Sorting and Searching
(2nd Edition). Addison-Wesley Professional, April 1998.

[KSW96] André E. Kézdy, Hunter S. Snevily, and Chi Wang. Partitioning permutations into
increasing and decreasing subsequences. J. Comb. Theory Ser. A, 73(2):353–359, 1996.

19

[LP94]

Christos Levcopoulos and Ola Petersson. Sorting shuﬄed monotone sequences. Inf.
Comput., 112(1):37–50, 1994.

[Man85]

Heikki Mannila. Measures of presortedness and optimal sorting algorithms. In IEEE
Trans. Comput., volume 34, pages 318–325, 1985.

[ML01]

[MN07]

[MP92]

[MR04]

R. L. Milidiú and E. S. Laber. Bounding the ineﬃciency of length-restricted preﬁx
codes. Algorithmica, 31(4):513–529, 2001.

V. Mäkinen and G. Navarro. Rank and select revisited and extended. Theoretical
Computer Science, 387(3):332–347, 2007.

Alistair Moﬀat and Ola Petersson. An overview of adaptive sorting. Australian Computer 
Journal, 24(2):70–77, 1992.

J. Ian Munro and S. Srinivasa Rao. Succinct representations of functions. In Proceedings
of the International Colloquium on Automata, Languages and Programming (ICALP),
volume 3142 of Lecture Notes in Computer Science (LNCS), pages 1006–1015. SpringerVerlag,
 2004.

[MRRR03] J. Ian Munro, Rajeev Raman, Venkatesh Raman, and S. Srinivasa Rao. Succinct representations 
of permutations. In Proceedings of the 30th International Colloquium on
Automata, Languages and Programming (ICALP), volume 2719 of Lecture Notes in
Computer Science (LNCS), pages 345–356. Springer-Verlag, 2003.

[MS76]

J. Ian Munro and Philip M. Spira. Sorting and searching in multisets. SIAM J. Comput.,
5(1):1–8, 1976.

[Mun96]

I. Munro. Tables. In Proc. 16th Conference on Foundations of Software Technology and
Theoretical Computer Science (FSTTCS), LNCS 1180, pages 37–42, 1996.

[NM07]

[Pˇ08]

G. Navarro and V. Mäkinen. Compressed full-text indexes. ACM Computing Surveys,
39(1):article 2, 2007.

M. Pˇatraşcu. Succincter. In Proc. 49th IEEE Annual Symposium on Foundations of
Computer Science (FOCS), pages 305–313, 2008.

[RNOM09] L. Russo, G. Navarro, A. Oliveira, and P. Morales. Approximate string matching with

compressed indexes. Algorithms, 2(3):1105–1136, 2009.

[RRR02]

R. Raman, V. Raman, and S. Rao. Succinct indexable dictionaries with applications to
encoding k-ary trees and multisets. In Proc. 13th Annual ACM-SIAM Symposium on
Discrete Algorithms (SODA), pages 233–242, 2002.

[Sad03]

[SN10]

K. Sadakane. New text indexing functionalities of the compressed suﬃx arrays. Journal
of Algorithms, 48(2):294–313, 2003.

K. Sadakane and G. Navarro. Fully-functional succinct trees.
ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 134–149, 2010.

In Proc. 21st Annual

20

[ST85]

[Wil83]

D. Sleator and R. Tarjan. Self-adjusting binary search trees. Journal of the ACM,
32(3):652–686, 1985.

D. Willard. Log-logarithmic worst case range queries are possible in space Θ(n). Information 
Processing Letters, 17:81–84, 1983.

[WMB99]

I. Witten, A. Moﬀat, and T. Bell. Managing Gigabytes. Morgan Kaufmann Publishers,
2nd edition, 1999.

21

