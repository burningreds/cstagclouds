An Eﬃcient Compression Code

for Text Databases(cid:1)

Nieves R. Brisaboa1, Eva L. Iglesias2, Gonzalo Navarro3, and Jos´e R. Param´a1

1 Database Lab., Univ. da Coru˜na, Facultade de Inform´atica

Campus de Elvi˜na s/n, 15071 A Coru˜na, Spain

{brisaboa,parama}@udc.es

2 Computer Science Dept., Univ. de Vigo
Escola Superior de Enxe˜ner´ıa Inform´atica

Campus As Lagoas s/n, 32001, Ourense, Spain

3 Dept. of Computer Science, Univ. de Chile

eva@uvigo.es

Blanco Encalada 2120, Santiago, Chile

gnavarro@dcc.uchile.cl

Abstract. We present a new compression format for natural language
texts, allowing both exact and approximate search without decompression.
 This new code –called End-Tagged Dense Code– has some advantages 
with respect to other compression techniques with similar features
such as the Tagged Huﬀman Code of [Moura et al., ACM TOIS 2000].
Our compression method obtains (i) better compression ratios, (ii) a simpler 
vocabulary representation, and (iii) a simpler and faster encoding.
At the same time, it retains the most interesting features of the method
based on the Tagged Huﬀman Code, i.e., exact search for words and
phrases directly on the compressed text using any known sequential pattern 
matching algorithm, eﬃcient word-based approximate and extended
searches without any decoding, and eﬃcient decompression of arbitrary
portions of the text. As a side eﬀect, our analytical results give new upper 
and lower bounds for the redundancy of d-ary Huﬀman codes.

Keywords: Text compression, D-ary Huﬀman coding, text databases.

1 Introduction

Text compression techniques are based on exploiting redundancies in the text
to represent it using less space [3]. The amount of text collections has grown
in recent years mainly due to the widespread use of digital libraries, documental 
databases, oﬃce automation systems and the Web. Current text databases
contain hundreds of gigabytes and the Web is measured in terabytes. Although
the capacity of new devices to store data grows fast, while the associated costs
decrease, the size of the text collections increases also rapidly. Moreover, cpu

(cid:1) This work is partially supported by CICYT grant (#TEL99-0335-C04), CYTED

VII.19 RIBIDI Project, and (for the third author) Fondecyt Grant 1-020831.

F. Sebastiani (Ed.): ECIR 2003, LNCS 2633, pp. 468–481, 2003.
c(cid:1) Springer-Verlag Berlin Heidelberg 2003

An Eﬃcient Compression Code for Text Databases

469

speed grows much faster than that of secondary memory devices and networks,
so storing data in compressed form reduces i/o time, which is more and more
convenient even in exchange for some extra cpu time.

Therefore, compression techniques have become attractive methods to save
space and transmission time. However, if the compression scheme does not allow
to search for words directly on the compressed text, the retrieval will be less
eﬃcient due to the necessity of decompression before the search.

Classic compression techniques, as the well-known algorithms of Ziv and
Lempel [16, 17] or the character oriented code of Huﬀman [4], are not suitable for
large textual databases. One important disadvantage of these techniques is the
ineﬃciency of searching for words directly on the compressed text. Compression
schemes based on Huﬀman codes are not often used on natural language because
of the poor compression ratios achieved. On the other hand, Ziv and Lempel
algorithms obtain better compression ratios, but the search for a word on the
compressed text is ineﬃcient. Empirical results [11] showed that searching on
a Ziv-Lempel compressed text can take half the time of decompressing that text
and then searching it. However, the compressed search is twice as slow as just
searching the uncompressed version of the text.

In [13], Moura et al. present a compression scheme that uses a semi-static
word-based model and a Huﬀman code where the coding alphabet is byteoriented.
 This compression scheme allows the search for a word on the compressed 
text without decompressing it in such a way that the search can be up
to eight times faster for certain queries. The key idea of this work (and others
like that of Moﬀat and Turpin [8]) is the consideration of the text words as
the symbols that compose the text (and therefore the symbols that should be
compressed). Since in Information Retrieval (IR) text words are the atoms of
the search, these compression schemes are particularly suitable for IR. This idea
has been carried on further up to a full integration between inverted indexes
and word-based compression schemes, opening the door to a brand new family
of low-overhead indexing methods for natural language texts [14, 9, 18].

The role played by direct text searching in the above systems is as follows.
In order to reduce index space, the index does not point to exact word positions
but to text blocks (which can be documents or logical blocks independent of
documents). A space-time tradeoﬀ is obtained by varying the block size. The
price is that searches in the index may have to be complemented with sequential
scanning. For example, in a phrase query the index can point to blocks where all
the words appear, but a only sequential search can tell whether the phrase actually 
appears. If blocks do not match documents, even single word searches have
to be complemented with sequential scanning of the candidate blocks. Under
this scenario, it is essential to be able of keeping the text blocks in compressed
form and searching them without decompressing.

Two basic search methods are proposed in [13]. One handles plain Huﬀman
code (over words) and explores one byte of the compressed text at a time. This
is quite eﬃcient, but not as much as the second choice, which compresses the
pattern and uses any classical string matching strategy, such as Boyer-Moore [10].

