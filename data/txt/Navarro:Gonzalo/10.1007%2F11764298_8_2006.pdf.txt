Practical Construction of

k-Nearest Neighbor Graphs in Metric Spaces(cid:2)

Rodrigo Paredes1, Edgar Ch´avez2, Karina Figueroa1,2, and Gonzalo Navarro1

1 Center for Web Research, Dept. of Computer Science, University of Chile
2 Escuela de Ciencias F´ısico-Matem´aticas, Univ. Michoacana, Mexico
{raparede, kfiguero, gnavarro}@dcc.uchile.cl, elchavez@umich.mx

Abstract. Let U be a set of elements and d a distance function deﬁned
among them. Let NNk(u) be the k elements in U−{u} having the smallest 
distance to u. The k-nearest neighbor graph (knng) is a weighted
directed graph G(U, E) such that E = {(u, v), v ∈ NNk(u)}. Several
knng construction algorithms are known, but they are not suitable to
general metric spaces. We present a general methodology to construct
knngs that exploits several features of metric spaces. Experiments suggest 
that it yields costs of the form c1n1.27 distance computations for low
and medium dimensional spaces, and c2n1.90 for high dimensional ones.

1 Introduction

Let U be a set of elements and d a distance function deﬁned among them.
Let NNk(u) be the k elements in U − {u} having the smallest distance to u
according to the function d. The k-nearest neighbor graph (knng) is a weighted
directed graph G(U, E) connecting each element to its k-nearest neighbors, thus
E = {(u, v), v ∈ NNk(u)}. Building the knng is a direct generalization of the all-
nearest-neighbor (ann) problem, so ann corresponds to the 1nng construction
problem. knngs are central in many applications: cluster and outlier detection
[14, 4], VLSI design, spin glass and other physical process simulations [6], pattern
recognition [12], query or document recommendation systems [3], and others.

There are many knng construction algorithms which assume that nodes are
points in RD and d is the Euclidean or some Lp Minkowski distance. However,
this is not the case in several knng applications. An example is collaborative
ﬁlters for Web searching, such as query or document recommendation systems,
where knngs are used to ﬁnd clusters of similar queries, to later improve the
quality of the results shown to the ﬁnal user by exploiting cluster properties [3].
To handle this problem one must resort to a more general model called metric
spaces. A metric space is a pair (X, d), where X is the universe of objects and d
is a distance function among them that satisﬁes the triangle inequality.
Another appealing problem in metric spaces is similarity searching [8]. Given
a ﬁnite metric database U ⊆ X, the goal is to build an index for U such that
(cid:2) Supported in part by Millennium Nucleus Center for Web Research, Grant

P04-067-F, Mideplan, Chile; and CONACyT, Mexico.

`A

lvarez and M. Serna (Eds.): WEA 2006, LNCS 4007, pp. 85–97, 2006.

C.
c(cid:2) Springer-Verlag Berlin Heidelberg 2006

R. Paredes et al.

86
later, given a query object q ∈ X, one can ﬁnd elements of U close to q using as
few distance computations as possible. See [8] for a comprehensive survey.

We have already demonstrated knng searching capabilities in general metric
spaces [20], where we give knng-based search algorithms with practical applicability 
in low-memory scenarios, or metric spaces of medium or high dimensionality.
 Hence, in this paper we focus on a metric knng construction methodology,
and propose two algorithms based on such methodology. According to our experimental 
results, they have costs of the form c1n1.27 distance computations
for low and medium dimensionality spaces, and c2n1.90 for high dimensionality
ones. Note that a naive construction requires O(n2) distance evaluations.

1.1 A Summary of Metric Space Searching
Given the universe of objects X, a metric space is a pair (X, d), where d : X×X →
R+ is any distance function in X that is symmetric and satisﬁes the triangle
inequality. Some examples are (RD, Lp), the space of strings under the edit
distance, or the space of documents under the cosine distances.
The metric database is a ﬁnite set U ⊆ X, n = |U|. A similarity query is an
object q ∈ X, and allows two basic types: the Range query (q, r) retrieves all
objects u ∈ U such that d(u, q) ≤ r; and the k-Nearest neighbor query NNk(q)
retrieves the k objects in U closest to q according to the distance d. A NNk(q)
algorithm is called range-optimal [16] if it uses the same number of distance evaluations 
as the equivalent range query whose radius retrieves exactly k objects.
We call this radius covering radius.
An index I is a data structure built over U using some cells from the whole
U× U distance matrix. I permits solving the above queries without comparing q
with each element in U. There are two kinds of indices: pivot based and compact
partition based. Search algorithms use I and some distance evaluations to discard 
– using the triangle inequality – as many objects as they can, to produce a
small candidate set C that could be relevant to q. Later, they exhaustively check
C by computing distances from q to each candidate to obtain the query result.
As the distance is considered expensive to compute, it is customary to use
the number of distance evaluations as the complexity measure both for index
construction and object retrieving. For instance, each computation of the cosine
distance takes 1.4 msecs in our machine (Pentium IV of 2 GHz). This is really
costly even compared with the operations introduced by the graph, such as the
shortest path computation using Dijkstra’s algorithm.

Many authors agree that the proximity query cost worsens quickly as the
intrinsic dimensionality of the space grows. This is known as the curse of dimensionality.
 Although there is not and accepted criterion to deﬁne the intrinsic
dimensionality in a metric space, a general agreement is that spaces with low variance 
and large mean in their distance histograms have a large intrinsic dimension.

1.2 Related Work on knng Construction
The naive approach to construct knngs uses n(n−1)
= O(n2) distance computations 
and O(kn) memory. For each u ∈ U we compute the distance to all the

2

Practical Construction of k-Nearest Neighbor Graphs

87

others, selecting the k lowest-distance objects. However, there are alternatives to
speed up the procedure. The proximity properties of the Voronoi diagram [2] or
its dual, the Delaunay triangulation, allow solving the problem more eﬃciently.
The ann problem can be optimally solved in O(n log n) time in the plane [13] and
in RD for any ﬁxed D [9, 22], but the constant depends exponentially on D. In
RD, knngs can be built in O(nk log n) time [22] and even in O(kn+n log n) time
[5, 6, 11]. Approximation algorithms have also been proposed [1]. However, these
alternatives, except the naive one, are unsuitable for metric spaces, as they use
coordinate information that is not necessarily available in general metric spaces.
Clarkson states the ﬁrst generalization of ann to metric spaces [10], where
the problem is solved using randomization in O(n log2n log2 Γ (U)) expected
time, where Γ (U) is the distance ratio between the farthest and closest pairs
of points in U. The author argues that in practice Γ (U) = nO(1), in which case
the approach is O(n log4 n) time. However, the analysis needs a sphere packing
bound in the metric space. Otherwise the cost must be multiplied by “sphere
volumes”, that are also exponential on the dimensionality. Moreover, the algorithm 
needs Ω(n2) space for high dimensions, which is too much for practical
applications.

In [15], another technique for general metric spaces is given. It solves n
range queries of decreasing radius by using a pivot-based index. As it is well
known, the performance of pivot-based algorithms worsens quickly as the dimension 
of the space grows, limiting the applicability of this technique. Our
pivot based algorithm (Section 2.4) can be seen as an improvement over this
technique.

Recently, Karger and Ruhl present the metric skip list [18], an index that uses
O(n log n) space and can be constructed with O(n log n) distance computations.
The index answers NN1(q) queries using O(log n) distance evaluations with high
probability. Later, Krauthgamer and Lee introduce navigating nets [19], another
index that can be constructed also with O(n log n) distance computations, yet
using O(n) space, and which gives an (1 + )-approximation algorithm to solve
NN1(q) queries in time O(log n)+(1/)O(1). Both of them could serve to solve the
ann problem with O(n log n) distance computations but not to build knngs. In
addition, the hidden constants are exponential on the intrinsic dimension, which
makes these approaches useful only in low dimensional metric spaces.

2 Our Methodology

We are interested in practical knng construction algorithms for general metric
spaces. This problem is equivalent to solve n NNk(u) queries for all u ∈ U. Thus,
a straightforward solution has two stages: the ﬁrst is to build some known metric
index I [8], and the second is to use I to solve the n queries. However, this basic
scheme can be improved if we take into account these observations:

– We are solving queries for all the elements in U, not for general objects in X.
If we solve the n queries jointly we can share costs through the whole process.
For instance, we can avoid some calculations by using the symmetry of d.

88

R. Paredes et al.

– We can upper bound some distances by computing shortest paths over the
knng under construction, maybe avoiding their actual computation. So, we
can use the very knng in stepwise reﬁnements to improve the second stage.

2.1 The Ingredients of the Recipe

The main data structure. Along all the algorithm, we use the Neighbor Heap
Array (NHA) to store the knng under construction. NHA can be regarded as
the union of priority queues NHAu, of size k, for all u ∈ U. At any point in
the process NHAu will contain the k elements closest to u known up to then,
and their distances to u. Formally, NHAu = {(xi1 , d(u, xi1)), . . . , (xik , d(u, xik ))}
sorted by decreasing d(u, xij ) (ij is the j-th neighbor identiﬁer).
For each u ∈ U, we initialize NHAu = {(⊥,∞), . . . , (⊥,∞)}, |NHAu| = k.
Let curCRu = d(u, xi1) be the current covering radius of u, that is, the distance
from u towards its current farthest neighbor candidate in NHAu.
In the ﬁrst stage, every distance computed to build the index I populates
NHA. In the second, we reﬁne NHA with the following distance computations.
We must ensure that |NHAu| = k upon successive additions. Hence, if we ﬁnd
some object v such that d(u, v) < curCRu, before adding (v, d(u, v)) to NHAu we
extract the farthest candidate from NHAu. This progressively reduces curCRu
from ∞ to the real covering radius. At the end, NHA stores the knng of U.
Using NHA as a graph. Once we calculate duv = d(u, v), if duv ≥ curCRu we
discard v as a candidate for NHAu. Also, due to the triangle inequality we can
discard all objects w such that d(v, w) ≤ duv − curCRu. Unfortunately, we do
not necessarily have stored d(v, w). However, we can upper bound d(v, w) with
the sum of edge weights traversed in the shortest paths over NHA from v to all
w ∈ U, dNHA(v, w). So, if duv ≥ curCRu, we also discard all objects w such that
dNHA(v, w) ≤ duv − curCRu.
d is symmetric. Every time a distance duv = d(u, v) is computed, we check
both duv < curCRu for adding (v, duv) to NHAu, and duv < curCRv for adding
(u, duv) to NHAv. This can both reduce curCRv, and cheapen the future query
for v, even when we are solving neighbors for another object.

U is ﬁxed. Assume we are solving query NNk(u), we have to check some already
solved object v, and curCRu ≤ curCRv. Then, if u /∈ NNk(v) ⇒ d(u, v) ≥
curCRv, so v /∈ NNk(u). Otherwise, if u ∈ NNk(v), then we already computed
d(u, v). Then, in those cases we avoid to compute d(u, v). Fig. 1(a) illustrates.
CheckOrderHeap (COH). Wecreatethepriority queue COH = {(u, curCRu),
u ∈ U} to complete NNk(u) queries in increasing curCRu order, because a small
radius query has larger discriminative power and produces candidates that are
closer to the query u. This reduces the CPU time and – as d is symmetric – could
increase the chance of improving candidate sets in NHA for other objects v. This,
in turn, could reduce curCRv and change the position of v in COH.

Practical Construction of k-Nearest Neighbor Graphs

89

KNN (Integer k, ObjectSet U)
Stage 1: Initialize NHA and construct the index I
1. For each u ∈ U Do NHAu ← {(⊥,∞), . . . , (⊥,∞)} // k pairs
2. Create I, all computed distances populate symmetrically NHA
Stage 2: Complete the NNk(u) for all u ∈ U
3. COH ← {(u, curCRu), u ∈ U}
4. For each (u, curCRu) ∈ COH, in increasing curCRu order Do
Create the candidate set C according to I // exclude NHAu
5.
While C (cid:6)= ∅ Do
6.
7.
8.
9.
10.
11.
12. Return NHA as a graph

c ← extract a candidate from C
If “U is ﬁxed” does not apply for u and c Then
duc ← d(u, c), try to insert c into NHAu
try to insert u into NHAc, update c in COH (symmetry)
use NHA as a graph and I to discard objects from C

c

u

r

C

R

u

u

d

(

u
,
v

)

v

curC R v

NNk(v)d

c

u

r

C

R

u u

d
(
u,v
)

curC R v

v

NNk(v)d

(b) Sketch of the methodology.

(a) U is ﬁxed.
Fig. 1. In 1(a), assume we are solving u, v is already solved, and curCRu ≤ curCRv. On
the top, if u (cid:6)∈ NNk(v) ⇒ d(u, v) ≥ curCRv ≥ curCRu. On the bottom, if u ∈ NNk(v),
we already computed d(u, v). Then, in those cases we avoid computing d(u, v). In
Fig. 1(b), we sketch the methodology.

The recipe. We split the process into two stages. The ﬁrst is to build I to
preindex the objects. The second is to use I and all the ingredients to solve the
NNk(u) queries for all u ∈ U. Fig. 1(b) depicts the methodology.
memory both to index U and to store the knng under construction.

For practical reasons, we allow that our algorithms use at most O(n(log n+k))

2.2 The Resulting Algorithms
Based on our methodology, we propose two knng construction algorithms focused 
on decreasing the total number of distance computations. They are:

1. Recursive partition based algorithm: In the ﬁrst stage, we build a preindex
by performing a recursive partitioning of the space. In the second stage, we
complete the NNk(u) queries using the order induced by the partitioning.

2. Pivot based algorithm: In the preindexing stage, we build the pivot index.
Later, we complete the NNk(u) queries by performing range-optimal queries.

The experiments conﬁrm that these algorithms are eﬃcient. For instance, in
the string space, the pivot-based algorithm requires CPU time of the empirical
form ctn1.85, and cdn1.26 in distance computations. In the high-dimensional document 
space, the recursive partition-based algorithm requires empirically cn1.955
both in distance computations and CPU time.

90

R. Paredes et al.

2.3 Recursive Partition Based Algorithm

This algorithm is based on using a preindex slightly diﬀerent to the Bisector
Tree (BST ) [17]. We call our modiﬁed BST the Division Control Tree (DCT ),
which is a binary tree representing the shape of the partitioning. The DCT node
structure is {p, l, r, pr}, which represents the parent, left and right children, and
partition radius of the node, respectively. The partition radius is the distance
from the node towards the farthest node of its partition. (With respect to the
BST structure, we have added the pointer p to easily navigate trough the tree.)
For simplicity we use the same name for the node and for its representative
in the DCT . Then, given a node u ∈ U, up, ul, and ur, refer to nodes that are
the parent, left child, and right child of u in the DCT , respectively, and also to
their representative nodes in U. Finally, upr refers to the partition radius of u.
In this algorithm, we use O(kn) space to store the NHA and O(n) to store
the DCT . The remaining memory is used as a cache of computed distances,
CD, whose size is limited to O(n log n). Thus, every time we need to compute a
distance, we check if it is present in CD, in which case we return the stored value.
Note that the CD ⊂ U2 × R+ can also be seen as graph of all stored distances.
The criterion to insert distances into CD depends on the stage (see later). Once
we complete the NNk(u), we remove its adjacency list from CD.

First stage: construction of DCT . We partition the space recursively to
construct the DCT , and populate symmetrically NHA and CD with all the
computed distances. The DCT is built as follows. Given the node root and
the set S, we choose children objects l and r from S. Then, we generate two
subsets: Sl, objects nearest to l, and Sr, objects nearest to r. Finally, we compute
both partition radii. The recursion follows with (l, Sl) and (r, Sr), ﬁnishing when
|S| < 2. Once we ﬁnish the division, leaves in the DCT have partition radii 0.
The DCT root is ﬁctitious, having no equivalent in U, and partition radius ∞.
Since the DCT has n nodes, its expected height is 2 ln n (the DCT construction 
is statistically identical to populating a binary search tree). For each DCT
level, each node computes two distances towards the splitting nodes, which accounts 
for 2n distances per level. So, we expect to compute 4n ln n distances in
the partitioning. As we store 2 edges per distance, we need to store 8n ln n in
CD. Hence, we ﬁx the maximum space of CD as 8n ln n = O(n log n).

Solving NNk(u) queries with DCT . The construction of DCT ensures that
every node has already computed distances to all of its ancestors, its ancestor’s
siblings, and its parent descent. Then, to ﬁnish the NNk(u) query, it is enough to
check whether there are relevant objects in all the descendants of u’s ancestors’
siblings. This corresponds to white nodes and subtrees in Fig. 2(a).

Nevertheless, the DCT allows us to avoid some work. Assume we are checking
whether v is relevant to u, and the balls (u, curCRu) and (v, vpr) do not intersect
each other, then we discard v and its partition. Otherwise, we recursively check
children vl and vr. Fig. 2(b) illustrates this. Hence, in the candidate set C, it
suﬃces to manage the set of ancestors’ siblings, and if it is not possible to discard
the whole sibling’s partition we add its children into C. Since it is more likely to

Practical Construction of k-Nearest Neighbor Graphs

91

ROOT

v

u

c

u

r

C

R

u

u

vl
d(u,v)

v

pr

v

vr

(a) Induced check sequence.

(b) DCT can discard partitions.

Fig. 2. Using the DCT to solve NNk(q) queries. In 2(a), u has been compared with all
black nodes and all the descent of its parent. To ﬁnish the query, we just process white
nodes and subtrees. In 2(b), as d(u, v) ≤ curCRu + vpr the partition of v intersects the
ball (u, curCRu), so we recursively check children vl and vr. As vr’s partition does not
intersect the ball (u, curCRu), we discard vr and its partition. However, we continue
the checking on vl’s partition as it intersects the ball (u, curCRu).
discard small partitions, we process C in order of increasing radius. This agrees
with the intuition that the partition radius of u’s parent’s sibling is likely the
smallest of C, and that some of its descendants could be relevant to u.
Second stage: Completing the queries. As CD can be seen as a graph, we
use NHA ∪ CD to upper bound distances: when d(u, v) ≥ curCRu, we discard
objects w such that their shortest path dNHA∪CD(v, w) ≤ d(u, v) − curCRu. We
do this by adding them to C marked as EXTRACTED.

In this stage, if we have available space in CD, we cache all the computed distances 
small enough so as to get into their respective queues in NHA, since these
distances can be used in future symmetric queries. Note that adding distances
to CD without considering the space limitation could increase its size beyond
control, as it is shown by the following average case analysis. With probability
n−k
n , a random distance is greater than the k-th shortest one (thus, not stored),
n it is lower, then it is stored in CD using one cell. The base
and with probability k
case uses k cells for the ﬁrst distances. Then, the recurrence for the average case
of edge insertions for each NHAu is: T (n, k) = T (n − 1, k) + k
n, T (k, k) = k. We
obtain T (n, k) = k(Hn − Hk + 1) = O(k log n
k ). As we have n priority queues, if
we do not consider the limitation, we could use O(nk log n
k ) memory cells, which
can be an unpractical memory requirement.

Finally, we combine all of these ideas to complete the NNk(u) queries for all
nodes in U. We begin by creating the priority queue COH. Then, for each node
u picked from COH we do the following. We add the edges of NHAu to CDu,
where CDu refers to the adjacency list of u in CD. (Due to the size limitation it
is likely that some of the u’s current neighbors do not belong CDu.) Then, we
compute shortest paths from all u’s ancestors discarding as many objects as we
can. Then, we ﬁnish the query NNk(u), and ﬁnally delete CDu.
To ﬁnish the query NNk(u), we start adding all u’s ancestors to C. Later, we
take objects c from C in increasing cpr order, and process c according one of the
following rules:

R. Paredes et al.

92
1. If c was already marked as EXTRACTED, we add its children {cl, cr} to C;
2. If “U is ﬁxed” applies for c and u, and d(u, c) /∈ CD, we add {cl, cr} to C; or
3. If we have d(u, c) stored in CD, we retrieve it, else we compute it and use “d
is symmetric”. Then, if d(u, c) < curCRu + cpr, we have region intersection,
so we add {cl, cr} to C. Next, we use NHA ∪ CD as a graph computing
shortest paths from c to discard as many object as we can.

2.4 Pivot-Based Algorithm

Pivot-based algorithms have good performance in low dimensional spaces, but
worsen quickly as the dimension grows. However, our methodology compensates
this failure in medium and high dimensions. In this algorithm we use O(kn)
space in NHA and O(n log n) space to store the pivot index.

First stage: construction of the pivot index. We select at random a set of
pivots P = {p1, . . . , p|P|} ⊆ U, and store a table of |P|n distances d(pj, u), j ∈
{1, . . . ,|P|}, u ∈ U. We give the same space in bytes to the table as that of the
cache of distances and the division control tree of the recursive based algorithm.
Then, in our implementation we use |P| = 12 ln n + 2.5 = O(log n).
Solving NNk(u) queries with the pivot table. To perform a range-optimal
query for u we use C as an array to store maximum lower bounds of distances from
u to other objects. Because of the triangle inequality, for each v ∈ U and p ∈ P,
|d(v, p)−d(u, p)| is a lower bound of d(u, v). Let Cv = maxp∈P{|d(v, p)−d(u, p)|}.
So, we can discard non-relevant objects v such that Cv ≥ curCRu.
Then, we store C values in a priority queue SortC = {(v,Cv), v ∈ U − (P ∪
NHAu ∪ {u})}. For each object v picked from SortC by ascending Cv, we check
if Cv < curCRu. In such case, when “U is ﬁxed” applies for u and v we avoid the
distance computation and process the next node, else we compute the distance
duv = d(u, v). So, if duv < curCRu we add v to NHAu (this could reduce
curCRu). Also, using “ d is symmetric” we can reﬁne NHAv and consequently
update v in COH. Finally, we use NHA as a graph computing shortest path from
v to extract from SortC as many object as we can. Each NNk(u) query ﬁnishes
when we reach a node v such that Cv ≥ curCRu, or SortC gets empty.
Second stage: Completing the queries. Since pivots p ∈ P compute distances 
towards all objects, once we compute the table, they have already solved
their k-nearest neighbors. So, we only have to complete n − |P| range-optimal
queries for objects u ∈ U − P. Notice that because of the symmetry of d, these
objects already have candidates in their respective queues in NHA.

3 Experimental Results

We have tested our algorithms on synthetic and real-world metric spaces. The ﬁrst
synthetic set is formed by 65,536 points uniformly distributed in the metric space
([0, 1]D, L2) (the unitary real D-dimensional cube with Euclidean distance). This

Practical Construction of k-Nearest Neighbor Graphs

93

Table 1. KNNrp and KNNpiv least square ﬁttings for distance evaluations and CPU
time for all the metric spaces. CPU time measured in microseconds.

Space

[0, 1]4
[0, 1]8
[0, 1]12
[0, 1]16
[0, 1]20
[0, 1]24
[0, 1]D

Gaussian σ = 0.1
Gaussian σ = 0.2
Gaussian σ = 0.3

String

Document

KNNrp

Dist. evals.
10.0n1.32
32.8n1.38
15.1n1.59
5.06n1.77
2.32n1.88
1.34n1.96

74.7n1.33
7.82n1.71
2.97n1.85
21.4n1.54
0.425n1.95

KNNrp

CPU time
0.311n2.24
0.642n2.11
1.71n2.03
0.732n2.14
0.546n2.18
0.656n2.16

1.13n2.07
1.13n2.09
0.620n2.17
1.09n2.09
193n1.96

0.455e0.19D n1.65

0.571e0.01D n2.14

0.685e0.23D n1.48

0.858e0.11D n2.15

KNNpiv
Dist. evals.
56.1n1.09
168n1.06
116n1.27
12.1n1.64
2.48n1.87
1.23n1.96

1260n0.91
16.3n1.60
3.86n1.81
99.9n1.26
0.840n1.86

KNNpiv
CPU time
0.787n2.01
15.5n1.69
20.1n1.79
6.87n1.97
2.77n2.10
1.29n2.16

63.5n1.63
8.70n1.94
3.78n2.06
10.8n1.85
364n1.87

space allows us to measure the eﬀect of the space dimension D on our algorithms.
The second set is formed by 65,536 points in a 20-dimensional space with Gaussian
distribution forming 256 clusters randomly placed in ([0, 1]20, L2). We consider
three standard deviations to make more crisp or more fuzzy clusters (σ = 0.1, 0.2,
0.3). Of course, we have not used the fact that vectors have coordinates, but have
treated them as abstract objects.

The ﬁrst real-world set is the string metric space under the edit distance, a
discrete function that measures the minimum number of character insertions,
deletions and replacements needed to make the strings equal. We index a random 
subset of 65,536 words taken from an English dictionary. The second set
is the document space under the cosine distance, a function that measures the
angle between two documents when they are represented as vectors in a highdimensional 
vector model. We index a random subset of 1,215 English documents
taken from the TREC-3 collection.

Experiments were run on an Intel Pentium IV of 2 GHz and 512 MB of RAM.
We measure distance evaluations and CPU time. For shortness we have called
the basic knng construction algorithm KNNb, the recursive partition based
algorithm KNNrp, and the pivot based algorithm KNNpiv. We are not aware
of any published knng practical implementation for general metric spaces.

We summarize our experimental results in Fig. 3, where we show distance
computations per element, and Table 1 for the least square ﬁttings computed
with R [21]. The dependence on k turns out to be so mild that we neglect k in
the ﬁttings, thus, costs have the form cnα. Even though in Table 1 we explicit
the constant c, from now on, we only refer to the exponent α.

Figs. 3(a), 3(b) and 3(c) show experimental results for RD. Fig. 3(c) shows
that, as D grows, the performance of our algorithms degrade, phenomenon
known as the curse of dimensionality. For instance, for D = 4, KNNpiv uses
cn1.10 distance evaluations, but for D = 24, it is cn1.96 distance evaluations. Notice 
that a metric space with dimensionality D > 20 is considered as intractable
[8]. Fig. 3(a) shows that for all dimensions our algorithms are subquadratic in
distance evaluations, instead of KNNb which is always cn2. For low and medium

94

R. Paredes et al.

Vector space: Distance evaluations per element vs n, k = 16

Vector space: Distance evaluations per element vs k, n = 65536

t

l

n
e
m
e
e
 
r
e
p
 
s
n
o

i
t

l

a
u
a
v
e

 

e
c
n
a

t
s
D

i

t

n
e
m
e
e

l

 
r
e
p

 
s
n
o

i
t

l

a
u
a
v
e
e
c
n
a

 

t
s
D

i

t

n
e
m
e
e

l

 
r
e
p

 
s
n
o

i
t

l

a
u
a
v
e
e
c
n
a

 

t
s
D

i

KNNb
KNNrp,  D =   8
KNNrp,  D = 12
KNNrp,  D = 16
KNNrp,  D = 20
KNNpiv, D =   8
KNNpiv, D = 12
KNNpiv, D = 16
KNNpiv, D = 20

 25000

 20000

 15000

 10000

 5000

 60000

 50000

 40000

 30000

 20000

 10000

t

l

n
e
m
e
e
 
r
e
p
 
s
n
o

i
t

l

a
u
a
v
e

 

e
c
n
a

t
s
D

i

 0
 2048

 4096

 16384
 8192
database size n

 32768

 65536

(a) In RD, dependence on n.

 0

 2

 4

 8

 16

 32

 64

 128

#neighbors per element in the graph
(b) In RD, dependence on k.

Vector space: Distance evaluations per element vs D, n = 65536

Document space: Distance evaluations per element vs k

KNNb
KNNrp,  k =   2
KNNrp,  k =   8
KNNrp,  k = 32
KNNpiv, k =   2
KNNpiv, k =   8
KNNpiv, k = 32

 60000

 50000

 40000

 30000

 20000

 10000

 0

 4

 8

 12
 16
dimension D

 20

 24

t

n
e
m
e
e

l

 
r
e
p

 
s
n
o

i
t

l

a
u
a
v
e
e
c
n
a

 

t
s
D

i

 650

 600

 550

 500

 450

 400

 350

 300

KNNb,   n = 1215
KNNrp,  n = 1215
KNNpiv, n = 1215

 2

 4

 8

 16

 32

 64

 128

#neighbors per element in the graph

(c) In RD, dependence on D.

(d) In Document space, dependence on k.

Gauss space: Distance evaluations per element vs n, k = 16

Gauss space: Distance evaluations per element vs k, n = 65536

KNNb
KNNrp,  var = 0.01
KNNrp,  var = 0.04
KNNrp,  var = 0.09
KNNpiv, var = 0.01
KNNpiv, var = 0.04
KNNpiv, var = 0.09

 35000

 30000

 25000

 20000

 15000

 10000

 5000

 0
 2048

 4096

 8192
 16384
database size n

 32768

 65536

t

n
e
m
e
e

l

 
r
e
p

 
s
n
o

i
t

l

a
u
a
v
e
e
c
n
a

 

t
s
D

i

 45000

 40000

 35000

 30000

 25000

 20000

 15000

 10000

 5000

 0

 2

 4

 8

 16

 32

 64

 128

#neighbors per element in the graph

(e) In Gaussian space, dependence on n.

(f) In Gaussian space, dependence on k.

String space: Distance evaluations per element vs n

String space: Distance evaluations per element vs k

l

t
n
e
m
e
e
 
r
e
p
 
s
n
o
i
t
a
u
a
v
e
 
e
c
n
a

l

t
s
D

i

 10000
 9000
 8000
 7000
 6000
 5000
 4000
 3000
 2000
 1000
 0
 2048

KNNb
KNNrp,  k =   2
KNNrp,  k =   8
KNNrp,  k = 32
KNNpiv, k =   2
KNNpiv, k =   8
KNNpiv, k = 32

l

t
n
e
m
e
e
 
r
e
p
 
s
n
o
i
t
a
u
a
v
e
 
e
c
n
a

l

 4096

 16384
 8192
database size n

 32768

 65536

t
s
D

i

 14000

 12000

 10000

 8000

 6000

 4000

 2000

 0

 2

KNNrp,  n = 32768
KNNrp,  n = 65536
KNNpiv, n = 32768
KNNpiv, n = 65536

 4

 8

 16

 32

 64

 128

#neighbors per element in the graph

(g) In String space, dependence on n.

(h) In String space, dependence on k.

Fig. 3. Distance evaluations per node during knng construction. Fig. 3(b)/3(f) follows
the legend of Fig. 3(a)/3(e).

Practical Construction of k-Nearest Neighbor Graphs

95
dimensions (D ≤ 16) ours have better performance than KNNb, being KNNpiv
the best of ours. Moreover, for lower dimensions (D ≤ 8) ours are only slightly
superlinear. Fig. 3(b) shows a sublinear dependence on k for all dimensions,
however, KNNpiv is more sensitive to k than KNNrp. Also, the dependence on
k diminishes as long as D grows, although it is always monotonically increasing
on k. Finally, it is shown that for k ≤ 4, our algorithms behave better than
KNNb, even in high dimensional spaces (KNNpiv in D = 20).

Figs. 3(e) and 3(f) show results in Gaussian space. For crisp clusters (σ = 0.1)
the performance of our algorithms improves signiﬁcantly, even for high values of
k. It is interesting to note that for k ≤ 8 our algorithms are more eﬃcient than
KNNb for the three variances. Again, KNNpiv has the best performance.
Figs. 3(g) and 3(h) show results for strings. The plots show that both KNNrp
and KNNpiv are subquadratic for all k ∈ [2, 128]. For instance, for n = 65, 536,
KNNrp costs 28%, and KNNpiv just 8%, of KNNb to build the 32nng.

Finally, Fig. 3(d) shows that our methodology save lots of work in the highdimensional 
document space. For instance, for n = 1, 215, KNNrp costs 63%,
and KNNpiv costs 67%, of KNNb to build the 8nng. These two last results
show that our methodology is also practical in real-world situations.
All of these conclusions are conﬁrmed in Table 1. We remark that in some
practical conditions (vectors in [0, 1]D with D ≤ 8 and k ≤ 32 and Gaussian
vectors with σ = 0.01 and k ≤ 8), KNNpiv also has better performance than
KNNb in CPU time. This is important since the Euclidean distance is very cheap
to compute. Note that KNNrp and KNNpiv turn out to be clearly subquadratic
on distances evaluations when considering the exponential dependence on D.

Note that in the metric space context, superquadratic CPU time in side computations 
is not as important as a subquadratic number of computed distances.
In fact, in the document space, KNNrp and KNNpiv perform better in CPU
time that KNNb, showing that in practice the leading complexity (computing
distances) is several orders of magnitude larger than other side computations
such as traversing pointers or scanning the pivot table.

4 Conclusions

We have presented a general methodology to construct the k-nearest neighbor
graph (knng) in general metric spaces. Based on our methodology we give two
algorithms. The ﬁrst is based on a recursive partitioning of the space (KNNrp),
and the second on the classic pivot technique (KNNpiv). Our methodology considers 
two stages: the ﬁrst indexes the space, and the second completes the knng
using the index and some metric and graph optimizations.
Experimental results conﬁrm the practical eﬃciency of our approach in vectorial 
metric spaces of wide dimensional spectrum (D ≤ 20), and real-world
metric spaces. For instance, in the string space, our algorithms achieve empirical 
CPU time of the form ctn1.85, and cdn1.26 in distance computations; and
in the high-dimensional document space, they reach empirical cn1.87 both in
distance computations and CPU time. In low dimensional metric spaces, our
algorithms behave even better. KNNpiv is in general better than KNNrp for

96

R. Paredes et al.

small and moderate k values, yet KNNrp is less sensitive to larger k values or
higher dimensional spaces.

Future work involves developing another knng constructing algorithm based
on the list of clusters [7] so that we can also obtain good performance in higher dimensional 
metric spaces. We are also researching how to enhance the data structure 
to allow dynamic insertions/deletions in reasonable time, so as to maintain
an up-to-date set of k-nearest neighbors for each element in the database.

Acknowledgement. We wish to thank Georges Dupret and Marco Patella for
their valuable comments.

References

1. S. Arya, D. Mount, N. Netanyahu, R. Silverman, and A. Wu. An optimal algorithm
for approximate nearest neighbor searching in ﬁxed dimension. In Proc. SODA’94,
pages 573–583, 1994.

2. F. Aurenhammer. Voronoi diagrams – a survey of a fundamental geometric data

structure. ACM Computing Surveys, 23(3), 1991.

3. R. Baeza-Yates, C. Hurtado, and M. Mendoza. Query clustering for boosting web

page ranking. In Proc. AWIC’04, LNCS 3034, pages 164–175, 2004.

4. M. Brito, E. Ch´avez, A. Quiroz, and J. Yukich. Connectivity of the mutual knearest 
neighbor graph in clustering and outlier detection. Statistics & Probability
Letters, 35:33–42, 1996.

5. P. Callahan. Optimal parallel all-nearest-neighbors using the well-separated pair

decomposition. In Proc. FOCS’93, pages 332–340, 1993.

6. P. Callahan and R. Kosaraju. A decomposition of multidimensional point sets with
applications to k nearest neighbors and n body potential ﬁelds. JACM, 42(1):67–
90, 1995.

7. E. Ch´avez and G. Navarro. A compact space decomposition for eﬀective metric

indexing. Pattern Recognition Letters, 26(9):1363–1376, 2005.

8. E. Ch´avez, G. Navarro, R. Baeza-Yates, and J.L. Marroquin. Searching in metric

spaces. ACM Computing Surveys, 33(3):273–321, September 2001.

9. K. Clarkson. Fast algorithms for the all-nearest-neighbors problem.

In Proc.

FOCS’83, pages 226–232, 1983.

10. K. Clarkson. Nearest neighbor queries in metric spaces. Discrete Computational

Geometry, 22(1):63–93, 1999.

11. M. Dickerson and D. Eppstein. Algorithms for proximity problems in higher dimensions.
 Computational Geometry Theory and Applications, 5:277–291, 1996.
12. R. Duda and P. Hart. Pattern Classiﬁcation and Scene Analysis. Wiley, 1973.
13. H. Edelsbrunner. Algorithms in Combinatorial Geometry. Springer-Verlag, 1987.
14. D. Eppstein and J. Erickson. Iterated nearest neighbors and ﬁnding minimal polytopes.
 Discrete & Computational Geometry, 11:321–350, 1994.

15. K. Figueroa. An eﬃcient algorithm to all k nearest neighbor problem in metric

spaces. Master’s thesis, Universidad Michoacana, Mexico, 2000. In Spanish.

16. G. Hjaltason and H. Samet. Incremental similarity search in multimedia databases.

Technical Report TR 4199, Dept. of Comp. Sci. Univ. of Maryland, Nov 2000.

17. I. Kalantari and G. McDonald. A data structure and an algorithm for the nearest

point problem. IEEE Trans. Software Eng., 9(5):631–634, 1983.

Practical Construction of k-Nearest Neighbor Graphs

97

18. D. R. Karger and M. Ruhl. Finding nearest neighbors in growth-restricted metrics.

In Proc. STOC’02, pages 741–750, 2002.

19. R. Krauthgamer and J. Lee. Navigating nets: simple algorithms for proximity

search. In Proc. SODA’04, pages 798–807, 2004.

20. R. Paredes and E. Ch´avez. Using the k-nearest neighbor graph for proximity
searching in metric spaces. In Proc. SPIRE’05, LNCS 3772, pages 127–138, 2005.
21. R Development Core Team. R: A language and environment for statistical computing.
 R Foundation for Statistical Computing, Vienna, Austria, 2004.

22. P. Vaidya. An O(n log n) algorithm for the all-nearest-neighbor problem. Discrete

& Computational Geometry, 4:101–115, 1989.

