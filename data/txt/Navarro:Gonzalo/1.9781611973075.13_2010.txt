Fully-Functional Succinct Trees

Kunihiko Sadakane∗

Gonzalo Navarro†

Abstract
We propose new succinct representations of ordinal trees,
which have been studied extensively. It is known that any
n-node static tree can be represented in 2n + o(n) bits
and a large number of operations on the tree can be supported 
in constant time under the word-RAM model. However 
existing data structures are not satisfactory in both
theory and practice because (1) the lower-order term is
Ω(n log log n/ log n), which cannot be neglected in practice,
(2) the hidden constant is also large, (3) the data structures
are complicated and diﬃcult to implement, and (4) the techniques 
do not extend to dynamic trees supporting insertions
and deletions of nodes.

We propose a simple and ﬂexible data structure, called
the range min-max tree, that reduces the large number
of relevant tree operations considered in the literature to
a few primitives, which are carried out in constant time
on suﬃciently small trees. The result is then extended
to trees of arbitrary size, achieving 2n + O(n/polylog(n))
bits of space. The redundancy is signiﬁcantly lower than
in any previous proposal, and the data structure is easily
implemented. Furthermore, using the same framework, we
derive the ﬁrst fully-functional dynamic succinct trees.

1 Introduction
Trees are one of the most fundamental data structures,
needless to say. A classical representation of a tree with
n nodes uses O(n) pointers or words. Because each
pointer must distinguish all the nodes, it requires log n
bits1 in the worst case. Therefore the tree occupies
Θ(n log n) bits, which causes a space problem for manipulating 
large trees. Much research has been devoted
to reducing the space to represent static trees [20, 26,
27, 29, 16, 17, 5, 10, 7, 8, 22, 19, 2, 18, 35, 21, 9] and
dynamic trees [28, 34, 6, 1], achieving so-called succinct
data structures for trees.

A succinct data structure stores objects using space
close to the information-theoretic lower bound, while simultaneously 
supporting a number of primitive operations 
on the objects in constant time. The informationtheoretic 
lower bound for storing an object from a universe 
with cardinality L is log L bits because in the worst

∗

†

National Institute of Informatics (NII), 2-1-2 Hitotsubashi,
Chiyoda-ku, Tokyo 101-8430, Japan. sada@nii.ac.jp. Work supported 
in part by the Grant-in-Aid of the Ministry of Education,
Science, Sports and Culture of Japan.

Department of Computer Science, University of Chile.
gnavarro@dcc.uchile.cl. Funded in part by Millennium Institute 
for Cell Dynamics and Biotechnology (ICDB), Grant ICM
P05-001-F, Mideplan, Chile.

1The base of logarithm is 2 throughout this paper.

case this number of bits is necessary to distinguish any
two objects. The size of the corresponding succinct data
structure is typically (1 + o(1)) log L bits.

3

(cid:2)
(cid:3)
2n−1
n−1

/(2n − 1) = 22n/Θ(n

In this paper we are interested in ordinal trees,
in which the children of a node are ordered. The
information-theoretic lower bound to store an ordinal
tree with n nodes is 2n − Θ(log n) bits because there
exist
2 ) such trees [26].
We assume that the computation model is the word
RAM with word length Θ(log n) in which arithmetic
and logical operations on Θ(log n)-bit integers and
Θ(log n)-bit memory accesses can be done in constant
time. Under this model, there exist many succinct
representations of ordinal trees achieving 2n + o(n) bits
of space.

Basically there exist three types of such tree
representations:
the balanced parentheses sequence
(BP) [20, 26], the level-order unary degree sequence
(LOUDS) [20, 8], and the depth-ﬁrst unary degree sequence 
(DFUDS) [5, 21]. An example of them is shown
in Figure 1. LOUDS is a simple representation, but it
lacks many basic operations, such as giving the subtree
size of a tree node. Both BP and DFUDS build on a
sequence of balanced parentheses, the former using the
intuitive depth-ﬁrst-search representation and the latter 
using a more sophisticated one. The advantage of
DFUDS, when it was created, was that it supported a
more complete set of operations in constant time, most
notably going to the i-th child of a node. Later, this
was also achieved using BP representation, yet requiring 
complicated additional data structures with nonnegligible 
lower-order terms in their space usage [22].
Another type of succinct ordinal trees, based on tree
covering [17, 19, 9], has also achieved constant-time support 
of known operations, yet inheriting the problem of
nonnegligible lower-order terms in size.

1.1 Our contributions
We focus on the BP representation, and achieve constant 
time for a large set of operations2. What distinguishes 
our proposal is its simplicity, which allows

2Moreover, as we manipulate a sequence of balanced parentheses,
 our data structure can be used to implement a DFUDS
representation as well.

134Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpOrdinal tree

2

4

3

1

2

1

6

5

7

8

6

BP

DFUDS

LOUDS

3

4

5

7

8

((()()())(()()))

2

1

((()((())))(()))
7 8
110111011000000
7 8
1

3 4 5

3 4 5

6

2

6

Figure 1: Succinct representations of trees.

easy implementation and derivation of dynamic variants 
with the same functionality; and its economy in
the sublinear-space structures, which results in a considerably 
smaller lower-order term in the space usage.
For the static case, we obtain the following result.

Theorem 1.1. For any ordinal tree with n nodes, all
operations in Table 1 except insert and delete are carried
out in constant time O(c2) with a data structure using
2n + O(n/ logc n) bits of space on a Θ(log n)-bit word
RAM, for any constant c >0. The data structure can
be constructed from the balanced parentheses sequence
of the tree, in O(n) time using O(n) bits.

Our data structure improves upon the lower-order
term in the space complexity of previous representa-
√
tions. For example, formerly the extra data structure
for level-ancestor has required O(n log log n/
log n)
bits [29], or O(n(log log n)2/ log n) bits3 [21], and
that for child has required O(n/(log log n)2) bits [22].
The previous representation with maximum functionality 
[9] supports all the operations in Table 1, except 
insert and delete,
in constant time using 2n +
O(n log log log n/ log log n)-bit space. Ours requires
O(n/ logc n) bits for all the operations.

For the dynamic case, the following theorem summarizes 
our results.

Theorem 1.2. On a Θ(log n)-bit word RAM, all operations 
on a dynamic ordinal tree with n nodes can
be carried out within the worst-case complexities given
in Table 1, using a data structure that requires 2n +
O(n log log n/ log n) bits. Alternatively, they can be carried 
out in O(log n) time using 2n + O(n/ log n) bits of
space.

There exist no previous dynamic data structures
supporting all the operations in Table 1. The data
structure of Raman and Rao [34] supports, for binary
trees, parent, left and right child, and subtree-size of
the current node in the course of traversing the tree in
constant time, and updates in O((log log n)1+) time.
Note that this data structure assumes that all traversals 
start from the root. Chan et al. [6] gave a dynamic
data structure using O(n) bits and supporting ﬁndclose 
and enclose, and updates, in O(log n/ log log n)
time. They show this time is indeed optimal, by reduction 
from dynamic rank/select on bitmaps and given
the lower bound of Fredman and Saks [14]. They also
gave another data structure using O(n) bits and supporting 
ﬁndclose, enclose, lca, leaf-rank, leaf-select, and
updates, in O(log n) time.

The simplicity and space-eﬃciency of our data
structures stem from the fact that any query operation
in Table 1 is reduced to a few basic operations on a
bit vector, which can be eﬃciently solved by a range
min-max tree. This approach is diﬀerent from previous
studies in which each operation needs distinct auxiliary
data structures. Therefore their total space is the
summation over all the data structures, which enlarges
the hidden constant in the lower-order term of the size.
For example, the ﬁrst succinct representation of BP [26]
supported only ﬁndclose, ﬁndopen, and enclose (and
other easy operations) and each operation used diﬀerent
data structures. Later, many further operations such
as lmost-leaf
[27], lca [35], degree [7], child and childrank 
[22], and level-ancestor [29], were added to this
representation by using other types of data structures
for each. There exists another elegant data structure
for BP supporting ﬁndclose, ﬁndopen, and enclose [16].
This reduces the size of the data structure for these
basic operations, but still has to add extra auxiliary
data structures for other operations.

Former static approaches use two-level data structures 
to reduce the size, which causes diﬃculties in dynamic 
case. Our approach using the range min-max
tree, instead, is easily translated to the dynamic setting,
resulting in simple and eﬃcient dynamic data structures
that support all of the operations in Table 1.

3This data structure is for DFUDS, but the same technique

can be also applied to BP.

1.2 Organization of the paper
In Section 2 we review basic data structures used in

135Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpTable 1: Operations supported by our data structure. The time complexities are for the dynamic case; in the
static case all operations are performed in constant time. The ﬁrst group is composed of basic operations, used
to implement the others, yet they could have other uses.
operation
inspect(i)
ﬁndclose(i)/ﬁndopen(i)
enclose(i)
rank((i)/rank)(i)
select((i)/select)(i)
rmqi(i, j)/RMQi(i, j)
pre-rank(i)/post-rank(i)
pre-select(i)/post-select(i)
isleaf(i)
isancestor(i, j)
depth(i)
parent(i)
ﬁrst-child(i)/last-child(i)
next-sibling(i)/prev-sibling(i)
subtree-size(i)
level-ancestor(i, d)

description
time complexity
O(log n/ log log n)
P [i]
O(log n/ log log n)
position of parenthesis matching P [i]
position of tightest open parent. enclosing node i O(log n/ log log n)
O(log n/ log log n)
number of open/close parentheses in P [1, i]
O(log n/ log log n)
position of i-th open/close parenthesis
O(log n/ log log n)
position of min/max excess value in range [i, j]
O(log n/ log log n)
preorder/postorder rank of node i
O(log n/ log log n)
the node with preorder/postorder i
O(log n/ log log n)
whether P [i] is a leaf
O(log n/ log log n)
whether i is an ancestor of j
O(log n/ log log n)
depth of node i
O(log n/ log log n)
parent of node i
O(log n/ log log n)
ﬁrst/last child of node i
O(log n/ log log n)
next/previous sibling of node i
O(log n/ log log n)
number of nodes in the subtree of node i
O(log n) or
ancestor j of i such that depth(j) =depth (i) − d
O(d + log n/ log log n)
O(log n/ log log n)
O(log n) or
O(d + log n/ log log n)
O(log n/ log log n)
O(log n/ log log n)
O(log n/ log log n)
O(log n/ log log n)
O(log n/ log log n)
O(log n/ log log n)
O(log n/ log log n)
O(log n/ log log n)
O(log n/ log log n)
O(log n/ log log n)
O(log n/ log log n)
O(log n/ log log n)

the lowest common ancestor of two nodes i, j
the (ﬁrst) deepest node in the subtree of i
number of children of node i
q-th child of node i
number of siblings to the left of node i
inorder of node i
node with inorder i
number of leaves to the left of leaf i
i-th leaf
leftmost/rightmost leaf of node i
insert node given by matching parent. at i and j
delete node i

level-next(i)/level-prev(i)
level-lmost(d)/level-rmost(d)

lca(i, j)
deepest-node(i)
degree(i)
child(i, q)
child-rank(i)
in-rank(i)
in-select(i)
leaf-rank(i)
leaf-select(i)
lmost-leaf(i)/rmost-leaf(i)
insert(i, j)
delete(i)

next/previous node of i in BFS order
leftmost/rightmost node with depth d

this paper. In Section 3 we describe the main ideas for
our new data structures for ordinal trees. Sections 4
and 5 describe the static construction.
In Sections 6
and 7 we give two data structures for dynamic ordinal
trees.
In Section 8 we conclude and give future work
directions.

2 Preliminaries
Here we describe the balanced parentheses sequence and
basic data structures used in this paper.

2.1 Succinct data structures for rank/select
Consider a bit string S[0, n − 1] of length n. We
rankc(S, i)
deﬁne rank and select
is the number of occurrences c ∈ {0, 1} in S[0, i],
and selectc(S, i) is the position of the i-th occurrence
of c in S. Note that rankc(S, selectc(S, i)) = i and

for S as follows:

selectc(S, rankc(S, i)) ≤ i.

There exist many succinct data structures for
rank/select [20, 25, 33]. A basic one uses n + o(n)
bits and supports rank/select in constant time on the
word RAM with word length O(log n). The space can
be reduced if the number of 1’s is small. For a string
with m 1’s, there exists a data structure for constant-
+ O(n log log n/ log n) =
time rank/select using log
m + O(m + n log log n/ log n) bits [33]. Recently
m log n
[30] the extra space has been reduced to m log n
m +
O(n tt/ logt n + n3/4) bits, performing rank and select
in O(t) time. This can be built in linear worst-case
time4.

n
m

(cid:2)

(cid:3)

4They use a predecessor structure by Pˇatra¸scu and Thorup
”, which

[31], more precisely their result achieving time “lg
is a simple modiﬁcation of van Emde Boas’ data structure.

(cid:2)−lg n

a

136Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpA crucial technique for succinct data structures is
table lookup. For small-size problems we construct a
table which stores answers for all possible sequences
for rank and select, we
and queries. For example,
√
use a table storing all answers for all 0,1 patterns of
length 1
n
diﬀerent patterns, we can store all answers in a universal
√
table (i.e., not depending on the bit sequence) that uses
n · polylog(n) = o(n/polylog(n)) bits, which can be
accessed in constant time on a word RAM with word
length Θ(log n).

2 log n. Because there exist only 2 1

2 log n =

2.2 Succinct tree representations
We will focus on the BP representation of trees. A
rooted ordered tree T , or ordinal tree, with n nodes
is represented by a string P [0, 2n − 1] of balanced
parentheses of length 2n. A node v ∈ T is represented
by a pair of matching parentheses ( . . . ) and all subtrees
rooted at the node are encoded in order between the
matching parentheses (see Figure 1 for an example).
Moreover, node v is identiﬁed with the position i of the
open parenthesis P [i] representing the node.

In the static setting, the tree does not change. In
the dynamic setting, we consider insertion and deletion
of internal nodes or leaves. More precisely, we accept
inserting a new pair of matching parentheses at any legal
position of P , as well as deleting any existing pair of
matching parentheses.

3 Fundamental concepts
In this section we give the basic ideas of our ordinal tree
representation. In the next sections we build on these
to deﬁne our static and dynamic representations.
We represent a possibly non-balanced5 parentheses
sequence by a 0,1 vector P [0, n−1] (P [i] ∈ {0, 1}). Each
opening/closing parenthesis is encoded by ( = 1, ) = 0.
First, we remind the reader that several operations
of Table 1 either are trivial in a BP representation, or
are easily solved using enclose, ﬁndclose, ﬁndopen, rank,
and select [26]. These are:

inspect(i) = rank1(P, i) − rank1(P, i − 1)

(if accessing P [i] is problematic)

isleaf(i) = [inspect(i + 1) = 0]

isancestor(i, j) = i ≤ j and

f indclose(P, j) ≤ f indclose(P, i)

depth(i) = rank1(P, i) − rank0(P, i)
parent(i) = enclose(P, i)

pre-rank(i) = rank1(P, i)

5Later we will use these constructions to represent arbitrary

chunks of a balanced sequence.

pre-select(i) = select1(P, i)
post-rank(i) = rank0(P, i)
post-select(i) = select0(P, i)
ﬁrst-child(i) = i + 1 (if i is not a leaf)
last-child(i) = ﬁndopen(P, ﬁndclose(P, i) − 1)

(if i is not a leaf)
next-sibling(i) = f indclose(i) + 1

(if P [f indclose(i) + 1] = 0,
then i is the last sibling)

prev-sibling(i) = f indopen(i − 1) (if P [i − 1] = 1

then i is the ﬁrst sibling)
subtree-size(i) = (f indclose(i) − i + 1)/2

Hence the above operations will not be considered
further in the paper. Let us now focus on a small set
of primitives needed to implement most of the other
operations. For any function g(·) on {0, 1}, we deﬁne
the following.
Definition 1. For a 0,1 vector P [0, n − 1] and a
function g(·) on {0, 1},

sum(P, g, i, j)

def=

j(cid:4)

fwd-search(P, g, i, d)

bwd-search(P, g, i, d)

rmq(P, g, i, j)

rmqi(P, g, i, j)

RMQ(P, g, i, j)

RMQi(P, g, i, j)

g(P [k])
{j | sum(P, g, i, j) =d}
{j | sum(P, g, j, i) = d}
{sum(P, g, i, k)}
{sum(P, g, i, k)}
{sum(P, g, i, k)}
{sum(P, g, i, k)}

k=i
def= min
j≥i
def= max
j≤i
def= min
i≤k≤j

def= argmin
i≤k≤j
def= max
i≤k≤j

def= argmax
i≤k≤j

The following function is particularly important.

Definition 2. Let π be the function such that π(1) =
1, π(0) = −1. Given P [0, n − 1], we deﬁne the excess
array E[0, n − 1] of P as an integer array such that
E[i] = sum(P, π, 0, i).

Note that E[i] stores the diﬀerence between the
number of opening and closing parentheses in P [0, i].
When P [i] is an opening parenthesis, E[i] = depth(i) is
the depth of the corresponding node, and is the depth
minus 1 for closing parentheses. We will use E as
a conceptual device in our discussions, it will not be

137Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpstored. Note that, given the form of π, it holds that
|E[i + 1]− E[i]| = 1 for all i.

The above operations are suﬃcient to implement
the basic navigation on parentheses, as the next lemma
shows. Note that the equation for ﬁndclose is well
known, and the one for level-ancestor has appeared as
well [29], but we give proofs for completeness.
Lemma 3.1. Let P be a BP sequence encoded by {0, 1}.
Then ﬁndclose, ﬁndopen, enclose, and level-ancestor
can be expressed as follows.

ﬁndclose(i) =fwd-search
(P, π, i, 0)
ﬁndopen(i) =bwd-search (P, π, i, 0)
enclose(i) =bwd-search (P, π, i, 2)

level-ancestor(i, d) =bwd-search (P, π, i, d + 1)

Proof. For ﬁndclose, let j > i be the position of the
closing parenthesis matching the opening parenthesis at
P [i]. Then j is the smallest index > i such that E[j] =
E[i]−1 = E[i−1] (because of the node depths). Since by
deﬁnition E[k] =E [i− 1] + sum(P, π, i, k) for any k > i,
j is the smallest index > i such that sum(P, π, i, j) = 0.
This is, by deﬁnition, fwd-search(P, π, i, 0).

For ﬁndopen, let j < i be the position of the opening
parenthesis matching the closing parenthesis at P [i].
Then j is the largest index < i such that E[j−1] = E[i]
(again, because of the node depths)6. Since by deﬁnition
E[k − 1] = E[i] − sum(P, π, k, i) for any k < i, j is the
largest index < i such that sum(P, π, j, i) = 0. This is
bwd-search(P, π, i, 0).

For enclose, let j < i be the position of the opening 
parenthesis that most tightly encloses the opening
parenthesis at P [i]. Then j is the largest index < i such
that E[j − 1] = E[i]− 2 (note that now P [i] is an opening 
parenthesis). Now we reason as for ﬁndopen to get
sum(P, π, j, i) = 2.

Finally, the proof for level-ancestor is similar to
that for enclose. Now j is the largest index < i such
that E[j − 1] = E[i] − d − 1, which is equivalent to
(cid:5)(cid:6)
sum(P, π, j, i) = d + 1.

We also have the following, easy or well-known,

equalities:

lca(i, j) =

⎧⎨
⎩

i (if isancestor(i, j))
j (if isancestor(j, i))
parent(rmqi(P, π, i, j) + 1)

deepest-node(i) = RMQi(P, π, i, ﬁndclose(i))

level-next(i) = fwd-search(P, π, ﬁndclose(i), 0)
6Note E[j] − 1 = E[i] could hold at incorrect places, where

P [j] is a closing parenthesis.

level-prev(i) = ﬁndopen(bwd-search(P, π, i, 0))

level-lmost(d) = fwd-search(P, π, 0, d)
level-rmost(d) = ﬁndopen(bwd-search(P, π, n−1,−d))

To compute degree, child, and child-rank, the following 
lemma is important.

Lemma 3.2. The number of children of node i is equal
to the number of occurrences of the minimum value in
E[i + 1, ﬁndclose(i) − 1].

Proof. Let d = E[i] =depth (i) and j = ﬁndclose(i).
Then E[j] =d − 1 and all excess values in E[i + 1, j − 1]
are ≥ d. Therefore, as |E[r + 1] − E[r]| = 1 for all r,
the minimum value in E[i + 1, j − 1] is d. Moreover,
for the range [ik, jk] corresponding to the k-th child of
i, E[ik] =d + 1, E[jk] =d, and all the values between
them are > d. Therefore the number of occurrences of
d, which is the minimum value in E[i+1, j −1], is equal
(cid:5)(cid:6)
to the number of children of i.

We also show that the above functions unify the
algorithms for computing rank/select on 0,1 vectors and
those for balanced parenthesis sequences. Namely, let
φ, ψ be functions such that φ(0) = 0, φ(1) = 1, ψ(0) =
1, ψ(1) = 0. Then the following equalities hold.

Lemma 3.3. For a 0,1 vector P ,

rank1(P, i) = sum(P, φ, 0, i)
select1(P, i) = fwd-search(P, φ, 0, i)
rank0(P, i) = sum(P, ψ, 0, i)
select0(P, i) = fwd-search(P, ψ, 0, i)

Therefore, in principle we must focus only on the
following set of primitives: fwd-search, bwd-search, sum,
rmqi, RMQi, degree, child, and child-rank. The few
remaining operations will be handled later.

Our data structure for queries on a 0,1 vector P is
basically a search tree in which each leaf corresponds to
a range of P , and each node stores the last, maximum,
and minimum values, within its subtree, of preﬁx sums
of P .

Definition 3. A range min-max tree for a vector
P [0, n − 1] and a function g(·) is deﬁned as follows.
Let [(cid:5)1..r1], [(cid:5)2..r2], . . . , [(cid:5)q..rq] be a partition of [0..n−1]
where (cid:5)1 = 0, ri + 1 = (cid:5)i+1, rq = n − 1. Then the i-th
leftmost leaf of the tree stores the sub-vector P [(cid:5)i, ri],
as well as e[i] =sum (P, g, 0, ri), m[i] = e[i − 1] +
rmq(P, g, (cid:5)i, ri) and M[i] = e[i − 1] + RMQ(P, g, (cid:5)i, ri).
Each internal node u stores in e[u]/m[u]/M[u] the
last/minimum/maximum of the e/m/M values stored

138Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpa
0/4

f
1/3
h

b
1/4
d

j
0/2
c
l
1/2 2/4 3/4 2/3 1/3 2/3 1/2 0/0
1212343432321232321210
(()((()())())(()())())

g

e

k

i

m/M
E
P

Figure 2: An example of the range min-max tree using
function π, and showing the m/M values.

in its child nodes. Thus, the root node stores e =
sum(P, g, 0, n − 1), m = rmq(P, g, 0, n − 1) and M =
RMQ(P, g, 0, n − 1).
Example 1. An example of range min-max tree is
shown in Figure 2. Here we use g = π, and thus the
nodes store the minimum/maximum values of array E
in the corresponding interval.

4 A simple data structure for moderate-size

trees

Building on the previous ideas, we give a simple data
structure to compute fwd-search, bwd-search, and sum
in constant time for arrays of moderate size. Then we
will consider further operations.
Let g(·) be a function on {0, 1} taking values in
{1, 0,−1}. We call such a function ±1 function. Note
that there exist only six such functions where g(0) (cid:8)=
g(1), which are indeed φ,−φ, ψ,−ψ, π,−π.
Let w be the bit length of the machine word in the
RAM model, and c ≥ 1 any constant. We have a (not
necessarily balanced) parentheses vector P [0, n − 1], of
moderate size n ≤ N = wc. Assume we wish to solve
the operations for an arbitrary ±1 function g(·), and let
G[i] denote sum(P, g, 0, i), analogously to E[i] for g = π.
Our data structure is a range min-max tree TmM for
vector P and function g(·). Let s = 1
2 w. We imaginarily
divide vector P into (cid:9)n/s(cid:10) blocks of length s. These
form the partition alluded in Deﬁnition 3: (cid:5)i = s·(i−1).
Thus the values m[i] and M[i] correspond to minima
and maxima of G within each block, and e[i] = G[ri].

Furthermore, the tree will be k-ary and complete,
for k = Θ(w/ log w). Because it is complete, the tree can
(cid:4)[0,O(n/s)],
be represented just by three integer arrays e
(cid:4)[0,O(n/s)], like a heap.
(cid:4)[0,O(n/s)], and M
m
Because −wc ≤ e
(cid:4)[i] ≤ wc for any i,
(cid:4)[i], m
(cid:4) and M
(cid:4) occupy at most (i.e., for k = 2)

(cid:4)[i], M

arrays e

(cid:4), m

s · (cid:9)log(2wc + 1)(cid:10) = O(nc log w/w) bits each. The
2 n
depth of the tree is (cid:9)logk(n/s)(cid:10) = O(c).

The following fact is well known; we reprove it for

completeness.
Lemma 4.1. Any range [i..j] ⊆ [0..n − 1] in TmM can
be represented by a disjoint union of O(ck) subranges
where the leftmost and rightmost ones may be subranges
of leaves of TmM , and the others correspond to whole
tree nodes.
Proof. Let a be the smallest value such that i ≤
ra and b be the largest such that j ≥ (cid:5)b. Then
the range [i..j]
is covered by the partition [i..j] =
[i..ra][(cid:5)a+1..ra+1] . . . [(cid:5)b..j] (we can discard the special
case a = b, as in this case we have already one leaf
covering [i..j]). Then [i..ra] and [(cid:5)b..j] are the leftmost
and rightmost leaf subranges alluded in the lemma; all
the others are whole tree nodes.
It remains to show that we can reexpress this
partition using O(ck) tree nodes. If all the k children
of a node are in the range, we replace the k children
by the parent node, and continue recursively level by
level. Note that if two parent nodes are created in a
given level, then all the other intermediate nodes of
the same level must be created as well, because the
original/created nodes form a range at any level. At
the end, there cannot be more than 2k − 2 nodes at any
level, because otherwise k of them would share a single
parent and would have been replaced. As there are c
levels, the obtained set of nodes covering [i..j] is of size
(cid:5)(cid:6)
O(ck).

Example 2. In Figure 2 (where s = k = 3), the range
[3..18] is covered by [3..5], [6..8], [9..17], [18..18]. They
correspond to nodes d, e, f, and a part of
leaf k,
respectively.

Computing fwd-search(P, g, i, d) is done as follows
(bwd-search is symmetric). First we check if the block
of i, [(cid:5)k, rk] for k = (cid:12)i/s(cid:13), contains fwd-search(P, g, i, d)
√
with table lookup using vector P , by precomputing a
simple universal table of 2s·2s2·log s = O(
2ww2 log w)
bits. If so, we are done. Else, we compute the global
(cid:4) = G[i − 1] + d = e[k] −
target value we seek, d
sum(P, g, i, rk) + d (again, the sum inside the block is
done in constant time using table lookup). Now we
divide the range [rk + 1, n − 1] into subranges I1, I2, . . .
represented by range min-max tree nodes u1, u2, . . . as
in Lemma 4.1. Then, for each Ij, we check if the target
(cid:4) is between m[uj] and M[uj], the minimum and
value d
maximum values of subrange Ij. Let Ik be the ﬁrst j
(cid:4) ≤ M[uj], then fwd-search(P, g, i, d)
such that m[uj] ≤ d
lies within Ik. If Ik corresponds to an internal tree node,

139Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpwe iteratively ﬁnd the leftmost child of the node whose
(cid:4), until we reach a leaf. Finally, we ﬁnd
range contains d
the target in the block corresponding to the leaf by table
lookup, using P again.

Example 3. In Figure 2, where G = E and g = π,
computing ﬁndclose(3) = fwd-search(P, π, 3, 0) = 12 can
be done as follows. Note this is equivalent to ﬁnding the
ﬁrst j > 3 such that E[i] = E[3 − 1] + 0 = 1. First
examine the node (cid:12)3/s(cid:13) = 1 (labeled d in the ﬁgure).
We see that the target 1 does not exist within d after
position 3. Next we examine node e. Since m[e] = 3 and
M[e] = 4, e does not contain the answer either. Next we
examine the node f. Because m[f] = 1 and M[f] = 3,
the answer must exist in its subtree. Therefore we scan
the children of f from left to right, and ﬁnd the leftmost
one with m[·] ≤ 1, which is node h. Because node h is
already a leaf, we scan the segment corresponding to it,
and ﬁnd the answer 12.

The sequence of subranges arising in this search
corresponds to a leaf-to-leaf path in the range minmax 
tree, and it contains O(ck) ranges according to
Lemma 4.1. We show now how to carry out this search
in time O(c2) rather than O(ck).
According to Lemma 4.1, the O(ck) nodes can be
partitioned into O(c) sequences of sibling nodes. We
will manage to carry out the search within each such
sequence in O(c) time. Assume we have to ﬁnd the
ﬁrst j ≥ i such that m[uj] ≤ d
(cid:4) ≤ M[uj], where
u1, u2, . . . , uk are sibling nodes in TmM . We ﬁrst check if
(cid:4) ≤ M[ui]. If so, the answer is ui. Otherwise,
m[ui] ≤ d
(cid:4)
if d
< m[ui], the answer is the ﬁrst j > i such that
m[uj] ≤ d
(cid:4), and if d
(cid:4)
> M[ui], the answer is the ﬁrst
j > i such that M[uj] ≥ d
Lemma 4.2. Let u1, u2, . . . a sequence of TmM nodes
containing consecutive intervals of P . If g(·) is a ±1
function and d < m[u1], then the ﬁrst j such that
d ∈ [m[uj], M[uj]] is the ﬁrst j > 1 such that d ≥ m[uj].
Similarly, if d > M[u1], then it is the ﬁrst j > 1 such
that d ≤ M[uj].
Proof. Since g(·) is a±1 function and the intervals
are consecutive, M[uj] ≥ m[uj−1] − 1 and m[uj] ≤
M[uj−1] + 1. Therefore, if d ≥ m[uj] and d < m[uj−1],
then d < M[uj] + 1, thus d ∈ [m[uj], M[uj]]; and of
course d (cid:8)∈ [m[uk], M[uk]] for any k < j as j is the ﬁrst
index such that d ≥ m[uj]. The other case is symmetric.(cid:5)(cid:6)

(cid:4).

i such that m[j] ≤ d
(the case M[j] ≥ d

Thus the problem is reduced to ﬁnding the ﬁrst j >
(cid:4), among (at most) k sibling nodes
(cid:4) is symmetric). We build a universal

cw

2 log(2w+1)

− c, the total space is O(

table with all the possible sequences of k/c m[·] values
and all possible −wc ≤ d
(cid:4) ≤ wc values, and for each such
(cid:4) we store the ﬁrst j in the sequence such
sequence and d
that m[j] ≤ d
(cid:4) (or we store a mark telling that there
is no such node in the sequence). Thus the table has
(2wc + 1)(k/c)+1 entries, and log(1 + k/c) bits per entry.
√
By choosing the constant of k = Θ(w/ log w) so that
k ≤
2w log w) (and
the arguments for the table ﬁt in a machine word). With
the table, each search for the ﬁrst node in a sequence of
siblings can be done by chunks of k/c nodes, which takes
O(k/(k/c)) = O(c) rather than O(k) time, and hence
the overall time is O(c2) rather than O(ck). Note that
k/c values to input to the table are stored in contiguous
(cid:4)[·] values in heap order. Thus
memory, as we store the m
we can access any k/c consecutive children values in
constant time. We use an analogous table for M[·].
Finally, the process to solve sum(P, g, i, j) in O(c2)
time is simple. We descend in the tree up to the
leaf
In the process we easily 
obtain sum(P, g, 0, (cid:5)k − 1), and compute the rest,
sum(P, g, (cid:5)k, j), in constant time using a universal table
we have already introduced. We repeat the process for
sum(P, g, 0, i − 1) and then subtract both results.

[(cid:5)k, rk] containing j.

We have proved the following lemma.

Lemma 4.3. In the RAM model with w-bit word size,
for any constant c ≥ 1 and a 0,1 vector P of length
n < wc, and a ±1 function g(·), fwd-search(P, g, i, j),
bwd-search(P, g, i, j), and sum(P, g, i, j) can be computed 
in O(c2) time using the range min-max tree
√
and universal lookup tables that require O(
2ww2 log w)
bits.

4.1 Supporting range minimum queries
Next we consider how to compute rmqi(P, g, i, j) and
RMQi(P, g, i, j).

2ww2 log w) bits.

Lemma 4.4. In the RAM model with w-bit word size,
for any constant c ≥ 1 and a 0,1 vector P of length
n < wc, and a ±1 function g(·), rmqi(P, g, i, j) and
RMQi(P, g, i, j) can be computed in O(c2) time using
√
the range min-max tree and universal lookup tables that
require O(
Proof. Because the algorithm for RMQi is analogous
to that for rmqi, we consider only the latter. From
Lemma 4.1, the range [i, j] is expressed by a disjoint partition 
of O(ck) subranges, each corresponding to some
node of the range min-max tree. Let μ1, μ2, . . . be the
minimum values of the subranges. Then the minimum
value in [i, j] is the minimum of them. The minimum
(cid:4), exvalues 
in each subrange are stored in array m
cept for at most two subranges corresponding to leaves

140Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php√

of the range min-max tree. The minimum values of
such leaf subranges are found by table lookups using P ,
by precomputing a universal table of O(
2ww2 log w)
bits. The minimum value of a subsequence μ(cid:3), . . . , μr
which shares the same parent in the range min-max
tree can be also found by table lookups. There are at
most k such values, and for consecutive k/c values we
use a universal table to ﬁnd their minimum, and re-
√
peat this c times, as before. The size of the table is
O(
2ww) bits (the k/c factor
is to account for queries that span less than k/c blocks,
so we can compute the minimum up to any value in the
sequence).

2w(k/c) log(k/c)) = O(

√

Let μ be

the minimum value we ﬁnd in
μ1, μ2, . . . , μm. If there is a tie, we choose the leftmost
one. If μ corresponds to an internal node of the range
min-max tree, we traverse the tree from the node to a
leaf having the leftmost minimum value. At each step,
we ﬁnd the leftmost child of the current node having
the minimum, in O(c) time using our precomputed table.
 We repeat the process from the resulting child, until
reaching a leaf. Finally, we ﬁnd the index of the minimum 
value in the leaf, in constant time by a lookup on
our other universal table. The overall time complexity
is O(c2).
(cid:5)(cid:6)

4.2 Other operations
The previous development on fwd-search, bwd-search,
rmqi, and RMQi, has been general, for any g(·). Applied
to g = π, they solve a large number of operations, as
shown in Section 3. For the remaining ones we focus
directly on the case g = π.

It is obvious how to compute degree(i), child(i, q)
and child-rank(i) in time proportional to the degree of
the node. To compute them in constant time, we use
Lemma 3.2, that is, the number of children of node i is
equal to the number of minimum values in the excess
(cid:4)[·] to the data
array for i. We add another array n
structure. In the range min-max tree, each node stores
the minimum value of its subrange. In addition to this,
(cid:4)[·] the number of the minimum values
we also store in n
of the subrange of each node in the tree.

Now we can compute degree(i) in constant time.
Let d = depth(i) and j = ﬁndclose(i). We partition the
range E[i+1, j−1] into O(ck) subranges, each of which
corresponds to a node of the range min-max tree. Then
for each subrange whose minimum value is d, we sum up
(cid:4)[·]).
the number of occurrences of the minimum value (n
The number of occurrences of the minimum value in leaf
subranges can be computed by table lookup on P , with
a universal table using O(
2ww2 log w) bits. The time
complexity is O(c2) if we use universal tables that let us

√

√

2ww) bits.

process chunks of k/c children at once, that is, the one
used for rmqi plus another telling the number of times
the minimum appears in the sequence. This table also
requires O(
Operation child-rank(i) can be computed similarly,
by counting the number of minima in E[parent(i), i−1].
Operation child(i, q) follows the same idea of degree(i),
(cid:4)[·] exceeds
except that, in the node where the sum of n
q, we must descend until the range min-max leaf that
contains the opening parenthesis of the q-th child. This
(cid:4)[·] values of each node, and
search is also guided by the n
is done also in O(c2) time by using another universal
√
(cid:4)[·]
table of O(
2w log w) bits (that tells us where the n
exceed some threshold in a sequence of k/c values).
For operations leaf-rank, leaf-select, lmost-leaf and
rmost-leaf, we deﬁne a bit-vector P1[0, n − 1] such
that P1[i] = 1 ⇐⇒ P [i] = 1 ∧ P [i + 1] = 0.
Then leaf-rank(i) =rank 1(P1, i) and leaf-select(i) =
select1(P1, i) hold. The other operations are computed
by lmost-leaf(i) = select1(P1, rank1(P1, i − 1) + 1) and
rmost-leaf(i) = select1(P1, rank1(P1, ﬁndclose(i))).

We recall the deﬁnition of inorder of nodes, which

is essential for compressed suﬃx trees.

Definition 4. ([35]) The inorder rank of an internal
node v is deﬁned as the number of visited internal nodes,
including v,
in the left-to-right depth-ﬁrst traversal,
when v is visited from a child of it and another child
of it will be visited next.

Note that an internal node with q children has q−1
inorders, so leaves and unary nodes have no inorder. We
deﬁne in-rank(i) as the smallest inorder value of internal
node i.
To compute in-rank and in-select, we use another
bit-vector P2[0, n − 1] such that P2[i] = 1 ⇐⇒ P [i] =
0∧P [i+1] = 1. The following lemma gives an algorithm
to compute the inorder of an internal node.

Lemma 4.5. ([35]) Let i be an internal node, and let
j = in-rank(i), so i = in-select(j). Then

in-rank(i) =rank
in-select(j) =enclose

1(P2, ﬁndclose(P, i + 1))
(P, select1(P2, j) + 1)

Note that in-select(j) will return the same node i

for any its degree(i) − 1 inorder values.

Note that we need not store P1 and P2 explicitly;
they can be computed from P when needed. We only
need the extra data structures for constant-time rank
and select, which can be reduced to the corresponding
sum and fwd-search operations on the virtual P1 and
P2 vectors.

141Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php(cid:4).

(cid:4)
φ, e

(cid:4)
φ, m

(cid:4), m

(cid:4), M

(cid:4)
ψ, m

(cid:4)
ψ, M

(cid:4), and n

4.3 Reducing extra space
Apart from vector P [0, n − 1], we need to store vectors 
e
In addition, to implement 
rank and select using sum and fwd-search, we
(cid:4)
would need to store vectors e
φ,
(cid:4)
and M
ψ which maintain the corresponding values for
functions φ and ψ. However, note that sum(P, φ, 0, i)
and sum(P, ψ, 0, i) are nondecreasing, thus the mini-
mum/maximum within the block is just the value of
the sum at the beginning/end of the block. Moreover,
as sum(P, π, 0, i) = sum(P, φ, 0, i) − sum(P, ψ, 0, i) and
sum(P, φ, 0, i) + sum(P, ψ, 0, i) =i , it turns out that
both eφ[i] = (ri + e[i])/2 and eψ[i] = (ri − e[i])/2 are redundant;
 analogous formulas hold for M and m and for
internal nodes. Moreover, any sequence of k/c consecutive 
such values can be obtained, via table lookup, from
the sequence of k/c consecutive values of e[·], because
the ri values increase regularly at any node. Hence we
do not store any extra information to support φ and ψ.
(cid:4) naively, we
require O(nc log(w)/w) bits of extra space on top of the
n bits for P .

If we store vectors e

(cid:4), and n

(cid:4), m

(cid:4), M

The space can be largely reduced by using a recent
technique by Pˇatra¸scu [30]. They deﬁne an aB-tree over
an array A[0, n − 1], for n a power of B, as a complete
tree of arity B, storing B consecutive elements of A in
each leaf. Additionally, a value ϕ ∈ Φ is stored at each
node. This must be a function of the corresponding
elements of A for the leaves, and a function of the
ϕ values of the children, and of the subtree size, for
internal nodes. The construction is able to decode the
B values of ϕ for the children of any node in constant
time, and to decode the B values of A for the leaves in
constant time, if they can be packed in a machine word.
the vector, B =
k = s is our arity, and our trees will be of size
N = Bc, which is slightly smaller than the wc
we have been assuming.
Our values are tuples
ϕ ∈ (cid:17)−Bc,−Bc, 0,−Bc(cid:18) . . .(cid:17)Bc, Bc, Bc, Bc(cid:18) encoding
the m, M, n, and e values at the nodes, respectively.
We give next their result, adapted to our case.

In our case, A = P is

tree [15] that is used internally on O(w) values. It could
be reduced to w time for any constant  > 0 and
navigation time O(1/), but we prefer to set c >3/2
to make it irrelevant.

These parameters still allow us to represent our
range min-max trees while yielding the complexities
we had found, as k = Θ(w/ log w) and N ≤ wc.
Our accesses to the range min-max tree are either (i)
partitioning intervals [i, j] into O(ck) subranges, which
are easily identiﬁed by navigating from the root in O(c)
time (as the k children are obtained together in constant
time); or (ii) navigating from the root while looking
for some leaf based on the intermediate m, M, n, or e
values.
Thus we retain all of our time complexities. The
space, instead, is reduced to N + 2 +O (
2w), where
the latter part comes from universal tables (ours also
shrink due to the reduced k and s). Note that our vector
P must be exactly of length N; padding is necessary
otherwise. Both the padding and the universal tables
will lose relevance for larger trees, as seen in the next
section.

√

The next theorem summarizes our results in this
c log w )c)

section. We are able of handling trees of Θ(( w
nodes, for any c >3/2.

Theorem 4.1. On a w-bit word RAM, for any constant 
c >3/2, we can represent a sequence P of N = Bc
parentheses, with suﬃciently small B = Θ( w
c log w ), computing 
all operations of Table 1 in O(c2) time, with a
data structure depending on P that uses N + 2 bits,
√
lookup tables (i.e., not depending on
and universal
P ) that use O(
√
2w) bits. The preprocessing time is
O(N +
2wpoly(w)) (the latter being needed only once
for universal tables) and the working space is O(N) bits.

In case we need to solve the operations that build
on P1 and P2, we need to represent their corresponding
φ functions (as ψ is redundant). This can still be
done with Lemma 4.6 using Φ = (2B + 1)6c and (B +
1) log(2B + 1) ≤ w

12c. Theorem 4.1 applies verbatim.

and B be

8c (thus B = Θ( w

Lemma 4.6. (adapted from Thm. 8 in [30])
Let Φ = (2B + 1)4c,
such that
(B + 1) log(2B + 1) ≤ w
c log w )).
An aB-tree of size N = Bc with values in Φ can be
√
stored using N + 2 bits, plus universal lookup tables of
O(
2w) bits. It can obtain the m, M, n or e values of
the children of any node, and descend to any of those
√
children, in constant time. The structure can be built
in O(N + w3/2) time, plus O(
2wpoly(w)) for the
universal tables.

The “+w3/2” construction time comes from a fusion

log n

5 A data structure for large trees
In practice, one can use the solution of the previous
section for trees of any size, achieving O( k log n
w logk n) =
O(
log w−log log n) =O (log n) time (using k = w/ log n)
for all operations with an extremely simple and elegant
data structure (especially if we choose to store arrays
(cid:4), etc.
in simple form). In this section we show how
m
to achieve constant time on trees of arbitrary size.

For simplicity, let us assume in this section that we
handle trees of size wc in Section 4. We comment at the
end the diﬀerence with the actual size Bc handled.

142Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpFor large trees with n > wc nodes, we divide the
parentheses sequence into blocks of length wc. Each
block (containing a possibly non-balanced sequence of
parentheses) is handled with the range min-max tree of
Section 4.

Let m1, m2, . . . , mτ ; M1, M2, . . . , Mτ ;

and
e1, e2, . . . , eτ ; be the minima, maxima, and excess
of the τ = (cid:9)2n/wc(cid:10) blocks, respectively. These values
are stored at the root nodes of each TmM tree and can
be obtained in constant time.

consider

5.1 Forward and backward searches on π
extending fwd-search(P, π, i, d)
and
We
bwd-search(P, π, i, d) to trees of arbitrary size. We focus
on fwd-search, asbwd-search is symmetric.
We ﬁrst try to solve fwd-search(P, π, i, d) within the
block j = (cid:12)i/wc(cid:13) of i. If the answer is within block j, we
are done. Otherwise, we must look for the ﬁrst excess
(cid:4) = ej−1 + sum(P, π, 0, i − 1 − wc · (j − 1)) + d in the
d
following blocks (where the sum is local to block j).
Then the answer lies in the ﬁrst block r > j such that
mr ≤ d
(cid:4) ≤ Mr. Thus, we can apply again Lemma 4.2,
(cid:4) (cid:8)∈ [mj+1, Mj+1], we must
starting at [mj+1, Mj+1]: If d
either ﬁnd the ﬁrst r > j + 1 such that mr ≤ j, or such
that Mr ≥ j. Once we ﬁnd such block, we complete
(cid:4) − er−1)
the operation with a local fwd-search(P, π, 0, d
query inside it.

The problem is how to achieve constant-time search,
for any j, in a sequence of length τ. Let us focus on left-
to-right minima, as the others are similar.

Definition 5. Let m1, m2, . . . , mτ be a sequence of integers.
 We deﬁne for each 1 ≤ j ≤ τ the left-toright 
minima starting at j as lrm(j) =(cid:17)j 0, j1, j2, . . .(cid:18),
where j0 = j, jr < jr+1, mjr+1 < mjr , and
mjr +1 . . . mjr+1−1 ≥ mjr .

The following lemmas are immediate.

Lemma 5.1. The ﬁrst element ≤ x after position j in
a sequence of integers m1, m2, . . . , mτ is mjr for some
r > 0, where jr ∈ lrm(j).

Lemma 5.2. Let
lrm(j)[pj + i] = lrm(j

lrm(j)[pj] =lrm (j

(cid:4))[pj(cid:2)].
(cid:4))[pj(cid:2) + i] for all i > 0.

Then

That is, once the lrm sequences starting at two
positions coincide in a position, they coincide thereafter.
Lemma 5.2 is essential to store all the τ sequences
lrm(j) for each block j, in compact form. We form
a tree Tlrm, which is essentially a trie composed of the
reversed lrm(j) sequences. The tree has τ nodes, one
per block. Block j is a child of block j1 = lrm(j)[1]
(note lrm(j)[0] = j0 = j), that is, j is a child of the

ﬁrst block j1 > j such that mj1 < mj. Thus each j-
to-root path spells out lrm(j), by Lemma 5.2. We add
a ﬁctitious root to convert the forest into a tree. Note
this structure is called 2d-Min-Heap by Fischer [11], who
shows how to build it in linear time.

Example 4. Figure 3 illustrates the tree built from
the sequence (cid:17)m1..m9(cid:18) = (cid:17)6, 4, 9, 7, 4, 4, 1, 8, 5(cid:18). Then
lrm(1) = (cid:17)1, 2, 7(cid:18), lrm(2) = (cid:17)2, 7(cid:18), lrm(3) = (cid:17)3, 4, 5, 7(cid:18),
and so on.

If we now assign weight mj − mj1 to the edge
between j and its parent j1, the original problem of
ﬁnding the ﬁrst jr > j such that mjr ≤ d
(cid:4) reduces to
ﬁnding the ﬁrst ancestor jr of node j such that the sum
(cid:4)(cid:4) = mj − d
(cid:4).
of the weights between j and jr exceeds d
Thus we need to compute weighted level ancestors in
Tlrm. Note that the weight of an edge in Tlrm is at
most wc.

Lemma 5.3. For a tree with τ nodes where each edge
has an integer weight in [1, W ], after O(τ log1+ τ) time
preprocessing, a weighted level-ancestor query is solved
in O(t + 1/) time on a Ω(log(τ W ))-bit word RAM.
The size of the data structure is O(τ log τ log(τ W ) +
τ W tt
logt(τ W ) + (τ W )3/4) bits.

Proof. We use a variant of Bender and Farach’s
(cid:17)O(τ log τ),O(1)(cid:18) algorithm [4]. Let us ignore weights
for a while. We extract a longest root-to-leaf path of
the tree, which disconnects the tree into several subtrees.
 Then we repeat the process recursively for each
subtree, until we have a set of paths. Each such path,
say of length (cid:5), is extended upwards, adding other (cid:5)
nodes towards the root (or less if the root is reached).
The extended path is called a ladder, and its is stored
as an array so that level-ancestor queries within a ladder 
are trivial. This guarantees that a node of height
h has also height h in its path, and thus at least its
ﬁrst h ancestors in its ladder. Moreover the union
of all ladders has at most 2τ nodes and thus requires
O(τ log τ) bits. For each tree node v, an array of its
(at most) log τ ancestors at depths depth(v) − 2i, i ≥ 0,
is stored (hence the O(τ log τ)-number space and construction 
time). To solve the query level-ancestor(v, d),
(cid:4) = depth(v) − d, the ancestor v
(cid:4) at distance
where d
(cid:4)(cid:4) = 2(cid:5)log d(cid:2)(cid:6) from v is computed. Since v
(cid:4) has height at
d
(cid:4)(cid:4) ancestors in its lad-
(cid:4)(cid:4), it has at least its ﬁrst d
least d
(cid:4) we need only the ancestor at distance
der. But from v
(cid:4) − d
(cid:4)(cid:4)
d

(cid:4)(cid:4), so the answer is in the ladder.

< d
To include the weights, we must be able to ﬁnd the
(cid:4) and the answer considering the weights, instead
node v
of the number of nodes. We store for each ladder of
length (cid:5) a sparse bitmap of length at most (cid:5)W , where

143Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php2

4

1

6

4

7

3

9

7

1

5

4

6

4

9

5

8

8

Figure 3: A tree representing the lrm(j) sequences of values m1 . . . m9.

the i-th 1 left-to-right represents the i-th node upwards
in the ladder, and the distance between two 1s, the
weight of the edge between them. All the bitmaps are
concatenated into one (so each ladder is represented
by a couple of integers indicating the extremes of its
bitmap). This long bitmap contains at most 2τ 1s,
and because weights do not exceed W , at most 2τ W
0s. Using Pˇatra¸scu’s sparse bitmaps [30],
it can be
represented using O(τ log W + τ W tt
logt(τ W ) + (τ W )3/4) bits
and do rank/select in O(t) time.

In addition, we store for each node the log τ accumulated 
weights towards ancestors at distances 2i, using 
fusion trees [15]. These can store z keys of (cid:5) bits
in O(z(cid:5)) bits and, using O(z5/6(z1/6)4) =O (z1.5) preprocessing 
time, answer predecessor queries in O(log (cid:3) z)
time (via an (cid:5)1/6-ary tree). The 1/6 can be reduced to
achieve O(z1+) preprocessing time and O(1/) query
time for any desired constant 0 < ≤ 1/2.
In our case this means O(τ log τ log(τ W )) bits
of space, O(τ log1+ τ) construction time, and O(1/)
access time. Thus we can ﬁnd in constant time,
from each node v, the corresponding weighted ancestor
(cid:4) using a predecessor query.
If this corresponds to
v
distance 2i, then the true ancestor is at distance < 2i+1,
(cid:4), where it is found
and thus it is within the ladder of v
using rank/select on the bitmap of ladders (each node
v has a pointer to its 1 in the ladder corresponding to
(cid:5)(cid:6)
the path it belongs to).

To apply this lemma for our problem of computing
fwd-search outside blocks, we have W = wc and τ = n
wc .
Then the size of the data structure becomes O( n log
2 n
wc +
n tt
logt n +n3/4). By choosing  = min(1/2, 1/c2), the query
time is O(c2 + t) and the preprocessing time is O(n) for
c ≥ 1.47.

5.2 Other operations
For computing rmqi and RMQi, we use a simple data
structure [3] on the mr and Mr values, later improved

For

to require only O(τ) bits on top of the sequence of
values [35, 12]. The extra space is thus O(n/w c) bits,
and it solves any query up to the block granularity.
For solving a general query [i, j] we should compare the
minimum/maximum obtained with the result of running
queries rmqi and RMQi within the blocks at the two
extremes of the boundary [i, j].
the remaining operations, we deﬁne pioneers 
[20]. We divide the parentheses sequence P [0, 2n−
1] into blocks of length wc. Then we extract pairs (i, j)
of matching parentheses (j = ﬁndclose(i)) such that
i and j belong to diﬀerent blocks.
If we consider a
graph whose vertex set consists of the blocks and whose
edge set consists of the pairs of parentheses, the graph
is outer-planar. To remove multiple edges, we choose
the tightest pair of parentheses for each pair of vertices.
These parentheses are called pioneers. Because pioneers
correspond to the edges (without multiplicity) of an
outer-planar graph, their number is O(n/wc). Furthermore,
 they form another balanced parentheses sequence
(cid:4) representing an ordinal tree with O(n/wc) nodes.
P
(cid:4) we use a compressed bit vector
C[0, 2n−1] such that C[i] = 1 indicates that parenthesis
P [i] is a pioneer. Using again Pˇatra¸scu’s result [30],
vector C can be represented in at most n
wc log(wc) +
O( n tt
logt n + n3/4) bits, so that operations rank and select
can be computed in O(t) time.

To encode P

For computing child and child-rank, it is enough to
consider only nodes which completely include a block
(otherwise the query is solved in constant time by
considering just two adjacent blocks). Furthermore,
among them, it is enough to consider pioneers because
if pair (i, j), with i and j in diﬀerent blocks, is not a
pioneer, then it must contain a pioneer matching pair
(cid:4) in the same
(i
(cid:4) is a descendant of i and all the
block of j. Thus i
(cid:4) +1, j−1],
children of i start within [i+1, i
thus all are contained in two blocks. Hence computing
child(i, q) and child-rank for a child of i can be done in
constant time by considering just these two blocks.

(cid:4) in the same block of i and j

(cid:4)] or within [j

(cid:4)), with i

(cid:4)

, j

144Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpThus we only need to care about pioneer nodes
containing at least one block; let us call marked these
nodes, of which there are only O(n/wc). We focus on
the children of marked nodes placed at the blocks fully
contained in them, as the others lie in at most the two
extreme blocks and can be dealt with in constant time.
For each marked node v we store a list formed
by the blocks fully contained in v, and that contain
(starting positions of) children of v. Since each block
contains children of at most one marked node fully
containing the block, each block belongs to at most one
list, and it stores its position in the list it belongs to.
All this data occupies O( n log n
) bits. In addition, the
contained blocks store the number of children of v that
start within them. The sequence of number of children
formed for each marked node v is stored as gaps between
consecutive 1s in a bitmap Cv. All these lists together
contain at most n 0s and O(n/wc) 1s, and thus can
be stored within the same space bounds of the other
bitmaps in this section.

wc

Using this bitmap child and child-rank can easily be
solved using rank and select. For child(v, q) on a marked
node v we start using p = rank1(Cv, select0(Cv, q)).
This tells the position in the list of blocks of v where
the q-th child of v lies. Then the answer corre-
(cid:4)-th minimum within that block, for
sponds to the q
(cid:4) = q − rank0(select1(Cv, p)).
For child-rank(u),
q
where v = parent(u) is marked, we start with z =
rank0(Cv, select1(Cv, pu)), where pu is the position of
the block of u within the list of v. Then we add to z
the number of minima in the block of u until u − 1.

wc

For degree, similar arguments show that we only
need to consider marked nodes, for which we simply
store all the answers within O( n log n

) bits of space.

Finally, the remaining operations require just rank
and select on P , or the virtual bit vectors P1 and P2. We
can make up a sequence with the accumulated number
of 1s in each of the τ blocks. The numbers add up to
O(n) and thus can be represented as gaps of 0s between
consecutive 1s in a bitmap, which can be stored within
the previous space bounds. Performing rank and select
on this bitmap, in time O(t), lets us know in which block
must we ﬁnish the query, using its range min-max tree.

5.3 The ﬁnal result
Recalling Theorem 4.1, we have O(n/B c) blocks, for
B = O( w
c log w ). The sum of the space for all the blocks is
2n +O(n/Bc), plus shared universal tables that add up
√
to O(
2w) bits. Padding the last block to size exactly
Bc adds up another negligible extra space.

On the other hand, in this section we have extended
the results to larger trees of n nodes, adding time
O(t) to the operations. By properly adjusting w to

Bc

2 n)

+ n tt

logt n +

√
B in the results, the overall extra space added is
O( n(c log B+log
2B + n3/4) bits. Assuming
pessimistically w = log n, setting t = c2, and replacing
B, we get that the time for any operation is O(c2), and
the total space simpliﬁes to 2n + O( n logc log n
logc−2 n ).
Construction time is O(n). We now analyze the
working space for constructing the data structure. We
ﬁrst convert the input balanced parentheses sequence
P into a set of aB-trees, each of which represents a
part of the input of length Bc. The working space is
O(Bc) from Theorem 4.1. Next we compute pioneers:
We scan P from left to right, and if P [i] is an opening
parenthesis, we push i in a stack, and if it is closing,
we pop an entry from the stack. Because P is nested,
the values in the stack are monotone. Therefore we can
store a new value as the diﬀerence from the previous
one using unary code. Thus the values in the stack
can be stored in O(n) bits. Encoding and decoding
the stack values takes O(n) time in total. It is easy to
compute pioneers from the stack. Once the pioneers are
identiﬁed, Pˇatra¸scu’s compressed representation [30] of
bit vector C is built in O(n) space too, as it also cuts the
bitmap into polylog-sized aB-trees and then computes
some directories over just O(n/polylog(n)) values.

The remaining data structures, such as the lrm
sequences and tree, the lists of the marked nodes, and
the Cv bitmaps, are all built on O(n/Bc) elements, thus
they need at most O(n) bits of space for construction.
By rewriting c−2−δ as c, for any constant δ > 0, we
get our main result on static ordinal trees, Theorem 1.1.

6 A simple data structure for dynamic trees
In this section we give a simple data structure for
dynamic ordinal trees.
In addition to the previous
query operations, we add now insertion and deletion
of internal nodes and leaves. We then consider a more
sophisticated representation giving sublogarithmic time
for almost all of the operations.

6.1 Memory management
We store a 0,1 vector P [0, 2n − 1] using a dynamic
min-max tree. Each leaf of the min-max tree stores a
segment of P in verbatim form. The length (cid:5) of each
segment is restricted to L ≤ (cid:5) ≤ 2L for some parameter
L > 0.

If insertions or deletions occur, the length of a
segment will change. We use a standard technique for
dynamic maintenance of memory cells [24]. We regard
the memory as an array of cells of length 2L each, hence
allocation is easily handled in constant time. We use
L + 1 linked lists sL, . . . , s2L where si stores all the
segments of length i. All the segments with equal length

145Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpi are packed consecutively, without wasting any extra
space, in the cells of linked list si. Therefore a cell (of
length 2L) stores (parts of) at most three segments,
and a segment spans at most two cells. Tree leaves
store pointers to the cell and oﬀset where its segment is
stored. If the length of a segment changes from i to j,
it is moved from si to sj. The space generated by the
removal is ﬁlled with the head segment in si, and the
removed segment is stored at the head of sj.
With this scheme, scanning any segment takes
O(L/ log n) time, by processing it by chunks of Θ(log n)
bits. This is also the time to compute operations
fwd-search, bwd-search, rmqi, etc. on the segment, using
proper universal tables. Migrating a node to another list
is also done in O(L/ log n) time.

If a migration of a segment occurs, pointers to the
segment from a leaf of the tree must change. For this
sake we store back-pointers from each segment to its
leaf. Each cell stores also a pointer to the next cell of
its list. Finally, an array of pointers for the heads of
sL, . . . , s2L is necessary. Overall, the space for storing a
0,1 vector of length 2n is 2n + O( n log n

L ) bits.

The rest of the dynamic tree will use sublinear
space, and thus we allocate ﬁxed-size memory cells for
the internal nodes, as they will waste at most a constant
fraction of the allocated space.

6.2 A dynamic tree
We give a simple dynamic data structure representing
an ordinal tree with n nodes using 2n + O(n/ log n)
bits, and supporting all query and update operations
in O(log n) worst-case time.
We divide the 0,1 vector P [0, 2n−1] into segments of
length from L to 2L, for L = log2 n. We use a balanced
binary tree for representing the range min-max tree.
If a node of the tree corresponds to a vector P [i, j],
the node stores i and j, as well as e = sum(P, π, i, j),
m = rmq(P, π, i, j), M = RMQ(P, π, i, j), and n, the
number of minimum values in P [i, j] regarding π. (Data
on φ for the virtual vectors P1 and P2 is handled
analogously.)

It is clear that fwd-search, bwd-search, rmqi, RMQi,
rank, select, degree, child and child-rank can be computed 
in O(log n) time, by using the same algorithms
developed for small trees in Section 4. These operations 
cover all the functionality of Table 1. Note the
values we store are local to the subtree (so that they
are easy to update), but global values are easily derived 
in a top-down traversal. For example, to solve
fwd-search(P, π, i, d) starting at the min-max tree root
v with children vl and vr, we ﬁrst compute the desired
(cid:4) = E[i − 1] + d, where E[i − 1] is found
global excess d
in a top-down traversal towards position i − 1, adding

√

up e(vl) each time we descend to vr. Once we obtain
(cid:4), we start again at the root and see if j(vl) ≥ i, in
d
which case try ﬁrst on vl. If the answer is not there or
(cid:4) − e(vl).
j(vl) < i, we try on vr, now seeking excess d
Because each node uses O(log n) bits, and the
number of nodes is O(n/L), the total space is 2n +
O(n/ log n) bits. This includes the extra O( n log n
L )
term for the leaf data. Note that we need to maintain
several universal tables that handle chunks of 1
2 log n
bits. These require just O(
n · polylog(n)) extra bits.
If insertion/deletion occurs, we update a segment,
and the stored values in the leaf for the segment.
If
the length of the segment exceeds 2L, we split it into
two and add a new node. If the length becomes shorter
than L, we ﬁnd the adjacent segment to the right. If
its length is L, we concatenate them; otherwise move
the leftmost bit of the right segment to the left one. In
this manner we can keep the invariant that all segments
have length L to 2L. Then we update all the values
in the ancestors of the modiﬁed leaves. If a balancing
operation occurs, we also update the values in nodes.
All these updates are easily carried out in constant time
per involved node, as the values to update are minima,
maxima, and sum over the two children values. Thus
the update time is also O(log n).
When (cid:9)log n(cid:10) changes, we must update the allowed
values for L, recompute universal tables, change the
width of the stored values, etc. M¨akinen and Navarro
[23] have shown how to do this for a very similar case
(dynamic rank/select on a bitmap). Their solution of
splitting the bitmap into 5 parts and moving border bits
across parts to deamortize the work applies verbatim to
our case, thus we can handle changes in (cid:9)log n(cid:10) without
altering the space nor the time complexity (except for
O(w) extra bits in the space due to a constant number
of system-wide pointers, a technicism we ignore). This
applies to the next solution too, where we will omit the
issue.

7 A faster dynamic data structure
√
Instead of the balanced binary tree, we use a B-tree with
branching factor Θ(
log n), as in previous work [6].
Then the depth of the tree is O(log n/ log log n). The
lengths of segments is L to 2L for L = log2 n/ log log n.
The required space for the range min-max tree and the
vector is now 2n+O(n log log n/ log n) bits (the internal
nodes use O(log3/2 n) bits but there are only O(
n
)
log n
internal nodes). Now each leaf can be processed in time
O(log n/ log log n).
√
log n ≤ k ≤ 2

√
Each internal node v of the range min-max tree has
k children, for
log n. Let c1, c2, . . . , ck
be the children of v, and [(cid:5)1..r1], . . . , [(cid:5)k..rk] be their

√

L

146Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpcorresponding subranges. We store (i) the children 
boundaries (cid:5)i, (ii) sφ[1, k] and sψ[1, k] storing
sφ/ψ[i] =sum (P, φ/ψ, (cid:5)1, ri), (iii) e[1, k] storing e[i] =
sum(P, π, (cid:5)1, ri), (iv) m[1, k] storing m[i] =e[i − 1] +
rmq(P, π, (cid:5)i, ri), and M[1, k] storing M[i] = e[i − 1] +
RMQ(P, π, (cid:5)i, ri). Note that the values stored are local
to the subtree (as in the simpler balanced binary tree
version) but cumulative with respect to previous siblings.
 Note also that storing sφ, sψ and e is redundant,
as noted in Section 4.3, but we need them in explicit
form to achieve constant-time searching into their values.


Apart from simple accesses, we need to support the

following operations within a node:
• p(i): the largest j such that (cid:5)j−1 ≤ i (or j = 1).
• wφ/ψ(i): the largest j such that sφ/ψ[j − 1] ≤ i (or

j = 1).

• f(i, d): the smallest j ≥ i such that m[j] ≤ d ≤

M[j].

• b(i, d): the largest j ≤ i such that m[j] ≤ d ≤ M[j].
• r(i, j, t): the t-th x such that m[x] is minimum in

m[i, j].

• R(i, j, t): the t-th x such that M[x] is maximum in

M[i, j].

• n(i, j): the number of times the minimum occurs

in m[i, j].

• update:

updates the data structure upon ±1

changes in some child.

√

Operations

fwd-search/bwd-search can then be
carried out via O(log n/ log log n) applications of
f(i, d)/b(i, d). Recalling Lemma 4.1, the interval of
interest is partitioned into O(
log n · log n/ log log n)
nodes of the B-tree, but these can be grouped into
O(log n/ log log n) sequences of siblings. Within each
such sequence a single f(i, d)/b(i, d) operation is sufﬁcient.
 Once the answer of interest j is ﬁnally found
within some internal node, we descend to its j-th
child and repeat the search until ﬁnding the correct 
leaf, again in O(log n/ log log n) applications of
f(i, d)/b(i, d). Operations rmqi and RMQi are solved
in very similar fashion, using O(log n/ log log n) applications 
of r(i, j, 1)/R(i, j, 1). Also, operations rank
and select on P are carried out in obvious manner
with O(log n/ log log n) applications of p(i) and wφ/ψ(i).
Handling φ for P1 and P2 is immediate; we omit it.

For degree we partition the interval as for rmqi
and then use m[r(i, j, 1)] in each node to identify those

holding the global minimum. For each node holding
the minimum, n(i, j) gives the number of occurrences
of the minimum in the node. Thus we apply r(i, j, 1)
and n(i, j) O(log n/ log log n) times. Operation childrank 
is very similar, by changing the right end of the
interval of interest, as before. Finally, solving child is
also similar, except that when we exceed the desired
rank in the sum (i.e., in some node n(i, j) ≥ t, where
t is the local rank of the child we are looking for), we
ﬁnd the desired min-max tree branch with r(i, j, t), and
continue until ﬁnding the proper leaf with one r(i, j, t)
operation per level.

By using the dynamic partial sums data structure 
[32] and the Super-Cartesian tree [13], we obtain:

Lemma 7.1. For a 0,1 vector of length 2n, there exists 
a data structure using 2n + O(n log log n/ log n)
bits supporting fwd-search and bwd-search in O(log n)
time, and all other operations (including update) in
O(log n/ log log n) time.

In many operations to support, we carry out
fwd-search(P, π, i, d) or bwd-search(P, π, i, d) for a small
constant d. Those particular cases can be made more
eﬃcient.

Lemma 7.2. For a 0,1 vector P , fwd-search(P, π, i, d)
and bwd-search(P, π, i, d) can be computed in O(d +
log n/ log log n) time.

The proofs will be given in the full paper.
This completes our main result in this section,

Theorem 1.2.

8 Concluding remarks
In this paper we have proposed ﬂexible and powerful
data structures for the succinct representation of ordinal
trees. For the static case, all the known operations are
done in constant time using 2n + O(n/polylog(n)) bits
of space, for a tree of n nodes. This largely improves
the redundancy of previous representations, by building
on a recent result [30]. The core of the idea is the range
min-max tree, which has independent interest. This
simple data structure reduces all of the operations to
a handful of primitives, which run in constant time on
polylog-sized subtrees.
It can be used in standalone
form to obtain a simple and practical implementation
that achieves O(log n) time for all the operations. We
then achieve constant time by using the range min-max
tree as a building block for handling larger trees.

For the dynamic case, there have been no data
structures supporting several of the usual tree operations.
 The data structures of this paper support all of
the operations, including node insertion and deletion,
in O(log n) time, and a variant supports most of them

147Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpin O(log n/ log log n) time. They are based on dynamic
range min-max trees, and especially the former is extremely 
simple and can be easily implemented.

Future work includes reducing the time complexities 
for all of the operations in the dynamic case to
O(log n/ log log n), as well as trying to improve the redundancy 
(this is O(n/ log n) for the simpler structure
and O(n log log n/ log n) for the more complex one).

Acknowledgments
We thank Mihai Pˇatra¸scu for conﬁrming us the construction 
cost of his aB-tree and rank/select data structure 
[30].

References

[1] D. Arroyuelo. An improved succinct representation for
dynamic k-ary trees. In Proc. 19th Annual Symposium
on Combinatorial Pattern Matching (CPM), LNCS
5029, pages 277–289, 2008.

[2] J. Barbay, J. I. Munro, M. He, and S. S. Rao. Succinct
indexes for strings, binary relations and multi-labeled
In Proc. 18th Annual ACM-SIAM Symposium
trees.
on Discrete Algorithms (SODA), pages 680–689, 2007.
[3] M. Bender and M. Farach-Colton. The LCA problem
revisited. In Proc. 4th Latin American Symposium on
Theoretical Informatics (LATIN), LNCS 1776, pages
88–94, 2000.

[4] M. Bender and M. Farach-Colton. The level ancestor
problem simpliﬁed. Theoretical Computer Science,
321(1):5–12, 2004.

[5] D. Benoit, E. D. Demaine, J. I. Munro, R. Raman,
V. Raman, and S. S. Rao. Representing trees of higher
degree. Algorithmica, 43(4):275–292, 2005.

[6] H.-L. Chan, W.-K. Hon, T.-W. Lam, and K. Sadakane.
Compressed indexes for dynamic text collections. ACM
Transactions on Algorithms, 3(2):article 21, 2007.

[7] Y.-T. Chiang, C.-C. Lin, and H.-I. Lu. Orderly
spanning trees with applications. SIAM Journal on
Computing, 34(4):924–945, 2005.

[8] O. Delpratt, N. Rahman, and R. Raman. Engineering
the LOUDS succinct tree representation. In Proc. 5th
Workshop on Eﬃcient and Experimental Algorithms
(WEA), pages 134–145. LNCS 4007, 2006.

[9] A. Farzan and J. I. Munro. A uniform approach
In Proc.
towards succinct representation of trees.
11th Scandinavian Workshop on Algorithm Theory
(SWAT), LNCS 5124, pages 173–184, 2008.

[10] P. Ferragina, F. Luccio, G. Manzini, and S. Muthukrishnan.
 Structuring labeled trees for optimal succinctness,
 and beyond. In Proc. 46th IEEE Annual Symposium 
on Foundations of Computer Science (FOCS),
pages 184–196, 2005.

[11] J. Fischer. Optimal succinctness for range minimum

queries. CoRR, abs/0812.2775, 2008.

[12] J. Fischer and V. Heun. A new succinct representation 
of RMQ-information and improvements in the enhanced 
suﬃx array. In Proc. 1st International Symposium 
on Combinatorics, Algorithms, Probabilistic and
Experimental Methodologies (ESCAPE), LNCS 4614,
pages 459–470, 2007.

[13] J. Fischer and V. Heun. Range median of minima
queries, super-cartesian trees, and text indexing.
In
Proc. 19th International Workshop on Combinatorial
Algorithms (IWOCA), pages 239–252, 2008.

[14] M. Fredman and M. Saks. The Cell Probe Complexity
In Proc. 21st Annual
of Dynamic Data Structures.
ACM Symposium on Theory of Computing (STOC),
pages 345–354, 1989.

[15] M. Fredman and D. Willard. Surpassing the information 
theoretic bound with fusion trees. Journal of
Computer and Systems Science, 47(3):424–436, 1993.
[16] R. F. Geary, N. Rahman, R. Raman, and V. Raman.
A simple optimal representation for balanced parenIn 
Proc. 15th Annual Symposium on Combitheses.

natorial Pattern Matching (CPM), LNCS 3109, pages
159–172, 2004.

[17] R. F. Geary, R. Raman, and V. Raman. Succinct ordinal 
trees with level-ancestor queries. In Proc. 15th Annual 
ACM-SIAM Symposium on Discrete Algorithms
(SODA), pages 1–10, 2004.

[18] A. Golynski, R. Grossi, A. Gupta, R. Raman, and S. S.
In Proc. 15th
Rao. On the size of succinct indices.
Annual European Symposium on Algorithms (ESA),
pages 371–382. LNCS 4698, 2007.

[19] M. He, J. I. Munro, and S. S. Rao. Succinct ordinal
In Proc. 34th Internatrees 
based on tree covering.
tional Colloquium on Automata, Languages and Programming 
(ICALP), LNCS 4596, pages 509–520, 2007.
[20] G. Jacobson. Space-eﬃcient static trees and graphs. In
Proc. 30th IEEE Annual Symposium on Foundations of
Computer Science (FOCS), pages 549–554, 1989.

[21] J. Jansson, K. Sadakane, and W.-K. Sung. UltraIn 
Proc.
succinct representation of ordered trees.
18th Annual ACM-SIAM Symposium on Discrete Algorithms 
(SODA), pages 575–584, 2007.

[22] H.-I. Lu and C.-C. Yeh. Balanced parentheses strike
back. ACM Transactions on Algorithms (TALG),
4(3):article 28, 2008.

[23] V. M¨akinen and G. Navarro. Dynamic entropycompressed 
sequences and full-text indexes. ACM
Transactions on Algorithms (TALG), 4(3):article 32,
2008.

[24] J. I. Munro. An implicit data structure supporting insertion,
 deletion, and search in O(log n) time. Journal
of Computer System Sciences, 33(1):66–74, 1986.

[25] J. I. Munro. Tables. In Proc. 16th Foundations of Software 
Technology and Computer Science (FSTTCS),
LNCS 1180, pages 37–42, 1996.

[26] J. I. Munro and V. Raman. Succinct representation of
balanced parentheses and static trees. SIAM Journal
on Computing, 31(3):762–776, 2001.

[27] J. I. Munro, V. Raman, and S. S. Rao. Space eﬃcient

148Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpsuﬃx trees.
2001.

Journal of Algorithms, 39(2):205–222,

[28] J. I. Munro, V. Raman, and A. J. Storm. Representing 
dynamic binary trees succinctly. In Proc. 15th Annual 
ACM-SIAM Symposium on Discrete Algorithms
(SODA), pages 529–536, 2001.

[29] J. I. Munro and S. S. Rao. Succinct representations
In Proc. 31th International Colloquium
of functions.
on Automata, Languages and Programming (ICALP),
LNCS 3142, pages 1006–1015, 2004.

[30] M. Pˇatra¸scu. Succincter.

In Proc. 49th IEEE Annual 
Symposium on Foundations of Computer Science
(FOCS), pages 305–313, 2008.

[31] M. Pˇatra¸scu and M. Thorup. Time-space trade-oﬀs
In Proc. 38th Annual ACM
for predecessor search.
Symposium on Theory of Computing (STOC), pages
232–240, 2006.

[32] R. Raman, V. Raman, and S. S. Rao. Succinct dynamic 
data structures. In Proc. 7th Annual Workshop
on Algorithms and Data Structures (WADS), LNCS
2125, pages 426–437, 2001.

[33] R. Raman, V. Raman, and S. S. Rao.

Succinct
indexable dictionaries with applications to encoding kary 
trees and multisets. In Proc. 13th Annual ACMSIAM 
Symposium on Discrete Algorithms (SODA),
pages 233–242, 2002.

[34] R. Raman and S. S. Rao. Succinct dynamic dictionaries 
and trees. In Proc. 30th International Colloquium
on Automata, Languages and Programming (ICALP),
LNCS 2719, pages 357–368, 2003.

[35] K. Sadakane. Compressed Suﬃx Trees with Full Functionality.
 Theory of Computing Systems, 41(4):589–
607, 2007.

149Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php