Indexing Variable Length Substrings for

Exact and Approximate Matching

Gonzalo Navarro1,(cid:2) and Leena Salmela2

1 Department of Computer Science, University of Chile

2 Department of Computer Science and Engineering,

gnavarro@dcc.uchile.cl

Helsinki University of Technology

lsalmela@cs.hut.fi

Abstract. We introduce two new index structures based on the q-gram
index. The new structures index substrings of variable length instead of
q-grams of ﬁxed length. For both of the new indexes, we present a method
based on the suﬃx tree to eﬃciently choose the indexed substrings so
that each of them occurs almost equally frequently in the text. Our
experiments show that the resulting indexes are up to 40 % faster than
the q-gram index when they use the same space.

1 Introduction

We consider indexing a text for exact and approximate string matching. Given a
text T = t1t2 . . . tn, a pattern P = p1p2 . . . pm, and an integer k, the approximate
string matching problem is to ﬁnd all substrings of the text such that the edit
distance between the substrings and the pattern is at most k. The edit distance
of two strings is the minimum number of character insertions, deletions, and
substitutions needed to transform one string into the other. We treat exact
string matching as a subcase of approximate string matching with k = 0.

Partitioning into exact search is a popular technique for approximate string
matching both in the online case [1,2,11,13], where the text is not preprocessed,
and in indexing approaches [3,4,7,10,12], where an index of the text is built.
Suppose that the edit distance between two strings, S and R, is at most k. If we
split S into k + 1 pieces, then at least one piece must have an exact occurrence
in R. In the online approach, we thus split the pattern into k + 1 pieces, search
for all the pieces in the text, and verify all the matches found for approximate
occurrences of the whole pattern using a dynamic programming algorithm that
runs in O(m2) time per veriﬁcation. In the indexing approach we have two
options. If we index all text positions, we can proceed as in the online case: split
the pattern into k + 1 pieces, search for all the pieces in the index, and verify all
the matches found. Another option is to index the text at ﬁxed intervals. Now we
search for all pattern substrings of corresponding length in the index and verify

(cid:2) Supported in part by Millennium Institute for Cell Dynamics and Biotechnology

(ICDB), Grant ICM P05-001-F, Mideplan, Chile.

J. Karlgren, J. Tarhio, and H. Hyyr¨o (Eds.): SPIRE 2009, LNCS 5721, pp. 214–221, 2009.
c(cid:2) Springer-Verlag Berlin Heidelberg 2009

Indexing Variable Length Substrings for Exact and Approximate Matching

215

1 2 3 4 5 6 7 8 9 0 1
a a a b a a b b a a $

2-gram positions diﬀerence coded positions
aa
ab
a$
ba
bb

1, 2, 5, 9 1, 1, 3, 4
3, 6
10
4, 8
7

3, 3
10
4, 4
7

Fig. 1. The 2-grams and the 2-gram index of the text T = “aaabaabbaa$”

the matches found to obtain the approximate occurrences of the whole pattern.
The q-gram index of Navarro and Baeza-Yates [7] takes the former approach,
while the q-samples index of Sutinen and Tarhio [12] utilizes the latter technique.
A problem of the q-gram index is that some q-grams may be much more
frequent than others, which raises veriﬁcation costs. A technique to choose the
k + 1 optimal pieces [7] was designed to alleviate this problem. In this work we
develop two new indexes, the preﬁx free and the preﬁx coalesced index, based
on the q-gram index [7]. The new indexes index substrings of variable length
instead of q-grams of ﬁxed length. The goal is to achieve roughly similar lengths
in all position lists. In the preﬁx free index the set of indexed substrings forms a
preﬁx free set, whereas in the preﬁx coalesced index this restriction is lifted. The
experimental results show that the new indexes are up to 40 % faster than the
q-gram index for the same space. Alternatively, the new indexes achieve as good
search times as the q-gram index using less space. For example, when m = 20
and k = 2 the new indexes are as fast as the q-gram index using 30 % less space.

2 q-Gram Index

In this section we review previous work on the q-gram index [7], which indexes all
q-grams of the text and uses partitioning into exact search to locate occurrences
of a pattern. The value of q is ﬁxed at construction time, and the q-grams that
occur in the text form the vocabulary of the index. Together with each q-gram
the index stores a list of positions where the q-gram occurs in the text. To save
space the position lists are diﬀerence coded and compressed with variable length
integer coding. The q-gram index can be built in O(n) time [7]. Figure 1 shows
the 2-grams of the text “aaabaabbaa$” and the corresponding 2-gram index.

To search for a pattern P with at most k diﬀerences, we ﬁrst extract k+1 nonoverlapping 
q-grams from the pattern. We then search for all these q-grams in the
index and ﬁnally use dynamic programming to verify the positions returned by
this search. For example, to search for the pattern P = “abbab” with at most k =
1 diﬀerence in the 2-gram index of Fig. 1, we ﬁrst extract two non-overlapping
2-grams from the pattern: “ab” and “ba”. Search on the index returns positions
3 and 6 for “ab” and positions 4 and 8 for “ba”. Verifying around these positions
we obtain the occurrences starting at positions 3 and 6.

In real texts some q-grams occur much more frequently than others, and if the
pattern is longer than (k+1)q, we have several diﬀerent ways of choosing the k+1

216

G. Navarro and L. Salmela

for (i = 1; i ≤ m; i++)
P [i, 0] = R[i, m + 1]
C[i, 0] = m + 1
for(r = 1; r ≤ k; r++)
for(i = 1; i ≤ m − r; i++)

P [i, r] = min(R[i, j] + P [j, r − 1] | i < j ≤ m − r + 1)
C[i, r] = j that minimizes the above expression

1.
2.
3.
4.
5.
6.
7.

Fig. 2. The dynamic programming algorithm for the optimal partitioning of the pattern

non-overlapping q-grams. Therefore we can speed up veriﬁcation considerably by
choosing the q-grams carefully. Furthermore, the pieces do not need to have the
exact length q. If a piece is shorter than q, we ﬁnd all q-grams starting with the
piece in the index and verify all their positions. If the piece is longer than q, we
search for the ﬁrst q-gram of the piece in the index. By allowing pieces shorter
than q, we can also search for patterns shorter than (k + 1)q.

Navarro and Baeza-Yates [7] give the following method for ﬁnding the optimal
partitioning of the pattern. It is relatively fast to compute the number of veriﬁcations 
a pattern piece will trigger. We use binary search to locate the q-gram(s) in
the index and obtain a contiguous region of q-grams. If we store the accumulated
list lengths, the number of veriﬁcations can easily be calculated by subtracting
the accumulated list lengths at the endpoints of the region. By performing this
search for all pattern pieces, we obtain a table R where R[i, j] gives the number of
veriﬁcations for the pattern piece pi . . . pj−1. Based on this table, we use dynamic
programming to compute the table P [i, k], which gives the total number of triggered 
veriﬁcations for the best partitioning for pi . . . pm with k diﬀerences, and
the table C[i, k], which gives the position where the next piece starts in order to
get P [i, k]. We then ﬁnd the smallest entry P [(cid:2)0, k] for 1 ≤ (cid:2)0 ≤ m−k, which gives
the ﬁnal number of veriﬁcations. The pattern pieces that give this optimal number 
of veriﬁcations begin at (cid:2)0, (cid:2)1 = C[(cid:2)0, k], (cid:2)2 = C[(cid:2)1, k − 1] . . . (cid:2)k = C[(cid:2)k−1, 1].
Figure 2 gives the pseudo code for the dynamic programming algorithm to ﬁnd
the optimal partitioning of the pattern. It runs in O(km2) time, whereas R is
easily built in O(qm log n) time, which can be reduced to O(qm log σ), where σ
is the alphabet size, if the q-gram vocabulary is stored in trie form.

3 Preﬁx Free Index

Our new variants of the q-gram index index substrings of varying length instead
of q-grams of ﬁxed length. The indexed substrings form the vocabulary of the
index. The aim is to choose the vocabulary so that each position of the text is
indexed and the lengths of the position lists are as uniform as possible. In the
ﬁrst variant, the preﬁx free index, we further require that the vocabulary is a
preﬁx free set, i.e. no indexed substring is a preﬁx of another indexed substring.
Let α be the threshold frequency and let the frequency of a string be the
number of occurrences it has in the text T . Note that the frequency of the empty

Indexing Variable Length Substrings for Exact and Approximate Matching

217

substring positions diﬀerence coded

substring positions diﬀerence coded

aaa
aab
aa$
ab
a$
b

1
2, 5
9
3, 6
10
4, 7, 8

positions
1
2, 3
9
3, 3
10
4, 3, 1

aa
aab
ab
a$
ba
bb

1, 9
2, 5
3, 6
10
4, 8
7

positions
1, 8
2, 3
3, 3
10
4, 4
7

(a) Preﬁx free index

(b) Preﬁx coalesced index

Fig. 3. A preﬁx free index and a preﬁx coalesced index for the text T = “aaabaabbaa$”

string is n. The vocabulary now consists of all such substrings S = s1 . . . si of
the text that the frequency of S is at most α and the frequency of the preﬁx
s1 . . . si−1 is greater than α. This choice ensures that the vocabulary is a preﬁx
free set and no position list is longer than α. Again the position lists are diﬀerence
coded and compressed with variable length integer coding. Figure 3(a) shows an
example of a preﬁx free index with threshold frequency three.

To search for a pattern P with at most k diﬀerences, we ﬁrst split the pattern
into k + 1 pieces and search for each piece in the index. If the indexed substrings
starting with the piece are longer than the piece, we return all positions associated 
with any substring starting with the piece. If an indexed substring is a preﬁx
of the piece, we return the positions associated with that indexed substring. The
positions returned by this search are then veriﬁed with the O(m2) dynamic programming 
algorithm to ﬁnd the approximate occurrences of the pattern. As an
example consider searching for the pattern P = “abbab” in the preﬁx free index
of Fig. 3(a) with at most k = 1 diﬀerence. We start by splitting the pattern into
two pieces: “ab” and “bab”. The search for “ab” in the index returns positions
3 and 6 and the search for “bab” returns positions 4, 7, and 8. We then verify
around these positions and ﬁnd the occurrences starting at positions 3 and 6.

Although the lengths of the position lists are more uniform than in the q-gram
index, we still beneﬁt from computing the optimal partitioning of the pattern.
First of all, the lengths of the position lists still vary, and thus the number of
veriﬁcations can be reduced by choosing pattern pieces with short position lists.
Secondly, if the pattern is too short to be partitioned into long enough pieces
such that we would get only one position list per pattern piece, it is not clear
how to select the pieces without the optimal partitioning technique.

Finding the optimal partitioning of the pattern works similarly to the q-gram
index. We ﬁrst use binary search to locate the indexed substring(s) corresponding
to each pattern piece pi . . . pj−1 for 1 ≤ i < j ≤ m + 1. This search returns
a contiguous region of indexed substrings. If we again store the accumulated
position list lengths, we can determine the number of triggered veriﬁcations fast.
This number is stored in the table R[i, j]. We then compute the tables P [i, k]
and C[i, k] and obtain from these tables the optimal partitioning of the pattern.
The overall time to ﬁnd the optimal partitioning is O(m2(log n + k)).

218

G. Navarro and L. Salmela

To choose the vocabulary of the index, we use a simpliﬁed version of the
technique of Klein and Shapira [5] for constructing ﬁxed length codes for compression 
of text. Their technique is based on the suﬃx tree of the text. A cut in
a suﬃx tree is deﬁned as an imaginary line that crosses exactly once each path
in the suﬃx tree from the root to one of the leaves. The lower border of a cut
is deﬁned to be the nodes immediately below the imaginary line that forms the
cut. Klein and Shapira show that a lower border of a cut forms a preﬁx free set
and a preﬁx of each suﬃx of the text is included in the lower border. Thus the
lower border of a cut can be used as a vocabulary in the preﬁx free index.

We choose the vocabulary as follows. First we build the suﬃx tree of the text
and augment it with the frequencies of the nodes. The frequency of a node is
the frequency of the corresponding substring of the text. We then traverse the
suﬃx tree in depth ﬁrst order. If the frequency of a node is at most the threshold
frequency α, we add the corresponding string S to the vocabulary and we also
add the corresponding leaves to the position list of the string S.
The suﬃx tree can be built in O(n) time. The traversal of the tree also takes
O(n) time and we do O(1) operations in each node. After the traversal the position 
lists are sorted, which takes O(n log α) total time. Finally the position lists
are diﬀerence coded and compressed taking O(n) total time. Thus the construction 
of the preﬁx free index takes O(n log α) time.

4 Preﬁx Coalesced Index

In the second new variant of the q-gram index, the preﬁx coalesced index, we
require that the vocabulary includes some preﬁx of each suﬃx of the text, and
if the vocabulary contains two strings S and R such that R is a proper preﬁx of
S, then all positions of the text starting with S are assigned only to the position
list of S. Again the position lists are diﬀerence coded and the diﬀerences are
compressed with variable length integer coding.

To choose the vocabulary, we build the suﬃx tree of the text and augment it
with the frequencies of the nodes. The suﬃx tree is traversed in depth ﬁrst order
so that the children of a node are traversed in descending order of frequency.
When we encounter a node whose frequency is at most the threshold frequency
α, we add the corresponding string to the vocabulary, subtract the original frequency 
of this node from the frequency of its parent node and reconsider adding
the string corresponding to the parent node to the vocabulary. When a string is
added to the vocabulary, we also add the leaves to its position list. Figure 3(b)
shows an example of a preﬁx coalesced index with threshold frequency two.
The suﬃx tree can again be built in O(n) time. Because we need to sort the
children of each node when traversing the suﬃx tree, the traversal now takes
O(n log σ) time, where σ is the size of the alphabet. After the traversal the
handling of the position lists takes O(n log α) as in the preﬁx free index. Thus
the construction of the preﬁx coalesced index takes O(n(log α + log σ)) time.

We reﬁne the searching procedure as follows. We again start by splitting the
pattern into k + 1 pieces and search the index for each piece. If the piece is a

Indexing Variable Length Substrings for Exact and Approximate Matching

219

preﬁx of several indexed substrings, we return all position lists associated with
these indexed substrings. If an indexed substring is a proper preﬁx of the piece,
we return only this position list. Otherwise searching on the index and optimal
partitioning of the pattern work exactly the same way as in the preﬁx free index.

5 Experimental Results

To save space our implementations of the preﬁx free and the preﬁx coalesced
indexes use a suﬃx array [6] instead of a suﬃx tree when constructing the index.
The traversal of the suﬃx tree is simulated using binary search on the suﬃx array.
For compressing the position lists in all the indexes, we use bytewise compression 
of the diﬀerences. In this scheme, the highest bit is 0 in the last byte of the
coding and 1 in other bytes. The integer is formed by concatenating the seven
lowest bits of all the bytes in the coding.

The experiments were run on a 1.0 GHz AMD Athlon dual core processor with
2 GB of memory, running Linux 2.6.23. The code is in C and compiled with gcc
using -O2 optimization. We used the 200 MB English text from the PizzaChili
site, http://pizzachili.dcc.uchile.cl, and the patterns are random substrings 
of the text. For each pattern length, 1,000 patterns were generated.

Figure 4 shows the search times for the q-gram index and the preﬁx free and
preﬁx coalesced indexes for various pattern lengths and values of k. The q-gram

m=10

m=20

)
s
(
 
e

t

a
c
o

l
 

o

t
 

e
m
T

i

)
s
(
 
e
a
c
o

t

l
 

o

t
 

e
m
T

i

 70

 60

 50

 40

 30

 20

 10

 0

 2.2

 70

 60

 50

 40

 30

 20

 10

 0

 2.2

q−gram index k=1
q−gram index k=0
pf index k=1
pf index k=0
pc index k=1
pc index k=0

 2.4

 2.6

 2.8

 3

 3.2

 3.4

 3.6

 3.8

 4

Space

m=30

q−gram index k=3
q−gram index k=2
q−gram index k=1
q−gram index k=0
pf index k=3
pf index k=2
pf index k=1
pf index k=0
pc index k=3
pc index k=2
pc index k=1
pc index k=0

 2.4

 2.6

 2.8

 3

 3.2

 3.4

 3.6

 3.8

 4

Space

)
s
(
 
e

t

a
c
o

l
 

o

t
 

e
m
T

i

)
s
(
 
e
a
c
o

t

l
 

o

t
 

e
m
T

i

 70

 60

 50

 40

 30

 20

 10

 0

 2.2

 70

 60

 50

 40

 30

 20

 10

 0

 2.2

q−gram index k=2
q−gram index k=1
q−gram index k=0
pf index k=2
pf index k=1
pf index k=0
pc index k=2
pc index k=1
pc index k=0

 2.4

 2.6

 2.8

 3

 3.2

 3.4

 3.6

 3.8

 4

Space

m=40

q−gram index k=4
q−gram index k=2
q−gram index k=0
pf index k=4
pf index k=2
pf index k=0
pc index k=4
pc index k=2
pc index k=0

 2.4

 2.6

 2.8

 3

 3.2

 3.4

 3.6

 3.8

 4

Space

Fig. 4. Search times for the diﬀerent indexes for various values of k and m. The space
fraction includes that of the text, so it is of the form 1 + index size
text size .

220

G. Navarro and L. Salmela

q−gram index
pf index
pc index

 2.5e+07

 2e+07

 1.5e+07

l

y
r
a
u
b
a
c
o
V
e
h

 

t
 
f

o

 

e
z
S

i

 1e+07

 5e+06

q−gram index
pf index
pc index

 300

 250

 200

 150

 100

 50

)
s
(
 

e
m

i
t
 

n
o

i
t
c
u
r
t
s
n
o
C

 0

 2.2

 2.4

 2.6

 2.8

 3.2

 3
Space

 3.4

 3.6

 3.8

 4

 0

 2.2

 2.4

 2.6

 2.8

 3

 3.2

 3.4

 3.6

 3.8

 4

Space

Fig. 5. The vocabulary size and the construction time of the q-gram index and the
preﬁx free and preﬁx coalesced indexes as a function of space usage

index was tested with 5 values of q: 4, 5, 6, 7, and 8. The preﬁx free index was
tested with 7 values for the threshold frequency α: 200, 500, 1,000, 2,000, 5,000,
10,000, and 20,000. The preﬁx coalesced index was also tested with 7 values for
the threshold frequency α: 100, 200, 500, 1,000, 2,000, 5,000, and 10,000. We see
that the new indexes generally achieve the same performance as the q-gram index
using less space. The preﬁx coalesced index allows more ﬂexibility in selecting
the vocabulary, and so the position list lengths are more uniform than in the
preﬁx free index. Thus the search times in the preﬁx coalesced index are slightly
lower than in the preﬁx free index. However, when we reduce the available space
the preﬁx free index becomes faster than the preﬁx coalesced index.

Figure 5 shows the vocabulary size for the diﬀerent indexes. We see that the
vocabulary of the q-gram index is much larger than the vocabulary of the preﬁx
free and the preﬁx coalesced indexes. Some of the q-grams are very frequent
in the text and their long position lists compress very eﬃciently, allowing the
q-gram index to use a larger vocabulary. In the preﬁx free and preﬁx coalesced
indexes the position lists have a more uniform length, and thus these lists do not
compress as well, so the vocabulary is much smaller. We can also see that the
vocabulary of the preﬁx free index is larger than the vocabulary of the preﬁx
coalesced index, again reﬂecting the lengths of the position lists.

Figure 5 also shows the construction time for the diﬀerent indexes. The construction 
time of the preﬁx free and preﬁx coalesced indexes increases only little
when space usage is increased because the most time consuming phase of their
construction is the construction of the suﬃx array, which takes the same time
regardless of the space usage of the ﬁnal index.

6 Conclusions and Further Work

We have presented two new indexes for exact and approximate string matching
based on the q-gram index. They index variable length substrings of the text
to achieve more uniform lengths of the position lists. The indexed substrings in
the preﬁx free index form a preﬁx free set, whereas in the preﬁx coalesced index

Indexing Variable Length Substrings for Exact and Approximate Matching

221

this restriction is lifted. Our experiments show that the new indexes are up to
40 % faster than the q-gram index for the same space. This shows that lists of
similar length are deﬁnitely beneﬁcial for the search performance, although they
are not as compressible and thus shorter substrings must be indexed.

Our techniques receive a parameter α, giving the maximum allowed list length,
and produce the largest possible index that fulﬁlls that condition. Instead, we
could set the maximum number of substrings to index, and achieve the most
uniform possible index of that vocabulary size. For this we would insert the
suﬃx tree nodes into a priority queue that sorts by frequency, and extract the
most frequent nodes (with small adaptations depending on whether the index is
preﬁx free or preﬁx coalesced). The construction time becomes O(n log n).

Future work involves extending more sophisticated techniques based on qgrams 
and q-samples, such as those requiring several nearby pattern pieces to be
found before triggering a veriﬁcation, and/or based on approximate matching of
the pattern pieces in the index [8,9].

References

1. Baeza-Yates, R., Navarro, G.: Faster approximate string matching. Algorithmica 
23(2), 127–158 (1999)

2. Baeza-Yates, R., Perleberg, C.: Fast and practical approximate string matching.

Information Processing Letters 59(1), 21–27 (1996)

3. Holsti, N., Sutinen, E.: Approximate string matching using q-gram places. In: Proc.

Finnish Symp. on Computer Science, pp. 23–32. University of Joensuu (1994)

4. Jokinen, P., Ukkonen, E.: Two algorithms for approximate string matching in static
texts. In: Tarlecki, A. (ed.) MFCS 1991. LNCS, vol. 520, pp. 240–248. Springer,
Heidelberg (1991)

5. Klein, S.T., Shapira, D.: Improved variable-to-ﬁxed length codes. In: Amir, A.,
Turpin, A., Moﬀat, A. (eds.) SPIRE 2008. LNCS, vol. 5280, pp. 39–50. Springer,
Heidelberg (2008)

6. Manber, U., Myers, G.: Suﬃx arrays: A new method for online string searches.

SIAM Journal on Computing 22(5), 935–948 (1993)

7. Navarro, G., Baeza-Yates, R.: A practical q-gram index for text retrieval allowing

errors. CLEI Electronic Journal 1(2) (1998)

8. Navarro, G., Baeza-Yates, R., Sutinen, E., Tarhio, J.: Indexing methods for approximate 
string matching. IEEE Data Engineering Bulletin 24(4), 19–27 (2001)
9. Navarro, G., Sutinen, E., Tarhio, J.: Indexing text with approximate q-grams.

Journal of Discrete Algorithms 3(2–4), 157–175 (2005)

10. Shi, F.: Fast approximate string matching with q-blocks sequences. In: Proc. WSP

1996, pp. 257–271. Carleton University Press (1996)

11. Sutinen, E., Tarhio, J.: On using q-gram locations in approximate string matching.
 In: Spirakis, P.G. (ed.) ESA 1995. LNCS, vol. 979, pp. 327–340. Springer,
Heidelberg (1995)

12. Sutinen, E., Tarhio, J.: Filtration with q-samples in approximate string matching.
In: Hirschberg, D.S., Meyers, G. (eds.) CPM 1996. LNCS, vol. 1075, pp. 50–63.
Springer, Heidelberg (1996)

13. Wu, S., Manber, U.: Fast text searching allowing errors. Communications of the

ACM 35(10), 83–91 (1992)

