Improved Dynamic Rank-Select Entropy-Bound

Structures(cid:2)

Rodrigo Gonz´alez and Gonzalo Navarro

Center for Web Research, Dept. of Computer Science, University of Chile

{rgonzale,gnavarro}@dcc.uchile.cl

Abstract. Operations rank and select over a sequence of symbols have
many applications to the design of succinct and compressed data structures 
to manage text collections, structured text, binary relations, trees,
graphs, and so on. We are interested in the case where the collections can
be updated via insertions and deletions of symbols. Two current solutions
stand out as the best in the tradeoﬀ of space versus time (considering all
the operations). One solution, by M¨akinen and Navarro, achieves compressed 
space (i.e., nH0 + o(n log σ) bits) and O(log n log σ) worst-case
time for all the operations, where n is the sequence length, σ is the
alphabet size, and H0 is the zero-order entropy of the sequence. The
other solution, by Lee and Park, achieves O(log n(1 + log σ
log log n )) amortized 
time and uncompressed space, i.e. n log σ + O(n) + o(n log σ) bits.
In this paper we show that the best of both worlds can be achieved.
We combine the solutions to obtain nH0 + o(n log σ) bits of space and
O(log n(1 + log σ
log log n )) worst-case time for all the operations. Apart from
the best current solution to the problem, we obtain several byproducts
of independent interest applicable to partial sums, text indexes, suﬃx
arrays, the Burrows-Wheeler transform, and others.

1 Introduction and Related Work

Compressed data structures aims at representing classical data structures such
as sequences, trees, graphs, etc., in little space while keeping the functionality of
the structure. That is, compressed data structures should operate without the
need to decompress them. This is a very active area of research stimulated by
today’s steep memory hierarchies and large available data sizes. See e.g. [15].

One of the most useful structures are the bit vectors with rank and select
operations: rank(B, i) gives the number of 1-bits in B[1, i] and select(B, i) gives
the position of the i-th 1 in B. This generalizes to sequences T [1, n] over an
alphabet Σ of size σ, where one aims at a (hopefully compressed) representation
eﬃciently supporting the following operations: access(T, i) returns the symbol
T [i]; rankc(T, i) returns the number of times symbol c appears in the preﬁx
T [1, i]; and selectc(T, i) returns the position of the i-th c in T .

(cid:3) Supported in part by Millennium Nucleus Center for Web Research, Grant P04-067F,
 Mideplan, and Fondecyt Grant 1-050493, Chile.

E.S. Laber et al. (Eds.): LATIN 2008, LNCS 4957, pp. 374–386, 2008.
c(cid:2) Springer-Verlag Berlin Heidelberg 2008

Improved Dynamic Rank-Select Entropy-Bound Structures

375

Improvements in rank/select operations on sequences have a great impact on
many other succinct data structures, especially on those aimed at text indexing,
but also labeled trees, structured texts, binary relations, graphs, and others [15].
The ﬁrst structure providing support for rank/select on a sequence of symbols 
was the wavelet tree [7,5]. Wavelet trees are perfectly balanced static trees of
height log σ (logarithms are in base 2 by default). They answer the three queries
in O(log σ) time, by working O(1) per tree level. They store a bitmap of length
n per level, which is preprocessed for constant-time binary rank/select queries.
Their total space requirement is n log σ + o(n log σ), where the extra sublinear
term is the space needed by the binary rank/select structures [14]. By representing 
those bitmaps in compressed form [16] the constant-time rank/select
queries are retained and the space becomes nH0(T ) + o(n log σ), where H0(T )
is the zero-order empirical entropy of T (that is,
, where c occurs
nc times in T ). Since the wavelet tree gives access(T, i) to any symbol T [i], it
can be used to replace T .

n log n
nc
nc

c∈Σ

(cid:2)

A stronger version of wavelet trees are multiary wavelet trees [3], which achieve
the same space but improve the query times to O(1 + log σ
log log n). The trick is to
make the tree ρ-ary for some ρ = O(logα n) and constant 0 < α < 1, so that its
height is reduced. Now the tree does not store a bitmap per level, but rather a
sequence over an alphabet of size ρ. They show how to do rank/select on those
sequences in constant time for such a small ρ.

In [10] they add dynamism to the sequences, by adding operations insertc(T, i)
inserts symbol c between T [i] and T [i + 1]; and delete(T, i) deletes T [i] from T .
They represent dynamic bitmaps B using nH0(B) + o(n) bits of space and
solve all operations in O(log n) time. This is done with a binary tree that stores
Θ(log2 n) bits at the leaves, and at internal nodes stores summary rank/select
information on the subtrees. For larger alphabets, a wavelet tree using dynamic
bitmaps yields a dynamic sequence representation that takes nH0(T )+o(n log σ)
bits and solves all the operations in time O(log n log σ).

Very recently, in [9] they manage to improve the time complexities of this
solution. They show that the O(log n) time complexities can be achieved for
alphabets of size up to σ = O(log n). They combine this tool with a multiary
wavelet tree to achieve O(log n(1 + log σ

log log n)) time.

The key to the success of [9] is a clever detachment of two roles of tree leaves
that are entangled in [10]: In the latter, the leaves are the memory allocation
unit (that is, whole leaves are allocated or freed), and also the information summarization 
unit (that is, the tree maintains information up to leaf granularity,
and the rest has to be collected by sequentially scanning a leaf). In [9] leaves
are the information summarization unit, but handle an internal linked list with
smaller memory allocation units. This permits moving symbols to accommodate
the space upon insertions/deletions within a leaf, without having to update summarization 
information for the data moved. This was the main bottleneck that
prevented the use of larger alphabets in O(log n) time in [10].

However, compared to [10], the work in [9] has several weaknesses: (1) it is
not compressed, but rather takes n log σ + O(n) + o(n log σ) bits of space; (2) in

376

R. Gonz´alez and G. Navarro

addition to not compressing T , the extra space includes an O(n) term, as shown;
(3) times are amortized, not worst-case.

log σ

In this paper we show that it is possible to obtain the best from both worlds.
We combine the works in [10,9] to obtain a structure that (1) takes nH0(T ) +
o(n log σ) bits of space, and (2) performs all the operations in O(log n(1 +
log log n)) worst-case time. (This is achieved even for the case where (cid:2)log n(cid:3)
changes and so does the length of the structure pointers in order to maintain
the promised space bounds.) The result becomes the most eﬃcient dynamic
representation of sequences, both in time and space, and we show immediate
applications to other succinct data structures such as compressed text indexes.
This combination is by no means simple. Some parts are not hard to merge,
such as the role detachment for leaves [9] with the compressed representation of
sequences [3] and multi-ary wavelet trees, plus the memory management techniques 
to support changes of (cid:2)log n(cid:3) within the same worst-case time bounds and
no extra space [10]. However, others require new algorithmic ideas. In [9] they
spend O(n) extra bits in bitmaps that maintain leaf-granularity information on
rank/select. We show that this can be replaced by dynamic partial sums, which
use sublinear space. However, we need σ partial sums and cannot aﬀord to update
them individually upon a leaf insertion/deletion. Hence we create a new structure 
where a collection of σ sequences are maintained in synchronization. The
second problem was that leaf splitting/merging in [9] triggered too many updates
to summarization data, which could not be handled in O(log n) worst-case time,
only in O(log n) amortized time. To get rid of this problem we redeﬁned the leaf
ﬁll ratio invariants, preferring a weaker condition that still ensures that leaves
are suﬃciently full and can be maintained within the O(log n)-worst-case-time
bound. Both ideas can be of independent interest.

As for the model of computation, our results (and all the mentioned ones) assume 
a RAM model with word size w = Ω(log n), so that operations on O(log n)
contiguous bits can be carried out in constant time. For the dynamic structures,
we always allocate ω(log n)-bit chunks of the same size (or a ﬁnite set of sizes),
which can be handled in constant time and asymptotically no extra space [17].

2 Collection of Searchable Partial Sums with Indels

(cid:2)
i

The Searchable Partial Sums with Indels (SPSI) problem [8] consists in maintaining 
a sequence S of nonnegative integers s1, . . . , sn, each one of k = O(log n)
l=1 sl; search(S, y) is
bits, supporting the following operations: sum(S, i) is
(cid:4)) ≥ y; update(S, i, x) updates si to si + x (x
the smallest i
can be negative as long as the result is not); insert(S, i, x) inserts a new integer
x between si−1 and si; and delete(S, i) deletes si from the sequence.

(cid:4) such that sum(S, i

It is possible to handle all these operations using kn + o(kn) bits of space and
O(log n) time per operation [10]. We now deﬁne an extension of this problem,
that we call Collection of Searchable Partial Sums with Indels (CSPSI). This
problem consists in maintaining a collection of σ sequences C = {S1, . . . , Sσ} of
nonnegative integers {sj
}1≤j≤σ,1≤i≤n, each one of k = O(log n) bits. We support

i

Improved Dynamic Rank-Select Entropy-Bound Structures

377

(cid:2)

i

l=1 sj

b2s2

1 ··· s1
b1+1 ··· s2

1 ··· s2
b1 s2
··· sσ

··· sσ
b1+1 ··· sσ

b1

b2

i to sj

i−1 and sj

(cid:4)) ≥ y; update(C, j, i, x) updates sj

l ; search(C, j, y) is the smallest
the following operations: sum(C, j, i) is
(cid:4) such that sum(C, j, i
i + x; insert(C, i)
i
i for all 1 ≤ j ≤ σ.; delete(C, i) deletes sj
inserts 0 between sj
i from the
sequence Sj for all 1 ≤ j ≤ σ; To perform delete(C, i) it must hold sj
i = 0 for
all 1 ≤ j ≤ σ. Next we show how to carry out all of these queries/operations in
O(σ + log n) time, using O(σkn) bits of space.
Data structure. We construct a red-black tree over C, where the size of each
2 log2 n to 2 log2 n bits (they are allocated to hold 2 log2 n bits1).
leaf goes from 1
1 ··· sσ
The leftmost leaf contains s1
b1, the second leftmost
b1+1 ··· s1
leaf contains s1
b2, and so on. The size of the
leftmost leaf is σkb1 bits, the size of the second leftmost leaf is σk(b2 − b1) bits,
and so on. The size of the leaves is variable and bounded, so b1, b2, . . . are such
2 log2 n ≤ σkb1, σk(b2−b1), . . . ≤ 2 log2 n hold2. Each internal node v stores
that 1
counters {rj(v)}1≤j≤σ and p(v), where rj(v) is the sum of the integers in the
left subtree for sequence Sj and p(v) is the number of positions stored in the
left subtree (for any sequence).
Computing sum(C, j, i). We traverse the tree to ﬁnd the leaf containing the
i-th position. We start with sum ← 0 and v ← root. If p(v) ≥ i we enter
the left subtree, otherwise we enter the right subtree with i ← i − p(v) and
sum ← sum+rj(v). We reach the leaf that contains the i-th position in O(log n)
time. Then we scan the leaf, summing up from where the sequence Sj begins, in
chunks of size 1
2 log n bits using a universal precomputed table Y , until we reach
position i. Table Y receives any possible sequence of dk bits, for d = (cid:7) 1
(cid:8),
2 log n
and gives the sum of the d k-bit numbers encoded. The last (at most d − 1)
integers must be added individually. (Note that if k > 1
2 log n we can just add
√
each number individually within the time bounds.) The sum query takes in total
O(log n) time, and table Y adds only O(
(cid:4) such that
Computing search(C, j, y). We enter the tree to ﬁnd the smallest i
(cid:4)) ≥ y. We start with pos ← 0 and v ← root. If rj(v) ≥ y we enter
sum(C, j, i
the left subtree, otherwise we enter the right subtree with y ← y − rj(v) and
pos ← pos + p(v). We reach the leaf that contains the i
(cid:4)-th position in O(log n)
time. Then we scan the leaf, summing up from where the sequence Sj begins,
in chunks of size 1
2 log n bits using table Y , until this sum is greater than y
(cid:4) integers; the answer is then pos + i
(cid:4). (Once an application of
after adding up i
the table exceeds y, we must reprocess the last chunk integer-wise.) The search
query takes in total O(log n) time.
Operation update(C, j, i, x). We proceed similarly to sum, updating rj(v) as we
traverse the tree. That is, we update rj(v) to rj(v) + x each time we go left from
v. When we reach the leaf we directly update sj
i + x. The update operation
takes in total O(log n) time.

n polylog(n)) bits of space.

i to sj

k

1 In most cases we ignore ﬂoors and ceilings for simplicity.
2 If σk > 2 log2 n, we just store σk bits per leaf. All the algorithms in the sequel are

simpliﬁed and the complexities are maintained.

378

R. Gonz´alez and G. Navarro

log n = O(1 + log n

σk

i for all j. This is done by ﬁrst copying the subsequences . . . sj

For the next operations, we note that a leaf has at most m = (cid:7) 2 log2 n

(cid:8) integers
from any sequence. Then a subsequence of a given sequence has at most mk bits.
So if we copy a subsequence in chunks of 1
2 log n bits, the subsequence will be
copied in 1 + 2mk
σ ) time in the RAM model (this requires shifting
bits, which in case it is not supported by the model, can be handled using small
universal tables of the kind of Y ). As we have σ sequences, we can copy a given
subsequence of them all in O(σ + log n) time. The next operations are solved by
a constant number applications of these copying operations.
Operation insert(C, i). We traverse the tree similarly to sum, updating p(v) as
we traverse the tree. That is, we increase p(v) by 1 each time we go left from
v. Then we copy the leaf arrived at to a new memory area, adding a 0 between
sj
i−1 and sj
i−1 for
all j, then adding 0 to each sequence, and ﬁnally copying the subsequences sj
i . . .
for all j. As we have just explained, this can be done in O(σ + log n) time.
If the new leaf uses more than 2 log2 n bits, the leaf is split in two. An overﬂowed 
leaf has m = (cid:7)2 log2 n/(σk)(cid:8) + 1 integers in each sequence. So we store
in the left leaf the ﬁrst (cid:7)m/2(cid:8) integers of each sequence and in the right leaf we
store the rest. These two copies can be done again in O(σ + log n) time. These
new leaves are made children of a new node μ. We compute each rj(μ) by scanning 
and summing on the left leaf. This summing can be done in O(σ + log n)
time using table Y . We also set p(μ) = (cid:7)m/2(cid:8). Finally, we check if we need to
rebalance the tree. If needed, the read-black tree is rebalanced with just one
rotation and O(log n) red-black tag updates. After a rotation we need to update
rj(· ) and p(· ) only for three nodes, which is easily done in O(σ) time. The insert
operation takes in total O(σ + log n) time.
Operation delete(C, i). We traverse the tree similarly to sum, updating p(v)
while we traverse the tree. That is, we decrease p(v) by 1 each time we go left
from v. Then we copy the leaf to a new memory area, deleting sj
i for all j,
similarly to insert, in O(σ + log n) time.

There are three possibilities after this deletion: (i) The new leaf uses more
2 log2 n bits, in which case we are done. (ii) The new leaf uses less than
than 1
2 log2 n and its sibling is also a leaf, in which case we merge it with its sibling,
1
again in O(σ + log n) time. Note that this merging removes the leaf’s parent but
does not require any recomputation of rj(· ) or p(· ). (iii) The new leaf uses less
2 log2 n and its sibling is an internal node μ, in which case by the red-black
than 1
tree properties we have that μ must have two leaf children. In this case we merge
our new leaf with the closest child of μ, updating the counters of μ in O(σ) time,
and letting μ replace the parent of our original leaf.

In cases (ii) and (iii), the merged leaf might use more than 2 log2 n bits. In
this case we split it again into two halves, just as we do in insert (and including
the recomputation of rj(· ) and p(· )). The tree might have to be rebalanced as
well. The delete operation takes in total O(σ + log n) time.
The space requirement is at most 4σkn bits for all the leaves. For each internal
node we have two pointers, a counter p(· ), and σ counters rj(· ) ≤ 2k·n, totalizing

Improved Dynamic Rank-Select Entropy-Bound Structures

379

O(log n) + σ(k + log n) = O(σ log n) bits per node. So, all the internal nodes use
O( σkn

log n ) bits. We have proved our claim.

log2 n σ log n) = O( σ2kn

Theorem 1. The Collection of Searchable Partial Sums with Indels problem
with σ sequences of n numbers of k bits can be solved, in a RAM machine
log n)) bits of space, supporting all the
of w = Ω(log n) bits, using O(σkn(1 + σ
operations in O(σ + log n) worst-case time. Note that, if σ = O(log n) the space
is O(σkn) and the time is O(log n).

We note that we have actually assumed that w = Θ(log n) in our space computation 
(as we have used w-bit system pointers). The general case w = Ω(log n)
can be addressed using exactly the same techniques developed in [10], using a
more reﬁned memory management with pointers of (log n)± 1 bits, and splitting
the sequence into three in a way that retains the worst-case complexities.

We also note that the space can be improved to σkn(1 + O( σ

log n)) by using a
ﬁner memory allocation policy for the leaves, just as done in the next sections
for sequences. The simpler result suﬃces for this paper.

3 Uncompressed Dynamic Rank-Select Structures for a

Small Alphabet

For a small alphabet of size σ = O(log n), we construct a red-black tree over
T [1, n] where each leaf contains a non-empty superblock of size up to 2 log2 n bits.
We will introduce invariants that guarantee that there are at most 1 + 2n log σ
log2 n
superblocks. Each internal node v stores counters r(v) and p(v), where r(v) is
the number of superblocks in the left subtree and p(v) is the number of symbols
stored in the left subtree. For each superblock i, we maintain sj
i , the number of
occurrences of symbol j in superblock i. We store all these sequences of numbers
using a Collection of Searchable Partial Sums with Indels, C. The length of each
sequence will be at most 2n log σ
log2 n integers, σ = O(log n) and k = O(log log n). So
the partial sums operate in O(log n) worst-case time.
Each superblock is further divided into blocks of

√
√
log n log n bits, so each
log n blocks. We maintain these blocks using a linked

superblock has up to 2
list. Only the last block could be not fully used.

A superblock storing less than log2 n bits is called sparse. Operations insert
and delete will maintain the invariant that no two consecutive sparse superblocks
may exist. This ensures that every consecutive pair of superblocks holds at least
log2 n bits from T , and thus that there are at most 1 + 2n log σ

log2 n superblocks.

The space usage of our structure is n log σ + O( n log σ√

log n), as σ = O(log n):
The text itself uses n log σ bits of space. The CSPSI uses O(σ log log n n log σ
log2 n ) =
O( n log log n log σ
) bits of space. Each pointer of the linked list of blocks uses
log n log n) blocks, totalizing O( n log σ√
O(log n) bits and we have O(
log n) bits. The
√
last block in each superblock is not necessarily fully used. We have at most
1 + 2n log σ
log n log n

log2 n superblocks, each of which can waste a full block of size

n log σ

log n

√

380

R. Gonz´alez and G. Navarro

bits, totalizing O( n log σ√
√
√
due to symbol misalignment, totalizing O( n log2 σ
log n log n) = O( n log log n log σ
· log n) = O( n log σ
The tree pointers and counters use O( n log σ
log2 n

log n) bits. Inside each block, we can lose at most log σ bits
log n log n ) bits.

log n ) bits.

2 log n bits (again, if σ > 1

√
Now we show how to carry out all the queries/operations in O(log n) time.
First, it is important to notice that each block can be scanned or shifted in
log n
time, using tables that process chunks of 1
2 log n we can
√
process each symbol individually within the time bounds). Given that there are
log n) blocks in a superblock, we can scan or shift elements within
at most O(
a superblock in O(log n) time, even considering block boundaries.
Computing access(T, i). We traverse the tree to ﬁnd the leaf containing the i-th
position. We start with sb ← 1 and pos ← i. if p(v) ≥ pos we enter the left
subtree, otherwise we enter the right subtree with sb ← sb + r(v) and pos ←
pos − p(v). We reach the leaf that contains the i-th position in O(log n) time.
Then we directly access the pos-th symbol of sb. Note that, within the same
O(log n) time, we can extract any O(log n)-bit long sequence of symbols from T .
Computing rankc(T, i). We ﬁnd the leaf containing the i-th position, just as
for access. Then we scan superblock sb from the ﬁrst block summing up the
occurrences of c up to the position pos, using a table Z to sum the c’s. We add
to this quantity sum(C, c, sb − 1), the number of times that c appears before
superblock sb. The rank query takes in total O(log n) time. Table Z is of the
same spirit of Y and requires O(σ
Computing selectc(T, i). We calculate j = search(C, c, i); this way we know
(cid:4)-th appearance of c within
that the i-th c belongs to superblock j and it is the i
(cid:4) = i − sum(C, c, j − 1). Then we traverse the tree to ﬁnd the
superblock j, for i
leaf representing superblock j. We start with sb ← j and pos ← 0. if r(v) ≥ sb we
enter the left subtree, otherwise we enter the right subtree with sb ← sb − r(v)
and pos ← pos + p(v). We reach the correct leaf in O(log n) time. Then we
(cid:4)-th
scan superblock j from the ﬁrst block, searching for the position of the i
appearance of symbol c within superblock j, using table Z. To this position we
add pos to obtain the ﬁnal result. The select query takes in total O(log n) time.
Operation insertc(T, i). We obtain sb and pos just like in the access query, except
that we start with pos ← i − 1, so as to insert right after position i − 1. Then,
if superblock sb contains room for one more symbol, we insert c right after the
pos-th position of sb, by shifting the symbols through the blocks as explained.
We also carry out update(C, c, sb, 1) and retraverse the path from the root to sb
adding 1 to p(v) each time we go left from v.

√
n polylog(n)) = O(

If this insertion causes an overﬂow in the last block of sb, we simply add a new
block at the end of the linked list to hold the trailing symbol (which is usually
not the same symbol inserted). In this case we ﬁnish in O(log n) time.

If, instead, the superblock is full, we cannot carry out the insertion yet. We
ﬁrst move one symbol to the previous superblock (creating a new one if this is not
possible). We ﬁrst deleted(T,·) the ﬁrst symbol d from block sb, which cannot
cause an underﬂow of sb. Now, we check how many symbols does superblock

√

n polylog(n)) bits.

Improved Dynamic Rank-Select Entropy-Bound Structures

381
sb − 1 have: we traverse the tree searching for it, and deduce its size from the
r(v) counters in the tree. If it can hold one more symbol, we insertd(T,·) the
removed symbol d at the end of superblock sb − 1. This recursive invocation to
insert will not overﬂow leaf sb − 1.
If superblock sb− 1 is also full or does not exist, then we are entitled to create
a sparse superblock between sb − 1 and sb, without breaking the invariant on
sparse superblocks. We create such an empty superblock and insert symbol d
into it, using the following procedure: We retraverse the path from the root to
sb, updating r(v) to r(v) + 1 each time we go left from v. When we arrive again
at leaf sb we create a new node μ with r(μ) = 1 and p(μ) = 1. Its left child is
the new empty superblock, where the single symbol d is inserted, and its right
child is sb. We also execute insert(C, sb) and update(C, sb, d, 1).

Finally, we check if we need to rebalance the tree. If it is needed, it can be
done with one rotation and O(log n) red-black tag updates, given that we use a
red-black tree. After a rotation we need to update r(· ) and p(· ) only for three
nodes. These updates can be done in O(1) time.

Now that we have ﬁnally made room to carry out the original insertion, we
rerun insertc(T, i) and it will not overﬂow again. The whole insert operation
takes O(log n) time.
Operation delete(T, i). We obtain sb and pos just as in the access query, updating
p(v) to p(v) − 1 each time we go left from v. Then we delete the pos-th position
(let c be the symbol deleted) of the sb-th superblock, by shifting the symbols
back through the blocks. If this deletion empties the last block, we free it. In
any case we call update(C, c, sb,−1) on the partial sums.

There are three possibilities after this deletion: (i) superblock sb is not sparse
after the deletion, in which case we are done; (ii) sb was already sparse before
the deletion, in which case we have only to check that it has not become empty;
(iii) sb turned to sparse due to the deletion, in which case we have to care about
the invariant on sparse superblocks.
If superblock sb becomes empty, we retraverse the path from the root to it,
updating r(v) to r(v)− 1 each time we go left from v, in O(log n) time. When we
arrive at leaf sb again, we delete it and do operation delete(C, sb). Finally, we
check if we need to rebalance the tree, in which case one rotation and O(log n)
red-black tag updates suﬃce, just as for insertion. After a rotation we also need
to update r(· ) and p(· ) only for three nodes. These updates take O(1) time.
If, instead, superblock sb turned to sparse, we make sure that neither superblocks 
sb− 1 or sb + 1 are also sparse. If they are not, then superblock sb can
become sparse and hence we ﬁnish without further intervention.
If superblock sb−1 is sparse, we delete(T,·) its last symbol d, and insertd(T,·)
it in the beginning of superblock sb. This recursive call brings no problems
because sb − 1 is already sparse, and we restore the non-sparse status of sb. If
superblock sb − 1 becomes empty, we delete it just as explained for the case of
superblock sb. The action is symmetric if sb − 1 is not sparse but sb + 1 is.

The delete operation takes in total O(log n) time.

382

R. Gonz´alez and G. Navarro

Theorem 2. Given a text T of length n over a small alphabet of size σ =
O(log n), the Dynamic Sequence with Indels problem under RAM model with
word size w = Ω(log n) can be solved using n log σ + O( n log σ√
log n) bits of space,
supporting all the operations in O(log n) worst-case time.

We note again that we have actually assumed that w = Θ(log n) in our space
computation, and that the general case w = Ω(log n) can be obtained using
exactly the same techniques developed in [10, Sec. 4.5, 4.6, and 6.4].

4 Compressed Dynamic Rank-Select Structures

Thm. 2 can be extended to use a compressed sequence representation, by just
changing the way we store/manage the blocks. The key idea is to detach the
representational and the physical (i.e., compressed) sizes of the storage units at
diﬀerent levels.

We use the same red-black tree over T [1, n], where each leaf contains a nonempty 
superblock representing up to 2 log2 n bits of the original text T (they will
actually store more or less bits depending on how compressible is the portion
of T they represent). The same superblock splitting/merging policy related to
sparse superblocks is used. Each internal node has the same counters and they
are managed in the same way. So all the queries/operations are exactly the same
up to the superblock level. Compression is encapsulated within superblocks.

√
In physical terms, a superblock is divided into blocks just as before, and they
are still of the same physical size,
log n log n bits. Depending on compressibility,
blocks will represent more or less symbols of the original text, as their physical
size is ﬁxed.

In a block of

In logical terms, a superblock is be divided into subblocks representing 1

2 log n
bits (that is, 1
2 logσ n symbols3) from T . We represent each subblock using the
(c, o)-pair encoding of [3]: The c part is of ﬁxed width and tells how many
occurrences of each alphabet symbol are there in the subblock; whereas the o
part is of variable width and gives the identiﬁer of the subblock among those
sharing the same c component. Each c component uses at most σ log log n bits;
while the o components use at most 1
2 log n bits each, and overall add up to
nH0(T ) + O(n log σ/ log n) bits [3, Sec. 3.1].

√
log n log n bits, we store as many subblocks as they ﬁt, wasting
at most σ log log n + 1
2 log n unused bits at the end. The universal tables (like
Y ) used to sequentially process the blocks in chunks of Θ(log n) bits must now
be modiﬁed to process the compressed sequence of (c, o) pairs. This is complex
because an insertion in a subblock introduces a displacement that propagates
over all the subblocks of the block, which must be completely recomputed and
rewritten (the physical size of the whole superblock can even double!). Fortunately 
all those tedious details have been already sorted out in [10, Sec. 5.2,
6.1, and 6.2], where their superblocks play the role of our blocks, and their tree
rearrangements are not necessary for us because we are within a leaf now. Their

3 Or just one symbol if 1

2 logσ n < 1.

Improved Dynamic Rank-Select Entropy-Bound Structures

383

“partial blocks” mechanism is also not useful for us, because we can tolerate
those propagations to extend over all the blocks of our superblocks. Hence only
the last block of our superblocks is not as full as misalignments permit.

The time achieved in [10] is O(1) per Θ(log n) physical bits. Even in the worst
case (where compression does not work at all in the superblock), the number of
physical bits will be 2 log2 n
2 log n) = O(log2 n + σ log n log log n),
and thus the time to solve any query or carry out any update on a superblock
will be O(log n + σ log log n).

2 log n (σ log log n + 1

1

We compute the space usage of these new structures, where it diﬀers from the
uncompressed version: The text itself uses nH0(T ) + O( σn log log n
) bits. Inside
each block, we can lose at most O(σ log log n + log n) bits due to misalignments,
totalizing O( n log σ(σ log log n+log n)
) bits. The extra space to operate the (c, o)
√
encoding is O(

n σ polylog(n)) bits.

log n log n

logσ n

√

The time and space complexities depend sharply on σ. Thus the solution is
indeed of interest only for rather small σ = o(log n/ log log n). For such a small
alphabet we have the following theorem. Again, all the issues of varying (cid:2)log n(cid:3)
and the case w = ω(log n) are handled just as in [10, Sec. 4.5, 4.6, and 6.4].

Theorem 3. Given a text T of length n over a small alphabet of size σ =
o(log n/ log log n) and zero-order entropy H0(T ), the Dynamic Sequence with
Indels problem under RAM model with word size w = Ω(log n) can be solved
using nH0(T ) + O( n log σ√
log n) bits of space, supporting all operations in O(log n)
worst-case time.

√
To extend our results for a large alphabet of size σ = Ω(log n/ log log n), we use a
generalized ρ-ary wavelet tree [3] over T , where ρ = Θ(
log n). Essentially, this
generalized wavelet tree makes a sequence with the ﬁrst log ρ bits of the symbols
at the ﬁrst level, the next log ρ bits at the second level (where the symbols with
the same ﬁrst log ρ bits are grouped in the same child of the root), and so on.
The tree has O(logρ σ) = O( log σ
log log n) levels. We store on each level a sequence
over an alphabet of size ρ, which can be handled using the solution of Thm. 3,
for which ρ is small enough. Hence each operation takes O(log n) time per level,
adding up O(log n log σ

log log n) worst-case time.

As shown in [3], the sum of the zero-order-entropy representations of the
sequences at each level adds up to the zero-order entropy of T . In addition, the
generalized ρ-ary wavelet tree handles changes in (cid:2)log n(cid:3) automatically, as this
is encapsulated within each level. We thus obtain our main theorem, where we
have included the case of small σ as well. We recall that, within the same time,
access can retrieve O(logσ n) consecutive symbols from T .
Theorem 4. Given a text T of length n over an alphabet of size σ and zeroorder 
entropy H0(T ), the Dynamic Sequence with Indels problem under RAM
model with word size w = Ω(log n) can be solved using nH0(T ) + O( n log σ√
log n) bits
of space, supporting all the operations in O(log n(1 + log σ
log log n)) worst-case time.

384

R. Gonz´alez and G. Navarro

5 Conclusions

We have shown that the best two existing solutions to the Dynamic Sequence
with Indels problem [10,9] can be merged so as to obtain the best from both. This
merging is not trivial and involves some byproducts that can be of independent
interest. We show now a couple of immediate consequences of our result.

In [10,11] it is shown that a wavelet tree built over the Burrows-Wheeler
Transform T bwt of a text T [1], and compressed using the (c, o) pair technique,
achieves high-order entropy space, namely nHh(T ) + o(n log σ) for any h + 1 ≤
α logσ n and constant 0 < α < 1, where Hh(T ) is the h-th order empirical entropy
of T [13]. This is used in [10] to obtain a dynamic text index that handles a
collection C of texts and permits searching for patterns, extracting text snippets,
and inserting/deleting texts in/from the collection. Using the deﬁnitions of [10,
Sec. 7] and using the same sampling step, we can state a stronger version of
those theorems:

Theorem 5. The Dynamic Text Collection problem can be solved with a data
structure of size nHh(C) + o(n log σ) + O(σh+1 log n + m log n + w) bits, simultaneously 
for all h. Here n is the length of the concatenation of m texts,
C = 0 T10 T2 ··· 0 Tm, and we assume that σ = o(n) is the alphabet size and
w = Ω(log n) is the machine word size under the RAM model. The structure
supports counting of the occurrences of a pattern P in O(|P| log n(1 + log σ
log log n))
time, and inserting and deleting a text T in O(|T| log n(1 + log σ
log log n)) time. After 
counting, any occurrence can be located in time O(log n + logσ n log log n).
Any substring of length (cid:9) from any T in the collection can be displayed in time
log log n)). For h ≤ (α logσ n)−1, for any constant
O(log n+logσ n log log n+(cid:9)(1+ log σ
0 < α < 1, the space complexity simpliﬁes to nHh(C)+o(n log σ)+O(m log n+w).
Another important application that derives from this one is the compressed
construction of text indexes. For example, a variant of the FM-index [3] requires
h-th entropy space once built, but in order to build it we need O(n log n) bits
of space. The previous theorem can be used in order to build the FM-index of
a text by starting with an empty collection and inserting the text T of interest.
Our new results make this process faster.

Theorem 6. The Alphabet-Friendly FM-index of a text T [1, n] over an alphabet
of size σ can be built using nHh(T ) + o(n log σ) bits, simultaneously for all h ≤
(α logσ n) − 1 and any constant 0 < α < 1, in time O(n log n(1 + log σ
We note that this is the same asymptotic space required for the ﬁnal, static, FMindex 
[3]. This result has some obvious consequences on building suﬃx arrays
[12] and computing the Burrows-Wheeler Transform [1] of T in little space, which
we omit for lack of space.

log log n)).

In [2] they show that the Dynamic Bit-Sequence with Indels problem can be
log log n) time for all operations, using O(n) bits of space. Combining
(log log n)2 ) time

solved in O( log n
with wavelet trees one achieves O(n log σ) bits of space and O( log n log σ

Improved Dynamic Rank-Select Entropy-Bound Structures

385

for Dynamic Sequences with Indels . This raises the challenge of achieving that
time within nH0 + o(n log σ) bits of space.

Alternatively, one would like to improve the space to high-order entropy (not
only for the Dynamic Text Collection problem, but for the Dynamic Sequence
with Indels problem). This has not been achieved even if we limit the operations
to access, insert, and delete. The dynamic support for the existing nHk-space
solutions to access is currently null or very rudimentary [18,6,4].

References

1. Burrows, M., Wheeler, D.: A block sorting lossless data compression algorithm.

Technical Report 124, Digital Equipment Corporation (1994)

2. Chan, H., Hon, W., Lam, T., Sadakane, K.: Compressed indexes for dynamic text

collections. ACM TALG 3(2), 21 (2007)

3. Ferragina, P., Manzini, G., M¨akinen, V., Navarro, G.: Compressed representations

of sequences and full-text indexes. ACM TALG 3(2) (2007) (article 20)

4. Ferragina, P., Venturini, R.: A simple storage scheme for strings achieving entropy

bounds. Theoretical Computer Science 372(1), 115–121 (2007)

5. Foschini, L., Grossi, R., Gupta, A., Vitter, J.: When indexing equals compression:
Experiments with compressing suﬃx arrays and applications. ACM TALG 2(4),
611–639 (2006)

6. Gonz´alez, R., Navarro, G.: Statistical encoding of succinct data structures. In:
Lewenstein, M., Valiente, G. (eds.) CPM 2006. LNCS, vol. 4009, pp. 295–306.
Springer, Heidelberg (2006)

7. Grossi, R., Gupta, A., Vitter, J.: High-order entropy-compressed text indexes. In:

Proc. 14th SODA, pp. 841–850 (2003)

8. Hon, W.-K., Sadakane, K., Sung, W.-K.: Succinct data structures for searchable
partials sums. In: Ibaraki, T., Katoh, N., Ono, H. (eds.) ISAAC 2003. LNCS,
vol. 2906, pp. 505–516. Springer, Heidelberg (2003)

9. Lee, S., Park, K.: Dynamic rank-select structures with applications to run-length
encoded texts. In: Ma, B., Zhang, K. (eds.) CPM 2007. LNCS, vol. 4580, pp. 95–
106. Springer, Heidelberg (2007)

10. M¨akinen, V., Navarro, G.: Dynamic entropy-compressed sequences and full-text
indexes. In: Lewenstein, M., Valiente, G. (eds.) CPM 2006. LNCS, vol. 4009,
pp. 307–318. Springer, Heidelberg (2006), ftp://ftp.dcc.uchile.cl/pub/users/
gnavarro/dynamic.ps.gz

11. M¨akinen, V., Navarro, G.: Implicit compression boosting with applications to selfindexing.
 In: Ziviani, N., Baeza-Yates, R. (eds.) SPIRE 2007. LNCS, vol. 4726, pp.
214–226. Springer, Heidelberg (2007)

12. Manber, U., Myers, G.: Suﬃx arrays: A new method for on-line string searches.

SIAM Journal of Computing 22, 935–948 (1993)

13. Manzini, G.: An analysis of the Burrows-Wheeler transform. Journal of the

ACM 48(3), 407–430 (2001)

14. Munro, I.: Tables. In: Chandru, V., Vinay, V. (eds.) FSTTCS 1996. LNCS,

vol. 1180, pp. 37–42. Springer, Heidelberg (1996)

15. Navarro, G., M¨akinen, V.: Compressed full-text indexes. ACM Computing Surveys 
39(1) (2007) (article 2)

386

R. Gonz´alez and G. Navarro

16. Raman, R., Raman, V., Rao, S.: Succinct indexable dictionaries with applications
to encoding k-ary trees and multisets. In: Proc. 13th SODA, pp. 233–242 (2002)
17. Raman, R., Rao, S.S.: Succinct dynamic dictionaries and trees. In: Baeten, J.C.M.,
Lenstra, J.K., Parrow, J., Woeginger, G.J. (eds.) ICALP 2003. LNCS, vol. 2719,
pp. 357–368. Springer, Heidelberg (2003)

18. Sadakane, K., Grossi, R.: Squeezing succinct data structures into entropy bounds.

In: Proc. 17th SODA, pp. 1230–1239 (2006)

