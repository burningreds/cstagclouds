Simple, Fast, and Eﬃcient

Natural Language Adaptive Compression(cid:1)

Nieves R. Brisaboa1, Antonio Fari˜na1, Gonzalo Navarro2, and Jos´e R. Param´a1

1 Database Lab., Univ. da Coru˜na, Facultade de Inform´atica

Campus de Elvi˜na s/n, 15071 A Coru˜na, Spain

{brisaboa,fari,parama}@udc.es

2 Center for Web Research, Dept. of Computer Science, Univ. de Chile

Blanco Encalada 2120, Santiago, Chile

gnavarro@dcc.uchile.cl

Abstract. One of the most successful natural language compression
methods is word-based Huﬀman. However, such a two-pass semi-static
compressor is not well suited to many interesting real-time transmission 
scenarios. A one-pass adaptive variant of Huﬀman exists, but it
is character-oriented and rather complex. In this paper we implement
word-based adaptive Huﬀman compression, showing that it obtains very
competitive compression ratios. Then, we show how End-Tagged Dense
Code, an alternative to word-based Huﬀman, can be turned into a faster
and much simpler adaptive compression method which obtains almost
the same compression ratios.

1 Introduction

Transmission of compressed data is usually composed of four processes: compression,
 transmission, reception, and decompression. The ﬁrst two are carried
out by a sender process and the last two by a receiver. This abstracts from communication 
over a network, but also from writing a compressed ﬁle to disk so
as to load and decompress it later. In some scenarios, especially the latter, compression 
and transmission usually complete before reception and decompression
start.

There are several interesting real-time transmission scenarios, however, where
those processes should take place concurrently. That is, the sender should be able
to start the transmission of compressed data without preprocessing the whole
text, and simultaneously the receiver should start reception and decompress the
text as it arrives. Real-time transmission is usually of interest when communicating 
over a network. This kind of compression can be applied, for example,
to interactive services such as remote login or talk/chat protocols, where small

(cid:1) This word is partially supported by CYTED VII.19 RIBIDI Project. It is also funded
in part (for the Spanish group) by MCyT (PGE and FEDER) grant(TIC2003-06593)
and (for the third author) by Millennium Nucleus Center for Web Research, Grant
(P01-029-F), Mideplan, Chile.

A. Apostolico and M. Melucci (Eds.): SPIRE 2004, LNCS 3246, pp. 230–241, 2004.
c(cid:1) Springer-Verlag Berlin Heidelberg 2004

Simple, Fast, and Eﬃcient Natural Language Adaptive Compression

231

messages are exchanged during the whole communication time. It might also
be relevant to transmission of Web pages, so that the exchange of (relatively
small) pages between a server and a client along the time enables adaptive compression 
by installing a browser plug-in to handle decompression. This might be
also interesting for wireless communication with hand-held devices with little
bandwidth and processing power.

Real-time transmission is handled with so-called dynamic or adaptive compression 
techniques. These perform a single pass over the text (so they are also
called one-pass) and begin compression and transmission as they read the data.
Currently, the most widely used adaptive compression techniques belong to the
Ziv-Lempel family [1]. When applied to natural language text, however, the
compression ratios achieved by Ziv-Lempel are not that good (around 40%).

Statistical two-pass techniques, on the other hand, use a semi-static model. A
ﬁrst pass over the text to compress gathers global statistical information, which
is used to compress the text in a second pass. The computed model is transmitted
prior to the compressed data, so that the receiver can use it for decompression.
Classic Huﬀman code [11] is a well-known two-pass method. Its compression ratio
is rather poor for natural language texts (around 60%). In recent years, however,
new Huﬀman-based compression techniques for natural language have appeared,
based on the idea of taking the words, not the characters, as the source symbols
to be compressed [13]. Since in natural language texts the frequency distribution
of words is much more biased than that of characters, the gain in compression is
enormous, achieving compression ratios around 25%-30%. Additionally, since in
Information Retrieval (IR) words are the atoms searched for, these compression
schemes are well suited to IR tasks. Word-based Huﬀman variants focused on
fast retrieval are presented in [7], where a byterather 
than bit-oriented coding
alphabet speeds up decompression and search.

Two-pass codes, unfortunately, are not suitable for real-time transmission.
Hence, developing an adaptive compression technique with good compression ratios 
for natural language texts is a relevant problem. In [8, 9] a dynamic Huﬀman
compression method was presented. This method was later improved in [12, 14].
In this case, the model is not previously computed nor transmitted, but rather
computed and updated on the ﬂy both by sender and receiver.

However, those methods are characterrather 
than word-oriented, and thus
their compression ratios on natural language are poor. Extending those algorithms 
to build a dynamic word-based Huﬀman method and evaluating its compression 
eﬃciency and processing cost is the ﬁrst contribution of this paper. We
show that the compression ratios achieved are in most cases just 0.06% over
those of the semi-static version. The algorithm is also rather eﬃcient: It compresses 
4 megabytes per second in our machine. On the other hand, it is rather
complex to implement.

Recently, a new word-based byte-oriented method called End-Tagged Dense
Code (ETDC) was presented in [3]. ETDC is not based on Huﬀman at all. It
is simpler and faster to build than Huﬀman codes, and its compression ratio
is only 2%-4% over the corresponding word-based byte-oriented Huﬀman code.

