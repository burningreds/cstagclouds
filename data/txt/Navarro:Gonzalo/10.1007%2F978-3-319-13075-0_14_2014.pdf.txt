Top-k Term-Proximity in Succinct Space

J. Ian Munro1, Gonzalo Navarro2, Jesper Sindahl Nielsen3, Rahul Shah4,

and Sharma V. Thankachan5(B)

1 Cheriton School of CS, University of Waterloo, Waterloo, Canada

2 Department of CS, University of Chile, Santiago, Chile

imunro@uwaterloo.ca

3 MADALGO, Aarhus University, Aarhus, Denmark

gnavarro@dcc.uchile.cl

4 School of EECS, Louisiana State University, Louisiana, USA

jasn@cs.au.dk

5 School of CSE, Georgia Institute of Technology, Georgia, USA

rahul@csc.lsu.edu

sharma.thankachan@gmail.com

Abstract. Let D = {T1, T2, . . . , TD} be a collection of D string documents 
of n characters in total, that are drawn from an alphabet set
Σ = [σ]. The top-k d ocument retrieval problem is to preprocess D into
a data structure that, given a query (P [1..p], k), can return the k documents 
of D most relevant to pattern P . The relevance is captured using
a predeﬁned ranking function, which depends on the set of occurrences
of P in Td. For example, it can be the term frequency (i.e., the number 
of occurrences of P in Td), or it can be the term proximity (i.e., the
distance between the closest pair of occurrences of P in Td), or a patternindependent 
importance score of Td such as PageRank. Linear space and
optimal query time solutions already exist for this problem. Compressed
and compact space solutions are also known, but only for a few ranking 
functions such as term frequency and importance. However, space
eﬃcient data structures for term proximity based retrieval have been
evasive. In this paper we present the ﬁrst sub-linear space data structure
for this relevance function, which uses only o(n) bits on top of any compressed 
suﬃx array of D and solves queries in time O((p + k) polylog n).

1 Introduction

Ranked document retrieval, that is, returning the documents that are most relevant 
to a query, is the fundamental task in Information Retrieval (IR) [1,6].
Muthukrishnan [19] initiated the study of this family of problems in the more
general scenario where both the documents and the queries are general strings
over arbitrary alphabets, which has applications in several areas [20]. In this scenario,
 we have a collection D = {T1, T2, . . . , TD} of D string documents of total
length n, drawn from an alphabet Σ = [σ], and the query is a pattern P [1..p]

Funded in part by NSERC of Canada and the Canada Research Chairs program,
Fondecyt Grant 1-140796, Chile, and NSF Grants CCF–1017623, CCF–1218904.

c(cid:2) Springer International Publishing Switzerland 2014
H.-K. Ahn and C.-S. Shin (Eds.): ISAAC 2014, LNCS 8889, pp. 169–180, 2014.
DOI: 10.1007/978-3-319-13075-0 14

170

J. I. Munro et al.

over Σ. Muthukrishnan considered a family of problems called thresholded document 
listing: given an additional parameter K, list only the documents where
some function score(P, d) of the occurrences of P in Td exceeded K. For example,
the document mining problem aims to return the documents where P appears at
least K times, whereas the repeats problem aims to return the documents where
two occurrences of P appear at distance at most K. While document mining has
obvious connections with typical term-frequency measures of relevance [1,6], the
repeats problem is more connected to various problems in bioinformatics [4,12].
Also notice that the repeats problem is closely related to the term proximity
based document retrieval in IR ﬁeld [5,29,32–34]. Muthukrishnan achieved optimal 
time for both problems, with O(n) space (in words) if K is speciﬁed at
indexing time and O(n log n) if speciﬁed at query time.

A more natural version of the thresholded problems, as used in IR, is top-k
retrieval: Given P and k, return k documents with the best score(P, d) values.
Hon et al. [15,16] gave a general framework to solve top-k problems for a wide
variety of score(P, d) functions, which takes O(n) space, allows k to be speciﬁed
at query time, and solves queries in O(p + k log k) time. Navarro and Nekrich
[22] reduced the time to O(p + k), and ﬁnally Shah et al. [30] achieved time
O(k) given the locus of P in the generalized suﬃx tree of D. Recently, Munro
et al. [18] introduced an O(n)-word index, that can ﬁnd the top-kth document
in O(log k) time, once the locus of P is given.

The problem is far from closed, however. Even the O(n) space (i.e., O(n log n)
bits) is excessive compared to the size of the text collection itself (n log σ bits),
and in data-intensive scenarios it often renders all these solutions impractical by
a wide margin. Hon et al. [16] also introduced a general framework for succinct
indexes, which use o(n) bits1 on top of a compressed suﬃx array (CSA) [21],
which represents D in a way that also provides pattern-matching functionalities
on it, all within space (|CSA|) close to that of the compressed collection. A CSA
ﬁnds the suﬃx array interval of P [1..p] in time ts(p) and retrieves any cell of the
suﬃx array or its inverse in time tSA. Hon et al. achieved O(ts(p)+k tSA log3+ε n)
query time, using O(n/ logε n) bits. Subsequent work (see [20,26]) improved the
initial result up to O(ts(p) + k tSA log2 k logε n) [24], and also considered compact
indexes, which may use o(n log n) bits on top of the CSA. For example, these
achieve O(ts(p) + k tSA log k logε n) query time using n log σ + o(n) further bits
k) query time using n log D + o(n log n) further bits [25].
[14], or O(ts(p) + k log
However, all these succinct and compact indexes work exclusively for the term
frequency (or closely related, e.g., TF-IDF) measure of relevance. For the simpler
case where documents have a ﬁxed relevance independent of P , succinct indexes
achieve O(ts(p) + k tSA log k logε n) query time [3], and compact indexes using
n log D + o(n log D) bits achieve O(ts(p) + k log(D/k)) time [10]. On the other
hand, there have been no succinct nor compact indexes for the term proximity
measure of relevance, tp(P, d) = min{{|i−j| > 0, Td[i..i+p−1] = Td[j..j+p−1] =
P} ∪ {∞}}. In this paper we introduce the ﬁrst such result as follows.

∗

1 If D = o(n), which we assume for simplicity in this paper. Otherwise it is

D log(n/D) + O(D) + o(n) bits.

Top-k Term-Proximity in Succinct Space

171

Theorem 1. Using a CSA plus o(n) bits data structure, one can answer top-k
term proximity queries in O(ts(p) + (log2 n + k(tSA + log k log n)) log2+ε n) time,
for any constant ε > 0.

2 Basic Concepts
Let T[1..n] = T1 ◦ T2 ◦ ··· TD be the text (from an alphabet Σ = [σ] ∪ {$})
obtained by concatenating all the documents in D. Each document is terminated
with a special symbol $, which does not appear anywhere else. A suﬃx T[i..n]
of T belongs to Td iﬀ i is in the region corresponding to Td in T. Thus, it holds
d = 1 + rankB(i − 1), where B[1..n] is a bitmap deﬁned as B[j] = 1 iﬀ T[j] = $
and rankB(i − 1) is the number of 1s in B[1..i − 1]. This operation is computed
in O(1) time on a representation of B that uses D log(n/D) + O(D) + o(n) bits
[28]. For simplicity, we assume D = o(n), and thus B uses o(n) bits.

Suﬃx Tree [31] of T is a compact trie containing all of its suﬃxes, where the
ith leftmost leaf, (cid:5)i, represents the ith lexicographically smallest suﬃx. It is also
called the generalized suﬃx tree of D, GST. Each edge in GST is labeled by a
string, and path(x) is the concatenation of the edge labels along the path from
the GST root to node x. Then path((cid:5)i) is the ith lexicographically smallest suﬃx
of T. The highest node x with path(x) preﬁxed by P [1..p] is the locus of P , and
is found in time O(p) from the GST root. The GST uses O(n) words of space.
Suﬃx Array [17] of T, SA[1..n], is deﬁned as SA[i] = n + 1 − |path((cid:5)i)|, the
starting position in T of the ith lexicographically smallest suﬃx of T. The suﬃx
range of P is the range SA[sp, ep] pointing to the suﬃxes that start with P ,
T[SA[i]..SA[i] + p − 1] = P for all i ∈ [sp, ep]. Also, (cid:5)sp (resp., (cid:5)ep) are the
leftmost (resp., rightmost) leaf in the subtree of the locus of P .

Compressed Suﬃx Array [8,11,21] of T , CSA, is a compressed representation of
SA, and usually also of T. Its size in bits, |CSA|, is O(n log σ) and usually much
less. The CSA ﬁnds the interval [sp, ep] of P in time ts(p). It can output any value
SA[i], and even of its inverse permutation, SA−1[i], in time tSA. For example, a
CSA using nHh(T) + o(n log σ) bits [2] gives ts(p) = O(p) and tSA = O(log1+ n)
for any constant  > 0, where Hh is the hth order empirical entropy.

Compressed Suﬃx Tree of T, CST, is a compressed representation of GST, where
node identiﬁers are their corresponding suﬃx array ranges. The CST can use
o(n) bits on top of a CSA [23] and compute (among others) the lowest common
ancestor (LCA) of two leaves (cid:5)i and (cid:5)j, in time O(tSA log n), and the Weiner link
Wlink(a, v), which leads to the node with path label a◦ path(v), in time O(tSA).2
Orthogonal Range Successor/Predecessor. Given n points in [n]×[n], an O(n log n)-
bit data structure can retrieve the point in a given rectangle with lowest
2 Using O(n/ log n) bits and no special implementation for operations SA−1[SA[i]±1].

172

J. I. Munro et al.

y-coordinate value, in time O(log n) for any constant  > 0 [27]. Combined with
standard range tree partitioning, the following result easily follows.
(cid:4) points in [n] × [n] × [n], a structure using O(n

(cid:4) log2 n) bits
Lemma 1. Given n
can support the following query in O(log1+ n) time, for any constant  > 0: ﬁnd
(cid:4)] with the lowest/highest x-coordinate.
the point in a region [x, x

(cid:4)]× [y, y

(cid:4)]× [z, z

3 An Overview of Our Data Structure

The top-k term proximity is related to a problem called range restricted searching,
 where one must report all the occurrences of P that are within a text range
T[i..j]. It is known that succinct data structures for that problem are unlikely
to exist in general, whereas indexes of size |CSA| + O(n/ log n) bits do exist
for patterns longer than Δ = log2+ n (see [13]). Therefore, our basic strategy
will be to have a separate data structure to solve queries of length p = π, for
each π ∈ {1, . . . , Δ}. Patterns with length p > Δ can be handled with a single
succinct data structure. More precisely, we design two diﬀerent data structures
that operate on top of a CSA:

– An O(n log log n/(π logγ n))-bits structure for handling queries of ﬁxed length

p = π, in time O(ts(p) + k(tSA + log log n + log k) π logγ n).

– An O(n/ log n+(n/Δ) log2 n)-bits structure for handling queries with p > Δ

in time O(ts(p) + Δ(Δ + tSA) + k log k log2 n(tSA + Δ log1+ n)).
By building the ﬁrst structure for every π ∈ {1, . . . , Δ}, any query can
be handled using the appropriate structure. The Δ structures for ﬁxed pattern 
length add up to O(n(log log n)2/ logγ n) = o(n/ logγ/2 n) bits, whereas
that for long patterns uses O(n/ log n) bits. By choosing ε = 4 = 2γ, the
space is O(n/ logε/4 n) bits. As for the time, the structures for ﬁxed p = π are
most costly for π = Δ, where their time is k(tSA + log log n + log k) Δ logγ n.
Adding up the time of the second structure, we get O(ts(p) + Δ(Δ + k(tSA +
log k log1+ n) log2 n), which is upper bounded by O(ts(p) + (log2 n + k(tSA +
log k log n)) log2+ε n). This yields Theorem 1.

Now we introduce some formalization to convey the key intuition. The term
proximity tp(P, d) can be determined by just two occurrences of P in Td, which
are the closest up to ties. We call them critical occurrences, and a pair of two
closest occurrences is a critical pair. There can be multiple critical pairs.
Deﬁnition 1. An integer i ∈ [1, n] is an occurrence of P in Td if the suﬃx
T[i..n] belongs to Td and T[i..i + p − 1] = P [1..p]. The set of all occurrences of
P in T is called Occ(P ).

Deﬁnition 2. An occurrence id of P in Td is a critical occurrence if there exists
d of P in Td such that |id − i
| = tp(P, d). The pair (id, i
(cid:4)
(cid:4)
(cid:4)
another occurrence i
d)
d
is called a critical pair of Td with respect to P .

Top-k Term-Proximity in Succinct Space

173

A key concept in our solution is that of candidate sets of occurrences, which
contain suﬃcient information to solve the top-k query (note that, due to ties, a
top-k query may have multiple valid answers).

Deﬁnition 3. Let Topk(P, k) be a valid answer for the top-k query (P, k). A
set Cand(P, k) ⊆ Occ(P ) is a candidate set of Topk(P, k) if, for each document
identiﬁer d ∈ Topk(P, k), there exists a critical pair (id, i
(cid:4)
d) of Td with respect to
(cid:4)
P such that id, i
d
Lemma 2. Given a CSA on D, a valid answer to query (P, k) can be computed
from Cand(P, k) in O(z log z) time, where z = |Cand(P, k)|.

∈ Cand(P, k).

Proof. Sort the set Cand(P, k) and traverse it sequentially. From the occurrences
(cid:4)
within each document Td, retain the closest consecutive pair (id, i
d), and ﬁnally
|. This takes O(z log z) time.
report k documents with minimum values |id − i
(cid:4)
d
We show that this returns a valid answer set. Since Cand(P, k) is a candidate
d) for d ∈ Topk(P, k), so this critical pair
(cid:4)
set, it contains a critical pair (id, i
| value) is chosen for each d ∈ Topk(P, k).
(or another with the same |id − i
(cid:4)
d
If the algorithm returns an answer other than Topk(P, k), it is because some
/∈ Topk(P, k) with the same
document d ∈ Topk(P, k) is replaced by another d
(cid:4)
(cid:7)(cid:8)
score tp(P, d

(cid:4)) = |id(cid:2) − i
d(cid:2)| = |id − i
(cid:4)
(cid:4)
d

| = tp(d).

Our data structures aim to return a small candidate set (as close to size k as

possible), from which a valid answer is eﬃciently computed using Lemma 2.
4 Data Structure for Queries with Fixed p = π ≤ Δ
We build an o(n/π)-bits structure for handling queries with pattern length p = π.

Lemma 3. There is an O(n log log n/(π logγ n))-bits data structure solving queries
(P [1..p], k) with p = π in O(ts(p) + k(tSA + log log n + log k)π logγ n) time.

The idea is to build an array F[1, n] such that a candidate set of size O(k),
for any query (P, k) with p = π, is given by {SA[i], i ∈ [sp, ep]∧ F[i] ≤ k}, [sp, ep]
being the suﬃx range of P . The key property to achieve this is that the ranges
[sp, ep] are disjoint for all the patterns of a ﬁxed length π. We build F as follows.

1. Initialize F[1..n] = n + 1.
2. For each pattern Q of length π,

(a) Find the suﬃx range [α, β] of Q.
(b) Find the list Tr1 , Tr2 , Tr3 , . . . of documents in the ascending order of

tp(Q,·) values (ties broken arbitrarily).

(c) For each document Trκ containing Q at least twice, choose a unique
(cid:4) ∈ [α, β],
(cid:4)]) is a critical pair of Trκ with respect

critical pair with respect to Q, that is, choose two elements j, j
(cid:4)
such that (irκ , i
rκ) = (SA[j], SA[j
(cid:4)] = κ.
to Q. Then assign F[j] = F[j

174

J. I. Munro et al.

The following observation is immediate.

Lemma 4. For a query (P [1..p], k) with p = π and suﬃx array range [sp, ep] for
P , the set {SA[j], j ∈ [sp, ep] ∧ F[j] ≤ k} is a candidate set of size at most 2k.
Proof. A valid answer for (P, k) are the document identiﬁers r1, . . . , rk considered
at construction time for Q = P . For each such document Trκ, 1 ≤ κ ≤ k, we
(cid:4) ∈ [sp, ep], and set
(cid:4)
rκ) = (SA[j], SA[j
have found a critical pair (irκ , i
(cid:4)] = κ ≤ k. All the other values of F[sp, ep] are larger than k (or ∞).
F[j] = F[j
The size of the candidate set is thus at most 2k (or less, if there are less than k
(cid:7)(cid:8)
documents where P occurs twice).
However, we cannot aﬀord to maintain F explicitly within the desired space
bounds. Therefore, we replace F by a sampled array F(cid:4). The sampled array is
(cid:4) = π logγ n and storing the logarithm of
built by cutting F into blocks of size π
the minimum value for each block. This will increase the size of the candidate
sets by a factor π

(cid:4)]), for j, j

(cid:4). More precisely, F(cid:4)[1, n/π
F(cid:4)

[j] = (cid:11)log min F [(j − 1)π

(cid:4)] is deﬁned as
(cid:4)
+ 1..jπ

(cid:4)

](cid:12) .

Since F(cid:4)[j] ∈ [0.. log n], the array can be represented using n log log n/ logγ n
bits. We maintain F(cid:4) with a multiary wavelet tree [9], which maintains the space
in O(n log log n/ logγ n) bits and, since the alphabet size is logarithmic, supports
in constant time operations rank and select on F(cid:4). Operation rank(j, κ) counts
the number of occurrences of κ in F(cid:4)[1..j], whereas select(j, κ) gives the position
of the jth occurrence of κ in F(cid:4).

Query Algorithm. To answer a query (P [1..p], k) with p = π using a CSA and
F(cid:4), we compute the suﬃx range [sp, ep] of P in time ts(p), and then do as follows.
1. Among all the blocks of F overlapping the range [sp, ep], identify those containing 
an element ≤ 2(cid:5)log k(cid:6), that is, compute the set

(cid:4)]}.

(cid:4)(cid:12) ∧ F(cid:4)
(cid:4) ∈ [(j − 1)π

[j] ≤ (cid:11)log k(cid:12)}.
(cid:4) + 1, jπ

(cid:4)(cid:12) ≤ j ≤ (cid:11)ep/π
(cid:4)], j ∈ Sblocks ∧ j

Sblocks = {j,(cid:11)sp/π
2. Generate Cand(P, k) = {SA[j
3. Find the query output from the candidate set Cand(P, k), using Lemma 2.
For step 1, the wavelet tree representation of F(cid:4) generates Sblocks in time O(1+
|Sblocks|): All the 2t positions3 j ∈ [sp, ep] with F(cid:4)[j] = t are j = select(rank(sp−
1, t)+i, t) for i ∈ [1, 2t]. We notice if there are no suﬃcient documents if we obtain
a j > ep, in which case we stop.
The set Cand(P, k) is a candidate set of (P, k), since any j ∈ [sp, ep] with
F[j] ≤ k belongs to some block of Sblocks. Also the number of j ∈ [sp, ep] with
F[j] ≤ 2(cid:5)log k(cid:6) is at most 2 · 2(cid:5)log k(cid:6) ≤ 4k, therefore |Sblocks| ≤ 4k.
(cid:4)), and it is generated in step 2 in
(cid:4)). Finally, the time for generating the ﬁnal output using Lemma 2
(cid:4)))) = O(kπ logγ n(log k + log log n + log π)). By considering that

Now, Cand(P, k) is of size |Sblocks|π

(cid:4) = O(kπ

time O(k tSA π
(cid:4) log(kπ
is O(kπ
π ≤ Δ = log2+ n, we obtain Lemma 3.
3 Except for t = 0, which has 2 positions.

Top-k Term-Proximity in Succinct Space

175

5 Data Structure for Queries with p > Δ

We prove the following result in this section.

Lemma 5. There is an O(n/ log n+(n/Δ) log2 n)-bits structure solving queries
(P [1..p], k), with p > Δ, in O(ts(p)+Δ(Δ+tSA)+k log k log2 n(tSA+Δ log1+ n))
time.

We start with a concept similar to that of a candidate set, but weaker in the

sense that it is required to contain only one element of each critical pair.

Deﬁnition 4. Let Topk(P, k) be a valid answer for the top-k query (P, k). A set
Semi(P, k) ⊆ [n] is a semi-candidate set of Topk(P, k) if it contains at least one
critical occurrence id of P in Td for each document identiﬁer d ∈ Topk(P, k).
Our structure in this section generates a semi-candidate set Semi(P, k). Then,
a candidate set Cand(P, k) is generated as the union of Semi(P, k) and the set
of occurrences of P that are immediately before and immediately after every
position i ∈ Semi(P, k). This is obviously a valid candidate set. Finally, we apply
Lemma 2 on Cand(P, k) to compute the ﬁnal output.

5.1 Generating a Semi-candidate Set

This section proves the following result.

Lemma 6. A structure of O(n(log log n)2/ logδ n) bits plus a CSA can generate
a semi-candidate set of size O(k log k logδ n) in time O(tSA k log k logδ n).

Let Leaf(x) (resp., Leaf(y)) be the set of leaves in the subtree of node x (resp.,

y) in GST, Leaf(x\y) = Leaf(x) \ Leaf(y). The following lemma holds.
Lemma 7. The set Semi(path(y), k)∪{SA[j], (cid:5)j ∈ Leaf(x\y)} is a semi-candidate
set of (path(x), k).
Proof. Let d ∈ Topk(path(x), k), then our semi-candidate set should contain id
(cid:4)
(cid:4)
or i
d for some critical pair (id, i
d). If there is some such critical pair where id or
d are occurrences of path(x) but not of path(y), then (cid:5)j or (cid:5)j(cid:2) are in L(x\y),
(cid:4)
i
(cid:4)
(cid:4)] = i
d, and thus our set contains it. If, on the other
for SA[j] = id and SA[j
(cid:4)
(cid:4)
hand, both id and i
d are occurrences of path(y) for all critical pairs (id, i
d), then
tp(path(y), d) = tp(path(x), d), and the critical pairs of path(x) are the critical
d for some such critical pair. (cid:7)(cid:8)
(cid:4)
pairs of path(y). Thus Semi(y, k) contains id or i
Our approach is to precompute and store Semi(path(y), k) for carefully selected
nodes y ∈ GST and k values, so that any arbitrary Semi(path(x), k) set can be
computed eﬃciently. The succinct framework of Hon et al. [16] is adequate for this.

Node Marking Scheme. The idea [16] is to mark a set Markg of nodes in GST
based on a grouping factor g: Every gth leaf is marked, and the LCA of any two
consecutive marked leaves is also marked. Then the following properties hold.

176

J. I. Munro et al.

1. |Markg| ≤ 2n/g.
2. If there exists no marked node in the subtree of x, then |Leaf(x)| < 2g.
3. If it exists, then the highest marked descendant node y of any unmarked

node x is unique, and |Leaf(x\y)| < 2g.

We use this idea, and a later reﬁnement [14]. Let us ﬁrst consider a variant
of Lemma 6 where k = κ is ﬁxed at construction time. We use a CSA and an
O(n/ logδ n)-bit CST on it, see Section 2. We choose g = κ log κ log1+δ n and,
for each node y ∈ Markg, we explicitly store a candidate set Semi(path(y), κ) of
size κ. The space required is O(|Markg|κ log n) = O(n/(log κ logδ n)) bits.

To solve a query (P, κ), we ﬁnd the suﬃx range [sp, ep], then the locus
node of P is x = LCA((cid:5)sp, (cid:5)ep). Then we ﬁnd y = LCA((cid:5)g(cid:5)sp/g(cid:6), (cid:5)g(cid:7)ep/g(cid:8)), the
highest marked node in the subtree of x. Then, by the given properties of
the marking scheme, combined with Lemma 7, a semi-candidate set of size
O(g + κ) = O(κ log κ log1+δ n) can be generated in O(tSAκ log κ log1+δ n) time.
To reduce this time, we employ dual marking scheme [14]. We identify a
(cid:4) = κ log κ logδ n. To avoid confusion, we call
larger set Markg(cid:2) of nodes, for g
(cid:4) ∈ Markg(cid:2), we precomthese 
prime nodes, not marked nodes. For each node y
(cid:4)), κ) of size κ. Let y be the (unique) highest
pute a candidate set Semi(path(y
(cid:4) to indicate which of
marked node in the subtree of y
(cid:4)), κ). By the
the κ nodes stored in Semi(path(y), κ) also belong to Semi(path(y
(cid:4)), κ) \ Semi(path(y), κ) must
same proof of Lemma 7, elements in Semi(path(y
(cid:4)\y). Then, instead of explicitly storing the
have a critical occurrence in Leaf(y
(cid:4)), κ) \ Semi(path(y), κ), we store their leftcritical 
positions id ∈ Semi(path(y
(cid:4)\y). Storing κ such positions in leaf order requires
to-right position in Leaf(y
O(κ log(g/κ)) = O(κ log log n) bits, using for example gamma codes. The total
space is O(|Markg(cid:2)|κ log log n) = O(n log log n/(log κ logδ)) bits.

(cid:4). Then we store κ bits in y

Now we can apply the same technique to obtain a semi-candidate set from
(cid:4)+κ) = O(κ log κ logδ n), in time O(tSAκ log κ logδ n).
Markg(cid:2), yet of smaller size O(g
We are now ready to complete the proof Lemma 6. We maintain structures as
described for all the values of κ that are powers of 2, in total O((n log log n/ logδ n)·
(cid:2)
i=1 1/i) = O(n(log log n)2/ logδ n) bits of space. To solve a query (P, k), we
compute κ = 2(cid:5)log k(cid:6)
< 2k and return the semi-candidate set of (P, κ) using the
corresponding structure.

log D

5.2 Generating the Candidate Set

The problem boils down to the task of, given P [1..p] and an occurrence q, ﬁnding
the occurrence of P closest to q. In other words, ﬁnding the ﬁrst and the last
occurrence of P in T[q + 1..n] and T[1..q + p − 1], respectively. We employ suﬃx
sampling to obtain the desired space-eﬃcient structure. The idea is to exploit
the fact that, if p > Δ, then for every occurrence q of P there must be an integer
j = Δ(cid:11)q/Δ(cid:12) (a multiple of Δ) and t ≤ Δ, such that P [1..t] is a suﬃx of T[1..j]
and P [t + 1..p] is a preﬁx of T[j + 1..n]. We call q an oﬀset-t occurrence of P .
Then, Cand(P, k) can be computed as follows:

Top-k Term-Proximity in Succinct Space

177

1. Find Semi(P, k) using Lemma 6.
2. For each q ∈ Semi(P, k) and t ∈ [1, Δ], ﬁnd the oﬀset-t occurrences of P that

are immediately before and immediately after q.

3. The occurrences found in the previous step, along with the elements in

Semi(P, k), constitute Cand(P, k).

In order to perform step 2 eﬃciently, we maintain the following structures.

– Sparse Suﬃx Tree (SST): A suﬃx T[Δi + 1..n] is a sparse suﬃx, and
the trie of all sparse suﬃxes is a sparse suﬃx tree. The sparse suﬃx range
of a pattern Q is the range of the sparse suﬃxes in SST that are preﬁxed
by Q. Given the suﬃx range [sp, ep] of a pattern, its sparse suﬃx range
[ssp, sep] can be computed in constant time by maintaining a bitmap B[1..n],
where B[j] = 1 iﬀ T[SA[j]..n] is a sparse suﬃx. Then ssp = 1 + rankB(sp −
1) and sep = rankB(sp). Since B has n/Δ 1s, it can be represented in
O((n/Δ) log Δ) bits while supporting rankB operation in constant time for
any Δ = O(polylog n) [28].

– Sparse Preﬁx Tree (SPT): A preﬁx T[1..Δi] is a sparse preﬁx, and the trie
of the reverses of all sparse preﬁxes is a sparse preﬁx tree. The sparse preﬁx
range of a pattern Q is the range of the sparse preﬁxes in SPT with Q as a
suﬃx. The SPT can be represented as a blind trie [7] using O((n/Δ) log n)
bits. Then the search for the sparse preﬁx range of Q can be done in O(|Q|)
time, by descending using the reverse of Q4. Note that the blind trie may
return a fake node when Q does not exist in the SPT.
– Orthogonal Range Successor/Predecessor Search Structure over a
set of (cid:11)n/Δ(cid:12) points of the form (x, y, z), where the yth leaf in SST corresponds 
to T[x..n] and the zth leaf in SPT corresponds to T[1..(x − 1)]. The
space needed is O((n/Δ) log2 n) bits (recall Lemma 1).

The total space of the structures is O((n/Δ) log2 n) bits. They allow computing 
ﬁrst oﬀset-t occurrence of P in T[q + 1..n] as follows: ﬁnd [sspt, sept] and
(cid:4)
(cid:4)
t], the sparse suﬃx range of P [t + 1..p] and the sparse preﬁx range of
[ssp
t, sep
P [1..t], respectively. Then, using an orthogonal range successor query, ﬁnd the
point (e,·,·) with the lowest x-coordinate value in [q + t + 1, n] × [sspt, sept] ×
t]. Then, e−t is the answer. Similarly, the last oﬀset-t occurrence of P in
(cid:4)
(cid:4)
[ssp
t, sep
T[1..q−1] is f−t, where (f,·,·) is the point in [1, q+t−1]×[sspt, sept]×[ssp
(cid:4)
(cid:4)
t]
t, sep
with the highest x-coordinate value.
First, we compute all the ranges [sspt, sept] using the SST. This requires
knowing the interval SA[spt, ept] of P [t + 1..p] for all 1 ≤ t ≤ Δ. We compute
these by using the CSA to search for P [Δ + 1..p] (in time at most ts(p)), which
gives [spΔ, epΔ], and then computing [spt−1, ept−1] = Wlink(P [t], [spt, ept]) for
t = Δ − 1, . . . , 1. Using an o(n)-bits CST (see Section 2), this takes O(Δ tSA)
time. Then the SST ﬁnds all the [sspt, sept] values in time O(Δ). Thus the time
spent on the SST searches is O(ts(p) + Δ tSA).

4 Using perfect hashing to move in constant time towards the children.

178

J. I. Munro et al.

Second, we search the SPT for reverse pattern preﬁxes of lengths 1 to Δ,
and thus they can all be searched for in time O(Δ2). Since the SPT is a blind
(cid:4)
(cid:4)
t] it returns are the correct
trie, it might be either that the intervals [ssp
t, sep
interval of P [1..t], or that P [1..t] does not terminate any sparse preﬁx. A simple
way to determine which is the case is to perform the orthogonal range search as
explained, asking for the successor e0 of position 1, and check whether the resulting 
position, e0−t, is an occurrence of P , that is, whether SA−1[e0 − t] ∈ [sp, ep].
This takes O(tSA + log1+ n) time per veriﬁcation. Considering the searches plus
veriﬁcations, the time spent on the SPT searches is O(Δ(Δ + tSA + log1+ n)).
(cid:4)
(cid:4)
Finally, after determining all the intervals [sspt, sept] and [ssp
t], we
t, sep
perform O(|Semi(P, k)|Δ) orthogonal range searches for positions q, in time
O(|Semi(P, k)|Δ log1+ n), and keep the closest one for each q.

Lemma 8 . Given a semi-candidate set Semi(P, k), where p > Δ, a candidate
set Cand(P, k) of size O(|Semi(P, k)|) can be computed in time O(ts(p) + Δ(Δ +
tSA + |Semi(P, k)| log1+ n)) using a data structure of O((n/Δ) log2 n) bits.

Thus, by combining Lemma 6 using δ = 2 (so its space is o(n/ log n) bits)

and Lemma 8, we obtain Lemma 5.

6 Concluding Remarks

We have obtained the ﬁrst succinct result for top-k term-proximity queries. The
following additional results will be presented in the full version of this paper.

1. Another trade-oﬀ for top-k term-proximity queries with space and query
time 2n log σ + o(n log σ) + O(n log log n) bits and O(p + k log k log1+ n),
respectively. Notice that, when log log n = o(log σ), the trade-oﬀ matches
with the best known result for top-k term-frequency queries [15].
2. In a more realistic scenario, score(·,·) is a weighted sum of PageRank, termfrequency 
and term-proximity with predeﬁned non-negative weights [33].
Top-k queries with such ranking functions can be handled using an index of
space 2n log σ + o(n log σ) bits in time O(p + k log k log4+ n).

References

1. Baeza-Yates, R., Ribeiro-Neto, B.: Modern Information Retrieval, 2nd edn.

Addison-Wesley (2011)

2. Belazzougui, D., Navarro, G.: Alphabet-independent compressed text indexing. In:
Demetrescu, C., Halld´orsson, M.M. (eds.) ESA 2011. LNCS, vol. 6942, pp. 748–759.
Springer, Heidelberg (2011)

3. Belazzougui, D., Navarro, G., Valenzuela, D.: Improved compressed indexes for

full-text document retrieval. J. Discr. Alg. 18, 3–13 (2013)

4. Benson, G., Waterman, M.: A fast method for fast database search for all knucleotide 
repeats. Nucleic Acids Research 22(22) (1994)

Top-k Term-Proximity in Succinct Space

179

5. Broschart, A., Schenkel, R.: Index tuning for eﬃcient proximity-enhanced query
processing. In: Geva, S., Kamps, J., Trotman, A. (eds.) INEX 2009. LNCS, vol.
6203, pp. 213–217. Springer, Heidelberg (2010)

6. B¨uttcher, S., Clarke, C.L.A., Cormack, G.: Information Retrieval: Implementing

and Evaluating Search Engines. MIT Press (2010)

7. Ferragina, P., Grossi, R.: The string B-tree: A new data structure for string search

in external memory and its applications. J. ACM 46(2), 236–280 (1999)

8. Ferragina, P., Manzini, G.: Indexing compressed text. J. ACM 52(4), 552–581

(2005)

9. Ferragina, P., Manzini, G., M¨akinen, V., Navarro, G.: Compressed representations

of sequences and full-text indexes. ACM Trans. Alg. 3(2), art. 20 (2007)

10. Gagie, T., Navarro, G., Puglisi, S.J.: New algorithms on wavelet trees and applications 
to information retrieval. Theor. Comp. Sci. 426–427, 25–41 (2012)

11. Grossi, R., Vitter, J.S.: Compressed suﬃx arrays and suﬃx trees with applications

to text indexing and string matching. SIAM J. Comput. 35(2), 378–407 (2005)

12. Gusﬁeld, D.: Algorithms on Strings, Trees and Sequences: Computer Science and

Computational Biology. Cambridge University Press (1997)

13. Hon, W.-K., Shah, R., Thankachan, S.V., Vitter, J.S.: On position restricted substring 
searching in succinct space. J. Discr. Alg. 17, 109–114 (2012)

14. Hon, W.-K., Shah, R., Thankachan, S.V., Vitter, J.S.: Faster compressed top-k

document retrieval. In: Proc. 23rd DCC, pp. 341–350 (2013)

15. Hon, W.-K., Shah, R., Thankachan, S.V., Scott Vitter, J.: Space-eﬃcient frameworks 
for topk 
string retrieval. J. ACM 61(2), 9 (2014)

16. Hon, W.-K., Shah, R., Vitter, J.S.: Space-eﬃcient framework for top-k string

retrieval problems. In: Proc. 50th FOCS, pp. 713–722 (2009)

17. Manber, U., Myers, G.: Suﬃx arrays: a new method for on-line string searches.

SIAM J. Comp. 22(5), 935–948 (1993)

18. Munro, J.I., Navarro, G., Shah, R., Thankachan, S.V.: Ranked document selection.
In: Ravi, R., Gørtz, I.L. (eds.) SWAT 2014. LNCS, vol. 8503, pp. 344–356. Springer,
Heidelberg (2014)

19. Muthukrishnan, S.; Eﬃcient algorithms for document retrieval problems. In: Proc.

13th SODA, pp. 657–666 (2002)

20. Navarro, G.: Spaces, trees and colors: The algorithmic landscape of document

retrieval on sequences. ACM Comp. Surv. 46(4), art. 52 (2014)

21. Navarro, G., M¨akinen, V.: Compressed full-text indexes. ACM Comp. Surv. 39(1),

art. 2 (2007)

22. Navarro, G., Nekrich, Y.: Top-k document retrieval in optimal time and linear

space. In: Proc. 23rd SODA, pp. 1066–1078 (2012)

23. Navarro, G., Russo, L.: Fast fully-compressed suﬃx trees. In: Proc. 24th DCC, pp.

283–291 (2014)

24. Navarro, G., Thankachan, S.V.: Faster top-k document retrieval in optimal space.
In: Kurland, O., Lewenstein, M., Porat, E. (eds.) SPIRE 2013. LNCS, vol. 8214,
pp. 255–262. Springer, Heidelberg (2013)

25. Navarro, G., Thankachan, S.V.: Top-k document retrieval in compact space and
near-optimal time. In: Cai, L., Cheng, S.-W., Lam, T.-W. (eds.) Algorithms and
Computation. LNCS, vol. 8283, pp. 394–404. Springer, Heidelberg (2013)

26. Navarro, G., Thankachan, S.V.: New space/time tradeoﬀs for top-k document

retrieval on sequences. Theor. Comput. Sci. 542, 83–97 (2014)

27. Nekrich, Y., Navarro, G.: Sorted range reporting. In: Fomin, F.V., Kaski, P. (eds.)

SWAT 2012. LNCS, vol. 7357, pp. 271–282. Springer, Heidelberg (2012)

180

J. I. Munro et al.

28. Pˇatra¸scu, M.: Succincter. In: Proc. 49th FOCS, pp. 305–313 (2008)
29. Schenkel, R., Broschart, A., Hwang, S., Theobald, M., Weikum, G.: Eﬃcient text
proximity search. In: Ziviani, N., Baeza-Yates, R. (eds.) SPIRE 2007. LNCS, vol.
4726, pp. 287–299. Springer, Heidelberg (2007)

30. Shah, R., Sheng, C., Thankachan, S.V., Vitter, J.S.: Top-k document retrieval in
external memory. In: Bodlaender, H.L., Italiano, G.F. (eds.) ESA 2013. LNCS, vol.
8125, pp. 803–814. Springer, Heidelberg (2013)

31. Weiner, P.: Linear pattern matching algorithm. In: Proc. 14th Annual IEEE Symposium 
on Switching and Automata Theory, pp. 1–11 (1973)

32. Yan, H., Shi, S., Zhang, F., Suel, T., Wen, J.-R.: Eﬃcient term proximity search

with term-pair indexes. In: CIKM, pp. 1229–1238 (2010)

33. Zhu, M., Shi, S., Li, M., Wen, J.-R.: Eﬀective top-k computation in retrieving
structured documents with term-proximity support. In: CIKM, pp. 771–780 (2007)
34. Zhu, M., Shi, S., Yu, N., Wen, J.-R.: Can phrase indexing help to process nonphrase 
queries? In: CIKM, pp. 679–688 (2008)

