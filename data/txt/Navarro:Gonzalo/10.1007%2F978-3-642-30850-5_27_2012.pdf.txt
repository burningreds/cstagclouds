Space-Eﬃcient Top-k Document Retrieval(cid:2)

Gonzalo Navarro and Daniel Valenzuela

Dept. of Computer Science, Univ. of Chile
{gnavarro,dvalenzu}@dcc.uchile.cl

Abstract. Supporting top-k document retrieval queries on general text
databases, that is, ﬁnding the k documents where a given pattern occurs
most frequently, has become a topic of interest with practical applications.
 While the problem has been solved in optimal time and linear
space, the actual space usage is a serious concern. In this paper we study
various reduced-space structures that support top-k retrieval and propose 
new alternatives. Our experimental results show that our novel
structures and algorithms dominate almost all the space/time tradeoﬀ.

1

Introduction

Ranked document retrieval is the basic task of most search engines. It consists in

preprocessing a collection of d documents, D = {D1, D2, . . . , Dd}, so that later,

given a query pattern P and a threshold k, one quickly ﬁnds the k documents
where P is “most relevant”.

The best known application scenario is that of documents being formed by
natural language texts, that is, sequences of words, and the query patterns being
words, phrases (sequences of words), or sets of words or phrases. Several relevance
measures are used, which attempt to establish the signiﬁcance of the query in a
given document [2]. The term frequency, the number of times the pattern appears
in the document, is the main component of most measures.

Ranked document retrieval is usually solved with some variant of a simple
structure called an inverted index [2]. This structure, which is behind most
search engines, handles well natural language collections. However, the term
“natural language” hides several assumptions that are key to the eﬃciency of
that solution: the text must be easily tokenized into words, there must not be
too many diﬀerent words, and queries must be whole words or phrases.

Those assumptions do not hold in various applications where document retrieval 
is of interest. The most obvious ones are documents written in Asian
languages , where it is not easy to split words automatically, and search engines
treat the text as a sequence of symbols, so that queries can retrieve any substring 
of the text. Other applications simply do not have a concept of word, yet
ranked retrieval would be of interest: DNA or protein sequence databases where
one seeks the sequences where a short marker appears frequently, source code
repositories where one looks for functions making heavy use of an expression or

(cid:2) Partially funded by Fondecyt Grant 1-110066, Chile.

R. Klasing (Ed.): SEA 2012, LNCS 7276, pp. 307–319, 2012.
c(cid:2) Springer-Verlag Berlin Heidelberg 2012

308

G. Navarro and D. Valenzuela

function call, MIDI sequence databases where one seeks for pieces where a given
short passage is repeated, and so on.

These problems are modeled as a text collection where the documents Di are
strings over an alphabet Σ, of size σ, and the queries are also simple strings. The
most popular relevance measure is the term frequency, meaning the number of occurrences 
of the string P in the strings Di (we discuss other measures in Section 6).
We call n =

(cid:2)|Di| the collection size and m=|P| the pattern length.

Muthukrishnan [17] pioneered the research on document retrieval for general
strings. He solved the simpler problem of “document listing”: reporting the occ
distinct documents where P appears in optimal time O(m + occ) and linear
space, O(n) integers (or O(n log n) bits). Muthukrishnan also considered various
other document retrieval problems, but not top-k retrieval.

The ﬁrst eﬃcient solution for the top-k retrieval problem was introduced by
Hon et al. [13]. They achieved O(m + log n log log n + k) time, yet the space was
superlinear, O(n log2 n) bits. Soon, Hon et al. [12] achieved O(m + k log k) time
and linear space, O(n log n) bits. Recently, Navarro and Nekrich [18] achieved
optimal time, O(m + k), and reduced the space from O(n log n) to O(n(log σ +
log d)) bits (albeit the constant is not small).

While these solutions seem to close the problem, it turns out that the space
required by O(n log n)-bit solutions is way excessive for practical applications.
A recent space-conscious implementation of Hon et al.’s index [20] showed that
it requires at least 5 times the text size.

Motivated by this challenge, there has been a parallel research track on how
to reduce the space of these solutions, while retaining eﬃcient search time
[21,22,12,7,5,3,19,11]. In this work we introduce a new variant with relevant
theoretical and practical properties, and show experimentally that it dominates
previous work. The next section puts our contribution in context.

2 Related Work

Most of the data structures for general text searching, and in particular the
classical ones for document retrieval [17,12], build on on suﬃx arrays [16] and

suﬃx trees [23]. Regard the collection D as a single text T [1, n] = D1D2 . . . Dd,
where each Di is terminated by a special symbol “$”. A suﬃx array A[1, n] is a
permutation of the values [1, n] that points to all the suﬃxes of T : A[i] points
to the suﬃx T [A[i], n]. The suﬃxes are lexicographically sorted in A: T [A[i], n] <
T [A[i+1], n] for all 1 ≤ i < n. Since the occurrences of any pattern P in T
correspond to suﬃxes of T that are preﬁxed by P , the occurrences are pointed
from a contiguous area in the suﬃx array A[sp, ep]. A simple binary search ﬁnds
sp and ep in O(m log n) time [16]. A suﬃx tree is a digital tree with O(n) nodes
where all the suﬃxes of T are inserted and unary paths are compacted. Every
internal node of the suﬃx tree corresponds to a repeated substring of T and
its associated suﬃx array interval;, suﬃx tree leaves correspond to the suﬃxes
and their corresponding suﬃx array cells. A top-down traversal in the suﬃx tree
ﬁnds the internal node (called the locus of P ) from where all the suﬃxes preﬁxed

Space-Eﬃcient Top-k Document Retrieval

309

with P descend, in O(m) time. Once sp and ep are known, the top-k query ﬁnds
the k documents where most suﬃxes in A[sp, ep] start.

A ﬁrst step towards reducing the space in top-k solutions is to compress
the suﬃx array. Compressed suﬃx arrays (CSAs) simulate a suﬃx array within

as little as nHk(T ) + o(n log σ) bits, for any k ≤ α logσ n and any constant
0 < α < 1. Here Hk(T ) is the k-th order entropy of T , a measure of its statistical
compressibility. The CSA, using |CSA| bits, ﬁnds sp and ep in time search(m),
−1[i], in time lookup(n). For example, a
and computes any cell A[i], and even A
CSA achieving the small space given above [6] achieves search(m) = O(m(1 +
log log n )) and lookup(n) = O(log1+ n) for any constant  > 0. CSAs also replace
the collection, as they can extract any substring of T .

log σ

In their very same foundational paper, Hon et al. [12] proposed an alternative
succinct data structure to solve the top-k problem. Building on a solution by
Sadakane [21] for document listing, they use a CSA for T and one smaller CSA
for each document Di, plus a little extra data, for a total space of 2|CSA|+o(n)+
d log(n/d)+O(d) bits. They achieve time O(search(m)+k log3+ n·lookup(n)), for
any constant  > 0. Gagie et al. [7] slightly reduced the time to O(search(m) +
k log d log(d/k) log1+ n · lookup(n)), and Belazzougui and Navarro [3] further
improved it to O(search(m) + k log k log(d/k) log n · lookup(n)).
The essence of the succinct solution by Hon et al. [12] is to preprocess topk 
answers for the lowest suﬃx tree nodes containing any range A[i · g, j · g]
for some sampling parameter g. Given the query interval A[sp, ep], they ﬁnd
the highest preprocessed suﬃx tree node whose interval [sp
] is contained in
[sp, ep]. They show that sp
< g, and then the cost of
(cid:3)−1]
correcting the precomputed answer using the extra occurrences at A[sp, sp
+1, ep] is bounded. For each such extra occurrence A[i], one ﬁnds out
and A[ep
its document, computes the number of occurrences of P within that document,
and lets the document compete in the top-k precomputed list. Hon et al. use the
individual CSAs and other data structures to carry out this task. The subsequent
improvements [7,3] are due to small optimizations on this basic design.

(cid:3) − sp < g and ep − ep

(cid:3)

(cid:3)

(cid:3)

(cid:3)

, ep

Gagie et al. [7] also pointed out that in fact Hon et al.’s solution can run
on any other data structure able to (1) telling which document corresponds to
a given A[i], and (2) count how many times the same document appears in
any interval A[sp, ep]. A structure that is suitable for this task is the document
array D[1, n], where D[i] is the document A[i] belongs to [17]. While in Hon
et al.’s solution this is computed from A[i] using d log(n/d) + O(d) extra bits
[21], we need more machinery for task (2). A good alternative was proposed
by M¨akinen and Valim¨aki [22] in order to reduce the space of Muthukrishnan’s
document listing solution [17]. The structure is a wavelet tree [10] on D. The
wavelet tree represents D using n log d + o(n) log d bits and not only computes
any D[i] in O(log d) time, but it can also compute operation ranki(D, j), which
is the number of occurrences of document i in D[1, j], in O(log d) time too. This
solves operation (2) as rankD[i](D, ep) − rankD[i](D, sp−1). With the obvious

disadvantage of the considerable extra space to represent D, this solution changes
lookup(n) by log d in the query time. Gagie et al. show many other combinations

310

G. Navarro and D. Valenzuela

that solve (1) and (2). One of the fastest uses Golynski et al.’s representation
[9] on D and, within the same space, changes lookup(n) to log log d in the time.
Very recently, Hon, Shah, and Thankachan [11] presented new combinations in
the line of Gagie et al., using also faster CSAs. The least space-consuming one
requires n log d + n o(log d) bits of extra space on top of the CSA of T , and
improves the time to O(search(m) + k(log k + (log log n)2+)).

Belazzougui and Navarro [3] used an approach based on minimum perfect
hash functions to replace the array D by a weaker data structure that takes
O(n log log log d) bits of space and supports the search in time O(search(m) +
k log k log1+ n · lookup(n)). This solution is intermediate between representing
D or the individual CSAs and it could have practical relevance.

Culpepper et al. [5] built on an improved document listing algorithm on
wavelet trees [8] to achieve two top-k algorithms, called Quantile and Greedy,
that use the wavelet tree alone (i.e., without Hon et al.’s [12] extra structures).
Despite their worst-case complexity being as bad as extracting the results one
by one in A[sp, ep], that is, O((ep − sp + 1) log d), in practice the algorithms
performed very well, being Greedy superior. They implemented Sadakane’s solution 
[21] of using individual CSAs for the documents and showed that the
overheads are very high in practice. Navarro et al. [19] arrived at the same conclusion,
 showing that Hon et al.’s original succinct scheme is not promising in
practice: both space and time were much higher in practice than Culpepper et
al.’s solution. However, their preliminary experiments [19] showed that Hon et
al.’s scheme could compete when running on wavelet trees.

Navarro et al. [19] also presented the ﬁrst implemented alternative to reduce
the space of wavelet trees, by using Re-Pair compression [15] on the bitmaps.
They showed that signiﬁcant reductions in space were possible in exchange for
an increase in the response time of Culpepper et al.’s Greedy algorithm (half the
space and twice the time is a common ﬁgure).

This review exposes interesting contrasts between the theory and the practice
in this area. On one hand, the structures that are in theory larger and faster
(i.e., the n log d-bits wavelet tree versus a second CSA of at most n log σ bits) are
in practice smaller and faster. On the other hand, algorithms with no worst-case
bound (Culpepper et al.’s [5]) perform very well in practice. Yet, the space of
wavelet trees is still considerably large in practice (about twice the plain size of
T in several test collections [19]), especially if we consider that they represent
totally redundant information that could be extracted from the CSA of T .

(cid:3)

(cid:3)−1] and A[ep

In this paper we study a new practical alternative. We use Hon et al.’s [12]
succinct structure on top of a wavelet tree, but instead of brute force we use a
variant of Culpepper et al.’s [5] method to ﬁnd the extra candidate documents
+1, ep]. We can regard this combination either as Hon
in A[sp, sp
et al.’s method boosting Culpepper et al. or vice versa. Culpepper et al. boost
Hon et al.’s method, while retaining its good worst-case complexities, as they
ﬁnd the extra occurrences more cleverly than by enumerating them all. Hon et
al. boost plain Culpepper et al.’s method by having precomputed a large part
of the range, and thus ensuring that only small intervals have to be handled.

Space-Eﬃcient Top-k Document Retrieval

311

We consider the plain and the compressed wavelet tree representations, and
the straightforward and novel representations of Hon et al.’s succinct structure.
We compare these alternatives with the original Culpepper et al.’s method (on
plain and compressed wavelet trees), to test the hypothesis that adding Hon
et al.’s structure is worth the extra space. Similarly, we include in the comparison 
the basic Hon et al.’s method (with and without compressed structure)
over Golynski et al.’s [9] sequence representation, to test the hypothesis that
using Culpepper et al.’s method over the wavelet tree is worth compared to the
brute force method over the fastest sequence representation [9]. This brute force
method is also at the core of the new proposal by Hon et al. [11].

Our experiments show that our new algorithms and data structures dominate
almost all the space/time tradeoﬀ for this problem, becoming a new practical
reference point.

3

Implementing Hon et al.’s Succinct Structure

The succinct structure of Hon et al. [12] is a sparse generalized suﬃx tree of
T (SGST; “generalized” means it indexes d strings). It is obtained by cutting
A[1, n] into blocks of length g and sampling the ﬁrst and last cell of each block
(recall that cells of A are leaves of the suﬃx tree). Then all the lowest common
ancestors (lca) of pairs of sampled leaves are marked, and a tree τk is formed
with those (at most) 2n/g marked internal nodes. The top-k answer is stored for
each marked node, using O((n/g)k log n) bits. This is done for k = 1, 2, 4, . . .,
and parameter g is of the form g = k · g
) log d log n)
bits. This is made o(n) by properly choosing g

. The ﬁnal space is O((n/g

(cid:3)

(cid:3)

(cid:3)

.

= 2

(cid:5)log k(cid:6)

, and let g = k

To answer top-k queries, they search the CSA for P , to obtain the suﬃx range
A[sp, ep] of the pattern. Then they turn to the closest higher power of two of k,
∗
be the corresponding g value. They now ﬁnd
k
the locus of P in the tree τk∗ by descending from the root until ﬁnding the ﬁrst
node v whose interval [spv, epv] is contained in [sp, ep]. They have at v the top-k
candidates for [spv, epv] and have to correct the answer considering [sp, spv−1]
and [epv+1, ep]. Now we introduce two implementations of this idea.

∗ · g

(cid:3)

3.1 Sparsiﬁed Generalized Suﬃx Tree (SGST)

Let us call li = A[i] the i-th leaf. Given a value of k we deﬁne g = k · g
,
for a space/time tradeoﬀ parameter g
, and sample n/g leaves l1, lg+1, l2g+1, . . .,
instead of sampling 2n/g leaves as in the theoretical proposal. We mark internal
SGST nodes lca(l1, lg+1), lca(lg+1, l2g+1), . . .. It is easy to prove that any v =
lca(lig+1, ljg+1) is also v = lca(lrg+1, l(r+1)g+1) for some r (precisely, r is the
rightmost sampled leaf descending from the child of v that is ancestor of lig+1).
Thus these n/g SGST nodes suﬃce and can be computed in linear time [4].

(cid:3)

(cid:3)

Now we note that there is a great deal of redundancy in the log d trees τk,
since the nodes of τ2k are included in those of τk, and the 2k candidates stored
in the nodes of τ2k contain those in the corresponding nodes of τk. To factor out

312

G. Navarro and D. Valenzuela

some of this redundancy we store only one tree τ , whose nodes are the same of τ1,
and record the class c(v) of each node v ∈ τ . This is c(v) = max{k, v ∈ τk} and
can be stored in log log d bits. Each node v ∈ τ stores the top-c(v) candidates
corresponding to its interval, using c(v) log d bits, and their frequencies, using
c(v) log n bits, plus a pointer to the table, and the interval itself, [spv, epv], using
2 log n bits. All the information on intervals and candidates is factored in this
way, saving space. Note that the class does not necessarily decrease monotonically 
in a root-to-leaf path of τ , thus we store all the topologies independently to
allow for eﬃcient traversal of the τk trees, for k > 1. Apart from topology information,
 each node of such τk trees contains just a pointer to the corresponding
node in τ , using log |τ| bits.
In our ﬁrst data structure, the topology of the trees τ and τk is represented
using pointers of log|τ| and log |τk| bits, respectively. To answer top-k queries,

we ﬁnd the range A[sp, ep] using a CSA (whose space and negligible time will
not be reported because it is orthogonal to all the data structures). Now we ﬁnd
the locus in the appropriate tree τk∗ top-down, binary searching the intervals
[spv, epv] of the children of the current node, and extracting those intervals
using the pointer to τ . By the properties of the sampling [12] it follows that we
will traverse in this descent nodes v ∈ τk∗ such that [sp, ep] ⊆ [spv, epv], until
reaching a node v so that [spv, epv] = [sp
reaching a leaf u ∈ τk such that [sp, ep] ⊆ [spu, epu], in which case ep− sp + 1 <
2g). This v is the locus of P in τk∗ , and we ﬁnd it in time O(m log σ). This is
negligible compared to the subsequent costs, as well as it is the CSA search.

] ⊆ [sp, ep] ⊆ [sp

+ g] (or

(cid:3) − g, ep

(cid:3)

(cid:3)

(cid:3)

, ep

3.2 Succinct SGST

Our second implementation uses represents the tree topologies without pointers.
 Although the tree operations are slightly slower than with pointers, this
slowdown occurs on a less signiﬁcant part of the search process, and a succinct
representation allows one to reduce the sampling parameter g for the same space.
Arroyuelo et al. [1] showed that, for the functionality it provides, the most
promising succinct representation of trees is the so-called LOUDS [14]. It requires
2N + o(N ) bits of space (in practice, as little as 2.1 N ) to represent a tree of N
nodes, and it solves many operations in constant time (less than a microsecond
in practice). We resort to their labeled trees [1] implementation, We encode the
values spv and epv, pointers to τ (in τk), and pointers to the candidates in a
separate array, indexed by the LOUDS rank of the node v, managing them as
Arroyuelo et al. [1] manage labels. We use that implementation [1].

4 A New Top-k Algorithm

We run a combination of the algorithm by Hon et al. [12] and those of Culpepper 
et al. [5], over a wavelet tree representation of the document array D[1, n].
Culpepper et al. introduce, among others, a document listing method (DFS) and
a Greedy top-k heuristic. We adapt these to our particular top-k subproblem.

Space-Eﬃcient Top-k Document Retrieval

313

If the search for the locus of P ends at a leaf u that still contains the interval
[sp, ep], Hon et al. simply scan A[sp, ep] by brute force and accumulate frequencies.
 We use instead Culpepper et al.’s Greedy algorithm, which is always better
than a brute-force scanning.
] ⊆ [sp, ep],
we start with the precomputed answer of the k ≤ k
most frequent documents in
[sp
+1, ep].
We use the wavelet tree of D to solve the following problem: Given an interval
D[l, r], and two subintervals [l1, r1] and [l2, r2], enumerate all the distinct values
in [l1, r1]∪ [l2, r2], and their frequencies in [l, r]. We propose two solutions, which

When, instead, the locus of P is a node v where [spv, epv] = [sp
(cid:3)

], and update it to consider the subintervals [sp, sp

(cid:3)−1] and [ep

, ep

∗

(cid:3)

, ep

(cid:3)

(cid:3)

(cid:3)

generalize the heuristics proposed by Culpepper et al. [5].

4.1 Restricted Depth-First Search (DFS)

Let us consider a wavelet tree [10] representation of an array D. At the root, a
bitmap B[1, n] stores B[i] = 0 if D[i] ≤ d/2 and B[i] = 1 otherwise. The left
child of the root is, recursively, a wavelet tree handling the subsequence of D
with values D[i] ≤ d/2, and the right child handles the subsequence of values
D[i] > d/2. Added over the log d levels, the wavelet tree requires n log d bits
of space. With o(n log d) additional bits we answer in constant time any query
rank0/1(B, i) over any bitmap B [14].
as [i0, j0] = [rank0(B, i−1)+1, rank0(B, j)], and into its right child as [i1, j1] =
[rank1(B, i−1)+1, rank1(B, j)], where B is the root bitmap. Those can then be

Note that any interval D[i, j] can be projected into the left child of the root

projected recursively into other wavelet tree nodes.

(cid:3)

Our restricted DFS algorithm begins at the root of the wavelet tree and

+1, ep]. More precisely, we count the number of zeros and ones in B in ranges

(cid:3)−1], and [l2, r2] =
tracks down the intervals [l, r] = [sp, ep], [l1, r1] = [sp, sp
[ep
[l1, r1] ∪ [l2, r2], as well as in [l, r], using a constant number of rank operations
on B. If there are any zeros in [l1, r1] ∪ [l2, r2], we map all the intervals into
there are any ones in [l1, r1]∪ [l2, r2], we continue on the right child of the node.

the left child of the node and proceed recursively from this node. Similarly, if

When we reach a wavelet tree leaf we report the corresponding document, and
the frequency is the length of the interval [l, r] at the leaf.

When solving the problem in the context of top-k retrieval, we can prune some
recursive calls. If, at some node, the size of the local interval [l, r] is smaller than
our current kth candidate to the answer, we stop exploring its subtree since it
cannot contain competitive documents.

4.2 Restricted Greedy

Following the idea of Culpepper et al., we can not only stop the traversal when
[l, r] is too small, but also prioritize the traversal of the nodes by their [l, r] value.
We keep a priority queue where we store the wavelet tree nodes yet to process,
and their intervals [l, r], [l1, r1], and [l2, r2]. The priority queue begins with one
element, the root. Iteratively, we remove the element with highest r−l+1 value

314

G. Navarro and D. Valenzuela

from the queue. If it is a leaf, we report it. Otherwise, we project the intervals into
its left and right children, and insert each such children containing nonempty

intervals [l1, r1] or [l2, r2] into the queue. As soon as the r−l+1 value of the

element we extract from the queue is not larger than the kth frequency known
at the moment, we can stop.

4.3 Heaps for the k Most Frequent Candidates

(cid:3)

(cid:3)

, ep

Our two algorithms solve the query assuming that we can easily know at each
moment which is the kth best candidate known up to now. We use a min-heap
data structure for this purpose. It is loaded with the top-k precomputed candidates 
corresponding to the interval [sp
]. At each point, the top of the heap
gives the kth known frequency in constant time. Given that the previous algorithms 
stop when they reach a wavelet tree node where r−l+1 is not larger than
the kth known frequency, it follows that each time the algorithms report a new
candidate, that candidate is more frequent than our kth known candidate. Thus
we replace the top of our heap with the reported candidate and reorder the heap
(which is always of size k, or less until we ﬁnd k distinct elements in D[sp, ep]).
Therefore each candidate reported costs O(log d+log k) time (there are also steps
that do not yield any result, but the overall bound is still O(g(log d + log k))).
A remaining issue is that we can ﬁnd again, in our DFS or Greedy traversal, a
node that was in the original top-k list, and thus possibly in the heap. This means
that the document had been inserted with its frequency in D[sp
], but since it
appears more times in D[sp, ep], we must now increase its frequency and restore
the min-heap invariant. It is not hard to maintain a hash table with forward and
backward pointers to the heap so that we can track their current positions and
replace their values. However, for the small k values used in practice (say, ten or
at most hundreds), it is more practical to scan the heap for each new candidate
to insert than to maintain all those pointers upon all operations.

(cid:3)

(cid:3)

, ep

5 Experimental Results

We test our implementations of Hon et al.’s succinct structure combined with a
wavelet tree (as explained, the original proposal is not competitive in practice
[19]). We used three test collections of diﬀerent nature: ClueWiki, a 141 MB
sample of ClueWeb09, formed by 3,334 Web pages from the English Wikipedia;
KGS, a 25 MB collection of 18,838 sgf-formatted Go game records (http://www.
u-go.net/gamerecords); and Proteins, a 60 MB collection of 143,244 sequences
of Human and Mouse proteins (http://www.ebi.ac.uk/swissprot).

Our tests were run on a 4-core 8-processors Intel Xeon, 2Ghz each, with 16GB
RAM and 2MB cache. We compiled using g++ with full optimization. For queries,
we selected 1,000 substrings at random positions, of length 3 and 8, and retrieved
the top-k documents for each, for k = 1 and 10.

Space-Eﬃcient Top-k Document Retrieval

315

(cid:3)

Choosing Our Best Variant. Our ﬁrst round of experiments compares our different 
implementations of SGSTs (i.e., the trees τk, see Section 3) over a single
implementation of wavelet tree (Alpha, choosing the best value for α in each
case [19]). We tested a pointer-based representation of the SGST (Ptrs, the
original proposal [12]), a LOUDS-based representation (LOUDS), our variant of
LOUDS that stores the topologies in a unique tree τ (LIGHT), and our variant of
LIGHT that does not store frequencies of the top-k candidates (XLIGHT). We used
sampling steps of 200 and 400 for g
. For each value of g, we obtain a curve with
various sampling steps for the rank computations on the wavelet tree bitmaps.
We also tested diﬀerent algorithms to ﬁnd the top-k among the precomputed
candidates and remaining leaves (see Section 4): Our modiﬁed greedy (Greedy),
our modiﬁed depth-ﬁrst-search (DFS), and the brute-force selection procedure of
the original proposal [12] on top of the same wavelet tree (Select). As this is
orthogonal to the data structures used, we compare these algorithms only on top
of the Ptrs structure. The other structures will be tested using the best method.
Figure 1 shows the results. Method Greedy is always better than Select (up
to 80% better) and DFS (up to 50%), which conﬁrms intuition. Using LOUDS
representation instead of Ptr had almost no impact on the time. This is because
time needed to ﬁnd the locus is usually negligible compared with that to explore
the uncovered leaves. Further costless space gains are obtained with variant
LIGHT. Variant XLIGHT, instead, reduces the space of LIGHT at a noticeable cost
in time that makes it not so interesting, except on Proteins. In various cases the
sparser sampling dominates the denser one, whereas in others the latter makes
the structure faster if suﬃcient space is spent. To compare with other techniques,
we will use variant LIGHT on ClueWiki and KGS, and XLIGHT on Proteins,
both with g

= 400. This combination will be called generically SSGST.

(cid:3)

Comparison with Previous Work. We now compare ours with previous work.
The Greedy heuristic [5] is run over diﬀerent wavelet-tree representations of the
document array: a plain one (WT-Plain) [5], a Re-Pair compressed one (WT-RP),
and a hybrid that at each wavelet tree level chooses between plain, Re-Pair, or
entropy-based compression of the bitmaps (WT-Alpha) [19]. We combine these
with our best implementation of Hon et al.’s structure (suﬃxing the previous
names with +SSGST). We also consider variant Goly+SSGST [7,11], which runs
the rank-based method (Select) on top of the fastest rank-capable sequence
representation of the document array (Golynski et al.’s [9], which is faster than
wavelet trees for rank but does not support our more sophisticated algorithms;
here we used the implementation at http://libcds.recoded.cl).

Our new structures dominate most of the space-time map. When using little 
space, variant WT-RP+SSGST dominates, being only ocassionally and slightly
superseded by WT-RP. When using more space, WT-Alpha+SSGST takes over,
and ﬁnally, with even more space, WT-Plain+SSGST becomes the best choice.
Most of the exceptions arise in Proteins, which due to its incompressibility [19]
makes WT-Plain+SSGST essentially the only interesting variant. The alternative
Goly+SSGST is no case faster than a Greedy algorithm over plain wavelet trees
(WT-Plain), and takes more space.

316

G. Navarro and D. Valenzuela

ClueWiki, m=3, k=1

ClueWiki, m=8, k=10

 10

 1

 0.1

 
y
r
e
u
q
 
 
r
e
p
 
)
s
c
e
s

i
l
i

m

(
 
e
m
T

i

 10

 1

 
y
r
e
u
q

 
 
r
e
p
 
)
s
c
e
s

i
l
i

m

(
 

e
m
T

i

 
y
r
e
u
q

 
 
r
e
p
 
)
s
c
e
s

i
l
i

m

(
 

e
m
T

i

 0.075

 0.07

 0.065

 0.06

 0.055

 0.05

 0.045

 0.04

 0.035

 0.03

 18

Ptrs 200 Greedy
Ptrs 200 DFS
Ptrs 200 Select
Ptrs 400 Greedy
Ptrs 400 DFS
Ptrs 400 Select
LOUDS 400 
LOUDS 200 
LIGHT 400 
LIGHT 200 
XLIGHT 400 
XLIGHT 200 

 
y
r
e
u
q
 
 
r
e
p
 
)
s
c
e
s

i
l
i

m

(
 
e
m
T

i

 10

 9

 9.5

 10

 10.5

 11

 11.5

 12

 12.5

 9

 9.5

 10

 10.5

 11

 11.5

 12

 12.5

Size (bpc)

KGS, m=3, k=1

Size (bpc)

KGS, m=8, k=10

 14

 15

 16

 17
Size (bpc)

 18

 19

 20

 21

Proteins, m=8, k=1

 100

 10

 1

 0.15

 0.14

 0.13

 0.12

 0.11

 0.1

 
y
r
e
u
q

 
 
r
e
p
 
)
s
c
e
s

i
l
i

m

(
 

e
m
T

i

 
y
r
e
u
q

 
 
r
e
p
 
)
s
c
e
s

i
l
i

m

(
 

e
m
T

i

 14

 15

 16

 17
Size (bpc)

 18

 19

 20

Proteins, m=8, k=10

 19

 20

 21

 22

 23

 24

 25

 26

 27

 28

 0.09

 18

 19

 20

 21

 22

 23

 24

 25

 26

 27

 28

Size (bpc)

Size (bpc)

Fig. 1. Our diﬀerent alternatives for top-k queries. On the left for k = 1 and pattern
length m = 3; on the right for k = 10 and m = 8.

6 Final Remarks

We can further reduce the space in exchange for possibly higher times. For
example the sequence of all precomputed top-k candidates can be Huﬀmancompressed,
 as there is much repetition in the sets and a zero-order compression
would yield space reductions of up to 25%. The pointers to those tables could also
be removed, by separating the tables by size, and computing the oﬀset within
each size using rank on the sequence of classes of the nodes in τ .

More in perspective, term frequency is probably the simplest relevance
measure. In Information Retrieval, more sophisticated ones like BM25 are used.
Such formula involves the sizes of the documents, and thus techniques like
Culpepper et al.’s [5] do not immediately apply. However, Hon et al.’s [12] does,

Space-Eﬃcient Top-k Document Retrieval

317

ClueWiki, m = 3

ClueWiki, m = 8

WT−RP (K=1)
WT−RP(K=10)
WT−RP + SSGST (K=1) 
WT−RP + SSGST (K=10) 
WT−Alpha (K=1)
WT−Alpha (K=10)
WT−Alpha + SSGST (K=1)
WT−Alpha + SSGST (K=10)
WT−Plain (K=1)
WT−Plain (K=10) 
WT−Plain + SSGST (K=1)
WT−Plain + SSGST (K=10)
Goly + SSGST (K=1) 
Goly + SSGST (K=10)

 8

 10

 12

 14

 16

 18

Size (bpc)

KGS, m = 3

 1000

 100

 10

 1

 0.1

 0.01

 6

 100

 10

 1

 0.1

 
y
r
e
u
q
 
 
r
e
p
 
)
s
c
e
s

i
l
i

m

(
 
e
m
T

i

 
y
r
e
u
q

 
 
r
e
p
 
)
s
c
e
s

i
l
i

m

(
 

e
m
T

i

 8

 10

 12

 14

 16

 18

 20

Size (bpc)

KGS, m = 8

 12

 14

 16

 18

 20

 22

 12

 14

 16

 18

 20

 22

Size (bpc)

Proteins, m = 3

Size (bpc)

Proteins, m = 8

 10

 1

 0.1

 
y
r
e
u
q

 
 
r
e
p
 
)
s
c
e
s

i
l
i

m

(
 

e
m
T

i

 1000

 100

 10

 1

 0.1

 0.01

 100

 10

 1

 0.1

 10

 1

 0.1

 0.01

 
y
r
e
u
q
 
 
r
e
p
 
)
s
c
e
s

i
l
i

m

(
 
e
m
T

i

 
y
r
e
u
q

 
 
r
e
p
 
)
s
c
e
s

i
l
i

m

(
 

e
m
T

i

 
y
r
e
u
q

 
 
r
e
p
 
)
s
c
e
s

i
l
i

m

(
 

e
m
T

i

 17

 18

 19

 20

 21

 22
 23
Size (bpc)

 24

 25

 26

 27

 28

 17

 18

 19

 20

 21

 22
 23
Size (bpc)

 24

 25

 26

 27

 28

Fig. 2. Comparison with previous work, for m = 3 (left) and m = 8 (right)

by simply storing the precomputed top-k answers according to BM25 and using
their brute-force traversal instead of our “restricted Greedy/DFS” methods. The
times would be very similar to the variant we called Select in this paper.

Sadakane [21] showed how to eﬃciently compute document frequencies (i.e.,
in how many documents does a pattern appear), in constant time and using just
2n+o(n) bits. With term frequency, these two measures are suﬃcient to compute
the popular tf-idf score. Note, however, that as long as queries are formed by
a single term, the top-k ranking is the same as given by term frequency alone.
Document frequency makes a diﬀerence on bag-of-word queries, which involve
several terms. Structures like those we have explored in this paper are able to
emulate a (virtual) inverted list, sorted by decreasing term frequency, for any
pattern, and thus enable the implementation of any top-k algorithm for bags of
words designed for inverted indexes.

318

G. Navarro and D. Valenzuela

References

1. Arroyuelo, D., C´anovas, R., Navarro, G., Sadakane, K.: Succinct trees in practice.

In: Proc. 11th ALENEX, pp. 84–97 (2010)

2. Baeza-Yates, R., Ribeiro-Neto, B.: Modern Information Retrieval, 2nd edn.

Addison-Wesley (2011)

3. Belazzougui, D., Navarro, G.: Improved Compressed Indexes for Full-Text Document 
Retrieval. In: Grossi, R., Sebastiani, F., Silvestri, F. (eds.) SPIRE 2011.
LNCS, vol. 7024, pp. 386–397. Springer, Heidelberg (2011)

4. Bender, M., Farach-Colton, M.: The LCA Problem Revisited. In: Gonnet, G.H.,
Viola, A. (eds.) LATIN 2000. LNCS, vol. 1776, pp. 88–94. Springer, Heidelberg
(2000)

5. Culpepper, J.S., Navarro, G., Puglisi, S.J., Turpin, A.: Top-k Ranked Document
Search in General Text Databases. In: de Berg, M., Meyer, U. (eds.) ESA 2010,
Part II. LNCS, vol. 6347, pp. 194–205. Springer, Heidelberg (2010)

6. Ferragina, P., Manzini, G., M¨akinen, V., Navarro, G.: Compressed representations

of sequences and full-text indexes. ACM Trans. Alg. 3(2), article 20 (2007)

7. Gagie, T., Navarro, G., Puglisi, S.J.: Colored Range Queries and Document Retrieval.
 In: Chavez, E., Lonardi, S. (eds.) SPIRE 2010. LNCS, vol. 6393, pp. 67–81.
Springer, Heidelberg (2010)

8. Gagie, T., Puglisi, S.J., Turpin, A.: Range Quantile Queries: Another Virtue of
Wavelet Trees. In: Karlgren, J., Tarhio, J., Hyyr¨o, H. (eds.) SPIRE 2009. LNCS,
vol. 5721, pp. 1–6. Springer, Heidelberg (2009)

9. Golynski, A., Munro, I., Rao, S.: Rank/select operations on large alphabets: a tool

for text indexing. In: Proc. 17th SODA, pp. 368–373 (2006)

10. Grossi, R., Gupta, A., Vitter, J.S.: High-order entropy-compressed text indexes.

In: Proc. 14th SODA, pp. 636–645 (2003)

11. Hon, W.-K., Shah, R., Thankachan, S.: Towards an optimal space-and-query-time

index for top-k document retrieval. CoRR, arXiv:1108.0554 (2011)

12. Hon, W.-K., Shah, R., Vitter, J.: Space-eﬃcient framework for top-k string retrieval

problems. In: Proc. 50th FOCS, pp. 713–722 (2009)

13. Hon, W.-K., Shah, R., Wu, S.-B.: Eﬃcient Index for Retrieving Top-k Most Frequent 
Documents. In: Karlgren, J., Tarhio, J., Hyyr¨o, H. (eds.) SPIRE 2009. LNCS,
vol. 5721, pp. 182–193. Springer, Heidelberg (2009)

14. Jacobson, G.: Space-eﬃcient static trees and graphs. In: Proc. 30th FOCS, pp.

549–554 (1989)

15. Larsson, J., Moﬀat, A.: Oﬀ-line dictionary-based compression. Proc. of the

IEEE 88(11), 1722–1732 (2000)

16. Manber, U., Myers, G.: Suﬃx arrays: a new method for on-line string searches.

SIAM J. Comp. 22(5), 935–948 (1993)

17. Muthukrishnan, S.: Eﬃcient algorithms for document retrieval problems. In: Proc.

13th SODA, pp. 657–666 (2002)

18. Navarro, G., Nekrich, Y.: Top-k document retrieval in optimal time and linear

space. In: Proc. 22nd SODA, pp. 1066–1078 (2012)

19. Navarro, G., Puglisi, S.J., Valenzuela, D.: Practical Compressed Document Retrieval.
 In: Pardalos, P.M., Rebennack, S. (eds.) SEA 2011. LNCS, vol. 6630, pp.
193–205. Springer, Heidelberg (2011)

Space-Eﬃcient Top-k Document Retrieval

319

20. Patil, M., Thankachan, S., Shah, R., Hon, W.-K., Vitter, J., Chandrasekaran, S.:

Inverted indexes for phrases and strings. In: Proc. SIGIR, pp. 555–564 (2011)

21. Sadakane, K.: Succinct data structures for ﬂexible text retrieval systems. J. Discr.

Alg. 5(1), 12–22 (2007)

22. V¨alim¨aki, N., M¨akinen, V.: Space-Eﬃcient Algorithms for Document Retrieval.
In: Ma, B., Zhang, K. (eds.) CPM 2007. LNCS, vol. 4580, pp. 205–215. Springer,
Heidelberg (2007)

23. Weiner, P.: Linear pattern matching algorithm. In: Proc. 14th Annual IEEE Symposium 
on Switching and Automata Theory, pp. 1–11 (1973)

