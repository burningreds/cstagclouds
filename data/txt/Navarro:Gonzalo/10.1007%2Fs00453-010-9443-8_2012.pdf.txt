Algorithmica (2012) 62:54–101
DOI 10.1007/s00453-010-9443-8

Stronger Lempel-Ziv Based Compressed Text Indexing

Diego Arroyuelo · Gonzalo Navarro ·
Kunihiko Sadakane

Received: 13 March 2008 / Accepted: 11 August 2010 / Published online: 8 September 2010
© Springer Science+Business Media, LLC 2010

Abstract Given a text T [1..u] over an alphabet of size σ , the full-text search problem
consists in ﬁnding the occ occurrences of a given pattern P[1..m] in T . In indexed
text searching we build an index on T to improve the search time, yet increasing the
space requirement. The current trend in indexed text searching is that of compressed
full-text self-indices, which replace the text with a more space-efﬁcient representation
of it, at the same time providing indexed access to the text. Thus, we can provide
efﬁcient access within compressed space.
The Lempel-Ziv index (LZ-index) of Navarro is a compressed full-text self-index
able to represent T using 4uHk(T ) + o(u log σ ) bits of space, where Hk(T ) denotes
the k-th order empirical entropy of T , for any k = o(logσ u). This space is about
four times the compressed text size. The index can locate all the occ occurrences of
a pattern P in T in O(m3 log σ + (m + occ) log u) worst-case time. Although this
index has proven very competitive in practice, the O(m3 log σ ) term can be excessive

A preliminary version of this paper appeared in Proc. of the 17th Annual Symposium
on Combinatorial Pattern Matching (CPM 2006), Vol. 4009 of LNCS, 2006.
D. Arroyuelo was supported by CONICYT PhD Fellowship Program.
G. Navarro was supported in part by Fondecyt Grant 1-080019.
K. Sadakane was supported in part by the Grant-in-Aid of the Ministry of Education, Science, Sports
and Culture of Japan.
D. Arroyuelo ((cid:2))
Yahoo! Research Latin America, Chile, Blanco Encalada 2120, Santiago, Chile
e-mail: darroyue@dcc.uchile.cl

G. Navarro
Dept. of Computer Science, Universidad de Chile, Blanco Encalada 2120, Santiago, Chile
e-mail: gnavarro@dcc.uchile.cl

K. Sadakane
Principles of Informatics Research Division, National Institute of Informatics, 2-1-2 Hitotsubashi,
Chiyoda-ku, Tokyo 101-8430, Japan
e-mail: sada@nii.ac.jp

Algorithmica (2012) 62:54–101

55

for long patterns. Also, the factor 4 in its space complexity makes it larger than other
state-of-the-art alternatives.

In this paper we present stronger Lempel-Ziv based indices (LZ-indices), improving 
the overall performance of the original LZ-index. We achieve indices requiring
(2 + )uHk(T ) + o(u log σ ) bits of space, for any constant  > 0, which makes
them the smallest existing LZ-indices. We simultaneously improve the search time
to O(m2 + (m + occ) log u), which makes our indices very competitive with state-
of-the-art alternatives. Our indices support displaying any text substring of length (cid:4)
in optimal O((cid:4)/ logσ u) time. In addition, we show how the space can be squeezed
to (1 + )uHk(T ) + o(u log σ ) to obtain a structure with O(m2) average search time
for m (cid:2) 2 logσ u. Alternatively, the search time of LZ-indices can be improved to
O((m + occ) log u) with (3 + )uHk(T ) + o(u log σ ) bits of space, which is much
less than the space needed by other Lempel-Ziv-based indices achieving the same
search time. Overall our indices stand out as a very attractive alternative for spaceefﬁcient 
indexed text searching.
Keywords Text compression · Compressed data structures · Compressed full-text
indices · Lempel-Ziv compression

1 Introduction

Text searching is a classic problem in Computer Science. Given a sequence of symbols 
T [1..u] (the text) over an alphabet  = {1, . . . , σ}, and given another (short)
sequence P[1..m] (the search pattern) over , the full-text search problem consists
in ﬁnding all the occ occurrences of P in T . There exist three typical kinds of queries,
namely:

– Existential queries: operation exists(P ) tells us whether pattern P occurs in T

or not.

– Cardinality queries: operation count(P ) counts the number of occurrences of

pattern P in T .

– Locating queries: operation locate(P ) reports the starting positions of the occ

occurrences of pattern P in T .

With the huge amount of text data available nowadays, the full-text search problem 
plays a fundamental role in modern computer applications, which include text
databases in general. Unlike word-based text searching, we wish to ﬁnd any text
substring, not only whole words or phrases. This has applications in texts where the
concept of word is not well deﬁned (e.g. Oriental languages), or texts where words do
not exist at all (e.g., DNA, protein, and MIDI pitch sequences, program code, etc.).
We assume that the text is large and known in advance to queries, and we need to
perform many queries on it. Therefore, we can construct an index on the text, which
is a data structure allowing efﬁcient access to the pattern occurrences, yet increasing
the space requirement.

Classical full-text indices, like sufﬁx trees [1] and sufﬁx arrays [33], have the problem 
of a high space requirement: they require O(u log u) and u log u bits respectively,

56

Algorithmica (2012) 62:54–101

which in practice is about 10–20 and 4 times the text size respectively, apart from the
text itself. Thus, we can have large texts which ﬁt into main memory, but whose corresponding 
sufﬁx tree (or array) does not. Using secondary storage for the indices is
several orders of magnitude slower, so one looks for ways to reduce their size, with
the main motivation of maintaining the indices of very large texts entirely in main
memory. Therefore, we seek to provide fast access to the text using as little space as
possible. The modern trend is to use the compressibility of the text to reduce the space
of the index. In recent years there has been much research on compressed text databases,
 focusing on techniques to represent the text and the index using little space,
yet permitting efﬁcient text searching [41].

1.1 Compressed Full-Text Self-Indexing

To provide fast access to the text using little space, a current trend is to use compressed 
full-text self-indices.

Deﬁnition 1 A compressed full-text index is one whose space requirement is proportional 
to the compressed text size under some compression model (e.g., O(uHk(T ))
bits of space1).

Therefore, the space of such an index can be reduced when the text is compressible.
 This track was started by Kärkkäinen and Ukkonen [26, 27], who studied text
indices based on repetitions, and deﬁned the ﬁrst indices based on the Lempel-Ziv
compression algorithms [29]. Later, Grossi and Vitter [22] deﬁned the Compressed
Sufﬁx Arrays, based on regularities of sufﬁx arrays to reduce their space. However,
and just like classical indices, all these indices still need the text to operate. An important 
space saving can be achieved if we lift this restriction.

Deﬁnition 2 A full-text self-index allows one to search and extract any part of the
text without storing the text itself.

A compressed full-text self-index replaces the text with a more space-efﬁcient representation 
of it (proﬁting from text compressibility to obtain smaller indices), at the
same time providing indexed access to the text [16, 41]. Taking space proportional
to the compressed text, replacing it, and providing efﬁcient indexed access to it is an
unprecedented breakthrough in text indexing and compression.

Ferragina and Manzini [14–16], and Sadakane [45] deﬁned the ﬁrst self-indices.
The latter is a representation of Compressed Sufﬁx Arrays [22] which does not need
the text to operate. The index of Ferragina and Manzini, on the other hand, is based
on the close relation between sufﬁx arrays and the Burrows-Wheeler transform [9].
Later, many other compressed self-indices were deﬁned, such as the ones by Grossi
et al. [21], Navarro [39, 40], Mäkinen and Navarro [30], Ferragina et al. [17], and

1uHk (T ), the k-th order empirical entropy of T , is a lower bound to the number of bits used to represent
T by any k-th order compressor. See Sect. 2.2 for more details.

Algorithmica (2012) 62:54–101

57

Russo and Oliveira [44], among others. An important survey about compressed selfindices 
is given by Navarro and Mäkinen [41].

Compressed full-text self-indices are not only useful to reduce the space requirement 
of text indices: they also have applications in cases where accessing the text is
so expensive that the index must search without having the text at hand, as occurs
with most Web search engines.

1.1.1 Extending the Set of Operations

As compressed full-text self-indices replace the text, we are also interested in opera-
tions:

– Displaying contexts around occurrences: operation display(P , (cid:4)) shows us a

context of (cid:4) symbols surrounding the occ occurrences of pattern P in T .
– Decompressing parts of the text: operation extract(i, j ) decompresses the substring 
T [i..j], for any text positions i (cid:3) j .
Thus we can see compressed self-indices as full-text indices compressing the text,
or as compressors allowing efﬁcient text extraction and indexed full-text searching.
For compressed full-text self-indices, which replace the text, being able to efﬁciently
extract arbitrary text substrings is one of the most basic and important problems
that indices must solve efﬁciently.

1.1.2 Families of Compressed Self-Indices

The main types of compressed self-indices [41] are:

– Compressed Sufﬁx Arrays [22] (CSAs for short), as for instance Sadakane’s CSA

(SAD-CSA) [45], and Grossi et al.’s CSA (GGV-CSA) [21].

– Indices based on backward search [16] (which are alternative ways to compress
sufﬁx arrays, known as the FM-index family), as for instance the Alphabet-Friendly
FM-index (AF-FMI) [17]. And

– Indices based on Lempel-Ziv compression [29, 48] (LZ-indices for short), as for
instance Kärkkäinen and Ukkonen’s LZ-index [27], Ferragina and Manzini’s LZindex 
[16], Navarro’s LZ-index (NAV-LZI) [39], and Russo and Oliveira’s LZindex 
(ILZI) [44].

In Table 1 we show the most efﬁcient existing compressed self-indices, where
the different families are separated by horizontal lines. We are particularly interested
in LZ-indices, since they have been proven to be very effective in practice for locating 
occurrences and extracting text [12, 40], outperforming other compressed indices.
 Also, when the texts are highly compressible, LZ-indices can in practice be
smaller and faster than alternative indices, and in other cases they offer very attractive 
space/time trade-offs [4]. What characterizes the particular niche of LZ-indices
is the O(uHk(T )) space combined with O(log u) time per located occurrence.

1.1.3 Previous Work on LZ-Indices

Historically, the ﬁrst compressed index based on Lempel-Ziv compression was that
of Kärkkäinen and Ukkonen [26, 27] (which is based on a speciﬁc version of the

58
Algorithmica (2012) 62:54–101
Table 1 Comparison of our LZ-index with alternative compressed self-indices. We assume σ =
O(polylog(u)) in all cases. Note our change of variable in GGV-CSA to make it easier to compare. All
indices must count before locating

Index

SAD-CSA [45]
GGV-CSA [21]

AF-FMI [17]

NAV-LZI [39]
ILZI [44]
Our LZ-index
Our larger LZ-index
Index

SAD-CSA

GGV-CSA

AF-FMI

Space in bits

(1 + )uH0(T ) + O(u log log σ )
(2 + )uHk (T ) + o(u log σ )
uHk (T ) + o(u log σ )
4uHk (T ) + o(u log σ )
(5 + )uHk (T ) + o(u log σ )
(2 + )uHk (T ) + o(u log σ )
(3 + )uHk (T ) + o(u log σ )

extract
O((cid:4) + log u)

σ u) O((cid:4)/ logσ u + log u log

σ u)

O(occ log

1

1+ u)

O(occ log u log
O(occ log1+ u)

O((cid:4) + log1+ u)
O((cid:4) log σ )
O((cid:4)/ logσ u)
O((cid:4)/ logσ u)
O((m + occ) log u) O((cid:4)/ logσ u)

count

locate

O(m log u)

O( m

logσ u

+ log
log

3+
1+ u
1−
1+ σ

)

O(m)

NAV-LZI
ILZI
Our LZ-index
Our larger LZ-index O(m)

O(m3 log σ + m log u + occ) O(occ log u)
O(m log u + occ)
O(occ log u)
O(m2 + m log u + occ)
O(occ log u)

Lempel-Ziv parsing algorithm of 1976 [29]). It has a locating time of O(m2 + (m +
occ) log u) and a space requirement of O(uHk(T )) bits, plus the text (as it is needed
to operate) [41]. Navarro’s LZ-index [39, 40], on the other hand, is a compressed
full-text self-index based on the Lempel-Ziv 1978 [48] (LZ78 for short) parsing of the
text. See Sect. 2.3 for a description of the LZ78 compression algorithm. The LZ-index
takes about 4 times the size of the compressed text, that is, 4uHk(T )+ o(u log σ ) bits,
for any k = o(logσ u) [16, 28], and supports locate queries in O(m3 log σ + (m +

occ) log u) worst-case time. The index can display a text context of length (cid:4) around
an occurrence found (and in fact any sequence of LZ78 phrases) in O((cid:4) log σ ) time,
or obtain the whole text in time O(u log σ ). The index is built in O(u log σ ) time.

Despite this index having been proven to be very competitive in practice [39, 40],
the O(m3 log σ ) term in the search time makes it appropriate only for short patterns.
Besides, in practice the space requirement of the LZ-index is relatively large compared 
with competing schemes: 1.2–1.6 times the text size (depending on the compressibility 
of the text) versus 0.6–0.7 and 0.3–0.8 times the text size of the CSA
[45] and the FM-index [16], respectively. Yet, the LZ-index is faster to locate and
to display the context of an occurrence, which as explained is very important for
self-indices.

So the challenge is: can we reduce the space requirement of the LZ-index while
retaining its good features? Previous work [4] studies the reduction of the space requirement 
of LZ-index from a practical approach. The result is an LZ-index requiring

Algorithmica (2012) 62:54–101

59

(2 + )uHk(T ) + o(u log σ ) bits of space, and with O( m2
 ) time on average for locate 
queries. However, the space of LZ-index cannot be further reduced by using
that approach.

1.2 Our Contribution

In this paper we go one step further on the results of previous work [4], not only
reducing by half the space requirement of the LZ-index, but also improving its time
complexities. The result is an attractive alternative to the state of the art in compressed
self-indexing.

First, in Sect. 4, we compress one of the data structures composing the original
LZ-index by using an approach which is, in some sense, related to the compression
of sufﬁx arrays [22, 45]. Second, in Sect. 5, we combine the balanced parentheses
representation of Munro and Raman [38] of the LZ78 trie with the xbw transform of
Ferragina et al. [13], whose powerful operations are useful for the LZ-index search
algorithm.
Although these approaches are very different, when σ = (polylog(u)) (that is,
on moderate-size alphabets, which are very common in practice) we achieve in both
cases (2 + )uHk(T ) + o(u log σ ) bits of space, for any constant  > 0, and simultaneously 
improve the original worst-case search time to O(m2 + (m + occ) log u).
Thus we achieve the same search time as the index of Kärkkäinen and Ukkonen [27],
yet ours are much smaller and do not need the text to operate. In both cases we also
present a version requiring (1 + )uHk(T ) + o(u log σ ) bits, with average search
time O(m2) if m (cid:2) 2 logσ u. This space can get as close as desired to the optimal
uHk(T ) under the k-th order entropy model. The worst-case time for extracting any
text substring of length (cid:4) is also improved to the optimal O((cid:4)/ logσ u) for all of our
indices.
Just as for the original LZ-index, our data structures require O(uHk(T )) bits and
spend O(log u) time per occurrence reported, if σ = (polylog(u)). This fast locating 
is the strongest point of our structure. Other data structures achieving the same or
better complexity for locating occurrences either are of size O(uH0(T )) bits plus a
non-negligible extra space of O(u log log σ ) [45], or they achieve this locating time
−1uHk(T ) +
for constant-size alphabets [16]. Finally, the GGV-CSA [21] requires 
1−2
1− ) per occur-
1− (log σ )
o(u log σ ) bits of space, with a locating time of O((log u)
1−3
1+
1− ), where 0 <  < 1/2
1− (log σ )
rence, after a counting time of O( m
is a constant. When  approaches 1/2, the space requirement approaches (from
above) 2uHk(T ) + o(u log σ ) bits, with a counting time of O( m
log σ ) and
still a locating time per occurrence of ω(log u).

+ (log u)

logσ u



+ log3 u

logσ u

In Table 1 we summarize the space and time complexities of some existing compressed 
self-indices (other less competitive ones are ignored [41]). Total locate times
in the table require counting the pattern occurrences ﬁrst. For counting the number of
occurrences of P in T , our data structures are not competitive with schemes requiring 
about the same space [17, 21]. Yet, in many practical situations, it is necessary to
report the occurrence positions, as well as displaying their contexts and extracting (or
uncompressing) any text substring. In this aspect, as explained, our LZ-indices are
superior.

60

Algorithmica (2012) 62:54–101

A new LZ-index, the Inverted LZ-index (ILZI for short) [44], has appeared independently 
and simultaneously with our work [6]. The ILZI is faster than our data
structures since it can report the pattern occurrences in O((m + occ) log u) time, but
at the price of a higher space requirement: (5+ )uHk(T )+ o(u log σ ) bits. However,
in this paper we also show that the same reporting time O((m + occ) log u) can be
obtained with a signiﬁcantly smaller LZ-index requiring (3+ )uHk(T )+ o(u log σ )
bits of space. In practice, our LZ-indices and the ILZI are comparable.

2 Basic Concepts

2.1 Model of Computation

In this paper we assume the standard word RAM model of computation, in which
we can access any memory word of length w, such that w = (log u), in constant
time.2 Standard arithmetic and logical operations (like additions, bit-wise operations,
etc.) are assumed to take constant time in this model. We measure the size of our data
structures in bits.

2.2 Empirical Entropy

uc
u log u
uc

A concept related to text compression is that of the k-th order empirical entropy of
a sequence, T , of symbols over an alphabet of size σ , denoted by Hk(T ) [34]. The
value uHk(T ) provides a lower bound to the number of bits needed to compress T
using any compressor that encodes each symbol considering only the context of k
symbols that precede it in T .
the zero-order empirical entropy of T is deﬁned as H0(T ) =
Formally,
(cid:2)
c∈
, where uc is the number of occurrences of symbol c in T . The sum
includes only those symbols c that occur in T , so that uc > 0. The k-th order empir-
|T s|
ical entropy of T is deﬁned as Hk(T ) = (cid:2)
u H0(T s ), where T s is the subsequence 
of T formed by all the symbols that occur preceded by the context s. Again,
we consider only contexts s that do occur in T .
An important property is that 0 (cid:3) Hk(T ) (cid:3) Hk−1(T ) (cid:3) ··· (cid:3) H0(T ) (cid:3) log σ , for
any k (cid:2) 0. This means that by considering longer contexts we can get more compression.


s∈k

2.3 Lempel-Ziv Compression

The Lempel-Ziv compression algorithm of 1978 (usually named LZ78 [48]) is based
on a dictionary of phrases, in which we add every new phrase computed. At the beginning 
of the compression, the dictionary contains a single phrase b0 of length 0 (i.e.,
the empty string). The current step of the compression is as follows: If we assume
that a preﬁx T [1..j] of T has been already compressed into a sequence of phrases
Z = b1 . . . br , all of them in the dictionary, then we look for the longest preﬁx of the

2log x means (cid:3)log2 x(cid:4) in this paper.

Algorithmica (2012) 62:54–101

61

rest of the text T [j + 1..u] which is a phrase of the dictionary. Once we have found
this phrase, say bs of length (cid:4)s, we construct a new phrase br+1 = (s, T [j + (cid:4)s + 1]),
write the pair at the end of the compressed ﬁle Z, i.e. Z = b1 . . . br br+1, and add the
phrase to the dictionary.
We will call Bi the string represented by phrase bi, thus Br+1 = Bs T [j + (cid:4)s + 1].
In the rest of the paper we assume that the text T has been compressed using the LZ78
algorithm into n+ 1 phrases, T = B0 . . . Bn, such that B0 = ε (the empty string). We
say that i is the phrase identiﬁer corresponding to Bi, for 0 (cid:3) i (cid:3) n.
Property 1 For all 1 (cid:3) t (cid:3) n, there exists (cid:4) < t and c ∈  such that Bt = B(cid:4) · c.

That is, every phrase Bt (except B0) is formed by a previous phrase B(cid:4) plus a symbol 
c at the end. This implies that the set of phrases is preﬁx closed, meaning that any
preﬁx of a phrase Bt is also an element of the dictionary. Therefore, a natural way to
represent the set of strings B0, . . . , Bn is a trie, which we call LZTrie.
Property 2 Every phrase Bi, 0 (cid:3) i < n, represents a different text substring.

This property is used in the LZ-index search algorithm (see Sect. 3). The only exception 
to this property is the last phrase Bn. We deal with the exception by appending
to T a special symbol “$” (cid:5)∈ , assumed to be smaller than any other symbol in the
alphabet. The last phrase will contain this symbol and thus will be unique too.

Example 1 In Fig. 1 we show the LZ78 phrase decomposition for our running example 
text T = “alabar_a_la_alabarda_para_apalabrarla”, where for
clarity we replace blanks by ‘_’, which is assumed to be lexicographically larger than
any other symbol in the alphabet. We show the phrase identiﬁers above each corresponding 
phrase in the parsing. In Fig. 3(a) we show the corresponding LZTrie. Inside
each LZTrie node we show the corresponding phrase identiﬁer.

4

11

12

13

7

8

9

5 6

1 2 3
17
a l ab ar _ a_ la _a lab ard a_p ara _ap al abr arl a$
Fig. 1 LZ78 phrase decomposition for the running example text T = “alabar_a_la_alabarda_
para_apalabrarla”, and the corresponding phrase identiﬁers

14 15

10

16

= (r3, c3), and so on until rk = 0
Deﬁnition 3 Let br = (r1, c1), br1
be phrases of the LZ78 parsing of T . The sequence of phrase identiﬁers r, r1, r2, . . .
is called the referencing chain starting at phrase r.

= (r2, c2), br2

The referencing chain starting at phrase r reproduces the way phrase br is formed
from previous phrases and it is obtained by successively moving to the parent in the
LZTrie.
Example 2 The referencing chain of phrase 9 in Fig. 3(a) is r = 9, r1 = 7, r2 = 2,
and r3 = 0.

62

Algorithmica (2012) 62:54–101

The compression algorithm takes O(u) time in the worst case and is efﬁcient in
practice provided we use the LZTrie, which allows rapid searching of the new text
preﬁx (for each symbol of T we move once in the trie). The decompression needs to
build the same dictionary (the pair that deﬁnes the phrase r is read at the r-th step of
the algorithm).

√
u (cid:3) n (cid:3) u

logσ u . Thus, log n = (log u) and n log u (cid:3)

Property 3 [48] It holds that
u log σ always hold.
Lemma 1 [28] It holds that n log n = uHk(T ) + O(u
In our work we assume k = o(logσ u) (and hence log σ = o(log u) to allow for k > 0);
therefore, n log n = uHk(T ) + o(u log σ ). Also, in the analysis throughout this paper
we will assume σ = O(polylog(u)); we generalize our results to larger values of σ
in Sect. 8.

1+k log σ
logσ u ) for any k.

2.4 Succinct Representations of Sequences and Permutations

A succinct data structure requires space close to the information-theoretic lower
bound, while supporting the corresponding operations efﬁciently. We review here
some results on succinct data structures, which are necessary to understand our work.

2.4.1 Data Structures for rank and select
Given a bit vector B[1..n], we deﬁne the operation rank0(B, i) (similarly rank1) as the
number of 0s (1s) occurring up to the i-th position of B. The operation select0(B, i)
(similarly select1) is deﬁned as the position of the i-th 0 (i-th 1) in B. We assume that
select0(B, 0) always equals 0 (similarly for select1). These operations are supported
in constant time and using n + o(n) bits [11], or even nH0(B) + o(n) bits [42].
Given a sequence S[1..u] over an alphabet , we generalize the above deﬁnition
for rankc(S, i) and selectc(S, i) for any c ∈ . If σ = O(polylog(u)), the solution
of Ferragina at al. [17] supports both rankc and selectc, as well as access to S[i]
for any i, in constant time and requiring uH0(S) + o(u) bits of space. Otherwise
log log u ) and the space is uH0(S) + o(u log σ ) bits. The representation
the time is O(
of Golynski et al. [20] requires n(log σ + o(log σ )) = O(n log σ ) bits of space [7],
supporting selectc in O(1) time, and rankc and access to S[i] in O(log log σ ) time.

log σ

2.4.2 Succinct Representation of Permutations
The problem here is to represent a permutation π of {1, . . . , n}, such that we can
−1(j ) in constant time and using as little space
compute both π(i) and its inverse π
as possible.
−1 is based on
the cycle notation of a permutation. The cycle for the i-th element of π is formed
by elements i, π(i), π(π(i)), and so on until the value i is found again, π k(i) = i.
Then, π k−1(i) = π
−1(i). Every element occurs in one and only one cycle of π. So,

Given π represented in plain form, an efﬁcient solution [37] for π

Algorithmica (2012) 62:54–101

63

−1(j ), instead of looking sequentially for j in π, we only need to look

to compute π
for j in its cycle.

To limit the length of cycles, we create subcycles of size O(1/) by adding a
backward pointer out of O(1/) elements in each cycle of π, for any 0 <  < 1.
We store the backward pointers in an array of n log n bits. We mark the elements
having a backward pointer using a bit vector supporting rank queries [42], which
also helps us to ﬁnd the backward pointer associated with a given element (see [37]
for details). Overall, this solution requires (1 + )n log n + O(n log 1
(1 + )n log n + n + o(n) bits of storage.

 ) + o(n) (cid:3)

2.5 Succinct Representation of Trees

Given a tree with n nodes, there exist a number of succinct representations requiring 
2n + o(n) bits, which is close to the information-theoretic lower bound of
2n − (log n) bits. We explain the representations that we will need in our work.

2.5.1 Balanced Parentheses

The balanced parentheses representation [38] is built from a depth-ﬁrst preorder traversal 
of the tree, writing an opening parenthesis when arriving at a node for the ﬁrst
time, and a closing parenthesis when going up (after traversing the subtree of the
node). In this way, each node is represented by a pair of opening and closing parentheses.
 We identify a tree node x with its opening parenthesis in the representation.
The subtree of x contains those nodes (parentheses) enclosed between the opening
parenthesis representing x and its matching closing parenthesis.
Let par[0..2n − 1] be the balanced parentheses sequence over the alphabet
{(, )}. The preorder position of a node in this representation can be computed
as the number of opening parentheses before the one representing the node. That
is, preorder(x) ≡ rank((par, x) − 1. In this way, the preorder of the tree root is
always 0. Given a preorder position p, the corresponding node is computed by
selectnode(p) ≡ select((par, p + 1).
This representation requires 2n + o(n) bits, supporting operations parent(x)
(which gets the parent of node x), subtreesize(x) (which gets the size of the subtree
rooted at x), depth(x) (which gets the depth of node x in the tree), and ancestor(x, y)
(which tell us whether node x is an ancestor of node y), all of them in O(1) time.
Operation child(x, i) (which gets the i-th child of node x) can be computed in O(i)
time.

Example 3 In Fig. 2(a) we show the balanced parentheses representation for the
LZTrie of Fig. 3(a), along with the sequence of phrase identiﬁers (ids) in preorder,
and the sequence of symbols labeling the edges of the trie (letts), also in preorder. As
the identiﬁer corresponding to the LZTrie root is always 0, we do not store it in ids.
The data associated with node x is stored at position preorder(x) both in ids and letts
sequences. Note this information is sufﬁcient to reconstruct LZTrie.

64

Algorithmica (2012) 62:54–101

Fig. 2 Succinct representations of LZTrie for the running example

2.5.2 DFUDS Representation

To obtain this representation [8] we perform a preorder traversal on the tree, and for
every node reached we write its degree in unary using parentheses. For example, 3
reads ‘((()’ under this representation. What we get is almost a balanced parentheses
representation: we only need to add a ﬁctitious ‘(’ at the beginning of the sequence.
A node of degree d is identiﬁed by the position of the ﬁrst of the d + 1 parentheses
representing the node. Given a node x in this representation, say at position i, its preorder 
position can be computed by counting the number of closing parentheses before
position i; in other words, preorder(x) ≡ rank)(par, x − 1) where par represents the
DFUDS sequence of the tree. Given a preorder position p, the corresponding node is
computed by selectnode(p) ≡ select)(par, p) + 1.
This representation requires also 2n + o(n) bits, and we can compute operations
parent(x), subtreesize(x), degree(x) (which gets the degree, i.e., the number of
children, of node x), ancestor(x, y),3 and child(x, i), all in O(1) time. Operation
depth(x) is also supported in constant time [25].
For cardinal trees (i.e., trees where each node has at most σ children, each child
labeled by a symbol in the set {1, . . . , σ}) we use the DFUDS sequence par plus an
array letts[1..n] storing the edge labels according to a DFUDS traversal of the tree: we
traverse the tree in depth-ﬁrst preorder, and every time we reach a node x, we write the
symbols labeling the children of x. In this way, the labels of the children of a given
node are all stored contiguously in letts, which will allow us to support operation
child(x, α) (which gets the child of node x with label α ∈ {1, . . . , σ}) efﬁciently.

Example 4 In Fig. 2(b) we show the DFUDS representation of LZTrie for our running
example.

We support operation child(x, α) as follows. Suppose that node x has position
(cid:8) = rank((par, p) − 1 be the position
(cid:8) − 1) be the

p within the DFUDS sequence par, and let p
in letts for the symbol of the ﬁrst child of x. Let nα = rankα(letts, p

3As ancestor(x, y) ≡ preorder(x) (cid:3) preorder(y) (cid:3) preorder(x) + subtreesize(par, x) − 1.

Algorithmica (2012) 62:54–101

65

(cid:8) − 1 in letts, and let i = selectα(letts, nα + 1) be the
number of αs up to position p
position of the (nα + 1)-th α in letts. If i lies between (and including) positions p
and
(cid:8) + 1), which,
(cid:8) + degree(x) − 1, then the child we are looking for is child(x, i − p
p
as we said before, is computed in constant time over par; otherwise x does not have a
child labeled α. We can also retrieve the symbol by which x descends from its parent
with letts[rank((par, parent(x)) − 1 + childrank(x) − 1], where the ﬁrst term stands
for the position in letts corresponding to the ﬁrst symbol of the parent of node x, and
the second term childrank(x) is the rank of node x within its siblings, which can be
computed in constant time [25].

(cid:8)

Thus, the time for operation child(x, α) depends on the representation we use for
rankα and selectα queries. Notice that child(x, α) could be supported in a straightforward 
way by binary searching the labels of the children of x, in O(log σ ) worst-case
time and not needing any extra space on top of array letts. The access to letts[·] takes
constant time in this case.
Instead, we can represent letts with the data structure of Ferragina et al. [17], which
requires n log σ + o(n log σ ) bits of space, and allows us to compute child(x, α) in
log σ
log log u ) time. These times are
O(1) whenever σ = O(polylog(u)) holds. On the other hand, we can use the data
structure of Golynski et al. [20], requiring O(n log σ ) bits of space, yet allowing us
to compute child(x, α) in O(log log σ ) time, and access to letts[·] also in O(log log σ )
time. In most of this paper we will use the representation of Ferragina et al., since it
is faster for polylog-sized alphabets.

log log u ) time. The access to letts[·] also takes O(

log σ

O(

The scheme we have presented to represent letts is slightly different from the original 
one [8], which achieves O(1) time for child(x, α) for any σ . However, ours is
simpler and allows us to efﬁciently access letts[·], which will be very important in
our indices to extract text substrings. We need to store the array of symbols explicitly
in case we need to access them (fortunately, this will not asymptotically affect the
space requirement of our results).

2.5.3 xbw Representation

The xbw transform of Ferragina et al. [13] is a succinct representation for labeled
trees: Given a labeled tree T , with n nodes and labels taken from an alphabet 
of size σ , the xbw transform of T is computed by traversing the tree in preorder,
and for each node writing a triplet in a table ST . The ﬁrst component of each triplet
(Slast) indicates whether the node is the last child of its parent in the tree, the second
component (Sα) is the symbol labeling the edge by which we reach the node, and
the third component (Sπ ) is the string labeling the path from the parent of the node
to the root of T . In this way each node is represented by a row in ST . As a last
step we perform an upward-path-sorting of the table by stably sorting the rows of ST
lexicographically according to the strings in Sπ .

Example 5 In Table 2 we show the xbw transform for the LZTrie of Fig. 3(a).

We have to add a dummy child to each leaf, labeling the dummy edge with a
special symbol  not in , so that the paths leading to the leaves appear in column

66

Algorithmica (2012) 62:54–101

Table 2 xbw representation for the LZTrie of Fig. 3(a)

i

1
2
3
4
5
6
7
8
9
10
11
12
13

Slast

Sα

Sπ

0
0
1
1
0
0
0
0
1
1
1
1
1

a
l
_


$
b
l
r
_
b


p
r

Empty string
Empty string
Empty string
$a
a
a
a
a
a
al
ara
a_
ba

i

14
15
16
17
18
19
20
21
22
23
24
25
26

Slast

Sα

1
1
1
1
1
1
1
0
0
1
1
1
1




a








a
d
l


a
p

Sπ

bal
dra
l
la
lra
pa_
p_a
ra
ra
ra
rba
_
_a

Sπ and later we can search for them. As we said before, each node in the LZTrie is
represented by a row in the table, and the row number is called the xbw position of
the node.
The xbw representation supports operations parent(x), child(x, i), and child(x, α),
all of them in O(1) time if σ = O(polylog(u)), and using 2n log σ + O(n) bits of
space, because the column Sπ of the table is not stored. The representation also allows
subpath queries, a very powerful operation which, given a string s, returns all the
nodes x such that s is a preﬁx of the string labeling the path from the parent of x to
the root. If σ = O(polylog(n)), subpath queries can be computed in O(|s|) time [13].
For general σ , the time for all these operations depends on the representation used for
log σ
Sα (since we need to support rank and select operations on it), which is O(
log log u )
time if we use the representation of [17], and O(log log σ ) time if we use the data
structure of [20], in which case the space requirement is O(n log σ ).

Because of the upward-path sorting in table ST , the result of a subpath query is a

contiguous interval in such table, containing the answers to the query.
Example 6 A subpath query for string ‘r’ yields the interval [21..24] in Table 2,
corresponding to the nodes with preorders 7, 8, and 9 in Fig. 3(a), plus a ﬁctitious leaf
which is a child of node with preorder 4. As another example, a subpath query for
string ‘ba’ yields the xbw interval [13..14], for node with preorder 4 plus a ﬁctitious
leaf which is a child of node with preorder 14. In all cases, note that the string s we
are looking for is a preﬁx of the corresponding string in Sπ .

3 The LZ-index Data Structure
Assume that the text T [1..u] has been compressed using the LZ78 algorithm into
n + 1 phrases T = B0 . . . Bn, as explained in Sect. 2.3. Next we describe the original

Algorithmica (2012) 62:54–101

67

LZ-index data structure and search algorithms, and introduce some improvements on
them.
Hereafter, given a string S = s1 . . . si, we will use Sr = si . . . s1 to denote its reverse.
 Moreover, Sr[i..j] will actually mean (S[i..j])r .

3.1 Original LZ-index Components

The following data structures compose the original LZ-index [39, 40]:

1. LZTrie: the trie formed by all the phrases B0 . . . Bn. Given the properties of LZ78
compression, this trie has exactly n+ 1 nodes, each one corresponding to a string.
n. In this trie there could

2. RevTrie: the trie formed by all the reverse strings B r

0 . . . B r

be internal nodes not representing any phrase. We call these nodes “empty”.

3. Node: a mapping from phrase identiﬁers to their node in LZTrie.
4. Range: a data structure for two-dimensional searching in the space [0..n]×[0..n].
We store the points {(preorderr (t ), preorderlz(t + 1)), t ∈ 0 . . . n− 1} in this structure,
 where preorderr (t ) is the RevTrie preorder of the node for phrase t (considering 
only the non-empty nodes in the preorder enumeration), and preorderlz(t + 1)
is the LZTrie preorder of node for phrase t + 1. For each such point, the corresponding 
t value is stored.

Figure 3 shows the LZTrie, RevTrie, Range, and Node data structures corresponding 
to our running example. We show preorder numbers outside each trie node. Empty
RevTrie nodes are shown in light gray. The next example gives a hint on the usage of
those structures for searching, which will be detailed in Sect. 3.3.

Example 7 To ﬁnd all phrases ending with substring ‘ab’ in the running example, we
search for the reversed string ‘ba’ in RevTrie, reaching the node with preorder 6. The
subtree of this RevTrie node contains the phrases we are looking for: phrases 3 and 9
(see Fig. 1). As the preorder interval in RevTrie deﬁned by this subtree is [6..7], this
means that the horizontal semi-inﬁnite range [−∞..∞] × [6..7] in Range also contains 
those phrases. To ﬁnd all phrases starting with ‘ar’, note that the LZTrie subtree 
for node with preorder (incidentally also) 6 (which corresponds to string ‘ar’)
contains the phrases starting with ‘ar’: phrases 4, 12, 10, and 16. The LZTrie preorder 
interval for this subtree is [6..9]. This means that the vertical semi-inﬁnite range
[6..9]×[−∞..∞] contains phrases i such that phrase i + 1 starts with ‘ar’: phrases
3, 11, 9, and 15. Finally, the range [6..9] × [6..7] contains the phrase numbers i such
that phrase i ends with ‘ab’ followed by phrase i + 1 starting with ‘ar’: phrases 3
and 9, see Fig. 3(c).

3.2 Succinct Representation of the LZ-index Components
In the original work [39], each of the four structures described requires n log n +
o(u log σ ) bits of space if they are represented succinctly.
– LZTrie is represented using the balanced parentheses representation of Sect. 2.5
requiring 2n+ o(n) bits; plus the sequence letts of symbols labeling each trie edge,

68

Algorithmica (2012) 62:54–101

e
l
p
m
a
x
e

g
n
i
n
n
u
r

e
h
t

r
o
f

s
t
n
e
n
o
p
m
o
c

x
e
d
n
i
-
Z
L

3

.

g
i
F

Algorithmica (2012) 62:54–101

69

requiring n log σ bits; and the sequence ids of n log n bits storing the LZ78 phrase
identiﬁers. Both letts and ids are stored in preorder, so we use preorder(x) to index
them. See Fig. 2(a) for an illustration.

– For RevTrie, balanced parentheses are also used to represent the Patricia tree [35]
(cid:8) (cid:3) 2n
structure of the trie, compressing empty unary nodes and so ensuring n
nodes. This requires at most 4n + o(n) bits. The RevTrie-preorder sequence of
identiﬁers (rids) is stored in n log n bits (i.e., we only store the identiﬁers for nonempty 
nodes). The symbols labeling the edges of the trie and the Patricia-tree skips
are not stored in this representation, since they can be retrieved by using the connection 
with LZTrie [39]. Therefore, the navigation on RevTrie is more expensive
than that on LZTrie.
– For Range, the data structure of Chazelle [10] permits two-dimensional range
searching in a grid of n pairs of integers in the range [0..n] × [0..n]. This data
structure supports range queries in O((occ + 1) log n) time, where occ is the number 
of occurrences reported, and requiring n log n + O(n log log n) bits of space
[31]. Note that since n is the number of LZ78 phrases of text T , the latter term
O(n log log n) is o(u log σ ). This data structure can count the number of points in
a given range in O(log n) time.
– Finally, Node is just a sequence of n pointers to LZTrie nodes. As LZTrie is implemented 
using balanced parentheses, Node[i] stores the position within the sequence 
for the opening parenthesis representing the node corresponding to phrase
i. As there are 2n such positions, we need n log 2n = n log n + n bits of storage.
See Fig. 3(d) for an illustration.
According to Lemma 1, the ﬁnal size of the LZ-index is 4uHk(T ) + o(u log σ ) bits
for k = o(logσ u) (and hence log σ = o(log u) to achieve more than zero-order com-

pression).

The succinct trie representations used in [39] implement (among others) operations 
parent(x) and child(x, α), both in O(log σ ) time for LZTrie, and O(log σ ) and
O(h log σ ) time respectively for RevTrie, where h is the depth of node x in RevTrie
(the h in the cost comes from the fact that we must access LZTrie to get the label of
a RevTrie edge). The operation ancestor(x, y) is implemented in O(1) time both in
LZTrie and RevTrie.

3.3 LZ-index Search Algorithm
Let us consider now the search algorithm for a pattern P[1..m] [39, 40]. For locate 
queries, pattern occurrences are reported in the format [[t, offset]], where t
is the phrase where the occurrence starts, and offset is the distance between the
beginning of the occurrence and the end of the phrase. Later, in Sect. 7, we will show
how to map these two values to a single text position. As we deal with an implicit
representation of the text (the LZTrie), and not the text itself, we distinguish three
types of occurrences of P in T , depending on the phrase layout.

3.3.1 Occurrences of Type 1

The occurrence lies inside a single phrase (there are occ1 occurrences of this type).
Given Property 1, every phrase Bt containing P is formed by a shorter phrase B(cid:4)

70

Algorithmica (2012) 62:54–101

followed by a symbol c. If P does not occur at the end of Bt , then B(cid:4) contains P as
well. We want to ﬁnd the shortest possible phrase Bi in the LZ78 referencing chain
for Bt that contains the occurrence of P .

Note that P r is a preﬁx of B r

i , so Bi can easily be found by searching for P r
(cid:8)
in RevTrie in O(m2 log σ ) time. Say we arrive at node vr . Any node v
r descending
from vr in RevTrie (including vr itself) corresponds to a phrase terminated with P .
This corresponds with subpath queries (see Sect. 2.5.3) in LZTrie. For each such
(cid:8)
r , we traverse and report the subtree of the corresponding LZTrie node vlz (found
v
(cid:8)
lz in the subtree of vlz, we report an occurrence
using rids and Node). For any node v
lz) − depth(vlz))]], where t is the phrase identiﬁer (ids) of node v
[[t, m + (depth(v
(cid:8)
(cid:8)
lz.
Occurrences of type 1 are located in O(m2 log σ + occ1) time, since locating each
occurrence takes constant time in LZTrie. For cardinality queries we just need to
(cid:8)
lz in that subtree corresponds
compute the subtree size of each vlz in LZTrie, as every v
to an occurrence of type 1.

3.3.2 Occurrences of Type 2
The occurrence spans two consecutive phrases, Bt and Bt+1, such that a preﬁx P[1..i]
matches a sufﬁx of Bt and the sufﬁx P[i + 1..m] matches a preﬁx of Bt+1 (there are
occ2 occurrences of this type). P can be split at any position, so we have to try them
all. The idea is that, for every possible split, we search for the reverse pattern preﬁx
P r[1..i] in RevTrie (obtaining node vr ) and for the pattern sufﬁx P[i + 1 . . . m] in
LZTrie (obtaining node vlz).

As in a trie all the strings represented in a subtree form a preorder interval, we have
two preorder intervals: one in the space of reversed phrases (phrases ﬁnishing with
P[1..i]) and one in that of the normal phrases (phrases starting with P[i + 1..m]). We
need to ﬁnd the phrase pairs (t, t+1) such that t is in the RevTrie preorder interval and
t + 1 is in the LZTrie preorder interval. As we have seen in Example 7, this is what
the range searching data structure (Range) is for. If we denote plz = preorder(vlz)
and pr = preorder(vr ), we must search Range for [plz..plz + subtreesize(vlz)− 1]×
[pr ..pr + subtreesize(vr ) − 1]. For every pair (t, t + 1) found, we report occurrence
[[t, i]]. Occurrences of type 2 are located in O(m3 log σ + (m + occ2) log n) time,
where the ﬁrst term comes from searching the tries (in particular, searching for the
O(m) partitions of P in the RevTrie), and the second one is for the m − 1 range
searches on RevTrie.

3.3.3 Occurrences of Type 3
The occurrence spans three or more phrases, Bt−1, . . . , B(cid:4)+1, such that P[i..j] =
Bt . . . B(cid:4), P[1..i − 1] matches a sufﬁx of Bt−1 and P[j + 1..m] matches a preﬁx
of B(cid:4)+1 (there are occ3 occurrences of this type). We need one more observation for
this part: Since the LZ78 algorithm guarantees that every phrase represents a different
string (Property 2), there is at most one phrase matching P[i..j] for each choice of i
and j . Therefore, if we partition P into more than two consecutive substrings, there
is at most one pattern occurrence for such partition, which severely limits occ3 to
O(m2), since this is the number of different partitions of P .

Algorithmica (2012) 62:54–101

71

Let us deﬁne matrix Clz[1..m, 1..m] and arrays Ai, for 1 (cid:3) i (cid:3) m, which store
information about the search. We ﬁrst identify the only possible phrase matching
each substring P[i..j]. This is done by searching for every pattern substring P[i..j]
in LZTrie, for i = 1, . . . , m and j = i, . . . , m. Thus, we perform a single search in the
trie for each i. We record in Clz[i, j] the LZTrie node corresponding to P[i..j], and
store the pair (id, j ) at the end of Ai, such that id is the phrase identiﬁer of the node
corresponding to P[i..j]. Note that since we search for P[i..j] for increasing j , we
get the values of id in increasing order, as the phrase identiﬁer of a node is always
larger than that of the parent node. Therefore, the corresponding pairs in Ai are stored
by increasing value of id. This process takes O(m2 log σ ) time.
Then we ﬁnd the O(m2) maximal concatenations of successive phrases that match
contiguous pattern substrings. For 1 (cid:3) i (cid:3) j (cid:3) m, for increasing j , we try to extend
the match of P[i..j] to the right. If id is the phrase identiﬁer for node Clz[i, j], then
we have to search for (id+ 1, r) in array Aj+1, for some r. Array Aj+1 can be binary
searched because it is sorted. If we ﬁnd (id + 1, r) in Aj+1, this means that Bid =
P[i..j] and Bid+1 = P[j + 1..r], which also means that the concatenation of phrases
Bid Bid+1 equals P[i..r]. We repeat the process from j = r, and stop when the pair
(id + 1, r) is not found in the corresponding array (this means that a concatenation
of phrases cannot be extended further, so the current concatenation is maximal). See
[39] for further details. As we have to perform O(m2) binary searches in arrays of
size O(m), this procedure takes O(m2 log m) worst-case time.
Let P[i..j] = Bt . . . B(cid:4) be a maximal concatenation. Then we check whether
phrase B(cid:4)+1 starts with P[j + 1..m], that is, we check whether Node[(cid:4) + 1] is a descendant 
of node Clz[j + 1, m], in constant time per maximal concatenation. Finally
we check whether phrase Bt−1 ends with P[1..i − 1], by starting from Node[i − 1] in
LZTrie and successively going to the parent to check whether the last i − 1 symbols,
read upwards, equal P r[1..i − 1], in O(m log σ ) time per maximal concatenation. If
all these conditions hold, we report an occurrence [[t − 1, i− 1]]. Overall, occurrences
of type 3 are located in O(m3 log σ ) time.

3.3.4 Overall Query Time
Note that each of the occ = occ1 + occ2 + occ3 possible occurrences of P lies exactly 
in one of the three cases above. Overall, the total search time to report the occ
occurrences of P in T is O(m3 log σ + (m + occ) log u).

3.3.5 Extracting Text Substrings

The original LZ-index is able to extract text substrings, yet not in the way we have
deﬁned before: we have to provide an LZ78 phrase number from where to start the
extraction. We assume also that the (cid:4) symbols we want to extract correspond to whole
phrases (in Sect. 7 we shall avoid these restrictions). Given phrase i, we follow the
upward path from Node[i] up to the LZTrie root, outputting the symbols labeling the
upward path. Then we perform the same procedure but now starting from Node[i+ 1]
in LZTrie, and so on until we extract the (cid:4) desired symbols, taking overall O((cid:4) log σ )
time, because operation parent is implemented in O(log σ ) time [39]. Finally, we
can uncompress the whole text T in O(u log σ ) time using the same idea, starting the
procedure from the ﬁrst LZ78 phrase.

72

Algorithmica (2012) 62:54–101

3.3.6 Improving the Algorithm for Finding Maximal Concatenations

In the case of occurrences of type 3, we now improve the algorithm for ﬁnding maximal 
concatenations of phrases, replacing the binary searches on arrays Ai by an
access to the correct position in matrix Clz.
When computing maximal concatenations of phrases, for each 1 (cid:3) i (cid:3) j (cid:3) m, for
increasing j , we try to extend the match of P[i..j] to the right. Let id be the phrase
identiﬁer for node Clz[i, j]. Then P[i..j] = Bid holds. Then, to check whether P[j +
1..r] = Bid+1 holds, instead of searching for (id + 1, r) in Aj+1 as before, we note
that r = j + l, where l is the length of phrase Bid+1, which in turn is computed as l =
depth(Node[id + 1]) in LZTrie. Then we check, in constant time, whether the node
Clz[j + 1, j + l] corresponds to identiﬁer id + 1. If so, this means that P[j + 1..r] =
Bid+1 holds, for r = j + l, and hence we can extend the concatenation of phrases to
P[i..r] = Bid Bid+1. We repeat the process for j = r and stop the procedure when the
above condition does not hold, or r becomes greater than m.

Note that by using this algorithm to ﬁnd maximal concatenations we reduce the
time to O(m2), which does not improve the total performance of the algorithm for
ﬁnding occurrences of type 3. However, the reduction will be relevant in Sects. 4
and 5 for improved versions of the LZ-index.
Lemma 2 Given the LZ78 parsing of text T $ = B0 . . . Bn, and given a pattern
P[1..m], we can compute the maximal concatenation of successive phrases Bt . . . B(cid:4)
that match contiguous pattern substrings P[i..j], for any 0 (cid:3) i (cid:3) j (cid:3) m, in O(m2)
time overall.

3.4 A More Compact Version of the LZ-index

In the practical implementation of LZ-index [39, 40], the Range data structure deﬁned
in Sect. 3.1 is replaced by RNode, which is a mapping from phrase identiﬁers to
their node in RevTrie. The RNode data structure requires n log n bits, so this practical

version of LZ-index also requires 4uHk(T ) + o(u log σ ) bits, for any k = o(logσ u).

The original search algorithm is modiﬁed as follows (occurrences of type 1 are

found as for the original LZ-index).

3.4.1 Occurrences of Type 2
For every possible split P[1..i] and P[i + 1..m] of P , assume the search for P r[1..i]
in RevTrie yields node vr , and the search for P[i + 1..m] in LZTrie yields node vlz.
Then, one checks each phrase t in the subtree of vr and reports it if Node[t + 1]
descends from vlz. Each such check takes constant time. Yet, if the subtree of vlz
has fewer elements, one does the opposite: check phrases from vlz in vr , using
RNode[t − 1].

Unlike when using Range, now the time to ﬁnd occurrences of type 2 is proportional 
to the size of the smallest subtree among those of vr and vlz, which can be
arbitrarily larger than the number of occurrences reported. That is, by using RNode
we have no worst-case guarantees at search time. However, the average search time

Algorithmica (2012) 62:54–101

Fig. 4 The original LZ-index
navigation structures over index
components

73

for occurrences of type 2 is O(n/σ m/2) [39, 40].4 This is O(1) for long patterns,
m (cid:2) 2 logσ n.

3.4.2 Occurrences of Type 3
For occurrences of type 3, after ﬁnding that P[i..j] = Bt . . . B(cid:4) is a maximal concatenation,
 one checks whether phrase B(cid:4)+1 starts with P[j + 1..m] by using operation 
ancestor(Clz[j + 1, m], Node[(cid:4) + 1]), just as in Sect. 3.3. Instead of checking 
symbol by symbol in the LZTrie to determine whether phrase Bt−1 ends with
P[1..i − 1], as is done with the original LZ-index, one simply checks whether
ancestor(Clz[1, i − 1], RNode[t − 1]) holds in RevTrie.

3.4.3 LZ-index as a Navigation Scheme

This version of the LZ-index can be seen as a navigation scheme, as shown in Fig. 4,
where solid arrows represent the main data structures of the index. Dashed arrows
are asymptotically “for free” in terms of space requirement, since they are followed
by applying rank on the corresponding parentheses structure (see Sect. 2.4). The four
solid arrows are in fact the four main components in the space usage of the index:
array of phrase identiﬁers in LZTrie (ids) and in RevTrie (rids), and mapping from
phrase identiﬁers to tree nodes in LZTrie (Node) and in RevTrie (RNode).

However, we can provide the same navigation functionality needed by the index

with a reduced scheme, which we can represent efﬁciently in the following way:

– LZTrie: the Lempel-Ziv trie, which is implemented with the following data struc-
tures:
• par[0..2n − 1]: the tree shape of LZTrie represented with DFUDS [8], requiring
2n + o(n) bits.
• letts[1..n]: the array of symbols labeling the edges of LZTrie, represented as
explained in Sect. 2.4 so as to allow operation child(x, α) in constant time, requiring 
n log σ + o(n) bits of space. We can get the i-th symbol in preorder by
ﬁrst ﬁnding the i-th node in preorder in par, and then retrieving its symbol as
explained in Sect. 2.4.

4The average is taken over the distribution of P , which is assumed to be statistically independent of T ,
that is, the probability of P[i..i + (cid:4)] = T [k..k + (cid:4)] is 1/σ (cid:4)+1. The text T can be arbitrary.

74

Algorithmica (2012) 62:54–101

• ids[1..n]: the array of LZ78 phrase identiﬁers in preorder. ids[0] = 0 always
holds, so we do not store this value. Note that ids is a permutation of {1, . . . , n},
and hence we use the representation [37] given in Sect. 2.4.2, such that the
−1(j ) can be computed in O(1/) time, requiring
inverse permutation ids
(1 + )n log n + n + o(n) bits, for any 0 <  < 1.

(cid:8)

(cid:8)

) bits of space.

(cid:8) + o(n

(cid:8)

log σ + o(n

– RevTrie: the Patricia tree [35] of the reversed LZ78 phrases, which is implemented
with the following data structures:
• rpar[0..2n
(cid:8) − 1]: the RevTrie structure, compressing empty unary paths and rep-
(cid:8) (cid:3) 2n nodes, because empty non-unary
resented with DFUDS, thus ensuring n
nodes still exist. The space requirement is 2n
) bits to support the same
functionalities as LZTrie.
• rletts[1..n
(cid:8)]: the array storing the ﬁrst symbol of each edge label in RevTrie,
represented as for LZTrie and requiring n
• B[1..n
(cid:8)]: a bit vector supporting rank and select queries, and requiring
(1 + o(1)) bits [36]. This bit vector marks the non-empty nodes: the j -th
(cid:8)
n
bit of B is 1 iff the node with preorder position j in rpar is not empty, otherwise
the bit is 0. Given a position p in rpar corresponding to a RevTrie node, the
associated bit in B is B[rank)(rpar, p − 1)].
• skips[1..n
(cid:8)]: the Patricia tree skip values of the nodes in preorder, using log log u
bits per node and inserting empty unary nodes when the skip exceeds log u. In
this way, one out of log u empty unary nodes could be explicitly represented.
In the worst case, there are O(u) empty unary nodes, of which O(u/ log u) are
explicitly represented. This adds O(u/ log u) nodes to n
, which translates into
O((n
nodes, symbols, and skips.

log u )(3 + log σ + log log u)) = o(u log σ ) bits overall for the RevTrie
– R[1..n]: a mapping from RevTrie preorder positions to LZTrie preorder positions.
Given a non-empty RevTrie node with preorder i (just counting non-empty nodes),
we deﬁne the corresponding LZTrie preorder as R[i] = ids
−1(rids[i]). This is a
permutation and is represented using again the succinct data structure for permutations,
 requiring (1 + )n log n + n + o(n) bits to represent R and compute R
−1 in
O(1/) worst-case time. Given a position p in rpar corresponding to a non-empty
RevTrie node, the associated R value (i.e., preorder in LZTrie) can be computed as
R[rank1(B, rank)(rpar, p − 1))].
In Fig. 5 we draw the navigation scheme. The search algorithm remains the same
since we can map preorder positions to nodes in the DFUDS representation of the tries
and vice versa (see Sect. 2.5.2), and also we can simulate the missing arrays: rids(i) ≡
ids[R[i]], RNode(i) ≡ selectnode(R
−1(i))), and Node(i) ≡ selectnode(ids
−1
(i)), all of which take O(1/) time.

−1(ids

(cid:8) + u

(cid:8)

3.4.4 Space and Time Analysis
The space requirement is (2 + )n log n + 3n log σ + 2n log log u + 10n + o(u) =
(2 + )n log n + o(u log σ ) bits, which according to Lemma 1 is (2 + )uHk(T ) +
o(u log σ ) bits, for any k = o(logσ u).

The child operation on RevTrie can now be computed in O(1) time thanks to
DFUDS, to rletts, and to the skips, versus the O(h log σ ) time of the original LZindex 
[39]. Now, because RevTrie is a Patricia tree and the underlying strings are not

Algorithmica (2012) 62:54–101

75

Fig. 5 A more compact navigation scheme over LZ-index components, requiring (2+ )uHk + o(u log σ )
bits

readily available, it is not obvious how to traverse it. The next lemma addresses this
issue.
Lemma 3 Given a string s ∈ 
, we can determine whether it is represented in
RevTrie or not (and ﬁnding the corresponding node in the afﬁrmative case) in O(|s|)
time.

∗

Proof To ﬁnd the node corresponding to string s, we descend from the RevTrie root,
using operation child(x, α) on the ﬁrst symbol of each edge label, which is stored in
rletts, and using the skips to compute the next symbol of s to use in the descent. If
some symbol of s cannot be matched while descending, then we determine that it is
not represented in RevTrie in O(|s|) time. Otherwise, assume that after consuming
string s in this way, we arrive at node vr with preorder j in RevTrie. The string
labeling the root-to-vr path in RevTrie can be computed by accessing the node vlz
with preorder R[j] in LZTrie, and then extracting the string labeling the vlz-to-root
path in LZTrie. Then we compare that string against s to verify that the node we
arrived at corresponds to s, or otherwise that s does not occur in RevTrie.
In the case where node vr in RevTrie is empty, R[j] is undeﬁned. Notice, however,
that there must be at least one non-empty node descending from this empty node,
since leaves in RevTrie cannot be empty as they always correspond to an LZ78 phrase.
Given that the string represented by every non-empty node in the subtree of node vr
has the string s as a preﬁx, the corresponding strings in LZTrie have sr as a sufﬁx.
So we can use any R value within the subtree of node vr in order to map to the
LZTrie and then extract the string it represents. We can compute the preorder j
of
(cid:8) = select1(B, rank1(B, j ) + 1),
the next non-empty node within that subtree by j
where rank1(B, j ) represents the number of non-empty nodes up to node vr . Thus,
(cid:8)] to access the LZTrie and extract the corresponding string. We know
we use R[j
when to stop extracting, since the length of the string represented by vr matches the
length of the string we are looking for.
(cid:4)

The overall cost for the descending process is therefore O(|s|).

(cid:8)

Operations child and parent on LZTrie can be also computed in O(1) time, versus
the O(log σ ) time of the original LZ-index. Hence, occurrences of type 1 are found

76

Algorithmica (2012) 62:54–101

n

in O(m + occ) time, by using the original algorithm of Sect. 3.3; occurrences of
type 2 are found by using the algorithm explained at the beginning of Sect. 3.4.1, in
σ m/2 ) average time, where the O(1/) factor comes from simulating Node and
O(
−1 and R
−1 respectively) for the checks of type 2; occurrences
RNode (by using ids
of type 3 are found in O( m2
 ) worst-case time, since we use the method of Lemma 2
to ﬁnd the maximal concatenations of phrases in O( m2
 ) time (because Node is sim-
ulated), and then we need to use Node and RNode to check every possible candidate,
in O( m2
 ) worst-case time as well (as explained in Sect. 3.4.2). Therefore, the occ
occurrences of P in T can be located in O( m2
σ m/2 ) average time, for 0 <  < 1.
Since the term O(
 ) on average for
m (cid:2) 2 logσ u.

 ) for m (cid:2) 2 logσ u, the time is O( m2

+ n

n

σ m/2 ) is O( 1



4 Sufﬁx Links in RevTrie
As we have seen in Sect. 3.4, for the LZ-index we can achieve (2 + )uHk(T ) +
o(u log σ ) bits of space and O(m2/) average search time for patterns of length m (cid:2)
2 logσ u [4]. Hence, two questions may arise:
Question 1 Can we reduce the space requirement of LZ-index to (1 + )uHk(T ) +
o(u log σ ) bits, that is, to almost optimal in terms of Hk?

Question 2 Can we retain worst-case guarantees at search time (as for the original
LZ-index), while still using at most (2 + )uHk(T ) + o(u log σ ) bits of storage (as
for the scheme of Fig. 5)?

In this section (and in the next one), we will ﬁnd afﬁrmative answers to these
questions. Speciﬁcally, Theorem 1 shall answer Question 1, and Theorem 2 shall
answer Question 2.

We will build on the more compact scheme described in Sect. 3.4 and illustrated

in Fig. 5. Given a LZTrie node with preorder position i, we deﬁne parentlz(i) ≡
preorder(parent(selectnode(i))). That is, parentlz is the parent operation working
on preorders rather than on DFUDS numbers. Let childlz(i, α) be deﬁned similarly
as childlz(i, α) ≡ preorder(child(selectnode(i), α)). Also, let us deﬁne lettslz(i) ≡
letts[rank((par, parent(x))− 1+ childrank(x)− 1], which yields the symbol by which
the node with preorder i descends from its parent, where node x is computed
as selectnode(i). Let strlz(i) denote the string represented by the node with preorder 
i in LZTrie. In the same way, we deﬁne strr (j ) for the node with preorder j
in RevTrie.

The idea is that we are going to compress the R mapping deﬁned for the compact 
LZ-index of Fig. 5. Let us see this array as a kind of sufﬁx array which, instead
of storing text positions, stores LZTrie preorder positions. R is a lexicographically
sorted array of the reversed LZ78 phases (because it is sorted according to RevTrie
preorders). Given a reversed phrase with preorder i in RevTrie (and preorder R[i] in

Algorithmica (2012) 62:54–101

77

LZTrie), its longest proper sufﬁx has position parentlz(R[i]) in LZTrie (as this corresponds 
to the longest proper preﬁx in LZTrie). Given a reverse phrase with position
j in LZTrie, its lexicographic rank is R

−1[j].

Given this analogy, the question is: can we compress the R mapping just as we can
compress a sufﬁx array [22, 45]? We deﬁne now the analogue in LZ-index to function
 of Compressed Sufﬁx Arrays [22, 45].
Deﬁnition 4 For every RevTrie preorder 1 (cid:3) i (cid:3) n we deﬁne function ϕ such that
ϕ(i) = R

−1(parentlz(R[i])), and ϕ(0) = 0.

∗

, then

We have the following properties for function ϕ.
Property 4 Given a non-empty node with preorder i in RevTrie, such that strr (i) =
ax, for some a ∈ , x ∈ 
1. strr (ϕ(i)) = x,
2. R[ϕ(i)] = parentlz(R[i]), and
3. lettslz(R[i]) = a.
Point 1 means that ϕ acts as a sufﬁx link in RevTrie: since strr (i) = ax, we have that
strlz(R[i]) = xr a (recall that node i in RevTrie corresponds to node R[i] in LZTrie).
Therefore, strlz(parentlz(R[i])) = xr , which ﬁnally means strr (R
−1(parentlz(R[i])))
= strr (ϕ(i)) = x. Point 2 implies that by following a sufﬁx link in RevTrie, we are
“going to the parent” in LZTrie, and it follows from applying R to both sides of
the equation in Deﬁnition 4, as Fig. 6 illustrates. Note that the edge connecting the
LZTrie nodes with preorders R[ϕ(i)] and R[i] is labelled a (as stated by point 3 in
Property 4), which is the same symbol we are missing when following the sufﬁx link
ϕ(i) in RevTrie.

We can prove that RevTrie is sufﬁx closed since LZTrie is preﬁx closed, hence

sufﬁx links are well deﬁned.

Lemma 4 Every non-empty node in RevTrie has a sufﬁx link.

∗

Proof Let us consider any non-empty node in RevTrie with preorder i, such that
strr (i) = ax, for a ∈  and x ∈ 
. As ax is a RevTrie phrase (with preorder i), then
xr a must be a LZTrie phrase (with preorder R[i]). By Property 1 of the LZ78 parsing
it follows that xr is also a LZTrie phrase and thus x must be a RevTrie phrase. Hence,
every non-empty node in RevTrie (i.e., every RevTrie node belonging to a reverse
(cid:4)
LZ78 phrase) has a sufﬁx link.

We will use Property 4 to reduce the space requirement of the R mapping: suppose 
that we do not store R[i], for the RevTrie node with preorder i in Fig. 6, but we
store R[ϕ(i)]; then note that R[i] can be computed as childlz(R[ϕ(i)], a). Russo and
Oliveira [44] also use properties of sufﬁx links in their index; however, their main objective 
is to reduce the locating complexity of their LZ-index to O((m + occ) log u),
not to reduce the space requirement as we do. In Sect. 6, however, we will show how
to use sufﬁx links to reduce the time complexity of our indices, achieving their same
locating complexity while requiring less space.

78

Algorithmica (2012) 62:54–101

Fig. 6 Illustration of Property 4. Preorder numbers, both in LZTrie and RevTrie, are shown outside each
node. Dashed arrows associate a RevTrie node with its corresponding node in LZTrie. This association is
given by the R mapping

Table 3 Illustration of the different components of our index for the running example. In the case of
RevTrie, array rids is shown just for simplicity, yet this is not explicitly stored. In each case, i indicates the
preorders in each trie

LZTrie components

ids[i]

lettslz(i) R

−1(i)

RevTrie components
rids[i] R[i]

i

ϕ(i) L[i] LB[i]

string in RevTrie

i

0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17

0
1
17
3
15
14
4
12
10
16
6
11
2
7
9
5
8
13

a
$
b
r
l
r
a
d
l
_
p
l
a
b
_
a
p

0
2
1
6
15
10
14
4
8
11
17
13
9
3
7
16
5
12

0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17

0
17
1
7
12
8
3
9
10
2
14
16
13
11
4
15
5
6

0
2
1
13
7
16
3
14
8
12
5
9
17
11
6
4
15
10

0
2
0
9
14
16
2
3
14
0
2
14
5
17
2
6
0
2

$
a
a
a
a
b
b
d
l
l
l
p
p
r
r
_
_

0
1
0
0
0
1
0
1
1
0
0
1
0
1
0
1
0

(empty string)
$a
a
al
ara
a_
ba
bal
dra
l
la
lra
pa_
p_a
ra
rba
_
_a

Example 8 In Table 3 we show some arrays composing the reduced version of LZindex 
for our running example, including the ϕ function and the set of reversed LZ78
phrases in RevTrie (in preorder, i.e., lexicographically sorted).

Algorithmica (2012) 62:54–101

79

4.1 Using Sufﬁx Links to Compute R
Let us show how to compute R[i] using function ϕ. We deﬁne array L[1..n], which
for each non-empty node with preorder i in RevTrie stores the ﬁrst symbol of the
string strr (i).
Example 9 In the RevTrie of Fig. 3(b), it holds that L[i] = ‘a’ for every i in the
preorder interval [2, 5]. In the same example, note that if we follow the sufﬁx link
ϕ(i), for 2 (cid:3) i (cid:3) 5, we discard the symbol ‘a’.

In the example of Fig. 6 we have L[i] = a; as we said before, this also means
that L[i] is the label of the edge connecting LZTrie nodes with preorders R[ϕ(i)] and
R[i]. In other words, L[i] = lettslz(R[i]).

Example 10 In Table 3 we show the values of L for our running example.

(cid:8)

(cid:8)[rank1(LB , i)] = L[i]. Hence, L[i] can be computed as L

It is not hard to prove that L[i] (cid:3) L[j] whenever i (cid:3) j : let i and j be preorder
numbers in RevTrie, such that i (cid:3) j . Therefore, for the strings corresponding to these
preorders it holds that strr (i) (cid:3) strr (j ). As L[i] and L[j] store the ﬁrst symbol of
strr (i) and strr (j ) respectively, then it holds that L[i] (cid:3) L[j]. Thus, L can be divided
into σ runs of equal symbols. In this way L can be represented by an array L
of
at most σ log σ bits and a bit vector LB of n + o(n) bits, such that LB[i] = 1 iff
L[i] (cid:5)= L[i − 1], for i = 2 . . . n, and LB[1] = 0 (this position belongs to the text
terminator “$”, which is not in the alphabet). For every i such that LB[i] = 1, we
(cid:8)[rank1(LB , i)] in
store L
O(1) time.
Given a RevTrie preorder position i, in order to compute R[i] we could follow
sufﬁx links in RevTrie starting from node with preorder i, until we reach the RevTrie
root. At this point we could apply, starting from the root of LZTrie, child operations
using the ﬁrst symbol of each RevTrie string we obtained while following sufﬁx links,
in reverse order. This procedure is formalized in the following lemma.
Lemma 5 Given a RevTrie preorder position 0 (cid:3) i (cid:3) n, the corresponding LZTrie
preorder position R[i] can be computed by the following recurrence:

R[i] =

(cid:3)

childlz(R[ϕ(i)], L[i])
0

if i (cid:5)= 0
if i = 0

Proof R[0] = 0 holds from the fact that the preorder position corresponding to the
empty string, both in LZTrie and RevTrie, is 0. To prove the other part, we note that
if x is the parent in LZTrie of node y with preorder position R[i], then the symbol 
labeling the edge connecting x to y is stored in L[i] = lettslz(R[i]). That is,
childlz(parentlz(R[i]), L[i]) = R[i]. The lemma follows from this fact and replacing
(cid:4)
ϕ(i) by Deﬁnition 4 in the recurrence.
Example 11 To compute R[13] for the example of Table 3, which corresponds to
string ‘p_a’ in RevTrie, we need to compute childlz(R[17], L[13]), where L[13] =

80

Algorithmica (2012) 62:54–101

‘p’ and ϕ(13) = 17 corresponds to the RevTrie preorder position of string ‘_a’.
Now, to compute R[17] we need to compute childlz(R[2], L[17]), where L[17] =
‘_’. Then, R[2] is computed as childlz(R[0], L[2]), which is just child(0, ‘a’).
At this point we must perform the child operations from the LZTrie root. Recall 
that we assume the childlz operation works on preorder positions, so we must
compute childlz(childlz(childlz(0, ‘a’), ‘_’), ‘p’) in LZTrie, which is the same as
childlz(childlz(1, ‘_’), ‘p’), which in turn is childlz(10, ‘p’), which ﬁnally yields the
node with preorder position 11. Hence, we conclude that R[13] = 11.

4.2 Compressing the R Mapping

As in the case of the  function of Compressed Sufﬁx Arrays [22, 45], we can prove
the following lemma for the ϕ function, which is the key to compressing the R mapping.

Lemma 6 For every i < j , if L[i] = L[j], then ϕ(i) < ϕ(j ).

Proof Let strr (i) denote the i-th string in the lexicographically sorted set of reversed
strings. Note that strr (i) < strr (j ) iff i < j . If i < j and L[i] = L[j] (i.e., strr (i)
and strr (j ) start with the same symbol), then strr (ϕ(i)) < strr (ϕ(j )) (as strr (ϕ(i))
is strr (i) without its ﬁrst symbol, recall Property 4, point 1), and thus ϕ(i) < ϕ(j ). (cid:4)

Corollary 1 Array ϕ can be partitioned into at most σ strictly increasing sequences.

(cid:8)

As a result, we replace R by ϕ, L

This fact is illustrated in Table 3, where the increasing runs of ϕ, corresponding to
runs of equal symbols in L, are separated by horizontal lines.
, and LB , and use them to compute a given
value R[i]. According to Corollary 1, we can represent ϕ using the idea of Sadakane
[45] to represent . Thus, ϕ can be encoded with nH0(let t s) + O(n log log σ )
bits, and hence we replace the n log n-bits representation of R by the nH0(let t s) +
O(n log log σ ) + n + o(n) = O(n log σ ) = o(u log σ ) bits of the representation of ϕ,
(cid:8)
, and LB .

L

−1 in O(1/) Time

4.3 Computing R and R
According to Lemma 5, the time to compute R[i] is O(|strr (i)|), which actually
corresponds to traversing LZTrie from the root with the symbols of strr (i) in reverse
order. However, the procedure of Lemma 5 can be adapted to allow constant-time
computation of R[i]. We store n values of R in an array R
, plus a bit vector RB of
n + o(n) bits indicating which values of R have been stored, ensuring that R[i] can
be computed in O(1/) time while requiring n log n extra bits.
To determine the R values to be explicitly stored, we ﬁx l = (1/) and carry out
a preorder traversal on LZTrie to mark the nodes (1) whose depth is j · l, for some
j > 0, and such that (2) the corresponding node height is greater or equal to l. Since
for every such marked node we have at least l non-marked nodes descending from it,
we mark O(n) nodes overall. We also ensure that, if we start at an arbitrary node

(cid:8)

Algorithmica (2012) 62:54–101

81

If the node to mark is at preorder position j , then we set RB[R

in LZTrie and go successively to the parent, in the worst case we must apply O(1/)
parent operations to ﬁnd a marked node. It can be the case that near the leaves of
the trie we must follow a longer path to get a marked node, because of condition (2)
above. However, notice that this path is never longer than 2l, which still is O(1/).
On the RevTrie side, this means that in the worst case we must follow O(1/) sufﬁx
links to ﬁnd a node whose R value has been stored.
−1(j )] = 1 (note
that RB is indexed by RevTrie preorder). After we mark the positions of R to be
stored, we scan RB sequentially from left to right, and for every i such that RB[i] = 1,
(cid:8)[rank1(RB , i)] = R[i]. Then, we free R since R[i] can be computed in
we set R
O(1/) worst-case time, as stated by the following lemma.
Lemma 7 Given a RevTrie preorder position 0 (cid:3) i (cid:3) n and given any 0 <  < 1, the
corresponding LZTrie preorder position R[i] can be computed in constant O(1/)
worst-case time by the following recurrence:

(cid:3)

R[i] =

child(R[ϕ(i)], L
(cid:8)[rank1(RB , i)]

R

(cid:8)[rank1(LB , i)])

if RB[i] = 0
if RB[i] = 1.

This structure requires n log n + O(n log σ ) = Hk(T ) + o(u log σ ) bits of space.
−1 using the explicit representation
Note that the same structure used to compute R
of R, can be used under this reduced-space representation of R, with cost O(1/2) to
−1(j ) (as we have to access O(1/) positions in R). However, we show
compute R
−1(j ) in O(1/) time, using a novel approach which basically
now how to compute R
consists in reverting the process used to compute R.
Deﬁnition 5 For every RevTrie preorder 0 (cid:3) i (cid:3) n and every symbol a ∈ , we
deﬁne function ϕ

−1(childlz(R[i], a)).

(i, a) = R

such that ϕ

(cid:8)

(cid:8)

We have the following immediate properties for function ϕ

(cid:8)

.

Property 5 Given a non-empty node with preorder i in RevTrie, such that strr (i) = x,
, then for a ∈  it holds that
for x ∈ 
∗
(i, a)) = ax,
(cid:8)
1. strr (ϕ
(i, a)] = childlz(R[i], a).
2. R[ϕ
(cid:8)

(cid:8)

Point (1) means that ϕ

Next we show how to efﬁciently compute ϕ

acts as a Weiner link [47] in RevTrie. Point (2) means that
by following a Weiner link by symbol a from node with preorder i, we are “going to
a child by symbol a” in LZTrie. See Fig. 7 for an illustration.
while requiring little space (since the
obvious way to represent it requires basically n log n bits). Let SW[1..n] be an array
of n log σ bits storing, for every RevTrie node, in preorder, the symbols by which the
node has Weiner links deﬁned, and let VW be a bit vector. Because of Property 5 (2)
(i.e., following a Weiner link by a symbol in RevTrie means going to the child by the
same symbol in LZTrie), we can use the LZTrie as an aid to construct SW : We perform

(cid:8)

82

Algorithmica (2012) 62:54–101

Fig. 7 Illustration of Property 5 for function ϕ
sponding node in RevTrie. This association is given by R

−1

(cid:8)

. Dashed arrows associate an LZTrie node with its correa 
preorder traversal on RevTrie, and for every non-empty node with preorder i, let d
be the degree of the corresponding LZTrie node R[i]. Then, we write the degree d in
unary in VW , in the format 10d . Thus, the 1s in VW will be used to locate the position
of a node within the data structure (via operation select1), while the 0s in VW shall
be used to locate the position for the symbols of the links of a given node, as we will
see soon. In the same traversal we also store in SW the symbols labeling the children
of node R[i] in LZTrie. We represent arrays VW and SW with data structures for rank
and select queries, requiring o(u log σ ) bits overall.

In order to understand how Weiner links can be represented in a compact way
and computed efﬁciently, we shall store them (conceptually) in such a way that we
can divide the resulting array into at most σ strictly increasing subsequences (note
that this cannot be ensured if we simply store the links in preorder). Let W[1..n]
be the (conceptual) array storing the sequence of Weiner links. We will have an increasing 
subsequence in W for every symbol in the alphabet; every such subsequence
stores the links going out by that symbol. Let CW[1..σ] be an array storing the starting 
position for the subsequence corresponding to every alphabet symbol. We then
carry out a new preorder traversal on RevTrie. For every non-empty node with preorder 
i (counting just non-empty nodes), let R[i] be the corresponding LZTrie node,
of degree d. Let i1 ← select1(VW , i + 1) be the position in VW corresponding to
the current RevTrie node. Let i2 ← rank0(VW , i1) + 1 be the starting position in
SW corresponding to the current node. Then, for every child j = 1, . . . , d of node
R[i], which is labeled by symbol s ← lettslz(childlz(R[i], j )) in LZTrie, we store
W[CW[s] + ranks (SW , i2 − 1)] ← R
i in RevTrie and a symbol a ∈ :

Given this representation, we can compute, for any non-empty node with preorder

−1(childlz(R[i], j )).

(cid:8)

(i, a) ≡ W[CW[a] + ranka(SW , rank0(VW , select1(VW , i + 1)) + 1)].

ϕ

Now it remains to show that this representation can be compressed.

Lemma 8 Array W can be partitioned into at most σ strictly increasing sequences.

Algorithmica (2012) 62:54–101

83

∗

in RevTrie, and position j corresponds to string y ∈ 

Proof Let positions i and j in W , for i < j , correspond to Weiner links going out
by the same symbol a ∈ . Assume that position i corresponds to the node for string
x ∈ 
. Since i < j and
given the way in which W is constructed, it follows that the preorder of the node
for x is smaller than the preorder of the node representing y. This also means that
x < y. Then, ax < ay also holds. Therefore the preorder stored at W[i] (i.e., the one
pointing to the node for string ax) is smaller than the preorder stored at W[j] (which
(cid:4)
points to the node for string ay).

∗

Thus, we could represent W in the same way as array ϕ, requiring overall 
O(n log σ ) = o(u log σ ) bits of space. However, we can do better. Let j ←
childr (0, a) be the preorder of the child of the RevTrie root by symbol a. Notice
that all Weiner links going out by a given symbol, say symbol a, point to a node
within the subtree of the node with preorder j . Since Lemma 8 states that the Weiner
links for symbol a appear in increasing order within the corresponding subsequence
of W , this means that the ﬁrst link for a points to the ﬁrst node in preorder within
the subtree of the node with preorder j , the second link points to the second node in
preorder within the subtree of the node with preorder j , and so on. This means that
just performing a rank on SW allows us to compute the corresponding link, so we do
not need to store array W . Formally, we have:

(cid:8)

(i, a) ≡ childr (0, a) + ranka(SW , rank0(VW , select1(VW , i + 1)) + 1) − 1.

ϕ

(1)

(cid:8)

(Note this mechanism can be regarded as extending the LF-mapping of the BurrowsWheeler 
transform [9, 16] to tries.)

(cid:8)
: suppose that
and its properties in order to compute R
We will use function ϕ
−1(j ) for the LZTrie node with preorder j in Fig. 7, but we store
we do not store R
−1(parentlz(j )), a). For
−1(j ) can be computed as ϕ
−1(parentlz(j )). Then R
R
every LZTrie node that has been marked to store an R value, as explained above, we
−1
−1 in array R
(cid:8)(cid:8)
. We mark in a bit vector R
also store the corresponding value of R
−1 value has been stored. This
B
(according to preorder in LZTrie) the nodes whose R
ensures that, starting at an arbitrary node in LZTrie, we shall ﬁnd a sampled node
after performing at most O(1/) parent operations. Then we can conclude:
Lemma 9 Given an LZTrie preorder position 0 (cid:3) j (cid:3) n and given any 0 <  < 1,
−1(j ) can be computed in O(1/)
the corresponding RevTrie preorder position R
worst-case time by the following recurrence:

(R

(cid:8)

−1(parentlz(j )), lettslz(j ))

(cid:3)

ϕ

(R

(cid:8)
(cid:8)(cid:8)[rank1(R

R

B , j )]
−1

−1
−1

B

B

[j] = 0
[j] = 1.

if R
if R

−1(j ) =

R

This structure requires n log n + O(n log σ ) = Hk(T ) + o(u log σ ) bits of space.

4.4 Space and Time Analysis

As now we store ids in n log n bits, ids
in n log n bits each, and ϕ,
, letts, and rletts in O(n log σ ) = o(u log σ ) bits, the total space requirement is
(cid:8)
ϕ

, and R

−1, R

(cid:8)

(cid:8)(cid:8)

84

Algorithmica (2012) 62:54–101

n

(1 + )n log n + o(u log σ ) bits (renaming 4 = ), and we provide the same navigation 
scheme as in Fig. 5. Occurrences of type 1 are found as usual, in O(m + occ1
 )
occ1
 ) term appears because we have to use R to map from
time, where the extra O(
RevTrie to LZTrie, which takes O(1/) each time. Occurrences of type 2 are found as
σ m/2 ) average time since now the access between tries
explained in Sect. 3.4.1, in O(
−1. For solving occurrences of type 3, we ﬁrst search for all
is provided by R and R
the pattern substrings in LZTrie in O(m2) time, and then compute the maximal concatenations 
of phrases, in O( m2
 ) time by using the improved algorithm of Lemma 2
−1 to simulate Node). Finally,
(the O(1/) factor comes from the fact that we use ids
for each of the O(m2) maximal concatenations found, we carry out the tests as ex-
−1.
plained in Sect. 3.4.2, with cost O( m2
We have proved:
Theorem 1 Given a text T [1..u] over an alphabet of size σ and let n be the number
of phrases in the LZ78 parsing of T , there exists a compressed full-text self-index
requiring (1 + )uHk(T ) + o(u log σ ) bits of space, for σ = O(polylog(u)), any
k = o(logσ u) and any 0 <  < 1. Given a pattern P[1..m], this index is able to
locate (and count) the occ occurrences of P in T in O( m2
σ m/2 ) average time,
which is O( m2

 ) because RNode is implemented by using R

 ) if m (cid:2) 2 logσ n.

+ u



Now we can get worst-case guarantees in the search process by adding Range, the
two-dimensional range search data structure deﬁned in Sect. 3 for the original LZindex,
 requiring n log n+ o(u log σ ) extra bits [31]. Occurrences of type 2 can now be
found in O((m+ occ2) log n) worst-case time by using Range. Occurrences of type 1
and type 3 are found as for the index of Theorem 1. Existential queries, on the other
hand, can be supported by ﬁrst looking whether there is any occurrence of type 1
(i.e., by looking for P r in RevTrie and then checking whether the corresponding
subtree is empty or not) in O(m) time. If there are no occurrences of type 1, we
check whether there is any occurrence of type 2 by partitioning the pattern and using
Range to count the number of occurrences for each partition. This takes O(m log n)
time overall, since we use Range just to count. Finally, if there are no occurrences
of type 2, we look for occurrences of type 3 in O(m2/) time. Hence, we have the
following theorem.
Theorem 2 Given a text T [1..u] over an alphabet of size σ , there exists a compressed 
full-text self-index requiring (2 + )uHk(T ) + o(u log σ ) bits of space, for
σ = O(polylog(u)), any k = o(logσ u) and any 0 <  < 1. Given a search pattern
P[1..m], this index is able to:
1. locate the occ occurrences of P in T in O( m2

 ) worst-case

time;

2. count the number of pattern occurrences in O( m2

 ) worst-case

time; and

3. determine whether P exists in T in O( m2





+ (m+ occ) log u+ occ
+ m log u + occ
+ m log u) worst-case time.



Algorithmica (2012) 62:54–101

85

In Sect. 8 we show that the theorem is valid for the more general case log σ =
o(log u). We leave for Sect. 7 the study of display and extract queries on our
indices.

5 Using the xbw Transform to Represent the LZTrie

A different idea to reduce the space requirement of LZ-index is to use the xbw transform 
of Ferragina et al. [13] to represent the LZTrie. We show that subpath queries,
which are efﬁciently supported by the xbw transform (see Sect. 2.5.3), are so powerful 
that we can carry out the work of both LZTrie and RevTrie with only the xbw
representation of LZTrie, thus achieving the same result as in Sect. 4 (always assuming 
σ = O(polylog(u))), yet by very different means. Ferragina et al. [13] have
shown how the xbw representation can be compressed in order to take advantage of
the tree regularities, which can be very important in practice and adds extra value to
this representation.

5.1 Index Deﬁnition

We represent the LZ-index with the following data structures:

– xbw LZTrie: the xbw representation [13] of LZTrie, where the nodes are lexicographically 
sorted according to their upward paths in the trie. We store,
• Sα: the array of symbols labeling the edges of the trie, in the order deﬁned in
Sect. 2.5.3. In the worst case LZTrie has 2n nodes (because of the dummy leaves
we add, recall Sect. 2.5.3). We represent this array by using a data structure for
rank and select [17], which are needed to compute the operations on xbw. The
space requirement is 2n log σ + o(n log σ ) bits.
• Slast: a bit array such that Slast[i] = 1 iff the corresponding node in LZTrie is the
last child of its parent. We represent this array with a data structure for rank and
select [36]. The space requirement is at most 2n + o(n) bits.
– Balanced parentheses LZTrie: the trie of the Lempel-Ziv phrases, implemented by,
• par: the balanced parentheses representation [38] of LZTrie. In order to index
the LZTrie leaves with xbw, we have to add a dummy child to each, as was
(cid:8) (cid:3) 2n nodes. Non-dummy
explained in Sect. 2.5.3 In this way, the trie has n
nodes are marked in a bit vector B[1..n
(cid:8)] in the same way as empty nodes are
marked in RevTrie (see Sect. 3.4). We represent array B with a data structure
(cid:8) + o(n) bits,
for rank and select queries [36]. The space requirement is 2n
which is 6n+ o(n) bits in the worst case. This sequence par is needed to support
some operations which are not supported by the xbw, such as ancestor(x, y) and
depth(x).
• ids: the array of LZ78 phrase identiﬁers in preorder, only for non-dummy nodes
(we ﬁnd the phrase identiﬁer for a given node by using rank1 on B). This array is
represented with the data structure for permutations [37], so that we can compute
−1 in O(1/) time, requiring (1+)n log n+n+o(n)
the inverse permutation ids
bits (recall Sect. 2.4.2).

(cid:8) + n

86

Algorithmica (2012) 62:54–101

– Pos: a mapping from xbw positions to the corresponding LZTrie preorder positions
(this is a permutation of LZTrie preorders). In the worst case there are 2n such
positions, so the space requirement is 2n log (2n) bits. We can reduce this space to
(cid:8)
one out of O(1/) values of Pos, so that
n log (2n) bits by storing in an array Pos
Pos[i] can be computed in O(1/) time. We need a bit vector PosB of 2n + o(n)
bits indicating which values of Pos have been stored. Assume we need to compute
the preorder position Pos[i], for a given xbw position i. If PosB[i] = 1, then such
(cid:8)[rank1(PosB , i)]. Otherwise, we simupreorder 
position is stored explicitly at Pos
late a preorder traversal in xbw from the node at xbw position i, until PosB[j] = 1,
for an xbw position j . Each preorder step we perform in xbw corresponds to moving 
to the next opening parenthesis in par. Once this j is found, we map to the
(cid:8)[rank1(PosB , j )]. If d is the number of nodes in prepreorder 
position j
(cid:8) − d is the preorder
order traversal from xbw position i to xbw position j , then j
position corresponding to the node at xbw position i.
−1, which can be done in O(1/2) time under this
scheme, requiring n log (2n) extra bits if we use the representation [37] for inverse
−1 in O(1/) time
permutations. However, we can support the computation of Pos
(cid:8)
as follows. For every node such that its Pos value has been stored in Pos
, we
. If we want to compute
also store the corresponding value of Pos
−1[i], we ﬁrst compute the preorder of the previous node that has been sampled
Pos
, at j = l(cid:12) i
(cid:13), where l = (1/). Then, we use the sample value stored at
(cid:8)(cid:8)
in Pos
(cid:13)] to map to the xbw, and then carry out i − j preorder steps in xbw, to ﬁnd
(cid:8)(cid:8)[(cid:12) i
Pos
the node corresponding to Pos

−1[i]. This takes O(1/) time in the worst case.

−1 in array Pos
(cid:8)(cid:8)

l

l

(cid:8) = Pos

We also need to compute Pos

– Range: a range search data structure in which we store the point k (belonging to
phrase identiﬁer k) at coordinate (x, y), where x is the xbw position of node for
phrase k and y is the LZTrie preorder position of node for phrase k+ 1. We use the
data structure of Chazelle [10], as for the original LZ-index. The space requirement
is n log n + O(n log log n) = n log n + o(u log σ ) bits.

Example 12 See Table 2 for an illustration of the xbw of LZTrie for the running
example, and Fig. 8 for an illustration of the balanced parentheses LZTrie and Pos.

In Fig. 9 we show the basic resulting navigation scheme following the notation 
of Sect. 3.4. The total space requirement is (2 + )n log n + 2n log σ +
11n + O(n log log n) + o(n) bits, which is (2 + )uHk(T ) + o(u log σ ) bits for
k = o(logσ u).

5.2 Search Algorithm

We depict now the search algorithm for a pattern P of length m.

5.2.1 Occurrences of Type 1

Recall from Sect. 3.3 that ﬁrst we need to ﬁnd all the phrases having P as a sufﬁx.
To do this we perform a subpath query with P r on the xbw representation of LZTrie,
simulating in this way the work done on RevTrie in the original scheme, in O(m)

Algorithmica (2012) 62:54–101

87

s
o
P

e
h
t

w
o
h
s

o
s
l
a

.

n
o
i
t
a
t
n
e
s
e
r
p
e
r

w
b
x

e

W

e
h
t
h
]
t
i

.

e
h
[
t

w
s
e
v
a
e
l

)
l
a
n
i
g
i
r
o
(

x
e
d
n
i
o
t

r
e
d
r
o

n
i
d
e
d
d
a

s
e
v
a
e
l

y
m
m
u
d

h
t
i

w

,
e
l
p
m
a
x
e

g
n
i
n
n
u
r

e
h
t

r
o
f

e
i
r
T
Z
L
f
o

n
o
i
t
a
t
n
e
s
e
r
p
e
r

s
e
s
e
h
t
n
e
r
a
p
-
d
e
c
n
a
l
a
B

8

.

g
i
F

)
)
x
(
r
e
d
r
o
e
r
p
,

B
(
1
k
n
a
r

s
d
i

s
a

d
e
t
u
p
m
o
c

e
b

n
a
c

r
e
ﬁ

i
t
n
e
d
i

)
s
n
o
i
t
i
s
o
p
w
b
x
o
t

s
r
e
d
r
o
e
r
p

e
i
r
T
Z
L
m
o
r
f
(

e
s
a
r
h
p

g
n
i
p
p
g
a
n
m
i
d
n
1
o
−
p
s
e
r
r
o
c

s
o
P
e
h
t
d
n
a

e
h
t

,

x

e
d
o
n

n
e
v
i
G

.

0

a

h
t
i

w
e
d
o
n

y
m
m
u
d

h
c
a
e

s
k
r
a
m

B

r
o
t
c
e
v

t
i

B

,
)
s
r
e
d
r
o
e
r
p
e
i
r
T
Z
L
o
t

,

2
e
l
b
a
T
e
e
s

,
s
n
o
i
t
i
s
o
p
w
b
x
m
o
r
f
(

g
n
i
p
p
a
m

88

Fig. 9 Basic navigation scheme
using the xbw representation of
the LZTrie

Algorithmica (2012) 62:54–101

time. Suppose that we obtain the interval [x1..x2] in the xbw of LZTrie, corresponding 
to all the nodes whose phrase ends with P . In other words, the interval [x1..x2]
contains the roots of the subtrees containing the nodes we are looking for to ﬁnd
occurrences of type 1. For each position i ∈ [x1..x2], we can get the corresponding
preorder in the parentheses representation using Pos(i), which takes O(1/) time,
and then selectnode(Pos(i)) over par yields the node position. As in the worst case
occ1
this mapping is carried out occ1 times, the overall time is O(
 ). Finally, we traverse 
the subtrees of these nodes in par and report all the identiﬁers found, in constant
time per occurrence as done with the usual LZ-index.

5.2.2 Occurrences of Type 2
To ﬁnd occurrences of type 2, for every possible partition P[1..i] and P[i + 1..m]
of P , we traverse the xbw from the root, using operation child(x, α) with the symbols 
of P[i + 1..m]. This takes O(m2) time overall for the m − 1 partitions of P .
In this way we are simulating the work done on LZTrie when searching for occurrences 
of type 2 in the original scheme. Once this is found, say at xbw position
j , we switch to the preorder tree (parentheses) using selectnode(Pos(j )) over par,
to get the node vlz whose subtree has preorder interval [y1..y2] of all the nodes
whose strings start with P[i + 1..m]. This takes overall O( m
 ) time, for the m − 1
partitions of P . Next we perform a subpath query for P[1..i] in xbw, and get the
xbw interval [x1..x2] of all the nodes whose strings ﬁnish with P[1..i] (actually we
have to perform x1 ← rank1(Slast, x1) and x2 ← rank1(Slast, x2) to avoid counting the
same node multiple times, see [13]). This also takes O(m2) time overall. Finally, we
search the Range data structure for [x1..x2] × [y1..y2] to obtain all phrase identiﬁers
t such that phrase Bt ﬁnishes with P[1..i] and phrase Bt+1 starts with P[i + 1..m],
in O((m + occ) log n) time overall.

5.2.3 Occurrences of Type 3

For occurrences of type 3, one proceeds mostly as with the original LZTrie (navigating 
the xbw instead), so as to ﬁnd all the nodes equal to substrings of P in O(m2)
time. Then, for each maximal concatenation of phrases P[i..j] = Bt . . . B(cid:4), we must
check whether phrase B(cid:4)+1 starts with P[j + 1..m] and whether phrase Bt−1 ﬁnishes 
with P[1..i − 1]. The ﬁrst check can be done in O(1/) time by using ids
−1:
as we have searched for all substrings of P in the trie, we know the LZTrie preorder

Algorithmica (2012) 62:54–101

89

−1(ids

−1(t − 1)).



interval of the descendants of P[j + 1..m], and thus we check whether the node at
−1((cid:4)+ 1) belongs to that interval. The second check can be done
preorder position ids
in O(1/) time, by determining whether t − 1 lies in the xbw interval of P[1..i − 1]
(that is, Bt−1 ﬁnishes with P[1..i − 1]). For this, we need Pos
−1, so that the position
is Pos
 ) time, occurrences of type 2
cost O(m2+ m
 ) time. Thus, we have
achieved Theorem 2 again with radically different means. The same complexities are
also achieved for count and exists queries. We can also obtain a version requiring

Summarizing, occurrences of type 1 cost O(m + occ

+ (m+occ) log n) time, and of type 3 cost O( m2

(1 + )uHk(T ) + o(u log σ ) bits and O(m2) average reporting time if m (cid:2) 2 logσ n
(as in Theorem 1), if we search for the occurrences of type 2 by using a checking
procedure similar to that used to check phrases t − 1 and (cid:4)+ 1 for the occurrences of
type 3.

6 Faster and Still Small LZ-indices

In Sect. 4, we have shown how to use sufﬁx links in RevTrie to reduce the space
requirement of the LZ-index. Russo and Oliveira [44] show how to use sufﬁx links to
reduce the locating time of their LZ-index to O((m + occ) log u), but not to reduce
the space of their index. On the other hand, Ferragina and Manzini [16] combine
the backward-search concept with a Lempel-Ziv-based scheme to achieve optimal
O(m + occ) locating time, without restrictions on m or occ. Yet, their index is even
larger, requiring O(uHk(T ) logγ u) bits of space, for any constant γ > 0.

In this section, we use sufﬁx links to speed up occurrences of type 2, using an
idea similar to that of [44], and we ﬁnd occurrences of type 3 as a particular case of
occurrences of type 2, using a similar idea to that of [16]. In this way we manage to
avoid the O(m2) term in the locating complexity of the LZ-index, achieving the same
locating time as [44], while reducing their space requirement of (5 + )uHk(T ) +
o(u log σ ) bits.

6.1 Index Deﬁnition

We build basically on the LZ-index of Theorem 1, composed of LZTrie, RevTrie, and
the R mapping (compressed using sufﬁx links ϕ). We add to LZTrie the data structure
of Jansson et al. [25] to compute level ancestor queries, LA(x, d), which returns the
ancestor at depth d of node x. This requires o(n) extra bits and supports LA queries
in constant time. Therefore, the overall space requirement of the three above data
structures is (1 + )uHk + o(u log σ ) bits.
To avoid the O(m2) term in the locating complexity, we should avoid occurrences
of type 3, since they make us check the O(m2) possible candidates. We cannot use the
same procedure as for occurrences of type 2 (using the Range data structure) because
LZTrie is only able to index whole phrases, not text sufﬁxes. Then, by using LZTrie
to query the Range data structure, we are only able to return the phrases starting with
a given sufﬁx P[i + 1..m] of the pattern, and therefore we can ﬁnd only occurrences
spanning two consecutive phrases (i.e., occurrences of type 2).

90

Algorithmica (2012) 62:54–101

Hence we add the alphabet friendly FM-index [17] of T (AF-FMI(T ) for short)
to our index. By itself this self-index is able to search for pattern occurrences, requiring 
uHk(T )+ o(u log σ ) bits of space. However, its locate time per occurrence is
O(log1+ u
log σ
log log u ), for any constant  > 0, which is greater than the O(log u) time
per occurrence of LZ-indices.

(cid:8)

(cid:8)

(cid:8)

(cid:8)

, j

) in Range, where i

(cid:8)] = p holds.

As AF-FMI(T ) is based on the Burrows-Wheeler Transform [9] of T (bwt(T ) for
short), it can be (conceptually) thought of as the sufﬁx array SAT of T . Recall that,
in a sufﬁx array, a given interval corresponds to a lexicographic interval of the text
sufﬁxes. The AF-FMI(T ) indexes text sufﬁxes. In particular, we will be interested
in those sufﬁxes that are aligned with the LZ78 phrase beginnings. By using this
structure to query the Range data structure (instead of using LZTrie) we will be able
to ﬁnd those text sufﬁxes that are aligned with LZ78 phrases and have P[i + 1..m] as
a preﬁx. Thus, P[i+1..m] can span more than two consecutive phrases, and therefore
we will consider occurrences of type 3 as a special case of occurrences of type 2.
To ﬁnd occurrences spanning several phrases we re-deﬁne Range, the data structure 
for 2-dimensional range searching. Now it will operate on the grid [1..u]×[1..n].
For each LZ78 phrase with identiﬁer id, for 0 < id (cid:3) n, assume that the RevTrie node
, and that phrase (id+ 1) starts at position p in T . Then we store
for id has preorder j
is the lexicographic order of the sufﬁx of T startthe 
point (i
ing at position p, i.e. SAT [i
Suppose that we search for a given string s2 in AF-FMI(T ) and get the interval
[i1, i2] in the bwt(T ) (equivalently, in the sufﬁx array of T ), and that the search for
1 in RevTrie yields a node such that the preorder interval for its subtree is
string sr
[j1, j2]. Then, a search for [i1, i2] × [j1, j2] in Range yields all phrases ending with
s1 such that the next phrase is aligned with an occurrence of s2 in T .
We transform the grid [1..u] × [1..n] indexed by Range into an equivalent grid
[1..n] × [1..n] by deﬁning a bit vector V[1..u], which indicates (with a 1) which
positions of AF-FMI(T ) point to the beginning of an LZ78 phrase. We represent V
with the data structure of [42] supporting rank queries, and using uH0(V ) + o(u) (cid:3)
+ o(u) = o(u log σ ) bits of storage (recall n (cid:3) u/ logσ u).
n log u
(cid:8)
n
Thus, instead of storing the point (i
(cid:8)
store the point (rank1(V , i
[rank1(V , i1), rank1(V , i2)] × [j1, j2].
As there is only one point per row and column of Range, we can use the data structure 
of Chazelle [10], which can be implemented by using n log n+ O(n log log n) =
uHk(T ) + o(u log σ ) bits [31]. As a result, the overall space requirement of our LZindex 
is (3 + )uHk(T ) + o(u log σ ), for any k = o(logσ u) and any 0 <  < 1.

) as in the previous deﬁnition of Range, we
). The search of the previous paragraph now becomes

+ o(u) (cid:3) u log log u

(cid:8)

, j

(cid:8)

), j

logσ u

6.2 Search Algorithm

For exists and count queries we can achieve O(m) time by just using the
AF-FMI(T ). We focus now on locate queries. Assume that P[1..m] = p1 . . . pm,
for pi ∈ . As explained, we need to consider only occurrences of P in T of type 1
and 2. Those of type 1 are found just as for the original LZ-index, in O(m + occ1
 )
time. The rest of the section is devoted to those of type 2.

Algorithmica (2012) 62:54–101

91

(cid:8)

, j

(cid:8)

(cid:8)

have to show how to ﬁnd efﬁciently the intervals in AF-FMI(T ) and in RevTrie.

To ﬁnd the pattern occurrences spanning two or more consecutive phrases we must
consider the m − 1 partitions P[1..i] and P[i + 1..m] of P , for 1 (cid:3) i < m. For
every partition we must ﬁnd all phrases terminated with P[1..i] such that the next
phrase starts at the same position as an occurrence of P[i + 1..m] in T . Hence, as
explained before, we must search for P r[1..i] in RevTrie and for P[i + 1..m] in AF-
FMI(T ). Thus, every partition produces two one-dimensional intervals, one in each
of the above structures.
If the search in RevTrie for P r[1..i] yields the preorder interval [j1, j2], and the
search for P[i + 1..m] in AF-FMI(T ) yields interval [i1, i2], the two-dimensional
range [rank1(V , i1), rank1(V , i2)] × [j1, j2] in Range yields all pattern occurrences
for the given partition of P . For every pattern occurrence we get a point (i
) from
Range. The corresponding phrase identiﬁer can be found as t = ids(R(j
)), to ﬁnally
report a pattern occurrence [[t, i]].
Overall, occurrences of type 2 are found in O((m+ occ2) log n) time. Yet, we still
The m − 1 intervals for P[i + 1..m] in AF-FMI(T ) can be found in O(m) time
thanks to the backward search concept, since the process to count the number of
occurrences of P[2..m] proceeds in m − 1 steps, each one taking constant time if
σ = O(polylog(u)) [16]: in the ﬁrst step we ﬁnd the BWT interval for pm, then we
ﬁnd the interval for occurrences of pm−1pm, then pm−2pm−1pm, and so on to ﬁnally
ﬁnd the interval for p2 . . . pm = P[2..m].
However, the work in RevTrie can take time O(m2) if we search for strings
P r[1..i] separately, as done for the indices of Sect. 4. Fortunately, some work done
to search for a given P r[1..i] can be reused to search for other strings. We have
to search for strings pm−1pm−2 . . . p1, pm−2 . . . p1, . . . , and p1 in RevTrie. Note that
every pj . . . p1 is the longest proper sufﬁx of pj+1pj . . . p1. Suppose that we successfully 
search for P r[1..m − 1] = pm−1pm−2 . . . p1, reaching the node with preorder
(cid:8)
in RevTrie, hence ﬁnding the corresponding preorder interval in RevTrie in O(m)
i
time. Now, to ﬁnd the node representing sufﬁx pm−2 . . . p1 we only need to follow
sufﬁx link ϕ(i
) (which takes O(1) time) instead of searching for it from the RevTrie
root (which would take O(m) time again). The process of following sufﬁx links can
be repeated m− 1 times up to reaching the node corresponding to string p1, with total
time O(m). This is the main idea to get the m−1 preorder intervals in RevTrie in time
less than quadratic. The general case is slightly more complicated and corresponds to
the descend and sufﬁx walk method used in [44].

(cid:8)

In what follows, we explain the way we implement descend and sufﬁx walk in
our data structure. However, we must prove a couple of properties for RevTrie in
order to be able to apply this method. First, we know that every non-empty node in
RevTrie has a sufﬁx link (see Lemma 4), yet we need to prove that every RevTrie node
(including empty-non-unary nodes) has also a sufﬁx link.

Lemma 10 Every empty non-unary node in RevTrie has a sufﬁx link.

Proof Assume that node vr in RevTrie is empty non-unary, and that it represents
string ax, for a ∈  and x ∈ 
. As node vr is empty non-unary, the node has at
least two children. In other words, there exist at least two strings of the form axy and

∗

92

Algorithmica (2012) 62:54–101

∗

axz, for y, z ∈ 
, y (cid:5)= z, both strings corresponding to non-empty nodes, and hence
these nodes have a sufﬁx link. These sufﬁx links correspond to strings xy and xz in
RevTrie. Thus, there must exist a non-unary node for string x, which is the sufﬁx link
(cid:4)
of node vr .

The descent process in RevTrie will be a little bit different from the one described
in the proof of Lemma 3. This time, we are going to reuse the work done for a string
already searched for in RevTrie, so we have to be sure that every time we arrive at a
RevTrie node, the string represented by that node matches the corresponding pattern
preﬁx (the usual skipping process of a Patricia tree does not ensure that). Thus, the
second property is that, although RevTrie is a Patricia tree and hence we store only
the ﬁrst symbol of each edge label, we can get the whole label in time linear in its
length.

Lemma 11 Any edge label of length l in RevTrie can be extracted in O(l) time.

.

(cid:8)

r

(cid:8)

r

(cid:8)

r

(cid:8)

r

(cid:8)

r

between nodes vr and v

Proof Assume that we are at node vr in RevTrie, and want to extract the label for
(cid:8)
r in RevTrie. Since we arrive at a node in RevTrie
edge evr v
by descending from the root, the length of the string represented by a given node can
be computed by summing up the skips we have seen in the descent. Let lvr and lv
be
the length of strings represented by nodes vr and v
the length of the label of edge evr v

− lvr is
(cid:8)
r has preorder j1 in RevTrie, we can access the LZTrie
− lvr ), in
= LA(R[j1], lv
(cid:8)
node from which to start the extraction of the label by v
(cid:8)
lz
lz-to-root path. Notice that
is the label of the v
constant time [25]. The label of evr v
with the level-ancestor query on LZTrie, we avoid extracting the string represented
by node vr in RevTrie, as it has been already extracted before descending to vr .

(cid:8)
r , respectively. Then, lv

If we assume that node v

In the case where v

r is an empty node, recall that the corresponding value R[j1] is
(cid:8)
undeﬁned. However, just as in the proof of Lemma 3, we can use any non-empty node
(cid:8)
r to map to the LZTrie. For instance, we can use the next nonwithin 
the subtree of v
r : let j2 = rank1(B, j1) + 1, then the length of the
(cid:8)
empty node within the subtree of v
corresponding string can be computed as depth(R[j2]) in LZTrie, and we compute
= LA(R[j2], depth(R[j2]) − lvr ), to ﬁnally extract the edge label by moving to
(cid:8)
v
(cid:4)
lz
the parent lv

− lvr times.

(cid:8)

(cid:8)

r

r

Thus, we search RevTrie as in a normal trie, comparing every symbol as we descend,
 without skipping as is done in Lemma 3. In this way, every time we arrive at a
RevTrie node, the string represented by that node will match the corresponding preﬁx
of the pattern.
Previously we showed that it is possible to search for all strings P r[1..i] in O(m)
time, assuming that P r[1..m − 1] exists in RevTrie (therefore all the P r[1..i]’s exist
in RevTrie). The general case is as follows. Let P r[1..m − 1] = pm−1 . . . p1 be the
longest string that we need to search for in RevTrie. We deﬁne three integer indices
on P r[1..m − 1], which guide the search:
i1, which marks the beginning of the pattern sufﬁx we are currently searching for. It
is initialized at 1 since we start searching for pm−1pm−2 . . . p1;

Algorithmica (2012) 62:54–101

93

i2, which indicates the current symbol in the pattern, which is being compared with
a symbol in an edge label, with the aim of descending to a child of the current node.
Notice that (P r )[i1..i2 − 1] is the part of the current pattern that has been matched
with the edge labels of RevTrie; and
i3, which delimits the string corresponding to the current node, which represents
string (P r )[i1..i3] in RevTrie. Thus (P r )[i3+ 1..i2− 1] will be the part of the pattern
that has been compared with the label of the edge leading to the node we are trying
to descend to.

Our descend and sufﬁx walk will be composed of three basic operations: descend,

sufﬁx, and retraverse.

Descend We start searching for pm−1pm−2 . . . p1 from the RevTrie root, using the
method of Lemma 11 and using i2 to indicate the current symbol being compared in
the descent. Every time we descend to a non-empty-unary child node (after matching
all the characters of an edge), we set i3 ← i2 and continue descending in the same
way from this node. If, when trying to descend to a child node, we ﬁnd an emptyunary 
node (which was added because of the skips in RevTrie, see Sect. 3.4), the
index i3 is not updated as explained before. In this case, we continue the descent with
i2 from the empty-unary node, using Lemma 11.

∗

Sufﬁx Now assume that, being at current node vr (with preorder j1 in RevTrie and
representing string ax, for a ∈ , x ∈ 
(cid:8)
), we cannot descend to a child node v
(with preorder j2 in RevTrie and representing string axyz, for y, z ∈ 
r
, such that
|yz| > 0). Let evr v
(cid:8)
r , with label yz, where
y = (P r )[i3 + 1..i2 − 1] and (P r )[i2] (cid:5)= z1. Hence there are no phrases ending with
P r[1..m − i1].
Then, we go on to consider the next sufﬁx P r[1..m − i1 − 1]. To reuse the work
done up to node vr (i.e. (P r )[i1..i3] = ax), we follow the sufﬁx link to get the node
ϕ(j1) representing string x, setting i1 ← i1 + 1.

be the edge between nodes vr and v

∗

(cid:8)

r

Retraverse We have reused the work up to x, but we had actually worked up to
xy. Notice that sufﬁx xy certainly exists in RevTrie, yet it could be represented by
an empty unary node which has been compressed in an edge. Therefore, from node
= yz could be split
ϕ(j1) we descend using y = (P r )[i3 + 1..i2 − 1]. The edge evr v
into a path of several nodes between nodes ϕ(j1) and ϕ(j2). As substring y has been
already checked in the previous step, the descent from node ϕ(j1) is done by skipping
and checking only the ﬁrst symbols of the edge labels (advancing i3 accordingly as
we reach new nodes). If we ﬁnd an empty unary node when trying to descend from
(cid:8)(cid:8)
node v
r to the next node, we jump directly to the position of the next non-empty
unary node (with preorder j3) and then compute the length l of the string represented
by that node.

(cid:8)

r

For this direct jump we need a bit vector E marking the empty unary nodes, in
preorder. We preprocess E with a data structure supporting rank and select, so this
requires n
) extra bits. The node with preorder j3 can be found by using rank
and select on E. The length l can be computed as the sum of the length of the current

(cid:8) + o(n

(cid:8)

94

Algorithmica (2012) 62:54–101

node plus ne · log u, where ne is the number of empty unary nodes between the current 
node and the one with preorder j3 (which can be computed as the number of 1s
between the corresponding positions in E), and log u comes from the skips of empty
unary nodes (recall Sect. 3.4). If l > |xy|, we resume the sufﬁx mode from v
(cid:8)(cid:8)
r . Otherwise,
 we stay in the retraverse mode from the node with preorder j3. This process
is carried out till string y is fully consumed, and then we resume the descend mode
from the corresponding node.

After we ﬁnd the ﬁrst sufﬁx P r[1..i] in RevTrie (if any), we are sure that every
sufﬁx of it also exists in RevTrie (because this trie is sufﬁx closed). The nodes corresponding 
to these sufﬁxes are found by following sufﬁx links.
Lemma 12 Given a string P of length m, we can search for strings P r[1..i], for
1 (cid:3) i < m, in RevTrie in O(m) time.

Proof Consider the method just described. Indices i1, i2, and i3 grow from 1 to at
most m. For every constant-time action we carry out, at least one of those indices
(cid:4)
increases. Thus the total work is O(m).

Therefore, we have proved:
Theorem 3 Given a text T [1..u] over an alphabet of size σ , there exists a compressed 
full-text self-index requiring (3 + )uHk(T ) + o(u log σ ) bits of space, for
σ = O(polylog(u)), any k = o(logσ u), and any 0 <  < 1. Given a search pattern
P[1..m], this index is able to:
1. locate the occ occurrences of P in T in O((m + occ
2. count pattern occurrences in O(m) worst-case time; and
3. determine whether P exists in T in O(m) worst-case time.

 ) log u) worst-case time;

7 Optimal Displaying of Text Substrings

7.1 Reporting Text Positions with LZ-index

As we said before, the original LZ-index is able to report occurrences in the format
[[t, offset]], where t is the phrase in which the occurrence starts and offset is
the distance between the beginning of the occurrence and the end of the phrase. The
same happens for our indices of Sects. 4, 5, and 6.
However, we can report occurrences as text positions by adding a bit vector
TPos[1..u] that marks with a 1 the text positions corresponding to the phrase beginnings.
 Thus, there are n 1s in TPos. Given a text position i, rank1(TPos, i)
is the phrase number i belongs to. Given a phrase identiﬁer j , select1(TPos, j )
yields the text position at which the j -th phrase starts. Therefore, given an occurrence 
in the format [[t, offset]], the text position for that occurrence can be
computed as select1(TPos, t + 1) − offset. Such TPos can be represented with

Algorithmica (2012) 62:54–101

95

uH0(TPos) + o(u) (cid:3) n log u
n (cid:3) u/ logσ u).

n

+ o(u) (cid:3) u log log u

logσ u

+ o(u) = o(u log σ ) bits [42] (recall

The algorithm for extract queries (of whole LZ78 phrases) described in
Sect. 3.3 can also be used on the indices of Theorems 1, 2 and 3, yet this time
providing the text positions from which to extract (rather than the phrase identi-
ﬁers), since these positions can be transformed into phrase identiﬁers by using data
−1, it takes
structure TPos. As the Node data structure is simulated by using ids
O((cid:4)(1 + 1
 logσ (cid:4) )) time to extract any text substring of length (cid:4). This is because we
perform (cid:4) parent operations to get the (cid:4) symbols we want to display, and we must
−1 each time we go on to extract the next phrase, which in the
pay O(1/) to use ids
(very) worst case is done O((cid:4)/ logσ (cid:4)) times.

To extract the text with xbw-based LZ-index of Sect. 5, we use TPos to transform
−1 to ﬁnd the preorder
the text positions into phrase identiﬁers, and then we use ids
position of the corresponding phrase, to ﬁnally map to the xbw representation of
−1 in O(1/) time. Then we move to the parent in the xbw,
LZTrie by using Pos
displaying the corresponding symbol stored in Sα. When we reach the tree root, we
−1 again to consider the next phrase, and map to the xbw again. The time is
use ids
therefore O((cid:4)(1 + 1

We can avoid the restriction of displaying only whole phrases by adding a data
structure for level-ancestor (LA) queries on LZTrie. The data structure [25] builds on
DFUDS, allows constant time computation of operation LA, and requires o(n) extra
bits of space. Thus, the part of a phrase that we do not need to display is skipped
by using the appropriate LA query. Yet, the displaying time is not optimal, since we
work O(1) per extracted symbol and on a RAM we are able to handle (log u) bits

per access, which means (log u/ log σ ) = (logσ u) symbols per access.

 logσ (cid:4) )).

7.2 Achieving Optimal Extracting Time

We describe a technique that can be plugged to any of the indices proposed in
Sects. 4, 5 and 6, for displaying any text substring T [i..i + (cid:4) − 1], in optimal
O(1+ (cid:4)/ logσ u) time. A compressed data structure [46] to display any text substring
of length (logσ u) in constant time, turns out to have similarities to the LZ-index.
We take advantage of this similarity to plug it into our indices, with some modiﬁcations,
 and obtain improved time to display text substrings. In [46], the authors added
auxiliary data structures of o(u log σ ) bits to LZTrie to support this operation efﬁciently.
 Given a position i in the text, we ﬁrst ﬁnd the phrase including the position
i by using rank1(TPos, i), and then ﬁnd the node of LZTrie that corresponds to the
phrase using Node (that is, the corresponding implementation of it). Then displaying
a phrase is equivalent to outputting the path going from the node to the root of LZTrie.
The auxiliary data structure, of size O(n log σ ) = o(u log σ ) bits, permits outputting
the path by chunks of (logσ u) symbols in O(1) time per chunk. As explained before,
 we can also display not only whole phrases, but any text substring within this
complexity. Thus the displaying can start backwards from anywhere in a phrase and,
of course, it can stop at any point as well.

We modify this method to plug it into our indices. In their original method [46], if
more than one consecutive phrases has length less than (logσ u)/2 each, their phrase

96

Algorithmica (2012) 62:54–101

identiﬁers are not stored. Instead the substring of the text including those phrases
is stored without compression. This guarantees efﬁcient displaying without increasing 
the space requirement. However this will cause the problem that we cannot ﬁnd
patterns including those phrases. Therefore in our modiﬁcation we store, for these
short phrases, both the phrases themselves and their phrase identiﬁers. The search
algorithm remains as before. To decode short phrases we can just output the explicitly 
stored substring including the phrases. For each phrase with length at most
√
(logσ u)/2, we store a substring of length log u containing the phrase. Because there
√
u) such phrases in the text (recall that all LZ78 phrases are differare 
at most O(
u log u) = o(u) bits. These auxiliary
ent), we can store all these substrings in O(
structures work as long as we can convert a phrase identiﬁer into a preorder position
−1) . Hence they can be applied to all the data structures
in LZtrie (that is, compute ids
in Sects. 4, 5, and 6.

Theorem 4 The indices of Theorems 1 and 2 (and also those of Sects. 5 and 6) can
be adapted to extract a text substring of length (cid:4) surrounding any text position in
optimal O(1+ (cid:4)
 logσ u ) worst-case time, using only o(u log σ ) extra bits of space, for
any 0 <  < 1.

8 Handling Larger Alphabets
For simplicity, throughout this paper we have assumed σ = O(polylog(u)), or equivalently 
log σ = O(log log u). Here we study the cases log σ = o(log u) and log σ =
(log u).
8.1 The Case log σ = o(log u)
As long as log σ = o(log u) holds, we can still have k = o(logσ u) > 0, while it also
holds that n log n = uHk(T ) + o(u log σ ) [28]. Therefore, the space requirements of
the indices of Theorems 1 to 3 stay the same.

Index of Sect. 4 The data structure of [17], which we use to represent letts and array
log σ
SW , has a time complexity of O(
log log u ) for rank and select queries; thus, we lose
the constant time for operations child(x, α) and ϕ
(x, α) on the tries, which would
increase the time complexity of the whole index. Nevertheless, we can represent letts
with the (more complicated) data structure used in [8], thus ensuring constant time
for child(x, α) for any σ , and retaining the same time complexity in our theorems. In
the case of SW we can use the following scheme, which is a variant of that used in
[13] to achieve constant-time rank over the sequence, requiring n log σ + o(u log σ )
bits of space.
(cid:8)[a, i] = 1 if and only if SW[i] =
a holds. Thus, ranka(SW , i) can be computed as the number of 1s within the a-th
(cid:8)
of σ n bits.
row of S
Since there are n 1s in this bit vector, we use the data structure of [42] that requires
(cid:5) + o(n) + O(log log σ n) bits of space, which is n log σ + O(n) bits. We add
log

be a σ × n binary matrix, such that S

Let S

, up to column i. We represent S

as a linear bit vector S

(cid:8)

(cid:4)
σ n
n

(cid:8)

(cid:8)

(cid:8)(cid:8)

Algorithmica (2012) 62:54–101

97

an array counting the number of 1s up to the beginning of each row in S
, using
σ log n extra bits. By using this representation, we are able to compute rank1(S
, i)
(cid:8)(cid:8)[i] = 1 holds [42]. In our representation,
in constant time, but only if we know that S
this means that we can compute ranka(SW , i) only if we know that SW[i] = a holds.
However, this is not the case, since the ranka in (1) is carried out over the ﬁrst position
of SW corresponding to the Weiner-link symbols of a node, which can store any
possible symbol, not necessarily a.

(cid:8)(cid:8)

(cid:8)

We devise the following data structure in order to ﬁnd the position of symbol
a within the Weiner-link symbols of a RevTrie node v with preorder i. Given the
portion of array SW corresponding to the symbols for the Weiner links of node v, we
can construct a DFUDS [8] directory on these symbols in order to compute, in O(1)
time, the position of any symbol within this segment. The space for the directory is
d log σ bits, where d is the number of Weiner links deﬁned for node v. We deﬁne
array D of n log σ bits, storing the DFUDS directory Dv for every RevTrie node in
preorder. Directory Dv is aligned with the positions in array SW for node v. Let
i2 ← rank0(VW , select1(VW , i+1))+1 be the starting position in SW for the Weinerlink 
symbols of v. Let j be the position of symbol a within the symbols of v, yielded
by Dv in O(1) time (Dv also allows us to know whether or not symbol a exists within
the symbols of node v). Then, we know that SW[i2 + j] = a holds, hence the ranka
in (1) must be computed up to position i2 + j , instead of just i2.
None of the remaining data structures of the index are affected by the alphabet
size. As a result, Theorem 2 can be extended for the case log σ = o(log u), rather
than only for σ = O(polylog(u)).

Index of Sect. 5 The times for the operations on the xbw representation of LZTrie
are affected by the alphabet size, depending on the representation used for Sα. If
we use the data structure of Golynski et al. [20], occurrences of type 1 are found in
O(m log log σ + occ
 ) time, because of the subpath query we perform on LZTrie; occurrences 
of type 2 are found in O(m2 log log σ + m
+ (m + occ) log n) time, where
the ﬁrst term comes from searching for the m − 1 partitions of P in xbw; and occurrences 
of type 3 are found in O(m2 log log σ + m2
 ), where the ﬁrst term comes
from searching for the O(m2) pattern substrings in the xbw representation of LZTrie.
+ log log σ ) + (m + occ) log u). We can
Overall, the time for locate is O(m2( 1

also replace O(log log σ ) for O(

log σ
log log u ) in all these ﬁgures.



Index of Sect. 6 For this index the only affected part is the Alphabet-Friendly FMindex,
 AF-FMI(T ), which still has a space requirement of uHk(T ) + o(u log σ ) bits
of space. The counting time is increased to O(m(1+ log σ
log log u )). Thus, the time for locate 
of this version of LZ-index now becomes O(m(1+ log σ
 ) log u),
=
which is still O((m+ occ
log log u
O(m log u). The counting time, on the other hand, now becomes O(m(1+ log σ
log log u )).
Thus, we have a more general version of Theorem 3:
Theorem 5 Given a text T [1..u] over an alphabet of size σ , there exists a compressed
full-text self-index requiring (3 + )uHk(T ) + o(u log σ ) bits of space, for any k =

log log u )+ (m+ occ
 ) log u), the same as stated by Theorem 3, since m

log σ

98

Algorithmica (2012) 62:54–101

log log u )) worst-case time;

 ) log u) worst-case time;

o(logσ u), any 0 <  < 1, and such that log σ = o(log u). Given a search pattern
P[1..m], this index is able to:
1. locate the occ occurrences of P in T in O((m + occ
2. count pattern occurrences in O(m(1 + log σ
3. determine whether P exists in T in O(m(1 + log σ
4. extract any text substring of length (cid:4) in O((cid:4)/( logσ u)) worst-case time.
8.2 The Case log σ = (log u)
For the case log σ = (log u), because of Lemma 1 we have that n log n = uHk(T )+
O(u(1+ k log σ )) bits of space, which is (u log σ ) even for k = 1. Thus, high-order
compression is lost. For k = 0 the space is uH0(T )+ o(u log σ ) bits of space, so zeroorder 
compression is retained. On the other hand, all the time complexities obtained
for the case log σ = o(log u) are valid for this larger σ . However, it has been shown
that the empirical-entropy model is not adequate for such a large alphabet [19].

log log u )) worst-case time; and

9 Conclusions and Future Work



(cid:4)

We have improved the overall performance of LZ-indices, achieving stronger compressed 
self-indices based on the Lempel-Ziv compression algorithm [48]. We
have reduced the space of Navarro’s LZ-index [39] to about a half, achieving
(2 + )uHk(T ) + o(u log σ ) bits of space to index a text T [1..u] with k-th order empirical 
entropy Hk, for any k = o(logσ u) and any 0 <  < 1. Our indices are able to
+ (m+ occ) log u)
search for the occ occurrences of a pattern P[1..m] in T in O( m2
worst-case time, as well as extracting any text substring of length (cid:4) in optimal
 logσ u ) time. Thus, we achieve the same locating time as the index of Kärkkäinen
O(
and Ukkonen [27], yet with a much smaller index which does not need the text to operate.
 We also showed how the space can be squeezed to (1+ )uHk(T )+ o(u log σ )
bits, with O(m2) average-case search time if m (cid:2) 2 logσ n. This space approaches, as
closely as desired, the optimal uHk(T ) under the k-th order empirical entropy model
for all k. However, this index does not provide worst-case guarantees at search time.
Thus, ours are the smallest existing compressed self-indices based on Lempel-Ziv
compression.
We also showed how to use an LZ-index to achieve O((m + occ) log u) time to
locate the pattern occurrences, requiring (3 + )uHk(T ) + o(u log σ ) bits of space.
This is much less than the space required by other LZ-indices having the same search
time.
Thus, we have achieved LZ-indices with space requirements ranging from
(1+ ) to (3+ ) times the empirical entropy of the text (plus lower-order terms), with
different achievements in the time complexities according with the space requirement
of the index. These indices are very competitive with state-of-the-art indices, both in
time and space requirement.

The most basic problems for compressed self-indices are that of searching and reproducing 
the text. However, there are many other functionalities that a self-index

Algorithmica (2012) 62:54–101

99

must provide in order to be fully useful, as for example the space-efﬁcient construction 
of the indices, secondary-memory capabilities (in cases where the text is
so huge that the corresponding compressed self-index does not ﬁt in main memory),
dynamic capabilities, and allowing more complex queries on the text (such as regularexpression 
and approximate searching).

Constructing the indices with little space is an important research topic regarding
their practicality [2, 23, 24, 32]. It has been shown [5] that all the indices deﬁned in
this paper can be constructed without requiring any extra space on top of the space
of the index itself, which adds extra value to our results. Also, it has been shown
that the LZ-index can be efﬁciently handled on secondary storage [3], by means of
adding redundancy to the index to avoid most random accesses. This provides a very
promising alternative, yet an interesting question is whether we can use techniques
similar to those of this paper to reduce the added redundancy. It has also been shown
that the LZ-indices (in particular the ILZI of [44]) are adequate for approximate string
matching [43].

A very important aspect is that of the practical implementations of compressed
indices, as many theoretical indices are proposed but never implemented. The
Pizza&Chili Corpus [18] provides practical implementations of compressed indices,
as well as some example texts. To show the practicality of our approach, there are
currently in the site some implementations of reduced schemes of LZ-index, based
on ideas which are similar to the ones described in this paper. These indices have
shown to be very competitive against others [4, 12], speciﬁcally for locate and
extract queries. We hope to achieve further results along this line.

Finally, the results obtained about succinct representation of sufﬁx and Weiner
links are of independent interest and could ﬁnd applications in other cases, such as
compressed sufﬁx trees.

References

1. Apostolico, A.: The myriad virtues of subword trees. In: Combinatorial Algorithms on Words. NATO

ISI Series, vol. 1, pp. 85–96. Springer, Berlin (1985)

2. Arroyuelo, D., Navarro, G.: Space-efﬁcient construction of LZ-index. In: Proc. 16th Annual International 
Symposium on Algorithms and Computation (ISAAC). LNCS, vol. 3827, pp. 1143–1152.
Springer, Berlin (2005)

3. Arroyuelo, D., Navarro, G.: A Lempel-Ziv text index on secondary storage. In: Proc. 18th Annual
Symposium on Combinatorial Pattern Matching (CPM). LNCS, vol. 4580, pp. 83–94. Springer, Berlin
(2007)

4. Arroyuelo, D., Navarro, G.: Practical approaches to reduce the space requirement of Lempel-Zivbased 
compressed text indices. Technical Report TR/DCC-2008-9, Department of Computer Science,
University of Chile, 2008. http://www.dcc.uchile.cl/TR/2008/TR_DCC-2008-009.pdf

5. Arroyuelo, D., Navarro, G.: Space-efﬁcient construction of Lempel-Ziv compressed text indexes.
Technical Report TR/DCC-2009-2, Department of Computer Science, University of Chile, 2009.
http://www.dcc.uchile.cl/TR/2009/TR_DCC-20090313-002.pdf

6. Arroyuelo, D., Navarro, G., Sadakane, K.: Reducing the space requirement of LZ-index. In: Proc.
17th Annual Symposium on Combinatorial Pattern Matching (CPM). LNCS, vol. 4009, pp. 319–330.
Springer, Berlin (2006)

7. Barbay, J., He, M., Munro, J.I., Rao, S.S.: Succinct indexes for strings, binary relations and multilabeled 
trees. In: Proc. 18th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA),
pp. 680–689 (2007)

100

Algorithmica (2012) 62:54–101

8. Benoit, D., Demaine, E., Munro, J.I., Raman, R., Raman, V., Rao, S.S.: Representing trees of higher

degree. Algorithmica 43(4), 275–292 (2005)

9. Burrows, M., Wheeler, D.J.: A block-sorting lossless data compression algorithm. Technical Report

124, Digital Equipment Corporation (1994)

10. Chazelle, B.: A functional approach to data structures and its use in multidimensional searching.

SIAM J. Comput. 17(3), 427–462 (1988)

11. Clark, D., Munro, J.I.: Efﬁcient sufﬁx trees on secondary storage. In: Proc. 7th Annual ACM-SIAM

Symposium on Discrete Algorithms (SODA), pp. 383–391 (1996)

12. Ferragina, P., González, R., Navarro, G., Venturini, R.: Compressed text indexes: from theory to prac-

tice!. ACM J. Exp. Algorithmics 13, Article 12 (2009). 30 pages

13. Ferragina, P., Luccio, F., Manzini, G., Muthukrishnan, S.: Structuring labeled trees for optimal succinctness,
 and beyond. In: Proc. 46th Annual Symposium on Foundations of Computer Science
(FOCS), pp. 184–196 (2005)

14. Ferragina, P., Manzini, G.: Opportunistic data structures with applications. In: Proc. 41st Annual IEEE

Symposium on Foundations of Computer Science (FOCS), pp. 390–398 (2000)

15. Ferragina, P., Manzini, G.: An experimental study of an opportunistic index. In: Proc. 12th Annual

ACM-SIAM Symposium on Discrete Algorithms (SODA), pp. 269–278 (2001)

16. Ferragina, P., Manzini, G.: Indexing compressed text. J. ACM 54(4), 552–581 (2005)
17. Ferragina, P., Manzini, G., Mäkinen, V., Navarro, G.: Compressed representations of sequences and

full-text indexes. ACM Trans. Algorithms 3(2), Article 20 (2007)

18. Ferragina, P., Navarro, G.: Pizza&Chili Corpus—compressed indexes and their testbeds (2005).

http://pizzachili.dcc.uchile.cl

19. Gagie, T.: Large alphabets and incompressibility. Inform. Process. Lett. 99(6), 246–251 (2006)
20. Golynski, A., Munro, J.I., Rao, S.S.: Rank/select operations on large alphabets: a tool for text indexing.
 In: Proc. 17th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pp. 368–373
(2006)

21. Grossi, R., Gupta, A., Vitter, J.S.: High-order entropy-compressed text indexes. In: Proc. 14th Annual

ACM-SIAM Symposium on Discrete Algorithms (SODA), pp. 841–850 (2003)

22. Grossi, R., Vitter, J.S.: Compressed sufﬁx arrays and sufﬁx trees with applications to text indexing

and string matching. SIAM J. Comput. 35(2), 378–407 (2005)

23. Hon, W.-K., Lam, T.W., Sadakane, K., Sung, W.-K., Yiu, M.: A space and time efﬁcient algorithm for

constructing compressed sufﬁx arrays. Algorithmica 48(1), 23–36 (2007)

24. Hon, W.-K., Sadakane, K., Sung, W.-K.: Breaking a time-and-space barrier in constructing full-text
indices. In: Proc. 44th Annual Symposium on Foundations of Computer Science (FOCS), pp. 251–260
(2003)

25. Jansson, J., Sadakane, K., Sung, W.-K.: Ultra-succinct representation of ordered trees. In: Proc. 18th

Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pp. 575–584 (2007)

26. Kärkkäinen, J.: Repetition-based text indexes. PhD thesis, Dept. of CS, University of Helsinki, Finland,
 1999

27. Kärkkäinen, J., Ukkonen, E.: Lempel-Ziv parsing and sublinear-size index structures for string matching.
 In: Proc. 3rd South American Workshop on String Processing (WSP), pp. 141–155 (1996)

28. Kosaraju, R., Manzini, G.: Compression of low entropy strings with Lempel-Ziv algorithms. SIAM

J. Comput. 29(3), 893–911 (1999)

29. Lempel, A., Ziv, J.: On the complexity of ﬁnite sequences. IEEE Trans. Inform. Theory 22(1), 75–81

(1976)

30. Mäkinen, V., Navarro, G.: Succinct sufﬁx arrays based on run-length encoding. Nord. J. Comput.

12(1), 40–66 (2005)

31. Mäkinen, V., Navarro, G.: Rank and select revisited and extended. Theor. Comp. Sci. 387(3), 332–347

(2007)

32. Mäkinen, V., Navarro, G.: Dynamic entropy-compressed sequences and full-text indexes. ACM Trans.

Algorithms 4(3), Article 32 (2008). 38 pages

33. Manber, U., Myers, G.: Sufﬁx arrays: a new method for on-line string searches. SIAM J. Comput.

22(5), 935–948 (1993)

34. Manzini, G.: An analysis of the Burrows-Wheeler transform. J. ACM 48(3), 407–430 (2001)
35. Morrison, D.R.: Patricia—practical algorithm to retrieve information coded in alphanumeric. J. ACM

15(4), 514–534 (1968)

36. Munro, J.I.: Tables. In: Proc. 16th Conference on Foundations of Software Technology and Theoretical 
Computer Science (FSTTCS). LNCS, vol. 1180, pp. 37–42. Springer, Berlin (1996)

Algorithmica (2012) 62:54–101

101

37. Munro, J.I., Raman, R., Raman, V., Rao, S.S.: Succinct representations of permutations. In: Proc. 30th
International Colloquium on Automata, Languages and Computation (ICALP). LNCS, vol. 2719, pp.
345–356. Springer, Berlin (2003)

38. Munro, J.I., Raman, V.: Succinct representation of balanced parentheses and static trees. SIAM

J. Comput. 31(3), 762–776 (2001)

39. Navarro, G.: Indexing text using the Ziv-Lempel trie. J. Discrete Algorithms 2(1), 87–114 (2004)
40. Navarro, G.: Implementing the LZ-index: theory versus practice. ACM J. Exp. Algorithmics 13, Article 
2 (2009). 49 pages

41. Navarro, G., Mäkinen, V.: Compressed full-text indexes. ACM Comput. Surv. 39(1), Article 2 (2007)
42. Raman, R., Raman, V., Rao, S.S.: Succinct indexable dictionaries with applications to encoding k-ary
trees and multisets. In: Proc. 13th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA),
pp. 233–242 (2002)

43. Russo, L., Navarro, G., Oliveira, A.: Approximate string matching with Lempel-Ziv compressed
indexes. In: Proc. 14th International Symposium on String Processing and Information Retrieval
(SPIRE). LNCS, vol. 4726, pp. 264–275. Springer, Berlin (2007)

44. Russo, L., Oliveira, A.: A compressed self-index using a Ziv-Lempel dictionary. Inf. Retr. 5(3), 501–

513 (2007)

45. Sadakane, K.: New text indexing functionalities of the compressed sufﬁx arrays. J. Algorithms 48(2),

294–313 (2003)

46. Sadakane, K., Grossi, R.: Squeezing succinct data structures into entropy bounds. In: Proc. 17th Annual 
ACM-SIAM Symposium on Discrete Algorithms (SODA), pp. 1230–1239 (2006)

47. Weiner, P.: Linear pattern matching algorithms. In: Proc. 14th Annual Symposium on Foundations of

Computer Science (FOCS), pp. 1–11 (1973)

48. Ziv, J., Lempel, A.: Compression of individual sequences via variable-rate coding. IEEE Trans. Inform.
 Theory 24(5), 530–536 (1978)

