Improved Grammar-Based Compressed Indexes

Francisco Claude1,(cid:2) and Gonzalo Navarro2,(cid:2)(cid:2)

1 David R. Cheriton School of Computer Science, University of Waterloo

2 Department of Computer Science, University of Chile

Abstract. We introduce the ﬁrst grammar-compressed representation
of a sequence that supports searches in time that depends only logarithmically 
on the size of the grammar. Given a text T [1..u] that is
represented by a (context-free) grammar of n (terminal and nonter-
minal) symbols and size N (measured as the sum of the lengths of
the right hands of the rules), a basic grammar-based representation
of T takes N lg n bits of space. Our representation requires 2N lg n +
N lg u +  n lg n + o(N lg n) bits of space, for any 0 <  ≤ 1. It can ﬁnd
the positions of the occ occurrences of a pattern of length m in T in
O
time, and extract any substring of
length (cid:3) of T in time O((cid:3) + h lg(N/h)), where h is the height of the
grammar tree.

(cid:3)
+ (m + occ) lg n

(m2/) lg

(cid:2)

(cid:2)

(cid:3)

lg u
lg n

1

Introduction and Related Work

Grammar-based compression is an active area of research that dates from at
least the seventies. A given sequence T [1..u] over alphabet [1..σ] is replaced by
a hopefully small (context-free) grammar G that generates just the string T . Let
n be the number of grammar symbols, counting terminals and nonterminals. Let
N = |G| be the size of the grammar, measured as the sum of the lengths of the
right-hand sides of the rules. Then the grammar-compressed representation of
T requires N lg n bits, versus the u lg σ bits required by a plain representation.
Grammar-based methods can achieve universal compression [20]. Unlike statistical 
methods, that exploit frequencies to achieve compression, grammar-based
methods exploit repetitions in the text, and thus they are especially suitable
for compressing highly repetitive sequence collections. These collections, containing 
long identical substrings, possibly far away from each other, arise when
managing software repositories, versioned documents, transaction logs, periodic
publications, and computational biology sequence databases. Good experimental
results have been obtained by using grammar-based indexes [8].

Finding the smallest grammar G∗

that represents a given text T is NPcomplete 
[32,6]. Moreover, the smallest grammar is never smaller than an LZ77
parse [34] of T . A simple method to achieve an O(lg u)-approximation to the
smallest grammar size is to parse T using LZ77 and then to convert it into a grammar 
[32]. A more sophisticated approximation [7] achieves ratio O(lg(u/|G∗|)).
(cid:2) Funded by Google U.S./Canada PhD Fellowship.
(cid:2)(cid:2) Funded by Fondecyt Grant 1-110066, Chile.

L. Calder´on-Benavides et al. (Eds.): SPIRE 2012, LNCS 7608, pp. 180–192, 2012.
c(cid:3) Springer-Verlag Berlin Heidelberg 2012

Improved Grammar-Based Compressed Indexes

181

While grammar-compression methods are strictly inferior to LZ77, and popular 
grammar compressors such as LZ78 [35], Re-Pair [23] and Sequitur [29],
may yield sizes much larger than |G∗| [6], some of those methods (in particular
Re-Pair) perform very well in practice, both in classical and repetitive settings.1
On the other hand, unlike LZ77, grammar compression allows one to decompress 
arbitrary substrings of T almost optimally [14,4]. The most recent result
[4] extracts any T [p, p + (cid:3) − 1] in time O((cid:3) + lg u), which is optimal. Unfortunately,
 that representation [4] requires O(N lg u) bits, possibly proportional but
in practice many times the size of the output of a grammar compressor. On the
practical side, applications like Comrad [22] achieve good space and time for
extracting substrings of T .

More ambitious than just extracting substrings from T is to ask for indexed
searches, that is, ﬁnding the occ occurrences in T of a given pattern P [1..m]. Selfindexes 
are compressed text representations that support both operations, extract
and search, in time depending only polylogarithmically on u. They have appeared
in the last decade [28], and have focused mostly on statistical compression. As
a result, they work well on classical texts, but not on repetitive collections [24].
Some of those self-indexes have been adapted to repetitive collections [24], but
they cannot reach the compression ratio of the best grammar-based methods.

Searching for patterns on grammar-compressed text has been faced mostly in
sequential form, that is, scanning the whole grammar. The best result [19] achieves
time O(N + m2 + occ). This may be o(u), but still linear in the size of the compressed 
text. There exist a few self-indexes based on LZ78-like compression
[12,1,31], but LZ78 is among the weakest grammar-based compressors. In particular,
 LZ78 was shown not to be competitive on highly repetitive collections [24].
The only self-index supporting general grammar compressors [10] operates on
“straight-line programs” (SLPs), where the right hands of the rules are of length
1 or 2. Given such a grammar they achieve, among other tradeoﬀs, 3n lg n+n lg u
bits of space and O(m(m + h) lg2 n) search time, where h is the height of the
parse tree of the grammar. A general grammar of n symbols and size N can
be converted into a SLP by adding at most N − n nonterminals and rules, and
increasing the height h by up to an O(lg(N − n)) factor.

More recently, a self-index based on LZ77 compression has been developed [21].
Given a parsing of T into ¯n phrases, the self-index uses ¯n lg ¯n+ 2¯n lg u + O(¯n lg σ)
bits of space, and searches in time O(m2¯h + (m + occ) lg ¯n), where ¯h is the nesting 
of the parsing. Extraction requires O((cid:3)¯h) time, where (cid:3) is the length of
the extracted substring. Experiments on repetitive collections [8,9] show that
the grammar-based compressor [10] can be competitive with the best classical
self-index adapted to repetitive collections [24] but, at least that particular implementation,
 is not competitive with the LZ77-based self-index [21].

Note that the search times in both self-indexes depend on h or ¯h. This is
undesirable as both are only bounded by n or ¯n. Very recently, a result combining 
grammars and LZ77 parsing [13] achieved O(m2 + (m + occ) lg lg ¯n) time
and O(¯n lg u lg lg ¯n) bits (i.e., slightly superlinear on the LZ77 compressed size).

1 See the statistics in http://pizzachili.dcc.uchile.cl/repcorpus.html

182

F. Claude and G. Navarro

Other close variants of LZ77 parsings also yield promising indexing results in
particular scenarios [17,25,11].

Our main contribution is a new representation of general context-free grammars.
 The following theorem summarizes its properties. Note that the space is
O(N lg u) and the search time is independent of h.

Theorem 1. Let a sequence T [1..u] be represented by a context free grammar 
with n symbols, size N and height h. Then, for any 0 <  ≤ 1, there
exists a data structure using at most 2N lg n + N lg u +  n lg n + o(N lg n)
(cid:2)
in T in time
bits that ﬁnds the occ occurrences of any pattern P [1..m]

(cid:3)

(cid:2)

(cid:3)

O

(m2/) lg

lg u
lg n

+ (m + occ) lg n

. It can extract any substring of length (cid:3)

from T in time O((cid:3) + h lg(N/h)). The structure can be built in O(u + N lg N )
time and O(u lg u) bits of working space.

In the rest of the paper we describe how this structure operates. First, we preprocess 
the grammar to enforce several invariants useful to ensure our time
complexities. Then we use a data structure for labeled binary relations [10] to
ﬁnd the “primary” occurrences of P , that is, those formed when concatenating
symbols in the right hand of a rule. To get rid of the factor h in this part of the
search, we introduce a new technique to extract the ﬁrst m symbols of the expansion 
of any nonterminal in time O(m). To ﬁnd the “secondary” occurrences
(i.e., those that are found as the result of the nonterminal containing primary
occurrences being mentioned elsewhere), we use a pruned representation of the
parse tree of T . This tree is traversed upwards for each secondary occurrence to
report. The grammar invariants introduced ensure that those traversals amortize
to a constant number of steps per occurrence reported. In this way we get rid of
the factor h on the secondary occurrences too.

2 Basic Concepts

2.1 Sequence Representations

Our data structures use succinct representations of sequences. Given a sequence
S[1..N ], over alphabet of size n, we need to support the following operations:
access(S, i) retrieves the symbol S[i]; ranka(S, i) counts number of occurrences
of a in S[1..i]; and selecta(S, j) computes position where the jth a appears in S.
For the case n = 2, Raman et al. [30] proposed two compressed representations
(cid:4)
that are useful when the number n
of 1s in S is small. One is the “fully indexable
lg N
+ N lg lg N/ lg N ) bits of space and
dictionary” (FID). It takes n
supports all the operations in constant time. A weaker one is the “indexable
+ lg lg N ) bits of space and supports
dictionary” (ID), that takes n
in constant time queries access(S, i), rank(S, i) if S[i] = 1, and select1(S, j).

n(cid:2) + O(n
(cid:4)

n(cid:2) + O(n

lg N

(cid:4)

(cid:4)

(cid:4)

For general sequences, the wavelet tree [16] requires N lg n+o(N ) bits [15] and
supports all three operations in O(lg n) time. Another representation, by Barbay
et al. [2], requires N lg n + o(N lg n) bits and solves access(S, i) in O(1) time and
select(S, j) in time O(lg lg n), or vice versa; rank(S, i) takes time O(lg lg n).

Improved Grammar-Based Compressed Indexes

183

2.2 Labeled Binary Relations
A labeled binary relation corresponds to a binary relation R ⊆ A × B, where
A = [1..n1] and B = [1..n2], augmented with a function L : A × B → L ∪ {⊥},
L = [1..(cid:3)], that deﬁnes labels for each pair in R, and ⊥ for pairs that are not
in R. Let us identify A with the columns and B with the rows in a table. We
describe a simpliﬁcation of a representation of binary relations [10], for the case
of this paper where each element of A is associated to exactly one element of B,

so |R| = n1. We use a string SB[1..n1] over alphabet [1..n2], where SB[i] is the
element of B associated to column i. A second string SL[1..n1] on alphabet [1..(cid:3)]
is stored, so that SL[i] is the label of the pair represented by SB[i].

If we use a wavelet tree for SB and a plain string representation for SL, the
total space is n1(lg n2+lg (cid:3))+O(n1) bits. With this representation we can answer,
among others, the following queries of interest in this paper: (1) Find the label
of the element b associated to a given a, SL[a], in O(1) time. (2) Given a1, a2, b1,
and b2, enumerate the k pairs (a, b) ∈ R such that a1 ≤ a ≤ a2 and b1 ≤ b ≤ b2,
in O((k + 1) lg n2) time.

2.3 Succinct Tree Representations
There are many tree representations for trees T with N nodes that take 2N +
o(N ) bits of space. In this paper we use one called DFUDS [3], which in particular 
answers in constant time the following operations (node identiﬁers v are
associated to a position in [1..2N ]): nodeT (p) is the node with preorder number
p; preorderT (v) is the preorder number of node v; leafrankT (v) is the number 
of leaves to the left of v; numleavesT (v) is the number of leaves below v;
parentT (v) is the parent of v; childT (v, k) is the kth child of v; nextsiblingT (v)
is the next sibling of v; rchildT (v) is k such that v is the kth child of its parent;
degreeT (v) is the number of children of v; depthT (v) is the depth of v; and
level -ancestorT (v, k) is the kth ancestor of v.

The DFUDS representation is obtained by traversing the tree in DFS order

and appending to a bitmap the degree of each node, written in unary.

3 Preprocessing and Representing the Grammar
Let G be a grammar that generates a single string T [1..u], formed by n (terminal
and nonterminal) symbols. The σ ≤ n terminal symbols come from an alphabet
Σ = [1, σ],2 and then G contains n − σ rules of the form Xi → αi, one per
nonterminal. This αi is called the right-hand side of the rule, and corresponds
(cid:4) |αi| the size of G. Note it holds
to the sequence of terminal and non-terminal symbols generated by Xi (without
recursively unrolling rules). We call N =
σ ≤ N , as the terminals must appear in the right-hand sides. We assume all
the nonterminals are used to generate the string; otherwise unused rules can be
found and dropped in O(N ) time.

2 Non-contiguous alphabets can be handled with some extra space [10].

184

F. Claude and G. Navarro

sequence of elements in X. We assume that Xn is the start symbol.

We preprocess G as follows. First, for each terminal symbol a ∈ Σ present in
G we create a rule Xa → a, and replace all other occurrences of a in the grammar
by Xa. As a result, the grammar contains exactly n nonterminal symbols X =
{X1, . . . , Xn}, each associated to a rule Xi → αi, where αi ∈ Σ or αi is a
Any rule Xi → αi where |αi| ≤ 1 (except for Xa → a) is removed by replacing
Xi by αi everywhere, decreasing n and without increasing N .
We further preprocess G to enforce the property that any nonterminal Xi,
except Xn and those Xi → a ∈ Σ, must be mentioned in at least two righthand 
sides. We traverse the rules of the grammar, count the occurrences of each
symbol, and then rewrite the rules, so that only the rules of those Xi appearing
more than once (or the excepted symbols) are rewritten, and as we rewrite
a right-hand side, we replace any (non-excepted) Xi that appears once by its
right-hand side αi. This transformation takes O(N ) time and does not alter N
(yet it may reduce n).
Note n is now the number of rules in the transformed grammar G. We will still
call N the size of the original grammar (the transformed one has size ≤ N + σ;
similarly its number of rules is at most n + σ).
We call F (Xi) the single string generated by Xi, that is F (Xi) = a if Xi → a
and F (Xi) = F (Xi1 ) . . .F (Xik ) if Xi → Xi1 . . . Xik . G generates the text T =
L(G) = F (Xn).
Our last preprocessing step, and the most expensive one, is to renumber the
nonterminals so that i < j ⇔ F (Xi)rev < F (Xj)rev, where Srev is string S read
backwards (usefulness of this will be apparent later). The sorting can be done in
time O(u + n lg n) and O(u lg u) bits of space [10], which dominates the previous
time complexities. Let us say that Xn became Xs after the reordering.

We deﬁne now a structure that will be key in our index.

Deﬁnition 1. The grammar tree of G is a general tree TG with nodes labeled
in X . Its root is labeled Xs. Let αs = Xs1 . . . Xsk . Then the root has k children
labeled Xs1 , . . . , Xsk . The subtrees of these children are deﬁned recursively, left
to right, so that the ﬁrst time we ﬁnd a symbol Xi in the parse tree, we deﬁne its
children using αi. However, the next times we ﬁnd a symbol Xi in our recursive
left-to-right traversal, we leave it as a leaf of the grammar tree (if we expanded
it, the resulting tree would be the parse tree of T with u leaf nodes). Also symbols
Xa → a are not expanded but left as leaves. We say that Xi is deﬁned in the
only internal node of TG labeled Xi.
Since each right-hand side αi (cid:9)= a ∈ Σ is written once in the tree, plus the root
Xs, the total number of nodes in TG is N + 1.

The grammar tree partitions T in a way that is useful for ﬁnding occurrences,

using a concept that dates back to K¨arkk¨ainen [18].

Deﬁnition 2. Let Xl1, Xl2 , . . . be the nonterminals labeling the consecutive
leaves of TG. Let Ti = F (Xli), then T = T1T2 . . . is a partition of T according 
to the leaves of TG. We call occurrences of a pattern P primary if they span
more than one Ti. Other occurrences are called secondary.

Improved Grammar-Based Compressed Indexes

185

Our self-index will represent G using two main components. One represents the
grammar tree TG using a DFUDS representation (Sec. 2.3) and a sequence of
labels (Sec. 2.1). This will be used to extract text and decompress rules. When
augmented with a secondary trie TS storing leftmost/rightmost paths in TG, the
representation will expand any preﬁx/suﬃx of a rule in optimal time [14].
The second component in our self-index corresponds to a labeled binary relation
(Sec. 2.2), where B = X and A is the set of proper suﬃxes starting at positions
j + 1 of rules αi: (αi[j], αi[j + 1..]) will be related for all Xi → αi and 1 ≤ j < |αi|.
The labels are numbers in the range [1, N +1]; we specify their meaning later. This
binary relation will be used to ﬁnd the primary occurrences of the search pattern.
Secondary occurrences will be tracked in the grammar tree.

4 Extracting Text

We ﬁrst describe a simple structure that extracts the preﬁx of length (cid:3) of any
rule in O((cid:3) + h) time. We then augment this structure to support extracting any
substring of length (cid:3) in time O((cid:3) + h lg(N/h)), and ﬁnally augment it further to
retrieve the preﬁx or suﬃx of any rule in optimal O((cid:3)) time. This last result is
fundamental for supporting searches, and is obtained by extending the structure
proposed by Gasieniec et al. [14] for SLPs to general context-free grammars
generating one string. The improvement does not work for extracting arbitrary
substrings, as in that case one has to ﬁnd ﬁrst the nonterminals that must be
expanded. This subproblem is not easy, especially in little space [4].
As said, we represent the topology of the grammar tree TG using DFUDS [3].
The sequence of labels associated to the tree nodes is stored in preorder in a
sequence X[1..N + 1], using the fast representation of Sec. 2.1 where we choose
constant time for access(X, i) = X[i] and O(lg lg n) time for selecta(X, j).
We also store a bitmap Y [1..n] that marks the rules of the form Xi → a ∈ Σ
with a 1-bit. Since the rules have been renumbered in (reverse) lexicographic
order, every time we ﬁnd a rule Xi such that Y [i] = 1, we can determine the
terminal symbol it represents as a = rank1(Y, i) in constant time.

4.1 Expanding Preﬁxes of Rules

Expanding a rule Xi that does not correspond to a terminal is done as follows.
By the deﬁnition of TG, the ﬁrst left-to-right occurrence of Xi in sequence X
corresponds to the deﬁnition of Xi; all the rest are leaves in TG. Therefore,
v = nodeTG (selectXi(X, 1)) is the node in TG where Xi is deﬁned. We traverse
the subtree rooted at v in DFS order. Every time we reach a leaf u, we compute
its label Xj = X[preorderTG (u)], and either output a terminal if Y [j] = 1 or
recursively expand Xj. This is in fact a traversal of the parse tree starting at
node v, using instead the grammar tree. Such a traversal takes O((cid:3) + hv) steps
[10], where hv ≤ h is the height of the parsing subtree rooted at v. In particular,
if we extract the whole rule Xi we pay O((cid:3)) steps, since we have removed unary
paths in the preprocessing of G and thus v has (cid:3) > hv leaves in the parse tree.

186

F. Claude and G. Navarro

(cid:4)

(cid:4)

The only obstacle to having constant-time steps are the queries selectXi(X, 1).
As these are only for the position 1, we can have them precomputed in a sequence
F [1..n] using n(cid:10)lg N(cid:11) = n lg n + O(N ) further bits of space.
The total space required for TG, considering the DFUDS topology, sequence
X, bitmap Y , and sequence F , is N lg n + n lg n + o(N lg n) bits. We reduce
the space to N lg n + δ n lg n + o(N lg n), for any 0 < δ ≤ 1, as follows. Form a
[1..N − n + 1] where the ﬁrst position of every symbol Xi in X has
sequence X
been removed, and mark in a bitmap Z[1..N + 1], with a 1, those ﬁrst positions
in X. Replace our sequence F by a permutation π[1..n] so that selectXi(X, 1) =
F [i] = select1(Z, π[i]). Now we can still access any X[i] = X
[rank0(Z, i)] if
−1[rank1(Z, i)]. Similarly,
Z[i] = 0. For the case Z[i] = 1 we have X[i] = π
, j − 1)) for j > 1. Then use Z, π, and
selectXi(X, j) = select0(Z, selectXi(X
X
−1. We
use for π a representation by Munro et al. [27] that takes (1 + δ)n lg n bits and
−1[j] in time O(1/δ), which will be
computes any π[i] in constant time and any π
the cost to access X. Although this will have an impact later, we note that for
extraction we only access X at leaf nodes, where it always takes constant time.3

instead of F and X.
All the operations retain the same times except for the access to π

(cid:4)

(cid:4)

4.2 Extracting Arbitrary Substrings

In order to extract any given substring of T , we add a bitmap L[1..u + 1] that
marks with a 1 the ﬁrst position of each Ti in T . We can then compute the
starting position of any node v ∈ TG as select1(L, leafrankTG (v) + 1).
To extract T [p, p+(cid:3)−1], we binary search the starting position p from the root
of TG. If we arrive at a leaf not representing a terminal, we go to its deﬁnition
in TG, translate position p to the area below the new node v, and continue
recursively. At some point we ﬁnally reach position p, and from there on we
extract the symbols rightwards. Just as before, the total number of steps is
O((cid:3) + h). Yet, the h steps require binary searches. As there are at most h binary
searches among the children of diﬀerent tree nodes, and there are N +1 nodes, at
worst the binary searches cost O(h lg(N/h)). The total cost is O((cid:3) + h lg(N/h)).
The number of ones in L is at most N . Since we only need select1 on L, we can
use an ID representation (see Sec. 2.1), requiring N lg(u/N ) + O(N + lg lg u) =
N lg(u/N ) + O(N ) bits (since N ≥ lg u in any grammar). Thus the total space
becomes N lg n + N lg(u/N ) + δ n lg n + o(N lg n) bits.

4.3 Optimal Expansion of Rule Preﬁxes and Suﬃxes

Our improved version builds on the proposal by Gasieniec et al. [14]. We extend
their representation using succinct data structures in order to handle general
3 Nonterminals Xa → a do not have a deﬁnition in TG, so they are not extracted from
X nor represented in π, thus they are accessed in constant time. They can be skipped
from π[1..n] with bitmap Y , so that in fact π is of length n − σ and is accessed as
π[rank0(Y, i)]; for π−1 we actually use select0(Y, π−1[j]).

Improved Grammar-Based Compressed Indexes

187

−1
S [j] in time O(1/).

grammars instead of only SLPs. Using their notation, call S(Xi) the string of
labels of the nodes in the path from any node labeled Xi to its leftmost leaf in the
parse tree (we take as leaves the nonterminals Xa ∈ X , not the terminals a ∈ Σ).
We insert all the strings S(Xi)rev into a trie TS. Note that each symbol appears
only once in TS [14], thus it has n nodes. Again, we represent the topology of TS
using DFUDS. Yet, its sequence of labels XS[1..n] turns out to be a permutation
in [1..n], for which we use again the structure [27] that takes (1 + )n lg n bits
and computes any XS[i] in constant time and any X
We can determine the ﬁrst terminal in the expansion of Xi, which labels
node v ∈ TS, as follows. Since the last symbol in S(Xi) is a nonterminal Xa
representing some a ∈ Σ, it follows that Xi descends in TS from Xa, which is
a child of the root. This node is va = level -ancestorTS (v, depthTS (v) − 1). Then
a = rank1(Y, XS[preorderTS (va)]).
A preﬁx of Xi is extracted as follows. First, we obtain the corresponding node
v ∈ TS as v = X
−1
S [Xi]. Then we obtain the leftmost symbol of v as explained.
The remaining symbols descend from the second and following children, in the
parse tree, of the nodes in the upward path from a node labeled Xi to its leftmost 
leaf, or which is the same, of the nodes in the downward path from the root
of TS to v. Therefore, for each node w in the list level -ancestorTS (v, depthTS (v)−
2), . . . , parentTS (v), v, we map w to its deﬁnition x ∈ TG, x = nodeTG
(selectXj (X, 1)) where Xj = XS[preorderTS (w)]. Once x is found, we recursively 
expand its children, from the second onwards, by mapping them back to
TS, and so on. Charging the cost to the new symbol to expand, and since there
are no unary paths, it follows that we carry out O((cid:3)) steps to extract the ﬁrst (cid:3)
symbols, and the extraction is real-time [14]. All costs per step are O(1) except
for the O(1/) to access X
For extracting suﬃxes of rules in G, we need another version of TS that stores

−1
S .

the rightmost paths. This leads to our ﬁrst result (choosing δ = o(1)).

Lemma 1. Let a sequence T [1..u] be represented by a context free grammar with
n symbols, size N , and height h. Then, for any 0 <  ≤ 1, there exists a data
structure using at most N lg n+ N lg(u/N )+ (2 + )n lg n+ o(N lg n) bits of space
that extracts any substring of length (cid:3) from T in time O((cid:3) + h lg(N/h)), and a
preﬁx or suﬃx of length (cid:3) of the expansion of any nonterminal in time O((cid:3)/).

5 Locating Patterns
A secondary occurrence of the pattern P inside a leaf of TG labeled by a symbol 
Xi occurs as well in the internal node of TG where Xi is deﬁned. If that
occurrence is also secondary, then it occurs inside a child Xj of Xi, and we can
repeat the argument with Xj until ﬁnding a primary occurrence inside some
Xk. Thus, to ﬁnd all the secondary occurrences, we can ﬁrst spot the primary
occurrences, and then ﬁnd all the copies of the nonterminal Xk that contain the
primary occurrences, as well as all the copies of the nonterminals that contain
Xk, recursively.

188

F. Claude and G. Navarro

We base our approach on the strategy proposed by K¨arkk¨ainen [18] to ﬁnd the

primary occurrences of P = p1p2 . . . pm. K¨arkk¨ainen considers the m−1 partitions
P = P1 · P2, P1 = p1 . . . pi and P2 = pi+1 . . . pm, for 1 ≤ i < m. In our case, for
each partition we will ﬁnd all the nonterminals Xk → Xk1 Xk2 . . . Xkr such that P1
is a suﬃx of some F (Xki ) and P2 is a preﬁx of F (Xki+1 ) . . .F (Xkr ). This ﬁnds each
primary occurrence exactly once. The secondary occurrences are then tracked in
the grammar tree TG. We handle the case m = 1 by ﬁnding all occurrences of Xp1
in TG using select over the labels, and treat them as primary occurrences.

5.1 Finding Primary Occurrences
As anticipated at the end of Sec. 3, we store a binary relation R ⊆ A × B to
ﬁnd the primary occurrences. It has n rows labeled Xi, for all Xi ∈ X = B, and
N − n columns4. Each column corresponds to a distinct proper suﬃx αi[j + 1..]
of a right-hand side αi. The labels belong to [1..N + 1]. The relation contains
one pair per column: (αi[j], αi[j + 1..]) ∈ R for all 1 ≤ i ≤ n and 1 ≤ j < |αi|.
Its label is the preorder of the (j + 1)th child of the node that deﬁnes Xi in TG.
The space for the binary relation is (N−n)(lg n+ lg N )+O(N ) bits.
Recall that, in our preprocessing, we have sorted X according to the lexicographic 
order of F (Xi)rev. We also sort the suﬃxes αi[j + 1..] lexicographically
with respect to their expansion, that is, F (αi[j + 1])F (αi[j + 2]) . . .F (αi[|αi|]).
This can be done in O(u + N lg N ) time in a way similar to how X was
sorted: Each suﬃx αi[j + 1..],
labeled p, can be associated to the substring 
T [select1(L, rankleaf TG (nodeTG (p)) + 1) . . . select1(L, rankleaf TG (v) + 1 +
numleavesTG (v)) − 1], where v is the parent of nodeTG (p). Then we can proceed
as in previous work [10].

1

Given P1 and P2, we ﬁrst ﬁnd the range of rows whose expansions ﬁnish with
in the expansions F (Xi)rev. Each comparison
P1, by binary searching for P rev
in the binary search needs to extract |P1| terminals from the suﬃx of F (Xi).
According to Lemma 1, this takes O(|P1|/) time. Similarly, we binary search
for the range of columns whose expansions start with P2. Each comparison needs
to extract (cid:3) = |P2| terminals from the preﬁx of F (αi[j + 1])F (αi[j + 2]) . . .. Let
r be the column we wish to compare to P2. We extract the label p associated
to the column in constant time (recall Sec. 2.2). Then we extract the ﬁrst (cid:3)
symbols from the expansion of nodeTG (p). If nodeTG (p) does not have enough
symbols, we continue with nextsiblingTG (p), and so on, until we extract (cid:3) symbols
or we exhaust the suﬃx of the rule. According to Lemma 1, this requires time
O(|P2|/). Thus our two binary searches require time O((m/) lg N ).

This time can be further improved by using the same technique as in previous
work [10]. The idea is to sample phrases at regular intervals and store the sampled
phrases in a Patricia tree [26]. We ﬁrst search for the pattern in the Patricia tree,
and then complete the process with a binary search between two sampled phrases
(we ﬁrst verify the correctness of the Patricia search by checking that our pattern
is actually within the range found). By sampling every lg u lg lg n/ lg n phrases,
4 Recall F(Xi) ≤ F(Xj ) iﬀ i ≤ j.

Improved Grammar-Based Compressed Indexes

189

(cid:2)

(cid:2)

(cid:3)(cid:3)

lg u
lg n

the resulting time for searching becomes O
o(N lg n) bits of extra space, as the Patricia tree needs O(lg u) bits per node.

and we only require

m lg

Once we identify a range of rows [a1, a2] and of columns [b1, b2], we retrieve all
the k points in the rectangle and their labels in time O((k + 1) lg n), according
to Sec. 2.2. The parents of all the nodes nodeTG (p), for each point p in the range,
correspond to the primary occurrences. In Sec. 5.2 we show how to report primary
and secondary occurrences starting directly from those nodeTG (p) positions.
We have to carry out this search for m−1 partitions of P , whereas each primary
(cid:3)
occurrence is found exactly once. Calling occ the number of primary occurrences,
the total cost of this part of the search is O

+ (m + occ) lg n

(m2/) lg

(cid:2)

(cid:2)

(cid:3)

.

lg u
lg n

5.2 Tracking Occurrences through the Grammar Tree

The remaining problem is how to track all the secondary occurrences triggered
by a primary occurrence, and how to report the positions where these occur

in T . Given a primary occurrence for partition P = P1 · P2 located at u =
nodeTG (p), we obtain the starting position of P in T by moving towards the
root while keeping count of the oﬀset between the beginning of the current
node and the occurrence of P . Initially, for node u itself, this is l = −|P1|.
Now, while u is not the root, we set l ← l + select1(L, rankleavesTG (u) + 1) −
select1(L, rankleavesTG (parentTG (u)) + 1), then u ← parentTG (u). When we

reach the root, the occurrence of P starts at l.

It seems like we are doing this h times in the worst case, since we need to
track the occurrence up to the root. In fact we might do so for some symbols,
but the total cost is amortized. Every time we move from u to v = parentTG (u),
we know that X[v] appears at least once more in the tree. This is because of
our preprocessing (Sec. 3), where we force rules to appear at least twice or be
removed. Thus v deﬁnes X[v], but there are one or more leaves labeled X[v],
and we have to report the occurrences of P inside them all. For this sake we
carry out selectX[v](X, i) for i = 1, 2 . . . until spotting all those occurrences
(where P occurs with the current oﬀset l). We recursively track them to the
root of TG to ﬁnd their absolute position in T , and recursively ﬁnd the other
occurrences of all their ancestor nodes. The overall cost amortizes to O(1) steps
per occurrence reported, as we can charge the cost of moving from u to v to the
other occurrence of v. If we report occ secondary occurrences we carry out O(occ)
steps, each costing O(lg lg n) time. We can thus use δ = O(1/ lg lg n) (Sec. 4.1)
so that the cost to access X[v] does not impact the space nor time complexity.
By adding up the space of Lemma 1 with that of the labeled binary relation,

and adding up the costs, we have our central result, Theorem 1.

6 Conclusions

We presented the ﬁrst grammar-based text index with locating time independent
on the height of the grammar. There are previous results on generating balanced
grammars to compress text, as the ones proposed by Rytter [32] and Sakamoto [33].

190

F. Claude and G. Navarro

These representations allow previous indexing techniques to guarantee sublinear
locating times, yet they introduce a penalty in the size of the grammar. Our index
also extends the grammar-based indexing techniques to a more general class of
grammars than SLPs, the only class explored so far in this scenario.

Our extraction time, O((cid:3)+h lg(N/h)), is not the optimal O((cid:3)+lg u). This time
can be achieved by adding O(N lg u) bits [4]. Within this space, our search time
can be improved by, in Sec. 5.1, (1) using full (not sampled) Patricia trees, and (2)
using a faster grid representation [5] to speed up primary occurrences (secondary
ones already take O(occ lg lg n) time). This yields the following simpliﬁed result:

Corollary 1. Let a sequence T [1..u] be represented by a context free grammar
of size N , and let 0 <  < 1 be any constant. Then, there exists a data structure
using O(N lg u) bits that ﬁnds the occ occurrences of any pattern P [1..m] in T
 N ). It can extract any substring of length (cid:3) from
in time O(m2 + (m + occ) lg
T in time O((cid:3) + lg u).

Several questions remain open: Is it possible to lower the dependence on m to
linear, as in some more limited schemes [11,25,17,31]? Is it possible to reduce the
space to N lg n + o(N lg n), that is, asymptotically the same as the compressed
text, as on statistical-compression-based self-indexes [28]? Is it possible to remove
h from the extraction complexity within less space than the current solutions [4]?

References

1. Arroyuelo, D., Navarro, G., Sadakane, K.: Reducing the Space Requirement of LZIndex.
 In: Lewenstein, M., Valiente, G. (eds.) CPM 2006. LNCS, vol. 4009, pp.
318–329. Springer, Heidelberg (2006)

2. Barbay, J., Gagie, T., Navarro, G., Nekrich, Y.: Alphabet Partitioning for Compressed 
Rank/Select and Applications. In: Cheong, O., Chwa, K.-Y., Park, K. (eds.)
ISAAC 2010, Part II. LNCS, vol. 6507, pp. 315–326. Springer, Heidelberg (2010)
3. Benoit, D., Demaine, E., Munro, I., Raman, R., Raman, V., Rao, S.S.: Representing

trees of higher degree. Algorithmica 43(4), 275–292 (2005)

4. Bille, P., Landau, G.M., Raman, R., Sadakane, K., Satti, S.R., Weimann, O.: Random 
access to grammar-compressed strings. In: Proc. 22nd SODA, pp. 373–389
(2011)

5. Chan, T., Larsen, K., Patrascu, M.: Orthogonal range searching on the RAM,

revisited. In: Proc. 27th SoCG, pp. 1–10 (2011)

6. Charikar, M., Lehman, E., Liu, D., Panigrahy, R., Prabhakaran, M., Sahai, A.,
Shelat, A.: The smallest grammar problem. IEEE Trans. Inf. Theo. 51(7), 2554–
2576 (2005)

7. Charikar, M., Lehman, E., Liu, D., Panigrahy, R., Prabhakaran, M., Rasala, A., Sahai,
 A., Shelat, A.: Approximating the smallest grammar: Kolmogorov complexity
in natural models. In: STOC, pp. 792–801 (2002)

8. Claude, F., Fari˜na, A., Mart´ınez-Prieto, M., Navarro, G.: Compressed q-gram indexing 
for highly repetitive biological sequences. In: Proc. 10th BIBE (2010)

9. Claude, F., Fari˜na, A., Mart´ınez-Prieto, M., Navarro, G.: Indexes for highly repetitive 
document collections. In: Proc. 20th CIKM, pp. 463–468 (2011)

Improved Grammar-Based Compressed Indexes

191

10. Claude, F., Navarro, G.: Self-indexed grammar-based compression. Fund.

Inf. 111(3), 313–337 (2010)

11. Do, H.H., Jansson, J., Sadakane, K., Sung, W.-K.: Fast Relative Lempel-Ziv Selfindex 
for Similar Sequences. In: Snoeyink, J., Lu, P., Su, K., Wang, L. (eds.) AAIM
2012 and FAW 2012. LNCS, vol. 7285, pp. 291–302. Springer, Heidelberg (2012)

12. Ferragina, P., Manzini, G.: Indexing compressed texts. J. ACM 52(4), 552–581

(2005)

13. Gagie, T., Gawrychowski, P., K¨arkk¨ainen, J., Nekrich, Y., Puglisi, S.J.: A Faster
Grammar-Based Self-index. In: Dediu, A.-H., Mart´ın-Vide, C. (eds.) LATA 2012.
LNCS, vol. 7183, pp. 240–251. Springer, Heidelberg (2012)

14. Gasieniec, L., Kolpakov, R., Potapov, I., Sant, P.: Real-time traversal in grammarbased 
compressed ﬁles. In: Proc. 15th DCC, pp. 458–458 (2005)

15. Golynski, A., Raman, R., Rao, S.S.: On the Redundancy of Succinct Data Structures.
 In: Gudmundsson, J. (ed.) SWAT 2008. LNCS, vol. 5124, pp. 148–159.
Springer, Heidelberg (2008)

16. Grossi, R., Gupta, A., Vitter, J.: High-order entropy-compressed text indexes. In:

Proc. 14th SODA, pp. 841–850 (2003)

17. Huang, S., Lam, T.W., Sung, W.K., Tam, S.L., Yiu, S.M.: Indexing Similar DNA
Sequences. In: Chen, B. (ed.) AAIM 2010. LNCS, vol. 6124, pp. 180–190. Springer,
Heidelberg (2010)

18. K¨arkk¨ainen, J.: Repetition-Based Text Indexing. Ph.D. thesis, Department of Computer 
Science, University of Helsinki, Finland (1999)

19. Kida, T., Matsumoto, T., Shibata, Y., Takeda, M., Shinohara, A., Arikawa, S.:
Collage system: a unifying framework for compressed pattern matching. Theor.
Comp. Sci. 298(1), 253–272 (2003)

20. Kieﬀer, J., Yang, E.H.: Grammar-based codes: A new class of universal lossless

source codes. IEEE Trans. Inf. Theo. 46(3), 737–754 (2000)

21. Kreft, S., Navarro, G.: Self-indexing Based on LZ77. In: Giancarlo, R., Manzini, G.

(eds.) CPM 2011. LNCS, vol. 6661, pp. 41–54. Springer, Heidelberg (2011)

22. Kuruppu, S., Beresford-Smith, B., Conway, T., Zobel, J.: Repetition-based compression 
of large DNA datasets. In: Proc. 13th RECOMB (2009) (poster)

23. Larsson, J., Moﬀat, A.: Oﬀ-line dictionary-based compression. Proc. of the

IEEE 88(11), 1722–1732 (2000)

24. M¨akinen, V., Navarro, G., Sir´en, J., V¨alim¨aki, N.: Storage and Retrieval of Individual 
Genomes. In: Batzoglou, S. (ed.) RECOMB 2009. LNCS, vol. 5541, pp.
121–137. Springer, Heidelberg (2009)

25. Maruyama, S., Nakahara, M., Kishiue, N., Sakamoto, H.: ESP-Index: A Compressed 
Index Based on Edit-Sensitive Parsing. In: Grossi, R., Sebastiani, F., Silvestri,
 F. (eds.) SPIRE 2011. LNCS, vol. 7024, pp. 398–409. Springer, Heidelberg
(2011)

26. Morrison, D.: PATRICIA – practical algorithm to retrieve information coded in

alphanumeric. J. ACM 15(4), 514–534 (1968)

27. Munro, J., Raman, R., Raman, V., Rao, S.S.: Succinct Representations of Permutations.
 In: Baeten, J.C.M., Lenstra, J.K., Parrow, J., Woeginger, G.J. (eds.) ICALP
2003. LNCS, vol. 2719, pp. 345–356. Springer, Heidelberg (2003)

28. Navarro, G., M¨akinen, V.: Compressed full-text indexes. ACM Comp. Surv. 39(1),

article 2 (2007)

29. Nevill-Manning, C., Witten, I., Maulsby, D.: Compression by induction of hierarchical 
grammars. In: Proc. 4th DCC, pp. 244–253 (1994)

30. Raman, R., Raman, V., Rao, S.: Succinct indexable dictionaries with applications

to encoding k-ary trees and multisets. In: Proc. 13th SODA, pp. 233–242 (2002)

192

F. Claude and G. Navarro

31. Russo, L., Oliveira, A.: A compressed self-index using a Ziv-Lempel dictionary. Inf.

Ret. 11(4), 359–388 (2008)

32. Rytter, W.: Application of Lempel-Ziv factorization to the approximation of

grammar-based compression. Theo. Comp. Sci. 302(1-3), 211–222 (2003)

33. Sakamoto, H.: A fully linear-time approximation algorithm for grammar-based compression.
 J. Discr. Alg. 3, 416–430 (2005)

34. Ziv, J., Lempel, A.: A universal algorithm for sequential data compression. IEEE

Trans. Inf. Theo. 23(3), 337–343 (1977)

35. Ziv, J., Lempel, A.: Compression of individual sequences via variable length coding.

IEEE Trans. Inf. Theo. 24(5), 530–536 (1978)

