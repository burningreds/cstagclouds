Improved Compressed Indexes

for Full-Text Document Retrieval(cid:2)

Djamal Belazzougui1 and Gonzalo Navarro2

1 LIAFA, Univ. Paris Diderot - Paris 7, France

2 Department of Computer Science, University of Chile

dbelaz@liafa.jussieu.fr

gnavarro@dcc.uchile.cl

Abstract. We give new space/time tradeoﬀs for compressed indexes
that answer document retrieval queries on general sequences. On a collection 
of D documents of total length n, current approaches require at
lg lg D ) or 2|CSA| + o(n) bits of space, where CSA is
least |CSA| + O(n lg D
a full-text index. Using monotone minimum perfect hash functions, we
give new algorithms for document listing with frequencies and top-k document 
retrieval using just |CSA| + O(n lg lg lg D) bits. We also improve
current solutions that use 2|CSA| + o(n) bits, and consider other problems 
such as colored range listing, top-k most important documents, and
computing arbitrary frequencies.

1 Introduction and Related Work

Full-text document retrieval is the problem of, given a collection of D documents
(i.e., general sequences over alphabet [1, σ]), concatenated into a text T [1, n],
preprocess T so as to later answer various queries of signiﬁcance in IR. The
problem has received much attention recently [16,22,24,11,8,7,4,12] as a natural
evolution over plain full-text indexing (which just counts and locates all the
individual occurrences of a pattern P [1, m] in T ) and for its applications in
IR on Oriental languages, software repositories, and bioinformatic databases. As
space is a serious problem in classical solutions [16,11], most of the focus has been
on extending compressed full-text indexes to answer various document retrieval
queries. The most studied queries, among several others, are the following.

Document listing: List the distinct documents where P appears.
Document listing with frequencies: List the distinct documents where P appears,

and the frequency (number of occurrences) of P in each.

Top-k retrieval: List the k documents where P appears most times.

A compressed full-text index [17] is used as the base data structure. This is
usually a compressed suﬃx array of T (we call this structure CSA and its bit
space |CSA|). The CSA simulates the suﬃx array A[1, n] [13], where A[i] points
(cid:2) Partially funded by Fondecyt Grant 1-110066, Chile. First author also partially supported 
by the French ANR-2010-COSI-004 MAPPI Project.

R. Grossi et al. (Eds.): SPIRE 2011, LNCS 7024, pp. 386–397, 2011.
c(cid:2) Springer-Verlag Berlin Heidelberg 2011

Improved Compressed Indexes for Full-Text Document Retrieval

387

to the ith lexicographically smallest suﬃx in T . The CSA ﬁnds the interval
A[sp, ep] of occurrences of P in time tsearch, usually O(m lg σ) or less [9,5]. It can
−1[i], in time O(tSA), usually O(lg1+ε n)
also compute any cell A[i], and even A
for any constant ε > 0. These indexes represent the text and the suﬃx array
within as little as nHh(T ) + o(n lg σ) bits, for any h ≤ α lgσ n and constant
α < 1. Here Hh(T ) is the empirical h-th order entropy of T [14], a lower bound
on the bits-per-symbol a statistical order-h compressor may achieve on T .

In the rest of the section we describe our contributions in context. We introduce 
at this point the concepts of binary rank, select, and of a monotone minimum 
perfect hash function (mmphf). Given a bitmap B[1, n] with m bits set,
operation rankb(B, i) counts the number of occurrences of bit b ∈ {0, 1} in
B[1, i], whereas selectb(B, j) is the position of the jth occurrence of bit b in B.
There exists a representation for B using lg
m +
O(m) + o(n) bits [21], solving both operations in constant time. As B can be reconstructed 
using operation rank, this space is asymptotically optimal. A mmphf
can be seen as a weaker structure on B, able to answer only rank1(B, i) whenever 
B[i] = 1 and giving an arbitrary value otherwise (the mmphf is unable to
tell whether B[i] = 1 or 0). As it cannot reconstruct B, the mmphf can be represented 
within less space than the previous lower bound: within O(m lg lg n
m) bits
it answers the limited rank query in constant time, and using O(m lg lg lg n
m)
bits it takes time O(lg lg n

+ O(lg lg m) + o(n) = m lg n

(cid:3)

(cid:2)
n
m

m) [2].

1.1 Document Listing with Frequencies

The pioneering work in this area [16] deﬁnes a document array E[1, n], where
E[i] tells the document to which suﬃx A[i] belongs. As noted by Sadakane [22],
a bitmap B[1, n] marking the document boundaries in T is enough to ﬁnd E[i] =
rank1(B, A[i]) in time O(tSA). The extra space for B is just D lg n
D +O(D)+o(n)
bits [21]. This permits simulating Muthukrishnan’s optimal document listing
algorithm [16] within time O(tSA) per document reported, in addition to the time
tsearch. The total space is |CSA| + O(n), the latter coming from range minimum
query (RMQ) data structures [6]. The space was made succinct by Hon et al. [11],
by sparsifying the RMQ structures over array blocks of size lgε n, so that the time
raises to O(tSA lgε n) and the space decreases to |CSA| + o(n) + D lg n
D + O(D).
We do not innovate on the plain document listing problem, but on the variant
that computes frequencies. The solutions build over plain document listing and
add extra data structures using two main approaches. A ﬁrst one stores, in addition 
to the CSA of the whole collection, one CSAd for each individual document
d, for a total space of 2|CSA| + O(n) [22] or 2|CSA| + o(n) + D lg n
D + O(D)
[11]. This extra |CSA| space is used to compute document frequencies along the
document listing. The times are as for document listing without frequencies.

A second approach [24,8] represents the document array directly, in the form
of a wavelet tree [9]. This data structure makes the document listing times
independent of tSA and enables algorithms that do not derive from Muthukrish-
nan’s [8], listing each document in O(lg D) time. The space, however, is at least
n lg D + o(n) (by using a recent encoding of the redundancy [20]).

388

D. Belazzougui and G. Navarro

Gagie et al. [7] abstracted this problem in terms of representing a sequence
E providing support for accessing any element of E, enumerating each distinct
element in a range of E, and computing sequence rankd(E, i) (the number of
occurrences of document d in E[1, i]), so that each document can be listed within
the sum of these three times. The abstraction enabled new space/time tradeoﬀs
for document listing with frequencies, achieving times as good as O(lg lg D).

An interesting observation of Gagie et al. was that one could use succinct
indexes over a given sequence representation, for example in order to support
the rankd operation on top of just the B bitmap. These “weaker” representations
that need an auxiliary mechanism to compute the cells of E are able to reduce
space. For example, they achieved O(n lg D
lg lg D ) bits with O(tSA lg lg D) time by
using a succinct index by Grossi et al. [10]. The very same lower bounds on
sequence rank given by Grossi et al. show that this tradeoﬀ is optimal.

Our ﬁrst major contribution improves upon this apparent lower bound. We
obtain a succinct index on top of the B bitmap that enables us to carry out
document listing with frequencies within less time and space than the best previous 
succinct index. We achieve O(n lg lg D) bits extra space and O(tSA) time,
or O(n lg lg lg D) bits space and O(tSA + lg lg D) time per reported document.

Our solution is based on mmphfs. As we can solve only a limited case of rank,
we cannot follow Gagie et al.’s framework [7]. Instead, we simulate Sadakane’s
method [22] using mmphfs instead of a second CSA. Our space/time results are
incomparable with those of Sadakane. Compared to the methods that represent
directly the document array, we obtain the least space, while the time comparison
depends on tSA (e.g., there are full-text indexes where tSA = O(lgε n lg1−ε σ) for
any ε > 0, yet they require O((1 + 1

ε )nHh(T )) bits [9]).

Actually our solution is general enough to solve the colored range listing problem,
 that is, ﬁnding the distinct colors (and their frequencies) of any range
in an array E[1, n] of D possible colors. Our solution is the ﬁrst in achieving 
optimal time (i.e., O(1) time per color reported) within succinct space (i.e.,
n lg D + n o(lg D) bits). Achieving this optimal time involves solving in linear
time a particular sorting problem, which can be of independent interest.

Table 1 summarizes our results on this part.

1.2 Top-k Document Retrieval
The pioneering work of Hon et al. [11] uses a sampled suﬃx tree [1] of o(n) extra
bits to reduce this problem to that of accessing E[i] and computing arbitrary frequencies 
(document listing with frequencies turns out to be a simpler problem).
They achieve time O(tsearch + k lg4+ε n) using 2|CSA| + o(n) bits.

Our second major contribution is the reduction of their time to O(tsearch +
k lg k lg2+ε n). First, we show that by choosing better the block sizes one can
reduce one lg n to lg k (in practice k is much smaller than n, and this improvement 
applies to many previous solutions). The other lg n is removed thanks to
an improved algorithm to compute arbitrary frequencies, that reduces the time
from their O(tSA lg n) to O(tSA lg lg n). While both ideas are simple, their impact
on performance is large and general.

Improved Compressed Indexes for Full-Text Document Retrieval

389

Table 1. Current and new results on document listing with frequencies (left side) and
colored range listing with frequencies (right side). On the left, the extra space is on top
of the |CSA| bits of the full-text index. The time complexities are in addition to tsearch,
and per each of the ndoc elements returned. They are valid for any constant ε > 0. On
the right we give total space, and total time per each of the ncol results reported.

Extra space
O(n lg n)
|CSA| + O(n)
|CSA| + o(n)
+D lg n

D + O(D)

n lg D + o(n)
n lg D + O(n)
n lg D + O(n)

Source
[16]
[22]
[11]

[8,4]
[7]
[7]
[7]
Ours
Ours

O(lg D
ndoc )
O( lg D
lg lg n )
O(lg lg D)

Extra Time

Space (colors)

Time (colors)

O(1)
O(tSA)

O(tSA lgε n)

O(n lg n)

n/a
n/a

O(1)
n/a
n/a

O(n lg D
lg lg D )
O(n lg lg D)
O(n lg lg lg D) O(tSA + lg lg D) n lg D + O(n lg lg lg D) O(lg lg D)

O(tSA lg lg D)

O(tSA)

n lg D + o(n)
n lg D + O(n)
n lg D + O(n)
n lg D + O(n lg D
lg lg D )
n lg D + O(n lg lg D)

O(lg D
ncol )
O( lg D
lg lg n )
O(lg lg D)
O(lg lg D)

O(1)

When representing the document array with support for rank operations,
arbitrary document counting is easy. Gagie et al. [7], apart from improving the
time achieved by Hon et al., gave several new space/time tradeoﬀs by replacing
the second |CSA|-bit space by rank-capable representations of E.

Replacing the document array by a weak representation based on mmphfs is
not straightforward, as mmphfs do not support general ranks. Our third main
contribution is a technique that modiﬁes Hon et al.’s sampled suﬃx tree [11] so
as to achieve the least space among the methods that represent the document
array, while increasing their time by an O(lg n) factor with respect to the most
space-consuming variant. The solution owes in part to the observation that there
are not too many candidates around a sampled suﬃx tree node to replace its
precomputed top-k documents. This observation can be useful in other scenarios.
Table 2 summarizes the state of the art and our contribution to the top-k
problem. As noted by Hon et al. [11], the bounds apply to the frequency mining
problem (list all documents with frequency over f), by running top-k queries
with k = 2j for consecutive j values. Our ﬁnal contribution is to reduce the time
to report the k most important documents (i.e., they have a ﬁxed priority) where
P appears, from O(tsearch + k lg3+ε n) [11] to O(tsearch + k lg k lg1+ε n).

2 Range Color Listing with Frequencies

We solve the following abstract problem: preprocess an array E[1, n] over D
colors so as to answer queries of the form: given i and j, list all the ncol distinct 
colors in E[i, j] and their number of occurrences. The connection with the
document listing problem with frequencies is obvious.
Muthukrishnan [16] solved this problem without reporting frequencies. He
builds an array F [1, n] where F [i] = max{j < i, E[j] = E[i]}. Then, using a
data structure that answers RMQ queries on F (rmq(i, j) = arg mini≤r≤j F [r])

390

D. Belazzougui and G. Navarro

Table 2. Current and new results on top-k retrieval, using the same conventions of
Table 1. The last column assumes tSA = O(lg1+ε n), as in optimal-space CSAs [5].

Simpliﬁed time

Source
[11]
[7]
Ours
[7]
[7]
Ours
Ours
Ours

Extra space
|CSA| + o(n) + D lg n
|CSA| + o(n) + D lg n
|CSA| + o(n) + D lg n
n lg D + o(n)
O(n lg D
lg lg D )
n lg D + o(n)
O(n lg D
lg lg D )

O(n lg lg lg D)

Extra Time
O(tSA lg3+ε n)

O(lg4+ε n)
D + O(D)
D + O(D) O(tSA lg D lg(D/k) lg1+ε n) O(lg4+ε n)
D + O(D) O(tSA lg k lg(D/k) lgε n) O(lg k lg2+ε n)

O(lg D lg(D/k) lgε n)

O(tSA lg D lg(D/k) lgε n)

O(lg2+ε n)
O(lg3+ε n)

O(lg k lg(D/k) lgε n)

O(lg k lg1+ε n)
O(tSA lg k lg(D/k) lgε n) O(lg k lg2+ε n)
O(lg k lg2+ε n)

O(tSA lg k lg1+ε n)

in constant time (e.g., Fischer’s [6] takes 2n + o(n) bits and does not access F ),
he ﬁnds the leftmost occurrences of all distinct colors in E[i, j] in time O(ncol).
For computing frequencies, Sadakane [22] ﬁnds also the rightmost occurrences
of the colors by building another RMQ structure on the array F built on the
reverse sequence E. The colors could be reported in diﬀerent order when listing
their rightmost or leftmost occurrences. He does not represent F nor F , and as
a consequence needs to mark the colors found in an array V [1, D]. The rest of
Sadakane’s solution is particular of document retrieval; we instead build on it to
obtain an improved solution to the general problem.

Theorem 1. We can augment a sequence of n colors in [1, D] with a structure
using O(n lg lg D) bits, so that range color listing with frequencies can be solved
in O(1) time per color reported, or using O(n lg lg lg D) bits and O(lg lg D) time.

The theorem assumes D = O(n); otherwise a mapping to the colors actually
occurring in the sequence, using O(n lg D

n ) + o(D) bits [21], must be added.

To achieve the result, for each color c we store in a mmphf fc the positions i
such that E[i] = c (i.e., fc(i) = rankc(E, i) if E[i] = c). Let nc be the frequency
of color c in E, then this structure occupies
nc ) bits, which by the
log-sum inequality is O(n(lg H0(E)+1)) = O(n lg lg D) bits. The two RMQ data
structures will add just O(n) bits. Then a query proceeds in four steps:

c O(nc lg lg n

(cid:4)

1. Use the RMQ on (virtual array) F to get the leftmost occurrences of the

ncol colors appearing in the interval. This step takes time O(ncol).

2. Use the RMQ on (virtual array) F to get the rightmost occurrences of the

ncol colors appearing in the interval. This step also takes time O(ncol).

3. Match the left and right occurrences of the ncol colors. This can be done via

4. For each color with leftmost and rightmost occurrences li and ri, report the

sorting, but we show how to do it in time O(ncol).
color and its frequency fc(ri) − fc(li) + 1 in constant time.

To avoid the sorting in step 3, we will slightly modify steps 1 and 2. We will
store V and the following additional structures:

Improved Compressed Indexes for Full-Text Document Retrieval

391

1. A vector R[1, D
lg n], where each cell occupies lg D bits; R uses at most D bits.
2. A dynamic vector Q storing triplets (ci, li, ri) and taking O(ncol lg n) bits.
3. A dynamic vector S storing leftmost positions (ci, li), in O(ncol lg n) bits.
4. A counter C.

Initially the bits in V and R are set to zero1, Q and S are empty, and C is set
to 1. We then run step 1, setting the bits in V as we progress, and appending
the unique colors and their leftmost positions (ci, li) in array S.
We now traverse S and, for each color ci, compute g = (cid:4)ci/ lg n(cid:5). Then, if
R[g] = 0, we set R[g] = C and c = rank1(V [g lg n + 1, (g + 1) lg n], lg n), which
can be computed in constant time in the RAM model [15]. Then we append c
copies of the dummy triplet (#, #, #) at the end of vector Q and ﬁnally update
counter C = C + c. At the end of this process array Q will be of size ncol and
each distinct color in E[i, j] will have an allocated position into Q.
We now retraverse S and write each pair (ci, li) in the triplet Q[R[g] + p],
where p = rank1(V [g lg n + 1, g lg n + r], r), g = (cid:4)ci/ lg n(cid:5), and r = ci − g lg n. So
V and R simulate pointers to array Q, where we have already the information
on leftmost positions, and now are prepared to write the rightmost positions.

Now we run step 2, but instead of using V to check if we have already reported
a color ci, we compute g and p as before and check whether Q[R[g] + p] =
(ci, li, #). If the third component is a #, then we had not seen the color before
and can set the component to ri. Otherwise we have already seen it.

Let us now consider the case where our mmphfs use O(nc lg lg lg n

Now Q has the input to step 4, and step 3 is avoided. Note our working space
O(ndoc lg n) bits of the query is of the same order needed to store the output.
nc ) bits. By
the log-sum inequality these add up to O(n lg lg lg D) bits. The time to query
fc is O(lg lg n
nc ). To achieve O(lg lg D) worst case, we use constant-time mmphfs
when n
nc ) =
n
O(
D lg lg D .
Adding over all the possible colors c, we have at most O(n) bits.

nc > D lg lg D. This implies that on those arrays we spend O(nc lg lg n

D lg lg D lg lg D) = O(n/D) bits, as it is increasing with nc and nc <

n

By applying the algorithm to document retrieval, where accesses to E are

through the CSA, we have the following result.

Theorem 2. We can augment a CSA on T [1, n] containing D documents with
a data structure using O(n lg lg D) bits, so that document listing with frequencies
can be solved in time O(tSA) per document reported, or one using O(n lg lg lg D)
bits and time O(tSA + lg lg D). The lg D in the space complexities can be replaced
by lg(H) + 1, where H =

and nd is the length of document d.

(cid:4) nd

n lg n
nd

3 Faster Top-k Retrieval

In this section we considerably improve the time complexities of Hon et al.’s
scheme [11] for top-k retrieval. Their solution partitions the suﬃx array into

1 This is done at indexing time. After a query returns the ncol results and sets those
ncol bits, we reset them to 0 one by one, leaving V and R ready for the next query.

392

D. Belazzougui and G. Navarro

(cid:4)

chunks of b = k(cid:5) bits. A suﬃx tree [1] on T is built and all the suﬃx tree
nodes that are lowest common ancestors (lca) of consecutive chunk endpoints
are represented in a sampled suﬃx tree, which contains O(n/b) nodes. At each
sampled node they store the top-k solution of its subtree.

When a pattern is mapped to the suﬃx array interval A[sp, ep], it is shown
(cid:4)− sp
that there exists a sampled node covering an area A[sp
and ep− ep
(cid:4) are less than b. Therefore one can simply collect the k precomputed
candidates and the (at most 2b) distinct documents mentioned in these remaining 
intervals, compute their frequencies in A[sp, ep], and take the k highest frequencies.
 By using y-fast tries [25] on the identiﬁers and on the frequencies, the
process takes time O(topb), where top = tSA + tcount + lg lg n and tcount is the time
to count an arbitrary frequency (the lg lg n will be absorbed by a lgε n later).

(cid:4)], where both sp

, ep

Since k is unknown at indexing time, this structure is built for all k powers
of 2 (i.e., lg D sampled trees), and at query time the next power of 2 is used. By
storing the top-k identiﬁers in increasing order [7] a node uses O(k lg(D/k)) bits,
and the total space is O((n/b)k lg D lg(D/k)) = O((n/(cid:5)) lg D lg(D/k)) bits. This
allows using b = k(cid:5) = k lg D lg(D/k) lgε

n, which determines the query time.

Something that is not properly considered by Gagie et al. [7] is that if the
trees are stored using pointers, then there is a component of O((n/b) lg n) bits
for k = 1, and thus (cid:5) must be at least lg1+ε n.

To avoid this we store the sampled tree in succinct form [23] using just 2 +
o(1) bits per node and supporting in O(1) time many operations, including lca,
preorder (whose consecutive values are used to index an array storing the top-
−1. For each pair of consecutive
k candidate data on each node), and preorder
chunk endpoints pi and pi+1 we store the preorder xi of the sampled tree node
lca(pi, pi+1). As xi ≥ xi−1, values xi + i are increasing, and thus can be stored
in a structure of (n/b) lg 2n
n/b + O(n/b) bits that retrieves any xi in constant time
[19]2. This space is O((n/b) lg b) = O(n
k lg D lg(D/k) lgε n) = o(n). Now we can
ﬁnd in constant time the lowest sampled node covering chunk interval [L, R] as
−1 for simplicity.
lca(preorder

−1(xR−1)). We will omit preorder

−1(xL), preorder

lg k+lg lg n

3.1 Lowering the lg D Factor to lg k
The fact that we wish to answer queries for any k ≤ D translates into a lg D
∗ on the maximum
factor in (cid:5), and into the time complexities. If we set a limit k
∗. We show now that, by carefully
k allowed at queries, this lg D becomes lg k
choosing (cid:5), we can convert the time to lg k.

Instead of choosing (cid:5) = lg D lg(D/k) lgε n so that all the sampled suﬃx trees
have the same size, we reduce it to the slightly increasing (cid:5) = lg k lg(D/k) lgε n.
lg k lgε n.
Then the space for a given k is (n/b)k lg(D/k) = (n/(cid:5)) lg(D/k) =
Added over all the k = 2j values this gives

j lgε n = O( n lg lg D

lgε n ) = o(n).

Therefore we obtain times O(topb) = O(topk lg k lg(D/k) lgε n). Note this aplg 
D
j=1

(cid:4)

n

n

plies also to previous solutions [7], as shown in Table 2.

2 Using a constant-time rank/select implementation on their internal bitmap H [15].

Improved Compressed Indexes for Full-Text Document Retrieval

393

3.2 Computing Arbitrary Frequencies

We additionally remove an O(lg n) factor from Hon et al.’s top-k retrieval query
time [11], while using the same asymptotic space. The following theorem states
the result building on the improved variant of Gagie et al. [7] and on Section 3.1.

Theorem 3. Given a concatenation T [1, n] of D documents, the top-k retrieval
problem can be solved in time O(tsearch + tSAk lg k lg(D/k) lgε n) while using
2|CSA| + o(n) + D lg n
D + O(D) bits of space, where tsearch is the time to ﬁnd
the suﬃx array interval of pattern P in the CSA of T , tSA is the time to compute
a position of the suﬃx array or its inverse, and ε > 0 is any constant.

The theorem is obtained just by noting that time tcount = O(tSA lg n) in Hon
et al.’s algorithm comes from a binary search for the epd such that an interval
[spd, epd] inside a local CSAd is mapped to a given interval [sp, ep] in the global
CSA. This binary search can be sped up by sampling every lg2 n positions in
CSAd and storing their corresponding position in the global CSA. This sampled
array stores (cid:4)nd/ lg2 n(cid:5) entries and thus takes O(nd/ lg n) bits of space for each
document d of length nd. The overall space is thus O(n/ lg n) = o(n).

We store that array of increasing values in a y-fast trie [25] so that a predecessor 
query takes O(lg lg n) time. Then the binary search for ep can be done
by ﬁrst querying the y-fast trie in time O(lg lg n), which will delimit an interval 
of size lg2 n, and then with a binary search within that interval in time
tcount = O(tSA lg lg n). They also need to ﬁnd spd given epd, which is similar. With
the optimum-space CSA used by Hon et al. [11] this time is O(lg1+ε n), and the
overall time reduces from O(lg4+ε n) per element returned, to O(lg k lg2+ε n).

4 Using Mmphfs for Top-k Retrieval

We now use mmphfs fc as in Section 2, instead of the local CSAd’s. This would
give tcount = lg lg D using O(n lg lg lg D) bits. Then the time would be O((tSA +
lg lg D + lg lg n)k(cid:5)) = O(tSAk(cid:5)), as the lg lg n term is absorbed by the lgε n in (cid:5).
The problem is that mmphfs do not give a way to compute arbitrary frequen-
(cid:4) − 1] and
(cid:4) + 1, ep]. In such a case we could easily ﬁnd its leftmost (li) and rightmost

cies. We could only do so if the document appeared both in A[sp, sp
A[ep
(ri) occurrence in A[sp, ep] and compute the frequency as fc(ri) − fc(li) + 1.

The candidates can be divided into four groups: (1) Appearing only inside
(cid:4) + 1, ep]; (3) appearing

(cid:4)]; (2) appearing both in A[sp, sp

(cid:4) − 1] and A[ep

(cid:4)

, ep

A[sp
only in A[sp, sp

(cid:4) − 1]; and (4) appearing only in A[ep

(cid:4) + 1, ep].

The only interesting candidates of group (1) are those in the precomputed
top-k list, for which we must store the frequencies, as we will have no other way
to compute them. This raises the lg(D/k) time of Section 3 to lg n. Candidates
of group (2) are found by scanning both subintervals, ﬁnding the documents that
appear in both, and their leftmost and rightmost positions. This is easily done in
time O(b lg lg n) with y-fast tries. Then we compute their frequencies using the
corresponding mmphf. How to handle the other two groups is considered next.

394

D. Belazzougui and G. Navarro

(cid:4)

, ep

√

4.1 Bounding the Number of Valid Candidates
We show that the number of documents that can make it to the top-k list if
they appear only to the left (or, similarly, to the right) chunk of the precomputed 
interval, is O(k
(cid:5)). This allows us to store all those potentially relevant
(cid:4)], we can
documents within the nodes. By storing their frequency in A[sp
(cid:4)] by just traversing the area
complete the frequency computation in A[sp, ep
(cid:4) − 1] and increasing the frequencies of the documents found (we omit
A[sp, sp
this step on documents that have already been found in both tails, as explained).
In order for a document to be out of the top-k list, but able to make it to
the list by scanning the chunk to the left of the sampled node, its frequency
must be betwen f − b + 1 and f, where f is the frequency of the kth most
frequent candidate stored. Therefore its frequency can be stored using O(lg b) =
O(lg k + lg lg n) bits. Moreover each document with frequency under f − (cid:5) + 1
must appear at least (cid:5) times in the chunk in order to have a chance, thus there
√
are at most b/k = (cid:5) such nodes. The rest need only O(lg (cid:5)) bits. Therefore the
(cid:5) lg lg n)
total space per node will be O(k lg n + k lg b + k
√
(note we are not storing the document identiﬁers of these extra candidates), and
the overall space for a given k = 2j will be O((n/b)k(lg n +
(cid:5) lg lg n)). For the
sum of spaces over j to be o(n) we need that (cid:5) = lg k lg1+ε n for some ε > 0.

√
(cid:5) lg (cid:5))) = O(k lg n + k

To know which documents are indeed candidates (i.e., can make it to the topk 
list so we have stored their frequency inside the node) we set up a bitmap of
length b marking the rightmost occurrence of such candidates, and their position
√
in the array of frequencies is obtained with rank1 on that bitmap (a second
bitmap distinguishes lg b-bit from lg (cid:5)-bit candidates). As it has at most k
(cid:5)
(cid:5) lg lg n) bits.
bits set, the bitmap can be stored within O(k
(cid:4) − 1] right to left. When we ﬁnd a 1 in this bitmap,
Thus we traverse A[sp, sp
this is the ﬁrst time we see a relevant candidate. We compute its identity in
(cid:4)] frequency using rank1 as explained. Now we
O(tSA) time and ﬁnd its A[sp
have the data to insert it (increasing its frequency by 1) into the y-fast trie. The
next occurrences (when the bitmap has value 0) correspond to candidates that
either have already been found (and thus are already inserted in the y-fast trie)
or candidates that cannot make it to the top-k list (and thus are not present in
the y-fast trie and we must not care about them).

√
(cid:5)) = O(k

(cid:5) lg

√

√

(cid:4)

, ep

The missing piece is to prove that there are suﬃciently few candidates.

√
2bk.

r=0 topk(s − r, e)| < k +

Lemma 1. Let topk(s, e) be k most frequent colors in an array E[s, e]. Then
there is a choice of topk(·,·) sets in case of frequency ties such that, for any b,
C(b) = | ∪b
Proof. Let us call st < s the position where k · t new elements have made it in
topk at some point, i.e., C(s − st) = C(0) + kt = k + kt. Let us call fr the kth
highest frequency in E[r, e]. Since all elements not in topk(s, e) have frequency
at most f = fs in E[s, e], a new element must appear at least once in E[r, s− 1]
to reach frequency f + 1 and force us choose it for topk(r, e). Hence s1 ≤ s − k.
Now, as k distinct elements have entered in topk(s1, e), it must hold that
fs1 ≥ f + 1, as we have seen k distinct elements reaching frequency f + 1. Thus
the (k + 1)th distinct element appearing in topk(r, e) must appear at least twice

Improved Compressed Indexes for Full-Text Document Retrieval

395

Thus as long as st ≥ s − b we have t(t+1)

in E[r, s−1], to jump from frequency at most f to at least f +2. Thus we need 2k
occurrences of elements that are incompatible with the previous k occurrences
in order to have k new distinct elements, thus s2 ≤ s − 3k.
Once these new k distinct elements enter in topk(s2, e), it holds that fs2 ≥
f + 2, and thus we need 3k incompatible occurrences for the next k occurrences,
and so on. Iterating the argument, it holds st ≤ s − t(t+1)
2b/k. Hence
the number of new elements entering into some topk(s − r, e) for 1 ≤ b ≤ r is
(cid:9)(cid:10)
C(b) < k(t + 1) < k +
In our case b = k(cid:5) so the bound is C(b) = O(k
(cid:5)). We have proved the main
result. The time simpliﬁes to O(tsearch + k lg k lg2+ε n) when tSA = lg1+ε n.
Theorem 4. Given a concatenation T [1, n] of D documents, the top-k retrieval
problem can be solved in time O(tsearch + tSAk lg k lg1+ε n) using O(n lg lg lg D)
extra bits, where tsearch is the time to ﬁnd the suﬃx array interval of pattern P
in the CSA of T , tSA is the time to compute a position of the suﬃx array or its
inverse, and ε > 0 is any constant.

k for all t ≥ 1.

k ≤ b, and thus t <

2

√

2bk.

(cid:5)

2

√

5 Top-k Most Important Document Retrieval

A particular variant of top-k document retrieval, somewhat easier than the one
that seeks for the highest frequencies, is one where the documents have a ﬁxed
importance or priority. An example would be the PageRank value of Web pages.
A way to handle this problem is to sort the documents by importance, so that
document i is the ith most important in the collection. Then the problem becomes 
that of ﬁnding the k smallest distinct values in E[sp, ep]. While methods
based on range quantile queries on wavelet trees [8] naturally report the documents 
in sorted order and thus automatically solve this problem in O(k lg D)
time by pruning the process after reporting k results, the situation is not that
easy for the other approaches that use potentially less space.

A solution comes from the same top-k retrieval technique of Hon et al. [11].
This time one stores the k smallest document values within each sampled node,
and traverses the tails of the interval looking for smaller document identiﬁers. No
frequencies need to be computed, which allows for an O(tSAk lg k lg(D/k) lgε n)
time solution, e.g., O(k lg k lg2+ε
n). This seems unimportant now that we have
reduced the complexity of the more diﬃcult top-k retrieval problem to the same
level. Yet, we show that this particular problem can be solved faster, removing
the lg(D/k) factor. When tSA = lg1+ε n, this gives time O(tsearch + k lg k lg1+ε n).
Theorem 5. Given a concatenation T [1, n] of D documents, the top-k most
important retrieval problem can be solved in time O(tsearch + tSAk lg k lgε n) while
using |CSA| + o(n) + D lg n
D + O(D) bits of space, where tsearch is the time to ﬁnd
the suﬃx array interval of pattern P in the CSA of T , tSA is the time to compute
a position of the suﬃx array or its inverse, and ε > 0 is any constant.
The result of Hon et al. [11] is achieved by using chunks of b = k(cid:5) positions for
(cid:5) = lg2+ε n (for the more reﬁned complexity we use (cid:5) = lg k lg(D/k) lgε n). Our
(cid:4) = k lg k lgε n.
idea is to further divide those chunks into lg(D/k) buckets of size b

396

D. Belazzougui and G. Navarro

For each chunk we build a small local sampled suﬃx tree. A query will then span
at most one global node, two local nodes, and two tail buckets.

Consider the endpoints p1 . . . pr of the buckets inside a given chunk, and call
v = lca(p1, pr) the lowest sampled global suﬃx tree node that covers the chunk.
Just as for the global scheme, ﬁnd in the suﬃx tree the lca nodes of each pair
of consecutive endpoints, lca(pi, pi+1). All those lca nodes are below v or are v.
(cid:4)) local sampled nodes. Moreover, if some node u =
lca(pi, pi+1) covers the whole chunk [p1, pr], then it must be an ancestor of
v = lca(p1, pr), but since it is also a descendant of v, we have u = v. That is,
the local sampled suﬃx tree nodes (that are not already global sampled suﬃx
tree nodes) cannot cover a chunk and hence span less than 2b positions.

There are overall O(n/b

Instead of storing the top-k document identiﬁers using O(k lg(D/k)) bits,
for these local sampled nodes we will store the positions of some occurrence of
those identiﬁers within the local sampled node, sorted by increasing position.
The identiﬁer must be obtained with an access to that position, which will
not change the complexity. Since local positions span less than 2b, they require
O(k lg(b/k)) = O(k lg (cid:5)) = O(k lg lg n) bits. The tree topology itself will require
2+ o(1) bits per node, as for the global tree. The total space for a given k = 2j is
lg k lgε n), which added over all k = 2j values gives o(n)
O((n/b
bits overall. We also must store a local node identiﬁer yi = preorder(lca(pi, pi+1))
k lg k lgε n ) = O( n lg lg n
for each bucket, which requires O((n/b
k lgε n ),
which added over all k = 2j values gives o(n) bits as well.

(cid:4)) lg b) = O(n lg k+lg lg n

(cid:4))k lg lg n) = O(n lg lg n

(cid:4)

(cid:4)

To query, we determine the interval A[sp, ep] of P and the covered chunk
[L, R], the covered bucket [l1, r1 = Lb
/b] to the left of chunk L, and the covered
bucket [l2 = Rb
/b, r2] to the right of chunk R. Then we ﬁnd the global sampled
node v = lca(xL, xR−1), and the local sampled nodes u1 = lca(yl1, yr1−1) and
u2 = lca(yl2, yr2−1). If u1 or u2 are equal to v we discard them. Now we take
the at most 3k candidates from v, u1 and u2, and also consider the elements in
(cid:4))) to extract all the
E[sp, r1b
candidate identiﬁers, plus O(k lg lg n) to maintain a heap of the smallest k values
seen in the process using a y-fast trie [25]. The time adds up to O(tSAk lg k lgε n).

l2 + 1, ep]. The time is O(tSA(k + b

(cid:4) − 1] and E[b

(cid:4)

6 Final Remarks
A natural next step is to implement these solutions. Many of our improvements
are easy to implement, and practical implementations of mmphfs exist [3]. A
recent empirical work [18] shows that the individual CSAd’s pose much space
overhead, at least if implemented naively. Instead, they compress wavelet trees
to 7-17 bpc (bits per text character), compared to the 4.5-6.0 bpc of the global
CSA. Over their same collections, our mmphf implementation takes 3-5 bpc and
gives sub-microsecond times. This shows that the alternative of using mmphfs
is very appealing compared to both using CSAd’s or wavelet trees.

References

1. Apostolico, A.: The myriad virtues of subword trees. In: Combinatorial Algorithms

on Words. NATO ISI Series, pp. 85–96. Springer, Heidelberg (1985)

Improved Compressed Indexes for Full-Text Document Retrieval

397

2. Belazzougui, D., Boldi, P., Pagh, R., Vigna, S.: Monotone minimal perfect hashing:

searching a sorted table with o(1) accesses. In: SODA, pp. 785–794 (2009)

3. Belazzougui, D., Boldi, P., Pagh, R., Vigna, S.: Theory and practise of monotone

minimal perfect hashing. In: ALENEX (2009)

4. Culpepper, J.S., Navarro, G., Puglisi, S.J., Turpin, A.: Top-k ranked document
search in general text databases. In: de Berg, M., Meyer, U. (eds.) ESA 2010.
LNCS, vol. 6347, pp. 194–205. Springer, Heidelberg (2010)

5. Ferragina, P., Manzini, G., M¨akinen, V., Navarro, G.: Compressed representations

of sequences and full-text indexes. ACM Trans. Alg. 3(2), art. 20 (2007)

6. Fischer, J.: Optimal succinctness for range minimum queries. In: L´opez-Ortiz, A.

(ed.) LATIN 2010. LNCS, vol. 6034, pp. 158–169. Springer, Heidelberg (2010)

7. Gagie, T., Navarro, G., Puglisi, S.J.: Colored range queries and document retrieval.
 In: Chavez, E., Lonardi, S. (eds.) SPIRE 2010. LNCS, vol. 6393, pp. 67–81.
Springer, Heidelberg (2010)

8. Gagie, T., Puglisi, S.J., Turpin, A.: Range quantile queries: Another virtue of
wavelet trees. In: Karlgren, J., Tarhio, J., Hyyr¨o, H. (eds.) SPIRE 2009. LNCS,
vol. 5721, pp. 1–6. Springer, Heidelberg (2009)

9. Grossi, R., Gupta, A., Vitter, J.: High-order entropy-compressed text indexes. In:

SODA, pp. 841–850 (2003)

10. Grossi, R., Orlandi, A., Raman, R.: Optimal trade-oﬀs for succinct string indexes.
In: Abramsky, S., Gavoille, C., Kirchner, C., Meyer auf der Heide, F., Spirakis, P.G.
(eds.) ICALP 2010. LNCS, vol. 6198, pp. 678–689. Springer, Heidelberg (2010)

11. Hon, W.-K., Shah, R., Vitter, J.S.: Space-eﬃcient framework for top-k string retrieval 
problems. In: FOCS, pp. 713–722 (2009)

12. Karpinski, M., Nekrich, Y.: Top-k color queries for document retrieval. In: SODA,

pp. 401–411 (2011)

13. Manber, U., Myers, G.: Suﬃx arrays: a new method for on-line string searches.

SIAM J. Comp. 22(5), 935–948 (1993)

14. Manzini, G.: An analysis of the Burrows-Wheeler transform. J. ACM 48(3), 407–

430 (2001)

15. Munro, I.: Tables. In: Chandru, V., Vinay, V. (eds.) FSTTCS 1996. LNCS,

vol. 1180, pp. 37–42. Springer, Heidelberg (1996)

16. Muthukrishnan, S.: Eﬃcient algorithms for document retrieval problems. In:

SODA, pp. 657–666 (2002)

17. Navarro, G., M¨akinen, V.: Compressed full-text indexes. ACM Comp. Surv. 39(1),

art. 2 (2007)

18. Navarro, G., Puglisi, S.J., Valenzuela, D.: Practical compressed document retrieval.
In: Pardalos, P.M., Rebennack, S. (eds.) SEA 2011. LNCS, vol. 6630, pp. 193–205.
Springer, Heidelberg (2011)

19. Okanohara, D., Sadakane, K.: Practical entropy-compressed rank/ select dictionary.
 In: ALENEX (2007)

20. Pˇatra¸scu, M.: Succincter. In: FOCS, pp. 305–313 (2008)
21. Raman, R., Raman, V., Rao, S.: Succinct indexable dictionaries with applications

to encoding k-ary trees and multisets. In: SODA, pp. 233–242 (2002)

22. Sadakane, K.: Succinct data structures for ﬂexible text retrieval systems. J. Discr.

Alg. 5(1), 12–22 (2007)

23. Sadakane, K., Navarro, G.: Fully-functional succinct trees. In: SODA, pp. 134–149

(2010)

24. V¨alim¨aki, N., M¨akinen, V.: Space-eﬃcient algorithms for document retrieval. In:
Ma, B., Zhang, K. (eds.) CPM 2007. LNCS, vol. 4580, pp. 205–215. Springer,
Heidelberg (2007)

25. Willard, D.E.: Log-logarithmic worst-case range queries are possible in space θ(n).

Inf. Process. Lett. 17(2), 81–84 (1983)

