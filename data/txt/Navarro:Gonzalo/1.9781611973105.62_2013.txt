Optimal Dynamic Sequence Representations ∗

Gonzalo Navarro†

Yakov Nekrich‡

Abstract

We describe a data structure that supports access, rank and
select queries, as well as symbol insertions and deletions, on
a string S[1, n] over alphabet [1..σ] in time O(lg n/ lg lg n),
which is optimal. The time is worst-case for the queries
and amortized for the updates. This complexity is better
than the best previous ones by a Θ(1 + lg σ/ lg lg n) factor.
Our structure uses nH0(S) + O(n + σ(lg σ + lg1+ε n)) bits,
where H0(S) is the zero-order entropy of S and 0 < ε < 1
is any constant. This space redundancy over nH0(S) is
also better, almost always, than that of the best previous
dynamic structures, o(n lg σ)+O(σ(lg σ+lg n)). We can also
handle general alphabets in optimal time, which has been an
open problem in dynamic sequence representations.

1

Introduction

supporting rank and select
String representations
queries are fundamental in many data structures, including 
full-text indexes [20, 15, 18], permutations
[18, 3], inverted indexes [10, 3], graphs [13], document
retrieval indexes [35], labeled trees [18, 5], XML indexes
[21, 14], binary relations [5], and many more. The problem 
is to encode a string S[1, n] over alphabet Σ = [1..σ]
so as to support the following queries:

ranka(S, i) = number of occurrences of a ∈ Σ

in S[1, i], for 1 ≤ i ≤ n.

selecta(S, i) = position in S of the i-th occurrence

of a ∈ Σ, for 1 ≤ i ≤ ranka(S, n).

access(S, i) = S[i].

There exist various representations of S that support 
these operations [20, 18, 15, 3, 7]. However, these
representations are static, that is, S cannot change. In
various applications one needs dynamism, that is, to
insert and delete symbols in S. There are not many dynamic 
solutions, however. All are based on the wavelet
tree representation [20]. The wavelet tree decomposes

gnavarro@dcc.uchile.cl.

∗Partially funded by Fondecyt grant 1-110066, Chile.
†Department of Computer Science, University of Chile. Email:
‡Laboratoire d’Informatique Gaspard Monge, Universit´e ParisEst 
& CNRS. This work was done while this author was at the
University of Chile. Email: yakov.nekrich@googlemail.com.

S hierarchically. In a ﬁrst level, it separates larger from
smaller symbols, by marking in a bitmap which symbols 
of S were larger and which were smaller. The two
subsequences of S are recursively separated. The lg σ
levels of bitmaps describe S, and access, rank and select
operations on S are carried out via lg σ rank and select
operations on the bitmaps. Insertions and deletions in
S can also be carried out by inserting and deleting bits
from lg σ bitmaps (see Section 2 for more details).

H0(S) = (cid:80)

In the static case, rank and select operations on
bitmaps take constant time, and therefore access, rank
and select on S takes O(lg σ) time [20]. This can
be reduced to O(1 + lg σ/ lg lg n) by using multiary
wavelet trees [15]. These separate the symbols into
ρ = o(lg n) ranges, and instead of a bitmap store a
sequence over an alphabet of size ρ.
In the dynamic
case, however, the operations on those bitmaps or sequences 
are slowed down. M¨akinen and Navarro [27]
obtained O(lg σ lg n) time for all the operations, including 
updates, by using dynamic bitmaps that handled
all the operations in time O(lg n). They simultaneously
compress the sequence to nH0(S) + o(n lg σ) bits. Here
a∈[1..σ](na/n) lg(n/na) ≤ lg σ is the zeroorder 
entropy of S, where na is the number of occurrences 
of a in S. Gonz´alez and Navarro [19] improved
the times to O((1 + lg σ/ lg lg n) lg n) by extending the
results to multiary wavelet trees. In this case, instead of
dynamic bitmaps, they handled dynamic sequences over
a small alphabet (of size ρ). Finally, He and Munro [22]
and Navarro and Sadakane [30] obtained the currently
best result, O((1 + lg σ/ lg lg n) lg n/ lg lg n) time, still
within the same space. They did so by improving the
times of the dynamic sequences on small alphabets to
O(lg n/ lg lg n), which is optimal even on bitmaps [17].
The Ω((lg n/ lg lg n)2) lower bound for dynamic range
counting in two dimensions [33], and the O(lg n/ lg lg n)
static upper bound using wavelet trees [9], suggest that
no more improvements are possible in this line.

In this paper we show that this dead-end can be
broken by abandoning the implicit assumption that, to
provide access, rank and select on S, we must provide
rank and select on the bitmaps (or sequences over [1..ρ]).
We show that all what is needed is to track positions of
S downwards and upwards along the wavelet tree.
It
turns out that this tracking can be done in constant

865

Copyright © SIAM.
Unauthorized reproduction of this article is prohibited.

Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phptime per level, breaking the Θ(lg n/ lg lg n) per-level
barrier. A second tool to achieve full independence of σ,
and to compress the redundancy space, is the alphabet
partitioning technique [3], which we exploit in a novel
way under a dynamic scenario.

As a result, we obtain the optimal time complexity
O(lg n/ lg lg n) for all the queries (worst-case) and update 
operations (amortized). This is Θ(1 + lg σ/ lg lg n)
times faster than what was believed to be the “ultimate”
solution. We also improve upon the space by compressing 
the redundancy of o(n lg σ) + O(σ(lg σ + lg n)) of
previous dynamic structures. Our space is nH0(S) +
O(n+σ(lg σ +lg1+ε n)) bits, for any constant 0 < ε < 1.
Finally, we also handle general alphabets, such as
Σ = R, or Σ = Γ∗ for a symbol alphabet Γ, in optimal
time. For example, in the comparison model for Σ = R,
the time is O(lg σ + lg n/ lg lg n), where σ is the number
of distinct symbols that appear in S; in the case Σ = Γ∗
for general Γ, the time is O(|p| + lg γ + lg n/ lg lg n),
where |p|
is the query length and γ the number of
distinct symbols of Γ that appear in the elements of S.
Handling varying alphabets has been a long-standing
problem on dynamic sequences, since wavelet trees do
not deal well with them. We work around this problem
by means of our dynamic alphabet partitioning scheme.
At the end we describe several applications where
our result oﬀers improved time/space tradeoﬀs. These
include compressed indexes for dynamic text collections,
construction of the Burrows-Wheeler transform [11] and
static compressed text indexes within compressed space,
and compressed representations of dynamic binary relations,
 directed graphs, and inverted indexes.

2 The Wavelet Tree

Let S be a string over alphabet Σ = [1..σ]. We associate
each a ∈ Σ to a leaf va of a full balanced binary tree
T . The essential idea of the wavelet tree structure is
the representation of elements from a string S by bit
sequences stored in the nodes of tree T . We associate
a subsequence S(v) of S with every node v of T . For
the root vr, S(vr) = S. In general, S(v) consists of all
occurrences of symbols a ∈ Σv in S, where Σv is the
set of symbols assigned to leaf descendants of v. The
wavelet tree does not store S(v) explicitly, but just a
bit vector B(v). We set B(v)[i] = t if the i-th element
of S(v) also belongs to S(vt), where vt is the t-th child
of v (the left child corresponds to t = 0 and the right
to t = 1). This data structure (i.e., T and bit vectors
B(v)) is called a wavelet tree.
For any symbol S[i] = a and every node v such that
a ∈ Σv, there is exactly one bit bv in B(v) that indicates
in which child of v the leaf vS[i] is stored. We will say
that such bv encodes S[i] in B(v); we will also say that

bit bv from B(v) corresponds to a bit bu from B(u) if
both bv and bu encode the same symbol S[i] in two nodes
v and u. Identifying the positions of bits that encode
the same symbol plays a crucial role in wavelet trees.
Other, more complex, operations rely on our ability to
navigate in the tree and keep track of bits that encode
the same symbol.

To implement access(S, i) we traverse a path from
the root to the leaf vS[i]. In each visited node we read
the bit bv that encodes S[i] and proceed to the bv-th
child of v. To compute ranka(S, i), we identify the last
bit b(cid:48) that precedes B(v)[i] and corresponds to some
symbol in S(va). To answer selecta(S, i), we identify the
index of the bit bv in B(v) that corresponds to S(va)[i].
The standard method used in wavelet trees for identifying 
the corresponding bits is to maintain rank/select
data structures on the bit vectors B(v). Let B(v)[e] = t;
we can ﬁnd the oﬀset of the corresponding bit in the
child of v by answering a query rankt(B(v), e).
If v
is the r-th child of a node u, we can ﬁnd the oﬀset
of the corresponding bit in u by answering a query
selectr(B(u), e). This approach leads to O(lg σ) query
times in the static case because rank/select queries on a
bit vector can be answered in constant time. However,
we need Ω(lg n/ lg lg n) time to support rank/select and
updates on a bit vector [17], which multiplies the operation 
times. A slight improvement can be achieved by
increasing the fan-out of the wavelet tree to Θ(lgε n):
as before, B(v)[e] = t if the e-th element of S(v) also
belongs to S(vt) for the t-th child vt of v. This enables 
us to reduce the height of the wavelet trees and
the query time by a Θ(lg lg n) factor. However, it seems
that further improvements that are based on dynamic
rank/select queries in every node are not possible.

In this paper we use a diﬀerent approach to identifying 
the corresponding elements. We partition sequences 
B(v) into blocks, which are stored in compact
list structures L(v). Pointers from selected positions
in L(v) to the structure L(u) in a parent node u (and
vice versa) enable us to navigate between nodes of the
wavelet tree in constant time. We extend the idea to
multiary wavelet trees. While similar techniques have
been used in some geometric data structures [31, 8], applying 
them on compressed data structures where the
bit budget is severely limited is much more challenging.

3 Basic Structure

We start by describing the main components of our
modiﬁed wavelet tree. Then, we show how our structure
supports access(S, i) and selecta(S, i). In the third part
of this section we describe additional structures that
enable us to answer ranka(S, i). Finally, we show how
to support updates.

866

Copyright © SIAM.
Unauthorized reproduction of this article is prohibited.

Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php3.1 Structure We assume that the wavelet tree T
has node degree ρ = Θ(lgε n). We divide sets B(v)
into blocks and store those blocks in a doubly-linked list
L(v). Each block Gj(v), except the last one, contains
Θ(lg3 n) consecutive elements from B(v); the last block
contains O(lg3 n) consecutive elements. For each Gj(v)
we maintain a data structure Rj(v) that supports rank
and select queries on elements of Gj(v). Since a block
contains a poly-logarithmic number of elements over an
alphabet of size ρ, we can answer rank and select queries
in O(1) time using O(|Gj(v)|/ lg1−ε n) additional bits,
for any constant 0 < ε < 1 (see Appendix A for details).
A pointer to an element B(v)[e] consists of two
parts: a unique id of the block Gj(v) that contains oﬀset
e and the index of e in Gj(v). Such a pair (block id,
local index) will be called the position of e in v.

We maintain pointers between selected corresponding 
elements in L(v) and its children.
If an element
B(v)[e] = t is stored in a block Gj(v) and B(v)[e(cid:48)] (cid:54)= t
for all e(cid:48) < e in Gj(v), then we store a pointer from e
to the oﬀset et of the corresponding element B(vt)[et]
in L(vt), where vt is the t-th child of v. If B(v)[e] = t
and the corresponding et in L(vt) is the ﬁrst oﬀset in its
block, then we also store a pointer from e to et. If there
is a pointer from e in L(v) to et in L(vt), then we also
store a pointer from et to e. All these pointers will be
called inter-node pointers. We describe how inter-node
pointers are implemented later in this section.

O((cid:80)

It is easy to see that the number of inter-node
pointers from e in L(v) to et in L(vt), for any ﬁxed
t, is Θ(g(v)), where g(v) is the number of blocks in
L(v). Hence, the total number of pointers that point
down from a node v is bounded by O(g(v)ρ). Since this
also equals the number of pointers that point up to v,
the total number of pointers in the wavelet tree equals

v∈T g(v)ρ) = O(n lg σ/ lg3−ε n + σ lgε n).
The pointers from a block Gj(v) are stored in a
data structure Fj(v). Using Fj(v), we can ﬁnd, for any
oﬀset e in Gj(v) and any 1 ≤ t ≤ ρ, the last e(cid:48) ≤ e in
Gj(v) such that there is a pointer from e(cid:48) to an oﬀset
e(cid:48)
t in L(vt). We describe in Appendix A how Fj(v)
implements the queries in constant time.

For the root node vr, we store a dynamic partialsum 
data structure K(vr) that contains the number of
positions in each block of L(vr). Using K(vr), we can
ﬁnd the block Gj(vr) that contains the i-th element of
S(vr) = S, as well as the number of elements in all
the blocks that precede a given block Gj(vr). Both
operations can be supported in O(lg n/ lg lg n) time
[23, 30]. The same data structures K(va) are also stored
in the leaves va of T . We observe that we do not store
a sequence B(va) in a leaf node va. Nevertheless, we
divide the (implicit) sequence B(va) into blocks and

List of blocks storing B(v)
j-th block of list L(v)
Supports rank/select/access inside Gj(v)
Pointers leaving from Gj(v)

L(v)
Gj(v)
Rj(v)
Fj(v)
Hj(v) Pointers arriving at Gj(v)
Pt(v)
K(v)
Dj(v) Deleted elements in Gj(v), for vr and va
DEL Global list of deleted elements in S.

Predecessor in L(v) containing symbol t
Partial sums on block lengths for vr and va

Table 1: Structures inside any node v of the wavelet
tree T , or only in the root node vr and the leaves va.

store the number of positions in each block in K(va);
we maintain K(va) only if L(va) consists of more than
one block. Moreover we store inter-node pointers from
the parent of va to va and vice versa. Pointers in a leaf
are maintained using the same rules of any other node.
For future reference, we provide the list of secondary

data structures in Table 1.

3.2 Access and Select Queries Assume the position 
of an element B(v)[e] = t in L(v) is known, and
let iv be the index of oﬀset e in its block Gj(v). Then
the position of the corresponding oﬀset et in L(vt) is
computed as follows. Using Fj(v), we ﬁnd the index
i(cid:48) of the largest e(cid:48) ≤ e in Gj(v) such that there is a
pointer from e(cid:48) to some e(cid:48)
t in L(vt). Due to our construction,
 such e(cid:48) must exist. Let i(cid:48)
t denote the
indexes of e(cid:48) and e(cid:48)
t respectively, and let G(cid:96)(vt) denote
the block that contains e(cid:48)
t. Let rv = rankt(Gj(v), iv)
and r(cid:48)
v = rankt(Gj(v), i(cid:48)
v). Due to our rules to deﬁne 
pointers, et also belongs to G(cid:96)(vt) and its index
is i(cid:48)
v). Thus we can ﬁnd the position of et in
O(1) time if the position of B(v)[e] = t is known.

t + (rv − r(cid:48)

v and i(cid:48)

Analogously, assume we know a position B(vt)[et]
at Gj(vt) and want to ﬁnd the position of the corresponding 
oﬀset e in its parent node v. Using Fj(vt) we
t ≤ et in Gj(vt) that has a pointer to
ﬁnd the last e(cid:48)
its parent, which exists by construction. Let e(cid:48)
t point
to e(cid:48), with index i(cid:48) in a block G(cid:96)(v). Let i(cid:48)
t and it be
the indexes of e(cid:48)
t and et in Gj(vt), respectively. Then,
by our construction, e is also in G(cid:96)(v) and its index is
selectt(G(cid:96)(v), rankt(G(cid:96)(v), i(cid:48)) + (it − i(cid:48)
To solve access(S, i), we visit the nodes v0 =
vr, v1 . . . vh = va, where h = lgρ σ is the height of T ,
vk is the tk-th child of vk−1 and B(vk−1)[ek−1] = tk
encodes S[i]. We do not ﬁnd out the oﬀsets e1, . . . , eh,
but just their positions. The position of e0 = i is found
in O(lg n/ lg lg n) time using the partial-sums structure
If the position of ek−1 is known, we can ﬁnd
K(vr).
that of ek in O(1) time, as explained above. When a
leaf node vh = va is reached, we know that S[i] = a.

t)).

867

Copyright © SIAM.
Unauthorized reproduction of this article is prohibited.

Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpTo solve selecta(S, i), we set eh = i and identify its
position in the list L(va) of the leaf va, using structure
K(va). Then we traverse the path vh, vh−1, . . . , v0 = vr
where vk−1 is the parent of vk, until the root node
is reached.
In every node vk, we ﬁnd the position of
ek−1 in L(vk−1) that corresponds to ek as explained
above. Finally, we compute the number of elements
that precede e0 in L(vr) using structure K(vr).

Clearly,

require O(lgρ σ +
lg n/ lg lg n) = O((lg σ + lg n)/ lg lg n) worst-case time.

access and select

3.3 Rank Queries We need some additional data
structures for the eﬃcient support of rank queries. In
every node v such that L(v) consists of more than one
block, we store a data structure P (v). Using P (v) we
can ﬁnd, for any 1 ≤ t ≤ ρ and for any block Gj(v),
the last block G(cid:96)(v) that precedes Gj(v) and contains
an element B(v)[e] = t. P (v) consists of ρ predecessor
data structures Pt(v) for 1 ≤ t ≤ ρ. We describe in
Section 4 a way to support these predecessor queries in
constant time in our scenario.

Let the position of oﬀset e be the i-th element in a
block Gj(v). P (v) enables us to ﬁnd the position of the
last e(cid:48) ≤ e such that B(v)[e(cid:48)] = t. First, we use Rj(v) to
compute r = rankt(Gj(v), i). If r > 0, then e(cid:48) belongs
to the same block as e and its index in the block Gj(v)
is selectt(Gj(v), r). Otherwise, we use Pt(v) to ﬁnd the
last block G(cid:96)(v) that precedes Gj(v) and contains an
element B(v)[e(cid:48)] = t. We then ﬁnd the last such element
in G(cid:96)(v) using R(cid:96)(v).

Now we are ready to describe the procedure to
answer ranka(S, i). The symbol a is represented as a
concatenation of symbols t0 ◦ t1 ◦ . . . ◦ th, where each
tk is between 1 and ρ. We traverse the path from the
root vr = v0 to the leaf va = vh. We ﬁnd the position
of e0 = i in vr using the data structure K(vr). In each
node vk, 0 ≤ k < h, we identify the position of the last
element B(vk)[e(cid:48)
k] = tk that precedes ek, using Ptk (vk).
Then we ﬁnd the oﬀset ek+1 in the list L(vk+1) that
corresponds to e(cid:48)
k.

When our procedure reaches the leaf node vh,
the element B(vh)[eh] encodes the last symbol a that
precedes S[i]. We know the position of oﬀset eh, say
index ih in its block G(cid:96)(vh). Then we ﬁnd the number
r of elements in all the blocks that precede G(cid:96)(vh) using
K(vh). Finally, ranka(S, i) = r + ih.

Since structures Pt answer queries in constant time,
the overall time for rank is O(lgρ σ + lg n/ lg lg n) =
O((lg σ + lg n)/ lg lg n).

3.4 Updates Now we describe how inter-node pointers 
are implemented. We say that an element of L(u) is
pointed if there is a pointer to its oﬀset. Unfortunately,

we cannot store the local index of a pointed element in
the pointer: when a new element is inserted into a block,
the indexes of all the elements that follow it are incremented 
by 1. Since a block can contain Θ(lg3 n) pointed
elements, we would have to update up to Θ(lg3 n) pointers 
after each insertion and deletion.

Therefore we resort to the following two-level
scheme. Each pointed element in a block is assigned
a unique id. When a new element is inserted, we assign
it the id max−id + 1, where max−id is the maximum
id value used so far. We also maintain a data structure 
Hj(v) for each block Gj(v) that enables us to ﬁnd
the position of a pointed element if its id in Gj(v) is
known. Implementation of Hj(v) is based on standard
word RAM techniques and a table that contains ids of
the pointed elements; details are given in Appendix A.
We describe now how to insert a new symbol a
into S at position i. Let e0, e1, . . . , eh be the oﬀsets
of the elements that will encode a = t0 ◦ . . . ◦ th in
vr = v0, v1, . . . , vh = va. We can ﬁnd the position of
e0 = i in L(vr) in O(lg n/ lg lg n) time using K(vr), and
insert t0 at that position, B(vr)[e0] = t0. Now, given the
position of ek, in L(vk), where B(vk)[ek] = tk, we ﬁnd
the position of the last e(cid:48)
k] = tk,
in the same way as for rank queries. Once we know the
position of e(cid:48)
k+1 in
L(vk+1) that corresponds to e(cid:48)
k. The element tk+1 must
be inserted into L(vk+1) immediately after e(cid:48)(cid:48)
k+1, at the
position of e(cid:48)(cid:48)

k < ek such that B(vk)[e(cid:48)
k in L(vk), we ﬁnd the position of e(cid:48)(cid:48)

k+1 + 1 = ek+1.

The insertion of a new element B(vk)[ek] = t
into a block Gj(vk) is supported by structure Rj(vk).
We must also update structures Fj(vk), Hj(vk) and
Pt(vk). These updates take O(1) time, see Section 4 for
structure Pt(vk) and Appendix A for the others. Since
pointers are bidirectional, changes to Fj(vk) trigger
changes in the F and H structures of vk−1 and vk+1.
If vk is the root node or a leaf, we also update K(vk).
If the number of elements in Gj(vk) exceeds 2 lg3 n,
we split Gj(vk) evenly into two blocks, Gj1(vk) and
Gj2(vk). Then, we rebuild the data structures R, F
and H for the two new blocks. Note that there are
inter-node pointers to Gj(vk) that now could become
dangling pointers, but all those can be known from
Fj(vk), since pointers are bidirectional, and updated to
point to the right places in Gj1(vk) or Gj2(vk). Finally,
if vk is the root or a leaf, then K(vk) is updated.

The total cost of splitting a block is dominated by
that of building the new data structures R, F and H.
These are easily built in O(lg3 n) time. Since we split
a block Gj(v) at most once per sequence of Θ(lg3 n)
insertions in Gj(v), the amortized cost incurred by
splitting a block is O(1). Therefore the total cost of an
insertion in L(v) is O(1). The insertion of a new symbol

868

Copyright © SIAM.
Unauthorized reproduction of this article is prohibited.

Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpleads to O(lgρ σ) insertions into lists L(v). Updates of
data structures K(vr) and K(va) take O(lg n/ lg lg n)
time. Hence, the total cost of an insertion is O(lgρ σ +
lg n/ lg lg n) = O((lg σ + lg n)/ lg lg n).

We describe how deletions are handled in Section 4,

where we also describe the data structure P (v).

3.5 Space We show in Appendix A how to manage 
the data in blocks Gj(v) so that all the elements
stored in lists L(v) use n lg σ bits. Since there are
O(n lg σ/ lg3 n + σ) blocks overall, all the pointers between 
blocks of the same lists add up to O(n lg σ/ lg2 n+
σ lg n) bits. All the data structures K(v) add up to
O(n/ lg2 n) bits. We showed before that the number of
inter-node pointers is O(n lg σ/ lg3−ε n + σ lgε n), hence
all inter-node pointers (i.e., Fj and Hj structures) use
O(n lg σ/ lg2−ε n+σ lg1+ε n) bits. Structures Pt(v) (Section 
4) use O(n lg σ/ lg2−ε n) bits as they have ρ inin 
Appendix A we show
tegers per block. Finally,
that each structure Rj(v) uses O(|Gj(v)|/ lg1−ε n) extra 
bits. Hence, all Rj(v)s for all blocks and nodes
use O(n lg σ/ lg1−ε n) bits. Thus the overall space is
n lg σ + O(n lg σ/ lg1−ε n + σ lg1+ε n) bits.
Finally, note that our structures depend on the
value of lg n, so they should be rebuilt when (cid:100)lg n(cid:101)
changes. M¨akinen and Navarro [27] describe a way to
handle this problem without aﬀecting the space nor the
time complexities, even in the worst-case scenario. The
result is completed in the next section, where we describe 
the changes needed to implement the predecessor
structures Pt.

4 Lazy Deletions and Data Structure P (u)

The main idea of our solution is based on lazy deletions:
we do not maintain exactly S but a supersequence S
of it. When a symbol S[i] = a is deleted from S, we
retain it in S but take a notice that S[i] = a is deleted.
When the number of deleted symbols exceeds a certain
threshold, we expunge from the data structure all the
elements marked as deleted. We deﬁne B(v) and the
list L(v) for the sequence S in the same way as B(v)
and L(v) are deﬁned for S.

Since elements of L(v) are never removed, we can
implement P (v) as an insertion-only data structure.
For any t, 1 ≤ t ≤ ρ, we store information about
all the blocks of a node v in a data structure Pt(v).
Pt(v) contains one element for each block Gj(v) and is
implemented as an incremental split-ﬁnd data structure
that supports insertions and splitting in O(1) amortized
time and queries in O(1) worst-case time [25]. The
splitting positions in Pt(v) are the blocks Gj(v) that
contain an occurrence of t, so the operation “ﬁnd” in
Pt(v) allows us to locate, for any Gj(v), the last block

preceding Gj(v) that contains an occurrence of t.

The insertion of a symbol t in L(v) may induce a
new split in Pt(v). Furthermore, overﬂows in a block
Gj(v), which convert it into two blocks Gj1 (v) and
Gj2 (v), induce insertions in Pt(v). Note that an overﬂow
in Gj(v) triggers ρ insertions in the Pt(v) structures,
but this O(ρ) time amortizes to o(1) because insertions
occur every Θ(lg3 n) operations.

Structures Pt(v) do not support “unsplitting” nor
removals. The replacement of Gj(v) by Gj1 (v) and
Gj2 (v) is implemented as leaving in Pt(v) the element
corresponding to Gj(v) and inserting one corresponding
to either Gj1(v) or Gj2(v). If Gj(v) contained t, then at
least one of Gj1(v) and Gj2(v) contain t, and the other
can be inserted as a new element (plus possibly a split,
if it also contains t).

We need some additional data structures to support
lazy deletions. A data structure K(v) stores the number
of non-deleted elements in each block of L(v) and
supports partial-sum queries. We will maintain K(v)
in the root of the wavelet tree and in all leaf nodes.
Moreover, we maintain a data structure Dj(v) for every
block Gj(v), where v is either the root or a leaf node.
Dj(v) can be used to count the number of deleted and
non-deleted elements before the i-th element in a block
Gj(v) for any query index i, as well as to ﬁnd the
index in Gj(v) of the i-th non-deleted element. The
implementation of Dj(v) is described in Appendix A.
We can use K(v) and Dj(v) to ﬁnd the index i in L(v)
where the i-th non-deleted element occurs, and to count
the number of non-deleted elements that occur before
the index i in L(v).

We also store a global list DEL that contains, in
any order, all the deleted symbols that have not yet
been expunged from the wavelet tree. For any symbol
S[i] in the list DEL we store a pointer to the oﬀset e
in L(vr) that encodes S[i]. Pointers in list DEL are
implemented in the same way as inter-node pointers.

4.1 Queries Queries are answered very similarly to
Section 3. The main idea is that we can essentially
ignore deleted elements except at the root and at the
leaves.

access(S, i): Exactly as in Section 3, except that e0
encodes the i-th non-deleted element in L(vr), and
is found using K(vr) and Dj(vr).

selecta(S, i): We ﬁnd the position of the oﬀset eh of the
i-th non-deleted element in L(vh), where vh = va,
using K(va). Then we move up in the tree exactly
as in Section 3. When the root node v0 = vr
is reached, we count the number of non-deleted
elements that precede oﬀset e0 using K(vr).

869

Copyright © SIAM.
Unauthorized reproduction of this article is prohibited.

Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpk ≤ ek such that B(vk)[e(cid:48)

ranka(S, i): We ﬁnd the position of the oﬀset e0 of the
i-th non-deleted element in L(vr). Let vk, tk be
deﬁned as in Section 3. In every node vk, we ﬁnd
the last oﬀset e(cid:48)
k] = tk.
Note that this element may be a deleted one, but
it still drives us to the correct position in L(vk+1).
We proceed exactly as in Section 3 until we arrive
at a leaf vh = va. At this point, we count the
number of non-deleted elements that precede oﬀset
eh using K(va) and Dj(va).

4.2 Updates Insertions are carried out just as in
Section 3. The only diﬀerence is that we also update
the data structure Dj(vk) when an element B(vk)[ek]
that encodes the inserted symbol a is added to a block
Gj(vk). When a symbol S[i] = a is deleted, we append
it to the list DEL of deleted symbols. Then we visit
each block Gj(vk) containing the element B(vk)[ek] that
encodes S[i] and update the data structures Dj(vk).
Finally, K(vr) and K(va) are also updated.

When the number of symbols in the list DEL
reaches n/ lg2 n, we perform a cleaning procedure and
get rid of all the deleted elements. Therefore DEL never
requires more than O(n/ lg n) bits.
Let B(vk)[ek], 0 ≤ k ≤ h, be the sequence of elements 
that encode a symbol S[i] ∈ DEL. The method
for tracking the elements B(vk)[ek], removing them from
their blocks Gj(vk), and updating the block structures is
symmetric to the insertion procedure described in Section 
3.
In this case we do not need the predecessor
queries to track the symbol to delete, as the procedure
is similar to that for accessing S[i]. When the size of a
block Gj(vk) falls below (lg3 n)/2 and it is not the last
block of L(vk), we merge it with Gj+1(vk), and then
split the result if its size exceeds 2 lg3 n. This retains
O(1) amortized time per deletion in any node vk, and
O((lg σ+lg n)/ lg lg n) amortized time to delete any S[i].
Once all the pointers in DEL are processed, we rebuild 
from scratch the structures P (v) for all nodes
v.
The total size of all the P (v) structures is
O(ρn lg σ/ lg3 n) elements. Since a data structure for incremental 
split-ﬁnd is constructed in linear time, all the
P (v)s are rebuilt in O(n lg σ/ lg3−ε n) time. Hence the
amortized time to rebuild the P (v)s is O(lg σ/ lg1−ε n),
which does not aﬀect the amortized time O((lg σ +
lg n)/ lg lg n) to carry out the eﬀective deletions.

We are ready to state a ﬁrst version of our result,
not yet compressing and with times depending on σ. In
Appendix A it is seen that the time for the operations
is the constant O(1/ε). Since the height of the wavelet
tree is lgρ σ = O((1/ε) lg σ/ lg lg n), the time for all the
operations on the string S is precisely O(((1/ε2) lg σ +

lg n)/ lg lg n). On the other hand, we have used blocks
of size Θ(lg3 n) as this is the minimum that guarantees
sublinear redundancy, but any larger exponent works as
well. With size Θ(lgc+3 n) we get the following result.

Theorem 4.1. A dynamic string S[1, n] over alphabet 
[1..σ] can be stored in a structure using n lg σ +
O(n lg σ/ lgc n + σ lg1+ε n) bits, for any constants c >
0 and 0 < ε < 1, and supporting queries access,
rank and select in time O(((c/ε2) lg σ + lg n)/ lg lg n).
Insertions and deletions of symbols are supported in
O(((c/ε2) lg σ + lg n)/ lg lg n) amortized time.

5 Compressed Space and Optimal Time

the space of

We now compress
the data structure 
to zero-order entropy (nH0(S) plus redundancy),
while improving the time performance to the optimal
O(lg n/ lg lg n). We use Theorem 4.1 in combination
with alphabet partitioning [3] to obtain the result.
We then consider general alphabets, which is possible
thanks to the fact that alphabet partitioning frees us
from alphabet dependencies via a simple mapping.

5.1 Alphabet Partitioning We use a technique inspired 
by an alphabet partitioning idea [3]. To each
symbol a we will assign a level (cid:96) = (cid:100)lg(n/na)(cid:101), where
a occurs na times in S, so that there are at most lg n
levels. Additionally, we assign level (cid:100)lg n(cid:101) + 1 to the
symbols of Σ not present in S. For each level (cid:96) we will
create a sequence S(cid:96)[1, n(cid:96)] containing the subsequence of
S formed by the symbols of level (cid:96), with their alphabet
remapped to [1..σ(cid:96)], where σ(cid:96) is the number of distinct
symbols of level (cid:96). We will also maintain a sequence of
levels Slev, so that Slev[i] is the level of S[i]. We represent 
Slev and the S(cid:96) strings using Theorem 4.1. A
few arrays handle the mapping between global symbols
of Σ and local symbols in strings S(cid:96): M [1, σ] gives the
level of each symbol, N [1, σ] gives the position of that
symbol inside the local alphabet of its level, and local
arrays M (cid:96)[1, σ(cid:96)] map local to global symbols. All these
are represented as plain arrays. Thus a symbol a ∈ Σ
is represented in string S(cid:96), at level (cid:96) = M [a], where it
is written as symbol a(cid:48) = N [a]. Conversely, a symbol a(cid:48)
in S(cid:96) corresponds to symbol a = M (cid:96)[a(cid:48)] ∈ Σ.

Barbay et al. [3] show how operations access, rank,
and select on S are carried out via a constant number
of operations in Slev and in some S(cid:96). We now extend
them to insertions and deletions. To insert symbol a
at position i in S, we ﬁnd its level (cid:96) = M [a] and its
translation a(cid:48) = N [a] inside S(cid:96). Now we insert (cid:96) at
position i in Slev, and a(cid:48) at position rank(cid:96)(Slev, i) in
S(cid:96). Deletion is similar: after mapping, we delete the
position S(cid:96)[rank(cid:96)(Slev, i)] and then the position Slev[i].

870

Copyright © SIAM.
Unauthorized reproduction of this article is prohibited.

Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpa or na = (cid:98)n(cid:48)

a. When na = 2n(cid:48)

If the symbol a we are inserting did not exist in S, it
will be assigned the last level (cid:96) = (cid:100)lg n(cid:101) + 1 and will not
appear in M (cid:96). In this case we add a at the end of M (cid:96),
M (cid:96)[σ(cid:96) +1] = a, increase σ(cid:96), set N [a] = σ(cid:96) and M [a] = (cid:96).
Then we proceed as in a normal insertion. Instead, if a
deletion removes the last occurrence of a, we use a more
global update mechanism we explain next.
Actually, we maintain levels (cid:96) = (cid:100)lg(n/na)(cid:101) only
approximately. First, since (cid:100)lg n(cid:101) is ﬁxed in our data
structure (see the end of Section 3.5), if we call n(cid:48) =
2(cid:100)lg n(cid:101), it holds (cid:98)n(cid:48)/2(cid:99) < n < n(cid:48), and use level (cid:96) =
(cid:100)lg(n(cid:48)/na)(cid:101) for a. We also keep track of the current
frequency in S of each symbol a ∈ Σ, na, and the
frequency a had when it was assigned its current level,
n(cid:48)
a. We retain the level (cid:96) assigned to a as long as
a/2(cid:99), we
n(cid:48)
a/2 < na < 2n(cid:48)
move a to a new level (cid:96)(cid:48) = (cid:100)lg(n(cid:48)/na)(cid:101) = (cid:96)±1, as follows.
We compute the mapping a(cid:48) = N [a] of a in S(cid:96), change
M [a] to (cid:96)(cid:48), and compute the new mapping a(cid:48)(cid:48) = σ(cid:96)(cid:48) + 1
. Now, for each of the na occurrences of a(cid:48) in
of a in S(cid:96)(cid:48)
S(cid:96), say S(cid:96)[i] = a(cid:48) (found using i = selecta(cid:48)(S(cid:96), 1)), we
compute its position j = select(cid:96)(Slev, i) in Slev, change
Slev[j] to (cid:96)(cid:48), remove symbol S(cid:96)[i], and insert symbol
a(cid:48)(cid:48) in S(cid:96)(cid:48)
at position rank(cid:96)(cid:48)(Slev, j). We also update
[a(cid:48)(cid:48)] = a and N [a] = a(cid:48)(cid:48),
the mappings: we set M (cid:96)(cid:48)
and move the last element of M (cid:96) to occupy the empty
slot left by a: M (cid:96)[a(cid:48)] = M (cid:96)[σ(cid:96)] and N [b] = a(cid:48), where
b = M (cid:96)[σ(cid:96)]. We ﬁnd all the occurrences of σ(cid:96) in S(cid:96)
and replace them by a(cid:48). Finally, we increase σ(cid:96)(cid:48) and
decrease σ(cid:96). When na = 0, we delete it from S(cid:96) instead
of moving it. Finally, we also rebuild each sequence S(cid:96)
periodically: we remember the number of symbols n(cid:48)
(cid:96)
in S(cid:96) at the last time we built it, and rebuild S(cid:96) when
n(cid:96) = (cid:98)n(cid:48)
The number of insertions or deletions that must
occur until we change the level of a is n(cid:48)
a/2 = Θ(na).
Therefore, the process of changing a symbol from one
level to another, which costs O(na) update operations
on Slev, S(cid:96), M (cid:96), M and N , is amortized over Θ(na)
updates. The same occurs with the symbol b mapped to
σ(cid:96) in S(cid:96), whose occurrences have to be re-encoded as a(cid:48):
Since (cid:100)lg(n(cid:48)/n(cid:48)
a)(cid:101), it holds nb = Θ(na).
The rebuilds of S(cid:96) and S amortize in the same way.

(cid:96)/2(cid:99) or n(cid:96) = 2n(cid:48)
(cid:96).

b)(cid:101) = (cid:100)lg(n(cid:48)/n(cid:48)

Note that we are letting the alphabet of the sequences 
S(cid:96) grow and shrink, which our wavelet trees do
not support. Rather, we create them with the maximum
possible alphabet size σ(cid:96) ≥ σ(cid:96). Since (cid:96) = (cid:100)lg(n(cid:48)/n(cid:48)
a)(cid:101) =
(cid:100)lg(n(cid:48)/n(cid:48)
b)(cid:101) for any pair of symbols a, b mapped to S(cid:96),
it follows that n(cid:48)
a/2. Since we retain that level
(cid:96) for them as long as n(cid:48)
it follows
that nb > n(cid:48)
a/4, and thus there cannot be more than
4n(cid:96)/n(cid:48)
a distinct symbols in S(cid:96). Since, on the other hand,
n(cid:96) < 2n(cid:48)
(cid:96), we can safely set the maximum alphabet size

b/2 < nb < 2n(cid:48)
b,

b > n(cid:48)

(cid:96)/n(cid:48)

a ≥ n(cid:48)/2(cid:96), thus we set σ(cid:96) = 2(cid:96)+3n(cid:48)

for S(cid:96) to σ(cid:96) = 8n(cid:48)
a for any a. A bound in terms of (cid:96)
is n(cid:48)
(cid:96)/n(cid:48). Note it holds
σ(cid:96) = O(n(cid:96)/na) for any a mapped to S(cid:96). Note also that
the eﬀective alphabet (i.e., symbols actually occurring)
of S(cid:96) is of size σ(cid:96) ≥ n(cid:96)/(4n(cid:48)

a) = σ(cid:96)/64.

a) ≥ n(cid:48)

(cid:96)/(8n(cid:48)

5.2 Time and Space The queries on Slev take
O(lg n/ lg lg n) time, because its alphabet is of size
O(lg n). Queries on S(cid:96) take O((lg σ(cid:96) + lg n)/ lg lg n) =
O(lg n/ lg lg n) time, since σ(cid:96) = O(n). The accesses to
M , N , and M (cid:96) are constant-time. Therefore, we reach
the optimal worst-case time O(lg n/ lg lg n) for the three
queries. Likewise, updates cost O(lg n/ lg lg n) amortized 
time.
Let us now consider the space. Each symbol a with
frequency na will be stored at a level (cid:96) = (cid:100)lg(n/na)(cid:101)±2,
in a sequence over an alphabet of size σ(cid:96). Therefore,
we will spend na lg σ(cid:96) + O(na lg σ(cid:96)/ lg2 n) bits for it,
according to Theorem 4.1 (we use lg n instead of lg na
to deﬁne superblock sizes; we consider soon the rest of
the space overhead). This is na lg(n(cid:96)/na) + O(na) bits,

O(na) bits. Now consider the occurrences of symbol (cid:96)
in Slev, which we will also charge to S(cid:96). These cost
n(cid:96) lg(n/n(cid:96)) + O(n(cid:96) lg lg n/ lg2 n) = n(cid:96) lg(n/n(cid:96)) + o(n(cid:96)).
Added to the space spent at S(cid:96) itself, and since the sum

which added over the whole S(cid:96) yields(cid:80) na lg(n(cid:96)/na) +
of the na’s is n(cid:96), we obtain(cid:80) na lg(n/na) + O(n(cid:96)) bits.
σ(cid:96) = Θ(σ(cid:96)), and(cid:80)

Theorem 4.1 also involves a cost of O(σ(cid:96) lg1+ε n)
bits per level (cid:96), which add up to O(σ lg1+ε n) since

Now, adding over the symbols a of all the levels, we
obtain the total space nH0(S) + O(n).

(cid:96) σ(cid:96) = σ.

In addition we spend O(σ(lg lg n+lg σ)) bits for the
arrays M , N and M (cid:96). Finally, recall that we also spend
space in storing deleted symbols, but these are at most
O(n/ lg2 n), and thus they cannot increase the entropy
by more than O(n/ lg n). This gives the ﬁnal result.

Theorem 5.1. A dynamic string S[1, n] over alphabet
[1..σ] can be stored in a structure using nH0(S) + O(n +
σ(lg σ + lg1+ε n)) bits, for any constant 0 < ε < 1,
and supporting queries access, rank and select in optimal 
time O((1/ε2) lg n/ lg lg n). Insertions and deletions 
of symbols are supported in O((1/ε2) lg n/ lg lg n)
amortized time.

5.3 Handling General Alphabets Our time results 
do not depend on the alphabet size σ, yet our space
does, in a way that ensures that σ gives no problems as
long as σ = O(n/ lg1+ε n) for some constant ε > 0.

Let us now consider the case where the alphabet Σ
is much larger than the eﬀective alphabet of the string,
that is, the set of symbols that actually appear in S at
a given point in time. Let us now use σ ≤ n to denote

871

Copyright © SIAM.
Unauthorized reproduction of this article is prohibited.

Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpthe eﬀective alphabet size. Our aim is to maintain the
space within nH0(S) + O(n + σ lg1+ε n) bits, even when
the symbols come from a large universe Σ = [1..|Σ|], or
even from a general ordered universe such as Σ = R or
Σ = Γ∗ (i.e., Σ are words over another alphabet Γ).

Our arrangement into strings S(cid:96) gives a simple
way to handle a sequence over an unbounded ordered
alphabet. By changing tables M and N to custom
structures to search Σ, and storing elements of Σ in
arrays M (cid:96), we obtain the following result.

Theorem 5.2. A dynamic string S[1, n] over a general
alphabet Σ can be stored in a structure using nH0(S) +
O(n+S(σ)+σ lg1+ε n) bits, for any constant 0 < ε < 1,
and supporting queries access, rank and select in time
O(T (σ) + (1/ε2) lg n/ lg lg n). Insertions and deletions
of symbols are supported in O(U(σ)+(1/ε2) lg n/ lg lg n)
amortized time. Here σ is the number of distinct
symbols of Σ occurring in S, S(σ) is the number of
bits used by a dynamic data structure to search over
σ elements in Σ plus to refer to σ elements in Σ, T (σ)
is the worst-case time to search for an element among
σ of them in Σ, and U(σ) is the amortized time to
insert/delete symbols of Σ in the structure.

times, which is optimal in the comparison model.

For example, if Σ = R we have O(lg σ + lg n/ lg lg n)
An interesting particular case is Σ = Γ∗ on a
general alphabet Γ, where we can store the eﬀective
set of strings in a data structure by Franceschini and
Grossi [16], so that operations involving a string p take
O(|p| + lg γ + lg n/ lg lg n), where γ is the number of
symbols of Γ actually in use.
Another particular case is that Σ is an integer
range [1..|Σ|], then time can be reduced to O(lg lg |Σ| +
lg n/ lg lg n) and the space increases by O(σ lg |Σ|) bits,
by using y-fast tries [36].

Yet another important particular case is when we
maintain a contiguous eﬀective alphabet [1..σ], and only
insert new symbols σ+1. In this case there is no penalty
for letting the alphabet grow dynamically.

Theorem 6.1. There exists a data structure for handling 
a collection C of texts over an alphabet [1, σ] within
size nHh(C) + O(n + σh+1 lg n + m lg n) bits, simultaneously 
for all h. Here n is the length of the concatenation 
of m texts, C = T1 ◦ T2 ··· ◦ Tm, and we assume 
that the alphabet size is σ = o(n). The structure
supports counting of the occurrences of a pattern P in
O(|P| lg n/ lg lg n) time. After counting, any occurrence
can be located in time O(lg2 n/ lg lg n). Any substring
of length (cid:96) from any T in the collection can be displayed
in time O(((cid:96) + lg n) lg n/ lg lg n)). Inserting or deleting
a text T takes O(lg n + |T| lg n/ lg lg n) amortized time.
For 0 ≤ h ≤ (α lgσ n) − 1, for any constant 0 < α < 1,
the space simpliﬁes to nHh(C) + O(n + m lg n) bits.

The theorem refers to Hh(C), the h-th order empirical 
entropy of sequence C [28]. This is a lower bound to
any semistatic statistical compressor that encodes each
symbol as a function of the h preceding symbols in the
sequence, and it holds Hh(C) ≤ Hh−1(C) ≤ H0(C) ≤
lg σ for any h > 0. To oﬀer search capabilities, the
Burrows-Wheeler Transform (BWT) [11] of C, Cbwt, is
represented, not C. K¨arkk¨ainen and Puglisi [26] showed
that, if Cbwt is split into superblocks of size Θ(σ lg2 n),
and a zero-order compressed representation is used for
each superblock, the total bits are nHh(C) + o(n).

We use their partitioning, and Theorem 5.1 to represent 
each superblock. The superblock sizes are easily
maintained upon insertions and deletions of symbols,
by splitting and merging superblocks and rebuilding
the structures involved, without aﬀecting the amortized
time per operation. They also need to manage a table 
storing the rank of each symbol up to the beginning
of each superblock. This is arranged, in the dynamic
scenario, with σ partial sum data structures containing
O(n/(σ lg2 n)) elements each, plus another one storing
the superblock lengths. This adds O(n/ lg n) bits and
O(lg n/ lg lg n) time per operation.

Finally, the locating and displaying overheads are
obtained by marking one element out of lg n, so that
the space overhead of O(n) is maintained.

6 Applications

Our new results impact in a number of applications that
build on dynamic sequences. We describe several here.

6.1 Dynamic Sequence Collections The standard
application of dynamic sequences, stressed out in several
previous papers [12, 27, 19, 30],
is to maintain a
collection C of texts, where one can carry out indexed
pattern matching, as well as inserting and deleting texts
from the collection. Plugging in our new representation
we can improve the time and space of previous work
(yet our update time is amortized).

6.2 Burrows-Wheeler Transform Another application 
of dynamic sequences is to build the BWT of
a text T , T bwt, within compressed space, by starting
from an empty sequence and inserting each new character,
 T [n], T [n − 1], . . ., T [1], at the proper positions.
The result is stated as the compressed construction of a
static FM-index [15], a compressed index that consists
essentially of a (static) wavelet tree of T bwt. Our new
representation improves upon the best previous result
on compressed space [30].

Theorem 6.2. The Alphabet-Friendly FM-index [15],

872

Copyright © SIAM.
Unauthorized reproduction of this article is prohibited.

Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpas well as the BWT [11], of a text T [1, n] over an
alphabet of size σ, can be built using nHh(T ) + O(n)
bits, simultaneously for all 1 ≤ h ≤ (α lgσ n) − 1 and
any constant 0 < α < 1, in time O(n lg n/ lg lg n). It
can also be built within the same time and nH0(T ) +
O(n + σ(lg σ + lg1+ε n)) bits, for any constant ε > 0 and
any alphabet size σ.

We are using Theorem 6.1 for the case h > 0, and
Theorem 5.1 to obtain a less alphabet-restrictive result
for h = 0. This is the ﬁrst time o(n lg n) time is obtained
within compressed space. Other space-conscious results
that achieve better time complexity (but more space)
are Okanohara and Sadakane [32], who achieved optimal
O(n) time within O(n lg σ lg lgσ n) bits, and Hon et al.
[24], who achieved O(n lg lg σ) time and O(n lg σ) bits.

6.3 Binary Relations Barbay et al. [4] show how to
represent a binary relation of t pairs relating n “objects”
with σ “labels” by means of a string of t symbols over
alphabet [1..σ] plus a bitmap of length t+n. The idea is
to traverse the matrix, say, object-wise, and write down
in a string the labels of the pairs found. Meanwhile
we append a 1 to the bitmap each time we ﬁnd a pair
and a 0 each time we move to the next object. Then
queries like: ﬁnd the objects related to a label, ﬁnd the
labels related to an object, and tell whether an object
and a label are related, are answered via access, rank
and select operations on the string and the bitmap.

A limitation in the past to make this representation
dynamic was that creating or removing labels implied
changing the alphabet of the string. Now we can use
Theorem 5.1 and the results of Section 5.3 to obtain
a fully dynamic representation. We illustrate the case
where labels and objects come from ﬁnite universes.

Theorem 6.3. A dynamic binary relation consisting
of t pairs relating n objects from [1..N ] with σ labels
from [1..L] can support the operations of counting and
listing the objects related to a given label, counting and
listing the labels related to a given object, and telling
whether an object and a label are related, all in time
O(lg lg(N L)+lg(n+t)/ lg lg(n+t)) per delivered datum.
Pairs, objects and labels can also be added and deleted
in amortized time O(lg lg(N L) + lg(n + t)/ lg lg(n + t)).
The space required is tH + n lg N + σ lg L + O(t + n +
σ(lg σ + lg1+ε t)) bits, where ε > 0 is any constant
1≤i≤σ(ti/t) lg(t/ti) ≤ lg σ, where ti is the
number of objects related to label i. Only labels and
objects with no related pairs can be deleted.

and H = (cid:80)

6.4 Directed Graphs A particularly interesting and
general binary relation is a directed graph with n nodes
and e edges. Our binary relation representation allows

one to navigate it in forward and backward direction,
and modify it, within little space.

Theorem 6.4. A dynamic directed graph consisting of
n nodes in [1..N ] and e edges can support the operations
of counting and listing the neighbors pointed from a
node, counting and listing the reverse neighbors pointing
to a node, and telling whether there is a link from
one node to another, all in time O(lg lg N + lg(n +
e)/ lg lg(n + e)) per delivered datum. Nodes and edges
can be added and deleted in amortized time O(lg lg N +
lg(n + e)/ lg lg(n + e)). The space required is eH +
n lg N + O(e + n(lg n + lg1+ε e)) bits, where ε is any
1≤i≤n(ei/e) lg(e/ei) ≤ lg n, where

constant and H =(cid:80)

ei is the outdegree of node i.

If we only modify edges and the nodes are ﬁxed, the
overheades related to N disappear. Note also that we
can change “outdegree” by “indegree” in the theorem
by representing the transposed graph, as operations are
symmetric. We can similarly transpose general binary
relations.

6.5 Inverted Indexes Finally, we consider an application 
where the symbols are words. Take a text T as
a sequence of n words, which are strings over a set of
letters Γ. The alphabet of T is Σ = Γ∗, and its eﬀective 
alphabet is called the vocabulary V of T , of size
|V | = σ. A positional inverted index is a data structure
that, given a word w ∈ V , tells the positions in T where
w appears [1].

A well known way to simulate a positional inverted
index within no extra space on top of the compressed
text is to use a compressed sequence representation for
T (over alphabet Σ), so that operation selectw(T, i) simulates 
access to the ith position of the list of word
w, whereas access to the original T is provided via
access(T, i). Operation rank can be used to emulate various 
inverted index algorithms, particularly for intersections 
[6]. The space is the zero-order entropy of the text
seen as a sequence of words, which is very competitive
in practice. Our new technique permits modifying the
underlying text, that is, it simulates a dynamic inverted
index. For this sake we use the technique of Section 5.3
and tries to handle a vocabulary over a ﬁxed alphabet.

Theorem 6.5. A text of n words with a vocabulary of
σ words and total length ν over a ﬁxed alphabet can be
represented within nH0(T )+O(n+ν lg n+σ lg1+ε n) bits
of space, where ε > 0 is an arbitrary constant and H0(T )
is the word-wise entropy of T . The representation
outputs any word T [i] = w given i, ﬁnds the position
of the ith occurrence of any word w, and tells the
number of occurrences of any word w up to position

873

Copyright © SIAM.
Unauthorized reproduction of this article is prohibited.

Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpi, all in time O(|w| + lg n/ lg lg n). A word w can be
inserted or deleted at any position in T in amortized
time O(|w| + lg n/ lg lg n).

Another kind of inverted index, a non-positional
one, relates each word with the documents where it
appears (not to the exact positions). This can be seen as
a direct application of our binary relation representation
[2], and our dynamization theorems apply to it as well.

7 Conclusions and Further Challenges

We have obtained O(lg n/ lg lg n) time for all the operations 
that handle a dynamic sequence on an arbitrary 
(known) alphabet [1..σ], matching lower bounds
that apply to binary alphabets [17]. Our structure
is faster than previous work [22, 30] by a factor of
Θ(1 + lg σ/ lg lg n).
It also reduces the redundancy
space, using nH0(S) + O(n + σ(lg σ + lg1+ε n)) bits, instead 
of nH0(S)+o(n lg σ)+O(σ(lg σ+lg n)) of previous
work. We also show how to handle general alphabets.
Our result can be applied to a number of problems; we
have described several ones.

The only remaining advantage of previous work
[22, 30]
is that their update times are worst-case,
whereas in our structure they are amortized. Obtaining
optimal worst-case time complexity for updates is an
interesting future challenge.

Another challenge is to simulate other operations
than access, rank and select. Obtaining the full functionality 
of wavelet trees with better time than the current 
dynamic ones [22, 30] is unlikely, as discussed in
the Introduction. Yet, there may be some intermediate
functionality of interest.

References

[1] R. Baeza-Yates and B. Ribeiro. Modern Information

Retrieval. Addison-Wesley, 2nd edition, 2011.

[2] J. Barbay, F. Claude, and G. Navarro. Compact richfunctional 
binary relation representations. In Proc. 9th
LATIN, LNCS 6034, pages 170–183, 2010.

[3] J. Barbay, T. Gagie, G. Navarro, and Y. Nekrich.
Alphabet partitioning for compressed rank/select and
applications.
In Proc. 21st ISAAC, pages 315–326
(part II), 2010.

[4] J. Barbay, A. Golynski, I. Munro, and S. S. Rao. Adaptive 
searching in succinctly encoded binary relations
and tree-structured documents. Theor. Comp. Sci.,
387(3):284–297, 2007.

[5] J. Barbay, M. He, I. Munro, and S. S. Rao. Succinct
indexes for strings, binary relations and multi-labeled
trees. ACM Trans. Alg., 7(4):article 52, 2011.

[6] J. Barbay and G. Navarro. Compressed representations 
of permutations, and applications. In Proc. 26th
STACS, pages 111–122, 2009.

[7] D. Belazzougui and G. Navarro. New lower and upper
bounds for representing sequences. In Proc. 20th ESA,
LNCS 7501, pages 181–192, 2012.

[8] G. E. Blelloch.

Space-eﬃcient dynamic orthogonal
point location, segment intersection, and range reporting.
 In Proc. 19th SODA, pages 894–903, 2008.

[9] P. Bose, M. He, A. Maheshwari, and P. Morin. Succinct
orthogonal range search structures on a grid with
applications to text indexing.
In Proc. 11th WADS,
pages 98–109, 2009.

[10] N. Brisaboa, A. Fari˜na, S. Ladra, and G. Navarro.
In Proc. 31st SIGIR,

Reorganizing compressed text.
pages 139–146, 2008.

[11] M. Burrows and D. Wheeler. A block sorting lossless
data compression algorithm. Technical Report 124,
Digital Equipment Corporation, 1994.

[12] H. Chan, W. Hon, T. Lam, and K. Sadakane. Compressed 
indexes for dynamic text collections. ACM
Trans. Alg., 3(2):21, 2007.

[13] F. Claude and G. Navarro. Extended compact Web
graph representations. In Algorithms and Applications
(Ukkonen Festschrift), pages 77–91. Springer, 2010.

[14] P. Ferragina, F. Luccio, G. Manzini, and S. Muthukrishnan.
 Compressing and indexing labeled trees, with
applications. J. ACM, 57(1), 2009.

[15] P. Ferragina, G. Manzini, V. M¨akinen, and G. Navarro.
Compressed representations of sequences and full-text
indexes. ACM Trans. Alg., 3(2):article 20, 2007.

[16] G. Franceschini and R. Grossi. A general technique for
managing strings in comparison-driven data structures.
In Proc. 31st ICALP, LNCS 3142, pages 606–617, 2004.
[17] M. Fredman and M. Saks. The cell probe complexity
of dynamic data structures. In Proc. 21st STOC, pages
345–354, 1989.

[18] A. Golynski, J. I. Munro, and S. S. Rao. Rank/select
operations on large alphabets: a tool for text indexing.
In Proc. 17th SODA, pages 368–373, 2006.

[19] R. Gonz´alez and G. Navarro. Rank/select on dynamic
compressed sequences and applications. Theor. Comp.
Sci., 410:4414–4422, 2009.

[20] R. Grossi, A. Gupta, and J. Vitter. High-order
entropy-compressed text indexes. In Proc. 14th SODA,
pages 841–850, 2003.

[21] A. Gupta, W.-K. Hon, R. Shah, and J. Vitter. Dynamic 
rank/select dictionaries with applications to
XML indexing. Tech.Rep. CSD TR #06-014, Purdue
Univ., 2006.

[22] M. He and I. Munro.

Succinct representations of
dynamic strings. In Proc. 17th SPIRE, pages 334–346,
2010.

[23] W.-K. Hon, K. Sadakane, and W.-K. Sung. Succinct
data structures for searchable partial sums. In Proc.
14th ISAAC, pages 505–516, 2003.

[24] W. K. Hon, K. Sadakane, and W. K. Sung. Breaking
a Time-and-Space Barrier in Constructing Full-Text
Indices. SIAM J. Comp., 38(6):2162–2178, 2009.

[25] H. Imai and T. Asano. Dynamic segment intersection
search with applications. In Proc. 25th FOCS, pages

874

Copyright © SIAM.
Unauthorized reproduction of this article is prohibited.

Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php393–402, 1984.

[26] J. K¨arkk¨ainen and S. J. Puglisi. Fixed block compresIn 
Proc. 18th SPIRE,

sion boosting in FM-indexes.
LNCS 7024, pages 174–184, 2011.

[27] V. M¨akinen and G. Navarro. Dynamic entropycompressed 
sequences and full-text indexes. ACM
Tran. Alg., 4(3):article 32, 2008.

[28] G. Manzini. An analysis of the Burrows-Wheeler

transform. J. ACM, 48(3):407–430, 2001.

[29] J. I. Munro. An implicit data structure supporting
insertion, deletion, and search in O(log n) time. J.
Comp. Sys. Sci., 33(1):66–74, 1986.

[30] G. Navarro and K. Sadakane. Fully-functional static
and dynamic succinct trees. CoRR, abs/0905.0768v5,
2010. To appear in ACM Trans. Alg.

[31] Y. Nekrich. A dynamic stabbing-max data structure 
with sub-logarithmic query time. In Proc. 22nd
ISAAC, pages 170–179, 2011.

[32] D. Okanohara and K. Sadakane.

A linear-time
Burrows-Wheeler transform using induced sorting. In
Proc. 16th SPIRE, LNCS 5721, pages 90–101, 2009.

[33] M. Patrascu. Lower bounds for 2-dimensional range

counting. In Proc. 39th STOC, pages 40–46, 2007.

[34] R. Raman and S. S. Rao. Succinct dynamic dictionarIn 
Proc. 30th ICALP, pages 357–368,

ies and trees.
2003.

[35] N. V¨alim¨aki and V. M¨akinen. Space-eﬃcient algoIn 
Proc. 18th CPM,

rithms for document retrieval.
pages 205–215, 2007.

[36] D. Willard. Log-logarithmic worst-case range queries
are possible in space θ(n). Inf. Proc. Lett., 17(2):81–84,
1983.

A Data Structures for Handling Blocks
We describe the way the data is stored in blocks Gj(v),
as well as the way the various data structures inside
blocks operate. All the data structures are based on the
same idea: We maintain a tree with node degree lgδ n
and leaves that contain o(lg n) elements. Since elements
within a block can be addressed with O(lg lg n) bits,
each internal node and each leaf ﬁts into one machine
word. Moreover, we can support searching and basic
operations in each node in constant time.

A.1 Data Organization The block data is physically 
stored as a sequence of miniblocks of Θ(lgρ n)
symbols. Thus there are O(lg2 n lg ρ) = O(lg2 n lg lg n)
miniblocks in a block. These miniblocks will be the
leaves of a τ -ary tree T , for τ = Θ(lgδ n) and some
constant 0 < δ < 1. The height of this tree is constant,
 O(1/δ). Each node of T stores τ counters telling
the number of symbols stored at the leaves that descend
from each child. This requires just O(τ lg lg n) = o(lg n)
bits. To access any position of Gj(v), we descend in T ,
using the counters to determine the correct child. When

we arrive at a leaf, we know the local oﬀset of the desired 
symbol within the leaf, and can access it directly.
Since the counters ﬁt in less than a machine word, a
small universal table gives the correct child in constant
time, therefore we have O(1) time access to any symbol
(actually to any Θ(lgρ n) consecutive symbols).

Upon insertions or deletions, we arrive at the correct 
leaf, insert or delete the symbol (in constant time
because the leaf contains Θ(lg n) bits overall), and update 
the counters in the path from the root (in constant
time as they have o(lg n) bits). The leaves may have lg n
to 2 lg n bits. Splits/merges upon overﬂows/underﬂows
are handled as usual, and can be solved in a constant
number of O(1)-time operations (T operates as a B-tree;
internal nodes may have τ to 2τ children).
The space overhead due to the nodes of T is
O(|Gj(v)| lgδ n lg lg n/ lg n) bits, where we also measure
|Gj(v)| in bits, not symbols. We consider now the space
used by the data itself.

In order not to waste space, the miniblock leaves
are stored using a memory management technique by
Munro [29]. For our case, it allows us to allocate, free,
and access miniblocks of length lg n to 2 lg n in constant
time.
Its space waste, given that our pointers are of
O(lg lg n) bits,
is O(lg lg n) per allocated miniblock,
which adds up to O(|Gj(v)| lg lg n/ lg n), plus a global
redundancy of O(lg2 n) bits. We use one structure per
block, handling its miniblocks, so the global redundancy
adds just O(n lgρ σ/ lg n) bits overall.

Each structure uses a memory area of ﬁxed-size
cells (inside which the variable-length miniblocks are
stored) that grows or shrinks at the end as miniblocks
are created or destroyed. A structure giving that
functionality is called an extendible array (EA) [34]. We
need to handle a set of O(n lgρ σ/ lg3 n) EAs, what is
called a collection of extendible arrays. Its functionality
includes accessing any cell of any EA, letting it grow
or shrink by one cell, and create and destroy EAs.
The following lemma, simpliﬁed from the original [34,
Lemma 1], and using words of lg n bits, is useful.

Lemma A.1. A collection of a EAs of total size s bits
sa lg n) bits of
can be represented using s + O(a lg n +
space, so that the operations of creation of an empty
EA and access take constant worst-case time, whereas
grow/shrink take constant amortized time. An EA of s(cid:48)
bits can be destroyed in time O(s(cid:48)/ lg n).

√

In our case a = O(n lgρ σ/ lg3 n) and s =
O(n lg σ), so the space overhead posed by the EAs is
O(n lgρ σ/ lg2 n + n lg σ/(lg n
lg lg n)) = o(n lg σ/ lg n).

√

A.2 Structure Rj(v) To support rank and select we
enrich T with further information per node. We store ρ

875

Copyright © SIAM.
Unauthorized reproduction of this article is prohibited.

Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpcounters with the number of occurrences of each symbol
in the subtree of each child. The node size becomes
O(τ ρ lg lg n) = O(lgε+δ n lg lg n) = o(lg n) as long as
ε + δ < 1. This dominates the total space overhead,
which becomes O(|Gj(v)| lgε+δ n lg lg n/ lg n).

With this information on the nodes we can easily
solve rank and select in constant time, by descending on
T and determining the correct child (and accumulating
data on the leftward children) in O(1) time using
universal tables. Nodes can also be updated in constant
time even upon splits and merges, since all the counters
can be recomputed in O(1) time.

A.3 Structure Fj(v) This structure stores all the
inter-node pointers leaving from block Gj(v), to its
parent and to any of the ρ children of node v.

The structure is a tree Tf very similar in spirit
to T . The pointers stored are inter-node, and thus
require Θ(lg n) bits. Thus we store a constant number
of pointers per leaf. For each pointer we store the
position in Gj(v) holding the pointer (relative to the
starting position of the leaf node inside Gj(v)) and the
target position. The internal nodes, of arity τ , maintain
information on the number of positions of Gj(v) covered
by each child, and the number of pointers of each kind
(1 + ρ counters) stored in the subtree of each child.
This requires O(τ ρ lg lg n) = o(lg n) bits, as before. To
ﬁnd the last position before i holding a pointer of a
certain kind (parent or t-th wavelet tree child, for any
1 ≤ t ≤ ρ), we traverse Tf from the root looking for
position i. At each node u, it might be that the child
u(cid:48) where we have to enter holds pointers of that kind,
or not. If it does, then we ﬁrst enter into child u(cid:48). If we
return with an answer, we recursively return it. If we
return with no answer, or there are no pointers of the
desired kind below u(cid:48), we enter into the last sibling to
the left of u(cid:48) that holds a pointer of the desired kind, and
switch to a diﬀerent mode where we simply go down the
tree looking for the rightmost child with a pointer of the
desired kind. It is not hard to see that this procedure
visits O(1/δ) nodes, and thus it is constant-time because
all the computations inside nodes can be done in O(1)
time with universal tables. When we arrive at the leaf,
there may be at most two pointers associated to the
desired position (one to the parent and another to a
wavelet tree child), so we can scan for the desired pointer
in constant time.

The tree Tf must be updated when a symbol t is
inserted before any other occurrence of t in Gj(v), when
a symbol is inserted at the ﬁrst position of Gj(v) and,
due to the bidirectionality, when pointers to Gj(v) are
created from the parent or a child of v.
It must be
updated analogously when deletion of pointers occur.

Those updates work just like on the tree T . Tf is also
updated upon insertions and deletions of symbols, even
if they do not have pointers, to maintain the positions
up to date. In this case we traverse Tf looking for the
position of the update, change the oﬀsets stored at the
leaf, and update the subtree sizes stored at the nodes.

A.4 Structure Hj(v) This structure manages the
inter-node pointers that point inside Gj(v). As explained 
in Section 3.4, we give a handle to the outside
nodes, that does not change over time, and Hj(v) translates 
handles to positions in Gj(v).

We store a tree Th that is just like Tf , where the
incoming pointers are stored. Th is simpler, however,
because at each node we only need to store the number
of positions covered by the subtree of each child. Also,
it is possible to traverse Th from a leaf to the root. We
also manage a table T bl so that T bl[h] points to the leaf
where the pointer corresponding to handle h is stored.
At the leaves we store, for each pointer, a backpointer to
T bl and the position in Gj(v) (in relative form). Given a
handle h, we go to the leaf, ﬁnd in constant time the one
pointing back to h, and move upwards up to the root,
adding to the position the number of positions covered
by leftward children of each node. At the end we have
obtained the position in constant time.

When pointers to Gj(v) are created or destroyed,
we insert or remove pointers in Th. We maintain a
list of empty cells in T bl for future handles. We must
also update Th upon symbol insertions and deletions in
Gj(v), to maintain the positions up to date. When a leaf
splits or merges, we update the pointers from a constant
number of positions in T bl, found with the backpointers.
T bl may contain up to Θ(lg3 n) pointers of O(lg lg n)
bits, but there can be only O(n lg σ/ lg3 n) pointers in
the structure, adding up to s = O(n lg σ lg lg n/ lg3 n)
bits, spread across a = O(n lgρ σ/ lg3 n) tables T bl.
Using again Lemma A.1, a collection of EAs poses an
overhead of o(n lg σ/ lg2 n).

A.5 Structure Dj(v) This is a simple tree Td similar
to T , storing at each node the number of positions and
the number of non-deleted positions below each child.
It should be obvious how it operates.

A.6 The Final Result While the raw data adds
up to n lg σ bits, the space overhead adds up to
O(n lg σ lgε+δ n lg lg n/ lg n). By rewriting δ = ε as the
original value of ε/2 and adjusting it inﬁnitesimally, we
have that the overhead is O(n lg σ/ lg1−ε n) bits, for any
0 < ε < 1. The time for the operations is, in all cases,
O(1/δ) = O(1/).

876

Copyright © SIAM.
Unauthorized reproduction of this article is prohibited.

Downloaded 07/22/18 to 200.9.97.193. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php