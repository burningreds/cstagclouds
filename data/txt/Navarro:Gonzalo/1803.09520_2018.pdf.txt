8
1
0
2

 
r
a

 

M
6
2

 
 
]
S
D
.
s
c
[
 
 

1
v
0
2
5
9
0

.

3
0
8
1
:
v
i
X
r
a

Universal Compressed Text Indexing 1

Gonzalo Navarro 2

Center for Biotechnology and Bioengineering (CeBiB),
Department of Computer Science, University of Chile.

gnavarro@dcc.uchile.cl

Nicola Prezza 3

Department of Computer Science, University of Pisa, Italy.

nicola.prezza@di.unipi.it

Abstract

The rise of repetitive datasets has lately generated a lot of interest in compressed
self-indexes based on dictionary compression, a rich and heterogeneous family that
exploits text repetitions in diﬀerent ways. For each such compression scheme, several
diﬀerent indexing solutions have been proposed in the last two decades. To date, the
fastest indexes for repetitive texts are based on the run-length compressed BurrowsWheeler 
transform and on the Compact Directed Acyclic Word Graph. The most
space-eﬃcient indexes, on the other hand, are based on the Lempel-Ziv parsing
and on grammar compression. Indexes for more universal schemes such as collage
systems and macro schemes have not yet been proposed. Very recently, Kempa and
Prezza [STOC 2018] showed that all dictionary compressors can be interpreted as
approximation algorithms for the smallest string attractor, that is, a set of text
positions capturing all distinct substrings. Starting from this observation, in this
paper we develop the ﬁrst universal compressed self-index, that is, the ﬁrst indexing
data structure based on string attractors, which can therefore be built on top of any
dictionary-compressed text representation. Let γ be the size of a string attractor
for a text of length n. From known reductions, γ can be chosen to be equal to
any repetitiveness measure: runs in the Burrows-Wheeler transform, size of the
smallest automaton recognizing all string’s suﬃxes, number of Lempel-Ziv phrases,
number of rules in a grammar or collage system, size of a macro scheme. Our index
takes O(γ lg(n/γ)) words of space and supports locating the occ occurrences of any
pattern of length m in O(m lg n + occ lg n) time, for any constant  > 0. This is,
in particular, the ﬁrst index for general macro schemes and collage systems. Our
result shows that the relation between indexing and compression is much deeper
than what was previously thought: the simple property standing at the core of all
dictionary compressors is suﬃcient to support fast indexed queries.

Key words: Repetitive sequences; Compressed indexes; String attractors

Preprint submitted to Elsevier Preprint

28 March 2018

1

Introduction

Eﬃciently indexing repetitive text collections is becoming of great importance
due to the accelerating rate at which repetitive datasets are being produced in
domains such as biology (where the number of sequenced individual genomes
is increasing at an accelerating pace) and the web (with databases such as
Wikipedia and GitHub being updated daily by thousands of users). A selfindex 
on a string S is a data structure that oﬀers direct access to any substring 
of S (and thus it replaces S), and at the same time supports indexed
queries such as counting and locating pattern occurrences in S. Unfortunately,
classic self-indexes — for example, the FM-index [16] — that work extremely
well on standard datasets fail on repetitive collections in the sense that their
compression rate does not reﬂect the input’s information content. This phenomenon 
can be explained in theory with the fact that entropy compression is
not able to take advantage of repetitions longer than (roughly) the logarithm
of the input’s length [18]. For this reason, research in the last two decades
focused on self-indexes based on dictionary compressors such as the LempelZiv 
1977 factorization (LZ77) [33], the run-length Burrows-Wheeler transform
(RLBWT) [9] and context-free grammars (CFGs) [30], just to name the most
popular ones. The idea underlying these compression techniques is to break
the text into phrases coming from a dictionary (hence the name dictionary
compressors), and to represent each phrase using limited information (typically,
 a pointer to other text locations or to an external set of strings). This
scheme allows taking full advantage of long repetitions; as a result, dictionarycompressed 
self-indexes can be orders of magnitude more space-eﬃcient than
entropy-compressed ones on repetitive datasets.

The landscape of indexes for repetitive collections reﬂects that of dictionary
compression strategies, with speciﬁc indexes developed for each compression
strategy. Yet, a few main techniques stand at the core of most of the indexes.
To date, the fastest indexes are based on the RLBWT and on the Compact Directed 
Acyclic Word Graph (CDAWG) [8,15]. These indexes achieve optimaltime 
queries (i.e., asymptotically equal to those of suﬃx trees [44]) at the
price of higher space consumption. Namely, the former index [21] requires
O(r lg(n/r)) words of space, r being the number of equal-letter runs in the
BWT of S, while the latter [2] uses O(e) words, e being the size of the smallest
automaton that recognizes all the substrings of S. These two measures (espe1 
This work started during Shonan Meeting 126 “Computation over Compressed
Structured Data”.
2 Partially funded by Fondecyt Grant 1-170048 and Basal Funds FB0001, Chile.
3 This work was partially done while the author was holding a post-doc position
at the Technical University of Denmark (DTU). Partially funded by the project
MIUR-SIR CMACBioSeq (“Combinatorial methods for analysis and compression
of biological sequences”) grant n. RBSI146R5L.

2

cially e) have been experimentally conﬁrmed to be not as powerful as others
— such as the size of LZ77 — on repetitive collections [3].

Better measures of repetitiveness include the size z of the LZ77 factorization 
of S, the size g of a CFG (i.e., sum of the lengths of the right-hands
of the rules) generating S, or the size grl of a run-length CFG [38] generating 
S. Indexes using O(z) or O(g) space do exist, but optimal-time queries
have not yet been achieved within this space. Kreft and Navarro [31] introduced 
a self-index based on LZ77 compression, which proved to be extremely
space-eﬃcient on highly repetitive text collections [12]. Their self-index uses
O(z) space and ﬁnds all the occ occurrences of a pattern of length m in time
O(m2h + (m + occ) lg z), where h ≤ z is the maximum number of times a
symbol is successively copied along the LZ77 parsing. A string of length (cid:96)
is extracted in O(h(cid:96)) time. Similarly, self-indexes of size O(g) building on
grammar compression [13,14] can locate all occ occurrences of a pattern in
O(m2 lg lg n + m lg z + occ lg z) time. Within this space, a string of length (cid:96)
can be extracted in time O(lg n + (cid:96)/ lgσ n) [5]. Alternative strategies based on
Block Trees (BTs) [4] appeared recently. A BT on S uses O(z lg(n/z)) space,
which is also the best asymptotic space obtained with grammar compressors
[11,42,23,24,41]. In exchange for using more space than LZ77 compression,
the BT oﬀers fast extraction of substrings: O((1 + (cid:96)/ lgσ n) lg(n/z)) time. A
self-index based on BTs has recently been described by Navarro [36]. Various
indexes based on combinations of LZ77, CFGs, and RLBWTs have also been
proposed [19,20,3,37,6]. Gagie et al. [21] give a more detailed survey.

The above-discussed compression schemes are the easiest to compute or approximate 
(generally, in linear time), but are not the most space-eﬃcient. More
powerful compressors (NP-complete to optimize) include macro schemes [43]
and collage systems [29]. Not much work exists in this direction, and no indexes 
are known for these particular compressors.

1.1 String attractors

As seen in the previous paragraphs, the landscape of self-indexes based on
dictionary compression — as well as that of dictionary compressors themselves
— is extremely fragmented, with several techniques being developed for each
distinct compression strategy. Very recently, Kempa and Prezza [28] gathered
all dictionary compression techniques under a common theory: they showed
that these algorithms are approximations to the smallest string attractor, that
is, a set of text positions “capturing” all distinct substrings of S.

Deﬁnition 1 (String attractor [28]) A string attractor of a string S[1..n]
is a set of γ positions Γ = {j1, . . . , jγ} such that every substring S[i..j] has an

3

occurrence S[i(cid:48)..j(cid:48)] = S[i..j] with jk ∈ [i(cid:48), j(cid:48)], for some jk ∈ Γ.

Their main result are reductions from dictionary compressors to string attractors 
of asymptotically the same size (and the other way round):

Theorem 1 ([28]) Let S be a string, and let α be any of these measures:

(2) the size g of a CFG for S,
(2) the size grl of a run-length CFG for S,
(3) the size c of a collage system for S,
(4) the size z of the LZ77 parse of S,
(5) the size b of a macro scheme for S.
Then, S has a string attractor of size γ = O(α).

Importantly, this implies that any data structure based on string attractors is
universal : given any dictionary-compressed text representation, we can induce
a string attractor and build the data structure on top of it. Indeed, the authors 
exploit this observation and provide the ﬁrst universal data structure for
random access, of size O(γ lg(n/γ)). Their extraction time within this space
is O(lg(n/γ) + (cid:96)/ lgσ n), and it can be made optimal by using slightly more
space. This suggests that compressed computation can be performed independently 
from the compression method used while at the same time matching
the lower bounds of individual compressors (at least for some queries such as
random access).

1.2 Our Contributions

In this paper we exploit the above observation, and describe the ﬁrst universal
self-index based on string attractors, that is, the ﬁrst indexing strategy not
depending on the underlying compression scheme. Since string attractors stand
at the core of the notion of compression, our result shows that the relation
between compression and indexing is much deeper than what was previously
thought: the simple string attractor property introduced in Deﬁnition 1 is
suﬃcient to support indexed pattern searches.

Theorem 2 Let a string S[1..n] have an attractor of size γ. Then, for any
constant  > 0, there exists a data structure of size O(γ lg(n/γ)) that, given
a pattern string P [1..m], outputs all the occ occurrences of P in S in time
O(m lg n+occ lg n). It can be built in O(n lg n) expected time and O(n) space.

We remark that no representation oﬀering random access within o(γ lg(n/γ))
space is known. Our index essentially matches the performance of the fastest
existing self-indexes operating within O(γ lg(n/γ)) space, and is the ﬁrst one

4

for macro schemes and collage systems.

To obtain our results, we adapt the block tree index of Navarro [36], which is
designed for block trees on the LZ77 parse, to operate on string attractors. The
result is also diﬀerent from the block-tree-like structure Kempa and Prezza
use for extraction [28], because that one is aligned with the attractors and
this turns out to be unsuitable for indexing. Instead, we use a block-tree-like
structure, which we dub Γ-tree, which partitions the text in regular form. We
moreover introduce recent techniques [21] to remove the quadratic dependency
on the pattern length in query times.

1.3 Notation

We denote by S[1..n] = S[1] . . . S[n] a string of length n over an alphabet of
size σ = O(n). Substrings of S are denoted S[i..j] = S[i] . . . S[j], and they are
called a preﬁx of S if i = 1 and a suﬃx of S if j = n. The concatenation of
strings S and S(cid:48) is denoted S · S(cid:48). We assume the RAM model of computation
with a computer word of ω = Ω(lg n) bits. By lg we denote the logarithm
function, to the base 2 when this matters. We assume the reader is familiar
with Karp-Rabin ﬁngerprinting [27].

2 Γ-Trees

Given a string S[1..n] over an alphabet [1..σ], with an attractor Γ of size γ, we
deﬁne a Γ-tree on S as follows. At the top level, numbered l = 0, we split S
into γ substrings (which we call blocks) of length b0 = n/γ. Each block is then
recursively split into two, so that if bl is the length of the blocks at level l then
it holds that bl+1 = bl/2, until reaching blocks of one symbol after lg(n/γ)
levels (that is, bl = n/(γ · 2l)). 4 At each level l, every block that is at distance
< bl from a position j ∈ Γ is marked (the distance between j and a block
S[i..i(cid:48)] is i− j if i > j, j − i(cid:48) if i(cid:48) < j, and 0 otherwise). Blocks S[i..i(cid:48)] that are
not marked are replaced by a pointer (cid:104)ptr1, ptr2, δ(cid:105) to an occurrence S[j(cid:48)..j(cid:48)(cid:48)]
of S[i..i(cid:48)] that includes a position j ∈ Γ, j(cid:48) ≤ j ≤ j(cid:48)(cid:48). Such an occurrence
exists by Deﬁnition 1. Moreover, it must be covered by 1 or 2 consecutive
marked blocks of the same level due to our marking mechanism, because all
the positions in S[j(cid:48)..j(cid:48)(cid:48)] are at distance < bl from j. Those 1 or 2 nodes of the
Γ-tree are ptr1 and ptr2, and δ is the oﬀset of j(cid:48) within ptr1 (δ = 0 if j(cid:48) is the
ﬁrst symbol inside ptr1).

4 For simplicity of description, we assume that n/γ is a power of 2.

5

In level l + 1 we explicitly store only the children of the blocks that were
marked in level l. The blocks stored in the Γ-tree (i.e., all blocks at level 0
and those having a marked parent) are called explicit. In the last level, the
marked blocks store their corresponding single symbol from S.

Fig. 1. Example of Γ-tree built on a text of length n = 24 with γ = 3 attractor positions 
(letters colored in black). Marked blocks are colored in gray. Each non-marked
block (in white) is associated with an occurrence (underlined) crossing an attractor
position, and therefore overlapping only marked blocks. Only explicit blocks are
shown.

See Figure 1 for an example of a Γ-tree. We can regard the Γ-tree as a binary
tree (with the ﬁrst lg γ levels chopped out), where the internal nodes are
marked nodes and have two children, and the leaves represent just one symbol.
If we call w the number of leaves, then there are w−γ (marked) internal nodes.
From the leaves, γ of them represent single symbols in the last level, while the
other w − γ leaves are unmarked blocks replaced by pointers. Thus, there are
in total 2w − γ nodes in the tree, of which w − γ are internal nodes, w − γ
are pointers, and γ store explicit symbols. Alternatively, w nodes are marked
(internal nodes plus leaves) and w − γ are unmarked (all leaves).
The Γ-tree then uses O(w) space. To obtain a bound in terms of n and γ, note
that, at each level, each j ∈ Γ may mark up to 3 blocks; therefore there are
w ≤ 3γ lg(n/γ) marked blocks in total, and thus the Γ-tree uses O(γ lg(n/γ))
space.

We now describe two operations on Γ-trees that are fundamental to support
eﬃcient indexed searches. The former is also necessary for a self-index, as
it allows us extracting arbitrary substrings of S eﬃciently. We remind that
this procedure is not the same described on the original structure of string
attractors [28], because the structures are also diﬀerent.

6

b b a b a b b a b a b b a b b a b a b b a b b aa b b ab a b b a b b ab a b b a b b a  b b a b  a b b a b a b b a b b a b a b b a b a b b b a b                          b b b b b a              2.1 Extraction

To extract a single symbol S[i], we ﬁrst it map to a local oﬀset 1 ≤ i(cid:48) ≤ b0 in its
corresponding level-0 block. In general, given the local oﬀset i(cid:48) of a character
in the current block at level l, we ﬁrst see if the current block is marked. If
so, we map i(cid:48) to a position in the next level l + 1, where the current block is
split into two blocks of half the length: if i(cid:48) ≤ bl+1, then we continue on the
left child with the same oﬀset; otherwise we subtract bl+1 from i(cid:48) and continue
on the right child. If, instead, i is not in a marked block, we take the pointer
(cid:104)ptr1, ptr2, δ(cid:105) stored for that block, and add δ to i(cid:48). If the result is i(cid:48) ≤ bl, then
we continue in the node ptr1 with oﬀset i(cid:48); otherwise we continue in ptr2 with
the oﬀset i(cid:48) − bl. In both cases, the new node is marked, so we proceed as on
marked blocks in order to move to the next level in constant time. The total
time to extract a symbol is then O(lg(n/γ)).
A substring of length (cid:96) can thus be extracted in time O((cid:96) lg(n/γ)), which will
be suﬃcient to obtain the search time complexity of Theorem 2. It is possible
to augment Γ-trees to match the complexity obtained by Kempa and Prezza
[28] on string attractors as well, though this would have no impact on our
results.

2.2 Fingerprinting

We now show that the Γ-tree can be augmented to compute the Karp-Rabin
ﬁngerprint of any text substring in logarithmic time.
Lemma 1 We can store a data structure of size O(γ lg(n/γ)) words supporting 
the computation of the Karp-Rabin ﬁngerprint of any substring in
O(lg(n/γ)) time.

Proof. We augment our Γ-tree as follows. At level 0, we store the Karp-Rabin
ﬁngerprint of the text preﬁxes ending at positions i · n/γ, for i = 1, . . . , γ,
together with the values σi·n/γ mod q, i = 1, . . . , γ, where q is the prime used
in the Karp-Rabin hash function. We ﬁrst show that this information allows
us to reduce the problem to that of computing two values: the ﬁngerprint of a
preﬁx and that of a suﬃx of an explicit block. Then, we show how to solve the
sub-problem of computing ﬁngerprints of preﬁxes/suﬃxes of explicit blocks.

We distinguish two main cases: the substring is (a) longer and (b) shorter than
2n/γ characters (i.e., twice the block size b0 at level 0). In case (a), at level 0
the substring must be composed of a suﬃx D of a block, followed by a sequence
Bi . . . Bj of blocks, followed by a preﬁx C of a block (D and/or C could be
empty). The ﬁngerprint of Bi . . . Bj can be computed in constant time using

7

the sampled ﬁngerprints of the text preﬁxes ending at block boundaries, so
the problem reduces to that of computing the ﬁngerprints of D and C. Note
that combining the ﬁngerprints of D, Bi . . . Bj, and C requires computing
σ|Bi...Bj| mod q and σ|C| mod q. The ﬁrst of these two values is sampled.
Since |C| ≤ n/γ, the second value can be computed in O(lg |C|) = O(lg(n/γ))
time using the fast exponentiation algorithm. In case (b), we have two sub-
cases: the substring (b.1) spans exactly two blocks or is the preﬁx/suﬃx of a
block, or (b.2) it is properly contained in a block at level 0. In case (b.1) we
are already done, as the substring can be decomposed into a suﬃx D and a
preﬁx C of an explicit block. In case (b.2), we simply use the Γ-tree structure
to map the substring to lower levels until we are back in case (b.1).

We now show how to compute the ﬁngerprint of a preﬁx of an explicit block
(at any level) in O(lg(n/γ)) time. Computing the ﬁngerprint of a suﬃx of an
explicit block is symmetric, so we omit it. First of all, we store the ﬁngerprints
of all explicit blocks. We distinguish two cases.
(A) We wish to compute the ﬁngerprint of B[1..k], for some k ≤ bl, and B
is a marked block at level l. Let Bleft and Bright be the children of B at level
l + 1. Then, the problem reduces to either (i) computing the ﬁngerprint of
Bleft [1..k] if k ≤ bl/2, or combining the ﬁngerprints of Bleft (sampled) and
Bright [1..k − bl/2]. In both sub-cases, the problem therefore reduces to that
of computing the ﬁngerprint of the preﬁx of a block at level l + 1, which is
explicit since B is marked.
(B) We wish to compute the ﬁngerprint of B[1..k], for some k ≤ bl, but B is
an unmarked explicit block. Then, B is linked (through a Γ-tree pointer) to an
occurrence in the same level spanning at most two blocks, both of which are
marked. If the occurrence of B spans only one marked block B(cid:48) at level l, then
B[1..bl] = B(cid:48)[1..bl] and we are back in case (A). Otherwise, the occurrence of
B spans two marked blocks B(cid:48) and B(cid:48)(cid:48) at level l: B[1..bl] = B(cid:48)[i..bl]B(cid:48)(cid:48)[1..i −
1], with i ≤ bl. For each pointer of this kind in the Γ-tree, we store the
ﬁngerprint of B(cid:48)[i..bl] (one word per unmarked block). We consider two subcases.
 (B.1) If k ≥ bl − i + 1, then B[1..k] = B(cid:48)[i..bl]B(cid:48)(cid:48)[1..k − (bl − i + 1)].
Since we explicitly store the ﬁngerprint of B(cid:48)[i..bl], the problem reduces again
to that of computing the ﬁngerprint of the preﬁx B(cid:48)(cid:48)[1..k − (bl − i + 1)] of a
marked (explicit) block. Note that, to combine the ﬁngerprints of B(cid:48)[i..bl] and
B(cid:48)(cid:48)[1..k − (bl − i + 1)], we need the exponential σk−(bl−i+1) mod q. This value
can be easily computed recursively by making sure that our recursive calls
return not only ﬁngerprints, but also the associated exponentials modulo q.
The ﬁnal case of our proof is (B.2): k < bl−i+1. Then, B[1..k] = B(cid:48)[i..i+k−1].
Although this is not the preﬁx nor the suﬃx of a block, note that B[1..k]B(cid:48)[i+
k..bl] = B(cid:48)[i..i + k − 1]B(cid:48)[i + k..bl] = B(cid:48)[i..bl]. Letting φ denote our ﬁngerprint
function, this implies that φ(B[1..k])·σbl−i−k+1 +φ(B(cid:48)[i+k..bl]) ≡q φ(B(cid:48)[i..bl]),
i.e., φ(B[1..k]) ≡q

(cid:17) · σ−bl+i+k−1. On the right-

(cid:16)

φ(B(cid:48)[i..bl]) − φ(B(cid:48)[i + k..bl])

8

hand side of this equation, we have three values. (1) φ(B(cid:48)[i..bl]) is explicitly
stored. (2) φ(B(cid:48)[i+k..bl]) is the ﬁngerprint of the suﬃx of an explicit (marked)
block, which we can compute using the symmetric for suﬃxes of case (A). (3)
σ−bl+i+k−1 mod q = σ−|B(cid:48)[i+k..bl]| mod q can be retrieved by making sure that
our recursive calls return not only ﬁngerprints and the associated exponentials
modulo q, but also the reciprocal of those exponentials modulo q.

To sum up, computing a preﬁx/suﬃx of an explicit block at level l reduces
to the problem of computing the preﬁx/suﬃx of an explicit block at level
l + 1 (plus a constant number of arithmetic operations to combine values).
In the worst case we navigate down to the leaves, where ﬁngerprints of single
characters can be computed in constant time. Combining this procedure into
(cid:3)
our main algorithm we obtain the claimed running time of O(lg(n/γ)).

3 A Universal Self-Index

Our self-index structure builds on the Γ-tree of S. It is formed by two main
components: the ﬁrst ﬁnds all the pattern positions that cross block boundaries,
 whereas the second ﬁnds the positions inside unmarked blocks.

Lemma 2 Any substring of S of length at least 2 either overlaps two explicit
blocks or is completely inside an unmarked block.

Proof. Consider the substring S[i..j]. If S crosses the boundary between two
blocks at level l = 0, then we are done because all those nodes are explicit.
Otherwise, let v be the deepest node of the Γ-tree that contains S[i..j]. Then,
either v is a leaf or v has two children that split S[i..j]. In the former case, v
cannot be at the last level because it contains a substring of length at least 2.
Therefore, it must be an unmarked block that is replaced by a pointer. In the
latter case, v is marked and thus its two children are explicit. In both cases,
(cid:3)
the lemma holds.

We exploit the lemma in the following way. We will deﬁne an occurrence of
P as primary if it overlaps two consecutive explicit blocks at some level. The
occurrences that are completely contained in an unmarked block are secondary.
By the lemma, every occurrence of P is either primary or secondary. We will
use a data structure to ﬁnd the primary occurrences and another to detect
the secondary ones. The primary occurrences are found by exploiting the fact
that a preﬁx of P matches at the end of an explicit block and the remaining
suﬃx of P matches the text that follows. Secondary occurrences, instead, are
found by detecting primary or other secondary occurrences within the area
where an unmarked block points.

9

We note that this idea is a variant of the classical one [26] used in all indexes 
based on LZ77 and CGFs. Now we show that the principle can indeed
be applied on attractors, which is the general concept underlying all those
compression methods (and others where no indexes exist yet), and therefore
uniﬁes all those particular techniques.

3.1 The Data Strucures

We describe the data structures used by our index. Overall, they require
O(γ lg(n/γ)) space.

Primary occurrences. Let B be a marked block at some level l, which is
divided into B = Bleft · Bright at level l + 1. Note that blocks Bleft and Bright
can be marked or unmarked. Then we collect the reverse block Brev
left (i.e., Bleft
read backwards) in the multiset Y and the block Bright in the multiset X. In
addition, for the blocks B1 . . . Bγ of level l = 0, we also add Brev
to Y and the
suﬃx Bi+1 . . . Bγ to X, for all 1 ≤ i < γ.

i

If a primary occurrence is not contained in any block, then it spans a sequence
Bi . . . Bi(cid:48) of blocks at level l = 0. We will then ﬁnd it as the concatenation
of a suﬃx of Bi with a preﬁx of Bi+1 . . . Bγ. Otherwise, let B be the shortest
(or deepest) marked block that contains the occurrence. Let B be split into
Bleft · Bright in the Γ-tree. Then the occurrence will span a suﬃx of Bleft and
a preﬁx of Bright (recall Lemma 2). Therefore, each primary occurrence will
be found in the concatenation of exactly one pair (cid:104)Xx, Yy(cid:105) created together in
the above process.

We then lexicographically sort X and Y , to obtain the strings X1, X2, . . . and
Y1, Y2, . . .. All the occurrences ending with a preﬁx of P will form a contiguous
range in the sorted multiset Y , whereas all those starting with a suﬃx of P will
form a contiguous range in the sorted multiset X. Each primary occurrence
of P will then correspond to a pair (cid:104)Xx, Yy(cid:105) where both Xx and Yy belong to
their range.

Our structure to ﬁnd the primary occurrences is a two-dimensional discrete
grid G storing one point (x, y) for each pair (cid:104)Xx, Yy(cid:105) of strings that were
created together. The grid G is of size (w − 1) × (w − 1), since there are
w − 1 pairs (cid:104)Xx, Yy(cid:105): one per internal Γ-tree node (of which there are w − γ),
plus γ − 1 for the blocks of level 0. We represent G using a two-dimensional
range search data structure requiring O(w) space [10] that reports the p points
lying inside any rectangle of the grid in time O((p + 1) lg w), for any constant
 > 0. We also store an array T [1..w − 1] that gives the position pos in S

10

corresponding to each point (x, y), precisely the ﬁrst position of Yy, sorted by
y-coordinate.

Secondary occurrences. To track the secondary occurrences, let us call
target and source the text areas S[i..i(cid:48)] and S[j(cid:48)..j(cid:48)(cid:48)], respectively, of an unmarked 
block and its pointer, so that there is some j ∈ Γ contained in S[j(cid:48)..j(cid:48)(cid:48)].
Let S[pos..pos+m−1] be an occurrence we have already found (using the grid
G, initially). Our aim is to ﬁnd all the sources that contain S[pos..pos+m−1],
since their corresponding targets then contain other occurrences of the pattern.

To this aim, we store the sources of all levels in an array R[1..w − γ], ordered
by starting position j(cid:48). On R, we build a predecessor search structure on the
j(cid:48) values, and a range maximum query (RMQ) data structure able to ﬁnd the
maximum endpoint j(cid:48)(cid:48) in any range of R. While a predecessor search using
O(w) space requires O(lg lgω(n/w)) time on a ω-bit-word machine [39], the
RMQ operates in constant time using just O(w) bits [17].

The total space of the data structures is thus O(w) = O(γ lg(n/γ)) words.

3.2 Queries

Primary occurrences. To search for a pattern P [1..m], we ﬁrst ﬁnd its
primary occurrences using G as follows. For each partition P< = P [1..k] and
P> = P [k + 1..m], for 1 ≤ k < m, we search Y for P rev
< and X for P>. For
each of the identiﬁed ranges [x1, x2]× [y1, y2], we extract all the p corresponding 
primary occurrences in time O((p + 1) lg w), with our range search data
structure.
We can obtain the m − 1 ranges in the set X in overall time O(m lg(mn/γ)),
by using the ﬁngerprint-based technique described by Gagie et al. [21].

Lemma 3 (adapted from [21, Lem. 5.2]) Let S[1..n] be a string on alphabet 
[1..σ] and X a sorted set of suﬃxes of S. If one can extract a substring
of length (cid:96) from S in time fe((cid:96)) and can compute a Karp-Rabin ﬁngerprint
of it in time fh((cid:96)), then one can build a data structure of size O(|X|) that
obtains the lexicographic ranges in X of the m − 1 suﬃxes of a given pattern
P in time O(m lg(σ)/ω + m(fh(m) + lg m) + fe(m)).

We can use an analogous structure to obtain the ranges in Y of the suﬃxes
of the reversed pattern.

11

In Sections 2.1 and 2.2 we show how to extract in time fe((cid:96)) = O((cid:96) lg(n/γ))
and how to compute a ﬁngerprint in time fh((cid:96)) = O(lg(n/γ)), respectively,
so the lemma shows that we can ﬁnd all the ranges in X and Y in time
O(m lg(σ)/ω+m(lg(n/γ)+lg m)+m lg(n/γ)) = O(m lg(mn/γ)) = O(m lg n).
Therefore, the total cost to extract the occp primary occurrences is O(m lg n +
occp lg w).
Patterns P of length m = 1 can be handled as P [1]∗, where ∗ stands for any
character. Thus we take [x1, x2] = [1, w] and carry out the search as a normal
pattern of length m = 2. To make this work also for the last position in S,
we assume as usual that S is terminated by a special symbol $ that cannot
appear in search patterns P .

Secondary occurrences. Let S[pos..pos + m − 1] be a primary occurrence
found. A predecessor search for pos gives us the rightmost position R[r] where
the sources start at pos or to its left. An RMQ on R[1..r] then ﬁnds the source
R[k] with the rightmost endpoint in R[1..r]. If even R[k].j(cid:48)(cid:48) < pos + m − 1,
then no source covers the occurrence. If, instead, R[k].j(cid:48)(cid:48) ≥ pos + m − 1,
then the source R[k] covers the occurrence and we process its corresponding
target as a secondary occurrence; in this case we also recurse on the ranges
R[1..k − 1] and R[k + 1..r] that are nonempty. It is easy to see that each
valid secondary occurrence is identiﬁed in O(1) time [35]. In addition, such
secondary occurrences spot, S[pos(cid:48)..pos(cid:48)+m−1], must be recursively processed
for further secondary occurrences. A similar procedure is described for the
LZ77-index [31].

The cost per secondary occurrence then amortizes to a predecessor search,
O(lg lgω(n/γ)) time.

The total search cost with occ primary and secondary occurrences is therefore 
O(m lg n + occ(lg(γ lg(n/γ)) + lg lgω(n/γ))) = O(m lg n + occ(lg γ +
lg lg(n/γ))) = O(m lg n + occ lg n), for any constant  > 0 deﬁned at indexing 
time (the choice of  aﬀects the constant that accompanies the size
O(γ lg(n/γ)) of structure G).

4 Construction

Our structure can be built in O(n lg n) expected time and O(n) space. If we
allow the index construction to be correct only with high probability, then the
space can be reduced to O(w) (plus read-only access to S).

12

4.1 Building the Γ-tree

Given the attractor Γ, we can build the index data structure as follows. At
each level l, we create an Aho-Corasick automaton [1] on the unmarked blocks
at this level (i.e., those at distance ≥ bl from any attractor), and use it to scan
the areas S[j − bl + 1..j + bl − 1] around all the attractor elements j ∈ Γ in
order to ﬁnd a proper pointer for each of those unmarked blocks. This takes
O(n) time per level. Since the areas around each of the γ attractor positions
are scanned at each level but they have exponentially decreasing lengths, the
scanning time adds up to O(n). Similarly, each unmarked block is preprocessed
only once. Therefore, we can build the Γ-tree in O(n) deterministic time and
space (we can discard the automaton of each level after we use it for scanning).
To reduce the space to O(w), instead of inserting the unmarked blocks into
an Aho-Corasick automaton, we compute their Karp-Rabin ﬁngerprints, store
them in a hash table, and scan the areas S[j−bl +1..j +bl−1] around attractor
elements j. This ﬁnds the correct sources for all the unmarked blocks with high
probability. Indeed, if we verify the potential collisions, the result is always
correct within O(n) expected time.

4.2 Building the Fingerprints

Building the structures for Lemma 1 requires (i) computing the ﬁngerprint of
every text preﬁx ending at block boundaries (O(n) time and O(w) space in
addition to S), (ii) computing the ﬁngerprint of every explicit block (O(w)
time and O(w) space starting from the leaves and combining results up to the
root), (iii) for each explicit unmarked block, computing the ﬁngerprint of one
of its suﬃxes and one of its preﬁxes (case (B) of Lemma 1). Since unmarked
blocks do not have children, each text character is encountered at most twice
while computing these ﬁngerprints (one for the preﬁx’s and one for the suﬃx’s
ﬁngerprints), which implies that these values can be computed in O(n) time
and O(w) space in addition to S.

This process, however, does not include ﬁnding a collision-free Karp-Rabin
hash function. As a result, the ﬁngerprinting is correct with high probability
only. We can use the de-randomization procedure of Bille et al. [7], which
guarantees to ﬁnd — in O(n lg n) expected time and O(n) words of space
— a Karp-Rabin hash function that is collision-free among substrings whose
lengths are powers of two. This is suﬃcient to deterministically check the
equality of substrings 5 in the z-fast trie used in the technique [21, Lem. 5.2]

5 If the length (cid:96) of the two substrings is not a power of two, then we compare their
preﬁxes and suﬃxes whose length is the largest power of two smaller than (cid:96).

13

that we use to quickly ﬁnd ranges of pattern suﬃxes/preﬁxes (in our Section 
3.2).

4.3 Building the Multisets X and Y

To build the multisets X and Y for the primary occurrences, we can build the
suﬃx arrays [34] of S and its reverse, Srev. This requires O(n) deterministic
time and space [25]. Then we can scan those suﬃx arrays in order to enumerate
X and Y in the lexicographic order.
To sort X and Y within O(w) space, we can build instead a sparse suﬃx
tree on the w positions of X or Y in S. This can be done in expected time
O(n

lg w) = o(n lg n) and O(w) space [22].

√

4.4 Structures to Track Occurrences

The construction of the grid data structure G requires O(w lg w) deterministic
time, and O(w) space. Arrays T and R can be built by radix sort in O(n)
time and space, or in O(w lg w) time and O(w) space using plain sorting.
The RMQ structure on R requires O(w) deterministic construction time and
O(w) bits [17]. The predecessor data structure [39], however, requires perfect 
hashing. This can be built in O(w) space and O(w lg lg n) = o(w lg n)
deterministic time [40], or in O(w) expected time.

5 Conclusions

We have introduced the ﬁrst universal self-index for repetitive text collections.
The index is based on a recent result [28] that uniﬁes a large number of dictionary 
compression methods into the single concept of string attractor. For
each compression method based on Lempel-Ziv, grammars, run-compressed
BWT, collage systems, macro schemes, etc. it is easy to identify an attractor
set of the same asymptotic size obtained by the compression method, say γ.
Thus, our construction automatically yields a self-index for each of those compression 
methods, within O(γ lg(n/γ)) space. This the minimum size of any
existing structure able to eﬃciently extract a substring from the compressed
text (i.e., within O(polylog n) time per symbol), and thus the minimum we
could hope for a self-index with the current state of the art.

This space also matches asymptotically that of the smallest known self-indexes

14

for repetitive sequences (if we exclude some [32] that cannot oﬀer search times
in O((poly m + occ) polylog n)), while our search time, O(m lg n + occ lg n)
for any constant  > 0, is close to the best times obtained for each such selfindex 
speciﬁcally developed for one compression format (see [21, Table 1]).
Moreover, our construction provides a self-index for compression methods for
which no such a structure existed before, such as collage systems and macro
schemes. Those can provide smaller attractor sets than those derived from the
more popular compression methods.

We can improve the search time of our index by using slightly more space.
Our current bottleneck in the per-occurrence query time is the grid data structure 
G, which uses O(w) space and returns each occurrence in time O(lg w).
Instead, a grid structure [10] using O(w lg lg w) = O(γ lg(n/γ) lg lg n) space
obtains the occp primary occurrences in time O((occp + 1) lg lg w). Therefore,
this slightly larger version of our index can search in time O(m lg n+occ lg lg n).
This complexity is close to that of some larger indexes in the literature for
repetitive string collections (see [21, Table 1]).

A number of avenues for future work are open, including supporting more
complex pattern matching, handling dynamic collections of texts, supporting
document retrieval, and implementing a practical version of the index. Any
advance in this direction will then translate into all of the existing indexes for
repetitive text collections.

References

[1] A. V. Aho and M. J. Corasick. Eﬃcient string matching: an aid to bibliographic

search. Communications of the ACM, 18(6):333–340, 1975.

[2] D. Belazzougui and F. Cunial. Fast label extraction in the CDAWG. In Proc.
24th International Symposium on String Processing and Information Retrieval
(SPIRE), LNCS 10508, pages 161–175, 2017.

[3] D. Belazzougui, F. Cunial, T. Gagie, N. Prezza, and M. Raﬃnot. Composite
In Proc. 26th Annual Symposium on

repetition-aware data structures.
Combinatorial Pattern Matching (CPM), LNCS 9133, pages 26–39, 2015.

[4] D. Belazzougui, T. Gagie, P. Gawrychowski, J. K¨arkk¨ainen, A. Ord´o˜nez, S. J.
Puglisi, and Y. Tabei. Queries on LZ-bounded encodings. In Proc. 25th Data
Compression Conference (DCC), pages 83–92, 2015.

[5] D. Belazzougui, S. J. Puglisi, and Y. Tabei. Access, rank, select in grammarcompressed 
strings. In Proc. 23rd Annual European Symposium on Algorithms
(ESA), LNCS 9294, pages 142–154, 2015.

15

[6] P. Bille, M. B. Ettienne, I. L. Gørtz, and H. W. Vildhøj. Time-space tradeoﬀs 
for Lempel-Ziv compressed indexing. In Proc. 28th Annual Symposium on
Combinatorial Pattern Matching (CPM), LIPIcs 78, pages 16:1–16:17, 2017.

[7] P. Bille, I. L. Gørtz, M. B. T. Knudsen, M. Lewenstein, and H. W. Vildhøj.
Longest common extensions in sublinear space.
In Proc. 26th Annual
Symposium on Combinatorial Pattern Matching (CPM), LNCS 9133, pages 65–
76, 2015.

[8] A. Blumer, J. Blumer, D. Haussler, R. McConnell, and A. Ehrenfeucht.
Complete inverted ﬁles for eﬃcient text retrieval and analysis. Journal of the
ACM, 34(3):578–595, 1987.

[9] M. Burrows and D. Wheeler. A block sorting lossless data compression

algorithm. Technical Report 124, Digital Equipment Corporation, 1994.

[10] T. M. Chan, K. G. Larsen, and M. P˘atra¸scu. Orthogonal range searching on the
RAM, revisited. In Proc. 27th ACM Symposium on Computational Geometry
(SoCG), pages 1–10, 2011.

[11] M. Charikar, E. Lehman, D. Liu, R. Panigrahy, M. Prabhakaran, A. Sahai, and
A. Shelat. The smallest grammar problem. IEEE Transactions on Information
Theory, 51(7):2554–2576, 2005.

[12] F. Claude, A. Fari˜na, M. Mart´ınez-Prieto, and G. Navarro. Universal indexes
for highly repetitive document collections. Information Systems, 61:1–23, 2016.

[13] F. Claude and G. Navarro.

Self-indexed grammar-based compression.

Fundamenta Informaticae, 111(3):313–337, 2010.

[14] F. Claude and G. Navarro.

Improved grammar-based compressed indexes.
In Proc. 19th International Symposium on String Processing and Information
Retrieval (SPIRE), LNCS 7608, pages 180–192, 2012.

[15] M. Crochemore and R. V´erin. Direct construction of compact directed acyclic
In Proc. 8th Annual Symposium on Combinatorial Pattern

word graphs.
Matching (CPM), LNCS 1264, pages 116–129. Springer, 1997.

[16] P. Ferragina and G. Manzini. Opportunistic data structures with applications.
In Proc. 41st Annual Symposium on Foundations of Computer Science (FOCS),
pages 390–398, 2000.

[17] J. Fischer and V. Heun.

Space-eﬃcient preprocessing schemes for range
minimum queries on static arrays. SIAM Journal on Computing, 40(2):465–
492, 2011.

[18] T. Gagie. Large alphabets and incompressibility.

Information Processing

Letters, 99(6):246–251, 2006.

[19] T. Gagie, P. Gawrychowski, J. K¨arkk¨ainen, Y. Nekrich, and S. J. Puglisi. A
faster grammar-based self-index.
In Proc. 6th International Conference on
Language and Automata Theory and Applications (LATA), LNCS 7183, pages
240–251, 2012.

16

[20] T. Gagie, P. Gawrychowski, J. K¨arkk¨ainen, Y. Nekrich, and S. J. Puglisi. LZ77based 
self-indexing with faster pattern matching. In Proc. 11th Latin American
Symposium on Theoretical Informatics (LATIN), LNCS 8392, pages 731–742,
2014.

[21] T. Gagie, G. Navarro, and N. Prezza. Optimal-Time Text Indexing in BWTruns 
Bounded Space. In Proc. 29th Annual ACM-SIAM Symposium on Discrete
Algorithms (SODA), pages 1459–1477, 2018.

[22] P. Gawrychowski and T. Kociumaka. Sparse suﬃx tree construction in optimal

time and space. CoRR, abs/1608.00865, 2016.

[23] A. Jez. Approximation of grammar-based compression via recompression.

Theoretical Computer Science, 592:115–134, 2015.

[24] A. Jez. A really simple approximation of smallest grammar. Theoretical

Computer Science, 616:141–150, 2016.

[25] J. K¨arkk¨ainen, P. Sanders, and S. Burkhardt.

Linear work suﬃx array

construction. Journal of the ACM, 53(6):918–936, 2006.

[26] J. K¨arkk¨ainen and E. Ukkonen. Lempel-Ziv parsing and sublinear-size index
In Proc. 3rd South American Workshop on

structures for string matching.
String Processing (WSP), pages 141–155, 1996.

[27] R. M. Karp and M. O. Rabin.

Eﬃcient randomized pattern-matching

algorithms. IBM Journal of Research and Development, 31(2):249–260, 1987.

[28] D. Kempa and N. Prezza. At the Roots of Dictionary Compression: String
Attractors. In Proceedings of the 50th Annual ACM Symposium on the Theory
of Computing (STOC). To appear., 2018.

[29] T. Kida, T. Matsumoto, Y. Shibata, M. Takeda, A. Shinohara, and S. Arikawa.
Collage system: A unifying framework for compressed pattern matching.
Theoretical Computer Science, 298(1):253–272, 2003.

[30] J. C. Kieﬀer and E. Yang. Grammar-based codes: A new class of universal
lossless source codes. IEEE Transactions on Information Theory, 46(3):737–
754, 2000.

[31] S. Kreft and G. Navarro. On compressing and indexing repetitive sequences.

Theoretical Computer Science, 483:115–133, 2013.

[32] S. Kreft and G. Navarro. On compressing and indexing repetitive sequences.

Theoretical Computer Science, 483:115–133, 2013.

[33] A. Lempel and J. Ziv. On the complexity of ﬁnite sequences.

IEEE

Transactions on Information Theory, 22(1):75–81, 1976.

[34] U. Manber and G. Myers. Suﬃx arrays: a new method for on-line string

searches. SIAM Journal on Computing, 22(5):935–948, 1993.

17

[35] S. Muthukrishnan. Eﬃcient algorithms for document retrieval problems.

In
Proc. 13th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA),
pages 657–666, 2002.

[36] G. Navarro. A Self-Index on Block Trees.

In Proc. 24th International
Symposium on String Processing and Information Retrieval (SPIRE), LNCS
10508, pages 278–289, 2017.

[37] T. Nishimoto, T. I, S. Inenaga, H. Bannai, and M. Takeda. Dynamic index, LZ
factorization, and LCE queries in compressed space. CoRR, abs/1504.06954,
2015.

[38] T. Nishimoto, T. I, S. Inenaga, H. Bannai, and M. Takeda. Fully dynamic data
structure for LCE queries in compressed space.
In Proc. 41st International
Symposium on Mathematical Foundations of Computer Science (MFCS), pages
72:1–72:15, 2016.

[39] M. Patrascu and M. Thorup. Time-space trade-oﬀs for predecessor search. In
Proc. 38th Annual ACM Symposium on Theory of Computing (STOC), pages
232–240, 2006.

[40] M. Ruzic. Constructing eﬃcient dictionaries in close to sorting time. In Proc.
35th International Colloquium on Automata, Languages and Programming
(ICALP), pages 84–95, 2008.

[41] W. Rytter. Application of Lempel-Ziv factorization to the approximation of
grammar-based compression. Theoretical Computer Science, 302(1-3):211–222,
2003.

[42] H. Sakamoto. A fully linear-time approximation algorithm for grammar-based

compression. Journal of Discrete Algorithms, 3(24):416–430, 2005.

[43] J. A. Storer and T. G. Szymanski. Data compression via textual substitution.

Journal of the ACM, 29(4):928–951, 1982.

[44] P. Weiner.

Linear pattern matching algorithms.

In Proc. 14th Annual

Symposium on Switching and Automata Theory, pages 1–11, 1973.

18

