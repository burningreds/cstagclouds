Practical Compressed Suﬃx Trees(cid:2)

Rodrigo C´anovas and Gonzalo Navarro

Department of Computer Science, University of Chile, Chile

{rcanovas,gnavarro}@dcc.uchile.cl

Abstract. The suﬃx tree is an extremely important data structure
for stringology, with a wealth of applications in bioinformatics. Classical 
implementations require much space, which renders them useless
for large problems. Recent research has yielded two implementations offering 
widely diﬀerent space-time tradeoﬀs. However, each of them has
practicality problems regarding either space or time requirements. In this
paper we implement a recent theoretical proposal and show it yields an
extremely interesting structure that lies in between, oﬀering both practical 
times and aﬀordable space. The implementation is by no means
trivial and involves signiﬁcant algorithm engineering.

1 Introduction

The suﬃx tree [18,30] is arguably the most important data structure for string
analysis. It has been said to have a myriad of virtues [2] and there are books dedicated 
to its applications in areas like bioinformatics [12]. Many complex sequence
analysis problems are solved through sophisticated traversals over the suﬃx tree,
and thus a fully-functional implementation supports a variety of navigation operations.
 These involve not only the classical tree navigation operations (parent,
child) but also speciﬁc ones such as suﬃx links and lowest common ancestors.

One serious problem of suﬃx trees is that they take much space. A naive
implementation can easily require 20 bytes per character, and a very optimized
one reaches 10 bytes [14]. A way to reduce this space to about 4 bytes per
character is to use a simpliﬁed structure called a suﬃx array [17], but it does
not contain suﬃcient information to carry out all the complex tasks suﬃx trees
are used for. Enhanced suﬃx arrays [1] extend suﬃx arrays so as to recover the
full suﬃx tree functionality, raising the space to about 6 bytes per character in
practice. Another heuristic space-saving methods [20] achieve about the same.
For example, on DNA, each character could be encoded with 2 bits, whereas
the alternatives we have considered require 32 to 160 bits per character (bpc).
Using suﬃx trees on secondary memory makes them orders of magnitude slower
as most traversals are non-local. This situation is also a heresy in terms of Information 
Theory: whereas the information contained in a sequence of n symbols
over an alphabet of size σ is n log σ bits in the worst case, all the alternatives
above require Θ(n log n) bits. (Our logarithms are in base 2.)

(cid:2) Partially funded by Millennium Institute of Cell Dynamics and Biotechnology
(ICDB), Grant ICM P05-001-F, Mideplan, and by Fondecyt Grant 1-080019, Chile.

P. Festa (Ed.): SEA 2010, LNCS 6049, pp. 94–105, 2010.
c(cid:2) Springer-Verlag Berlin Heidelberg 2010

Practical Compressed Suﬃx Trees

95

Recent research on compressed suﬃx trees (CSTs) has made much progress
in terms of reaching space requirements that approach not only the worst-case
space of the sequence, but even its information content. All these can be thought
of as a compressed suﬃx array (CSA) plus some extra information that encodes
the tree topology and longest common preﬁx (LCP) information.

The ﬁrst such proposal was by Sadakane [27]. It requires 6n bits on top of
his CSA [26], which in turn requires nH0 + O(n log log σ) bits, where H0 is the
zero-order entropy of the sequence. This structure supports most of the tree
navigation operations in constant time (except, notably, going down to a child,
which is an important operation). A recent implementation [29] achieves a few
tens of microseconds per operation, but in practice the structure requires about
25–35 bpc (close to a suﬃx array), and thus its applicability is limited.

The second proposal was by Russo et al. [25]. It requires only o(n) bits on top
of a CSA. By using an FM-index [6] as the CSA, one achieves nHk + o(n log σ)
bits of space, where Hk is the k-th order empirical entropy of the sequence,
for suﬃciently low k ≤ α logσ n, for any constant 0 < α < 1. The navigation
operations are supported in polylogarithmic time (at best Θ(log n log log n) in
their paper). This structure was implemented by Russo and shown to achieve
very little space, around 4–6 bpc, which makes it extremely attractive when the
sequence is large compared to the available main memory. On the other hand,
the structure is much slower than Sadakane’s. Each navigation operation takes
the order of milliseconds, which is comparable to disk operation times.

Both existing implementations are unsatisfactory in either time or space
(though certainly excell on the other aspect), and become very far extremes
of a tradeoﬀ: Either one has suﬃcient main memory to spend 30 bpc, or one has
to spend milliseconds per navigation operation.

In this paper we present a third implementation, which oﬀers a relevant
space/time tradeoﬀ between these two extremes. One variant shows to be superior 
to the implementation of Sadakane’s CST in both space and time: it uses
13–16 bpc (i.e., half the space) and requires a few microseconds (i.e., several
times faster) per operation. A second alternative works within 8–12 bpc and
requires a few hundreds of microseconds per operation, that is, smaller than our
ﬁrst variant and still several times faster than Russo’s implementation.

Our implementation is based on a third theoretical proposal, by Fischer et
al. [8], which achieves nHk(2 max(1, log(1/Hk))+1/+O(1))+o(n log σ) bpc (for
the same k as above and any constant  > 0) and navigation times of the form
O(log n). Their proposal involves several theoretical solutions, whose eﬃcient
implementation was far from trivial, requiring signiﬁcant algorithm engineering
that completely changed the original proposal in some cases. After experimental
study of several alternatives, we choose the two variants described above.

Our work opens the door to a number of practical suﬃx tree applications, particularly 
relevant to bioinformatics. Our implementation will be publicly available 
in the Pizza&Chili site (http://pizzachili.dcc.uchile.cl). We plan to
apply it to solve concrete bioinformatic problems on large instances.

96

R. C´anovas and G. Navarro

Table 1. Operations over the nodes and leaves of the suﬃx tree

Operation
Root()
Locate(v)
Ancestor(v, w)
SDepth(v)/TDepth(v)
Count(v)
Parent(v)
FChild(v)
NSibling(v)
SLink(v)
SLinki(v)
LCA(v, w)
Child(v, a)
Letter(v, i)
LAQS (v, d)/LAQT (v, d)

Description
the of the suﬃx tree.
suﬃx position i if v is the leaf of suﬃx Ti,n, otherwise NULL.
true if v is an ancestor of w.
string-depth/tree-depth of v.
number of leaves in the subtree rooted at v.
parent node of v.
alphabetically ﬁrst child of v.
alphabetically next sibling of v.
suﬃx-link of v; i.e., the node w s.th. π(w) = β if π(v) = aβ for a ∈ Σ.
iterated suﬃx-link: the node w s.th. π(w) = β if π(v) = aβ for a ∈ Σi.
lowest common ancestor of v and w.
node w s.th. the ﬁrst letter on edge (v, w) is a ∈ Σ.
ith letter of v’s path-label, π(v)[i].
the hightest ancestor of v with string-depth/tree-depth ≤ d.

2 Compressed Suﬃx Trees

A suﬃx array over a text T [1, n] is an array A[1, n] of the positions in T , lexicographically 
sorted by the suﬃx starting at the corresponding position of T .
That is, T [A[i], n] < T [A[i + 1], n] for all 1 ≤ i < n. Note that every substring
of T is the preﬁx of a suﬃx, and that all suﬃxes starting with a given pattern
P appear consecutively in A, hence a couple of binary searches ﬁnd the area
A[sp, ep] containing all the positions where P occurs in T .

There are several compressed suﬃx arrays (CSAs) [21,5], which oﬀer essentially 
the following functionality: (1) Given a pattern P [1, m], ﬁnd the interval 
A[sp, ep] of the suﬃxes starting with P ; (2) obtain A[i] given i; (3) obtain 
A−1[j] given j. An important function the CSAs implement is Ψ(i) =
A−1[(A[i] mod n) + 1] and its inverse, usually much faster than computing A
and A−1. This function lets us move virtually in the text, from the suﬃx i that
points to text position j = A[i], to the one pointing to j + 1 = A[Ψ(i)].

A suﬃx tree is a compact trie (or digital tree) storing all the suﬃxes of T .
This is a labeled tree where each text suﬃx is read in a root-to-leaf path, and the
children of a node are labeled by diﬀerent characters. Leaves are formed when
the preﬁx of the corresponding suﬃx is already unique. Here “compact” means
that unary paths are converted into a single edge, labeled by the string formed
by concatenating the involved character labels. If the children of each node are
ordered lexicographically by their string label, then the leaves of the suﬃx tree
form the suﬃx array of T . Several navigation operations over the nodes and
leaves of the suﬃx tree are of interest. Table 1 lists the most common ones.

In order to get a suﬃx tree from a suﬃx array, one needs at most two extra
pieces of information: (1) the tree topology; (2) the longest common preﬁx (LCP)
information, that is, LCP[i] is the length of the longest common preﬁx between
T [A[i − 1], n] and T [A[i], n] for i > 1 and LCP[1] = 0 (or, seen another way,
the length of the string labeling the path from the root to the lowest common
ancestor node of suﬃx tree leaves i and i − 1). Indeed, the suﬃx tree topology
can be implicit if we identify each suﬃx tree node with the suﬃx array interval

Practical Compressed Suﬃx Trees

97

(cid:5)

containing the leaves that descend from it. This range uniquely identiﬁes the
node because there are no unary nodes in a suﬃx tree.

Consequently, a compressed suﬃx tree (CST) is obtained by enriching the
CSA with some extra data. Sadakane [27] added the topology of the tree (using
4n extra bits) and the LCP data. The LCP was compressed to 2n bits by noticing
that, if sorted by text order rather than suﬃx array order, the LCP numbers
decrease by at most 1. Let LCP
[j +
1] ≥ LCP
[j] − 1. Thus the numbers can be diﬀerentially encoded, h[j + 1] =
[j] + 1 ≥ 0, and then represented in unary over a bitmap
LCP
H[1, 2n] = 0h[1]10h[2] . . . 10h[n]1. Then, to obtain LCP[i], we look for LCP
[A[i]],
and this is extracted from H via rank/select operations. Here rankb(H, i) counts
the number of bits b in H[1, i] and selectb(H, i) is the position of the i-th b in
H. Both can be answered in constant time using o(n) extra bits of space [19].
Then LCP

[j] = select1(H, j) − 2j, assuming LCP

be the permuted LCP array, then LCP

[0] = 0.

(cid:5)

[j + 1] − LCP

(cid:5)

(cid:5)

(cid:5)

(cid:5)

(cid:5)

(cid:5)

Russo et al. [25] get rid of the parentheses, by instead identifying suﬃx tree
nodes with their corresponding suﬃx array interval. By sampling some suﬃx tree
nodes, most operations can be carried out by moving, using suﬃx links, towards
a sampled node, ﬁnding the information stored in there, and transforming it as
we move back to the original node. The suﬃx link operation, deﬁned in Table 1,
can be computed using Ψ and the lowest common ancestor operation [27].

A New Theoretical CST Proposal. Fischer et al. [8] prove that array
H in Sadakane’s CST is compressible as it has at most 2r ≤ 2(nHk + σk)
runs of 0s or 1s, for any k. Let z1, z2, . . . , zr the lengths of the runs of 0s and
o1, o2, . . . , or the same for the 1s. They create arrays Z = 10z1−110z2−1 . . . and
O = 10o1−110o2−1 . . ., with overall 2r 1s out of 2n, and thus can be compressed
to 2r log n

r + O(r) + o(n) bits and support constant-time rank and select [24].

Their other improvement over Sadakane’s CST is to get rid of the tree topology
and replace it with suﬃx array ranges. Fischer et al. show that all the navigation
can be simulated by means of three operations: (1) RMQ(i, j) gives the position
of the minimum in LCP[i, j]; (2) PSV (i) ﬁnds the last value smaller than LCP[i]
in LCP[1, i − 1]; and (3) NSV (i) ﬁnds the ﬁrst value smaller than LCP[i] in
LCP[i + 1, n]. All these could easily be solved in constant time using O(n)
extra bits of space on top of the LCP representation, but Fischer et al. give
sublogarithmic-time algorithms to solve them with only o(n) extra bits.
As examples, the parent of node [i, j] is [PSV (i), NSV (i) − 1]; the LCA between 
nodes [i, j] and [i(cid:5), j(cid:5)] is [PSV (p), NSV (p)−1], where p = RMQ(min(i, i(cid:5)),
max(j, j(cid:5))); and the suﬃx link of [i, j] is [PSV (Ψ(i)), NSV (Ψ(j)) − 1].

Our Contribution. The challenge faced in this paper is to implement this CST.
This can be divided into (1) how to represent LCP eﬃciently in practice, and (2)
how to compute eﬃciently RMQ, PSV , and NSV over this LCP representation.
We study each subproblem and compare the resulting CST with previous ones.
Our experiments were performed on 100 MB of the protein, sources, XML
and DNA texts from Pizza&Chili. The computer is an Intel Core2 Duo at 3.16
GHz, with 8 GB of RAM and 6 MB cache, running Linux version 2.6.24-24.

98

R. C´anovas and G. Navarro

3 Representing Array LCP

The following alternatives were considered to represent LCP:

√

Sad-Gon: Encodes H in plain, using the rank/select implementation of Gonz´alez
[10], which takes 0.1n bits over the 2n used by H itself and answers select in
O(log n) time via binary search.

Sad-OS: Like the previous one, but using the dense array implementation of
Okanohara and Sadakane [22] for H. This requires about the same space as
the previous one and answers select in O(log4 r/ log n) time.

FMN-RRR: Encodes H in compressed form as in Fischer et al. [8], i.e., by
encoding bitmaps Z and O. We use the compressed representation by Raman
et al. [24] as implemented by Claude [4]. This costs 0.54n extra bits on top of
the entropy of the two bitmaps, 2r log n
r + O(r). Select takes O(log n) time.
FMN-OS: Like the previous one, but instead of Raman et al. technique, we
use the sparse array implementation by Okanohara and Sadakane [22]. This
requires 2r log n

r + O(r) bits and solves select in time O(log4 r/ log m).

PT: Inspired on an LCP construction algorithm [23], we store a particular sampling 
of LCP values, and compute the others using the sampled ones. Given
v + v) bytes of space and
a parameter v, the sampling requires n + O(n/
computes any LCP[i] by comparing at most some T [j, j + v] and T [j(cid:5), j(cid:5) + v].
As we must obtain these symbols using Ψ up to 2v times, the idea is slow.
PhiSpare: This is inspired in another construction [13]. For a parameter q, store
q with the LCP values for all text positions q · k.
(cid:5)
in text order an array LCP
Now assume SA[i] = qk+b, with 0 ≤ b < k. If b = 0, then LCP[i] = LCP
(cid:5)
q[k].
q[k + 1] −
(cid:5)
Otherwise, LCP[i] is computed by comparing at most q + LCP
q[k] symbols of the suﬃxes T [SA[i− 1], n] and T [SA[i], n]. The space is
(cid:5)
LCP
n/q integers and the computation requires O(q) applications of Ψ on average.
DAC: The directly addressable codes of Ladra et al. [3]. Most LCP values are
small (O(logσ n) on average), and thus require few bits. Yet, some can be
much longer. Thus we can ﬁx a block length b and divide each number, of
(cid:7) bits, into (cid:5)(cid:7)/b(cid:6) blocks of b bits. Each block is stored using b + 1 bits, the
last one telling whether the number continues in the next block or ﬁnishes in
the current one. Those blocks are then rearranged to allow for fast random
access. There are two variants of this structure, both implemented by Ladra:
one with ﬁxed b (DAC ), and another using diﬀerent b values for the ﬁrst,
second, etc. blocks, so as to minimize the total space (DAC -Var). Note we
represent LCP and not LCP

, thus we do not need to compute A[i].

(cid:5)

RP: Re-Pair [15] is a grammar-based compression method that factors out repetitions 
in a sequence. It has been used [11] to compress the diﬀerentially
encoded suﬃx array, SA(cid:5)[i] = SA[i] − SA[i − 1], which contains repetitions
because SA can be partitioned into r areas that appear elsewhere in SA with
the values shifted by 1 [16]. Note that LCP must then contain the same repetitions 
shifted by 1, and therefore Re-Pair compression of the diﬀerential
LCP should perform similarly [8]. To obtain LCP[i] we store sampled absolute 
LCP values and decompress the nonterminals since the last sample.

Practical Compressed Suﬃx Trees

99

xml.100MB, LCP

proteins.100MB, LCP

)
s
d
n
o
c
e
s
o
r
c
m

i

(
 

n
o

i
t

a
r
e
p
o

 

 
r
e
p
e
m
T

i

 100

 10

 1

 0.1

 0

Sad-Gon
Sad-OS
FMN-RRR
FMN-OS
PT
PhiSpare
DAC
DAC-VAR
RP

)
s
d
n
o
c
e
s
o
r
c
m

i

(
 

n
o

i
t

a
r
e
p
o

 

 
r
e
p
e
m
T

i

 2

 4

 6

 8

 10

 12

 14

 16

 18

Bits per character

 1000

 100

 10

 1

 0.1

Sad-Gon
Sad-OS
FMN-RRR
FMN-OS
PT
PhiSpare
DAC
DAC-VAR
RP

 0

 2

 4

 6

 10

 8
 12
Bits per character

 14

 16

 18

 20

Fig. 1. Space/time for accessing LCP array

Experimental Comparison. We tested the diﬀerent LCP implementations
by accessing 100,000 random positions of the LCP array. Fig. 1 shows the
space/times achieved on two texts (the others gave similar results). Only P T
and PhiSpare display a space/time tradeoﬀ; in the ﬁrst we use v = 4, 6, 8 and
for the second q = 16, 32, 64.

As it can be seen, DAC /DAC -Var and the representations of H dominate the
space-time tradeoﬀ map (PhiSpare and PT can use less space but they become
impractically slow). For the rest of the paper we will keep only DAC and DAC -
Var, which give the best time performance, and FMN -RRR and Sad-Gon, which
have the most robust performance at representing H.

4 Computing RMQ, PSV , and NSV

Once a representation for LCP is chosen, one must carry out operations RMQ,
PSV , and NSV on top of it (as they require to access LCP). We ﬁrst implemented 
verbatim the theoretical proposals of Fischer et al. [8]. For NSV , the
idea is akin to the recursive ﬁndclose solution for compressed trees [9]: the array 
is divided into blocks and some values are chosen as pioneers so that, if a
position is not a pioneer, then its NSV answer is in the same block of that of
its preceding pioneer (and thus it can be found by scanning that block). Pioneers 
are marked in a bitmap so as to map them to a reduced array of pioneers,
where the problem is recursively solved. We experimentally veriﬁed that it is
convenient to continue the recursion until the end instead of storing the explicit
answers at some point. The block length L yields a space/time tradeoﬀ since,
at each level of the recursion, we must obtain O(L) values from LCP. PSV is
symmetric, needing another similar structure.

For RMQ we apply an existing implementation [7] on the LCP array, remembering 
that we do not have direct access to LCP but have to use any of the
access methods we have developed for it. This accesses at most 5 cells of LCP,
yet it requires 3.25n bits. In the actual theoretical proposal [8] this is reduced to
o(n) but many more accesses to LCP would be necessary; we did not implement
that verbatim as it has little chances of being practical.

100

R. C´anovas and G. Navarro

The ﬁnal data structure, that we call FMN -NPR, is composed of the structure

to answer NSV plus the one for PSV plus the structure to calculate RMQ.

4.1 A Novel Practical Solution
We propose now a diﬀerent solution, inspired in Sadakane and Navarro’s succinct
tree representation [28]. We divide LCP into blocks of length L. Now we form
a hierarchy of blocks, where we store the minimum LCP value of each block i
in an array m[i]. The array uses n
L log n bits. On top of array m, we construct a
perfect L-ary tree Tm where the leaves are the elements of m and each internal
node stores the minimum of the values stored in its children. The total space for
Tm is n
L log n(1 + O(1/L)) bits, so if L = ω(log n), the space used is o(n) bits.
To answer NSV (i), we look for the ﬁrst j > i such that LCP[j] < p = LCP[i],
using Tm to ﬁnd it in time O(L log(n/L)). We ﬁrst search sequentially for the
answer in the same block of i. If it is not there, we go up to the leaf that represents
the block and search the right siblings of this leaf. If some of these sibling leaves
contain a minimum value smaller than p, then the answer to NSV (i) is within
their block, so we go down to their block and ﬁnd sequentially the leftmost
position j where LCP[j] < p. If, however, no sibling of the leaf contains a
minimum smaller than p, we continue going up the tree and considering the
right siblings of the parent of the current node. At some node we ﬁnd a minimum
smaller than p and start traversing down the tree as before, ﬁnding at each level
the ﬁrst child of the current node with a minimum smaller than p. PSV is
symmetric. As the minima in Tm are explicitly stored, the heaviest part of the
cost in practice is the O(L) accesses to LCP cells at the lowest levels.
To calculate RMQ(x, y) we use the same Tm and separate the search in three
parts: (a) We calculate sequentially the minimum value in the interval [x, L(cid:5) x
L(cid:6)−
1] and its leftmost position in the interval; (b) we do the same for the interval
L(cid:8)− 1) using Tm. Finally we compare
[L(cid:7) y
the results obtained in (a), (b) and (c) and the answer will be the one holding
the minimum value, choosing the leftmost to break ties. For each node in Tm we
also store the local position in the children where the minimum occurs, so we
do not need to scan the child blocks when we go down the tree. The extra space
incurred is just n
L log L(1+O(1/L)) bits. The ﬁnal data structure, if L = ω(log n),
requires o(n) bits and can compute NSV , PSV and RMQ all using the same
auxiliary structure. We call it CN -NPR.

L(cid:8), y]; (c) we calculate RMQ(L(cid:5) x

L(cid:6), L(cid:7) y

Experimental Comparison. We tested the performance of the diﬀerent NPR
implementations by performing 100,000 NSV and RMQ queries at diﬀerent random 
positions in the LCP array. Fig. 2 shows the space/time achieved for each
implementation on two texts (the others gave very similar results). We used
the slower Sad-Gon implementation for LCP to enhance the diﬀerences in time
performance. We obtained space/time tradeoﬀs by using diﬀerent block sizes
L = 8, 16, 32 (so the times for RMQ on FMN -NPR are not aﬀected). Clearly
CN -NPR displays the best performance for NSV , both in space and time. For
RMQ, one can see that the best time obtained with CN -NPR dominates, in time
and space, the FMN -NPR curve. Thus CN -NPR is our chosen implementation.

Practical Compressed Suﬃx Trees

101

xml.100MB, NPR

proteins.100MB, NPR

FMN NSV
FMN RMQ
CN NSV
CN RMQ

)
s
d
n
o
c
e
s
o
r
c
m

i

(
 

n
o

i
t

a
r
e
p
o

 

 
r
e
p
e
m
T

i

 2

 4

 6

 8

 10

 12

Bits per character

 350

 300

 250

 200

 150

 100

 50

 0

 0

FMN NSV
FMN RMQ
CN NSV
CN RMQ

 2

 4

 6

 8

 10

 12

Bits per character

)
s
d
n
o
c
e
s
o
r
c
m

i

(
 

n
o

i
t

a
r
e
p
o

 

 
r
e
p
e
m
T

i

 200

 180

 160

 140

 120

 100

 80

 60

 40

 20

 0

 0

Fig. 2. Space/time for the operations NSV and RMQ

5 Our Compressed Suﬃx Tree

Our CST implementation applies our CN -NPR algorithms of Section 4 on top
of some LCP representation from those chosen in Section 3. This solves most
of the tree traversal operations by using the formulas provided by Fischer et
al. [8], which we do not repeat for lack of space. In some cases, however, we have
deviated from the theoretical algorithms for practical considerations.

TDepth: We proceed by brute force using Parent, as there is no practical solution 
in the proposal.

(cid:5)

NSib: There is a bug in the original formula [8] in the case v is the next-tolast 
child of its parent. According to them, N Sib([vl, vr]) ﬁrst obtains its
parent [wl, wr], then checks whether vr = wr (in which case there is no next
sibling), then checks whether wr = vr + 1 (in which case the next sibling is
leaf [wr, wr]), and ﬁnally answers [vr + 1, z− 1], where z = RMQ(vr + 2, wr).
This RMQ is aimed at ﬁnding the end of the next sibling of the next sibling,
but it fails if we are near the end. Instead, we replace it by the faster z =
(i, d) generalizes NSV by ﬁnding the next
NSV
value smaller or equal to d, and is implemented almost like NSV using Tm.
Child: The children are ordered by letter. We need to extract the children
sequentially using F Child and N Sib, to ﬁnd the one descending by the
correct letter, yet extracting the Letter of each is expensive. Thus we ﬁrst
ﬁnd all the children sequentially and then binary search the correct letter
among them, thus reducing the use of Letter as much as possible.
(cid:5)

LAQS(v, d): Instead of the slow complex formula given in the original paper, we
(vr, d)−1].

(vr + 1, LCP[vr + 1]). NSV

(and PSV

use NSV
This is a complex operation we are supporting with extreme simplicity.

): LAQS([vl, vr], d) = [PSV

(vl +1, d), NSV

(cid:5)

(cid:5)

(cid:5)

(cid:5)

LAQT (v, d): There is no practical solution in the original proposal. We proceed
as follows to achieve the cost of d Parent operations, plus sume LAQS ones,
all of which are reasonably cheap. Since SDepth(v) ≥ T Depth(v), we ﬁrst try
v(cid:5) = LAQS(v, d), which is an ancestor of our answer; let d(cid:5) = T Depth(v(cid:5)).
If d(cid:5) = d we are done; else d(cid:5) < d and we try v(cid:5)(cid:5) = LAQS(v, d + (d − d(cid:5))).

102

R. C´anovas and G. Navarro

We compute d(cid:5)(cid:5) = T Depth(v(cid:5)(cid:5)) (which is measured by using d(cid:5)(cid:5) − d(cid:5) P arent
operations until reaching v(cid:5)) and iterate until ﬁnding the right node.

6 Comparing the CST Implementations

We compare all the CST implementations: V¨alim¨aki et al.’s [29] implementation
of Sadakane’s compressed suﬃx tree [27] (CST-Sadakane); Russo’s implementation 
of Russo et al.’s “fully-compressed” suﬃx tree [25] (FCST); and our best
variants. These are called Our CST in the plots. Depending on their LCP representation,
 they are suﬃxed with Sad-Gon, FMN -RRR, DAC , and DAC -Var.
We do not compare some operations like Root and Ancestor because they are
trivial in all implementations; Locate and Count because they depend only on the
underlying compressed suﬃx array (which is mostly orthogonal, thus Letter is
suﬃcient to study it); SLinki because it is usually better to do SLink i times; and
LAQS and LAQT because they are not implemented in the alternative CSTs.
We typically show space/time tradeoﬀs for all the structures, where the space
is measured in bpc (recall that these CSTs replace the text, so this is the overall
space required). The times are averaged over a number of queries on random
nodes. We use four types of node samplings, which make sense in diﬀerent typical 
suﬃx tree traversal scenarios: (a) Collecting the nodes visited over 10,000
traversals from a random leaf to the root (used for Parent, SDepth, and Child
operations); (b) same but keeping only nodes of depth at least 5 (for Letter);
(c) collecting the nodes visited over 10,000 traversals from the parent of a random 
leaf towards the root via suﬃx links (used for SLink and TDepth); and (d)
taking 10,000 random leaf pairs (for LCA). For space limitations, and because
the outcomes are consistent across texts, we show the results of each operation
over one text only, choosing in each case a diﬀerent text. The standard deviation
divided by the average is in the range [0.21,2.56] for CST-Sadakane, [0.97,2.68]
for FCST, [0.65,1.78] for Our CST Sad-Gon, [0.64,2.50] for Our CST FMN -
RRR, [0.59,0.75] for Our CST DAC, and [0.63,0.91] for Our CST DAC -Var.
The standard deviation of the estimator is thus at most 1/100th of that.

Fig. 3 shows space/time tradeoﬀs for six operations. The general conclusion
is that our CST implementation does oﬀer a relevant tradeoﬀ between the two
rather extreme existing variants. Our CSTs can operate within 8–12 bpc (that
is, at most 50% larger than the plain byte-based representation of the text, and
replacing it) while requiring a few hundred microseconds for most operations
(the “small and slow” variants Sad-Gon and FMN -RRR); or within 13–16 bpc
and carry out most operations within a few microseconds (the “large and fast”
variants DAC /DAC -Var). In contrast, the FCST requires only 4–6 bpc (which is,
remarkably, as little as half the space required by the plain text representation),
but takes the order of milliseconds per operation; and Sadakane’s CST takes
usually a few tens of microseconds per operation but requires 25–35 bpc, which
is close to uncompressed suﬃx arrays (not uncompressed suﬃx trees, though).
We remark that, for many operations, our “fast and large” variant takes half
the space of Sadakane’s CST implementation and is many times faster. Exceptions 
are Parent and TDepth, where Sadakane’s CST stores the explicit tree

Practical Compressed Suﬃx Trees

103

sources.100MB, Parent

xml.100MB, Child

CST-Sadakane
Our CST with Sad-Gon 
Our CST with FMN-RRR
Our CST with DAC
Our CST with DAC-Var
FCST

)
s
d
n
o
c
e
s
o
r
c
m

i

(
 

n
o

i
t

a
r
e
p
o

 

 
r
e
p
e
m
T

i

CST-Sadakane
Our CST with Sad-Gon 
Our CST with FMN-RRR
Our CST with DAC
Our CST with DAC-Var
FCST

 100000

 10000

 1000

 5

 10

 15

 20

 25

 30

 35

 100

 5

 10

Bits per character

proteins.100MB, SLink

CST-Sadakane
Our CST with Sad-Gon 
Our CST with FMN-RRR
Our CST with DAC
Our CST with DAC-Var
FCST

 15

 20
Bits per character

dna.100MB, LCA

 25

 30

CST-Sadakane
Our CST with Sad-Gon 
Our CST with FMN-RRR
Our CST with DAC
Our CST with DAC-Var
FCST

 10000

 1000

 100

 10

 1

)
s
d
n
o
c
e
s
o
r
c
m

i

(
 
n
o
i
t
a
r
e
p
o
 
r
e
p
 
e
m
T

i

)
s
d
n
o
c
e
s
o
r
c
m

i

(
 
n
o
i
t
a
r
e
p
o
 
r
e
p
 
e
m
T

i

)
s
d
n
o
c
e
s
o
r
c
m

i

(
 

n
o

i
t

a
r
e
p
o

 

 
r
e
p
e
m
T

i

)
s
d
n
o
c
e
s
o
r
c
m

i

(
 
n
o
i
t
a
r
e
p
o
 
r
e
p
 
e
m
T

i

)
s
d
n
o
c
e
s
o
r
c
m

i

(
 
n
o
i
t
a
r
e
p
o
 
r
e
p
 
e
m
T

i

)
s
d
n
o
c
e
s
o
r
c
m

i

(
 

n
o

i
t

a
r
e
p
o

 

 
r
e
p
e
m
T

i

 10000

 1000

 100

 10

 1

 0.1

 10000

 1000

 100

 10

 1

 1000

 100

 10

 1

 0.1

 1

 0.1

 5

 10

 15

 20
Bits per character

proteins.100MB, SDepth

 25

 30

CST-Sadakane
Our CST with Sad-Gon 
Our CST with FMN-RRR
Our CST with DAC
Our CST with DAC-Var
FCST

 1000

 100

 10

 0.1

 5

 10

 15

 20

 25

Bits per character

dna.100MB, TDepth

CST-Sadakane
Our CST with Sad-Gon 
Our CST with FMN-RRR
Our CST with DAC
Our CST with DAC-Var
FCST

 1e+06

 100000

 10000

 1000

 100

 10

 1

 0.1

 1

 5

 10

 15

 20
Bits per character

sources.100MB, Letter(i)

CST-Sadakane
Our CST with Sad-Gon 
Our CST with FMN-RRR
Our CST with DAC
Our CST with DAC-Var
FCST

 25

 30

 0.01

 5

 10

 15

 20

 25

Bits per character

dna.100MB, full traversal

CST-Sadakane
Our CST with Sad-Gon 
Our CST with FMN-RRR
Our CST with DAC
Our CST with DAC-Var
FCST

 1000

 100

 10

)
s
d
n
o
c
e
s
o
r
c
m

i

(
 

e
d
o
n

 
r
e
p

 

e
m
T

i

 2

 3

parameter i

 4

 5

 1

 5

 10

 15

 20

 25

Bits per character

Fig. 3. Space/time tradeoﬀ performance ﬁgures for several operations, time for
Letter(i) as a function of i, and a full traversal computing SDepth. Note the logscale.

104

R. C´anovas and G. Navarro

topology, and thus takes a fraction of a microsecond. On the other hand, our
CST carries out LAQS (not shown) in the same time of Parent, whereas this is
much more complicated for the alternatives (they do not even implement it). For
Child, where we descend by a random letter from the current node, the times are
higher than for other operations as expected, yet the same happens to all the
implementations. We note that the FCST is more eﬃcient on operations LCA
and SDepth, which are its kernel operations, yet it is still slower than our “small
and slow” variant. Finally, TDepth is an operation where all but Sadakane’s
CST are relatively slow, yet on most suﬃx tree algorithms the string depth is
much more relevant than the tree depth. Our LAQT (v, d) (not shown) would
cost about d times the time of our TDepth.
At the bottom of the ﬁgure we show Letter(i), as a function of i. It depends
only on the CSA structure, and requires either applying i−1 times Ψ, or applying
once SA and SA−1. The former choice is preferred for the FCST and the latter in
Sadakane’s CST. For our CST, using Ψ iteratively was better for these i values,
as the alternative requires around 70 microseconds.

The ﬁgure ﬁnishes with a basic suﬃx tree traversal algorithm: the classical one
to detect the longest repetition in a text. This traverses all of the internal nodes
using FChild and NSib and reports the maximum SDepth. Although Sadakane’s
CST takes advantage of locality, our “large and fast” variant is pretty close using
half the space. Our “small and slow” variant, instead, requires a few hundred
microseconds as expected, yet the FCST has a special implementation for full
traversals and, this time, it beats our slow variant in space and time.

Acknowledgements. We thank Francisco Claude, Johannes Fischer, Rodrigo
Gonz´alez, Juha K¨arkk¨ainen, Susana Ladra, Veli M¨akinen, Simon Puglisi, Lu´ıs
Russo, and Kunihiko Sadakane for code, help to use it, and discussions.

References

1. Abouelhoda, M., Kurtz, S., Ohlebusch, E.: Replacing suﬃx trees with enhanced

suﬃx arrays. J. Discr. Algorithms 2(1), 53–86 (2004)

2. Apostolico, A.: The myriad virtues of subword trees. In: Combinatorial Algorithms

on Words. NATO ISI Series, pp. 85–96. Springer, Heidelberg (1985)

3. Brisaboa, N., Ladra, S., Navarro, G.: Directly addressable variable-length codes. In:
Hyyro, H. (ed.) SPIRE 2009. LNCS, vol. 5721, pp. 122–130. Springer, Heidelberg
(2009)

4. Claude, F., Navarro, G.: Practical rank/Select queries over arbitrary sequences. In:
Amir, A., Turpin, A., Moﬀat, A. (eds.) SPIRE 2008. LNCS, vol. 5280, pp. 176–187.
Springer, Heidelberg (2008)

5. Ferragina, P., Gonz´alez, R., Navarro, G., Venturini, R.: Compressed text indexes:

From theory to practice. ACM J. Exp. Algor. 13, article 12 (2009)

6. Ferragina, P., Manzini, G., M¨akinen, V., Navarro, G.: Compressed representations

of sequences and full-text indexes. ACM TALG 3(2), article 20 (2007)

7. Fischer, J., Heun, V.: A new succinct representation of RMQ-information and
improvements in the enhanced suﬃx array. In: Chen, B., Paterson, M., Zhang, G.
(eds.) ESCAPE 2007. LNCS, vol. 4614, pp. 459–470. Springer, Heidelberg (2007)

Practical Compressed Suﬃx Trees

105

8. Fischer, J., M¨akinen, V., Navarro, G.: Faster entropy-bounded compressed suﬃx

trees. Theor. Comp. Sci. 410(51), 5354–5364 (2009)

9. Geary, R., Rahman, N., Raman, R., Raman, V.: A simple optimal representation

for balanced parentheses. Theor. Comp. Sci. 368, 231–246 (2006)

10. Gonz´alez, R., Grabowski, S., M¨akinen, V., Navarro, G.: Practical implementation

of rank and select queries. In: Proc. 4th WEA (posters), pp. 27–38 (2005)

11. Gonz´alez, R., Navarro, G.: Compressed text indexes with fast locate. In: Ma, B.,
Zhang, K. (eds.) CPM 2007. LNCS, vol. 4580, pp. 216–227. Springer, Heidelberg
(2007)

12. Gusﬁeld, D.: Algorithms on Strings, Trees and Sequences: Computer Science and

Computational Biology. Cambridge University Press, Cambridge (1997)

13. K¨arkk¨ainen, J., Manzini, G., Puglisi, S.J.: Permuted longest-common-preﬁx array.
In: Kucherov, G., Ukkonen, E. (eds.) CPM 2009. LNCS, vol. 5577, pp. 181–192.
Springer, Heidelberg (2009)

14. Kurtz, S.: Reducing the space requirements of suﬃx trees. Soft. Pract. Exp. 29(13),

1149–1171 (1999)

15. Larsson, J., Moﬀat, A.: Oﬀ-line dictionary-based compression. Proc. of the

IEEE 88(11), 1722–1732 (2000)

16. M¨akinen, V., Navarro, G.: Succinct suﬃx arrays based on run-length encoding.

Nordic J. Comp. 12(1), 40–66 (2005)

17. Manber, U., Myers, E.: Suﬃx arrays: a new method for on-line string searches.

SIAM J. Comp., 935–948 (1993)

18. McCreight, E.: A space-economical

suﬃx tree construction algorithm. J.

ACM 32(2), 262–272 (1976)

19. Munro, I.: Tables. In: Chandru, V., Vinay, V. (eds.) FSTTCS 1996. LNCS,

vol. 1180, pp. 37–42. Springer, Heidelberg (1996)

20. Munro, I., Raman, V., Rao, S.: Space eﬃcient suﬃx trees. J. Algor. 39(2), 205–222

(2001)

21. Navarro, G., M¨akinen, V.: Compressed full-text indexes. ACM Comp. Surv. 39(1),

article 2 (2007)

22. Okanohara, D., Sadakane, K.: Practical entropy-compressed rank/select dictionary.

In: Proc. 9th ALENEX (2007)

23. Puglisi, S., Turpin, A.: Space-time tradeoﬀs for longest-common-preﬁx array computation.
 In: Hong, S.-H., Nagamochi, H., Fukunaga, T. (eds.) ISAAC 2008. LNCS,
vol. 5369, pp. 124–135. Springer, Heidelberg (2008)

24. Raman, R., Raman, V., Rao, S.: Succinct indexable dictionaries with applications
to encoding k-ary trees and multisets. In: Proc. 13th SODA, pp. 233–242 (2002)

25. Russo, L., Navarro, G., Oliveira, A.: Fully-Compressed Suﬃx Trees. In: Laber, E.S.,
Bornstein, C., Nogueira, L.T., Faria, L. (eds.) LATIN 2008. LNCS, vol. 4957, pp.
362–373. Springer, Heidelberg (2008)

26. Sadakane, K.: New text indexing functionalities of the compressed suﬃx arrays. J.

Algor. 48(2), 294–313 (2003)

27. Sadakane, K.: Compressed suﬃx trees with full

functionality. Theor. Comp.

Sys. 41(4), 589–607 (2007)

28. Sadakane, K., Navarro, G.: Fully-functional succinct trees. In: Proc. 21st SODA,

pp. 134–149 (2010)

29. V¨alim¨aki, N., Gerlach, W., Dixit, K., M¨akinen, V.: Engineering a compressed suﬃx
tree implementation. In: Demetrescu, C. (ed.) WEA 2007. LNCS, vol. 4525, pp.
217–228. Springer, Heidelberg (2007)

30. Weiner, P.: Linear pattern matching algorithms. In: IEEE Symp. Swit. and Aut.

Theo., pp. 1–11 (1973)

