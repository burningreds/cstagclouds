Dynamic Fully-Compressed Suﬃx Trees

Lu´ıs M.S. Russo1,3,(cid:2), Gonzalo Navarro2,(cid:2)(cid:2), and Arlindo L. Oliveira1

1 INESC-ID / IST, R. Alves Redol 9, 1000 Lisboa, Portugal

aml@algos.inesc-id.pt

2 Dept. of Computer Science, University of Chile

3 Dept. of Computer Science, University of Lisbon, Portugal

gnavarro@dcc.uchile.cl

lsr@di.fc.ul.pt

Abstract. Suﬃx trees are by far the most important data structure
in stringology, with myriads of applications in ﬁelds like bioinformatics,
data compression and information retrieval. Classical representations of
suﬃx trees require O(n log n) bits of space, for a string of size n. This is
considerably more than the n log2 σ bits needed for the string itself, where
σ is the alphabet size. The size of suﬃx trees has been a barrier to their
wider adoption in practice. A recent so-called fully-compressed suﬃx
tree (FCST) requires asymptotically only the space of the text entropy.
FCSTs, however, have the disadvantage of being static, not supporting
updates to the text. In this paper we show how to support dynamic
FCSTs within the same optimal space of the static version and executing
all the operations in polylogarithmic time. In particular, we are able to
build the suﬃx tree within optimal space.

1 Introduction and Related Work

Suﬃx trees are extremely important for a large number of string processing problems.
 Their many virtues have been described by Apostolico [1] and Gusﬁeld [2].
The combinatorial properties of suﬃx trees have a profound impact in the bioinformatics 
ﬁeld, which needs to analyze large strings of DNA and proteins with
no predeﬁned boundaries. This partnership has produced several important results,
 but it has also exposed the main shortcoming of suﬃx trees. Their large
space requirements, together with their need to operate in main memory to be
useful in practice, renders them inapplicable in the cases where they would be
most useful, that is, on large texts.

The space problem is so important that it originated a plethora of research
results, ranging from space-engineered implementations [3] to novel data structures 
that simulate suﬃx trees, most notably suﬃx arrays [4]. Some of those
space-reduced variants give away some functionality in exchange. For example 
suﬃx arrays miss the important suﬃx link navigational operation. Yet, all

(cid:2) Supported by the Portuguese Science and Technology Foundation by grant

SFRH/BPD/34373/2006 and project ARN, PTDC/EIA/67722/2006.

(cid:2)(cid:2) Partially funded by Millennium Institute for Cell Dynamics and Biotechnology,

Grant ICM P05-001-F, Mideplan, Chile.

P. Ferragina and G. Landau (Eds.): CPM 2008, LNCS 5029, pp. 191–203, 2008.
c(cid:2) Springer-Verlag Berlin Heidelberg 2008

192

L.M.S. Russo, G. Navarro, and A.L. Oliveira

these classical approaches require O(n log n) bits, while the indexed string requires 
only n log σ bits (we write log for log2), n being the size of the string
and σ the size of the alphabet. For example the human genome requires 700
Megabytes, while even a space-eﬃcient suﬃx tree on it requires at least 40 Gigabytes 
[5], and the reduced-functionality suﬃx array requires more than 10 Gigabytes.
 This is particularly evident in DNA because log σ = 2 is much smaller
than log n.

These representations are also much larger than the size of the compressed
string. Recent approaches [6] combining data compression and succinct data
structures have achieved spectacular results for the pattern search problem. For
example Ferragina et al. [7] presented an index that requires nHk + o(n log σ)
bits and counts the occurrences of a pattern of length m in time O(m(1 +
(logσ log n)−1)). Here nHk denotes the k-th order empirical entropy of the string
[8], a lower bound on the space achieved by any compressor using k-th order
modeling. As that index is also able of reproducing any text substring, its space
is asymptotically optimal in the sense that no k-th order compressor can achieve
asymptotically less space to represent the text.

It turns out that it is possible to use this kind of data structures, that we
will call compressed suﬃx arrays (CSAs)1, and, by adding a few extra structures,
 support all the operations provided by suﬃx trees. Sadakane presented
the ﬁrst compressed suﬃx tree (CST) [5], adding 6n bits on top of the CSA.
Recently Russo et al. [9] achieved a fully-compressed suﬃx tree (FCST), which
works over the smallest existing CSA [7], adding only o(n log σ) bits to it. Hence
the FCST breaks the Θ(n) extra-bits space barrier and retains asymptotic space
optimality.

Albeit very interesting as a ﬁrst step, the FCST has the limitation of being
static, and moreover of being built from the uncompressed suﬃx tree. CSAs
have recently overcome this limitation, starting with the structure by Chan et
al. [10]. In its journal version this work included the ﬁrst dynamic CST, which
builds on Sadakane’s (static) CST [5] and retains its Θ(n) extra space penalty.
On the other hand, the smallest existing CSA [7] was made dynamic within the
same space by M¨akinen et al. [11], which was recently improved by Gonz´alez et
al. [12] so as to achieve logarithmic time slowdown. In this paper we make the
FCST dynamic by building on this latter dynamic CSA. We retain the optimal
space complexity and polylogarithmic time for all the operations.

A comparison between Chan et al.’s CST and our FCST is shown in Table 
1. Our FCST is not signiﬁcantly slower, yet it requires much less space (e.g.
one can realistically predict 25% of Chan et al.’s CST space on DNA). For the
table we chose the smallest existing dynamic CSA, so that we show the time
complexities that can be obtained within the smallest possible space for both
CSTs.

All these dynamic structures, as well as ours, indeed handle a collection of
texts, where whole texts are added/deleted to/from the collection. Construction
in compressed space is achieved by inserting a text into an empty collection.

1 These are also called compact suﬃx arrays, FM-indexes, etc., see [6].

Dynamic Fully-Compressed Suﬃx Trees

193

Table 1. Comparing compressed suﬃx tree representations. The operations are deﬁned
along Section 2. Time complexities, but not space, are big-O expressions. We give the
generalized performance (assuming Ψ, t, Φ ≥ log n) and an instantiation using δ =
(logσ log n) log n. For the instantiation we also assume σ = O(polylog(n)), and use the
dynamic FM-Index variant of Gonz´alez et al. [12] as the compressed suﬃx array (CSA),
for which the space holds for any k ≤ α logσ(n) − 1 and any constant 0 < α < 1.

Ours
|CSA| + O((n/δ) log n)

Space in bits

SDep
Count/ Ancestor
Parent
SLink
SLinki
Letter / Locate
LCA
FChild/ NSib

Child

WeinerLink
Insert(T ) /
Delete(T )

= (logσ log n) log2 n Ψδ
= log n 1
= log n (Ψ + t)δ
= log n (Ψ + t)δ

Chan et al. [10]
|CSA| + O(n) + o(n)
= nHk + O(n) + o(n log σ)
Φ
log n
log n
Ψ
Φ
Φ
log n
log n

= nHk + o(n log σ)
= (logσ log n) log2 n
= 1
= (logσ log n) log2 n
= (logσ log n) log2 n
= (logσ log n) log2 n Φ + (Ψ + t)δ = (logσ log n) log2 n
= (logσ log n) log2 n
= (logσ log n) log2 n Φ
= (logσ log n) log2 n
= log n (Ψ + t)δ
= log n (Ψ + t)δ + Φ log δ + (log n) log(n/δ)
= ((logσ log n) log2 n) log log n
Φ log σ = (log log n) log2 n (Ψ + t)δ + Φ log δ + (log n) log(n/δ)
= ((logσ log n) log2 n) log log n
= log n
|T|(Ψ + t)δ = |T|(logσ log n) log2 n

t
|T|(Ψ + t)δ

= log n t
= |T|(logσ log n) log2 n

2 Basic Concepts

Fig. 1 illustrates the concepts in this section. We denote by T a string; by Σ the
alphabet of size σ; by T [i] the symbol at position (i mod n) (so the ﬁrst symbol
(cid:3) concatenation; by T = T [..i− 1].T [i..j].T [j + 1..] respectively
is T [0]); by T.T
a preﬁx, a susbtring and a suﬃx; by Parent(v) the parent node of node v;
(cid:3)) whether v is an ancestor of v
(cid:3);
by TDep(v) its tree-depth; by Ancestor(v, v
(cid:3)) the lowest common ancestor.
by LCA(v, v

The path-label of a node v in a labeled tree is the concatenation of the
edge-labels from the root down to v. We refer indiﬀerently to nodes and to their
path-labels, also denoted by v. The i-th letter of the path-label is denoted as
Letter(v, i) = v[i]. The string-depth of a node v, denoted by SDep(v), is the
length of its path-label. Child(v, X) is the node that results of descending from
v by the edge whose label starts with symbol X, if it exists. The suﬃx tree of T
is the deterministic compact labeled tree for which the path-labels of the leaves
are the suﬃxes of T . We assume that T ends in a terminator symbol $ that does
not belong to Σ. The generalized suﬃx tree of a collection C of texts is the
tree that results from merging the respective suﬃx trees. Moreover each text is
assumed to have a distinct terminator. For a detailed explanation see Gusﬁeld’s
book [2]. The suﬃx-link of a node v (cid:2)= Root of a suﬃx tree, denoted SLink(v),
is a pointer to node v[1..]. Note that SDep(v) of a leaf v identiﬁes the suﬃx of T

194

L.M.S. Russo, G. Navarro, and A.L. Oliveira

a
b

b

b

b

b
a
b
$

a
b
$

a
b
$

b
a
b
$

$

$

$

2
0

3
5

1
4

5
2

4
3

0
6
6A:
1
Fig. 1. Suﬃx tree T of string
abbbab, with the leaves numbered.
The arrow shows the SLink between 
node ab and b. Below it we
show the suﬃx array. The portion
of the tree corresponding to node
b and respective leaves interval is
within a dashed box. The sampled
nodes have bold outlines.

b

a

b

$

b

a

b

b

b

a

0

3

1

4

5

6

2

Fig. 2.
Reverse
tree T R

1

2

i: 01 234 56 7890 12 345 67 8901
((0)((1)(2))((3)(4)((5)(6))))

B: 1 0
( 0

i: 0

B: 1 0
( 0

i: 0

0
1

0
1

0
2

0
2

101101 0
(3)(4) 5
1 23 4

0
6

1
)
5

1101101 0
((3)(4) 5
12 34 5

0 11
6 ))
67

Fig. 3. Parentheses representations 
of trees. The parentheses
on top represent the suﬃx tree,
those in the middle the sampled
tree, and those on the bottom the
sampled tree when b is also sampled 
along with the B bitmap.
The numbers are not part of the
representation; they are shown
for clarity. The rows labeled i:
give the index of the parentheses.

starting at position n−SDep(v) = Locate(v). For example T [Locate(ab$)..] =
T [7 − 3..] = T [4..] = ab$. The suﬃx array A[0, n − 1] stores the Locate
values of the leaves in lexicographical order. Note that in a generalized suﬃx
tree Locate must also identify the text to which the suﬃx corresponds. When
−1 they are computed within
we use arithmetic expressions involving A and A
a given text, i.e. they do not jump to another text. Moreover for simplicity we
use only one text in our example and hence omit the text identiﬁer. The suﬃx
tree nodes can be identiﬁed with suﬃx array intervals: each node corresponds to
the range of leaves that descend from v. The node b corresponds to the interval
[3, 6]. Hence the node v will be represented by the interval [vl, vr]. Leaves are also
represented by their left-to-right index (starting at 0). For example by vl − 1 we
refer to the leaf immediately before vl, i.e. [vl−1, vl−1]. With this representation
we can Count in constant time the number of leaves that descend from v. The
number of leaves below b is 4 = 6 − 3 + 1. This is precisely the number of times
that the string b occurs in the indexed string T . We can also compute Ancestor
in O(1) time: Ancestor(v, v

(cid:3)) ⇔ vl ≤ v

≤ vr.

≤ v

(cid:3)
r

(cid:3)
l

3 Static Fully-Compressed Suﬃx Trees and Our Plan

In this section we brieﬂy explain the static FCST we build on [9]. The FCST
consists of a compressed suﬃx array, a δ-sampled tree S, and mappings between
these structures. We also give the road map of our plan to dynamize the FCST.

Compressed Suﬃx Arrays (CSAs) are compact and functional representations 
of suﬃx arrays [6]. Apart from the basic functionality of retrieving

Dynamic Fully-Compressed Suﬃx Trees

195

−1[A[v] + i], let us assume that the CSA can also compute A

A[i] = Locate(i) (within a time complexity that we will call Φ = Ω(log n)),
state-of-the-art CSAs support operation SLink(v) for leaves v. This is called
ψ(v) in the literature: A[ψ(v)] = A[v] + 1, and thus SLink(v) = ψ(v), let
its time complexity be Ψ = Ω(log n). The iterated version of ψ, denoted ψi,
can usually be computed faster than O(iΨ) with CSAs. This is achieved as
−1 within
ψi(v) = A
O(Φ) time. CSAs might also support the WeinerLink(v, a) operation [13]: for a
node v the WeinerLink(v, X) gives the suﬃx tree node with path-label X.v[0..].
This is called the LF mapping in CSAs, and is a kind of inverse of ψ, let its time
complexity be t = Ω(log n). Consider the interval [3, 6] that represents the leaves
whose path-labels start by b. In this case we have that LF(a, [3, 6]) = [1, 2], i.e.
by using the LF mapping with a we obtain the interval of leaves whose pathlabels 
start by ab. We extend of LF to strings, LF(X.Y, v) = LF(X, LF(Y, v)).
CSAs also implement Letter(v, i) for leaves v. The easiest case is the ﬁrst
letter of a given suﬃx, Letter(v, 0) = T [A[v]]. This corresponds to v[0], the
ﬁrst letter of the path-label of leaf v. Dynamic CSAs implement v[0] in time
O(log n). In general, Letter(v, i) = Letter(SLinki(v), 0) is implemented in
O(Φ) time. CSAs are usually self-indexes, meaning that they replace the text:
they can extract any substring, of size (cid:10), of the indexed text in O(Φ + (cid:10)Ψ) time.
In this paper we will use a dynamic CSA for this part [12], which implements
these operations with logarithmic slowdown to its static version [7]. The dynamic
CSA actually handles a collection of texts, where insertions and deletions of
whole texts T are carried out in time O(|T|(Ψ + t)).
The δ-Sampled Tree exploits the property that suﬃx trees are self-similar,
(cid:3))) whenever the expressions are
SLink(LCA(v, v
well deﬁned. This means, roughly, that the tree structure below SLink(v) contains 
the tree structure below v. Because of this regularity it is possible to store
only a few sampled nodes instead of the whole suﬃx tree. A δ-sampled tree S,
from a suﬃx tree T of Θ(n) nodes, chooses O(n/δ) nodes such that, for each
node v, node SLinki(v) is sampled for some i < δ. Such a sampling can be
obtained by choosing nodes with SDep(v) ≡δ/2 0 such that there is another
(cid:3)). For such a sampling Lemma 1 holds, where
node v
LCSA(v, v
Lemma 1. Let v, v
d = min(δ, r + 1). Then
SDep(LCA(v, v
By itself however this property leads to an entangled loop of operations, because
LCA depends on SLink and SLink(v) = LCA(ψ(vl), ψ(vr)) depends on LCA.
}) we
Using CSAs and observing that LCA(v, v
(cid:3))) = max0≤i<d{i + SDep(LCSA(
can simplify this equation to SDep(LCA(v, v
ψi(min{vl, v
Therefore the kernel operations can be computed as:
SDep(v) = SDep(LCA(v, v)) = max0≤i<d{i + SDep(LCSA(ψi(vl), ψi(vr)))}
LCA(v, v

(cid:3))) = max0≤i<d{i + SDep(LCSA(SLinki(v), SLinki(v

(cid:3) be nodes such that SLinkr(LCA(v, v

(cid:3))) = Root, and let

(cid:3)) is the lowest common sampled ancestor of v and v

(cid:3):

(cid:3)) = LF(v[0..i − 1], LCSA(ψi(min{vl, v

(cid:3))) = LCA(SLink(v), SLink(v

(cid:3) for which v = SLinkδ/2(v

(cid:3)) = LCA(min{vl, v

}), ψi(max{vr, v

}), ψi(max{vr, v

})))

(cid:3)
r

(cid:3)
l

(cid:3)
l

})))}.

(cid:3)
r

(cid:3))))}.

}, max{vr, v

(cid:3)
r

(cid:3)
l

196

L.M.S. Russo, G. Navarro, and A.L. Oliveira

from which SLink is obtained as well. The i in the last equation is the one
(cid:3))). Operation Parent(v) is easily computed
that maximizes SDep(LCA(v, v
on top of LCA. These operations take time O((Ψ + t)δ), except that SDep
takes O(Ψ δ).

Note that we have to solve LCSA. This requires to solve LCAS, that is, LCA
queries on the sampled tree S, and also to map nodes to sampled nodes using
operation LSA (see later). The sampled tree also needs to solve ParentS and
store SDepT . The rest is handled by the CSA.
For the dynamic version, we ﬁrst show how the suﬃx tree T changes upon
insertion and deletion of texts T to the collection. Then we show how to maintain
the sampling properties of S under those updates of the (virtual) T . This will
require some more data to be stored in the sampled nodes. Finally, we will make
use of a dynamic parentheses representation for the sampled tree, which will
already give us LCAS and ParentS, as well as a way to associate data to
nodes and insert/delete nodes. Note that we just have to show how to provide
this basic tree functionality, as the remaining operations are obtained as in the
static version.
To support TDep, however, they add other O(n/δ) nodes to the sampling,
such that for any node v the node Parentj(v) is sampled, for some 0 ≤ j <
δ. We have not found a way to eﬃciently maintain this second sampling in
a dynamic scenario. As a consequence, our dynamic FCST does not support
operation TDep nor those that require it [9]: LAQt and LAQs. The basic
navigation operations FChild and NSib also require TDep, but we will present
a diﬀerent idea that solves them together with Child and a generalization of it,
using just the CSA.

Mapping Between the CSA and the Sampled Tree. For every node v of
the sampled tree we need to obtain the corresponding interval [vl, vr]. On the
other hand, given a CSA interval [vl, vr] representing node v of T , the lowest
sampled ancestor LSA(v) gives the lowest sampled tree node containing v. With
LSA we can compute LCSA(v, v

(cid:3)) = LCAS(LSA(v), LSA(v

(cid:3))).

In this paper we introduce a new method to implement these mappings that

is eﬃcient and simpler than the one presented in the static version [9].

4 Updating the Suﬃx Tree and Its Sampling

In this section we explain how to modify a suﬃx tree to reﬂect changes caused
by inserting and removing a text T to/from the suﬃx tree.

The CSA of M¨akinen et al. [11], on which we build, inserts T in right-to-left
order. It ﬁrst determines the position of the new terminator2 and then uses LF

2 This insertion point is arbitrary in that CSA, thus there is no order among the texts.
Moreover, all the terminators are the same in the CSA, yet it can be easily modiﬁed
to handle diﬀerent terminators.

Dynamic Fully-Compressed Suﬃx Trees

197

to ﬁnd the consecutive positions of longer and longer suﬃxes, until the whole
T is inserted. This right-to-left method perfectly matches with Weiner’s algorithm 
[13] to build the suﬃx tree of T : it ﬁrst inserts suﬃx T [i + 1..] and then
suﬃx T [i..], ﬁnding the points in the tree where the node associated to the new
suﬃx is to be created if it does not already exist. The node is found by using
Parent until the WeinerLink operation returns a non-empty interval. This
requires one Parent and one WeinerLink amortized operation per symbol
of T . This algorithm has the important invariant that the intermediate data
structure is a suﬃx tree. Hence, by carrying it out in synchronization with the
CSA insertion algorithm, we can use the current CSA to implement Parent
and WeinerLink.
To maintain the property that the intermediate structure is a suﬃx tree,
deletion of a text T must proceed by ﬁrst locating the node of T that corresponds
to T , and then using SLinks to remove all the nodes corresponding to its suﬃxes
in T . We must simultaneously remove the leaves in the CSA (M¨akinen et al.’s
CSA deletes a text right-to-left as well, but it is easy to adapt to use Ψ instead
of LF to do it left-to-right).
We now explain how to update the sampled tree S whenever nodes are inserted
or deleted from the (virtual) suﬃx tree T . The sampled tree must maintain, at all
times, the property that for any node v there is an i < δ such that SLinki(v) is
sampled. The following concept from Russo et al. [14] is fundamental to explain
how to obtain this result.
Deﬁnition 1. The reverse tree T R of a suﬃx tree T is the minimal labeled
tree that, for every node v of T , contains a node vR denoting the reverse string
of the path-label of v.
We note we are not maintaining nor sampling T R, we just use it as a conceptual
device. Fig. 2 shows a reverse tree. Observe that since there is a node with pathlabel 
ab in T there is a node with path-label ba in T R. We can therefore deﬁne
a mapping R that maps every node v to vR. Observe that for any node v of
T , except for the Root, we have that SLink(v) = R
−1(Parent(R(v))). This
mapping is partially shown in Figs. 1 and 2 by the numbers. Hence the reverse
tree stores the information of the suﬃx links. By Height(vR) we refer to the
distance between v and its farthest descendant leaf. For a regular sampling we
choose the nodes for which TDep(vR) ≡δ/2 0 and Height(vR) ≥ δ/2. This
is equivalent to our sampling rules on T (Section 3): Since the reverse suﬃxes
form a preﬁx-closed set, T R is a non-compact trie, i.e. each edge is labeled by a
single letter. Thus, SDep(v) = TDep(vR). The rule for Height(vR) is obviously
related to that on SLink(v) by R. See Fig. 2 for an example of this sampling.
Likewise, stating that there is an i < δ for which SLinki(v) is sampled is the
same as stating that there is an i < δ for which TDep(Parenti(vR)) ≡δ/2 0 and
Height(Parenti(vR)) ≥ δ/2 . Since TDep(Parenti(vR)) = TDep(vR) − i,
the ﬁrst condition holds for exactly two i’s in [0, δ[. Since Height is strictly
increasing the second condition holds for sure for the largest i. Notice that since
every sampled node has at least δ/2 descendants that are not sampled, this
means that we sample at most (cid:7)4n/δ(cid:8) nodes from a suﬃx tree with ≤ 2n nodes.

198

L.M.S. Russo, G. Navarro, and A.L. Oliveira

Notice that whenever a node is inserted or removed from a suﬃx tree it never
changes the SDep of the other nodes in the tree, hence it does not change any
TDep in T R. This means that whenever the suﬃx tree is modiﬁed the only
nodes that can be inserted or deleted from the reverse tree are the leaves. In T
this means that when a node is inserted it does not break a chain of suﬃx links;
it is always added at the beginning of such a chain. Weiner’s algorithm works
precisely by appending a new leaf to a node of T R.

(cid:3))R is exactly δ/2 and TDep((v

(cid:3) meets the sampling condition and we sample it.

Assume that we are using Weiner’s algorithm and decide that the node X.v
should be added and we know the representation of node v. All we need to do
to update the structure of the sampled tree is to verify that if by adding (X.v)R
as a child of vR in T R we increase the Height of some of ancestor, in T R, that
will now become sampled. Hence we must scan upwards in T R to verify if this
is the case. Notice that we already carry out this scanning as a side eﬀect of
computing SLink(v), which also gives us the required SDep information. Also,
we do not need to maintain Height values. Instead, if the distance from (X.v)R
(cid:3))R) ≡δ/2 0, then
to the closest sampled node (v
we know that v
Deleting a node (i.e. a leaf in T R) is slightly more complex and involves some
reference counting. This time assume we are deleting node X.v, again we need to
scan upwards, this time to decide whether to make a node non-sampled. However
(cid:3)R) ≥ δ/2
SDep(v)− SDep(v
(cid:3) counts how
because of some other descendant. Therefore every sampled node v
many descendants it has at distance δ/2. A node becomes non-sampled only when
this counter reaches zero. Insertions and deletions of nodes in T must update
these counters, by increasing/decreasing them whenever inserting/deleting a leaf
at distance exactly δ/2 from nodes.

(cid:3)) < δ/2 is not enough, as it may be that Height(v

Hence to Insert or Delete a node requires O((Ψ + t)δ) time, plus the
time to manipulate the structure that holds the topology of S: we need to
carry out insertions/deletions of nodes, while maintaining information associated 
to them (SDep, reference counts). Section 5.2 shows that those operations 
do not dominate the time O((Ψ + t)δ) needed to maintain the sampling
conditions.

5 Dynamic Fully-Compressed Suﬃx Trees

In this section we present the compact data structures we use and create to
handle our dynamic structures: the CSA, the sampled tree, and the mappings.

5.1 Dynamic Compressed Suﬃx Arrays

To maintain a dynamic CSA we use the following result by Gonz´alez et al. [12],
which is an improvement upon those of M¨akinen et al. [11]:
Theorem 1. A dynamic CSA over a collection C of texts can be stored
within nHk(C) + o(n log σ) bits, for any k ≤ α logσ(n) − 1 and any constant
0 < α < 1, supporting all the operations with times t = Ψ = O(((logσ log n)−1 +

Dynamic Fully-Compressed Suﬃx Trees

199

(cid:3)

(cid:2)

1) log n), Φ = O((logσ log n) log2 n), and inserting/deleting texts T in time
O(|T|(t + Ψ)).
Note that for a collection with p texts it is necessary to store the positions of the
texts in A. This requires O(p log n) bits but it is not an issue unless the texts
are very short [11].

Therefore, the problem of maintaining a dynamic CSA is already solved, except 
that we promised to support operation ChildT (and some derivatives) directly 
on the CSA. Indeed, ChildT (v, X) can be easily computed in O(Φ log n)
(cid:3) where
time by binary searching for the interval of v = [vl, vr] formed by those v
Letter(v
, SDep(v) + 1) = X. Similarly, FChild(v) can be determined by
computing X = Letter(vl, SDep(v) + 1) and then ChildT (v, X). To compute
(cid:3)
NSib(v) the process is similar: If Parent(v) = [v
r > vr, then we
(cid:3)
compute X = Letter(v
r + 1, SDep(v) + 1) and do ChildT (v, X). All the time
complexities are thus dominated by that of ChildT .
Now we show how ChildT can be computed in a more general and eﬃcient
way. The generalized branching for nodes v1 and v2 consists in determining the
node with path-label v1.v2 if it exists. A simple solution is to binary search the
(cid:3)) ∈ v2, where m =
interval of v1 for the sub-interval of the v
SDep(v1). This approach requires O(Φ log n) time and it was ﬁrst considered
using CSA’s by Huynh et al. [15]. Thus we are able to generalize ChildT (v, X),
which uses v2 as the sub-interval of A of the suﬃxes starting with X.

(cid:3)’s such that ψm(v

(cid:3)
r] and v

(cid:3)
l, v

−1. Thus, we could sample A and A

This general solution can be improved by noticing that we are using SLinki
at arbitrary positions of the CSA for the binary search. Recall that SLinki is
−1 regularly so as to
solved via A and A
store their values explicitly. That is, we explicitly store the values A[jδ] and
−1[jδ] for all j. To solve a generalized branching, we start by building a table
A
of ranges D[0] = v2 and D[i] = LF(v1[m − i..m − 1], v2), for 1 ≤ i < δ. If
m < δ the answer is D[m]. Otherwise, we binary search the interval of v1,
accessing only the sampled elements of A. To determine the branching we should
−1[A[jδ] + m] for some jδ values in v1. To use the cheaper
compute ψm(jδ) = A
−1 as well, we need that A[jδ] + m be divisible by δ, thus we instead
sampled A
(cid:3) = (cid:7)(A[jδ] + m)/δ(cid:8)δ − A[jδ]. Hence instead of verifying that
compute ψm
ψm(jδ) ∈ v2, we verify that ψm
(cid:3)]. After this process we still have
to binary search an interval of size O(δ), which is carried out naively.

(cid:2) ∈ D[m − m

for m

The overall process requires time O(Φ + (Ψ + t)δ) to access the last letters
of v1 and build D, plus O((log n) log(n/δ)) for binary searching the samples;
plus O(Φ log δ) for the ﬁnal binary searches. We have assumed O(log n) time to
−1 values in a dynamic scenario, whereas in a static
access the sampled A and A
scenario3 it would be O(1).

In fact in a dynamic scenario we do not store exactly the A[jδ] values; instead
(cid:3)] is
we guarantee that for any k there is a k
(cid:3) to use
sampled, and the same for A
can be easily obtained in O(log n) time. Those sampled sequences are not hard to
maintain. For example, M¨akinen et al. [11, Sec. 7.1 of journal version] describes

−1. Still the sampled elements of A and the m

(cid:3) such that k − δ < k

(cid:3) ≤ k and A[k

3 This speedup immediatly improves the results of Huynh et al. [15].

200

L.M.S. Russo, G. Navarro, and A.L. Oliveira

−1 (called SC in there), and essentially how to maintain A
how to maintain A
(called SA in there; the only missing point is to maintain approximately spaced
samples in A, which can be done exactly as for A

−1).

5.2 Dynamic Sampled Trees
The sampled tree contains only O(n/δ) nodes. As such it could be stored with
pointers using only O((n/δ) log n) bits. Instead we use a dynamic parentheses
data structure given by Chan et al. [10], which already supports LCA.
Theorem 2. A list of O(n/δ) balanced parentheses can be maintained in O(n/δ)
bits supporting the following operations in O(log n) time:
– FindMatch(u), ﬁnds the matching parenthesis of u;
– Enclose(u), ﬁnds the nearest pair of matching parentheses that encloses u;
(cid:3)), ﬁnds the nearest pair of parentheses that encloses
– DoubleEnclose(u, u
(cid:3)), inserts or deletes the matching parentheses

both u and u
– Insert(u, u

(cid:3);
(cid:3)), Delete(u, u
(cid:3).

located at u, u

The Enclose primitive computes ParentS in the sampled tree. Likewise the
DoubleEnclose primitive computes the LCAS operation. In Section 5.3 we
explain how to update the parentheses sequence when a node becomes sampled
or non-sampled (i.e. , how to maintain the mapping with the CSA). Operations
Rank and Select on the sequence of parentheses S can also be used to store information 
on the nodes, by mapping between the parentheses sequence and their
preorder values and vice versa: Rank(cid:2)((cid:2)(S, i) gives the preorder number of the
node identiﬁed by the opening parenthesis at S[i], while Select(cid:2)((cid:2)(S, j) identiﬁes 
the j-th node (in preorder) in S. Rank and Select over the parentheses
bitmap can be handled using the following theorem.
Theorem 3 ([11]). A bitmap of n bits supporting Rank, Select, Insert and
Delete in O(log n) time can be maintained in nH0 + O(n/
Each node of S must also store its SDep. This is not complicated because the SDep
of the nodes of T does not change, at least using Weiner’s algorithm. Thus we maintain 
a balanced tree where the SDep values can be read, inserted, and deleted, at
the positions given by Rank(cid:2)((cid:2)(S, i). When a node becomes sampled/non-sampled
we insert/delete in this sequence. A similar mechanism is used to store the reference 
counts used for the sampling; in this case the stored values can be modiﬁed as
well. Thus O(log n) time suﬃces for simulating the tree operations on S.

log n) bits.

√

5.3 Mapping from CSA to the Sampled Tree and Back
The lowest sampled ancestor LSA is the way to map from the CSA to S. LSA
is computed by using an operation Reduce(v), that receives the numeric representation 
of leaf v and returns the position, in the parentheses representation
of the sampled tree, where that leaf should be. Consider for example the leaf
numbered 5 in Fig. 3. This leaf is not sampled, but in the original tree it appears
somewhere between leaf 4 and the end of the tree, more speciﬁcally between

Dynamic Fully-Compressed Suﬃx Trees

201

parenthesis ’)’ of 4 and parenthesis ’)’ of the Root. We assume Reduce returns
the ﬁrst parenthesis, i.e. Reduce(5) = 4. In this case since the parenthesis we
obtain is a ’)’ we know that LSA should be the parent of that node. Hence we
compute LSA as follows:

(cid:2)

LSA(v) =

Reduce(v)
Parent(Reduce(v)) , otherwise

, if S[Reduce(v)] = (cid:3)((cid:3)

We present a new way to compute Reduce in O(log n) time and o(n) bits
(cf. [9]). We use a bitmap B initiated with n bits all equal to 0. Now for every
node v = [vl, vr] we insert a 1 at Select0(B, vl) and after Select0(B, vr), which
yields a bitmap with n + O(n/δ) bits. In our example it is 1000101101001, see
Fig. 3. Hence we have the following relation Reduce(v) = Rank1(B, Select0(
B, v + 1)) − 1. We do not store B uncompressed, but rather using Theorem 3,
which requires only O((n/δ) log n) bits as there are few 1’s in B. When a node
[vl, vr] becomes sampled we insert matching parentheses at S[Reduce(vl)] and
after S[Reduce(vr)]. Also, it is necessary to insert the new 1’s in B as before.
Fig. 3 illustrates the eﬀect of sampling b = [3, 6].

Updating S when a sampled node v becomes non-sampled is easy, as we can
(cid:3) to delete. We must also delete the corresponding
obtain the parentheses u, u
1’s in B; note that the relative position of a 1 in a run of 1’s is irrelevant.
Therefore Reduce can be computed in O(log n) time. According to our previous
explanation, so can LSA and LCSA, for leaves.

To map in the other direction, each node in the sampled tree must know
its corresponding interval [vl, vr]. This is also easy to obtain from B. Let u
be the position in S of the opening parenthesis that identiﬁes sampled node
(cid:3) = FindMatch(u). Now vl =
v. The corresponding closing parenthesis is u
Rank0(B, Select1(B, u + 1)) and vr = Rank0(B, Select1(B, u

(cid:3) + 1)) − 1.

6 Putting All Together

The following theorem summarizes our result.

Theorem 4. It is possible to represent the suﬃx tree of a dynamic text collection
within the space and time bounds given in Table 1. The space and the variables
Ψ, Φ, t, can be instantiated to the values of Theorem 1 for δ = ω(logσ n), or to
−1, LF, and T [A[v]], in times O(Ψ),
another dynamic CSA supporting ψ, A, A
O(Φ), O(Φ), O(t), and O(log n), respectively, provided texts are inserted in right-
to-left order and deleted in left-to-right order within the given time bounds.
We note that Theorem 4 assumes that (cid:11)log n(cid:12) is ﬁxed, and so is δ. This assumption 
is not uncommon in dynamic data structures, even if it aﬀects assertions
like that of pointers taking O(log n) bits. The CSA used in Theorem 1 can handle 
varying (cid:11)log n(cid:12) within the same worst-case space and complexities, and the
same happens with Theorem 3, which is used for the mapping bitmap B. The
only remaining part is the sampled tree. We discuss now how to cope with it
while retaining the same space and worst-case time complexities.

202

L.M.S. Russo, G. Navarro, and A.L. Oliveira

− with δ

We use δ = (cid:11)log n(cid:12)·(cid:11)logσ

(cid:11)log n(cid:12)(cid:12), which will change whenever (cid:11)log n(cid:12) changes
(sometimes will change by more than 1). Let us write δ = Δ((cid:10)) = (cid:10)(cid:11)logσ (cid:10)(cid:12). We
maintain (cid:10) = (cid:11)log n(cid:12). As S is small enough, we can aﬀord to maintain three
− = Δ((cid:10) − 1), and S+ sampled with
copies of it: S sampled with δ, S
δ+ = Δ((cid:10) + 1). When (cid:11)log n(cid:12) increases (i.e. n doubles), S
− is discarded, the
−, the current S+ becomes S, we build a new S+ sampled
current S becomes S
with Δ((cid:10) + 2), and (cid:10) is increased. A symmetric operation is done when (cid:11)log n(cid:12)
decreases (i.e. n halves due to deletions), so let us focus on increases from now
on. Note this can occur in the middle of the insertion of a text, which must be
suspended, and then resumed over the new set of sampled trees.
The construction of the new S+ can be done by retraversing all the suﬃx
tree T deciding which nodes to sample according to the new δ+. An initially
empty parentheses sequence and a bitmap B initialized with zeros would give
the correct insertion points from the chosen intervals, as both structures are
populated. To ensure that we consider each node of T once, we process the
leaves in order (i.e. v = [0, 0] to v = [n − 1, n − 1]), and for each leaf v we
also consider all its ancestors [vl, vr] (using ParentT ) as long as vr = v. For
each node [vl, vr] we consider, we apply SLink at most δ+ times until either
(cid:3) = SLinki([vl, vr]) which either is sampled in S+, or
we ﬁnd the ﬁrst node v
(cid:3) was not sampled we insert it into S+, and
SDep(v
in both cases we increase its reference count if i = δ+/2 (recall Section 4).
All the δ+ suﬃx links in T are computed in O(δ+(Ψ + t)) time, as they form a
single chain. Therefore the solution maintains the current complexities, yet only
in an amortized sense.

(cid:3)) ≡δ+/2 0 and i ≥ δ+/2. If v

Deamortization can be achieved by the classical method of interleaving the
normal operations of the data structure with the construction of the new S+.
By performing a constant number of operations on the new S+ for each inser-
tion/deletion operation over C, we can ensure that the new S+ will be ready in
time. The challenge is to maintain the consistency of the traversal of T while
texts are inserted/deleted.
As we insert a text, the operations that update T consist of insertion of leaves,
and possibly creation of a new parent for them. Assume we are currently at node
[vl, vr] in our traversal of T to update S+. If a new node [v
(cid:3)
(cid:3)
r] we are inserted
l, v
(cid:3)
(cid:3)
r = vr and
r < vr, or v
is behind the current node in our traversal order (that is, v
(cid:3)
(cid:3)
(cid:3)
r] immediately; otherwise we leave this for the
l > vl), then we consider [v
v
l, v
(cid:3)
(cid:3)
r] in our traversal. Recall from Section 4 that
moment when we will reach [v
l, v
those new insertions do not aﬀect the existing SDeps nor suﬃx link paths, and
hence can be considered independently of the current traversal process. Similarly,
deleted nodes that fall behind the current node are processed immediately, and
the others left for the traversal to handle it.

If (cid:10) decreases while we are still building S+, we can discard it even before
having completed its construction. Note that in general discarding a tree when (cid:10)
changes involves freeing several data structures. This can also be done progressively,
 interleaved with the other operations.

Dynamic Fully-Compressed Suﬃx Trees

203

7 Conclusions
We presented the ﬁrst dynamic fully-compressed representation of suﬃx trees
(FCSTs). Static FCSTs broke the Θ(n) bits barrier of previous representations
at a reasonable (and in some cases no) time complexity penalty, while retaining
a surprisingly powerful set of operations. Dynamic FCSTs permit not only managing 
dynamic collections, but also building static FCSTs within optimal space.
Hence the way is open to practical implementations of this structure, which can
run in main memory for very large texts.

We also gave some relevant results for the static case, as we improved or
simpliﬁed the operations Reduce and Child. A challenge for future work is to
obtain operations TDep, LAQt, and LAQs, which we were not able to maintain
in a dynamic scenario.
Acknowledgments. We are grateful to Veli M¨akinen and Johannes Fisher for
pointing out the generalized branching problem to us.

References

1. Apostolico, A.: Combinatorial Algorithms on Words. In: The myriad virtues of

subword trees. NATO ISI Series, pp. 85–96. Springer, Heidelberg (1985)

2. Gusﬁeld, D.: Algorithms on Strings, Trees and Sequences. Cambridge University

Press, Cambridge, UK (1997)

3. Giegerich, R., Kurtz, S., Stoye, J.: Eﬃcient implementation of lazy suﬃx trees.

Softw. Pract. Exper. 33(11), 1035–1049 (2003)

4. Manber, U., Myers, E.W.: Suﬃx arrays: A new method for on-line string searches.

SIAM J. Comput. 22(5), 935–948 (1993)

5. Sadakane, K.: Compressed suﬃx trees with full functionality. Theory Comput.

Syst. 41, 589–607 (2007), http://dx.doi.org/10.1007/s00224-006-1198-x

6. Navarro, G., M¨akinen, V.: Compressed full-text indexes. ACM Comp. Surv. 39(1),

7. Ferragina, P., Manzini, G., M¨akinen, V., Navarro, G.: Compressed representations

of sequences and full-text indexes. ACM Trans. Algor. 3(2), 20 (2007)

8. Manzini, G.: An analysis of the Burrows-Wheeler transform. J. ACM 48(3), 407–

2 (2007)

430 (2001)

9. Russo, L., Navarro, G., Oliveira, A.: Fully-Compressed Suﬃx Trees. In: LATIN.

LNCS, vol. 4957, pp. 362–373. Springer, Heidelberg (2008)

10. Chan, H.-L., Hon, W.-K., Lam, T.-W., Sadakane, K.: Compressed indexes for dynamic 
text collections. ACM Trans. Algorithms 3(2) (2007)

11. M¨akinen, V., Navarro, G.: Dynamic entropy-compressed sequences and full-text
indexes. In: Lewenstein, M., Valiente, G. (eds.) CPM 2006. LNCS, vol. 4009, pp.
307–318. Springer, Heidelberg (to appear in ACM TALG, 2006)

12. Gonz´alez, R., Navarro, G.: Improved dynamic rank-select entropy-bound structures.
 In: LATIN. LNCS, vol. 4957, pp. 374–386. Springer, Heidelberg (2008)

13. Weiner, P.: Linear pattern matching algorithms. In: IEEE Symp. on Switching and

Automata Theory, pp. 1–11 (1973)

14. Russo, L., Oliveira, A.: A compressed self-index using a Ziv-Lempel dictionary. In:
Crestani, F., Ferragina, P., Sanderson, M. (eds.) SPIRE 2006. LNCS, vol. 4209,
pp. 163–180. Springer, Heidelberg (2006)

15. Huynh, T.N.D., Hon, W.-K., Lam, T.W., Sung, W.-K.: Approximate string matching 
using compressed suﬃx arrays. Theor. Comput. Sci. 352(1-3), 240–249 (2006)

