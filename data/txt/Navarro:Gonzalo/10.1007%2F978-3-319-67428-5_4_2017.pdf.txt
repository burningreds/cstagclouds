LZ78 Compression in Low Main Memory Space

Diego Arroyuelo1, Rodrigo C´anovas2, Gonzalo Navarro3(B),

and Rajeev Raman4

1 Departamento de Inform´atica, Universidad T´ecnica Federico Santa Mar´ıa,

Vicu˜na Mackenna 3939, San Joaqu´ın, Santiago, Chile

2 LIRMM and IBC, 161 rue Ada, 34095 Montpellier Cedex 5, France

darroyue@inf.utfsm.cl

3 Deptartment of Computer Science, University of Chile,

yigorc@gmail.com

Beauchef 851, Santiago, Chile

gnavarro@dcc.uchile.cl

4 Department of Informatics, University of Leicester,

F33 Computer Science Building, University Road, Leicester, UK

r.raman@mcs.le.ac.uk

Abstract. We present the ﬁrst algorithms that perform the LZ78 compression 
of a text of length n over alphabet [1..σ], whose output is z
integers, using only O(z lg σ) bits of main memory. The algorithms read
the input text from disk in a single pass, and write the compressed output 
to disk. The text can also be decompressed within the same main
memory usage, which is unprecedented too. The algorithms are based on
hashing and, under some simplifying assumptions, run in O(n) expected
time. We experimentally verify that our algorithms use 2–9 times less
time and/or space than previously implemented LZ78 compressors.

1 Introduction

The Ziv-Lempel algorithm of 1978 [19] (known as LZ78) is one of the most
famous compression algorithms. Its variants (especially LZW [17]) are used in
software like Unix’s Compress and formats like GIF. Compared to the stronger
LZ77 format [18], LZ78 has a more regular structure, which has made it the
preferred choice for compressed sequence representations supporting optimaltime 
access [16] and compressed text indexes for pattern matching [3,7,15] and
document retrieval [5,6].

Compared to LZ77, the LZ78 compressed output is also easier to build. For
example, a simple and classical implementation compresses a text of length n
n), in O(n lg σ)
over an alphabet [1..σ] into z integers, where
deterministic or O(n) randomized time, using O(z lg n) = O(n lg σ) bits of space.
A comparable result for LZ77 was obtained only recently [11] and it required
sophisticated compressed suﬃx array construction algorithms.

n ≤ z = O(n/ lgσ

√

This collaboration started during the Dagstuhl Seminar 16431, “Computation over
Compressed Structured Data”. We also acknowledge the funding from Millennium
Nucleus Information and Coordination in Networks ICM/FIC RC130003 (G.N.).

c(cid:2) Springer International Publishing AG 2017
G. Fici et al. (Eds.): SPIRE 2017, LNCS 10508, pp. 38–50, 2017.
DOI: 10.1007/978-3-319-67428-5 4

LZ78 Compression in Low Main Memory Space

39

The time and main memory space required by compression algorithms is
important. Building the compressed ﬁle within less main memory allows us compressing 
larger ﬁles without splitting them into chunks, yielding better compression 
in general. The fastest deterministic LZ78 compression algorithms require
O(n) time, but O(n lg n) bits of main memory [8]. If the main memory is limited 
to O(n lg σ) bits (i.e., proportional to the input text size), then the time
increases to O(n lg lg σ) [11]. Finally, if we limit the main memory to O(z lg n)
bits (i.e., proportional to the compressed text size, like the classic scheme), then
the compression time becomes O(n(1 + lg σ/ lg lg n)) [1], which improves the
classic O(n lg σ) time. If we allow randomization, then the classic scheme yields
O(n) expected time and O(z lg n) bits of space.

In this paper we show that the LZ78 compression can be carried out within
just O(z lg σ) bits of main memory, which is less than any other previous scheme,
and asymptotically less than the size of the compressed ﬁle, z(lg z + lg σ) bits.
Ours are randomized and streaming algorithms. They read the text once from
disk, and output the compressed ﬁle to disk as well, and therefore they may run
within memory sizes unable to ﬁt even the compressed ﬁle. One of our algorithms
requires O(n) expected compression time, but may rewrite the output multiple
times, whereas the other takes O(n lg σ) expected time but writes the output
only once. Both are able to decompress the ﬁle in a single O(n)-time pass on
disk and using O(z lg σ) bits of main memory, where previous decompression
algorithms need to store the whole compressed text in main memory.

Our results hold under some simplifying assumptions on randomness. Nevertheless,
 our experimental results demonstrate that these assumptions do not
aﬀect the practical competitiveness of the new algorithms, which outperform
current alternatives in space and/or time by a factor from 2 to 9.

To obtain the result, we build on a hash-based trie representation [14], which
has the advantage that the addresses of the nodes do not change as we insert new
leaves, and that O(lg σ) bits are suﬃcient to encode the trie nodes since some
of the information is implicit in their hash addresses. The main challenge is to
design schemes so that the hash tables can grow as the LZ78 parsing progresses,
so as to ensure that they have only O(z) cells without knowing z in advance.

2 LZ78 Compression

The LZ78 compression algorithm [19] parses the text into a sequence of phrases.
Assume we are compressing a text T [1..n] and we have already processed T [1..i−1]
into r phrases B0B1B2 ··· Br−1, where phrase B0 represents the empty string.
Then, to compute Br we ﬁnd the longest preﬁx T [i..j − 1] (with j − 1 < n) that is
equal to some Bq, with q < r. Then we deﬁne Br = Bq . T [j], which is represented
as the pair (q, T [j]), and we continue the parsing from T [j + 1].

If we add a unique terminator character to T , then every phrase represents
a diﬀerent text substring. We call z the ﬁnal number of phrases generated. It is
known that

n), where σ is the size of the alphabet of T .

√
n ≤ z = O(n/ lgσ

The usual way to carry out the parsing eﬃciently is to use the so-called
LZTrie. This is a trie with one node per phrase, where the root node corresponds

40

D. Arroyuelo et al.

to B0, and the node of Br = Bq . T [j] is the child of the node of phrase Bq with
the edge labeled by T [j]. Since the set of phrases is preﬁx-closed, LZTrie has z
nodes. Then, to process T [i..j−1], we traverse LZTrie from the root downwards,
following the characters T [i], T [i + 1] . . . until falling out of the tree at a node
representing phrase Bq. Then we create a new node for Br, which is the child
of Bq labelled by T [j]. Since the trie has z nodes, it requires O(z(lg n + lg σ)) =
O(n lg σ) bits for the parsing, which can be done in O(n lg σ) deterministic time
(using binary search on the children) or O(n) randomized time (using hash tables
to store the children, whose sizes double when needed).

The usual way to represent the LZ78 parsing in the compressed ﬁle consists
of two (separate or interlaced) arrays, S[1..z] of z lg σ bits, and A[1..z] of z lg z
bits. If Br = Bq . T [j] is a phrase, then we represent it by storing A[r] = q and
S[r] = T [j]. For decompressing a given phrase Br, we follow the referencing
chain using array A, obtaining the corresponding symbols from array S, until
we read a 0 in A. Thus S[r], S[A[r]], S[A[A[r]], . . . obtains Br in reverse order
in O(|Br|) time, and the complete text is decompressed in O(n) time.

An alternative way to represent the LZ78 parsing [3] uses a succinct encoding
of LZTrie, which uses just 2z + z lg σ + o(z) bits, 2z + o(z) for the topology
and z lg σ for the labels. It also stores an array L[1..z], such that L[r] stores the
preorder number of the LZTrie node corresponding to phrase Br. This array
requires z lg z bits. To extract the text for phrase Br, we start from the node
with preorder L[r] in LZTrie, and obtain the symbols labeling the upward path
up to the root, by going successively to the parent in the trie. Using succinct tree
representations that support going to the parent in constant time, this procedure
also yields O(n) total decompression time.

This second representation is more complex and uses slightly more space
than the former, more precisely, the 2z + o(z) bits for the tree topology. Yet, it is
sometimes preferred because it allows for operations other than just decompressing 
T . For instance, Sadakane and Grossi [16] show how to obtain any substring
n) time (i.e., in time proportional to the
of length (cid:3) of T in optimal O((cid:3)/ lgσ
number of machine words needed to store (cid:3) symbols). In this work, a diﬀerent 
representation of LZTrie will allow us carrying out the compression within
O(z lg σ) bits of main memory.

3 Previous Work on LZ78 Construction

A classic pointer-based implementation of LZTrie, with balanced binary trees
to handle the children of each node, carries out the compression in O(n lg σ)
time and O(z lg n) bits of space.

Jansson et al. [10] introduce a particular trie structure to represent LZTrie,
(cid:2)
which still uses O(z lg n) bits of space but reduces the construction time to
n lg σ ·
. The algorithm needs two passes on the text, each of
O
which involves n lg σ bits of I/O if it is stored on disk.

lg n lg lg lg n

(lg lg n)

(cid:3)

2

Arroyuelo and Navarro [2] manage to perform a single pass over the text, in
exchange for 2z lg z additional bits of I/O, and a total time of O(n(lg σ+lg lg n)).

LZ78 Compression in Low Main Memory Space

41

Table 1. Previous and new LZ78 compression algorithms. Times with a star mean
expected time of randomized algorithms. We ﬁrst list the classic schemes, then the
deterministic methods, from fastest and most space-consuming to slowest and least
space-consuming. At the end, our randomized method uses less space than all the
others, and also matches the fastest ones in expectation.

Reference

Classic [19]

RAM space in bits Compression time
O(z lg n)
O(z lg n)
(1 + )n lg n + O(n) O(n/2)

O(n lg σ)
∗
O(n)

Fischer et al. [8]
K¨oppl and Sadakane [11] O(n lg σ)
O(z lg n)

Jansson et al. [10]

z(lg n + lg σ + 2)
Arroyuelo et al. [1]
Arroyuelo and Navarro [2] z(lg n + lg σ + 2)
This paper

O(z lg σ)

(cid:2)
(cid:2)

O(n lg lg σ)
n lg σ ·
O
n lg σ ·

O

(cid:3)

2

(lg lg n)

lg n lg lg lg n

(cid:3)

1

lg lg n

O(n lg σ + n lg lg n)
∗
O(n)

The peak memory usage is z(lg n + lg σ + 2) bits. They use the compact LZTrie
representation described in the previous section. An obstacle to further reducing
the space is that they need to build the whole LZTrie before they can build
the array L, because preorder numbers vary as new leaves are inserted. Later
improvements on dynamic tries introduced by Arroyuelo et al. [1] reduce the
. Notice that this is O(n) for small alphabets, σ = polylog(n).
time to O
However, the peak space usage remains the same.

n lg σ
lg lg n

(cid:2)

(cid:3)

Fischer et al. [8] ﬁnally obtained linear worst-case time. They construct the

LZ78 parsing using (1 + )n lg n + O(n) bits of space in O(n/2) time.

Recently, K¨oppl and Sadakane [11] showed how to construct the parsing in

O(n lg lg σ) time, using O(n lg σ) bits of working space; note this is Ω(z lg n).

Table 1 shows all these previous results, and our contribution in context. Our

results hold under some simplifying assumptions that are described in Sect. 8.

4 Dynamic Compact Tries

We will make use of the following data structure to maintain a dynamic trie of
up to t nodes that uses O(t lg σ) bits, while supporting insertion of edges, and
navigation upwards and downwards from nodes, within constant randomized
time [14]. The structure has two components:
1. A closed hash table H[0..M − 1], where M = m/α, m is an upper bound to
the number of nodes, and the constant α < 1 is the load factor to use. Table
H is a simple array that stores information on the nodes of the trie, using
only lg σ + O(1) bits per entry.
2. An array D[0..M − 1] to store information about the collisions (all entries
initialized with value −1).

42

D. Arroyuelo et al.

Each trie node y is identiﬁed with the position where it is stored in H;
sometimes we will write p(y) explicitly to refer to this position. This position
will not change with time. The root is placed at an arbitrary position, say H[0].
Every other node y is represented by a pair (x, c), where x is (the position in H
of) the parent of y and c is the character labeling the edge between x and y.
The hash function used to place y in H is h(y) = ((a· w(y)) mod P ) mod M,
where a is an integer chosen at random in [1, P − 1], P is the ﬁrst prime such
that P > M · σ, and w(y) = p(x) · σ + (c − 1). The value we store in the cell of
H associated with y is v(y) = ((a · w(y)) mod P ) div M.

With this mechanism, since P = O(M σ) [9, p. 343], it holds v(y) = O(σ),
and thus the values stored in H require lg σ + O(1) bits. With this information
we can still reconstruct (a · w(y)) mod P = v(y) · M + h(y), and then w(y) =
a−1 · (a · w(y)) mod P , where a−1 mod P is easily computed from a and stored
with the index. From w(y) we recover the pair (x, c), which allows us traversing the
trie upwards.

On the other hand, to insert a new child y = (x, c) from the position p(x), we
compute h(y) and try to write v(y) at H[h(y)]. If the cell is free (which we signal
with D[h(y)] = −1), then we write H[h(y)] ← v(y) and D[h(y)] ← 0. If the cell
is not free, we probe consecutive positions H[p] with p = (h(y) + k) mod M, for
k = 1, 2, . . .. The following cases may occur:
1. D[p] = −1, in which case we terminate with H[p] ← v(y) and D[p] ← k, so
that D[p] indicates the number of probes between h(y) and the ﬁnal position
p where y is ﬁnally written. Note that p will become p(y), and from p(y) we
can recover h(y) without knowing y, with h(y) = (p(y) − D[p(y)]) mod M.
2. D[p] (cid:5)= −1, H[p] = v(y), and (p−D[p]) mod M = h(y), thus node y is already
3. D[p] (cid:5)= −1, but H[p] (cid:5)= v(y) or (p − D[p]) mod M (cid:5)= h(y), thus the cell is

stored in H, so we should not insert it.

occupied by another node and we must continue with the next value of k.

Case 2 also shows how to traverse the trie downwards, from the current node

towards its child labeled by c, to ﬁnd the node y = (x, c).

Note that the values stored in D are constant in expectation, as they record
the insertion time for each element. Poyias et al. [14] show how D can be represented 
with a data structure using O(z) bits and constant amortized-time
operations. We refer the reader to their article for further details.

5 Using a Fixed Hash Table

In this section we show how to do the parsing of T [1..n] within O(n lg σ/ lgσ
n)
bits of main memory. This space is already O(z lg σ) on incompressible texts; we
will later achieve it for all texts.

We use a compact dynamic trie to build the LZTrie associated with the
LZ78 parse of T , and to compress T accordingly. We set m to an upper bound on
m− 3) ≥ n.
the number of LZTrie nodes: m is the smallest number with m(lgσ
n). Further, we will use an array L[0..z] to store in L[r] the
Thus m = Θ(n/ lgσ

LZ78 Compression in Low Main Memory Space

43

position in H where the LZTrie node of block Br is stored. Each entry of L
takes (cid:7)lg M(cid:8) = lg n + O(1) bits, but the array is generated directly on disk.

To perform the parsing of a new phrase T [i..j], we start from the trie root (say,
x0, with p(x0) = 0), and use the mechanism described in the previous section to
compute x1 = (x0, T [i]), x2 = (x1, T [i+1]), and so on until xj−i+1 = (xj−i, T [j])
does not exist in the trie. At this point we insert xj−i+1 = (xj−i, T [j]), write to
disk the next value L[r] ← p(xj−i+1), and continue with T [j + 1..n].

Overall, compression is carried out within the O(n lg σ/ lgσ

n) bits of main
memory used by H and D, in O(n) expected time if H is chosen from a universal
family of hash functions, and T and L are read/written from/to disk in streaming
mode. When we ﬁnish the parsing, we write H and D at the end of L in the ﬁle,
and add some header information including n, σ, M, P , a, a−1.

Decompression can also be made in streaming mode and using memory space
only for H and D, which is not possible in classical schemes where each phrase
is stored as a pointer to its earlier position in the ﬁle. We load the LZTrie
into memory (i.e., tables H and D). Now we read the consecutive entries of
L[1..z] in streaming mode. For each new entry L[r] = p, we start from H[p] and
decode x0 = (x−1, c−1) from it; then we decode x−1 = (x−2, c−2), and so on,
until we reach the root x−s = L[0]. Then we append c−sc−s+1 . . . c−2c−1 to the
decompressed text in streaming mode. The stack may require up to z lg σ bits
in extreme cases, but this is still within our main memory budget. Its use can
also be avoided in standard ways, at the expense of increased I/Os.

Note that this structure permits retrieving the contents of any individual
block Br, by traversing the LZTrie upwards from L[r], just as done for decompression.
 This can make it useful as a succinct data structure as well.

√

n), since

The obvious disadvantage of this simple scheme is that it uses more than
n) (that is,
O(z lg σ) bits of space when T is highly compressible, z = o(n/ lgσ
√
when it is most interesting to compress T !). A simple workaround is to start
n is the smallest possible value for z. If,
assuming that z = O(
during the parsing, this limit is exceeded, we double the value of z and repeat the
whole process. Since we may rerun the process O(lg z) times, the total expected
time is O(n lg z) = O(n lg n) (in LZ78, lg z = Θ(lg n)). In exchange, the main
memory space is now always O(z lg σ) bits. Further, the extra space added to
the compressed ﬁle due to the tables H and D is just O(z lg σ). Apart from the
increased time, a problem with this scheme is that it reads T several times from
disk, and thus it is not a streaming algorithm. In the next sections we explore
two faster solutions that in addition scan T only once.

6 Using a Growing Table

We can obtain O(z lg σ) bits of space for any text by letting H and D grow as
√
√
more blocks are produced along the parsing. We start with a hash table of size
n/α2, since
n is a lower bound on z. Then, whenever the load factor in H
reaches α, we allocate a new table H(cid:3) (and D(cid:3)) with size multiplied by 1/α, and
load them with all the current trie nodes. We will read T only once, but we will
still perform multiple rewriting passes on L.

44

D. Arroyuelo et al.

The main challenge is how to remap all the nodes x from H to H(cid:3), since
their position p(x) in H are their identity, which is mentioned not only in L but
also in their children y = (p(x), c). That is, in order to map y to its new position
p(cid:3)(y) in H(cid:3), we need to know the mapped position p(cid:3)(x) of its parent x, that
is, we must map the LZTrie nodes top-down. Yet, we cannot simply perform a
DFS traversal on LZTrie, because we cannot eﬃciently enumerate the children
of a node x in less than O(σ) time.

n from the sampled node we reached.

We can reduce the time to O(z lgσ

n and whose height is at least lgσ

We remap the nodes as follows. We traverse L from left to right (on disk), and
traverse upwards from each position L[r] in H up to the root. All the nodes from
the parent of L[r] must already exist in H(cid:3), so we stack the symbols traversed
in the upward path on H and use them to traverse downwards in H(cid:3) from the
root. Then we insert in H(cid:3) (and D(cid:3)) the new node that corresponds to L[r], and
rewrite L[r] with the new position in H(cid:3). If Br corresponds to T [i..j], then our
retraversal costs O(j−i) time, so the expected time to retraverse T [1..n(cid:3)] is O(n(cid:3)).
Since we perform O(lg z) passes, the total cost may reach O(n lg z) = O(n lg n).
n) = O(n) by storing, when we have to
load H(cid:3), O(z/ lgσ
n) sampled nodes of H in a (classic) hash table W , which
stores the position in H(cid:3) of each sampled position in H. Table W uses O(z lg σ)
bits, which is within our budget. We will guarantee that every node of LZTrie
n will be
whose depth is a multiple of lgσ
n) nodes are sampled and that we traverse
sampled. This ensures that O(z/ lgσ
n nodes of H from any cell L[r] before reaching a sampled node,
less than 2 lgσ
from which we can descend in H(cid:3) and insert L[r] in time O(lgσ
n). Thus we
do the translation in O(|L| lgσ
n) time. Since the size of L grows by a factor of
n).
1/α each time we create a larger table, the total work amounts to O(z lgσ
To obtain the sampling invariant, we start by sampling the root. Then, every
n cells or
time we traverse from the node of L[r] upwards, if we traverse lgσ
more before ﬁnding a sampled node, we sample the node we traversed that is at
distance lgσ
Once H(cid:3) and D(cid:3) are built, we continue with them and discard H and D. The
peak space usage of the tables, when old and new ones are active, is (1/α2 +
1/α)z lg σ + O(z) = O(z lg σ) bits. Note that we can always keep the entries of L
within lg z + O(1) bits, slightly expanding them when we retraverse L to rewrite
the new positions in H(cid:3). At the end, L may need to point to a table H whose
size is z/α2, thus using z lg z + O(z) bits. To store H and D, we ﬁrst write a
bitvector B of length at most z/α2 indicating which entries are (cid:5)= −1 in D. This
requires O(z) bits. Only the z ﬁlled entries of H and D are then written to the
compressed ﬁle. The ﬁnal compressed ﬁle size is then z(lg z + lg σ) + O(z) bits.
Note that the O(z) bits spent in L can be eliminated with a ﬁnal pass on L
replacing L[r] by rank1(B, L[r]), which is the number of 1s in B up to position
L[r]. This can be computed in O(z) time, and the values can be recovered in O(z)
time at decompression time using the complementary query select1(B, L[r]) [4].

LZ78 Compression in Low Main Memory Space

45

7 Using Multiple Hash Tables

A way to avoid rebuilding the hash table is to create additional hash tables apart
from the original one, H0 = H. When the load factor of H reaches α, we allocate
a new table H1, with |H1| = 2|H0|, where all the subsequent insertions will take
place. When H1 becomes full enough, we create H2, with |H2| = 2|H1|, and so
on, each time doubling the previously allocated space. Each table Hh has its
own value Mh, prime Ph, and so on.

To properly address the nodes, we need to build a global address that can
point to entries in any table. We regard the tables as their concatenation, that is,
H0H1H2 . . . The addresses within table Hh are built by adding |H0H1 . . . Hh−1|
to the normal address computation within Hh. The prime Ph must then be larger
than (M0 + M1 + . . .+ Mh)· σ, so as to store any element (p(x), c) where p(x) is a
global address. This requires only O(1) extra bits per cell to store Ph/Mh ≤ 2σ.
Assume we are at a node x in a table Hg and want to add a child y = (x, c)
in the current table Hh. The entry (p(x), a) will be inserted in Hh, leaving no
indication in Hg of the existence of y. This means that, if we want to descend
from x by c, we must probe tables Hg, Hg+1, . . . , Hh to see if it was inserted in
later tables. Therefore, the cost of traversing towards a child grows to O(lg z), as
we can build that many tables during the parsing. However, since the children
are inserted later than their parents, the current table index does not decrease
as we move down from the root towards the node where we will insert the new
block, and thus we do these O(lg z) probes once per inserted block, for a total
time of O(z lg z) = O(n lg σ).

Instead, the parent x is decoded immediately from y = (p(x), a), since p(x) is
√
a global address, and this allows decompressing in O(n) time. Finding the table
Hg from p(x) is a matter of dividing p(x) by
n and then ﬁnding the logarithm
in base 2, which is done in constant time in most architectures (and in theory,
using constant precomputed tables of small size).

This technique has the advantage that it treats T and L in streaming mode,
as it does not have to retraverse them. The values written on L are ﬁnal (note
that their width grows along the process, each time we start using a new table).
These can be compacted as in the previous section if we are willing to perform
a second pass on L.

8 Simplifying Assumptions

Our expected-case analysis inherits some simpliﬁcations from Poyias et al. [14],
when it assumes constant expected time for hashing with linear probing.

A ﬁrst one is that analyses usually assume that the hash function is chosen
independently of the set of values to hash. In our scheme, however, the values
(p(x), c) to hash depend on the hash function h(x) itself. So, at least in principle,
the typical assumptions to prove 2-independence do not hold, even if we changed
our function to the standard ((a0 + a1 · w(y)) mod M) mod P for randomly
chosen a0 and a1.

46

D. Arroyuelo et al.

Another issue is that 2-independence may not be suﬃcient to assume randomness 
in the case of linear probing. This has only been proved assuming
5-independence [12,13]. To make it 5-independent, the component a· w(y) of our
hash function should become a0 + a1w(y) + a2w(y)2 + a3w(y)3 + a4w(y)4. We
do not know how to invert such a function in order to ﬁnd w(y) given h(y) and
v(y).

In the next section we show, however, that those theoretical reservations do

not have a signiﬁcant impact on the practical performance of the scheme.

9 Experimental Results

In this section we experimentally evaluate our new algorithms with some previous
implemented alternatives. We measure compression and decompression time,
RAM usage and overhead of the ﬁnal ﬁle size compared with the standard LZ78
format. All the experiments were performed on an Intel(R) Core(TM) i7-5500U
CPU at 2.40 GHz. The operating system was Ubuntu 16.04.2 LTS, version 4.4.0-
72-generic Linux kernel. Our compressors were implemented in C++11, using
g++ version 4.8.4.

The texts considered are a highly compressible XML text, an English text,
and a less compressible Protein ﬁle, all obtained from the Pizza&Chili Corpus1.
We also used a DNA ﬁle generated by extracting a preﬁx of a human genome2.
Table 2 lists the test ﬁles used and their main statistics. For the compression
ratio we assume that each of the z phrases gives the parent phrase number and
the symbol. For the former, the next 2i phrases use i + 1 bits, starting from the
second with i = 0. For the latter, we use (cid:7)lg σ(cid:8) bits.

Table 2. Text ﬁles used in the experiments.

File name

XML

English

Proteins

Size n
(Megabytes)

282.42

1,024.00

1,129.20

Human Genome 3,182.00

σ

97

237

length (n/z)

Number z Avg. phrase Compr. ratio
of phrases
16,205,171 18.27
96,986,744 11.07
27 147,482,019
8.03
51 227,419,107 14.67

20.50%
37.96%
48.55%
27.96%

Figure 1 shows the maximum RAM used by each structure during compression,
 and the resulting compression time. Our approaches are labeled HLZ (ﬁxed
hash table of maximum size, no rebuilding), MHLZ (multiple hash tables) and
GHLZ (growing hash tables, no sampling). We obtain tradeoﬀs by using various
load factors for the hash tables, 1/α = 1.05, 1.10, 1.20, 1.40, 1.60.

1 http://pizzachili.dcc.uchile.cl/texts.
2 http://hgdownload.cse.ucsc.edu/goldenPath/hg18/bigZips/est.fa.gz.

LZ78 Compression in Low Main Memory Space

47

XML

English

r
e

t
c
a
r
a
h
c
 
r
e
p

 
s
d
n
o
c
e
s
o
r
c
M

i

r
e
t
c
a
r
a
h
c
 
r
e
p
 
s
d
n
o
c
e
s
o
r
c
M

i

HLZ
MHLZ
GHLZ
LZ78-Min
LZ78-UC

 1.2

 1

 0.8

 0.6

 0.4

 0.2

 0

 0

 1

 2

 4

 3
 5
Bits per character

 6

 7

 8

 1.2

 1

 0.8

 0.6

 0.4

 0.2

 0

 0

Proteins

HLZ
MHLZ
GHLZ
LZ78-Min
LZ78-UC

 2

 4

 6

 8

 10

 12

 14

 16

 18

Bits per character

r
e

t
c
a
r
a
h
c
 
r
e
p

 
s
d
n
o
c
e
s
o
r
c
M

i

r
e
t
c
a
r
a
h
c
 
r
e
p
 
s
d
n
o
c
e
s
o
r
c
M

i

 1.2

 1

 0.8

 0.6

 0.4

 0.2

 0

 1.2

 1

 0.8

 0.6

 0.4

 0.2

 0

HLZ
MHLZ
GHLZ
LZ78-Min
LZ78-UC

 0  1  2  3  4  5  6  7  8  9  10  11  12  13

Bits per character

DNA

HLZ
MHLZ
GHLZ
LZ78-Min
LZ78-UC

 0

 1

 2

 3

 4

 5

 6

 7

 8

 9

 10

Bits per character

Fig. 1. Maximum RAM and time used during compression.

As previous work, we include LZ78-Min, the compact representation of
Arroyuelo and Navarro [2], and LZ78-UC, their uncompressed baseline, both
implemented in C.

It can be seen that MHLZ always outperforms GHLZ in space/time, using
1.0–2.2 bits and 0.2–0.3 µs per symbol with 1/α = 1.40. For the same space, the
overhead of using multiple tables is lower than that of rebuilding the table, which
implies rereading the L array from disk. In general, the time of MHLZ is very
sensitive to high load factors, without signiﬁcantly improving the space. With
a suﬃciently low load factor, instead, it outperforms all the others in time and
space. It even gets close to the time of HLZ, always below 0.2 µs, with much less
space (with the exception of Proteins, where the ﬁnal-size guess of HLZ is nearly
optimal). The maximum space usage of GHLZ occurs when it has to expand the
table, at which moment it has the old and new tables in RAM. This requires
more space than MHLZ even when the MHLZ tables are emptier on average.
LZ78-Min, instead, requires 2–3 times more space and is up to 4 times slower.
Finally, LZ78-UC requires 6–9 times more space than MHLZ, and is not faster
than HLZ.

Figure 2 shows the RAM used by each structure during decompression. This
time GHLZ always obtains the best time of MHLZ but using slightly less space.
GHLZ uses 0.9–1.8 bits and 0.1–0.2 µs per symbol, even outperforming HLZ,
which uses much more space (except on Proteins). GHLZ does not need to make

48

D. Arroyuelo et al.

the hash tables grow at decompression, thus it is much faster and uses less space
than MHLZ, which has emptier tables. MHLZ is faster than for compression
because it traverses the paths upwards, but it still uses multiple tables, and this
poses some time overhead. LZ78-Min and LZ78-UC are identical for decompression,
 requiring 2–3 times more space but being 2–3 times faster than GHLZ.

XML

English

HLZ
MHLZ
GHLZ
LZ78-Min
LZ78-UC

 0

 0.5

 1

 1.5

 2

 2.5

 3

 3.5

 4

 4.5

 5

Bits per character

Proteins

HLZ
MHLZ
GHLZ
LZ78-Min
LZ78-UC

 0.4

 0.3

 0.2

 0.1

 0

 0.4

 0.3

 0.2

 0.1

r
e
t
c
a
r
a
h
c
 
r
e
p
 
s
d
n
o
c
e
s
o
r
c
m

i

r
e
t
c
a
r
a
h
c
 
r
e
p
 
s
d
n
o
c
e
s
o
r
c
m

i

 0

 0

 1

 2

 3

 4

 5

Bits per character

r
e
t
c
a
r
a
h
c
 
r
e
p
 
s
d
n
o
c
e
s
o
r
c
m

i

r
e
t
c
a
r
a
h
c
 
r
e
p
 
s
d
n
o
c
e
s
o
r
c
m

i

 0.4

 0.3

 0.2

 0.1

 0

 0

 0.4

 0.3

 0.2

 0.1

 0

 0.5

HLZ
MHLZ
GHLZ
LZ78-Min
LZ78-UC

 1

 2

 3

 4

 5

 6

Bits per character

DNA

HLZ
MHLZ
GHLZ
LZ78-Min
LZ78-UC

 1

 1.5

 2

 2.5

 3

 3.5

Bits per character

Fig. 2. Maximum RAM and time used during decompression.

Finally, Fig. 3 shows the ratio between the actual compressed ﬁle size and the
output of a classical LZ78 compressor (see Table 2). We exclude the HLZ baseline
because it does not really compress. While MHLZ poses 30%–40% of overhead,
GHLZ requires 25%–35%. We note that, to reach this overhead, we need to use
1/α = 1.1 or less, that is, almost the slowest. In this case, it is preferable to
use GHLZ, which uses 1–3 bits and 0.5–0.8 µs per symbol for compression and
0.1–0.2 µs for decompression. If we want to have the fastest MHLZ compression
times, we must accept an overhead of 40%–45%. On the other hand, LZ78-Min
has an overhead of 4%–15%.

LZ78 Compression in Low Main Memory Space

49

XML

English

MHLZ
GHLZ
LZ78-Min
LZ78-UC

 1

 2

 3

 4

 5

 6

 7

 8

Bits per character at compression (RAM)

Proteins

MHLZ
GHLZ
LZ78-Min
LZ78-UC

d
a
e
h
r
e
v
o

 

i

n
o
s
s
e
r
p
m
o
C

d
a
e
h
r
e
v
o

 

i

n
o
s
s
e
r
p
m
o
C

 1.7

 1.6

 1.5

 1.4

 1.3

 1.2

 1.1

 1

 0

MHLZ
GHLZ
LZ78-Min
LZ78-UC

 4

 2
 12
Bits per character at compression (RAM)

 10

 6

 8

 14

 1.7

 1.6

 1.5

 1.4

 1.3

 1.2

 1.1

 1

DNA

MHLZ
GHLZ
LZ78-Min
LZ78-UC

 0

 1

 2

 3

 4

 5

 6

 7

 8

 9

 10

d
a
e
h
r
e
v
o

 

i

n
o
s
s
e
r
p
m
o
C

d
a
e
h
r
e
v
o

 

i

n
o
s
s
e
r
p
m
o
C

 1.7

 1.6

 1.5

 1.4

 1.3

 1.2

 1.1

 1

 0

 1.7

 1.6

 1.5

 1.4

 1.3

 1.2

 1.1

 1

 0  1  2  3  4  5  6  7  8  9  10 11 12 13 14 15 16 17

Bits per character at compression (RAM)

Bits per character at compression (RAM)

Fig. 3. Maximum RAM used at compression versus ratio of the ﬁnal ﬁle size over the
classical output size.

10 Conclusions

We have presented new LZ78 compression/decompression algorithms based on
hashing, which under some simplifying assumptions use O(z lg σ) bits of main
memory in expectation, while running in O(n lg σ) time for compression and
O(n) time for decompression, where n is the text length, z the number of LZ78
phrases, and σ the alphabet size. Our algorithms read the text once, in streaming
mode, and write the output to disk. There exists no previous algorithm using so
little main memory.

Our experiments show that our new methods use 2–3 times less space for
compression than the most space-eﬃcient implemented compressor in the literature,
 while being up to 4 times faster. Compared to a classical baseline, our
compressor uses 6–9 times less space and is only 50% slower. Our decompressor
uses 2–3 times less space than both baselines, but it is 2–3 times slower.

For example, our compressor can use up to 3 bits and 0.8 µs per symbol and
our decompressor up to 2 bits and 0.2 µs per symbol, posing a space overhead
around 30% over the optimally compressed ﬁle.

Our compressors and the competing algorithms are publicly available at

https://github.com/rcanovas/Low-LZ78.

50

D. Arroyuelo et al.

An interesting line of future work is to use these hash-based tries as compressed 
text representations that retrieve any text substring [16], or for the
compressed-space construction of LZ78-based text indexes [2].

Acknowledgements. We thank the reviewers for their insightful comments.

References

1. Arroyuelo, D., Davoodi, P., Satti, S.R.: Succinct dynamic cardinal trees. Algorithmica 
74(2), 742–777 (2016)

2. Arroyuelo, D., Navarro, G.: Space-eﬃcient construction of Lempel-Ziv compressed

text indexes. Inf. Comput. 209(7), 1070–1102 (2011)

3. Arroyuelo, D., Navarro, G., Sadakane, K.: Stronger Lempel-Ziv based compressed

text indexing. Algorithmica 62(1), 54–101 (2012)

4. Clark, D.R.: Compact PAT trees. Ph.D. thesis, University of Waterloo, Canada

(1996)

5. Ferrada, H., Navarro, G.: A Lempel-Ziv compressed structure for document listing.
In: Kurland, O., Lewenstein, M., Porat, E. (eds.) SPIRE 2013. LNCS, vol. 8214,
pp. 116–128. Springer, Cham (2013). doi:10.1007/978-3-319-02432-5 16

6. Ferrada, H., Navarro, G.: Eﬃcient compressed indexing for approximate top-k
string retrieval. In: Moura, E., Crochemore, M. (eds.) SPIRE 2014. LNCS, vol.
8799, pp. 18–30. Springer, Cham (2014). doi:10.1007/978-3-319-11918-2 3

7. Ferragina, P., Manzini, G.: Indexing compressed texts. J. ACM 52(4), 552–581

(2005)

8. Fischer, J., I, T., K¨oppl, D.: Lempel Ziv Computation in Small Space (LZ-CISS).
In: Cicalese, F., Porat, E., Vaccaro, U. (eds.) CPM 2015. LNCS, vol. 9133, pp.
172–184. Springer, Cham (2015). doi:10.1007/978-3-319-19929-0 15

9. Hardy, G.H., Wright, E.M.: An Introduction to the Theory of Numbers, 6th edn.

Oxford University Press, Oxford (2008)

10. Jansson, J., Sadakane, K., Sung, W.: Linked dynamic tries with applications to
LZ-compression in sublinear time and space. Algorithmica 71(4), 969–988 (2015)
11. K¨oppl, D., Sadakane, K.: Lempel-Ziv Computation in Compressed Space (LZ-

CICS). In: Proceedings of 26th Data Compression Conference, pp. 3–12 (2016)

12. Pagh, A., Pagh, R., Ruzic, M.: Linear probing with 5-wise independence. SIAM

Rev. 53(3), 547–558 (2011)

13. Patrascu, M., Thorup, M.: On the k-independence required by linear probing and

minwise independence. ACM Trans. Algorithms 12(1) (2016). Article 8

14. Poyias, A., Puglisi, S.J., Raman, R.: m-Bonsai: a practical compact dynamic trie.
In: Preliminary Version Proceedings of SPIRE 2015. LNCS, vol. 9309 (2017). CoRR
abs/1704.05682. http://arxiv.org/abs/1704.05682,

15. Russo, L.M.S., Oliveira, A.L.: A compressed self-index using a Ziv-Lempel dictionary.
 Inf. Retrieval 11(4), 359–388 (2008)

16. Sadakane, K., Grossi, R.: Squeezing succinct data structures into entropy bounds.
In: Proceedings of 17th Annual ACM-SIAM Symposium on Discrete Algorithms
(SODA), pp. 1230–1239 (2006)

17. Welch, T.A.: A technique for high performance data compression. IEEE Comput.

17(6), 8–19 (1984)

18. Ziv, J., Lempel, A.: A universal algorithm for sequential data compression. IEEE

Trans. Inf. Theory 23(3), 337–343 (1977)

19. Ziv, J., Lempel, A.: Compression of individual sequences via variable length coding.

IEEE Trans. Inf. Theory 24(5), 530–536 (1978)

