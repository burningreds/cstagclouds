Improving Semistatic Compression Via

Pair-Based Coding(cid:2)

Nieves R. Brisaboa1, Antonio Fari˜na1, Gonzalo Navarro2, and Jos´e R. Param´a1

1 Database Lab., Univ. da Coru˜na, Facultade de Inform´atica, Campus de Elvi˜na s/n,

15071 A Coru˜na, Spain

{brisaboa,fari,parama}@udc.es

2 Dept. of Computer Science, Univ. de Chile, Blanco Encalada 2120, Santiago, Chile

gnavarro@dcc.uchile.cl

Abstract. In the last years, new semistatic word-based byte-oriented
compressors, such as Plain and Tagged Huﬀman and the Dense Codes,
have been used to improve the eﬃciency of text retrieval systems, while
reducing the compressed collections to 30–35% of their original size.

In this paper, we present a new semistatic compressor, called PairBased 
End-Tagged Dense Code (PETDC). PETDC compresses English
texts to 27–28%, overcoming the optimal 0-order preﬁx-free semistatic
compressor (Plain Huﬀman) in more than 3 percentage points. Moreover,
PETDC permits also random decompression, and direct searches using
fast Boyer-Moore algorithms.

PETDC builds a vocabulary with both words and pairs of words. The
basic idea in which PETDC is based is that, since each symbol in the
vocabulary is given a codeword, compression is improved by replacing
two words of the source text by a unique codeword.

1 Introduction

The grown in size and number of text databases during the last decade makes
compression even more attractive. Compression exploits the redundancies in the
text to represent it using less space [1]. It is well-known that cpu speed has been
growing faster than disk and network bandwidth during the last years. Therefore,
reducing the size of the text, even at the expense of some cpu time, is useful
because it reduces the I/O time needed to load it from disk or to transmit it
thought a network.

Joining compression and block addressing indexes [12] improves the eﬃciency
of that retrieval structures. Those indexes are smaller than standard inverted
indexes because their entries do not point to exact word positions. Instead of
that, entries point to those blocks where a word appears. This has a main draw-
back: some searches (i.e. phrase search) need traversing the text in the pointed

(cid:2) This work is partially supported (for the Spanish group) by MCyT (PGE and
FEDER) grant(TIC2003-06593), MEC (PGE and FEDER) grant (TIN2006-15071-
C03-03), Xunta de Galicia Grant (PGIDIT05SIN10502PR) and (for the third au-
thor) by Millennium Nucleus Center for Web Research, grant (P04-067-F), Mideplan,
Chile.

I. Virbitskaite and A. Voronkov (Eds.): PSI 2006, LNCS 4378, pp. 124–134, 2007.
c(cid:2) Springer-Verlag Berlin Heidelberg 2007

Improving Semistatic Compression Via Pair-Based Coding

125

blocks, what usually implies decompressing the block. However, if the text is
compressed with a technique that permits direct search in the compressed text,
scanning the compressed blocks is much faster.

A good compressor for text databases has to join two main properties: i)
to permit direct search into the compressed text by compressing the search
pattern and then looking for this compressed version, and ii) to allow local
decompression, which permits to decompress any portion of the compressed ﬁle
without the need of decompressing it from the beginning. From the two main
families of compressors (semistatic and adaptive compressors) only the semistatic
ones join those two properties. Adaptive compressors, such as those from the
well-known Ziv-Lempel family [15,16] learn the data distribution of the source
symbols as they compress the text and therefore, the mapping “source symbol
↔ codeword” is adapted as compression progresses. Albeit there exist methods
to search text compressed with adaptive compressors [14,8], they are not very
eﬃcient. Decompression is usually needed during searches as the code given
to a source symbol may vary along the text. Semistatic compressors (such as
Huﬀman [10]) perform a ﬁrst pass over the source text to obtain the distinct
source symbols and to count their number of occurrences. Then, they associate
each source symbol with a code that do not change during the second pass
(where each source symbol is replaced by the corresponding codeword). Since the
mapping source symbol ↔ codeword does not vary, direct searching is allowed.
Classic Huﬀman is a well-known technique. It is a character-based method
that generates an optimal preﬁx 1 coding. Unfortunately, it is not well-suited
for text databases because of its poor compression ratio (around 60%). In [11],
Moﬀat proposed using words instead of characters along with a Huﬀman coding
scheme. As a result, compression ratio was reduced to around 25–30%. Moreover,
using words instead of characters, gave the key to the integration of compression
and text retrieval systems, since words are also the atoms of those systems.

Based on Moﬀat’s idea, in [7] two word-based byte-oriented Huﬀman codes
were presented. The ﬁrst one, named Plain Huﬀman (PH) is a Huﬀman-based
code that uses bytes instead of bits as target alphabet. By using bytes instead
of bits, decompression speed was improved at the expense of compression ratio,
which grew up to around 30–35%. The second compressor, named Tagged Huﬀman 
(TH), uses the ﬁrst bit of each byte to mark the beginning of a codeword.
Therefore, only 7 bits of each byte can be used to create the codewords, and
a loss in compression of around 3 percentage points exists. However, since the
beginning of a codeword can be recognized, random decompression is allowed
and direct searches can be done by using a fast Boyer-Moore matching algorithm
[2].

In [5,4] we presented End-Tagged Dense Code (ETDC), a statistical semistatic
technique that maintains the good capabilities of Tagged Huﬀman for searches
while improving its compression ratio and using a simpler and faster coding
scheme.

1 In a preﬁx code, no codeword is a preﬁx of another, a property that ensures that the
compressed text can be decoded as it is processed, since a lookahead is not needed.

126

N.R. Brisaboa et al.

In this paper, we present a modiﬁcation over ETDC (see next section) that
we call Pair-Based End-Tagged Dense Code (PETDC), which improves its compression 
ratio (around 28–30%) by exploiting the co-occurrence of words in the
source text. Moreover, PETDC permits direct searching the compressed text,
as well as fast random decompression. The paper is structured as follows: In
Section 2, ETDC is shown. Then PETDC is described in Section 3. In Section 4,
empirical results measuring the eﬃciency of PETDC in compression ratio, compression 
and decompression speed are given. Finally, some conclusions end the
paper.

2 Related Work: End-Tagged Dense Code

End-Tagged Dense Code (ETDC) [5,4] is a semistatic compression technique,
which is the basis of the new PETDC presented in this paper.

Plain Huﬀman Code [7] is a word-based Huﬀman code that assigns a sequence
of bytes (rather than bits) to each word. In Tagged Huﬀman [7], the ﬁrst bit of
each byte is reserved to ﬂag whether the byte is the ﬁrst of its codeword. Hence,
only 7 bits of each byte are used for the Huﬀman code. Note that the use of a
Huﬀman code over the remaining 7 bits is mandatory, as the ﬂag bit is not useful
by itself to make the code a preﬁx code. The tag bit permits direct searching the
compressed text by just compressing the pattern and then running any classical
string matching algorithm like Boyer-Moore [13,9]. In Plain Huﬀman this does
not work, as the pattern could occur in the text not aligned to any codeword [7].
Instead of using a ﬂag bit to signal the beginning of a codeword, ETDC signals
the end of the codeword. That is, the highest bit of any codeword byte is 0 except
for the last byte, where it is set to 1.

This change has surprising consequences. Now the ﬂag bit is enough to ensure
that the code is a preﬁx code regardless of the contents of the other 7 bits of
each byte.

ETDC obtains a better compression ratio than Tagged Huﬀman while keeping
all its good searching and decompression capabilities. On the other hand, ETDC
is easier to build and faster in both compression and decompression.

In general, ETDC can be deﬁned over symbols of b bits, although in this paper
we focus on the byte-oriented version where b = 8.
Deﬁnition 1. Given source symbols with decreasing probabilities {pi}0≤i<n the
corresponding codeword using the End-Tagged Dense Code is formed by a sequence 
of symbols of b bits, all of them representing digits in base 2b−1 (that
is, from 0 to 2b−1 − 1), except the last one which has a value between 2b−1 and
2b − 1, and the assignment is done in a sequential fashion.
That is, the ﬁrst word is encoded as 1
0000001, until
¯
the 128th as 1
1111111. The 129th word is coded as 0
0000000:1
0000000, 130th as
¯
¯
¯
0000001 and so on until the (1282 + 128)th word 0
0
0000000:1
1111111:1
1111111.
¯
¯
¯
¯
Note that the code depends on the rank of the words, not on their actual frequency.
 As a result, only the sorted vocabulary must be stored with the compressed 
text to allow the decompressor to recover the source text.

0000000, the second as 1
¯

Improving Semistatic Compression Via Pair-Based Coding

127

It is clear that the number of words encoded with 1, 2, 3 etc, bytes is ﬁxed
(speciﬁcally 128, 1282, 1283 and so on) and does not depend on the word frequency 
distribution. Generalizing, being k the number of bytes in each codeword
(k ≥ 1) words at positions i:

2b−1 2(b−1)(k−1) − 1

2b−1 − 1

≤ i < 2b−1 2(b−1)k − 1
2b−1 − 1

will be encoded with k bytes. These clear limits mark the change points in the
codeword lengths and will be relevant in the PETDC that we present in this
paper.

But not only the sequential procedure is available to assign codewords to the
words. There are simple encode and decode procedures that can be eﬃciently
implemented, because the codeword corresponding to symbol in position i is
obtained as the number x written in base 2b−1, where x = i − 2(b−1)k−2b−1
and
k =

, and adding 2b−1 to the last digit.

2b−1−1

log2(2b−1+ (2b−1−1)i)

b−1

(cid:2)

(cid:3)

Function encode obtains the codeword Ci = encode(i) for a word at the ith 
position in the ranked vocabulary. Function decode gets the position i =
decode(Ci) in the vocabulary for a codeword Ci. Both functions take just O(l)
time, where l = O(log(i)/b) is the length in digits of codeword Ci. Those functions 
are eﬃciently implemented through just bit shifts and masking.

End-Tagged Dense Code is simpler, faster, and compresses 7% better than
Tagged Huﬀman codes. In fact, ETDC only produces an overhead of about
2% over Plain Huﬀman. On the other hand, since the last bytes of codewords
are distinguished, ETDC has all the search capabilities of Tagged Huﬀman code.
Empirical comparisons between ETDC and Huﬀman codes can be found in [5,4].

3 Pair-Based End-Tagged Dense Code

PETDC is a semistatic compressor based on ETDC. As in all semistatic compressors,
 a ﬁrst pass over the source text is performed in order to gather the
diﬀerent words of the source text (vocabulary) and their number of occurrences.
After that ﬁrst pass, an encoding scheme is used to assign a codeword to each
symbol in the vocabulary. A second pass over the source text replaces each source
symbol by the codeword associated to it. In addition, the compression process
ends by storing the vocabulary in such a way that, for each codeword, we can
obtain its corresponding original symbol.

However, there are some diﬀerences between PETDC and other semistatic
compressors such as ETDC, PH, etc. During the ﬁrst pass, PETDC obtains an
initial vocabulary by gathering the diﬀerent words and their number of occurrences 
in the source text. Moreover, PETDC collects also all the diﬀerent pairs
of words that appear adjacent in the source text and counts their number of
occurrences. PETDC aims to take advantage of the co-occurrence of words in
the text by including some pairs in the vocabulary (which is composed by both
single-words and pairs). Its main idea is simple: In classic semistatic compressors,

128

N.R. Brisaboa et al.

each symbol in the vocabulary has a unique codeword assigned by the encoding
scheme (in this case, ETDC is used). Therefore, replacing two source words by
a unique codeword during the second pass may need less bytes than replacing
two single words by two codewords. Example 1 clariﬁes this situation.

Example 1. Let us compress the sequence of words: ADCBACDCCDABABA
CDC with ETDC, assuming that special bytes of only 3 bits are used, and that
our words occupy 1 byte each. Therefore, the mapping word-codeword generated
01, D ← 1
10, B ← 1
by the encoding schema is: C ← 1
11. Hence, all
¯
¯
¯
the words can be encoded with only one byte, and the size of the compressed
text is 18 bytes. In this case, the vocabulary consists of only 4 words. As a result,
the size of the compressed ﬁle is 18 + 4 = 22 bytes.

00, A ← 1
¯

Let us add to the vocabulary the most frequent pair of words ‘BA’, and
compress the same text again. In this case, the vocabulary contains the symbols
‘C’, ‘D’, ‘BA’, ‘A’, and ‘B’, while the number of occurrences is 6, 4, 3, 2, and 0
respectively. Now the compressed text uses 15 bytes, and the vocabulary needs
(cid:6)(cid:7)
6 bytes. Therefore, the compressed ﬁle occupies 15 + 6 = 21 bytes.

From Example 1 we show that processing some pairs of words as a unique source
symbol reduces the size of the compressed text. However, as a drawback, the
size of the vocabulary grows when a pair is added. Therefore, the existence of a
trade-oﬀ between compressed text size and vocabulary size has to be taken into
account when pairs are added to the vocabulary.

3.1 Deciding Which Pairs Should Be Added to the Vocabulary

Adding all the diﬀerent pairs to the vocabulary is not a good idea because the
vocabulary would grow too much. In Figure 1(a), we show the evolution of the
size of a compressed ﬁle (as the sum of the size of the compressed data and the
size of the vocabulary) depending on the number of pairs added. As expected,
including the most frequent pairs in the vocabulary improves compression. However,
 at some point, the gain obtained by replacing two words by a unique
codeword does not compensate the growth of the vocabulary size.

In Figure 1(b) it is shown that the previous curve has multiple local minima.
This fact prevents us of looking for a heuristic to speed up the process by just
breaking the addition of pairs to the vocabulary when the addition of a new
pair worsens the compression. Instead of that, PETDC process all the pairs and
applies a heuristic to determine which ones have to be added.

Used heuristic. Let us assume that a pair αβ, composed of two words α and β,
is a candidate to be added to the vocabulary. Let us deﬁne fx as the number of
occurrences of a word or pair x. Let us also deﬁne Cx as the codeword that the
encoding scheme2 assigns to x, and let |Cx| be the length of that codeword. The
heuristic is based on comparing the number of bytes needed to encode all the

2 The codeword assigned to a word by ETDC depends only on the rank in the sorted

vocabulary.

Improving Semistatic Compression Via Pair-Based Coding

129

)
s
e
t
y
b
M

(
 
e
z
S

i

1.0

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0

Calgary Corpus (2Mbytes)

size of compressed file

x:2045
y:9.594

size of compressed text

size of the vocabulary

)
s
e

t
y
b
M

(
 

e

l
i
f
 

d
e
s
s
e
r
p
m
o
c
 
f

o

 

e
z
S

i

0.9630

0.9625

0.9620

0.9615

0.9610

0.9605

0.9600

0.9595

0.9590

2500

5000

Number of pairs added to the vocabulary

7500

10000

12500

Calgary Corpus (2Mbytes)

minimum (2045, 9.594)

1500 1750 2000 2250 2500 2750 3000 3250 3500

Number of pairs added to the vocabulary

(a) whole process.

(b) area close to maximum compression.

Fig. 1. Evolution of compressed ﬁle as pairs are added

occurrences of α and β in the text in two cases: i) The pair is skipped (skipbytes),
and ii) the pair is added to the vocabulary (addbytes). Once those values are
computed, the pair αβ is added to the vocabulary if skipbytes > addbytes and
skipped otherwise. Values skipbytes and addbytes are given by the two following
expressions:

skipbytes = fα ∗ |Cα| + fβ ∗ |Cβ|
addbytes = fαβ ∗ |Cαβ| + (fα − fαβ) ∗ |C
(cid:4)
α and C

(cid:4)
β are the codewords assigned to the words α and β assuming
Where C
that the pair αβ is added, and therefore their number of occurrences is fα − fαβ
and fβ − fαβ respectively. The term ‘K’ is an estimation of the number of bytes
needed to store any pair into the vocabulary. In general, K = 5.

(cid:4)
α

| + (fβ − fαβ) ∗ |C

(cid:4)
β

| + K

Particular cases. There are two special situations that arise when pairs of
words are considered:

– After adding a pair αβ to the vocabulary it is necessary to ensure that any
pair ending in α or beginning in β will not be included later. This happens
because, by an eﬃciency issue, we do not store all the words that precede or
follow any occurrence of αβ in the text. As a result, given the text ’γαβδαμ’,
adding the pair αβ implies that the pairs γα, βδ, and δα cannot be added to
the vocabulary. This is done by just marking α as “disabled as ﬁrst word of
pair” and marking β as “disabled as last word of pair”, and ﬁnally checking
those ﬂags before adding a pair to the vocabulary.

– Sequences of the same word: The appearance of sequences of the same word
α such as α1α2α3α4 might lead us to count erroneously the number of occurrences 
of the pair αα. Note that α2α3 is not a valid pair if the pair α1α2
is chosen. To avoid this problem, when a sequence is detected we only count
the occurrences of the pairs that start in odd positions.

130

N.R. Brisaboa et al.

3.2 Data Structures and Compression Procedure

The data structures used by the compressor are sketched in Figure 2. There are
two well-deﬁned parts: i) Data structures that make up the vocabulary, and ii)
data structures needed to hold the candidate pairs.

pairsVector

pairsVector

2

1

9

2

7

12

13

5

3

4

5

6

8

7

4 11 3

8

9

10

11

12

2

1

9

2

7

12

13

5

3

4

5

6

8

7

4 11 3

8

9

10

11

12

freq w1 w2

word

freq

type

e1

e2

w1 w2

freq w1 w2

word

freq

type

e1

e2

w1 w2

word

code type w1 w2

h
a
s
h
T
a
b
e

l

o

f

c
a
n
d

i
t

a

t

e

p
a
i
r
s

(
h
a
s
h
P
a
i
r
s
)

1

2

3 14 2

3

1

6 14

4

1

6

2

5

2

2

9

6

7

3

6

9

8

2

9

2

9

3

9

6

10

11

1

6

6

12

2

2

6

1

2

3

4

5

6

7

8

9

10

11

12

13

2

2 14

13

A

7 w t

t

C

6 w t

t

h
a
s
h
T
a
b
e

l

o

f

w
o
r
d
s

(
h
a
s
h
W
o
r
d
s
)

1

2

3 14 2

3

1

6 14

4

1

6

2

5

2

2

9

6

7

3

6

9

8

2

9

2

D

5 w t

t

9

3

9

6

10

11

1

6

6

12

2

2

6

13

2

2 14

h
a
s
h
T
a
b
e

l

o

f

c
a
n
d

i
t

a

t

e

p
a
i
r
s

(
h
a
s
h
P
a
i
r
s
)

1

2

3

4

5

6

7

8

9

10

11

12

13

A

4 w f

t

BA

3

p

14 2

C

6 w t

t

D

5 w t

t

h
a
s
h
T
a
b
e

l

o

f

w
o
r
d
s

(
h
a
s
h
W
o
r
d
s
)

h
a
s
h
T
a
b
e

l

o

f

w
o
r
d
s

(
h
a
s
h
W
o
r
d
s
)

A

c2 w

BA

c5 p 14 2

C

c1 w

D

c4 w

1

2

3

4

5

6

7

8

9

10

11

DC

c3 p

9

6

12

13

14

14

B

3 w t

t

14

14

B

0 w t

f

14

B

c6 w

wordsVect

topVect

2

1

4

1

6

2

4

2

9 14

3

4

5

6

7

8

4

3

3

4

3

5

2

6

1

7

8

9

wordsVect

topVect

2

1

4

1

6

2

4

2

9

14

4

3

4

5

6

3

3

3

4

2

5

1

6

7

-

7

wordsVect

6

2 11

9

4

14

8

1

2

3

4

5

6

7

8

8

9

(a) State after 1st pass

(b) State after adding pair BA

(c) State before 2nd pass

Fig. 2. Structures used in PETDC for text “ADCBACDCCDACADABABACDC”

– The vocabulary of the compressor consists of: A hash table used to locate
a word quickly (hashWords) and two vectors: wordsVect and topVect. The
hash table hashWords contains eight ﬁelds: i) type tells if an entry is either
a word ‘w’ or a pair ‘p’, ii) if the entry has type ‘w’, word stores the word
in ascii, iii) freq counts the number of occurrences of the entry, iv-v) e1 and
e2 ﬂag if the word is enabled to be the ﬁrst or second component of a pair
respectively, vi-vii) w1 and w2 store, for an entry of type ‘p’, pointers to the
words that form the pair, and viii) code stores the codeword assigned to each
entry of the vocabulary after the code generation phase.

Vector wordsVect serve us to maintain the vocabulary sorted by frequency.
Then, slot 1 of wordsVect points to the entry of hashWords where the most
frequent word (or pair) in the source text is stored. Assuming that wordsVect
is sorted decreasingly by frecuency, vector topVect[f] keeps track of the ﬁrst
entry in wordsVect whose frequency is f.

– Managing the candidate pairs includes also the use of two main data struc-
tures: i) A hash table hashPairs with ﬁelds freq, w1, and w2, used to give a
fast access to each candidate pair, and ii) a vector pairsVector that maintains
all the candidate pairs sorted, in the same way as wordsVect.

Compressing with PETDC. Compression consists of ﬁve main phases:

– First pass along the text. As shown, during this pass, PETDC obtains the
diﬀerent v single-words and the diﬀerent p candidate pairs that appear in

Improving Semistatic Compression Via Pair-Based Coding

131

the text. Moreover, it also counts their occurrences. The process costs O(n),
being n the number of words in the text. When the ﬁrst pass ends, vectors
pairsVect and wordsVect are sorted by decreasing frequency. Finally, topVect
is initialized. Starting from element n downto 1, topV ec[i] = j if j is the ﬁrst
entry in wordsV ect such that hashW ords[wordsV ect[j]].f req = i. If (cid:2)j such
that hashW ords[wordsV ect[j]].f req = i, then topV ect[i] = topV ect[i + 1].
The overall cost of this ﬁrst phase is O(n)+O(v log(v))+O(p log(p))+O(v) =
O(n) + O(p log(p)). Since n (cid:9) p, we empirically proved that it costs O(n).
– Choosing and adding candidate pairs. During this phase, pairsV ector is traversed 
(O(p)). A candidate pair αβ is either added to the vocabulary or
discarded, by applying the heuristic explained in Section 3.1. To compute
that heuristic we need to know the current position3 of α and β in the vocabulary 
to know the size of their respective codewords (|Cx|). We also need
to assume that the pair αβ is added, and we need to compute the new ranks
of αβ, α, and β in the new ordered vocabulary. Since maintaining the vocabulary 
ordered upon inserting a pair is too expensive, we only maintain
topV ect updated in such a way that, given a frequency f, topV ect[f] stores
the rank, in a sorted vocabulary, of the ﬁrst entry of frequency f. Then,
being x an entry with frequency fx, we estimate |Cx| as |C(topV ect[fx])|. It
costs O(F ) time, where F is the frequency of the second most frequent entry
of the vocabulary. Of course, αβ is also inserted into hashW ords and into
wordsV ector. The overall cost of this phase is O(paF + p) = O(paF ), being
pa the number of pairs added to the vocabulary. Figure 2(b) shows the result
of adding the pair “BA” to the vocabulary.

– Code Generation Phase. The only data structures needed in this phase are
(cid:4) entries) is ordered by fredepicted 
in Figure 2(c). The vocabulary (with v
(cid:4))
quency and the encoding scheme of ETDC is used. Encoding takes O(v
time. As a result, hashW ords will contain the mapping entryi → codei ∀i ∈
1 . . . v

(cid:4). The cost of this phase is O(v

(cid:4) log v

(cid:4)).

– Second pass. The text is traversed again reading two words at a time and replacing 
source words by codewords. If the read pair αβ belongs to hashW ords
then the codeword Cαβ is output and two new words γδ are read. Otherwise
Cα is output and the only the following word γ is read to form a new pair
βγ. This phase takes O(n) time.

– storing the vocabulary. As in ETDC, the vocabulary is stored along with the
compressed data to permit decompression. A bitmask is used to save the
(cid:4) entries of the vocabulary follow that bitmask. A
type of entry. Then the v
single-word is written in ascii (ending in ‘\0’). To store a pair αβ we write
the relative positions of α and β in the vocabulary (encoded with the on-
the-ﬂy Cw = getCode(i) function used in [3] in order to save space). Finally,
the whole vocabulary is encoded with character-based Huﬀman.

Decompressing text compressed with PETDC. Decompression starts by
loading the vocabulary into a vector. Then each codeword is easily parsed due

3 Using the encoding scheme of ETDC, we can compute in O(log i) time the codeword

Ci = getCode(i) where i is the rank of a word wi in the vocabulary.

132

N.R. Brisaboa et al.

to the ﬂag bit that marks the end of a codeword. For each codeword Ci, the
function i = decode(Ci) [3] is used to obtain the entry i that contains either the
word or the pair associated to Ci.
Searching text compressed with PETDC. In text compressed with PETDC,
a word α can appear alone or as a part of one or more pairs αβ, γα,... Therefore
searches will usually imply using a multi-pattern matching algorithm. When we
load the vocabulary, we can easily generate for each single-word α, a list with
the codewords of the pairs in which it appears. After that, an algorithm from
the Boyer-Moore family such as Set Horspool [9,13] is used to search for those
codewords and for the codeword Cα.

4 Empirical Results

We compare PETDC against other semi-static word-based compressors such as
PH and ETDC and against two well-known compressors such as Gnu gzip4, a ZivLempel 
compressor and Seward’s bzip2 5, a compressor based on the BurrowsWheeler 
transform [6]. We used some large text collections from trec-2 6,
namely AP Newswire 1988 (AP) and Ziﬀ Data 1989-1990 (ZIFF), as well as
some from trec-4: Congressional Record 1993 and Financial Times 1991 to
1994. We used the spaceless word model [7] to create the vocabulary; that is, if
a word was followed by a space, we just encoded the word, otherwise both the
word and the separator were encoded.

Our ﬁrst experiment is focused in comparing compression ratio. Table 1 shows
in the two ﬁrst columns the corpora used and their size. Columns three to six
gives information about applying PETDC such as the number of candidate pairs,
the number of occurrences of the most frequent pair, the number of pairs added,
and the number of entries (pairs and words) of the vocabulary. The last ﬁve
columns show compression ratio (in percentage) for the compressors used. It can
be seen that PETDC improves the compression of PH and ETDC by around 3
and 4 percentage points respectively. Gaps against gzip grow up to 6−7 percentage 
points. Finally, PETDC is overcome by Bzip2 in around 1 − 3 percentage
points.
We focus now on comparing PETDC against other alternatives in compression 
and decompression speed. An isolated Intel R(cid:2)Pentium R(cid:2)-IV 3.00 GHz system
(16Kb L1 + 1024Kb L2 cache), with 512 MB single-channel DDR-400Mhz was
used in our tests. It ran Mandrake Linux 9.2. The compiler used was gcc version
3.3.1 and -O9 compiler optimizations were set. Time results measure cpu user
time in seconds. As it is shown in Table 2, PETDC pays the extra-cost of managing 
pairs during compression, being around 2.5 times slower than ETDC and
PH, and around 1.5 − 2 times slower than gzip. However, it is much faster than
bzip2. In decompression, the extra-cost of PETDC consists only in processing

4 http://www.gnu.org.
5 http://www.bzip.org.
6 http://trec.nist.gov.

Improving Semistatic Compression Via Pair-Based Coding

133

Table 1. Compressing with PETDC and comparison in compression ratio with others

Corpus

Calgary
FT91

cand. highest
freq.
pairs
53,595
2,618
18,030
239,428
589,692
70,488

Size
(bytes)
2,131,045
14,749,355
CR 51,085,545

pairs
added
4,705
12,814
38,090
FT92 175,449,235 1,592,278 222,505 113,610
ZIFF 185,200,215 2,585,115 344,262 194,731
FT93 197,586,294 1,629,925 236,326 124,547
FT94 203,783,923 1,666,650 242,816 123,583
AP 250,714,271 1,574,819 663,586 118,053
ALL FT 591,568,807 3,539,265 709,233 314,568

entries
vocab PETDC
35,700
41.31
30.72
88,495
27.66
155,803
28.46
397,671
29.04
418,132
27.79
415,976
27.78
418,601
355,674
28.80
27.80
891,308

Compression ratio (%)

PH ETDC
43.54
34.02
31.30
32.33
33.41
32.43
32.39
32.73
32.22

42.39
33.13
30.41
31.52
32.52
31.55
31.50
31.92
31.34

gzip bzip2
37.10
29.14
27.06
36.42
24.14
33.29
27.10
36.48
25.11
33.06
25.32
34.21
25.27
34.21
37.32
27.22
25.91
34.94

Table 2. Comparison in compression and decompression time

Corpus

Compression time (seconds)

Decompression time (seconds)
PETDC PH ETDC gzip bzip2 PETDC PH ETDC gzip bzip2
0.06 0.04 0.32
0.25 0.20 2.24
0.65 0.64 7.52
2.50 2.36 29.42
2.32 2.20 27.00
2.75 2.62 32.58
2.87 2.63 33.78
3.41 3.43 39.42
8.12 7.49 89.67

0.88
0.37 0.26
0.59 0.33
1.47 1.71
6.33
3.07 1.48
4.01 5.83 21.26
8.85 3.97
33.24 13.86 13.85 20.28 76.47
33.47 13.84 13.87 20.20 79.70
35.81 15.61 15.30 21.34 80.21
37.11 15.84 15.73 22.08 88.96
48.50 20.06 20.26 30.66 105.48
ALL FT 113.26 45.29 45.12 64.82 254.45

0.06 0.06
0.30 0.25
0.78 0.66
2.90 2.39
2.83 2.35
3.17 2.73
3.33 2.81
4.11 3.41
9.63 8.00

Calgary
FT91
CR
FT92
ZIFF
FT93
FT94
AP

the bitmask in the header of the vocabulary ﬁle and rebuilding the pairs from
the pointers to single-words. Therefore, the loss of speed against PH, ETDC,
and gzip is small (under 20%), and PETDC becomes around 6 − 8 times faster
than bzip2.

5 Conclusions

We have presented a new semistatic pair-based byte-oriented compressor that
we named Pair-Based End-Tagged Dense code(PETDC). It takes advantage of
using both words and pairs of words (exploiting the co-occurrence of words) to
improve the compression obtained by similar word-based semistatic techniques
such as PH or ETDC.

Dealing with pairs has a cost in compression speed (PETDC is around 2.5
times slower than ETDC) and in decompression (PETDC is 20% slower than
ETDC). However, the new technique is able to reduce English texts to 27–28% of
its original size (over 3 percentage points better than PH). Moreover, it maintains
the ability of performing direct searches and random decompression of a portion
of the text.

To sum up, PETDC is a technique well-suited to use in Text Databases
due to its good compression ratio and decompression speed, as well as for its
good search capabilities. The main drawback with respect to others might be

134

N.R. Brisaboa et al.

its medium compression speed. However, in a text retrieval scenario a medium
compression speed is only a minor problem since compression is done only once.

Acknowledgments. We want to thank `Angel Y`a˜nez Miragaya for his help in
the implementation of PETDC.

References

1. T. C. Bell, J. G. Cleary, and I. H. Witten. Text Compression. P.Hall, 1990.
2. Robert S. Boyer and J. Strother Moore. A fast string searching algorithm. Communications 
of the ACM, 20(10):762–772, October 1977.

3. N. Brisaboa, A. Fari˜na, G. Navarro, and Jos´e R. Param´a. Simple, fast, and eﬃcient
natural language adaptive compression. In Proceedings of the 11th SPIRE, LNCS
3246, pages 230–241, 2004.

4. N. Brisaboa, A. Fari na, G. Navarro, and J. Param´a. Lightweight natural language

text compression. Information Retrieval, 2006. To appear.

5. N.R. Brisaboa, E.L. Iglesias, G. Navarro, and Jos´e R. Param´a. An eﬃcient compression 
code for text databases. In Proceedings of the 25th ECIR, LNCS 2633,
pages 468–481, 2003.

6. M. Burrows and D. J. Wheeler. A block-sorting lossless data compression algorithm.
 Technical Report 124, 1994.

7. E. Silva de Moura, G. Navarro, N. Ziviani, and R. Baeza-Yates. Fast and ﬂexible

word searching on compressed text. ACM TOIS, 18(2):113–139, 2000.

8. M. Farach and M. Thorup. String matching in lempel-ziv compressed strings. In

Proceedings of the 27th ACM-STOC, pages 703–712, 1995.

9. R. N. Horspool. Practical fast searching in strings. SPE, 10(6):501–506, 1980.
10. D. A. Huﬀman. A method for the construction of minimum-redundancy codes.

Proc. Inst. Radio Eng., 40(9):1098–1101, 1952.

11. A. Moﬀat. Word-based text compression. SPE, 19(2):185–198, 1989.
12. G. Navarro, E.S. de Moura, M. Neubert, N. Ziviani, and R. Baeza-Yates. Adding

compression to block addressing inverted indexes. IR, 3(1):49–77, 2000.

13. G. Navarro and M. Raﬃnot. Flexible Pattern Matching in Strings – Practical online 
search algorithms for texts and biological sequences. Cambridge Univ. Press,
2002.

14. G. Navarro and J. Tarhio. Boyer-Moore string matching over Ziv-Lempel compressed 
text. In Proceedings of the 11th CPM, LNCS 1848, pages 166–180, 2000.
15. J. Ziv and A. Lempel. A universal algorithm for sequential data compression. IEEE

16. J. Ziv and A. Lempel. Compression of individual sequences via variable-rate coding.

TIT, 23(3):337–343, 1977.

IEEE TIT, 24(5):530–536, 1978.

