New Lower and Upper Bounds

for Representing Sequences(cid:2)

Djamal Belazzougui1 and Gonzalo Navarro2

1 LIAFA, Univ. Paris Diderot - Paris 7, France

2 Department of Computer Science, University of Chile

dbelaz@liafa.jussieu.fr

gnavarro@dcc.uchile.cl

Abstract. Sequence representations supporting queries access, select
and rank are at the core of many data structures. There is a considerable 
gap between diﬀerent upper bounds, and the few lower bounds,
known for such representations, and how they interact with the space
used. In this article we prove a strong lower bound for rank, which holds
for rather permissive assumptions on the space used, and give matching
upper bounds that require only a compressed representation of the sequence.
 Within this compressed space, operations access and select can
be solved within almost-constant time.

1

Introduction

A large number of data structures build on sequence representations. In particular,
 supporting the following three queries on a sequence S[1, n] over alphabet
[1, σ] has proved extremely useful:
– access(S, i) gives S[i];
– selecta(S, j) gives the position of the jth occurrence of a ∈ [1, σ] in S; and
– ranka(S, i) gives the number of occurrences of a ∈ [1, σ] in S[1, i].
For example, Ferragina and Manzini’s FM-index [9], a compressed indexed representation 
for text collections that supports pattern searches, is most successfully
implemented over a sequence representation supporting access and rank [10].
Grossi et al. [18] had used earlier similar techniques for text indexing, and invented 
wavelet trees, a compressed sequence representation that solves the three
queries in time O(lg σ). The time was reduced to O( lg σ
lg lg n ) with multiary wavelet
trees [10,17].1 Golynski et al. [16] used these operations for representing labeled
trees and permutations, and proposed another representation that solved the
operations in time O(lg lg σ), and some even in constant time. This representation 
was made compressed by Barbay et al. [1]. Further applications of the three
operations to multi-labeled trees and binary relations were uncovered by Barbay
et al. [2]. Ferragina et al. [8] and Gupta et al. [20] devised new applications to
(cid:2) Partially funded by Fondecyt Grant 1-110066, Chile. First author also partially supported 
by the French ANR-2010-COSI-004 MAPPI Project.

1 For simplicity, throughout this paper we will assume that lg x means max(1, lg x).

Similarly, O(x) will mean O(max(1, x)) and o(x) will mean o(max(1, x)).

L. Epstein and P. Ferragina (Eds.): ESA 2012, LNCS 7501, pp. 181–192, 2012.
c(cid:2) Springer-Verlag Berlin Heidelberg 2012

182

D. Belazzougui and G. Navarro

XML indexing. Barbay et al. [3,1] gave applications to representing permutations
and inverted indexes. Claude and Navarro [7] presented applications to graph
representation. M¨akinen and V¨alim¨aki [29] and Gagie et al. [13] applied them to
document retrieval on general texts.

The most basic case is that of bitmaps, when σ = 2. In this case obvious applications 
are set representations supporting membership and predecessor search.
We assume throughout this article the RAM model with word size w = Ω(lg n).
Jacobson [21] achieved constant-time rank using o(n) extra bits on top of a plain
representation of S, and Munro [23] and Clark [6] achieved also constant-time
select. Golynski [14] showed a lower bound of Ω(n lg lg n/ lg n) extra bits for
supporting both operations in constant time if S is to be represented in plain
form (i.e., as an array of n bits), and gave matching upper bounds. When S
c n) bits
can be represented arbitrarily, Patrascu [25] achieved lg
of space, where m is the number of 1s in S and c is any constant, and showed
this is optimal [28].

+ O(n/ lg

(cid:2)
n
m

(cid:3)

(cid:4)

a∈[1,σ]

na
n lg n

For general sequences, a useful measure of compressibility is the zero-order
na , where na is the number of occurentropy 
of S, H0(S) =
rences of a in S. This can be extended to the k-th order entropy, Hk(S) =
(cid:4)
A∈[1,σ]k |TA|H0(TA), where TA is the string of symbols following k-tuple A
1
in S. It holds 0 ≤ Hk(S) ≤ Hk−1(S) ≤ H0(S) ≤ lg σ for any k, but the entropy
n
measure is only meaningful for k < lgσ n. See Manzini [22] and Gagie [12] for a
deeper discussion.

When representing sequences supporting these operations, we may aim at
using O(n lg σ) bits of space, but frequently one aims for less space. We may
aim at succinct representation of S, taking n lg σ + o(n lg σ) bits, at a zero-order
compressed representation, taking at most nH0(S)+o(n lg σ) bits (we might also
wish to compress the redundancy, o(n lg σ), to achieve for example nH0(S) +
o(nH0(S))), or at a high-order compressed representation, nHk(S) + o(n lg σ).
Upper and lower bounds for sequence representations supporting the three
operations are far less understood over larger alphabets. When σ = O(polylog n),
the three operations can be carried out in constant time over a data structure
using nH0(S)+o(n) bits [10]. For larger alphabets, this solution requires the same
space and answers the queries in time O( lg σ
lg lg n ) [10,17]. Another class of solutions
[16,19,1], especially attractive for “large alphabets”, achieves time O(lg lg σ) for
rank. For access and select they oﬀer complementary complexities, where one
of the operations is constant-time and the other requires O(lg lg σ) time. They
achieve zero-order compressed space, nH0(S) + o(nH0(S)) + o(n) bits [1], and
even high-order compressed space, nHk(S) + o(n lg σ) for any k = o(lgσ n) [19].
There are several curious aspects in the map of the current solutions for
general sequences. On one hand, the times for access and select seem to be
complementary, whereas that for rank is always the same. On the other hand,
there is no smooth transition between the complexity of one solution, O( lg σ
lg lg n ),
and that of the other, O(lg lg σ).

The complementary nature of access and select is not a surprise. Golynski
[15] gave lower bounds that relate the time performance that can be achieved

New Lower and Upper Bounds for Representing Sequences

183

for these operations with the redundancy of an encoding of S on top of its
information content. The lower bound acts on the product of both times, that
is, if t and t(cid:4)
are the time complexities, and ρ is the bit-redundancy per symbol,
then ρ · t · t(cid:4)
= Ω((lg σ)2/w) holds for a wide range of values of σ. The upper
bounds for large alphabets [16,19] match this lower bound.

Although operation rank seems to be harder than the others (at least no
constant-time solution exists except for polylog-sized alphabets), no general
lower bounds on this operation have been proved. Only a recent result for the
case in which S must be encoded in plain form states that if one solves rank
within a = O( lg σ
lg lg σ ) access to the sequence, then the redundancy per symbol
is ρ = Ω((lg σ)/a) [19]. Since in the RAM model one can access up to w/ lg σ
symbols in one access, this implies a lower bound of ρ· t = Ω((lg σ)2/w), similar
to the one by Golynski [15] for the product of access and select times.

In this article we make several contributions that help close the gap between

lower and upper bounds on sequence representation.

1. We prove the ﬁrst general lower bound on rank, which shows that this
operation is, in a sense, noticeably harder than the others: No structure
using O(n · wO(1)) bits can answer rank queries in time o(lg lg σ
lg w ). Note the
space includes the rather permissive O(n· polylog n). For this range of times
our general bound is much stronger than the existing restricted one [19],
which only forbids achieving it within n lg σ + O(n lg2 σ/(w lg lg σ
lg w )) bits.
Our lower bound uses a reduction from predecessor queries.

2. We give a matching upper bound for rank, using O(n lg σ) bits of space and
answering queries in time O(lg lg σ
lg w ). This is lower than any time complexity
achieved so far for this operation within O(n · wO(1)) bits, and it elegantly
uniﬁes both known upper bounds under a single and lower time complexity.
This is achieved via a reduction to a predecessor query structure that is
tuned to use slightly less space than usual.

3. We derive succinct and compressed representations of sequences that achieve
time O( lg σ
lg w ) for access, select and rank, improving upon previous results
[10]. This yields constant-time operations for σ = wO(1). Succinctness is
achieved by replacing universal tables used in other solutions with bit manipulations 
on the RAM model. Compression is achieved by combining the
succinct representation with existing compression boosters.

4. We derive succinct and compressed representations of sequences over larger
alphabets, which achieve time O(lg lg σ
lg w ) for rank, which is optimal, and
almost-constant time for access and select. The result improves upon almost
all succinct and compressed representations proposed so far [16,2,1,19]. This
is achieved by plugging our O(n lg σ)-bit solutions into existing succinct and
compressed data structures.

Our results assume a RAM model where bit shifts, bitwise logical operations,
and arithmetic operations (including multiplication) are permitted. Otherwise
we can simulate them with universal tables within o(n) extra bits of space, but
all lg w in our upper bounds become lg lg n.

184

D. Belazzougui and G. Navarro

2 Lower Bound for rank

(cid:5)

a

(cid:6)(cid:6)

(cid:3)−lg n

(cid:5)
lg

Our technique is to reduce from a predecessor problem and apply the densityaware 
lower bounds of Patrascu and Thorup [26]. Assume that we have n keys
from a universe of size u = nσ, then the keys are of length (cid:5) = lg u = lg n + lg σ.
According to branch 2 of Patrascu and Thorup’s result, the time for predecessor
, where a = lg(s/n) +
queries in this setting is lower bounded by Ω
lg w and s is the space in words of our representation (the lower bound is in
the cell probe model for word length w, so the space is always expressed in
number of cells). The lower bounds holds even for a more restricted version
of the predecessor problem in which one of two colors is associated with each
element and the query only needs to return the color of the predecessor. We
assume σ = O(n); the other case will be considered at the end of the section.
The reduction is as follows. We divide the universe n· σ into σ intervals, each
of size n. This division can be viewed as a binary matrix of n columns by σ rows,
where we set a 1 at row r and column c iﬀ element (r − 1) · n + c belongs to the
set. We will use four data structures.

1. A plain bitvector L[1, n] which stores the color associated with each element.

The array is indexed by the original ranks of the elements.

2. A partial sums structure R stores the number of elements in each row. It
is a bitmap concatenating the σ unary representations, 1nr 0, of the number
of 1s in each row r ∈ [1, σ]. Thus R is of length n + σ and can give in
constant time the number of 1s up to (and including) any row r, count(r) =
rank1(R, select0(R, r)) = select0(R, r)− r, in constant time and O(n + σ) =
O(n) bits of space [23,6].

3. A column mapping data structure C that maps the original columns into
a set of columns where (i) empty columns are eliminated, and (ii) new
columns are created when two or more 1s fall in the same column. C is a
bitmap concatenating the n unary representations, 1nc0, of the numbers nc
of 1s in each column c ∈ [1, n]. So C is of length 2n. Note that the new
matrix of mapped columns has also n columns (one per element in the set)
and exactly one 1 per column. The original column c is then mapped to
col(c) = rank1(C, select0(C, c)) = select0(C, c) − c, using constant time and
O(n) bits. Note that col(c) is the last of the columns to which the original
column c might have been expanded.

4. A string S[1, n] over alphabet [1, σ], so that S[c] = r iﬀ the only 1 at column
c (after column remapping) is at row r. Over this string we build a data
structure able to answer queries rankr(S, c).

Queries are done in the following way. Given an element x ∈ [1, u], we ﬁrst
deompose it into a pair (r, c) where x = (r − 1) · n + c. In a ﬁrst step, we
compute count(r − 1) in constant time. This gives us the count of elements up
to point (r − 1) · n. Next we must compute the count of elements in the range
[(r − 1) · n + 1, (r − 1) · n + c]. For doing that we ﬁrst remap the column to
c(cid:4)
), which gives the

= col(c) in constant time, and ﬁnally compute rankr(S, c(cid:4)

New Lower and Upper Bounds for Representing Sequences

185

number of 1s in row r up to column c(cid:4)
. Note that if column c was expanded to
several ones, we are counting the 1s up to the last of the expanded columns, so
that all the original 1s at column c are counted at their respective rows. Then
the rank of the predecessor of x is p = count(r − 1) + rankr(S, col(c)). Finally,
the color associated with x is given by L[p].

Theorem 1. Given a data structure that supports rank queries on strings of
length n over alphabet [1, σ] in time t(n, σ) and using s(n, σ) bits of space, we
can solve the colored predecessor problem for n integers from universe [1, nσ] in
time t(n, σ) + O(1) using a data structure that occupies s(n, σ) + O(n) bits.

By the reduction above we get that any lower bound for predecessor search for n
keys over a universe of size nσ must also apply to rank queries on sequences of
length n over alphabet of size σ. In our case, if we aim at using O(n· wO(1)) bits
of space, this lower bound (branch 2 [26]) is Ω
.

(cid:3)−lg n

= Ω

(cid:5)

lg

lg lg σ
lg w

(cid:6)

(cid:5)

(cid:6)

lg(s/n)+lg w

For σ = Θ(n) and w = Θ(lg n), the bound is simply Ω(lg lg σ). In case σ =
lg w ) must still be a lower bound, as otherwise we could break it in

ω(n), Ω(lg lg σ
the case σ = O(n) by just declaring σ artiﬁcially larger.
Theorem 2. Any data structure that uses space O(n · wO(1)) bits to represent
a sequence of length n over alphabet [1, σ], must use time Ω(lg lg σ
lg w ) to answer
rank queries.

For simplicity, assume w = Θ(lg n). This lower bound is trivial for small lg σ =
O(lg lg n) (i.e., σ = O(polylog n)), where constant-time solutions for rank exist
that require only nH0(S) + o(n) bits [10]. On the other hand, if σ is suﬃciently
large, lg σ = Ω((lg lg n)1+) for any constant  > 0, the lower bound becomes simply 
Ω(lg lg σ), where it is matched by known compact and compressed solutions
[16,1,19] requiring as little as nH0(S) + o(nH0(S)) + o(n) or nHk(S) + o(n lg σ)
bits.

The range where this lower bound has not yet been matched is ω(lg lg n) =
lg σ = o((lg lg n)1+), for any constant  > 0. The next section presents a new
matching upper bound.

3 Optimal Upper Bound for rank

We now show a matching upper bound with optimal time and space O(n lg σ)
bits. In the next sections we make the space succinct and even compressed.

We reduce the problem to predecessor search and then use an existing solution
for that problem. The idea is simply to represent the string S[1, n] over alphabet
[1, σ] as a matrix of σ rows and n columns, and regard the matrix as the set
of n points {(S[c] − 1) · n + c, c ∈ [1, n]} over the universe [1, nσ]. Then we
store an array of n cells containing (cid:4)r, rankr(S, c)(cid:5), where r = S[c], for the point
corresponding to column c in the set.
To query rankr(S, c) we compute the predecessor of (r − 1) · n + c. If it is a
pair (cid:4)r, v(cid:5), for some v, then the answer is v. Else the answer is zero.

186

D. Belazzougui and G. Navarro

This solution requires n lg σ + n lg n bits for the pairs, on top of the space
of the predecessor query. If σ ≤ n we can reduce this extra space to 2n lg σ by
storing the pairs (cid:4)r, rankr(S, c)(cid:5) in a diﬀerent way. We virtually cut the string
into chunks of length σ, and store the pair as (cid:4)r, rankr(S, c) − rankr(S, c −
(c mod σ))(cid:5). The rest of the rankr information is obtained in constant time and
O(n) bits using Golynski et al.’s [16] reduction to chunks: They store a bitmap
A[1, 2n] where the matrix is traversed row-wise and we append to A a 1 for
each 1 found in the matrix and a 0 each time we move to the next chunk (so
we append n/σ 0s per row). Then the remaining information for rankr(S, c)
is rankr(S, c − (c mod σ)) = select0(A, p1) − select0(A, p0) − (c div σ), where
p0 = (r − 1) · n/σ and p1 = p0 + (c div σ) (we have simpliﬁed the formulas by
assuming σ divides n).

Theorem 3. Given a solution for predecessor search on a set of n keys chosen
from a universe of size u, that occupies space s(n, u) and answers in time t(n, u),
there exists a solution for rank queries on a sequence of length n over an alphabet
[1, σ] that runs in time t(n, nσ) + O(1) and occupies s(n, nσ) + O(n lg σ) bits.

In the extended version of their article, Patrascu and Thorup [27] give an upper
bound matching the lower bound of branch 2 and using O(n lg u) bits for n
elements over a universe [1, u], and give hints to reduce the space to O(n lg(u/n)).
For completeness, we do this explicitly in an extended version of the present
paper [5, App. A]. By using this predecessor data structure, the following result
on rank is immediate.

Theorem 4. A string S[1, n] over alphabet [1, σ] can be represented in O(n lg σ)
bits, so that operation rank is solved in time O(lg lg σ
Note that, within this space, operations access and select can also be solved in
constant time.

lg w ).

4 Optimal-Time rank in Succinct and Compressed Space

We start with a sequence representation using n lg σ+o(n lg σ) bits (i.e., succinct)
that answers access and select queries in almost-constant time, and rank in time
O(lg lg σ
lg w ). This is done in two phases: a constant-time solution for σ = wO(1),
and then a solution for general alphabets. Then we address compression.

4.1 Succinct Representation for Small Alphabets
Using multiary wavelet trees [10] we can obtain succinct space and O( lg σ
lg lg n )
time for access, select and rank. This is constant for lg σ = O(lg lg n). We start
by extending this result to the case lg σ = O(lg w), as a base case for handling
larger alphabets thereafter. More precisely, we prove the following result.

Theorem 5. A string S[1, n] over alphabet [1, σ] can be represented using n lg σ
+ o(n lg σ) bits so that operations access, select and rank can be solved in time
lg w ). If σ = wO(1), the space is n(cid:6)lg σ(cid:7) + o(n) bits and the times are O(1).
O( lg σ

New Lower and Upper Bounds for Representing Sequences

187

A multiary wavelet tree for S[1, n] divides, at the root node v, the alphabet [1, σ]
into r contiguous regions of the same size. A sequence Rv[1, n] recording the region 
each symbol belongs to is stored at the root node (note Rv is a sequence
over alphabet [1, r]). This node has r children, each handling the subsequence
of S formed by the symbols belonging to a given region. The children are decomposed 
recursively, thus the wavelet tree has height O(lgr σ). Queries access,
select and rank on sequence S[1, n] are carried out via O(lgr σ) similar queries
on the sequences Rv stored at wavelet tree nodes [18]. By choosing r such that
lg r = Θ(lg lg n), it turns out that the operations on the sequences Rv can be
carried out in constant time, and thus the cost of the operations on the original
sequence S is O( lg σ

lg lg n ) [10].

In order to achieve time O( lg σ

lg w ), we need to handle in constant time the
operations over alphabets of size r = wβ, for some 0 < β < 1, so that lg r =
Θ(lg w). This time we cannot resort to universal tables of size o(n), but rather
must use bit manipulation on the RAM model.

The sequence Rv[1, n] is stored as the concatenation of n ﬁelds of length lg r,
into consecutive machine words. Thus achieving constant-time access is trivial:
To access Rv[i] we simply extract the corresponding bits, from the (1 + (i − 1) ·
lg r)-th to the (i · lg r)-th, from one or two consecutive machine words, using bit
shifts and masking.

Operations rank and select are more complex. We will proceed by cutting
the sequence Rv into blocks of length b = wα symbols, for some β < α < 1.
First we show how, given a block number i and a symbol a, we extract from
R[1, b] = Rv[(i − 1) · b + 1, i · b] a bitmap B[1, b] such that B[j] = 1 iﬀ R[j] = a.
Then we use this result to achieve constant-time rank queries. Next, we show
how to solve predecessor queries in constant time, for several ﬁelds of length lg w
bits ﬁtting in a machine word. Finally, we use this result to obtain constant-time
select queries.
Projecting a Block. Given sequence R[1, b] = Rv[1 + (i − 1) · b, i · b], which is of
length b · (cid:5) = wα lg r < wα lg w = o(w) bits, where (cid:5) = lg r, and given a ∈ [1, r],
we extract B[1, b · (cid:5)] such that B[j · (cid:5)] = 1 iﬀ R[j] = a.
To do so, we ﬁrst compute X = a · (0(cid:3)−11)b. This creates b copies of a within
(cid:5)-bit long ﬁelds. Second, we compute Y = R xor X, which will have zeroed
ﬁelds at the positions j where R[j] = a. To identify those ﬁelds, we compute
Z = (10(cid:3)−1)b − Y , which will have a 1 at the highest bit of the zeroed ﬁelds in
Y . Now W = Z and (10(cid:3)−1)b isolates those leading bits.

Constant-time rank Queries. We now describe how we can do rank queries in
√
constant time for Rv[1, n]. Our solution follows that of Jacobson [21]. We choose a
w− 1)/ lg r. For each a ∈ [1, r], we
superblock size s = w2 and a block size b = (
store the accumulated values per superblock, ranka(Rv, i· s) for all 1 ≤ i ≤ n/s.
We also store the within-superblock accumulated values per block, ranka(Rv, i ·
b) − ranka(Rv,(cid:9)(i · b)/s(cid:10) · s), for 1 ≤ i ≤ n/b. Both arrays of counters require,
√
over all symbols, r((n/s)· w + (n/b)· lg s) = O(nwβ (lg w)2/
w) bits. Added over

188

D. Belazzougui and G. Navarro

lg w ) wavelet tree levels, the space required is O(n lg σ lg w/w1/2−β ) bits.

the O( lg σ
This is o(n lg σ) for any β < 1/2, and furthermore it is o(n) if σ = wO(1).
To solve a query ranka(Rv, i), we need to add up three values: (i) the superblock 
accumulator at position (cid:9)i/s(cid:10), (ii) the block accumulator at position
(cid:9)i/b(cid:10), (iii), the bits set at B[1, (i mod b) · (cid:5)], where B corresponds to the values
equal to a in Rv[(cid:9)i/b(cid:10) · b + 1,(cid:9)i/b(cid:10)· b + b]. We have shown above how to extract
B[1, b · (cid:5)], so we count the number of bits set in C = B and 1(i mod b)·(cid:3).

√
This counting is known as a popcount operation. Given a bit block of length
w − 1, with bits set at positions multiple of (cid:5), we popcount it using the

b(cid:5) =
following steps:

1. We ﬁrst duplicate the block b times into b ﬁelds. That is, we compute X =

C · (0b(cid:3)−11)b.
2. We now isolate a diﬀerent bit in each diﬀerent ﬁeld. This is done with Y =
X and (0b(cid:3)10(cid:3)−1)b. This will isolate the ith aligned bit in ﬁeld i.
3. We now sum up all those isolated bits using the multiplication Z = Y ·
(0b(cid:3)+(cid:3)−11)b. The end result of the popcount operation lies at the bits Z[b2(cid:5) +
1, b2(cid:5) + lg b].

4. We ﬁnally extract the result as c = (Z (cid:11) b2(cid:5)) and (1lg b).

Constant-time select Queries. The solution to select queries is similar but more
technical. For lack of space we describe it in the extended version [5, Sec. 4.1.3].

4.2 Succinct Representation for Larger Alphabets

We assume now lg σ = ω(lg w); otherwise the previous section achieves succinctness 
and constant time for all operations.

We build on Golynski et al.’s solution [16]. They ﬁrst cut S into chunks of
length σ. With bitvector A[1, 2n] described in Section 3 they reduce all the
queries, in constant time, to within a chunk. For each chunk they store a bitmap
X[1, 2σ] where the number of occurrences of each symbol a ∈ [1, σ] in the chunk,
na, is concatenated in unary, X = 1n1 01n20 . . . 1nσ 0. Now they introduce two
complementary solutions.
Constant-time Select. The ﬁrst one stores, for each consecutive symbol a ∈ [1, σ],
the chunk positions where it appears, in increasing order. Let π be the resulting
permutation, which is stored with the representation of Munro et al. [24]. This
requires σ lg σ(1 + 1/f (n, σ)) bits and computes any π(i) in constant time and
any π−1(j) in time O(f (n, σ)), for any f (n, σ) ≥ 1. With this representation
they solve, within the chunk, selecta(i) = π(select0(X, a − 1) − (a − 1) + i) in
constant time and access(i) = 1 + rank0(select1(X, π−1(i))) in time O(f (n, σ)).
For ranka(i), they basically carry out a predecessor search within the interval
of π that corresponds to a: [select0(X, a − 1) − (a − 1) + 1, select0(X, a) − a].
They have a sampled predecessor structure with one value out of lg σ, which
takes just O(σ) bits. With this structure they reduce the interval to size lg σ,
and a binary search completes the process, within overall time O(lg lg σ).

New Lower and Upper Bounds for Representing Sequences

189

To achieve optimal time, we sample one value out of lg σ

lg w within chunks. We
build the predecessor data structures of Patrascu and Thorup [27], mentioned in
Section 3, over the sampled values. Added over all the chunks, these structures
take O((n/ lg σ
lg w ) lg σ) = O(n lg w) = o(n lg σ) bits (as we assumed lg σ = ω(lg w)).
The predecessor structures take time O(lg lg σ
lg w ) (see Theorem 10 in the extended
version [5, App. A]). The search is then completed with a binary search between
two consecutive sampled values, which also takes time O(lg lg σ
Constant-time Access. This time we use the structure of Munro et al. on π−1, so
we compute any π−1(j) in constant time and any π(i) in time O(f (n, σ)). Thus
we get access in constant time and select in time O(f (n, σ)).

lg w ).

1

Now the binary search of rank needs to compute values of π, which is not
anymore constant time. This is why Golynski et al. [16] obtained time slightly
over lg lg σ time for rank in this case. We instead set the sampling step to
( lg σ
f (n,σ) . The predecessor structures on the sampled values still answer in time
lg w )
lg w ), but they take O((n/( lg σ
O(lg lg σ
f (n,σ) ) lg σ) bits of space. This is o(n lg σ)
lg w )
provided f (n, σ) = o(lg lg σ
lg w ). On the other hand, the time for the binary search
is O(

1

f (n,σ)
f (n,σ) lg lg σ

lg w ), as desired.

The following theorem, which improves upon Golynski et al.’s [16] (not only

as a consequence of a higher low-order space term), summarizes our result.
Theorem 6. A string S[1, n] over alphabet [1, σ], σ ≤ n, can be represented using 
n lg σ + o(n lg σ) bits, so that, given any function ω(1) = f (n, σ) = o(lg lg σ
lg w ),
(i) operations access and select can be solved in time O(1) and O(f (n, σ)), or
vice versa, and (ii) rank can be solved in time O(lg lg σ

lg w ).

For larger alphabets we must add a dictionary mapping [1, σ] to the (at most)
n symbols actually occurring in S, in the standard way.

4.3 Zero-Order Compression
Barbay et al. [1] showed how, given a representation R of a sequence in n lg σ +
o(n lg σ) bits, its times for access, select and rank can be maintained while
reducing its space to nH0(S) + o(nH0(S)) + o(n) bits. This can be done even
if R works only for σ ≥ (lg n)c for some constant c. The technique separates
the symbols according to their frequencies into O(lg n) classes. The sequence of
classes is represented using a multiary wavelet tree [10], and the subsequences
of the symbols of each class are represented with an instance of R.
We can use this technique to compress the space of our succinct representations.
 By using Theorem 5 as our structure R, we obtain the following result,
which improves upon Ferragina et al. [10].

Theorem 7. A string S[1, n] over alphabet [1, σ] can be represented using
nH0(S) +o(nH0(S)) + o(n) bits so that operations access, select and rank can

190

D. Belazzougui and G. Navarro

be solved in time O( lg σ
lg w ). If σ = wO(1), the space is nH0(S) + o(n) and the
operation times are O(1).
To handle larger alphabets, we use Theorem 6 as our structure R. The only technical 
problem is that the subsequences range over a smaller alphabet [1, σ(cid:4)
], and
Theorem 6 holds only for lg σ(cid:4)
= ω(lg w). In subsequences with smaller alphabets 
we can use Theorem 5, which give access, select and rank times O( lg σ(cid:2)
lg w ).
lg w ≤ f (n, σ), else use Theorem 6.
More precisely, we use that structure for lg σ(cid:2)
This gives the following result, which improves upon Barbay et al.’s [1].
Theorem 8. A string S[1, n] over alphabet [1, σ], σ ≤ n, can be represented using 
nH0(S)+o(nH0(S))+o(n) bits, so that, given any function ω(1) = f (n, σ) =
o(lg lg σ
lg w ), (i) operations access and select can be solved in time O(f (n, σ)), and
(ii) rank can be solved in time O(lg lg σ

lg w ).

4.4 High-Order Compression

Ferragina and Venturini [11] showed how a string S[1, n] over alphabet [1, σ] can
be stored within nHk(S) + o(n lg σ) bits, for any k = o(lgσ n), so that it oﬀers
constant-time access to any O(lgσ n) consecutive symbols.

We provide select and rank functionality on top of this representation by
adding extra data structures that take o(n lg σ) bits, whenever lg σ = ω(lg w).
The technique is similar to those used by Barbay et al. [2] and Grossi et al. [19].
We divide the text logically into chunks, as with Golynski et al. [16], and for
each chunk we store a monotone minimum perfect hash function (mmphf) fa
for each a ∈ [1, σ]. Each fa stores the positions where symbol a occurs in the
chunk, so that given the position i of an occurrence of a, fa(i) gives ranka(i)
within the chunk. All the mmphfs can be stored within O(σ lg lg σ) = o(σ lg σ)
bits and can be queried in constant time [4]. With array X we can know, given
a, how many symbols smaller than a are there in the chunk.
Now we have suﬃcient ingredients to compute π−1 in constant time: Let a
be the ith symbol in the chunk (obtained in constant time using Ferragina and
Venturini’s structure), then π−1(i) = fa(i) + select0(X, a− 1)− (a− 1). Now we
can compute select and rank just as done in the “constant-time access” branch
of Section 4.2. The resulting theorem improves upon Barbay et al.’s results [2]
(they did not use mmphfs).
Theorem 9. A string S[1, n] over alphabet [1, σ], for σ ≤ n and lg σ = ω(lg w),
can be represented using nHk(S) + o(n lg σ) bits for any k = o(lgσ n) so that,
given any function ω(1) = f (n, σ) = o(lg lg σ
lg w ), (i) operation access can be solved
in constant time, (ii) operation select can be solved in time O(f (n, σ)), and (ii)
operation rank can be solved in time O(lg lg σ

lg w ).

To compare with the corresponding result by Grossi et al. [19] (who do use
n lg σ
mmphfs) we can ﬁx the redundancy to O(
lg lg σ ), where they obtain O(lg lg σ)
time for select and rank, whereas we obtain the same time for select and our
improved time for rank, as long as lg σ = Ω(lg w lg lg w lg lg lg w).

New Lower and Upper Bounds for Representing Sequences

191

5 Conclusions

This paper considerably reduces the gap between upper and lower bounds for
sequence representations providing access, select and rank queries. Most notably,
 we give matching lower and upper bounds Θ(lg lg σ
lg w ) for operation rank,
which was the least developed one in terms of lower bounds. The issue of the
space related to this complexity is basically solved as well: we have shown it can
be achieved even within compressed space, and it cannot be surpassed within
space O(n · wO(1)). On the other hand, operations access and select can be
solved, within the same compressed space, in almost constant time (i.e., as close
to O(1) as desired but not both reaching it, unless we double the space).

There are still some intriguing issues that remain unclear:

1. Golynski’s lower bounds [15] leave open the door to achieving constant time
for access and select simultaneously, with O(n(lg σ)2/ lg n) bits of redundancy.
 However, this has not yet been achieved for the interesting case
ω(lg w) = lg σ = o(lg n). We conjecture that this is not possible and a
stronger lower bound holds.

2. While we can achieve constant-time select and almost-constant time for
access (or vice versa) within zero-order entropy space, we can achieve only
the second combination within high-order entropy space. If simultaneous
constant-time access and select is not possible, then no solution for the
ﬁrst combination can build over a compressed representation of S giving
constant-time access, as it has been the norm [2,1,19].

3. We have achieved high-order compression with almost-constant access and
select times, and optimal rank time, but on alphabets of size superpolynomial 
in w. By using one Golynski’s binary rank/select index [14] per symbol
over Ferragina and Venturini’s representation [11], we get high-order compression 
and constant time for all the operations for any σ = o(lg n). This
leaves open the interesting band of alphabet sizes Ω(lg n) = σ = wO(1).

References
1. Barbay, J., Gagie, T., Navarro, G., Nekrich, Y.: Alphabet Partitioning for Compressed 
Rank/Select and Applications. In: Cheong, O., Chwa, K.-Y., Park, K.
(eds.) ISAAC 2010, Part II. LNCS, vol. 6507, pp. 315–326. Springer, Heidelberg
(2010)

2. Barbay, J., He, M., Munro, J.I., Rao, S.S.: Succinct indexes for strings, binary

relations and multi-labeled trees. In: Proc. 18th SODA, pp. 680–689 (2007)

3. Barbay, J., Navarro, G.: Compressed representations of permutations, and applications.
 In: Proc. 26th STACS, pp. 111–122 (2009)

4. Belazzougui, D., Boldi, P., Pagh, R., Vigna, S.: Monotone minimal perfect hashing:
searching a sorted table with o(1) accesses. In: Proc. 20th SODA, pp. 785–794
(2009)

5. Belazzougui, D., Navarro, G.: New lower and upper bounds for representing sequences.
 CoRR, arXiv:1111.26211v1 (2011) http://arxiv.org/abs/1111.2621v1
6. Clark, D.: Compact Pat Trees. PhD thesis, University of Waterloo, Canada (1996)
7. Claude, F., Navarro, G.: Extended Compact Web Graph Representations. In:
Elomaa, T., Mannila, H., Orponen, P. (eds.) Ukkonen Festschrift 2010. LNCS,
vol. 6060, pp. 77–91. Springer, Heidelberg (2010)

192

D. Belazzougui and G. Navarro

8. Ferragina, P., Luccio, F., Manzini, G., Muthukrishnan, S.: Compressing and indexing 
labeled trees, with applications. Journal of the ACM 57(1) (2009)

9. Ferragina, P., Manzini, G.: Indexing compressed texts. Journal of the ACM 52(4),

552–581 (2005)

10. Ferragina, P., Manzini, G., M¨akinen, V., Navarro, G.: Compressed representations
of sequences and full-text indexes. ACM Transactions on Algorithms 3(2), article
20 (2007)

11. Ferragina, P., Venturini, R.: A simple storage scheme for strings achieving entropy

bounds. Theoretical Computer Science 372(1), 115–121 (2007)

12. Gagie, T.: Large alphabets and incompressibility. Information Processing Letters 
99(6), 246–251 (2006)

13. Gagie, T., Navarro, G., Puglisi, S.J.: Colored Range Queries and Document Retrieval.
 In: Chavez, E., Lonardi, S. (eds.) SPIRE 2010. LNCS, vol. 6393, pp. 67–81.
Springer, Heidelberg (2010)

14. Golynski, A.: Optimal lower bounds for rank and select indexes. Theoretical Computer 
Science 387(3), 348–359 (2007)

15. Golynski, A.: Cell probe lower bounds for succinct data structures. In: Proc. 20th

SODA, pp. 625–634 (2009)

16. Golynski, A., Munro, I., Rao, S.: Rank/select operations on large alphabets: a tool

for text indexing. In: Proc. 17th SODA, pp. 368–373 (2006)

17. Golynski, A., Raman, R., Rao, S.S.: On the Redundancy of Succinct Data Structures.
 In: Gudmundsson, J. (ed.) SWAT 2008. LNCS, vol. 5124, pp. 148–159.
Springer, Heidelberg (2008)

18. Grossi, R., Gupta, A., Vitter, J.: High-order entropy-compressed text indexes. In:

Proc. 14th SODA, pp. 841–850 (2003)

19. Grossi, R., Orlandi, A., Raman, R.: Optimal Trade-Oﬀs for Succinct String Indexes.
In: Abramsky, S., Gavoille, C., Kirchner, C., Meyer auf der Heide, F., Spirakis, P.G.
(eds.) ICALP 2010. LNCS, vol. 6198, pp. 678–689. Springer, Heidelberg (2010)

20. Gupta, A., Hon, W.-K., Shah, R., Vitter, J.: Dynamic rank/select dictionaries
with applications to XML indexing. Technical Report CSD TR #06-014, Purdue
University (July 2006)

21. Jacobson, G.: Space-eﬃcient static trees and graphs. In: Proc. 30th FOCS, pp.

549–554 (1989)

22. Manzini, G.: An analysis of the Burrows-Wheeler transform. Journal of the

ACM 48(3), 407–430 (2001)

23. Munro, I.: Tables. In: Chandru, V., Vinay, V. (eds.) FSTTCS 1996. LNCS,

vol. 1180, pp. 37–42. Springer, Heidelberg (1996)

24. Munro, J.I., Raman, R., Raman, V., Rao, S.S.: Succinct Representations of Permutations.
 In: Baeten, J.C.M., Lenstra, J.K., Parrow, J., Woeginger, G.J. (eds.)
ICALP 2003. LNCS, vol. 2719, pp. 345–356. Springer, Heidelberg (2003)

25. Patrascu, M.: Succincter. In: Proc. 49th FOCS, pp. 305–313 (2008)
26. Patrascu, M., Thorup, M.: Time-space trade-oﬀs for predecessor search. In: Proc.

38th STOC, pp. 232–240 (2006)

27. Patrascu, M., Thorup, M.: Time-space trade-oﬀs for predecessor search. CoRR,

cs/0603043v1 (2008), http://arxiv.org/pdf/cs/0603043v1

28. Patrascu, M., Viola, E.: Cell-probe lower bounds for succinct partial sums. In:

Proc. 21st SODA, pp. 117–122 (2010)

29. V¨alim¨aki, N., M¨akinen, V.: Space-Eﬃcient Algorithms for Document Retrieval.
In: Ma, B., Zhang, K. (eds.) CPM 2007. LNCS, vol. 4580, pp. 205–215. Springer,
Heidelberg (2007)

