Fast and Compact Preﬁx Codes(cid:2)

Travis Gagie1,2, Gonzalo Navarro2, and Yakov Nekrich3

1 Research Group in Genome Informatics, Bielefeld University

2 Department of Computer Science, University of Chile

travis.gagie@gmail.com

3 Department of Computer Science, University of Bonn

gnavarro@dcc.uchile.cl

yasha@cs.uni-bonn.de

Abstract. It is well-known that, given a probability distribution over
n characters, in the worst case it takes Θ(n log n) bits to store a preﬁx
code with minimum expected codeword length. However, in this paper
we ﬁrst show that, for any  with 0 <  < 1/2 and 1/ = O(polylog(n)), it
takes O(n log log(1/)) bits to store a preﬁx code with expected codeword
(cid:2)
length within an additive  of the minimum. We then show that, for any
constant c > 1, it takes O
bits to store a preﬁx code with
expected codeword length at most c times the minimum. In both cases,
our data structures allow us to encode and decode any character in O(1)
time.

n1/c log n

(cid:3)

1 Introduction

Compression is most important when space is in short supply, so popular compressors 
are usually heavily engineered to reduce their space usage. Theory has
lagged behind practice in this area, however, and there remain basic open questions 
about the space needed for even the simplest kinds of compression. For
example, while compression with preﬁx codes is familiar to any student of information 
theory, very little has been proven about compression of preﬁx codes.
Suppose we are given a probability distribution P over an alphabet of n characters.
 Until fairly recently, the only general bounds known seem to have been,
ﬁrst, that it takes Θ(n log n) bits in the worst case to store a preﬁx code with
minimum expected codeword length and, second, that O(n) bit suﬃce to store
a preﬁx code with expected codeword length within 1 of the minimum.

In 1998 Adler and Maggs [1] showed it generally takes more than (9/40)n1/(20c)
log n bits to store a preﬁx code with expected codeword length at most cH(P ),
where H(P ) is P ’s entropy and a lower bound on the expected codeword length.
(In this paper we consider only binary codes, and by log we always mean log2.)
In 2006 Gagie [6,7] (see also [8]) showed that, for any constant c ≥ 1, it takes
O(cid:4)
bits to store a preﬁx code with expected codeword length at most

n1/c log n

(cid:5)

(cid:2) Funded in part by Millennium Institute for Cell Dynamics and Biotechnology

(ICDB), Grant ICM P05-001-F, Mideplan, Chile.

J. van Leeuwen et al. (Eds.): SOFSEM 2010, LNCS 5901, pp. 419–427, 2010.
c(cid:2) Springer-Verlag Berlin Heidelberg 2010

420

T. Gagie, G. Navarro, and Y. Nekrich

(cid:5)

n1/c−

length at most cH(P )+o(log n) in O(cid:4)

cH(P ) + 2. He also showed his upper bound is nearly optimal because, for any
positive constant , we cannot always store a preﬁx code with expected codeword
bits. Gagie proved his upper bound
by describing a data structure that stores a preﬁx code with the prescribed
expected codeword length in the prescribed space and allows us to encode and
decode any character in time at most proportional to its codeword’s length. This
data structure has three obvious defects: when c = 1, it is as big as a Huﬀman
tree, whereas its redundancy guarantee can be obtained with just O(n) bits [10];
when H(P ) is small, a possible additive increase of 2 in the expected codeword
length may be prohibitive; and it is slower than the state of the art.

In this paper we answer several open questions related to eﬃcient representation 
of codes. First, in Section 3 we show that, for any  with 0 <  < 1/2
and 1/ = O(polylog(n)), it takes O(n log log(1/)) bits to store a preﬁx code
with expected codeword length within an additive  of the minimum. Thus, if we
can tolerate an additive increase of, say, 0.01 in the expected codeword length,
then we can store a preﬁx code using only O(1) bits per character. Second, in
Section 4 we show that, for any constant c > 1, it takes O(cid:4)
bits to
store a preﬁx code with expected codeword length at most c times the minimum,
√
with no extra additive increase. Thus, if we can tolerate a multiplicative increase
of, say, 2.01, then we can store a preﬁx code in O(
n) bits. In both cases, our
data structures allow us to encode and decode any character in O(1) time on
a unit-cost word RAM.

n1/c log n

(cid:5)

2 Related Work
A simple pointer-based implementation of a Huﬀman tree takes O(n log n) bits
and it is not diﬃcult to show this is an optimal upper bound for storing a preﬁx
code with minimum expected codeword length. For example, suppose we are
given a permutation π over n characters. Let P be the probability distribution
that assigns probability 1/2i to the π(i)th character, for 1 ≤ i < n, and probability 
1/2n−1 to the π(n)th character. Since P is dyadic, every preﬁx code with
minimum expected codeword length assigns a codeword of length i to the π(i)th
character, for 1 ≤ i < n, and a codeword of length n − 1 to the π(n)th character.
 Therefore, given any preﬁx code with minimum expected codeword length
and a bit indicating whether π(n − 1) < π(n), we can ﬁnd π. Since there are
n! choices for π, in the worst case it takes Ω(log n!) = Ω(n log n) bits to store
a preﬁx code with minimum expected codeword length.

Considering the argument above, it is natural to ask whether the same lower
bound holds for probability distributions that are not so skewed, and the answer
is ‘no’. A preﬁx code is canonical [19] if the ﬁrst codeword is a string of 0s and
any other codeword can be obtained from its predecessor by adding 1, viewing
its predecessor as a binary number, and appending some number of 0s. (See,
e.g., [16,14] for more recent work on canonical codes.) Given any preﬁx code,
without changing the length of the codeword assigned to any character, we can
put the code into canonical form by just exchanging left and right siblings in

Fast and Compact Preﬁx Codes

421

the code-tree. Moreover, we can reassign the codewords such that, if a character
is lexicographically the jth with a codeword of length (cid:6), then it is assigned the
jth consecutive codeword of length (cid:6). It is clear that it is suﬃcient to store the
codeword length of each character to be able to reconstruct such a code, and
thus the code can be represented in O(n log L) bits, where L is the length of the
longest codeword.

The above gives us a ﬁner upper bound. For example, Katona and Nemetz [13]
showed that, if a character has probability p, then any Huﬀman code assigns it
a codeword of length at most about log(1/p)/ log φ, where φ ≈ 1.618 is the golden
ratio, and thus L is at most about 1.44 log(1/pmin), where pmin is the smallest
probability in P . Alternatively, one can enforce a value for L and pay a price
in terms of expected codeword length. Milidi´u and Laber [15] showed how, for
any L > (cid:6)log n(cid:7), we can build a preﬁx code with maximum codeword length at
most L and expected codeword length within 1/φL−(cid:3)log(n+(cid:3)log n(cid:4)−L)(cid:4)−1 of the
minimum. Their algorithm works by building a Huﬀman tree T1; removing all
the subtrees rooted at depth greater than L; building a complete binary tree T2
of height h whose leaves are those removed from T1; ﬁnding the node v at depth
L−h−1 in T1 whose subtree T3’s leaves are labelled by characters with minimum
total probability (which they showed is at most 1/φL−(cid:3)log(n+(cid:3)log n(cid:4)−L)(cid:4)−1); and
replacing v by a new node whose subtrees are T2 and T3.

A simple upper bound for storing a preﬁx code with expected codeword length
within a constant of the minimum, follows from Gilbert and Moore’s proof [10]
that we can build an alphabetic preﬁx code with expected codeword length less
than H(P ) + 2 and, thus, within 2 of the minimum. Moreover, in an optimal
alphabetic preﬁx code, the expected codeword length is within 1 of the minimum 
[18,20] which, in turn, is within 1 of the entropy H(P ). In an alphabetic
preﬁx code, the lexicographic order of the codewords is the same as that of the
characters, so we need store only the code-tree and not the assignment of codewords 
to characters. If we store the code-tree in a succinct data structure due
to Munro and Raman [17], then it takes O(n) bits and encoding and decoding
any character takes time at most proportional to its codeword length. This can
be improved to O(1) by using table lookup, but doing so may worsen the space
bound unless we also restrict the maximum codeword length, which may in turn
The code-tree of a canonical code can be stored in just O(cid:4)
increase the expected codeword length.

bits: By its deﬁnition,
 we can reconstruct the whole canonical tree given only the ﬁrst codeword
of each length. Unfortunately, Gagie’s lower bound [7] suggests we generally cannot 
combine results concerning canonical codes with those concerning alphabetic
preﬁx codes.

(cid:5)

L2

selves, viewed as binary numbers. Suppose we build an O(cid:4)

Constant-time encoding and decoding using canonical code-trees is simple.
Notice that if two codewords have the same length, then the diﬀerence between
(cid:5)
their ranks in the code is the same as the diﬀerence between the codewords them-
-bit array A and
a dictionary D supporting predecessor queries, each storing the ﬁrst codeword
of each length. Given the length of a character’s codeword and its rank among

L2

422

T. Gagie, G. Navarro, and Y. Nekrich

codewords of the same length (henceforth called its oﬀset), we can ﬁnd the actual 
codeword by retrieving the ﬁrst codeword of that length from A and then,
viewing that ﬁrst codeword as a binary number, adding the oﬀset minus 1. Given
a binary string starting with a codeword, we can ﬁnd that codeword’s length and
oﬀset by retrieving the string’s predecessor in D, which is the ﬁrst codeword of
the same length; truncating the string to the same length in order to obtain the
actual codeword; and subtracting the ﬁrst codeword from the actual codeword,
viewing both as binary numbers, to obtain the oﬀset minus 1. (If D supports
numeric predecessor queries instead of lexicographic predecessor queries, then
we store the ﬁrst codewords with enough 0s appended to each that they are
all the same length, and store their original lengths as auxiliary information.)
Assuming it takes O(1) time to compute the length and oﬀset of any character’s
codeword given that character’s index in the alphabet, encoding any character
takes O(1) time. Assuming it takes O(1) time to compute any character’s index
in the alphabet given its codeword’s length and oﬀset, decoding takes within
a constant factor of the time needed to perform a predecessor query on D. For
simplicity, in this paper we consider the number used to represent a character in
the machine’s memory to be that character’s index in the alphabet, so ﬁnding
the index is the same as ﬁnding the character itself.
In a recent paper on adaptive preﬁx coding, Gagie and Nekrich [9] (see also
[12]) pointed out that if L = O(w), where w is the length of a machine word,
(cid:5)
then we can implement D as an O(cid:4)
-bit dictionary data structure due to
Fredman and Willard [5] such that predecessor queries take O(1) time. (We
note that Beame and Fich’s well-known lower bound [2] on predecessor queries
does not apply when the size of the dictionary is proportional to the length of
a word.) This seems a reasonable assumption since, for any string of length m
with log m = O(w), if P is the probability distribution that assigns to each
character probability proportional to its frequency in the string, then the smallest 
positive probability in P is at least 1/m; therefore, the maximum codeword
length in either a Huﬀman code or a Shannon code for P is O(w). Gagie and
Nekrich used O(n log n)-bit arrays to compute the length and oﬀset of any char-
acter’s codeword given that character’s index in the alphabet, and vice versa,
and thus achieved O(1) time for both encoding and decoding.

w2

(cid:3)

log σ

log log n

A technique we will use to obtain our ﬁrst result, presented in section 3, is
the wavelet tree of Grossi et al. [11], and more precisely the multiary variant
due to Ferragina et al. [3]. The latter represents a sequence S[1, n] over an
(cid:2)
alphabet Σ of size σ such that the following operations can be carried out in
O
time on the RAM model with a computer word of length Ω(log n):
(1) Given i, retrieve S[i]; (2) given i and a ∈ Σ, compute ranka(S, i), the number
of occurrences of a in S[1, i]; (3) given j and a ∈ Σ, compute selecta(S, j),
(cid:3)
(cid:2)
the position in S of
j-th occurrence of a. The wavelet tree requires
the
nH0(S) +O
bits of space, where H0(S) ≤ log σ is the empirical zeroorder 
entropy of S, deﬁned as H0(S) = H({occ(a, S) /n}a∈σ), where occ(a, S)
is the number of occurrences of a in S. Thus nH0(S) is a lower bound to the

n log log n

logσ n

Fast and Compact Preﬁx Codes

423

n

occ(a,S).

(cid:6)

output size of any zero-order compressor applied to S. It will be useful to write
H0(S) =

occ(a,S)

log

a∈σ

n

Our second result is based on constructing a length-restricted canonical code
with maximum codeword length L. We divide all symbols into “probable” symbols 
that are assigned codewords of length at most L/c + 2 and “improbable”
symbols that are assigned codewords of length greater than L/c + 2. It will be
shown in section 4 that all “probable” symbols can be encoded and decoded in
O(1) time using O(n1/c log n) bits. We replace all codewords of length at least
L/c + 3 with codewords of length L, so that the “improbable” symbols can be
encoded and decoded in constant time but we do not have to store the new
codewords explicitly.

3 Additive Increase in Expected Codeword Length

In this section we exchange a small additive penalty over the optimal preﬁx code
for a space-eﬃcient representation of the encoding, which in addition enables
encode/decode operations in constant time.
It follows from Milidi´u and Laber’s bound [15] that, for any  with 0 <  < 1/2,
there is always a preﬁx code with maximum codeword length L = (cid:6)log n(cid:7) +
(cid:6)logφ(1/)(cid:7) + 1 and expected codeword length within an additive

1

φL−(cid:3)log(n+(cid:3)log n(cid:4)−L)(cid:4)−1

≤

1

φlogφ(1/) = 

≤

1

φL−(cid:3)log n(cid:4)−1
(cid:5)
L2 + n log L

to store such a code in O(cid:4)
of the minimum. The techniques described in the previous section give a way
bits, yet it is not immediately obvious
constant-time encoding and decoding using O(cid:4)
how to do constant-time encoding and decoding. Alternatively, we can achieve
bits for the codetree,
 if L = O(w).

w2 + n log n

(cid:5)

To achieve constant encoding and decoding times without ruining the space,
we use multiary wavelet trees. We use a canonical code, and sort the characters
(i.e., leaves) alphabetically within each depth, as described in the previous section.
Let S[1, n] be the sequence of depths in the canonical code-tree, so that S[a] (1 ≤
a ≤ n) is the depth of the character a. Now, the depth and oﬀset of any a ∈ Σ
is easily computed from the wavelet tree of S: the depth is just S[a], while the
(cid:5)
character is selectd(S, o). The O(cid:4)
oﬀset is rankS[a](S, a). Inversely, given a depth d and an oﬀset o, the corresponding
-bit data structure of Gagie and Nekrich [9]
(cid:3)
(cid:2)
converts in constant time pairs (depth,oﬀset) into codes and vice versa (if L =
O(w)), whereas the multiary wavelet tree on S requires n log L + O
(cid:2)
(cid:3)
bits of space and completes encoding/decoding in time O
restriction 1/ = O(polylog(n)), the space is O(cid:4)
(cid:5)
w2
is O(1). This is the key to the result of this section.
Theorem 1. For any  with 0 <  < 1/2 and 1/ = O(polylog(n)), and under
the RAM model with computer word size w, so that the text to encode is of

. Under the
+ n log L + o(n) and the time

n log log n

log log n

logL n

log L

w2

(cid:5)

w2

T. Gagie, G. Navarro, and Y. Nekrich

(cid:5)
w2 + n log log(1/)

424
length 2O(w), we can store a preﬁx code with expected codeword length within
an additive term  of the minimum, using O(cid:4)
bits, such that
encoding and decoding any character takes O(1) time.
Proof. The data structure we have described achieves the given time bounds if
we assume the text to encode is of length m = 2O(w), as usual under the RAM
model of computation, and thus L = O(w) enables constant-time encoding and
As for the space, we have shown it is O(cid:4)
decoding [9].
+ n log L + o(n). To achieve the
claim of the theorem we show that H0(S) is at most log(L−(cid:6)log n(cid:7) + 1) +O(1),
so we can store S in O(n log(L − log n + 1) + n) = O(n log log(1/)) bits.
To see this, consider S as two interleaved subsequences, S1 and S2, of length n1
and n2, with S1 containing those lengths less than or equal to (cid:6)log n(cid:7) and S2
containing those greater. Thus nH0(S) ≤ n1H0(S1) + n2H0(S2) + n.

(cid:6)

1≤(cid:5)≤(cid:3)log n(cid:4) occ((cid:6), S1) log

Since there are at most 2(cid:5) codewords of length (cid:6), assume we complete S1
with spurious symbols so that it has exactly 2(cid:5) occurrences of symbol (cid:6). This
completion cannot decrease n1H0(S1) =
occ((cid:5),S1), as
increasing some occ((cid:6), S1) to occ((cid:6), S1) + 1 produces a diﬀerence of f(n1) −
f(occ((cid:6), S1)) ≥ 0, where f(x) = (x + 1) log(x + 1) − x log x is increasing. Hence
we can assume S1 contains exactly 2(cid:5) occurrences of symbol 1 ≤ (cid:6) ≤ (cid:6)log n(cid:7);
straightforward calculation then shows that n1H0(S1) = O(n1).
On the other hand, S2 contains at most L−(cid:6)log n(cid:7) distinct values, so H0(S2) ≤
log(L−(cid:6)log n(cid:7)), unless L = (cid:6)log n(cid:7), in which case S2 is empty and n2H0(S2) = 0.
Thus n2H0(S2) ≤ n2 log((cid:6)logφ(1/)(cid:7) + 1) = O(n2 log log(1/)).
Combining both bounds, we get H0(S) = O(1 + log log(1/)) and the theorem
(cid:9)(cid:10)

n1

holds.

In other words, under mild assumptions, we can store a code using
O(n log log(1/)) bits at the price of increasing the average codeword length
by , and in addition have constant-time encoding and decoding. For constant ,
this means that the code uses just O(n) bits at the price of an arbitrarily small
constant additive penalty over the shortest possible preﬁx code.

4 Multiplicative Increase in Expected Codeword Length

In this section we focus on a multiplicative rather than an additive penalty over
the optimal preﬁx code, in order to achieve a sublinear-sized representation of
the encoding, which still enables constant-time encoding and decoding.

Our main idea is to divide the alphabet into probable and improbable characters 
and to store information about only the probable ones. Given a constant
c > 1, we use Milidi´u and Laber’s algorithm [15] to build a preﬁx code with
maximum codeword length L = (cid:6)log n(cid:7) + (cid:6)1/(c − 1)(cid:7) + 1. We call a character’s
there are at most 2L/c+3 − 1 = O(cid:4)
codeword short if it has length at most L/c + 2, and long otherwise. Notice
characters with short codewords. Also,
although applying Milidi´u and Laber’s algorithm may cause some exceptions,
characters with short codewords are usually more probable than characters with

n1/c

(cid:5)

(cid:5)

w2

(cid:5)

(cid:5)

n1/c log n

We transform this length-restricted preﬁx code into a canonical code as described 
in Section 2; speciﬁcally, we sort the characters lexicographically within
each depth. We use a dictionary data structure F due to Fredman, Koml´os and
data structure takes O(cid:4)
Szemer´edi [4] to store the indices of the characters with short codewords. This
bits and supports membership queries in O(1)
time, with successful queries returning the target character’s codeword. We also
build (cid:11)L/c(cid:12) + 2 arrays that together store the indices of all the characters with
short codewords; for 1 ≤ (cid:6) ≤ (cid:11)L/c(cid:12) + 2, the (cid:6)th array stores the indices of
Again, we store the ﬁrst codeword of each length in O(cid:4)
the characters with codewords of length (cid:6), in lexicographic order by codeword.
bits overall, following
Gagie and Nekrich [9], such that it takes O(1) time to compute any codeword
given its length and oﬀset, and vice versa. With these data structures, we can
encode and decode any character with a short codeword in O(1) time. To encode,
 we perform a membership query on the dictionary to check whether the
character has a short codeword; if it does, we receive the codeword itself as satellite 
information returned by the query. To decode, we ﬁrst ﬁnd the codeword’s
length (cid:6) and oﬀset j in O(1) time as described in Section 2. Since the codeword
is short, (cid:6) ≤ (cid:11)L/c(cid:12) + 2 and the character’s index is stored in the jth cell of the
(cid:6)th array. These data structures use a total of O(cid:4)
bits of space.
We replace each long codeword with new codewords: instead of a long codeword 
α of length (cid:6), we insert 2L+1−(cid:5) new codewords α · s, where · denotes
concatenation and s is an arbitrary binary string of length L + 1 − (cid:6). Figure 1
shows an example. Since c > 1, we have n1/c < n/2 for suﬃciently large n, so
we can assume without loss of generality that there are fewer than n/2 short
codewords; hence, the number of long codewords is at least n/2. Since every
long codeword is replaced by at least two new codewords, the total number of
new codewords is at least n. Since new codewords are obtained by extending
all codewords of length (cid:6) > L/c + 1 in a canonical code, all new codewords are
binary representations of consecutive integers. Therefore the i-th new codeword
equals to αf + i − 1, where αf is the ﬁrst new codeword. If a is an infrequent
character, we encode it with the a-th new codeword, αf + a − 1. To encode
a character a, we check whether a belongs to the dictionary F . If a ∈ F , then
we output the codeword for a. Otherwise we encode a as αf + a − 1. To decode
a codeword α, we read its preﬁx bitstring sα of length L + 1 and compare sα
with αf . If sα ≥ αf , then α = sα is the codeword for sα − αf + 1. Otherwise, the
codeword length of the next codeword α is at most L/c+1 and α can be decoded
(cid:5)
codewords we just described, so the total space used is still O(cid:4)
as described in the previous paragraph. Notice we do not need to store the new
w2 + n1/c log n

w2 + n1/c log n

long ones. We will hereafter call infrequent characters those encoded with long
codewords in the code of Milidi´u and Laber.

Fast and Compact Preﬁx Codes

425

bits.
Theorem 2. For any constant c > 1, under the RAM model with computer word
size w, so that the text to encode is of length 2O(w), we can store a preﬁx code
(cid:5)
with expected codeword length within c times the minimum in O(cid:4)
w2 + n1/c log n
bits, such that encoding and decoding any character takes O(1) time.

426

T. Gagie, G. Navarro, and Y. Nekrich

j

00
010
011
1000
1001
1010
1011

j
c
i
a
h
m
p

b
d
e
f
g
k
l
n
o

c

i

a

h

m

p

1100000 + char − 1

b

d e f g

k l

n o

b

d

f

g

l

n

o

e

k

Fig. 1. An example with n = 16 and c = 3. The tree consisting of the nodes drawn as
large circles and squares (in black) is the result of applying the algorithm of Milidi´u
and Laber on the original preﬁx code. Now, we set L = 6 according to our formula,
and declare short the codeword lengths up to (cid:2)L/c(cid:3) + 2 = 4. Short codewords — i.e.,
those above the dashed line — are stored unaltered in a dictionary (in blue). Longer
codewords — i.e., those below the dashed line — are changed: All are extended up to
length L + 1 = 7 and reassigned a code according to their values in the contiguous slots
of length 7 (in red).

Proof. The structure described throughout the section achieves the promised
time and space bounds. We analyze now the expected codeword length.

By analysis of the algorithm by Milidi´u and Laber [15] we can see that the
codeword length of a character in their length-restricted code exceeds the codeword 
length of the same character in an optimal code by at most 1, and only when
the codeword length in the optimal code is at least L−(cid:6)log n(cid:7)− 1 = (cid:6)1/(c− 1)(cid:7).
Hence, the codeword length of a character encoded with a short codeword exceeds 
the codeword length of the same character in an optimal code by a factor
(cid:3)1/(c−1)(cid:4) ≤ c. Every infrequent character is encoded with a code-
(cid:3)1/(c−1)(cid:4)+1
of at most
word of length L + 1. Since the codeword length of an infrequent character in
the length-restricted code is more than L/c + 2, its length in an optimal code
is more than L/c + 1. Hence, the codeword length of a long character in our
code is at most L+1
< c times greater than the codeword length of the same
L/c+1
character in an optimal code. Hence, the average codeword length for our code
(cid:9)(cid:10)
is less than c times the optimal one.

Fast and Compact Preﬁx Codes

427

pected length within c times the optimum, in O(cid:4)

Again, under mild assumptions, this means that we can store a code with exbits 
and allowing

constant-time encoding and decoding.

(cid:5)

n1/c log n

References

1. Adler, M., Maggs, B.M.: Protocols for Asymmetric Communication Channels.

Journal of Computer and System Sciences 63(4), 573–596 (2001)

2. Beame, P., Fich, F.E.: Optimal Bounds for the Predecessor Problem and Related

Problems. Journal of Computer and System Sciences 65(1), 38–72 (2002)

3. Ferragina, P., Manzini, G., M¨akinen, V., Navarro, G.: Compressed Representations 
of Sequences and Full-Text Indexes. ACM Transactions on Algorithms 3(2),
Article 20 (2007)
4. Fredman, M.L., Koml´os, J., Szemer´edi, E.: Storing a Sparse Table with O(1) Worst

Case Access Time. Journal of the ACM 31(3), 538–544 (1984)

5. Fredman, M.L., Willard, D.E.: Surpassing the Information Theoretic Bound with

Fusion Trees. Journal of Computer and System Sciences 47(3), 424–436 (1993)

6. Gagie, T.: Compressing Probability Distributions. Information Processing Letters 
97(4), 133–137 (2006)

7. Gagie, T.: Large alphabets and incompressibility. Information Processing Letters 
99(6), 246–251 (2006)

8. Gagie, T.: Dynamic asymmetric communication. Information Processing Letters 
108(6), 352–355 (2008)

9. Gagie, T., Nekrich, Y.: Worst-Case Optimal Adaptive Preﬁx Coding. In: Proceedings 
of the Algorithms and Data Structures Symposium (WADS), pp. 315–326
(2009)

10. Gilbert, E.N., Moore, E.F.: Variable-Length Binary Encodings. Bell System Technical 
Journal 38, 933–967 (1959)

11. Grossi, R., Gupta, A., Vitter, J.: High-Order Entropy-Compressed Text Indexes.
In: Proceedings of the 14th Symposium on Discrete Algorithms (SODA), pp. 841–
850 (2003)

12. Karpinski, M., Nekrich, Y.: A Fast Algorithm for Adaptive Preﬁx Coding. Algorithmica 
55(1), 29–41 (2009)

13. Katona, G.O.H., Nemetz, T.O.H.: Huﬀman Codes and Self-Information. IEEE

Transactions on Information Theory 22(3), 337–340 (1976)

14. Klein, S.T.: Skeleton Trees for the Eﬃcient Decoding of Huﬀman Encoded Texts.

Information Retrieval 3(4), 315–328 (2000)

15. Milidi´u, R.L., Laber, E.S.: Bounding the Ineﬃciency of Length-Restricted Preﬁx

Codes. Algorithmica 31(4), 513–529 (2001)

16. Moﬀat, A., Turpin, A.: On the Implementation of Minimum-Redundancy Preﬁx

Codes. IEEE Transactions on Communications 45(10), 1200–1207 (1997)

17. Munro, J.I., Raman, V.: Succinct Representation of Balanced Parentheses and

Static Trees. SIAM Journal on Computing 31(3), 762–776 (2001)

18. Nakatsu, N.: Bounds on the Redundancy of Binary Alphabetical Codes. IEEE

Transactions on Information Theory 37(4), 1225–1229 (1991)

19. Schwarz, E.S., Kallick, B.: Generating a Canonical Preﬁx Encoding. Communications 
of the ACM 7(3), 166–169 (1964)

20. Sheinwald, D.: On Binary Alphabetic Codes. In: Proceedings of the Data Compression 
Conference (DCC), pp. 112–121 (1992)

