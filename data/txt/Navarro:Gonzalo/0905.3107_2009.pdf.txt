9
0
0
2

 

y
a
M
9
1

 

 
 
]
S
D
.
s
c
[
 
 

1
v
7
0
1
3

.

5
0
9
0
:
v
i
X
r
a

Fast and Compact Preﬁx Codes

Travis Gagie1, Gonzalo Navarro2, and Yakov Nekrich3

1 Research Group in Genome Informatics

Bielefeld University

travis.gagie@gmail.com

2 Department of Computer Science

University of Chile

gnavarro@dcc.uchile.cl

3 Department of Computer Science

University of Bonn

yasha@cs.uni-bonn.de

Abstract. It is well-known that, given a probability distribution over n characters, in the
worst case it takes Θ(n log n) bits to store a preﬁx code with minimum expected codeword 
length. However, in this paper we ﬁrst show that, for any 0 <  < 1/2 with 1/ =
O(polylog(n)), it takes O(n log log(1/)) bits to store a preﬁx code with expected codeword
length within  of the minimum. We then show that, for any constant c > 1, it takes

O(cid:0)n1/c log n(cid:1) bits to store a preﬁx code with expected codeword length at most c times

the minimum. In both cases, our data structures allow us to encode and decode any character
in O(1) time.

1

Introduction

Compression is most important when space is in short supply, so popular compressors
are usually heavily engineered to reduce their space usage. Theory has lagged behind
practice in this area, however, and there remain basic open questions about the space
needed for even the simplest kinds of compression. For example, while compression
with preﬁx codes is familiar to any student of information theory, very little has
been proven about compression of preﬁx codes. Suppose we are given a probability
distribution P over an alphabet of n characters. Until fairly recently, the only general
bounds known seem to have been, ﬁrst, that it takes Θ(n log n) bits in the worst case
to store a preﬁx code with minimum expected codeword length and, second, that it
takes O(n) bits to store a preﬁx code with expected codeword length within 1 of the
minimum.

6] (see also [7]) showed that, for any constant c ≥ 1, it takes O(cid:16)
in O(cid:16)

In 1998 Adler and Maggs [1] showed it generally takes more than (9/40)n1/(20c) log n
bits to store a preﬁx code with expected codeword length at most cH(P ), where H(P )
is P ’s entropy and a lower bound on the expected codeword length. (In this paper
we consider only binary codes, and by log we always mean log2.) In 2006 Gagie [5,
bits to
store a preﬁx code with expected codeword length at most cH(P )+2. He also showed
his upper bound is nearly optimal because, for any positive constant , we cannot
always store a preﬁx code with expected codeword length at most cH(P ) + o(log n)
bits. Gagie proved his upper bound by describing a data structure that
stores a preﬁx code with the prescribed expected codeword length in the prescribed
space and allows us to encode and decode any character in time at most proportional
to its codeword’s length. This data structure has three obvious defects: when c = 1,

n1/c−(cid:17)

(cid:17)

n1/c log n

Paper Submitted to PSC

it is as big as a Huﬀman tree, whereas its redundancy guarantee can be obtained
with just O(n) bits [9]; when H(P ) is small, a possible additive increase of 2 in the
expected codeword length may be prohibitive; and it is slower than the state of the
art.
In this paper we answer several open questions related to eﬃcient representation of
codes. First, in Section 3 we show that, for any 0 <  < 1/2 with 1/ = O(polylog(n)),
it takes O(n log log(1/)) bits to store a preﬁx code with expected codeword length
within  of the minimum. Thus, if we can tolerate an additive increase of, say, 0.01
in the expected codeword length, then we can store a preﬁx code using only O(1)
bits per character. Second, in Section 4 we show that, for any constant c > 1, it
bits to store a preﬁx code with expected codeword length at most
√
c times the minimum, with no extra additive increase. Thus, if we can tolerate a
multiplicative increase of, say, 2.01 then we can store a preﬁx code in O(
n) bits. In
both cases, our data structures allow us to encode and decode any character in O(1)
time.

takes O(cid:16)

(cid:17)

n1/c log n

2 Related work
A simple pointer-based implementation of a Huﬀman tree takes O(n log n) bits, and
it is not diﬃcult to show this is an optimal upper bound for storing a preﬁx code with
minimum expected codeword length. For example, suppose we are given a permutation 
π over n characters. Let P be the probability distribution that assigns probability
1/2i to the π(i)th character, for 1 ≤ i < n, and probability 1/2n−1 to the π(n)th character.
 Since P is dyadic, every preﬁx code with minimum expected codeword length
assigns a codeword of length i to the π(i)th character, for 1 ≤ i < n, and a codeword
of length n−1 to the π(n)th character. Therefore, given any preﬁx code with minimum
expected codeword length and a bit indicating whether π(n − 1) < π(n), we can ﬁnd
π. Since there are n! choices for π, in the worst case it takes Ω(log n!) = Ω(n log n)
bits to store a preﬁx code with minimum expected codeword length.

Considering the argument above, it is natural to ask whether the same lower
bound holds for probability distributions that are not so skewed, and the answer is
no. A preﬁx code is canonical [18] if the ﬁrst codeword is a string of 0s and any other
codeword can be obtained from its predecessor by adding 1, viewing its predecessor
as a binary number, and appending some number of 0s. (See, e.g., [15, 13] for more
recent work on canonical codes.) Given any preﬁx code, without changing the length
of the codeword assigned to any character, we can put the code into canonical form
by just exchanging left and right siblings in the code-tree. Moreover, we can reassign
the codewords such that, if a character is lexicographically the jth with a codeword of
length (cid:96), then it is assigned the jth consecutive codeword of length (cid:96). It is clear that it
is suﬃcient to store the codeword length of each character to be able of reconstructing
such a code, and thus the code can be represented in O(n log L) bits, where L is the
longest codeword.

The above gives us a ﬁner upper bound. For example, Katona and Nemetz [12]
showed that, if a character has probability p, then any Huﬀman code assigns it a codeword 
of length at most about log(1/p)/ log φ, where φ ≈ 1.618 is the golden ratio,
and thus L is at most about 1.44 log(1/pmin), where pmin is the smallest probability in
P . Alternatively, one can enforce a value for L and pay a price in terms of expected

2

Fast and Compact Preﬁx Codes

codeword length. Milidi´u and Laber [14] showed how, for any L > (cid:100)log n(cid:101), we can
build a preﬁx code with maximum codeword length at most L and expected codeword 
length within 1/φL−(cid:100)log(n+(cid:100)log n(cid:101)−L)(cid:101)−1 of the minimum. Their algorithm works by
building a Huﬀman tree T1; removing all the subtrees rooted at depth greater than
L; building a complete binary tree T2 of height h whose leaves are those removed
from T1; ﬁnding the node v at depth L − h − 1 in T1 whose subtree T3’s leaves are
labelled by characters with minimum total probability (which they showed is at most
1/φL−(cid:100)log(n+(cid:100)log n(cid:101)−L)(cid:101)−1); and replacing v by a new node whose subtrees are T2 and
T3.

A simple upper bound for storing a preﬁx code with expected codeword length
within a constant of the minimum, follows from Gilbert and Moore’s proof [9] that we
can build an alphabetic preﬁx code with expected codeword length less than H(P )+2
and, thus, within 2 bits of the minimum. Moreover, in an optimal alphabetic preﬁx
code, the expected codeword length is within 1 of the minimum [17, 19] which, in
turn, is within 1 of the entropy H(P ). In an alphabetic preﬁx code, the lexicographic
order of the codewords is the same as that of the characters, so we need store only the
code-tree and not the assignment of codewords to characters. If we store the codetree 
in a succinct data structure due to Munro and Raman [16], then it takes O(n)
bits and encoding and decoding any character takes time at most proportional to its
codeword length. This can be improved to O(1) by using table lookup, but doing so
may worsen the space bound unless we also restrict the maximum codeword length,
which may in turn increase in the expected codeword length.

The code-tree of a canonical code can be stored in just O(L2) bits: By its deﬁnition,
we can reconstruct the whole canonical tree given only the ﬁrst codeword of each
length. Unfortunately, Gagie’s lower bound [6] suggests we generally cannot combine
results concerning canonical codes with those concerning alphabetic preﬁx codes.

Constant-time encoding and decoding using canonical code-trees is simple. Notice
that if two codewords have the same length, then the diﬀerence between their ranks
in the code is the same as the diﬀerence between the codewords themselves, viewed
as binary numbers. Suppose we build an O(L2)-bit array A and a dictionary D supporting 
predecessor queries, each storing the ﬁrst codeword of each length. Given the
length of a character’s codeword and its rank among codewords of the same length
(henceforth called its oﬀset), we can ﬁnd the actual codeword by retrieving the ﬁrst
codeword of that length from A and then, viewing that ﬁrst codeword as a binary
number, adding the oﬀset minus 1. Given a binary string starting with a codeword,
we can ﬁnd that codeword’s length and oﬀset by retrieving the string’s predecessor in
D, which is the ﬁrst codeword of the same length; truncating the string to the same
length in order to obtain the actual codeword; and subtracting the ﬁrst codeword
from the actual codeword, viewing both as binary numbers, to obtain the oﬀset minus 
1. (If D supports numeric predecessor queries instead of lexicographic predecessor
queries, then we store the ﬁrst codewords with enough 0s appended to each that they
are all the same length, and store their original lengths as auxiliary information.)
Assuming it takes O(1) time to compute the length and oﬀset of any character’s
codeword given that character’s index in the alphabet, encoding any character takes
O(1) time. Assuming it takes O(1) time to compute any character’s index in the alphabet 
given its codeword’s length and oﬀset, decoding takes within a constant factor
of the time needed to perform a predecessor query on D. For simplicity, in this paper

3

Paper Submitted to PSC

we consider the number used to represent a character in the machine’s memory to be
that character’s index in the alphabet, so ﬁnding the index is the same as ﬁnding the
character itself.
In a recent paper on adaptive preﬁx coding, Gagie and Nekrich [8] (see also [11])
pointed out that if L = O(w), where w is the length of a machine word, then we can
implement D as an O(w2)-bit data structure due to Fredman and Willard [4] such
that predecessor queries take O(1) time. This seems a reasonable assumption since,
for any string of length m with log m = O(w), if P is the probability distribution
that assigns to each character probability proportional to its frequency in the string,
then the smallest positive probability in P is at least 1/m; therefore, the maximum
codeword length in either a Huﬀman code or a Shannon code for P is O(w). Gagie and
Nekrich used O(n log n)-bit arrays to compute the length and oﬀset of any character’s
codeword given that character’s index in the alphabet, and vice versa, and thus
achieved O(1) time for both encoding and decoding.

(cid:17)

A technique we will use to obtain our result is the wavelet tree of Grossi et al. [10],
erations can be carried out in O(cid:16) log σ
and more precisely the multiary variant due to Ferragina et al. [2]. The latter represents 
a sequence S[1, n] over an alphabet Σ of size σ such that the following optime 
on the RAM model with a computer
word of length Ω(log n): (1) Given i, retrieve S[i]; (2) given i and c ∈ Σ, compute
rankc(S, i), the number of occurrences of c in S[1, i]; (3) given j and c ∈ Σ, compute
nH0(S) +O(cid:16) n log log n
selectc(S, j), the position in S of the j-th occurrence of c. The wavelet tree requires
bits of space, where H0(S) ≤ n log σ is the empirical zero-order
entropy of S, deﬁned as H0(S) = H({nc/n}c∈σ), where nc is the number of occurcompressor 
applied to S. It will be useful to write H0(S) =(cid:80)
rences of c in S. Thus nH0(S) is a lower bound to the output size of any zero-order

log log n

(cid:17)

logσ n

c∈σ

n log n
nc
nc

.

3 Additive increase in expected codeword length

In this section we exchange a small additive penalty over the optimal preﬁx code
for a space-eﬃcient representation of the encoding, which in addition enables en-
code/decode operations in constant time.
It follows from Milidi´u and Laber’s bound [14] that, for any  > 0, there is always
a preﬁx code with maximum codeword length L = (cid:100)log n(cid:101) + (cid:100)log(2/)(cid:101) and expected
codeword length within

φL−(cid:100)log(n+(cid:100)log n(cid:101)−L)(cid:101)−1 ≤

1

1

φL−(cid:100)log n(cid:101)−1 ≤ − log φ < 

of the minimum. The techniques described in the previous section give a way to store
such a code in O(L2 + n log L) bits, yet it is not immediate how to do constant-time
encoding and decoding. Alternatively, we can achieve constant-time encoding and
decoding using O(w2 + n log n) bits for the code-tree.

To achieve constant encoding and decoding times without ruining the space, we
use multiary wavelet trees. We use a canonical code, and sort the characters (i.e.,
leaves) alphabetically within each depth, as described in the previous section. Let
S[1, n] be the sequence of depths in the canonical code-tree, so that S[c] (1 ≤ c ≤ n) is
the depth of the character c. Now, the depth and oﬀset of any c ∈ Σ is easily computed

4

Fast and Compact Preﬁx Codes

(cid:17)

logL n

(cid:17)

log log n

requires n log L +O(cid:16) n log log n
O(cid:16) log L

from the wavelet tree of S: the depth is just S[c], while the oﬀset is rankS[c](S, c).
Inversely, given a depth d and an oﬀset o, the corresponding character is selectd(S, o).
The O(w2)-bit data structure of Gagie and Nekrich [8] converts in constant time
pairs (depth,oﬀset) into codes and vice versa, whereas the multiary wavelet tree on S
bits of space and completes encoding/decoding in time
. Under the restriction 1/ = polylog(n), the space is O(w2)+n log L+o(n)
and the time is O(1).
Going further, we note that H0(S) is at most log(L − (cid:100)log n(cid:101) + 1) + O(1), so
we can store S in O(n log(L − log n + 1) + n) = O(n log log(1/)) bits. To see this,
consider S as two interleaved subsequences, S1 and S2, of length n1 and n2, with S1
containing those lengths less than or equal to (cid:100)log n(cid:101) and S2 containing those greater.
Thus nH0(S) ≤ n1H0(S1) + n2H0(S2) + n.
Since there are at most 2(cid:96) codewords of length (cid:96), assume we complete S1 with
cannot decrease n1H0(S1) = (cid:80)
spurious symbols so that it has exactly 2(cid:96) occurrences of symbol (cid:96). This completion
, as increasing some nc to nc + 1
produces a diﬀerence of f (n)− f (nc) ≥ 0, where f (x) = (x + 1) log(x + 1)− x log x is
increasing. Hence we can reason as if S1 contained exactly 2(cid:96) occurrences of symbol
1 ≤ (cid:96) ≤ (cid:100)log n(cid:101), where straightforward calculation shows that n1H0(S1) = O(n1).
On the other hand, S2 contains at most L − (cid:100)log n(cid:101) distinct values, so H0(S2) ≤
log(L − (cid:100)log n(cid:101)), unless L = (cid:100)log n(cid:101), in which case S2 is empty and n2H0(S2) = 0.
Thus n2H0(S2) ≤ n2 log(cid:100)log(2/)(cid:101) = O(n2 log log(1/)).
Combining both bounds, we get H0(S) = O(1 + log log(1/)). If we assume the
text to encode is of length m = 2O(w), as usual under the RAM model of computation,
then L = O(w) and the following theorem holds.
Theorem 1. For any 0 <  < 1/2 with 1/ = O(polylog(n)), and under the RAM
model with computer word size w, so that the text to encode is of length 2O(w), we
can store a preﬁx code with expected codeword length within an additive term  of
the minimum, using O(w2 + n log log(1/)) bits, such that encoding and decoding any
character takes O(1) time.

1≤c≤(cid:100)log n(cid:101) nc log n1
nc

In other words, under mild assumptions, we can store a code using O(n log log(1/))
bits at the price of increasing the average codeword length by , and in addition have
constant-time encoding and decoding. For constant , this means that the code uses
just O(n) bits at the price of an arbitrarily small constant additive penalty over the
shortest possible preﬁx code.

4 Multiplicative increase in expected codeword length

In this section we focus on a multiplicative rather than additive penalty over the optimal 
preﬁx code, in order to achieve a sublinear-sized representation of the encoding,
which still enables constant-time encoding and decoding.

Our main idea is to divide the alphabet into probable and improbable characters
and to store information about only the probable ones. Given a constant c > 1,
we use Milidi´u and Laber’s algorithm [14] to build a preﬁx code with maximum
codeword length L = (cid:100)log n(cid:101) + (cid:100)1/(c − 1)(cid:101) + 1. We call a character’s codeword
short if it has length at most L/c + 2, and long otherwise. Notice there are at most

5

Paper Submitted to PSC

2L/c+3−1 = O(cid:16)

n1/c(cid:17)

characters with short codewords. Also, although applying Milidi´u
and Laber’s algorithm may cause some exceptions, characters with short codewords
are usually more probable than characters with long codewords. We will hereafter
call infrequent characters those encoded with long codewords in the code of Milidi´u
and Laber.

(cid:17)

n1/c log n

characters with short codewords. This data structure takes O(cid:16)

We transform this length-restricted preﬁx code as described in Section 2, namely,
we sort the characters lexicographically within each depth. We use a dictionary data
structure F due to Fredman, Koml´os and Szemer´edi [3] to store the indices of the
bits and
supports membership queries in O(1) time, with successful queries returning the target 
character’s codeword. We also build (cid:98)L/c(cid:99) + 2 arrays that together store the
indices of all the characters with short codewords; for 1 ≤ (cid:96) ≤ (cid:98)L/c(cid:99) + 2, the (cid:96)th
array stores the indices the characters with codewords of length (cid:96), in lexicographic
order by codeword. Again, we store the ﬁrst codeword of each length in O(w2) bits
overall, following Gagie and Nekrich [8], such that it takes O(1) time to compute any
codeword given its length and oﬀset, and vice versa. With these data structures, we
can encode and decode any character with a short codeword in O(1) time. To encode,
we perform a membership query on the dictionary to check whether the character has
a short codeword; if it does, we receive the codeword itself as satellite information
returned by the query. To decode, we ﬁrst ﬁnd the codeword’s length (cid:96) and oﬀset j
in O(1) time as described in Section 2. Since the codeword is short, (cid:96) ≤ (cid:98)L/c(cid:99) + 2
and the character’s index is stored in the jth cell of the (cid:96)th array.
We replace each long codeword with new codewords: instead of a long codeword α
of length (cid:96), we insert 2L+1−(cid:96) new codewords α · s, where · denotes concatenation and
s is an arbitrary binary string of length L + 1 − (cid:96). Figure 1 shows an example. Since
c > 1, we have n1/c < n/2 for suﬃciently large n, so we can assume without loss of
generality that there are fewer than n/2 short codewords; hence, the number of long
codewords is at least n/2. Since every long codeword is replaced by at least two new
codewords, the total number of new codewords is at least n. Since new codewords
are obtained by extending all codewords of length (cid:96) > L/c + 1 in a canonical code,
all new codewords are binary representations of consecutive integers. Therefore the
i-th new codeword equals to αf + i − 1, where αf is the ﬁrst new codeword. If a is
an infrequent character, we encode it with the a-th new codeword, αf + a − 1. To
encode a character a, we check whether a belongs to the dictionary F . If a ∈ F , then
we output the codeword for a. Otherwise we encode a as αf + a − 1. To decode a
codeword α, we read its preﬁx bitstring sα of length L + 1 and compare sα with αf .
If sα ≥ αf , then α = sα is the codeword for sα − αf + 1. Otherwise, the codeword
length of the next codeword α is at most L/c + 1 and α can be decoded as described
in the previous paragraph.

By analysis of the algorithm by Milidi´u and Laber [14] we can see that the codeword 
length of a character in their length-restricted code exceeds the codeword length
of the same character in an optimal code by at most 1, and only when the codeword
length in the optimal code is at least L−(cid:100)log n(cid:101)−1 = (cid:100)1/(c−1)(cid:101). Hence, the codeword
length of a character encoded with a short codeword exceeds the codeword length of
(cid:100)1/(c−1)(cid:101) ≤ c. Every
(cid:100)1/(c−1)(cid:101)+1
the same character in an optimal code by a factor of at most
infrequent character is encoded with a codeword of length L + 1. Since the codeword
length of an infrequent character in the length-restricted code is more than L/c + 2,

6

Fast and Compact Preﬁx Codes

Figure 1. An example with n = 16 and c = 3. The black tree is the result of applying the algorithm of
Milidi´u and Laber on the original preﬁx code. Now, we set L = 6 according to our formula, and declare
short the codeword lengths up to (cid:98)L/c(cid:99) + 2 = 4. Short codewords are stored unaltered in a dictionary (blue).
Longer codewords are changed: All are extended up to length L + 1 = 7 and reassigned a code according to
their values in the contiguous slots of length 7 (in red).

its length in an optimal code is more than L/c + 1. Hence, the codeword length of
a long character in our code is at most L+1
L/c+1 < c times greater than the codeword
length of the same character in an optimal code. Hence, the average codeword length
for our code is less than c times the optimal one.

Theorem 2. For any constant c > 1, under the RAM model with computer word
size w, so that the text to encode is of length 2O(w), we can store a preﬁx code with
bits, such
that encoding and decoding any character takes O(1) time.

expected codeword length within c times the minimum in O(cid:16)
length within c times of the optimum, in O(cid:16)

Again, under mild assumptions, this means that we can store a code with expected
bits and allowing constant-time

w2 + n1/c log n

(cid:17)

(cid:17)

n1/c log n

encoding and decoding.

References

[1] M. Adler and B. M. Maggs: Protocols for asymmetric communication channels. Journal of Computer

and System Sciences, 63(4) 2001, pp. 573–596.

[2] P. Ferragina, G. Manzini, V. M¨akinen, and G. Navarro: Compressed representations of sequences
[3] M. L. Fredman, J. Koml´os, and E. Szemer´edi: Storing a sparse table with O(1) worst case access

and full-text indexes. ACM Transactions on Algorithms, 3(2) 2007, article 20.

time. Journal of the ACM, 31(3) 1984, pp. 538–544.

7

1100000 + char − 1ciahmp000100111000100110101011abcdefghijklmnopbdefgklnobdefgklnojPaper Submitted to PSC

[4] M. L. Fredman and D. E. Willard: Surpassing the information theoretic bound with fusion trees.

Journal of Computer and System Sciences, 47(3) 1993, pp. 424–436.

[5] T. Gagie: Compressing probability distributions. Information Processing Letters, 97(4) 2006, pp. 133–

137.

[6]
: Large alphabets and incompressibility. Information Processing Letters, 99(6) 2006, pp. 246–251.
[7]
: Dynamic asymmetric communication. Information Processing Letters, 108(6) 2008, pp. 352–355.
[8] T. Gagie and Y. Nekrich: Worst-case optimal adaptive preﬁx coding, in Proceedings of the Algorithms

and Data Structures Symposium (WADS), 2009, to appear.

[9] E. N. Gilbert and E. F. Moore: Variable-length binary encodings. Bell System Technical Journal,

38 1959, pp. 933–967.

[10] R. Grossi, A. Gupta, and J. Vitter: High-order entropy-compressed text indexes, in Proc. 14th

Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), 2003, pp. 841–850.

[11] M. Karpinski and Y. Nekrich: A fast algorithm for adaptive preﬁx coding. Algorithmica, to appear.
[12] G. O. H. Katona and T. O. H. Nemetz: Huﬀman codes and self-information. IEEE Transactions

on Information Theory, 22(3) 1976, pp. 337–340.

[13] S. T. Klein: Skeleton trees for the eﬃcient decoding of Huﬀman encoded texts. Information Retrieval,

3(4) 2000, pp. 315–328.

[14] R. L. Milidi´u and E. S. Laber: Bounding the ineﬃciency of length-restricted preﬁx codes. Algorithmica,
 31(4) 2001, pp. 513–529.

[15] A. Moffat and A. Turpin: On the implementation of minimum-redundancy preﬁx codes.

IEEE

Transactions on Communications, 45(10) 1997, pp. 1200–1207.

[16] J. I. Munro and V. Raman: Succinct representation of balanced parentheses and static trees. SIAM

Journal on Computing, 31(3) 2001, pp. 762–776.

[17] N. Nakatsu: Bounds on the redundancy of binary alphabetical codes. IEEE Transactions on Information

Theory, 37(4) 1991, pp. 1225–1229.

[18] E. S. Schwarz and B. Kallick: Generating a canonical preﬁx encoding. Communications of the

ACM, 7(3) 1964, pp. 166–169.

[19] D. Sheinwald: On binary alphabetic codes, in Proceedings of the Data Compression Conference, 1992,

pp. 112–121.

8

