Text Indexing and Searching in Sublinear Time

J. Ian Munro∗

Gonzalo Navarro†

Yakov Nekrich‡

Abstract

We introduce the ﬁrst index that can be built in o(n) time for a text of length n, and also
queried in o(m) time for a pattern of length m. On a constant-size alphabet, for example, our
index uses O(n log1/2+ε n) bits, is built in O(n/ log1/2−ε n) deterministic time, and ﬁnds the occ
pattern occurrences in time O(m/ log n + √log n log log n + occ), where ε > 0 is an arbitrarily
small constant. As a comparison, the most recent classical text index uses O(n log n) bits, is
built in O(n) time, and searches in time O(m/ log n + log log n + occ). We build on a novel text
sampling based on diﬀerence covers, which enjoys properties that allow us eﬃciently computing
longest common preﬁxes in constant time. We extend our results to the secondary memory
model as well, where we give the ﬁrst construction in o(Sort(n)) time of a data structure with
suﬃx array functionality, which can search for patterns in the almost optimal time, with an
additive penalty of O(qlogM/B n log log n), where M is the size of main memory available and
B is the disk block size.

7
1
0
2
 
c
e
D
0
2

 

 
 
]
S
D
.
s
c
[
 
 

1
v
1
3
4
7
0

.

2
1
7
1
:
v
i
X
r
a

∗Cheriton School of Computer Science, University of Waterloo. Email imunro@uwaterloo.ca.
†CeBiB — Center of Biotechnology and Bioengineering, Department of Computer Science, University of Chile.

Email gnavarro@dcc.uchile.cl. Funded with Basal Funds FB0001, Conicyt, Chile.

‡Cheriton School of Computer Science, University of Waterloo. Email: yakov.nekrich@googlemail.com.

Introduction

1
We address the problem of indexing a text T [0..n − 1], over alphabet [0..σ − 1], in sublinear time
on a RAM machine of w = Θ(log n) bits. This is not possible when we build a classical index (e.g.,
a suﬃx tree [28] or a suﬃx array [22]) that requires Θ(n log n) bits, since just writing the output
requires time Θ(n). It is also impossible when log σ = Θ(log n) and thus just reading the n log σ
bits of the input text requires time Θ(n). On smaller alphabets (which arise frequently in practice,
for example on DNA, protein, and letter sequences), the opportunity of sublinear-time indexing
arises when the text comes packed in words of logσ n characters and we build a compressed index
that uses o(n log n) bits. For example, there exist various indexes that use O(n log σ) bits [25]
(which is asymptotically the best worst-case size we can expect for an index on T ) and could be
built, in principle, in time O(n/ logσ n). Still, only linear-time indexing on compressed space has
been achieved so far [4, 5, 23].

σ

n), which is o(n) for small alphabets: log σ = o(log1/3−ε′

When the alphabet is small, one may also aim at RAM-optimal pattern search, that is, count
the number of occurrences of a (packed) string Q[0..m − 1] in T in time O(m/ logσ n). There
exist some classical indexes using O(n log n) bits and counting in time O(m/ logσ n + polylog(n))
[19, 26, 10], as well as compressed ones [23]. Some can be built in linear deterministic time [10, 23].
In this paper we obtain a ﬁrst signiﬁcant advance in the direction of indexes that can be built and
queried in sublinear time. We introduce an index using O(nplog1+ε n log σ) bits for any constant
ε > 0, which is nearly a geometric mean between the classical and the best compressed sizes. It can
be built in time O(n log σ/ log1/2−ε
n) for
some constant ε′ > 0 (this includes the important practical σ = O(polylog n)). The index can count
in optimal time (plus a small additive polylogarithmic penalty), O(m/ logσ n + plogσ n log log n).
After counting the occurrences of Q, any such occurrence can be reported in O(1) time.
Our technique is reminiscent to the Geometric BWT [13], where a text is sampled regularly, so
that the sampled positions can be indexed with a suﬃx tree in sublinear space. In exchange, all
the possible alignments of the pattern and the samples have to be checked in a two-dimensional
range search data structure. To speed up the search, we use instead an irregular sampling that is
dictated by a diﬀerence cover, which guarantees that for any two positions there is a small value
that makes them sampled if we add the value to both. This enables us to compute in constant time
the longest common preﬁx of any two text positions with only the sampled suﬃx tree. With this
information we can eﬃciently ﬁnd the locus of each alignment from the previous one. Diﬀerence
covers were used in the past for suﬃx tree construction [20], but never for improving query times.
We also extend our model to secondary memory, with main memory size M and block size
In this case, we can build the index in O((n/B)qlogM/B n log σ) I/Os and report the occ
B.
occurrences of the pattern in O(n/(B logσ n) + qlogM/B n log log n + occ/B) I/Os. Note that, for
log σ = o(logM/B n), this is the ﬁrst suﬃx array construction taking time o(Sort(n)), which is
possible because we actually sort only the sampled suﬃxes. This demonstrates that, while Sort(n)
is a lower bound for full suﬃx array construction [15], we can build in less time an index that
emulates its search functionality in near-optimal time.

2 Preliminaries and Diﬀerence Covers
We denote by |S| the number of symbols in a sequence S or the number of elements in a set S. For
two strings X and Y , LCP (X, Y ) denotes the longest common preﬁx of X and Y . For a string X
and a set of strings S, LCP (X,S) = maxY ∈S LCP (X, Y ). We assume that the concepts associated
with suﬃx trees [28] are known. We describe in more detail the concept of diﬀerence cover.

1

Deﬁnition 1 Let D = { a0, a1, . . . , at−1 } be a set of t integers satisfying 0 ≤ ai ≤ s. We say that
D is a diﬀerence cover modulo s of size t, denoted DC(s, t), if for every d satisfying 1 ≤ d ≤ s − 1
there is a pair 0 ≤ i, j ≤ t − 1 such that d = (ai − aj) mod s.

Colburn and Ling [14] describe a diﬀerence cover DC(Θ(r2), Θ(r)) for any positive integer r
that is based on a result of Wichmann [29]. Consider a sequence b1, . . . , b6r+3 where bi = 1 for
1 ≤ i ≤ r, br+1 = r + 1, bi = 2r + 1 for r + 2 ≤ i ≤ 2r + 1, bi = 4r + 3 for 2r + 2 ≤ i ≤ 4r + 2,
bi = 2r + 2 for 4r + 3 ≤ i ≤ 5r + 3, and bi = 1 for 5r + 4 ≤ i ≤ 6r + 3. We set a0 = 0 and
ai = ai−1 + bi for i = 1, . . . , 6r + 3.
Lemma 1 [14] The set A = { a0, . . . , a6r+3 } is a diﬀerence cover DC(12r2 + 18r + 6, 6r + 4).
The following property of a diﬀerence cover will be extensively used in our data structure.

Lemma 2 Let D = { a0, . . . , at−1 } be a diﬀerence cover modulo s. Then for any 1 ≤ i, j ≤ s
there exists a non-negative integer h(i, j) < s such that (i + h(i, j)) mod s ∈ D and (j + h(i, j))
mod s ∈ D. If D is known, we can compute h(i, j) for all i, j in time O(s2).
Proof : For any x there exists a pair fx ∈ D, ex ∈ D satisfying ex − fx = x mod s by deﬁnition of a
diﬀerence cover. Let d[x] = fx. Then both d[x] and (d[x]+x) mod s are in D. Let h(i, j) = (d[(j−i)
mod s]− i) mod s. Then i+ h(i, j) = d[(j− i) mod s] ∈ D and j + h(i, j) = d[(j− i) mod s]+ ((j− i)
mod s) ∈ D.

(cid:3)

3 Data Structure
We divide the text T [0..n − 1] into blocks of Θ(logσ n) consecutive symbols. To be precise, every
block consists of s = 12r2 + 18r + 6 symbols where r = Θ(plogσ n) for an appropriately chosen
small constant. To avoid tedious details, we assume that plogσ n is an integer and that the text
length is divisible by s. We select 6r + 4 positions in every block that correspond to the diﬀerence
cover of Lemma 1. That is, all positions si + aj for i = 0, 1, . . . , (n/s) − 1 and 0 ≤ j ≤ 6r + 3 are
selected. The total number of selected positions is O(n/r). The set S′ consists of all suﬃxes starting
at selected positions. A substring between two consecutive selected symbols T [si + aj..si + aj+1− 1]
is called a sub-block ; note that a sub-block may be as long as 4r + 3. Our data structure consists
of the following three components.

1. The suﬃx tree T ′ for suﬃxes starting at selected positions, which uses O((n/r) log n) =
O(n√log n log σ) bits. Thus T ′ is a compacted trie for the suﬃxes in S′. Suﬃxes are represented 
as strings of meta-symbols where every meta-symbol corresponds to a substring of
logσ n consecutive symbols. Deterministic dictionaries and van Emde Boas data structures
are used at the nodes to descend by the meta-symbols in constant time, so T ′ can be built in
time O((n/r) log log n) = O(n log log n/plogσ n) [27]. Given a query pattern Q, we can identify 
all selected suﬃxes starting with Q in O(|Q|/ logσ n) time, plus an O(log log n) additive
term.

2. A data structure on a set Q of two-dimensional points. Each point of Q corresponds to a pair
(revi, indi) for i = 1, . . . , (n/s) − 1 where indi is the index of the i-th selected suﬃx of T in
the lexicographically sorted set S′ and revi is an integer that corresponds to the reverse subblock 
preceding that i-th selected suﬃx in T . Our data structure supports two-dimensional
range reporting queries on Q with some restrictions: it can report all points in a query range
[r1, r2] × [i1, i2], where r1 = i · σf and r2 = (i + 1) · σf for 0 ≤ i < σ and 1 ≤ f < 4r + 3.

2

3. A data structure for suﬃx jump queries on T ′. Given a string Q[0..q − 1], its locus node u,
and a positive integer i ≤ r, a suﬃx jump query returns the locus of Q[i..q − 1] or determines
that Q[i..q − 1] does not occur in T ′. The suﬃx jump structure has essentially the same
functionality as the suﬃx links, but we do not store suﬃx links explicitly in order to save
space and improve the construction time.

Using our structure, we can ﬁnd all the occurrences in T of a pattern Q[0..m − 1] whenever
m > 4r + 3. Occurrences of Q are classiﬁed according to their positions relative to selected
symbols. An occurrence of Q is an i-occurrence if the i-th symbol of Q is a selected symbol,
T [f..f + m − 1] = Q[0..m − 1] and the symbol T [f + i] is selected. First, we identify all 0occurrences 
by looking for Q in T ′. We traverse the path corresponding to Q in T ′. Let Q0 denote
the longest preﬁx of Q that is found in T ′, with locus u0, and let m0 = |Q0|. If m0 = m, then u0 is
the locus of Q and we report all 0-occurrences of Q by reporting positions of suﬃxes in the subtree
of u0. If m0 < m, then there are no 0-occurrences of Q in T . Next, we compute a 1-jump of u0 and
ﬁnd the location in T ′ that corresponds to Q[1..m0 − 1]. If Q[1..m0 − 1] does not exist, then there
are no 1-occurrences of Q. If it exists, we traverse the path in T ′ starting from that location. Let
Q1 = Q[1..m1−1] denote the longest preﬁx of Q[1..m−1] found in T ′, with locus u1. If m > m1 +1,
then there again there are no 1-occurrences of Q. If m = m1 + 1, then u1 is indeed the locus of
Q[1..m − 1]. In this case, every 1-occurrence of Q corresponds to an occurrence of Q1 in T that
is preceded by Q[0]. We can identify them by answering a two-dimensional point reporting query
[rev1, rev2] × [ind1, ind2] where ind1 (resp. ind2) is the leftmost (rightmost) leaf in the subtree
of u1 and rev1 (rev2) is the smallest (largest) integer value of any reverse sub-block that starts
with Q[0]. In order to avoid technical complications we maintain several (four) data structures for
range reporting queries. Points representing (suﬃx, sub-block) pairs are classiﬁed according to the
sub-block size. If a selected suﬃx T [f..] is preceded by another selected suﬃx T [f − 1], then we do
not need to store a point for this suﬃx. In all other cases the size of the sub-block that precedes a
suﬃx is either r or 2r or 4r + 2 or 2r + 1. We assign the point representing the (suﬃx,sub-block)
pair to one of the four data structures. When we want to report i-occurrences of Q for i ≤ r, we
query all four data structures; however when i > r we query only three last data structures (with
points representing sub-blocks of size 2r, 4r + 2, and 2r + 1).

We proceed and report i-occurrences for i = 2, . . . , 4r + 3 using the same method. Suppose that
we have already considered the possible j-occurrences of Q for j = 0, . . . , i − 1. Let t be such that
mt = max(m0, . . . , mi−1). We compute the (i− t)-jump from Qt. If Q[i..mt− 1] is found in T ′, with
locus ut, but mt < m − t, we traverse from ut downwards to complete the path for Q[i..m − 1]. If
the locus of Q[i..m − 1] is found, we report all i-occurrences by answering a two-dimensional query
as described above.
The total time is O(m/ logσ n + r(ts + tq)) where tq is the time needed to answer a reporting
query and ts is the time needed to compute a suﬃx jump. In Section 4 we show how to construct a
reporting data structure with tq = O(log log n), which then can report each occurrence in constant
time. Then, in Sections 5 and 6, we will show how to implement suﬃx jumps in ts = O(log log n).
This yields our main result.

Theorem 1 Given a text T of length n over an alphabet of size σ, we can build an index within
O(nplog1+ε n log σ) bits in time O(n log σ/ log1/2−ε
n), so that it can count the occurrences of
a pattern of length m in time O(m/ logσ n + plogσ n log log n), after which it can report each
occurrence position in O(1) time.

σ

A pattern shorter than 4r + 4 may not cross a sub-block boundary and thus some occurrences

3

may go undetected with the method above. For those, we use instead a special index, described in
Appendix A.1.

4 Range Reporting

Our method builds upon the recent previous work on wavelet tree construction [24, 3], and applications 
of wavelet trees to range predecessor queries [6] as well as on results for compact range
reporting [12, 11]. We are given a set Q of O(n/r) points. First we sort the points by x-coordinates
(this is easily done by scanning the leaves of T ′, which are already sorted lexicographically by
the selected suﬃxes), and keep the y-coordinates of every point in a sequence Y . Each element
of Y can be regarded as a string of length O(r) = O(plogσ n), or equivalently, a number of
O(r log σ) = O(√log n log σ) bits. Next we construct the range tree for Y using a method similar
to the wavelet tree [18] construction algorithm. Let Y (uo) = Y for the root node uo. We classify
elements of Y (uo) according to the ﬁrst bit and generate sequences Y (ul) and Y (ur) that must be
stored in the left and right children of u, respectively. Y (ul) and Y (ur) are sequences that contain
the y-coordinates of the points that must be stored in ul and ur; they are organized in the same
way as Y (uo). Then nodes ul and ur are recursively processed in the same manner. When we
generate the sequence for a node u of depth d, we assign elements to Y (ul) and Y (ur) according
to their d-th bit. The total time needed to assign elements to nodes and generate Y (u), using a

recent method [24, 3], is O((n/r)(r log σ)/plog n/ log σ) = O(nqlog3 σ/ log n), and the space is
O((n/r)(r log σ)) = O(n√log σ) bits.

any range minimum data structure for this purpose, see e.g.,[8]. We can traverse Y (u) and identify

For every sequence Y (u) we also construct an auxiliary data structure that supports threesided 
queries.
If u is a right child, we create a data structure that returns all elements in a
range [x1, x2] × [0, h] stored in Y (u). To this end, we divide Y (u) into groups Gi(u) of g =
(1/2)√log n/ log σ consecutive elements (the last group may contain up to 2g elements). Let mini(u)
denote the smallest element in every group and let Y ′(u) denote the set of all mini(u). We construct
a data structure that supports three-sided queries on Y ′(u); it uses O((|Y (u)| log σ/√log n) log n) =
O(|Y (u)|√log n log σ) bits and answers queries in O(log log n + k) time; for example, we can use
the smallest element in each group in O(|Y (u)|(log σ)/√log n) time. Since the number of points in
Y ′(u) is O(|Y (u)|/g), the data structure for Y ′(u) can be created in O(|Y (u)|/g) time, which does
not alter our previous space and construction time. Every group can be encoded with O(√log n)
bits; we can ask at most log n diﬀerent range minimum queries on a group. Hence we can store
precomputed answers to all possible group queries in a table of size O(√n log2 n) bits. If u is a left
child, we use the same method to construct the data structure that returns all elements in a range
[x1, x2] × [h, +∞) from Y (u).
An orthogonal range reporting query [x1, x2]× [y1, y2] is answered by ﬁnding the lowest common
ancestor v of the leaves that hold y1 and y2. Then we visit the right child vr of v, identify the range
[x′1, x′2] and report all points in Y (vr)[x′1..x′2] with y-coordinates that do not exceed y2; here x′1 is
the index of the smallest (with respect to its x-coordinate) element in Y (u) that is larger than or
equal to x1 and x′2 is the index of the largest (with respect to x-coordinate) element of Y (u) that is
smaller than or equal to x2. We also visit the left child vl of v, and answer a symmetric three-sided
query.

An answer to our three-sided query returns positions in Y (vl) (resp. in Y (vr)). That is, we
know the y-coordinates of reported points, but we do not know their x-coordinates. We need
an additional data structure to translate positions in Y (u) into points that must be reported.
While our range tree can be used for this purpose, the cost of decoding every point would be
i with node

O(√log σ log n). To resolve this problem, we construct a series of wavelet trees T W

4

degrees 2di for di = (log σ log n)iε and i = 0, 1, 2, . . . ,⌈1/2ε⌉. Henceforth ε denotes an arbitrarily
small positive constant. To simplify the description we assume that logε
σ n and log σ are integers.
For an integer x the x-ancestor of a node v is the lowest ancestor w of v, such that the height of
w is divisible by x. Thus T W
for l = ⌈(1/2)ε⌉ is a tree of height 1; it has one internal node with
2dl = 2√log σ log n children. The tree T W
has 2di
is a binary tree of height √log n log σ. Our decoding procedure moves from a
children. And T W
node u to its d1-ancestor u1, then from u1 to the d2-ancestor of u, and so on.

is a tree of height dl−i; every internal node of T W

0

i

i

l

l

l

i

k

0

The sequence UP 0(u) stored in a node u of T W

contains exactly the same elements as the
sequence Y (u) and elements are stored in the same order. That is, the i-th element of UP 0(u)
corresponds to the i-th element of Y (u) and both of them encode information about the same
point. However, we keep diﬀerent information in sequences UP : the value of UP 0[i] is the position
of the element corresponding to UP 0(u) in the d1-ancestor u1 of u in T W
0 . We observe that the
height of u1 is divisible by d1; hence u1 corresponds to a node of T W
1 . For k ≥ 1 and each
node uk of T W
k , UP k(uk)[i] contains the position of the element corresponding to UP k(uk) in the
d0-ancestor uk+1 of uk in T W
k . In order to decode the element Y (u)[i] we look-up the elements
UP 0(u0)[i0], UP 1(u1)[i1], UP 2(u2)[i2], . . ., where u0 = u, i0 = i, uk is the d1-ancestor of uk−1,
and ik = UP k−1(uk−1)[ik−1]. Since we look-up exactly one element in each T W
k , an element is
decoded in O(1/ε) = O(1) time. It remains to describe how the sequences UP i(u) in trees T W
are
generated.

) and T W

for l = ⌈1/2ε⌉. T W

among children. We start at the root node ul of T W

We will employ an auxiliary sequence ˜Y during our construction algorithm. ˜Y will not be stored
in the wavelet trees, but it is used at the construction stage and helps us distribute elements in
nodes of T W
is a tree of
height 1 (i.e. ul is the single node in T W
l−1 is a tree of height d1 = (log σ log n)ε. We set
˜Y (ul) = Y (ul). The sequence ˜Y (ul) is divided into chunks of size fl = σ2√logσ n = 2√log σ log n. We
record for every Y (ul)[i] its relative position i mod fl within a chunk. All elements in the chunk
are sorted by values of the ﬁrst h · q bits of ˜Y (ul)[i] where h = dl and q = 1/d1. That is, we sort
elements by the ﬁrst (log σ log n)1/2−ε bits All values of ˜Y (ul)[i] = j and their relative positions
within a chunk are copied to the j-th child of ul. Relative positions are stored in the sequence
UP (uj). We keep track of the chunks in binary sequences C(uj). Every 1 in C(uj) corresponds to
an element stored in UP (uj) and every 0 indicates the end of a chunk. Thus if mj elements from
a chunk are added to a UP (uj), we append mj 1’s followed by a 0 to C(uj). We also copy values
of ˜Y (ul)[i] to ˜Y (uj). But when the sequences UP (uj) and C(uj) are generated we “prune” the
values of ˜Y (uj): for each ˜Y (uj) we ignore the ﬁrst h · q bits and also ignore the bits at positions
2h · q + 1..h. The total time needed to produce the sequences is dominated by the time to sort
chunks. We produce the sequences for descendants of ul with 2, 3, . . ., ⌈1/ε⌉ in a similar way. Next
we visit nodes of T W
l−1. In the general
case each sequence ˜Y (uk) stored in a node uk ∈ T W
is a sequence of (dl−k)-bits integers. Let uk−1
be the node that corresponds to uk in T W
k−1; the root of
k−1 is uk−1 and its height is equal to d1. Sequences for T W (uk−1) are generated using ˜Y (uk). We
T W
divide ˜Y (uk) into chunks of size 2dl−k and sort relative positions within a chunk by using selected
bits of ˜Y [i].

l−1 and execute the same procedure in every node ul−1 of T W

k−1. Let T W (uk−1) denote the subtree of T W

k

l

The most time-consuming step during the construction is sorting the elements of a sequence in a
chunk. This task is equivalent to sorting 2di integers of di bits. We will show in the full version that
this takes O(2di · (d2
is t · dl−j and
the sorting step is performed d1 times (that is, each chunk is sorted d1 times). Hence the total time
i−1 is O(t· (dl · dj · d1)). Since dl = √log σ log n and d1 can be estimated
to create the sequences for T W
with O(logε n), the total time spent on all levels is tcons = O(plog σ/ log n logε nPl

i / log n)) time. The total number of elements in all nodes of T W

i=1 di). Since

j

5

i=1 di = O(√log n log σ), we have tcons = O(t log σ logε n)

Pl
Lemma 3 For a set of t = O(n/r) points on t × σO(r) grid, where r = O(plogσ n) there is an
O(t log1+ε n log σ)-bit data structure that can be constructed in O(t log σ logε n) = O(n log3/2 σ/ log1/2−ε n)
time and supports orthogonal range reporting queries in time O(log log t + pocc) where pocc is the
number of reported points.

Our method of constructing an external-memory range reporting structure follows the same
basic range tree approach. However the data structure is much simpler because we do not aim for
almost-linear space and linear construction time.

Lemma 4 There exists an O(m log σ log t)-word data structure that supports range reporting queries
on m × σt grid and can be constructed in O((m/B) log σ log t) I/Os.
Proof : We generate the sequences Y (u) in the same way as in the internal-memory data structure.
But we also keep in every node the sequence X(u) that explicitly contains the x-coordinates of
points. All sequences can be generated in O((m/B) log σ log t) I/Os. For all points stored in a
node, we create a data structure for three-sided queries. A data structure for three-sided queries
on m points can be constructed in O((m/B)) I/Os.
(cid:3)

5 Suﬃx Jumps
In this section we show how suﬃx jumps can be implemented eﬃciently. Let Q[0..q − 1] denote
a query substring and let u be its locus node. We need to ﬁnd the locus of Qi = Q[i..q − 1] or
determine that it does not exist.
Our solution is based on using the properties of diﬀerence covers. We also use the heavy path
decomposition of T ′. Let heavy(u) denote the heavy path that contains a node u. Let H(u)
denote the suﬃx associated with this heavy path. In order to quickly traverse the path labeled
by Q[i..m], we will move down the heavy paths in T ′ and compare corresponding heavy suﬃxes.
Let Su = T [fu..] denote an arbitrary suﬃx in the subtree of u. Our task is to ﬁnd the longest
common preﬁx between the suﬃx T [fu + i..] and a suﬃx from S′. The diﬃculty is that we possibly
do not keep the suﬃx T [fu + i..] in T ′. Nevertheless we can compute this by using properties of a
diﬀerence cover.

Lemma 5 Let S1 = T [f1..] and S2 = T [f2..] be two suﬃxes from the set S′. We can compute
|LCP (T [f1 + i..], S2)| for any 1 ≤ i ≤ r in O(1) time.
Proof : Let δ = h(0, i) where the function h(j, i) is as deﬁned in Lemma 2. By deﬁnition, both
T [f1 + i + δ..] and T [f2 + δ..] are in S′. Hence we can compute ℓ = |LCP (T [f1 + i + δ..], T [f2 + δ..])|
in O(1) time with a lowest common ancestor query [7]. Since δ ≤ s = O(logσ n), the substrings
T [f1 + i..f1 + i + δ − 1] and T [f2..f2 + δ − 1] ﬁt into O(1) words. Hence, we can compute the length
ℓ′ of their lowest common preﬁx in O(1) time. If ℓ′ < δ, then |LCP (T [f1 + i..], S2)| = ℓ′. Otherwise
|LCP (T [f1 + i..], S2)| = δ + ℓ

(cid:3)

We compute the locus of Qi by applying Lemma 5 log n times. Our procedure starts at the root
node and visits O(log n) heavy suﬃxes. We set j = i, the node v is the root node, and v′ is the
child of v that is labeled with Q[j..j + t − 1], with t = logσ n. We identify the heavy suﬃx H(v′)
and compute ℓ = |LCP (H(v′), Qi)|. Suppose that ℓ < q − i. If the heavy path heavy(v′) does not
have a node v1 of string depth (exactly) ℓ, then Qi does not occur in T ′. Otherwise, we check for
the child v′1 of v1 that is labelled with Q[ℓ..ℓ + t − 1]. If v′1 does not exist, then again Qi does not

6

occur in T ′. Otherwise, we compute |LCP (H(v′1), Qi)|. The procedure continues until we ﬁnd a
suﬃx H(v′k) such that |LCP (H(v′k), Qi)| ≥ q − i, which implies that Qi occurs in T ′. We consider
the leaf corresponding to H(v′k) and ﬁnd its ancestor ut with string depth ℓ using a weighted level
ancestor query [1], for which we can preprocess T ′ in O(n/r) time and O((n/r) log n) bits of space,
so as to answer the query in time O(log log n). Clearly, this ﬁnal ut is the locus of Qi.
Our search procedure visits at most log n heavy paths. For every heavy path, we keep the
string depths of all its nodes in a dictionary data structure; hence we can determine if there is a
node of depth ℓ on a path heavy(v′k) in O(1) time. These dictionaries add O((n/r) log n) bits and
O(n/r) log log n) time to the construction of T ′. The total time for a suﬃx jump query is O(log n).
Lemma 6 Suppose that we know Q′ = LCP (Q[0..q−1],S′) and its locus in T ′. Let Qi = Q′[i..q′−1]
where q′ = |Q′|. We can compute Q′i = LCP (Qi,S′) and its locus in T ′ in O(log n) time for any
i ≤ 4r + 3.
6 Suﬃx Jumps for Short Patterns
The result of Lemma 6 already provides us an eﬃcient solution when |Q| ≥ log3 n ≥ log n log3/2
σ n. In
this section we will show that a suﬃx jump can be computed in O(log log n) time when |Q| ≤ log3 n.
Our basic idea is to construct a set X0 of selected substrings with length up to log3 n. We also
create superset X ⊃ X0 that contains all substrings that could be obtained by trimming the ﬁrst
i ≤ 4r + 3 symbols from strings in X0. Using lexicographic naming and special dictionaries on X ,
we pre-compute answers to all suﬃx jump queries for strings from X . When we read the query
string Q and try to match it in S′ we simultaneously try to match Q in the set X ; if some preﬁx
Q[0..l] has no match in X , then we try to ﬁnd a match by trimming leading symbols from Q. That
is, we look for string Q[1..l], Q[2..l], . . . in X until some Q[i..l] ∈ X is found. Thus for every read
preﬁx Q[0..l] of Q we know the smallest index i such that Q[i..l] ∈ X . This information, recorded
in an auxiliary array mlcp, enables us to compute suﬃx jumps “from Q into X0”. A more detailed
description is given below.
Let S1 denote the set obtained by sorting suﬃxes in S′ and selecting every log10 n-th suﬃx. We
denote by X the set of all substrings T [i + f1..i + f2] such that the suﬃx T [i..] is in the set S1 and
0 ≤ f1 ≤ f2 ≤ log3 n. We denote by X0 the set of substrings T [i..i + f ] such that the suﬃx T [i..] is
in the set S1 and 0 ≤ f ≤ log3 n. Thus X0 contains all preﬁxes of length up to log3 n for all suﬃxes
from S1; the set X contains all strings that could be obtained by suﬃx jumps from strings of X .
All substrings in X are assigned unique integer names. We sort all substrings in X ; then we
traverse the sorted list and assign an integer num(S) to each substring S, so that num(S1) =
num(S2) iﬀ S1 = S2. Our goal is to store pre-computed solutions to suﬃx jump queries. To this
end, we keep three dictionary data structures. The dictionary D0 contains the names num(S) for
all S ∈ X0. A dictionary D contains the names num(S) for all substrings S ∈ X . For every entry
x ∈ D, with x = num(S), we store (1) the length ℓ(S) of the string S, (2) the length ℓ(S′) and the
name num(S′) where S′ is the longest preﬁx of S satisfying S′ ∈ X0, (3) the smallest i such that
S = S′[i..l] for some S′ ∈ X0 (i.e., the smallest i such that S is an i-jump of some S′ ∈ X0), and (4)
for all j, 1 ≤ j ≤ 4r + 3 − i, the names num(Sj) where Sj = S[j..ℓ(S) − 1] is the string obtained
by trimming the ﬁrst j leading symbols of S. The dictionary Dp contains all pairs (x, α), where x
is an integer and α is a string, such that the length of α is at most logσ n, x = num(S) for some
S ∈ X , and the concatenation S · α is also in X . Dp can be viewed as a (non-compressed) trie on
X . Using Dp, we can navigate among the strings in X : if we know num(S) for some S ∈ X , we
can look-up the concatenation Sα in X for any string α of length at most logσ n. The dictionary D
enables us to compute suﬃx jumps between strings in X : if we know num(S[0..l]) for some S ∈ X ,
we can look-up num(S[i..l]) in O(1) time.

7

The sets of substrings and dictionaries described above can be constructed in O(m) time as
follows. Let m denote the number of selected suﬃxes in S′. The total number of suﬃxes in
S1 is O(m/ log10 n); the number of substrings associated with each suﬃx in S1 is O(log6 n) and
their total length is O(log9 n). Therefore the total number of strings in X0 is k = O(m/ log7 n)
log10 n · log6 n) = O(m/ log4 n). The number of strings in X is
and their total length is p = O( m
O((m/ log10 n) · log6 n) = O(m/ log4 n) and their total length can be bounded by O((m/ log10 n) ·
log9 n) = O(m/ log n). All strings of X can be generated in O(m) time; we can sort all strings and
compute their lexicographic names in O(m + (m/ log2 n) · log n) = O(m) time because k strings
of total length p can be sorted in O(k log n + p) time [9]. Next, we construct the dictionary D0
that contains names num(S) of all S ∈ X0. For every x = num(S) in D0 we keep a pointer to
the string S. D0 can be constructed in O(|X0|(log log n)2) time [27]. When we generate strings of
X , we also record the information about suﬃx jumps and preﬁxes. Thus we have pointers to all
relevant suﬃx jumps and all preﬁxes for any string S in X . Using pointers to preﬁxes for a string
S and the dictionary D0, we can determine the longest preﬁx S′ of S in O(log log n) time. Now we
have the information stored with elements of D (items (1)-(4)). The dictionary D with k elements
can be constructed in O(k(log log n)2) = o(m) time [27]. Finally we construct the dictionary Dp by
inserting all strings into a trie data structure; for every node of this trie we store the name num(S)
of the corresponding string S. Since the number of branching nodes is O(k), the dictionary Dp is
constructed in O(k(log log n)2 + p) = O(m) time. Hence the total time needed to construct data
structures for suﬃx jumps on short query strings is O(m) = O(n/r). Now we describe how suﬃx
jumps are computed.

We explained that using the dictionary D, we can compute suﬃx jumps within X .

In fact,
using D and some additional information, we can compute approximate suﬃx jumps. That is, for a
string Q[0..l− 1] and an index i, we can ﬁnd LCP (Q[i..l− 1],X0) and its locus node u for any string
Q. Due to our choice of substrings in X0, LCP (Q[i..],S′) is “close” to u. The necessary additional
information is stored in the array mlcp[0..|q| − 1]. Let F (i) = max{h| Q[i..h] occurs in X }. Then
mlcp[i] = (g, xg) such that g ≤ i, F (g) ≥ F (j) for any j ≤ i, and xg = num(Q[g..F (g)]). We will
show below how the suﬃx jump is computed when the required slot of mlcp is available. Then we
will describe the method to compute mlcp while we traverse the path of Q in T ′. Our procedure
will ﬁnd the existing loci of all Q[i..l − 1] for 0 ≤ i ≤ 4r + 3 in time O(r log log n + |Q|).
Lemma 7 Let i, f , l be positive integers such that f < i ≤ 4r + 3 and l ≤ log3 n. Suppose that we
know the location of Q[f..l] in T ′ for some query string Q. If mlcp[i] for some i > f is known, then
we can identify the location of Q[i..l] in T ′ or determine that it does not occur in T ′ in O(log log n)
time. If mlcp[i − 1] is known and F (i − 1) = l, we can also identify the location of Q[i..l] in T ′ or
determine that it does not occur in T ′ in O(log log n) time.
Proof : The suﬃx jump is implemented in two stages: ﬁrst, we ﬁnd the longest preﬁx of Q[i..q − 1]
stored in X0 and identify the corresponding suﬃx S in S1. During the second stage we ﬁnish
the suﬃx jump using Lemma 5. Suppose that mlcp[i] is known. By deﬁnition of mlcp, mlcp[i]
contains the name of a substring Q[j..l1] for some j ≤ i. Using D, we ﬁnd the name of the string
Qi = Q[i..l1]. Since we know that |LCP (Q[i..l],X0)| ≤ l1 − i + 1, LCP (Q[i..],X0) is a preﬁx of
Q[i..l1] and we can ﬁnd it by a look-up in D. If F (i − 1) = l, then Q[i − 1..l] occurs in X . Hence,
Q[i..l] also occurs in X by deﬁnition of X . In this case, again, LCP (Q[i..l],X0) can be found with
a look-up in the dictionary D. Thus we can ﬁnd the longest preﬁx Q[i..l′] ∈ X0 in O(1) time. Let
u1 denote the locus of Q[i..l′] in T ′.
If l′ = l, then we know the location of Q[i..l] in T ′. If l′ < l, let v denote the locus node of
Q[i..l′] in T and let v1 denote the child of v that is labeled with the symbol Q[l′ + 1..l′ + logσ n− 1].

8

The subtree Tv rooted at node v1 contains no suﬃxes from the set S1. Hence the number of leaves
in Tv does not exceed log10 n. We compare Q[i..q − 1] with heavy suﬃxes in Tv using the method
described in Section 5. For k = 1, 2, . . ., we compute ℓk = |LCP (H(vk), Qi)| using Lemma 5. If
ℓk ≥ l − i + 1, we ﬁnd the location corresponding to Qi by answering a weighted ancestor query. If
ℓk < l − i + 1, we check for a node w with string depth ℓk on heavy(vk). If w exists and has a child
vk+1 labeled with Q[ℓk + 1], we continue by computing LCP (H(vk+1), Qi). Otherwise we report
that Q[i..l] does not occur in T ′.
We need to perform O(log log n) look-ups in D0 in order to ﬁnd the longest common preﬁx of
Q[i..l1] in X0. The subtree Tv has O(log10 n) leaves. Hence any path from v1 to its leaf descendant 
intersects O(log log n) heavy paths. We spend O(1) time in every heavy path. A weighted
level ancestor query is answered in O(log log n) time [1]. Hence we can compute a suﬃx jump in
O(log log n) time.
(cid:3)

It remains to describe the procedure for computing the values of mlcp as we traverse the tree
T ′. We use two variables, i and g; i indicates the starting position of the currently processed preﬁx
Q[i..] (thus the suﬃx jumps for j = 1, . . . , i − 1 are already computed and we currently look for
the locus of Q[i..|Q| − 1]) and g is the index of the slot in the array mlcp that we will compute
next. At the beginning both i and g are set to 0. At each step we read the block of d = logσ n
symbols of Q and move down by d symbols in the tree T ′. Suppose that we are about read the
block Q[l..l + d − 1]. If we can match Q[l + 1..l + d] in T , we move down in the tree. Then we
look for the pair (num(Q[g..l]), Q[l + 1..l + d]) in Dp. If this pair is found, then Q[g..l + d] ∈ X
and we read the next block of d symbols in Q. Otherwise Q[g..l] is the longest preﬁx of Q[g..] that
can be matched (ignoring an additive term of up to d). In this case, we set mlcp[g] := l and, if
g < 4r + 3, increment g by 1. Then we look up (num(Q[g..l]), Q[l + 1..l + d]) in Dp for the new
value of g. If this pair is not found, we mlcp[g] := l and increment g again. This process is repeated
until g = 4r + 3 or (num(Q[g..l]), Q[l + 1..l + d]) ∈ Dp for some g. Next, we increment l by g and
proceed with reading the next block. If we cannot match Q[l..l + d] in T ′ or the complete string Q
is read and l = |Q| − 1, then we compute the j-suﬃx jump for j = 1, 2, . . ., 4r + 3 until we ﬁnd
Q[i + j..l] that occurs in T ′. We set i := i + j and g := max(g, i). If l < |Q| − 1, we read the next
block of symbols from Q and proceed as described above.
At every step we know mlcp[k] for all k, 1 ≤ k ≤ g − 1 maintain the following invariant: either
g ≥ i or g = i − 1 and F (i − 1) = l. Hence we can always apply Lemma 7 and each suﬃx jump
is computed in O(log log n) time. Every block of d symbols is processed in O(1) time. Hence the
total time needed to ﬁnd the loci of all Q[i..|Q|] is O(r log log n + |Q|/ logσ n).
Lemma 8 Suppose that |Q| ≤ log3 n. In time O(r log log n + |Q|/ logσ n) we can ﬁnd all existing
loci of Q[i..|Q|], 0 ≤ i < 4r + 3, in T ′.
7 External-Memory Data Structure

In this section we extend our index to the secondary memory scenario. The fact that we do not
sort all the suﬃxes allows us to build the index in time o(Sort(n)), which is impossible for a full
suﬃx array. The main challenge is to handle, within the desired I/Os, pattern lengths that are
larger than log3 n but still not larger than B log3 n; for longer ones the times of the sampled suﬃx
tree search suﬃce.

Theorem 2 If the alphabet size is a constant, then there is an external-memory data structure that

supports pattern matching queries in O(|Q|/(B logσ n) + max(logB n,qlogM/B n log log n) + occ/B)

9

I/Os and can be constructed in O( n
size of internal memory.

B · qlogM/B n) I/Os, where B is the block size and M is the

Our data structure consists of the following components:

(a) We keep the suﬃx array and suﬃx tree T ′ for selected suﬃxes. We set r = qlogM/B n.

Positions of selected suﬃxes in the text are chosen according to Lemma 1, as before. We also
construct an inverse suﬃx array SAI[0..(n/r) − 1] for the selected suﬃxes: SAI[f ] is the rank of
the suﬃx starting at T [⌊f /s⌋s + aj] where j = f mod s.

(b) We keep the data structure, described in Section 6, that supports suﬃx jumps for short
query strings, |Q| ≤ log3 n. This data structure plays an ancillary role only, it helps us obtain the
lower bound on LCP (Q[i..l],S′) as shown in the following lemma. We need this bound to provide
the functionality of (c).

Lemma 9 Suppose that we know the location of Q[f..l] in T ′ for some query string Q and some
integer l. If mlcp[i] for some i > f is known, then we can identify the location of LCP (Q[i..l],S′)
in T ′ or determine that |LCP (Q[i..l],S′)| ≥ log3 n in O(log log n) time.
Proof : If l > log3 n, we can ﬁnd the locus of Q[f.. log3 n − 1] using a weighted level ancestor
query [2] and set l to log3 n. Then we proceed as in Lemma 7.

(cid:3)

(c) We also keep another data structure that supports suﬃx jumps in O(log log n) I/Os per
jump when the size of a query string does not exceed B log3 n. This structure is the novel part of
our construction and is described in Appendix B.

position, i.e.
Appendix A.2.

Our basic approach is the same as in the internal-memory data structure.

The above data structure supports pattern matching for strings that cross at least one selected
|Q| ≥ 4r + 3. The index for very short patterns, |Q| < 4r + 3, is described in
In order to ﬁnd
occurrences of Q, we locate Q in the tree T ′ that contains all sampled suﬃxes. Using an external
memory variant of the suﬃx tree [17], this step can be done in O(|Q|/(B logσ n) + logB N ) I/Os.
Then we execute 4r + 3 suﬃx jumps queries and ﬁnd the loci of all strings Qi = Q[i..|Q|] for
1 ≤ i < 4r + 3. For every Qi that occurs in T ′ we answer a two-dimensional range reporting
query in O(log log n) I/Os. We can execute each suﬃx jump in O(log n) I/Os using the method of
Section 5. This slow method incurs an additional cost of O(r log n). However, the total query cost
is not aﬀected by suﬃx jumps if Q is suﬃciently large, |Q| ≥ B · r · log2 n. Components (b) and
(c) are intended to support suﬃx jumps when |Q| < B log3 n. We will show later how suﬃx jumps
can be computed in O(r log log n) I/Os when the length of Q is smaller than B log3 n.

All parts of our data structure can be constructed in O( Sort(n)

r

) I/Os where Sort(n) = O((n/B) logM/B n)

is the cost of sorting n values. To obtain our result, we set r = qlogM/B n.
8 Construction Algorithms

We start with the construction algorithm for our external-memory data structure. We consider the
set of strings Pij = T [si + aj..s(i + 1) + aj − 1], i.e., all strings of length s that start at selected
positions. Since r = O(qlogM/B n) and the alphabet size σ is a constant, every string ﬁts into one
word. Hence sorting all strings takes O((n/(Br)) logM/B n) = O((n/B)qlogM/B n log σ) I/Os.

10

We assign lexicographic names to every string and construct texts Tk = [tak ...tak+s−1][tak+s...tak+2s−1]...

for k = 0, . . ., 4r + 3. That is, the i-th symbol in Tk is the lexicographic name of the string
T [ak + is..ak + (i + 1)s − 1]. Let T denote the concatenation of texts Tk, T = T1$T2$ . . . T4r+3$.
Since T consists of O(n/r) symbols, we construct a suﬃx array, and a suﬃx tree, and an inverse
suﬃx array SAI for sampled suﬃxes in O(Sort(n/r)) I/Os [20, 15]. We can also construct the
string B-tree on suﬃxes within the same time bounds [16].

Recall that X is the set of strings T [i + f1..i + f2], 1 ≤ f1 ≤ f2 ≤ log3 n, such that T [i..] ∈ S1.
We can generate all strings from X in O(n/(B log n)) I/Os. When strings are generated, we start
with a string T [i..i + log3 n − 1] and produce all preﬁxes T [i..i + f ] of that string; then we create
all strings obtained by suﬃx jumps from each preﬁx T [i..i + f ]. We keep track of preﬁx and
suﬃx jump relationships between strings. After that, all strings are sorted and sorted strings are
assigned lexicographic names. We need to collect information about preﬁxes and suﬃx jumps. Let
L denote the list that contains for every string s = T [is..js] in X: its starting position is, its length
ℓs = js− is + 1, its end position js, and its lexicographic name num(s). We sort L by is· n + ℓs; thus
strings s that start at the same position are grouped together and sorted by their lengths. Then
we traverse L from right to left and record for every string s its longest preﬁx from X0. Next we
sort L by num(s) so that strings with the same lexicographic names num(s) are grouped together.
We traverse the list and identify the longest preﬁx for every unique num(s). Next, we sort L by
js · n + is. In this way strings with the same end position are grouped together and sorted by their
starting positions: every T [i..j] is followed by T [i + 1..j], T [i + 2..j], etc. We traverse L from left
to right and record information about suﬃx jumps for every string. Data structures for the set Y
are constructed in a similar way.
In order to construct data structures of Lemma 14 and Lemma 15 we need to know shifted
ranks of some suﬃxes, i.e. we must know the rank of T [f + h(i, j)..] for every T [f..] in V. This
information can be also obtained by careful sorting and extracting data from the inverse suﬃx
array. We traverse the suﬃx array of sampled suﬃxes and mark every (s log3 n)-th suﬃx. Next
we sort marked suﬃxes by their starting positions in the text. Let Lm denote the list of marked
suﬃxes. We traverse the inverse suﬃx array SAI; for every position SAI[x] that corresponds to a
marked suﬃx T [f..], we add the values of SAI[x + 1], . . ., SAI[x + s] to the entry for T [f..]. Next
we again sort the entries of Lm (with added information) by their ranks. Since we now know the
shifted ranks of marked suﬃxes, we can construct data structures for sets V.
Now we show how the internal-memory data structure can be constructed. First, we obtain
a concatenated text T , deﬁned above. Since T consists of O(n/r) meta-symbols and each metasymbol 
is a string that ﬁts into a (log n)-bit word, we can sort all meta-symbols in O(n/r) time.
Then we can generate T and construct the suﬃx array for T in O(n/r) time. Next, we insert all
suﬃxes into a suﬃx tree. We already described the construction of data structures D and Dp for
the set X once the suﬃx array for the sampled suﬃxes is available. We can also construct the
range reporting data structures using Lemma 3. Thus the total time that is needed to construct
the index is O(n/r) = O(n log σ/ log1/2−ε

n).

σ

9 Conclusion

In this paper we described an index data structure with sublinear pre-processing time. When
the alphabet size is a constant, our index uses O(n log1/2+ε n) bits and can be constructed in
O(n/ log1/2−ε n) time. In the full version of this paper we will show that similar sublinear runtimes
can be also achieved for other string analysis talks, such as the Lempel-Ziv factorization of the text
T .

11

References

[1] A. Amir, G. M. Landau, M. Lewenstein, and D. Sokol. Dynamic text and static pattern

matching. ACM Transactions on Algorithms, 3(2):19, 2007.

[2] Amihood Amir, Gad M. Landau, Moshe Lewenstein, and Dina Sokol. Dynamic text and static

pattern matching. ACM Transactions on Algorithms, 3(2), May 2007.

[3] Maxim Babenko, Pawe lGawrychowski, Tomasz Kociumaka, and Tatiana Starikovskaya.
Wavelet trees meet suﬃx trees.
In Proc. of the 26th Annual ACM-SIAM Symposium on
Discrete Algorithms, SODA ’15, pages 572–591, Philadelphia, PA, USA, 2015. Society for
Industrial and Applied Mathematics.

[4] J. Barbay, F. Claude, T. Gagie, G. Navarro, and Y. Nekrich. Eﬃcient fully-compressed sequence 
representations. Algorithmica, 69(1):232–268, 2014.

[5] D. Belazzougui and G. Navarro. Optimal lower and upper bounds for representing sequences.

ACM Trans. Alg., 11(4):article 31, 2015.

[6] Djamal Belazzougui and Simon J. Puglisi. Range predecessor and lempel-ziv parsing.

In
Proc. of the 27th Annual ACM-SIAM Symposium on Discrete Algorithms, SODA ’16, pages
2053–2071, Philadelphia, PA, USA, 2016. Society for Industrial and Applied Mathematics.

[7] M. A. Bender, M. Farach-Colton, G. Pemmasani, S. Skiena, and P. Sumazin. Lowest common

ancestors in trees and directed acyclic graphs. Journal of Algorithms, 57(2):75–94, 2005.

[8] Michael A. Bender and Martin Farach-Colton. The LCA problem revisited. In Proc. 4th Latin

American Symposiumon Theoretical Informatics (LATIN 2000), pages 88–94, 2000.

[9] Jon L. Bentley and Robert Sedgewick. Fast algorithms for sorting and searching strings. In
Proceedings of the 8th Annual ACM-SIAM Symposium on Discrete Algorithms, SODA ’97,
pages 360–369, Philadelphia, PA, USA, 1997. Society for Industrial and Applied Mathematics.

[10] P. Bille, I. L. Gørtz, and F. R. Skjoldjensen. Deterministic indexing for packed strings. In

Proc. 28th CPM, LIPIcs 78, page article 6, 2017.

[11] Timothy M. Chan, Kasper Green Larsen, and Mihai Patrascu. Orthogonal range searching on
the ram, revisited. In Proc. 27th ACM Symposium on Computational Geometry (SoCG 2011).

[12] Bernard Chazelle. A functional approach to data structures and its use in multidimensional

searching. SIAM J. Comput., 17(3):427–462, 1988.

[13] Y.-F. Chien, W.-K. Hon, R. Shah, S. V. Thankachan, and J. S. Vitter. Geometric BWT:
Compressed text indexing via sparse suﬃxes and range searching. Algorithmica, 71(2):258–
278, 2015.

[14] Charles J. Colbourn and Alan C. H. Ling. Quorums from diﬀerence covers. Inf. Process. Lett.,

75(1-2):9–12, 2000.

[15] Martin Farach-Colton, Paolo Ferragina, and S. Muthukrishnan. On the sorting-complexity of

suﬃx tree construction. J. ACM, 47(6):987–1011, November 2000.

[16] Paolo Ferragina. Personal communication.

12

[17] Paolo Ferragina and Roberto Grossi. The string b-tree: A new data structure for string search

in external memory and its applications. J. ACM, 46(2):236–280, March 1999.

[18] R. Grossi, A. Gupta, and J. S. Vitter. High-order entropy-compressed text indexes. In Proc.
14th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 841–850, 2003.

[19] R. Grossi and J. S. Vitter. Compressed suﬃx arrays and suﬃx trees with applications to text

indexing and string matching. SIAM J. Comp., 35(2):378–407, 2005.

[20] J. K¨arkk¨ainen, P. Sanders, and S. Burkhardt. Linear work suﬃx array construction. Journal

of the ACM, 53(6):918–936, 2006.

[21] Juha K¨arkk¨ainen and Esko Ukkonen. Sparse suﬃx trees. In Proc. 2nd Annual International

ConferenceComputing and Combinatorics (COCOON ’96), pages 219–230, 1996.

[22] U. Manber and G. Myers. Suﬃx arrays: a new method for on-line string searches. SIAM J.

Comp., 22(5):935–948, 1993.

[23] J. I. Munro, G. Navarro, and Y. Nekrich. Fast compressed self-indexes with deterministic

linear-time construction. CoRR, abs/1707.01743, 2017.

[24] J. I. Munro, Y. Nekrich, and J. S. Vitter. Fast construction of wavelet trees. Theoretical

Computer Science, 638:91–97, 2016.

[25] G. Navarro and V. M¨akinen. Compressed full-text indexes. ACM Comp. Surv., 39(1):article

2, 2007.

[26] G. Navarro and Y. Nekrich. Time-optimal top-k document retrieval. SIAM J. Comp., 46(1):89–

113, 2017.

[27] M. Ruzic. Constructing eﬃcient dictionaries in close to sorting time. In Proc. 35th International
Colloquium on Automata, Languages and Programming (ICALP A), LNCS 5125, pages 84–95
(part I), 2008.

[28] P. Weiner. Linear pattern matching algorithms. In Proc. 14th FOCS, pages 1–11, 1973.

[29] B. Wichmann. A note on restricted diﬀerence bases. Journal of the London Mathematical

Society, s1-38(1):465–466, 1963.

13

A Index for Small Patterns

A.1 Main Memory

An internal-memory data structure for small query strings consists of two tables. Let p = 4r + 3
and p1 = 2p. The value of the parameter r is selected so that p1 ≤ (1/2) logσ n. We regard the
text as an array A[0..n/p] of length-p1 strings, A[i] = T [ip..ip + p1 − 1]. Entries of the table T blm
correspond to strings of length p1: T blm[α] contains all positions where α occurs in A. The table
T bls contains all the possible length-j strings, for j = 1, . . ., p. Each entry T bls[β] contains the
list of length-p1 strings α1, . . ., αg such that T blm[αi] is not empty and β is a substring of αi
beginning in the ﬁrst p positions of αi (i.e., β = αi[j..j + |β| − 1] for some 0 ≤ j < p). We can
traverse A and generate T blm in O(n/r) time. Then we visit all slots of T blm; for every α such
that T blm is not empty, we consider all appropriate sub-strings β of α and add α to T bls[β] with
the appropriate oﬀset of β in α (we may add the same α several times with diﬀerent oﬀsets). Since
T blm has σp1 = O(√n) entries and every α has r2 relevant sub-strings, the table T bls is generated
in O(√n · r2) = o(n/r) time.

To report occurrences of a query string q, we examine the list T bls[q]; for every string αi in

T bls[q], we visit the entry T blm[αi] and report all positions of T bl[αi] in A (with their oﬀset).

Lemma 10 There exists a data structure that uses O(n/r) words and reports all occurrences of
a query string Q in O(occ) time if |Q| ≤ 4r + 3. This data structure can be constructed in time
o(n/r).

A.2 External Memory

In this section we describe the index for small patterns, i.e., for strings of length less than 4r + 3.
An occurrence of such a string does not necessarily cross a sampled position in the text. Hence our
main method does not work.

We can obtain A1 from A0 in O( n

We set d = 9r and d1 = 2d. The text T is regarded as an array A0[0..n/d]; every slot of A0
contains a length-d1 string. For every slot A0[i] = α, we record its position i and a value α. That is,
the position of A[i] is its starting position in the original text. First we sort the elements of A0 by
their values. Since A0 consists of n/d words, it can be sorted in O((n/d) logM/B n) I/Os. Then we
construct arrays Aj[] for 1 ≤ j ≤ d−1. Aj is obtained from Aj−1 by removing the ﬁrst symbols from
strings stored in slots of Aj−1 and sorting the resulting strings. Thus each slot of Aj[t] contains a
string of length d−j so that Aj[t] corresponds to some slot A[t′] with the ﬁrst j characters removed.
To be precise, for every Aj[t] = αj . . . αd−1 there are up to σj entries A[t′] = α0 . . . αd−1. Entries of
Aj are also sorted by their values.
Bd ) I/Os. We traverse the array A0 and identify parts of A0
that start with the same symbol. We ﬁnd indices k0, k1, . . ., kσ−1, kσ = n such that all A0 start
with the same character: for all i, kj ≤ i < kj+1, A0[i] = ajα′i for the j-th alphabet symbol aj and
some length-(d1 − 1) string α′i. Next, we merge sub-arrays A0[ki..ki+1 − 1] for i = 0, 1, . . ., σ − 1.
We can merge two-sub-arrays A[f1..l1] and A[f2..l2] in O( l1+l2−f1−f2
+ 1) I/Os and obtain the new
sub-array sorted by α′i. Hence we can obtain the array A1 with values sorted by α′i in O( n
B log σ)
I/Os. Finally we traverse A1 and remove the ﬁrst symbol from every element. We can obtain Aj+1
from Aj in the same way. The cost of constructing Aj+1 is bounded by O((nj/B) log σ) where nj is
the size of Aj. For every array Aj, a dictionary Dj contains all distinct values that occur in Aj. Dj
is implemented as a van Emde Boas data structure and can be constructed in O((mj/B) + (n/Bd))
I/Os, where mj is the number of elements in Dj. We observe that mj is bounded by the number of
distinct strings with length (d − j), mj ≤ (1/σ)j · σd. Since d = O(r) and r ≤ logM/B n, all Aj and
Dj are constructed in O((n/Bd) logM/B n) + d · ((n/Bd) log σ) + (σd)/B) = O((n/B) log σ) I/Os.

B

14

Suppose that we want to report all occurrences of a string Q. Let Qs be the lexicographically
smallest length-d1 string that starts with Q and let Qm be the lexicographically largest length-d1
string that starts with Q. Strings Qs and Qm correspond to integers qs and qm respectively. We
query data structures D0, D1, . . ., Dd−1. For every j 0 ≤ j ≤ d − 1 we ﬁnd succ(qs, Dj ) where
succ(qs, Dj) is the smallest element y ∈ Dj such that y ≥ qs. We traverse the sorted list of elements
in Aj until an element y′ > qm is found. For every element between y and y′, we report its position
in T . We answer r successor queries in O(r log log n) I/Os. Since r = O(qlogM/B n), the query is
answered in O(qlogM/B n log log n + occ/B) I/Os.

Lemma 11 There exists a data structure that uses O(n/B) blocks of space and reports all occurrences 
of a query string Q in O(qlogM/B n log log n + occ/B) I/Os if |Q| ≤ 4r + 3. This data

structure can be constructed in O(n/B) I/Os.

B Suﬃx Jumps for Middle-Length Patterns in External Memory

In this section we show how to compute suﬃx jumps in O(log log n) I/Os when the query string
is not too long, |Q| ≤ B log3 n. Our construction consists of two parts. The ﬁrst part of our data
structure follows the method previously employed for short patterns (see Section 6) with small
modiﬁcations and a diﬀerent choice of parameters.

The set S2 is obtained by selecting every (B3 log10 n)-th suﬃx from S. We deﬁne Y0 and Y in
exactly the same way as in Section 6: for each suﬃx T [i..] in S2, Y0 contains all substrings T [i..i+f ]
for 0 ≤ f ≤ B log3 n and Y contains all substrings T [i + f1..i + f2] for 0 ≤ f1 ≤ f2 ≤ B log3 n. Thus
Y0 contains all preﬁxes of T [i..i + B log3 n] and Y contains all substrings of T [i..i + B log3 n] for
every suﬃx T [i..] in S2. Each substring in Y is assigned a unique name numy. We also construct
dictionaries E, E0, and Ep, deﬁned as the dictionaries D∗ of Section 6. Let mlcpy denote the array
for the query pattern Q, such that mlcpy[i] = (j, y) so that numy(Q[j..l]) = y for some j ≤ i and
l − i + 1 ≥ LCP (Q[i..q],X ). When an array mlcpy is available, we can compute LCP (Q[i..l′],Y0)
in O(1) I/Os.
Lemma 12 Let i, f , l be positive integers such that f < i ≤ 4r + 3 and l ≤ B log3 n. Suppose that
we know the location of Q[f..l] in T ′ for some query string Q. If mlcpy[i] for some i > f is known,
then we can ﬁnd LCP (Q[i..l],Y0) and identify its location in T ′ in O(1) I/Os.
Proof : The proof is the same as used in Lemma 7. If mlcpy[i] is known, then we know the index
j and the name numy(Q[j..l′]) such that f ≤ j ≤ i and |LCP (Q[i..l],Y0)| ≤ l′ − i + 1. Since
numy(Q[j..l′]) and j are known, we can look-up numy(Q[i..l′]) in E and ﬁnd the longest preﬁx of
Q[i..l′] in Y0 using E.
(cid:3)

The second part of our construction is a data structure for searching in a set of B3 logO(1) n
suﬃxes from S′. This structure, described in Lemmas 14 and 15, enables us to ﬁnish the search for
LCP by looking among leaf descendants of LCP (Q[i..l],Y0). First, we show in Lemma 13 how a
suﬃx jump on B3 logO(1) n suﬃxes can be implemented when B is bounded by a polylogarithmic
function. Then we consider the case B ≥ s in Lemmas 14 and 15.
Lemma 13 Suppose that a subtree Tv has O(B3 log10 n) leaves and let V denote the set of suﬃxes
in Tv. If B < s log3 n, then we can ﬁnd LCP (Suf [i..],V) for any i ≤ 4r + 3 in O(log log n) I/Os
with a data structure that uses O(|V|) words of space.

15

Proof : If B < s log3 n, then V contains O(s3 log19 n) = O(log25 n) consecutive suﬃxes. We can ﬁnd
LCP (Suf [i..],V) in O(log log n) I/Os by applying Lemma 5 O(log log n) times as described in the
second part of Lemma 7.

(cid:3)

Lemma 14 Let V denote the set of O(B/(s log3 n)) suﬃxes. Suppose that |LCP (Suf, Suf′)| > s
for some suﬃx Suf and for any suﬃx Suf′ from V. Then for any i, 0 ≤ i < r, we can ﬁnd
LCP (Suf [i..],V) in O(1) I/Os with a data structure that uses O(|V| log2 n) space.
Proof : Suppose that Suf = T [f..]. We can read O(r) consecutive suﬃxes that follow T [f..] in SAI
with O(1) I/Os because B > r.

In order to support LCP queries on V we classify both the suﬃxes in V and the query suﬃxes
according to their positions in blocks of the text T . In fact, we create diﬀerent data structures for
diﬀerent starting positions of the query suﬃx Suf [i..]. For any k, 0 ≤ k ≤ s− 1, we keep a separate
data structure Dk that answers LCP queries for suﬃxes Suf such that Suf [i..] = T [gs + k..]
for some g ≥ 0. The rank rank(Suf ) of a suﬃx Suf is its position in the lexicographic order
of all sampled suﬃxes. For a suﬃx Suf = T [f..] we deﬁne its shifted suﬃx shrank(Suf, k, j) =
rank(T [f + h(k, aj )..]; that is the shifted rank shrank(T [f..], k, j). Data structure Dk contains
shifted ranks of suﬃxes that are needed to ﬁnd LCP (T [f..],V) for f = gs + k. Dk consists of
sub-structures Dk,j. Each Dk,j contains information about suﬃxes from V that start at position
ls + aj for some l ≥ 0: for every suﬃx Suf = T [ls + aj..] we store shrank(Suf, k, j). Shifted ranks
stored in Dk,j can be used to search for LCP among (aj)-suﬃxes, i.e., suﬃxes starting at position
ls + aj for a ﬁxed j. In order to ﬁnd the best candidate for LCP (T [f..],V) among (aj)-suﬃxes, we
look up xj = shrank(T [f..], k, j) and ﬁnd prevj = pred(xj, Dk,j) and nextj = succ(xj, Dk,j). We
ignore up to s leading symbols, but this does not aﬀect the correctness of our method because we
know that |LCP (Suf, Suf′)| > s for all suﬃxes Suf′ in V. However we cannot compare suﬃxes by
shifted ranks stored in diﬀerent Dk,j. If two suﬃxes Suf1 and Suf2 are stored in Dk,j1 and Dk,j2
respectively, then shrank(Suf1, k, j1) and shrank(Suf2, k, j2) are ranks of suﬃxes that are shifted
by h(k, aj1) and h(k, aj2) symbols respectively. Therefore we cannot compare prevj1 with prevj2
and determine the best candidate. Besides, suﬃxes with ranks prevj1 and prevj2 can be stored in
diﬀerent parts of the suﬃx array.

We use marginal values in order to ﬁnd the best candidate with an additive error at most
B. We keep O(log3 n) marginal values for every shifted suﬃx in Dk,j. For i = 0, 1, . . . , 2 log3 n −
1, the marginal value pmargin(Suf, i) is the rank of the leftmost suﬃx Sufi < Suf such that
(B/2)(i + 1) > |LCP (Sufi, Suf )| ≥ (B/2) · i. For i = 2 log3 n, pmargin(Suf, i) is the rank of the
leftmost suﬃx Sufi < Suf such that |LCP (Sufi, Suf )| ≥ B log3 n. For i = 0, 1, . . . , 2 log3 n − 1,
the marginal value nmargin(Suf, i) is the rank of the rightmost suﬃx Suf′i > Suf such that
(B/2)(i + 1) > |LCP (Suf′i, Suf )| ≥ (B/2) · i. For i = 2 log3 n, nmargin(Suf, i) is the rank of
the rightmost suﬃx Suf′i > Suf such that |LCP (Suf′i, Suf )| ≥ B log3 n. For every suﬃx Suf of
rank v, such that v is stored in Dk,j, we keep at most 4 log3 n marginal values pmargin(Suf, i) and
nmargin(Suf, i). To complete the search, we will also need a string B-tree TV . TV is a subtree of
the suﬃx tree induced by suﬃxes in V.
Consider a suﬃx T [f..] of rank p and a suﬃx T [g..] with the shifted rank v ∈ Dk,j. Suppose
that shrank(T [f..], k, j) ≤ shrank(T [g..], k, j) and pmargin(i) ≤ shrank(T [f..], k, j) < pmargin(t)
for some i ≤ t. Then ℓ = LCP (T [f + h(k, aj ), g + h(k, aj )] satisﬁes (B/2)i ≤ ℓ < (B/2)(i +
1). Since h(k, aj ) ≤ s and the ﬁrst s symbols in T [f..] and T [g..] are equal, we have (B/2)i ≤
|LCP (T [f..], T [g..])| ≤ (B/2)(i + 1) + s ≤ (B/2)i + B. Hence marginal values and data structures
Dk,j provide an estimate for the LCP of any suﬃx and a set of (aj)-suﬃxes. Since all auxiliary
data structures ﬁt into one block, the query takes only O(1) I/Os.

16

Now we summarize our method and describe the complete procedure to answer an LCP query.
Let Suf = T [f..] denote some selected suﬃx. Since we assume s < B, we can read SAI[pos(f )],
SAI[pos(f )+1], . . ., SAI[pos(f )+s] into main memory in O(1) I/Os. We denote here by pos(f ), for
f = ls + at, the position corresponding to f in the array SAI. We also read the data structure for V
into main memory. Let vj = SAI[x] for x = pos(f + i + h(k, j)) and k = at + i. All relevant slots of
SAI were read into main memory. We can ﬁnd prevj = pred(vj, Dk,j) and nextj = succ(vj, Dk,j)
for all j. Then we estimate LCP (Suf [i..], prevj) and LCP (Suf [i..], nextj) as described above.
Let Suf1 denote the suﬃx that provides the longest estimated LCP. The suﬃx Suf1 with rank r1
provides an approximation for LCP (Suf,V) with an additive error of at most B. Let u1 denote
the locus of LCP (Suf,V) in the string B-tree TV . The locus node u1 can be found in O(log log n)
I/Os using a weighted level ancestor query [2]. We can move down by at most B symbols from u1
and identify LCP (Suf,V) in O(1) I/Os using the standard search procedure in a string B-tree. (cid:3)
Lemma 15 Let V denote a set of O(B3 log10 n) consecutive suﬃxes in T ′. Suppose that, for some
suﬃx Suf , some i ≤ 4r + 3, and for any suﬃx Suf′ from V, it holds |LCP (Suf [i..], Suf′)| > s.
Then we can ﬁnd LCP (Suf [i..],V) in O(log log n) I/Os with a structure that uses O(|V|) space.
Proof : We assign all suﬃxes of V to nodes of a conceptual tree T A. Every leaf of T A stores
s3 log9 n suﬃxes and every internal node has B/(s log3 n) children. We associate a set A(ν) with
every node ν of T A; A(ν) contains two representative suﬃxes, i.e., the smallest and the largest
suﬃx, from every child of ν.
In order to ﬁnd LCP (Suf,V) we start at the root node of T and move down to the leaf. In every
internal node ν, we can ﬁnd the child that contains LCP (Suf,V) in O(1) I/Os using Lemma 14.
When we reach a leaf node, we can ﬁnish the search in O(log log n) I/Os. Let v denote the lowest
common ancestor of all suﬃxes from ℓi in the suﬃx tree T ′ and let v1 denote the child of v that
contains the LCP. The subtree Tv rooted at v1 has at most O(log16 n) leaves. Every path from v1 to
a leaf node intersects O(log log n) heavy paths. Since we know that the locus of LCP (Suf [i..],V)
is in Tv, we can ﬁnd it in O(log log n) I/Os by applying Lemma 5 O(log log n) times.

(cid:3)

Now we are ready to describe suﬃx jumps on middle-length patterns.

Lemma 16 Suppose that we know the location of Q[f..l] in T ′ for some query string Q and some
integer l ≤ log3 n. If mlcpy[i] for some i > f is known, then we can identify the location of Q[i..l]
in T ′ or determine that it does not occur in T ′ in O(log log n) time.
Proof : We can compute LCP (Q[i..l],Y0), its length l1, and its locus u0 in O(1) I/Os as described
in Lemma 7. Let u denote the child of u0 that is labeled with Q[l1..l1 + t − 1]. The subtree Tu
rooted at node u contains O(B3 log10 n) suﬃxes. If the string depth of u is larger than s, we can
ﬁnd LCP (Q[i..l],Tu) in O(log log n) I/Os using Lemma 15. If the string depth of u is smaller than
or equal to s, we use Lemma 9 to ﬁnd the locus v of LCP (Q[i.. log2 n],S′) in T ′. Suppose that the
locus node v is found. Since v is below u, the number of suﬃxes in the subtree Tv does not exceed
O(B log4 n). Again we can ﬁnd LCP (Q[i..l],Tu) in O(log log n) I/Os using Lemma 15.

(cid:3)

17

