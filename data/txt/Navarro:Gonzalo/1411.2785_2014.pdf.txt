4
1
0
2

 

v
o
N
1
1

 

 
 
]
S
D
.
s
c
[
 
 

1
v
5
8
7
2

.

1
1
4
1
:
v
i
X
r
a

Faster Compressed Quadtrees

Travis Gagie1, Javier I. Gonz´alez-Nova2,

Susana Ladra3, Gonzalo Navarro4 and Diego Seco2

1 Helsinki Institute for Information Technology (HIIT) and

Department of Computer Science, University of Helsinki, Finland

2 Department of Computer Science, University of Concepci´on, Chile

3 Database Laboratory, University of A Coru˜na, Spain

4 Department of Computer Science, University of Chile, Chile

Abstract

Real-world point sets tend to be clustered, so using a machine word for
each point is wasteful. In this paper we ﬁrst bound the number of nodes in the
quadtree for a point set in terms of the points’ clustering. We then describe a
quadtree data structure that uses O(1) bits per node and supports faster queries
than previous structures with this property. Finally, we present experimental
evidence that our structure is practical.

1

Introduction

Storing and querying two-dimensional points sets is fundamental in computational
geometry, geographic information systems, graphics, and many other ﬁelds. Most
researchers have aimed at designing data structures whose size, measured in machine
words, is linear in the number of points. That is, data structures are considered
small if they store a set of n points on a u × u grid in O(n) words of O(log u) bits
each. Using O(n log u) bits is within a constant factor of optimality when the points
are distributed sparsely and randomly over the grid, but we can often do better on
real-word point sets because they tend to be clustered and, therefore, compressible.
Quadtrees [12] tend to have o(n log u) nodes when the points are clustered, but
pointer-based quadtree data structures can still take Ω(n log u) bits. One way to
avoid storing pointers is to store the points’ coordinates instead [9], but that also
takes Ω(n log u) bits. Hudson [11] gave a structure that uses O(n) bits when the
points are spaced appropriately and we are willing to tolerate some distortion of
the points’ positions. Recently, de Bernardo et al. [7] and Venkat and Mount [16]
independently proposed similar structures based on static and dynamic succinct tree
representations, respectively (see, e.g., [1, 6, 13]). Both structures use O(1) bits
per node in the quadtree and have the same asymptotic query times as traditional
structures, which support only edge-by-edge navigation. Venkat and Mount noted,
however, that

1

Figure 1: A set of points, indicated by 1s, on a 16 × 16 grid (left); the quadtree for
those points (right). The heavy lines in the quadtree indicate the path to the leaf
corresponding to the shaded point on the grid.

“A method for compressing paths or moving over multiple edges at
once using a succinct structure may speed up the many algorithms that
rely on traversal of the quadtree.”

In Section 2 we review the ideas behind quadtrees and prove a simple upper bound
on the number of nodes in terms of the points’ clustering. In Section 3 we describe a
quadtree data structure that uses O(1) bits per node in the quadtree and allows us to
move over multiple edges at once. In Section 4 we show how this lets us perform faster
membership queries. Finally, in Section 5 we present experimental evidence that our
structure is practical. We leave as future work making our structure dynamic.

2 Space Bound
Let P be a set of n points on a u × u grid. If n is 0 or u2 points then the quadtree
for the grid is only a root storing either 0 or 1, respectively; otherwise, the root
stores 1 and has four children, which are the quadtrees of the grid’s four quadrants.
Figure 1 shows an example, taken from [7]. Notice the order of the quadrants is
top-left, top-right, bottom-left, bottom-right, instead of the counterclockwise order
customary in mathematics. This is called the Morton or Z-ordering and it is useful
because, assuming u is a power of 2 and the origin is at the top right — without
loss of generality, since we can manipulate the coordinate system to make it so —
the obvious binary encoding of a root-to-leaf path is the interleaving of the binary
representations of the corresponding point’s yand 
x-coordinates.

For example, if we imagine the edges descending from each internal node in Figure 
1 are labelled 0, 1, 2, 3 from left to right, then the thick edges are labelled 2, 1, 1, 2;
the obvious binary encoding for this path is 10 01 01 10. The coordinates for the
shaded point, which corresponds to the leaf at the end of this path, are (6, 9), so

2

interleaving the binary representations 1001 and 0110 of its yand 
x-coordinates also
gives 10 01 01 10 . We can interleave a point’s coordinates in O(1) time on a RAM [8];
this operation is also fast in practice on real machines, e.g., using pre-computed tables.
The quadtree has height at most lg u and O(n log u) nodes, and a subtree rooted
at depth d encodes the points on a 2lg u−d × 2lg u−d square. Given a query rectangle R,
we can ﬁnd all the points in P ∩ R by starting at the root and visiting all the nodes
whose subtrees’ squares overlap R, recording the leaves storing 1s. This is called
range reporting or, in the special case R = [x, x + 1) × [y, y + 1), a membership test
for (x, y). If we report k points then we visit O(k log u) nodes. If the points in P are
clustered, however, then intuitively the root-to-leaf paths in the quadtree will share
many nodes and we will use less space and time.

Theorem 1. Suppose we can partition P into c clusters, not necessarily disjoint, with

n1, . . . , nc points and diameters (cid:96)1, . . . , (cid:96)c. Then the quadtree has O(c log u +(cid:80)

i ni log (cid:96)i)

nodes.
Proof. Let S be an (cid:96) × (cid:96) square on the grid, let C = S ∩ P , and let A be the set of
ancestors in the quadtree of the points in C. (For simplicity, we identify points in P
with their corresponding leaves in the quadtree.) Let A(cid:48) be the ancestors of only the
corners of S (which may or may not be in P ). Notice

|A| ≤ |A ∪ A(cid:48)| ≤ |A\A(cid:48)| + |A(cid:48)| < |A\A(cid:48)| + 4 lg u .

For any ancestor v of a point in C that has depth at most lg(u/(cid:96)), v’s subtree
contains all the points in a square of size at least 2(cid:96) × 2(cid:96). Therefore, the square must
contain at least one corner of S, so v ∈ A(cid:48). It follows that

|A\A(cid:48)| ≤ |C|(lg u − lg(u/(cid:96))) = |C| lg (cid:96) ,

so |A| < |C| lg (cid:96) + 4 lg u.

The proof above is something like a two-dimensional analogue of Gupta, Hon,
In the full version of this paper we will

Shah and Vitter’s [10] analysis of tries.
consider higher dimensions and give bounds with respect to hierarchical clustering.

3 Structure

The structure by de Bernardo et al. [7] mentioned in Section 1 is a variation of
Brisaboa, Ladra and Navarro’s [5] k2-tree structure. Brisaboa, Ladra and Navarro
designed k2-trees to compress the Web graph, and de Bernardo et al. adapted it to
other domains, such as geographic data. The main diﬀerence is that if P contains
all the 2lg u−d × 2lg u−d points encoded by the subtree of a node at depth d, then in
de Bernardo et al.’s structure that subtree is only the node itself, conforming to the
deﬁnition of a quadtree; in Brisaboa, Ladra and Navarro’s structure, that subtree has
height lg u − d and 22(lg u−d) leaves. Thus, the original k2-tree can have more nodes
than the quadtree.

3

Figure 2: The heavy-path decomposition of the binary tree for the example from Figure 
1. Nodes storing 1s are black; nodes storing 0s are shown hollow, and discarded;
thick edges belong to heavy paths. The numbers below the black leaves indicate our
ordering of the paths.

For many applications it is rare that point sets contain large squares that are
completely ﬁlled. Therefore, in this version of this paper we make the simplifying
assumption that each internal node of the quadtree has at least one descendant storing
a 0, so both versions of the k2-tree have the same number of nodes as the quadtree.
We will remove this assumption in the full version.

To store a quadtree, we ﬁrst replace each internal node by a binary tree of height
2 and remove any node that has no descendant storing a 1; this increases the size
of the whole tree by a factor of at most 7/5. Let T be the resulting binary tree.
In addition to simplifying our construction, this modiﬁcation makes quadtrees more
practical in higher dimensions (see [2]), which we will also consider in the full version
of this paper.

We then perform a heavy-path decomposition [15] of T . That is, we partition T
into root-to-leaf paths, called heavy paths, such that the path containing a node v
also contains the child of v with the most leaf descendants (breaking ties arbitrarily).
One well-known property of this decomposition is that each root-to-leaf path in T
consists of O(log n) initial segments of heavy paths. Figure 2 shows the heavy-path
decomposition of the binary tree for our example from Figure 1.

We encode each heavy path h as a binary string whose 0s and 1s indicate which of
h’s nodes are left children and which are right children (considering the root as a left
child, say, for simplicity) in increasing order by their depths. We sort the encodings
into decreasing order by length, breaking ties such that if two paths h and h(cid:48) have
the same length and their topmost nodes are v and v(cid:48), then the encodings of h and
h(cid:48) appear in the same order as the encodings of the paths containing the parents of
v and v(cid:48). (Notice v and v(cid:48) cannot have the same parent, since they have the same
height and the tree is binary.) The numbers below the leaves in Figure 2 indicate
how we order the paths in our example. We store the concatenation H of the paths’
encodings, which consists of |T| bits. We say the bit H[i] corresponds to the node v
if H[i] indicates whether v is a left child or a right child.

For each depth d < lg u (considering the root to have depth 0 and leaves to have
depth lg u), we store a bitvector Ld with 1s indicating which nodes at that depth in T
have two children; see, e.g., [14] for a discussion of bitvectors. These bitvectors have

4

1234567891011121314as many bits as there are internal nodes in T . For our example,

H = 000000110 10010100 1100010 110111 001001 10010 1010 1000 1110 1110 010 10 1 1 ,
L0 = 1-------- ,
L1 = -1------- 0------- ,
L2 = --1------ -0------ 1------ ,
L3 = ---1----- --0----- -0----- 0----- 0----- ,
L4 = ----1---- ---0---- --1---- -1---- -0---- 1---- ,
L5 = -----0--- ----1--- ---0--- --0--- --0--- -0--- 0--- 0--- 0--- 0--- ,
L6 = ------0-- -----1-- ----0-- ---0-- ---0-- --0-- -0-- -0-- -0-- -0-- 0-- ,
L7 = -------1- ------0- -----0- ----0- ----0- ---0- --1- --0- --0- --0- -0- 0- ;
dashes and spaces are only for legibility. Storing H and all the Lds takes O(1) bits
per node in T and, therefore, also O(1) bits per node in the original quadtree. For
each length, we also store the starting position in H of the ﬁrst encoding with that
length; this also takes a total of O(T ) bits. In the full version of this paper we will
give more details and discuss ways in which we can slightly reduce our space usage.
Suppose H[i] corresponds to node v in T . Given i, in O(1) time we can determine
the length of the heavy path containing v, that path’s rank in our ordering, v’s depth
in T , and whether v is the top node in its path.
If v is the top node in the jth
path starting at depth d then, by our choice of ordering, v’s parent u is the jth
node at depth d − 1 that Ld−1 indicates has two children.
It follows that we can
compute in O(1) time which bit of H corresponds to u, via a select query on Ld−1
and some arithmetic. (Since select queries are quite slow in practice, however, even
in applications involving ascents it may be better for us simply to backtrack.) For
example, if v is the top node in the ninth path in our ordering, which is the third
path starting at depth 5, then u is the third node at depth 4 which L4 indicates has
two children. Since the third 1 in L4 is its fourth bit, u is the node at depth 4 in T
in the fourth path in our ordering.

Once we know v’s depth d in T and its path’s ranking in our ordering, we know
immediately whether v is a leaf and we can check Ld to see whether v has two children.
If v is the jth node at depth d which has two children then the child of v that is not in
the same path as v, is the top node in the jth path starting at depth d + 1. It follows
that we can compute in O(1) time which bit of H corresponds to w, via a rank query
on Ld and some arithmetic; the other child of v corresponds to H[i + 1], and that bit
tells us which child is which. Reversing the example in the previous paragraph, if v
is the node at depth 4 in T in the fourth path in our ordering, then it is the third
node at depth 4 which L4 indicates has two children. Therefore, one of v’s children
is the node at depth 5 in the same path, while v’s other child is the top node in the
third path starting at depth 5, which is the ninth path in our ordering.

5

4 Membership

Suppose we want to perform a membership query for (x, y). We compute the label
on the path to (x, y) in O(1) time, as described in Section 1. We set v to be the root
of T , then repeat the following steps until we reach (x, y) or can descend no further:
we ﬁnd the longest common preﬁx of the remainder of the path label for (x, y) and
the encoding of the heavy path starting at v (except that we ignore the ﬁrst bit of
H, which is 0 and corresponds to the root), which takes O(1) time because the path
label and the encoding are O(log u) bits; we descend the initial segment of the heavy
path encoded by that common preﬁx; if we reach (x, y), then we report (x, y) ∈ P ;
if the node we are currently visiting has only one child, then we report (x, y) (cid:54)∈ P ;
otherwise, we set v to be the child of the node we are currently visiting that is not
in the same heavy path, and continue. In total, the query takes time proportional
to the number of initial segments we traverse. Since we only descend, we never need
select queries.

To perform a membership query for (6, 9) in our example, we compute the path
label 10010110 and set v to be the root; we ﬁnd that this label does not share any
non-empty preﬁx with the encoding of the heavy path starting at v (ignoring the
leading 0 because v is the root, although in this case it makes no diﬀerence); and
we set v to be the root’s right child. We then ﬁnd that the remainder of the path
label (which is all of it) shares a preﬁx of length 6, 100101, with the encoding of the
heavy path starting at v; we descend to the 6th node on that path; we set v to be
that node’s right child. Finally, we ﬁnd the remainder of the path label, 10, shares
a preﬁx of length 2, all of 10, with the encoding of the heavy path starting at v; we
descend to the 2nd node on that path; and we report (6, 9) ∈ P .
Since each root-to-leaf path in T consists of O(log n) initial segments of heavy
paths, our data structure obviously supports membership queries in O(log n) time. If
the query point is isolated, we use even less time.
Theorem 2. We can store P using O(1) bits per node in the quadtree such that a
membership query for (x, y) takes O(ming{log(u/g) + log kg}) ⊆ O(log n) time, where
kg is the number of points in P within distance g of (x, y).

Proof. Any node v at depth at least 2 lg(u/g)+2 in T whose subtree’s square contains
(x, y), has at most kg leaf descendants. It follows that the path from v to the deepest 
node w of T whose subtree’s square contains (x, y), consists of O(log kg) initial
segments of heavy paths. To see why, consider that if we ascend from w to v, every
time we move from the top-most node in one heavy path to its parent in another
heavy path, the number of leaf descendants in the subtree below us at least doubles.
Since the path from the root to v has length O(log(u/g)), the path from the root to
w consists of O(log(u/g) + log kg) initial segments of heavy paths.

We note that combining Theorems 1 and 2 suggests our structure should be particularly 
suited to applications in which, e.g., points are highly clustered but queries
are chosen according to a very diﬀerent distribution.

6

store P in O(c log u +(cid:80)
Corollary 3. Suppose we can partition a set P of n points into c clusters, not
necessarily disjoint, with n1, . . . , nc points and diameters (cid:96)1, . . . , (cid:96)c. Then we can
i ni log (cid:96)i) bits such that a membership query for (x, y) takes
O(ming{log(u/g) + log kg}) ⊆ O(log n) time, where kg is the number of points in P
within distance g of (x, y).

5 Experiments

We have implemented the data structure from Section 3 and compared it experimentally 
with the k2-tree. Due to space constraints, we present the results only for
membership queries; in the full paper we will present results for range reporting. All
the experiments presented here were performed in an Intel Core i7-3820@3.60GHz,
32GB RAM, running Ubuntu server (kernel 3.13.0-35). We compiled with gnu/g++
version 4.6.3 using -O3 directive.

In our implementation we make use of the bitvector implementations available
in LibCDS (https://github.com/fclaude/libcds). Speciﬁcally, we use three types
of bitvectors. Our ﬁrst variant, a heavy-path implementation with plain bitvectors,
 we call HPp; for our second variant, named HPc, we use compressed bitmaps.
LibCDS provides several implementations of compressed bitmaps, and we use either
the Raman, Raman and Rao (RRR) implementation or Sadakane’s SDArray (for each
dataset, we select the one achieving better compression).

We compare these two variants with two conﬁgurations of k2-trees. The ﬁrst variant,
 named k2-treeb, consists of a basic version of the k2-tree where the degree k = 2
for all the levels of the tree. The second variant, named k2-treeh, is the conﬁguration
considered as optimal for the k2-tree [5], which includes a hybrid approach with different 
k values for the levels of the tree and a vocabulary of leaf submatrices to obtain
better compression. We did not compare to Venkat and Mount’s structure because
they have not yet made an implementation available. We also did not compare to the
classical quadtree representations since the k2-tree is an order of magnitude smaller
and has better access time [7].

For our experimental evaluation we use grid datasets from diﬀerent domains: geographic 
information systems (GIS), social networks (SN), web graphs (WEB) and
RDF datasets (RDF). For GIS data we use the Geonames dataset, which contains
more than 9 million populated places, and convert it into three grids with diﬀerent
resolutions: Geo-sparse, Geo-med, and Geo-dense. (The higher the resolution, the
sparser the matrix.) For SN and WEB we consider the grid associated with the adjacency 
matrix of two Web graphs (indochina-2004, uk-2002) and two social networks
(dblp-2011, enwiki-2013) obtained from the Laboratory for Web Algorithmics1 [4, 3].
Finally, we use RDF data obtained from the dbpedia dataset2. This RDF dataset
contains triples (S,P,O) indicating subjects that are related to objects with a speciﬁc 
predicate. Thus, each predicate deﬁnes a binary relation among subjects and
objects that can be represented in a grid with points. We create three diﬀerent grids

1http://law.dsi.unimi.it
2http://wiki.dbpedia.org/Downloads351

7

Table 1: Description of the datasets and space comparison
Space (bpp)
k2-treeh

Points (n)
9,188,290
9,328,003
9,335,371
6,707,236
101,355,853
194,109,311
298,113,762
98,714,022
7,936,138
138,303

k2-treeb
16.68
30.27
44.19
10.76
16.96
2.57
3.30
9.80
31.61
45.69

Type
GIS
GIS
GIS
SN
SN

File
Geo-dense
Geo-med
Geo-sparse
dblp-2011
enwiki-2013
indochina-2004 WEB
uk-2002
triples-dense
triples-med
triples-sparse

Grid (u)
524,288
4,194,304
67,108,864
986,324
4,206,785
7,414,866
WEB 18,520,486
RDF
66,973,084
66,973,084
RDF
RDF
66,973,084

HPp
13.27 18.50
24.97
39.67
9.84 12.62
14.66 18.56
1.22
4.29
2.04
5.04
6.93 12.19
26.95
46.98

HPc
15.34
31.77 21.84
45.36 28.55
10.69
15.33
4.09
4.94
10.40
32.94 23.26
45.96 29.97

for our experiments, selecting predicates with diﬀerent numbers of related objects
(triples-sparse, triples-med, and triples-dense).

Table 1 gives the main characteristics of the datasets used: name of the dataset,
size of the grid (u), number of points it contains (n) and the space achieved by the four
representations compared: k2-treeb, k2-treeh, HPp, and HPc. The space is measured
in bits per points (bpp), dividing the total space of the structure by the number
of points (n) in the grid. We can observe that HPP obtains the worst compression
among all the alternatives, but HPc obtains better results than k2-treeh for some of
the datasets, which is remarkable as this conﬁguration of the k2-tree exploits several
compression techniques that may be considered for future extensions of this proposal
and may allow the HPc variant to reduce its space. HPc clearly outperforms k2-treeh
for very sparse grids.

We now analyze the time performance of our proposed structure. We distinguish
three diﬀerent types of membership queries: empty cells, ﬁlled cells and isolated ﬁlled
cells (top 100,000 most isolated ﬁlled cells), and measure average times per query in
nanoseconds. We show in Figure 3 the results obtained by the four representations
over two kinds of grid datasets, GIS and SN. (Due to space constraints we omit results
for the WEB and RDF datasets.) We can observe that for both scenarios we obtain
similar performance. The k2-tree representation obtains better results when querying
empty cells, as the computation for reaching a zero node in the k2-tree is lighter than
using heavy paths. However, HP becomes the best alternative when querying ﬁlled
cells: our non-compressed data structure is always the fastest one for cells with values,
and much faster for isolated points. In this latter case, even the compressed variant
of our structure outperforms the most optimized k2-tree both in time and space.

6 Conclusions

We have presented a fast space-eﬃcient representation of quadtrees, answering in
the aﬃrmative to the conjecture of Venkat and Mount [16]. Our structure has nice
theoretical bounds and it is practical. Space requirements are similar to other space8


Figure 3: Space and time usages of our HP methods and the k2-tree variants, for the GIS
(left) and SN (right) datasets and each type of membership query: empty cells (top), ﬁlled
cells (center) and isolated ﬁlled cells (bottom). Curves for the GIS datasets show results
for Geo-sparse, Geo-med, and Geo-dense; curves for the SN datasets show results for dblp2011 
and enwiki-2013. For example, consider the graph for the GIS dataset and queries on
isolated ﬁlled cells (bottom left): the middle point on the curve for HPc is to the left of
and below the bend in the curve for k2-treeh; this means that, for the Geo-med dataset, the
HPc structure uses less space than the k2-treeh structure and also answers faster. (Notice
the query type does not aﬀect space usages.)

9

 0 20 40 60 80 100 120 140 160 180 0 5 10 15 20 25 30 35 40 45 50time (nanosec/query)space (bits/point)GIS - membership queries for empty cellsk2-treebk2-treehHPpHPc 0 100 200 300 400 500 600 700 800 900 0 2 4 6 8 10 12 14 16 18 20time (nanosec/query)space (bits/point)SN - membership queries for empty cellsk2-treebk2-treehHPpHPc 0 100 200 300 400 500 600 700 800 900 1000 0 5 10 15 20 25 30 35 40 45 50time (nanosec/query)space (bits/point)GIS - membership queries for filled cellsk2-treebk2-treehHPpHPc 0 200 400 600 800 1000 1200 0 2 4 6 8 10 12 14 16 18 20time (nanosec/query)space (bits/point)SN - membership queries for filled cellsk2-treebk2-treehHPpHPc 0 100 200 300 400 500 600 0 5 10 15 20 25 30 35 40 45 50time (nanosec/query)space (bits/point)GIS - membership queries for isolated filled cellsk2-treebk2-treehHPpHPc 0 100 200 300 400 500 600 0 2 4 6 8 10 12 14 16 18 20time (nanosec/query)space (bits/point)SN - membership queries for isolated filled cellsk2-treebk2-treehHPpHPceﬃcient representations of quadtrees, e.g., the k2-trees, but our structure is faster
handling isolated ﬁlled cells. In the full version of this paper we will generalize our
structure to higher dimensions, give bounds in terms of hierarchical clustering and
present experimental results for range reporting.

Acknowledgments

Many thanks to Timothy Chan and Yakov Nekrich for directing us toward Venkat and
Mount’s paper. The ﬁrst author is also grateful to the late Ken Sevcik for introducing
him to some concepts used in this paper.

References

[1] D. Arroyuelo, R. C´anovas, G. Navarro, and K. Sadakane. Succinct trees in

practice. In Proc. ALENEX, pages 84–97, 2010.

[2] N. Bereczky, A. Duch, K. N´emeth, and S. Roura. Quad-K-d trees.

In Proc.

LATIN, pages 743–754, 2014.

[3] P. Boldi, M. Rosa, M. Santini, and S. Vigna. Layered label propagation: A
In

multiresolution coordinate-free ordering for compressing social networks.
Proc. WWW, pages 587–596, 2011.

[4] P. Boldi and S. Vigna. The WebGraph framework I: Compression techniques.

In Proc. WWW, pages 595–601, 2004.

[5] N. R. Brisaboa, S. Ladra, and G. Navarro. Compact representation of web

graphs with extended functionality. Info. Sys., 39:152–174, 2014.

[6] P. Davoodi and S. S. Rao. Succinct dynamic cardinal trees with constant time

operations for small alphabet. In Proc. TAMC, pages 195–205, 2011.

[7] G. de Bernardo, S. ´Alvarez-Garc´ıa, N. R. Brisaboa, G. Navarro, and O. Pedreira.
In Proc. SPIRE, pages

Compact querieable representations of raster data.
96–108, 2013.

[8] F. E. Fich. Constant time operations for words of length w. 1999.
[9] I. Gargantini. An eﬀective way to represent quadtrees. CACM, 25:905–910, 1982.
[10] A. Gupta, W.-K. Hon, R. Shah, and J. S. Vitter. Compressed data structures:

Dictionaries and data-aware measures. TCS, 387:313–331, 2007.

[11] B. Hudson. Succinct representation of well-spaced point clouds. Technical

Report abs/0909.3137, CoRR, 2009.

[12] G. M. Morton. A computer oriented geodetic data base; and a new technique

in ﬁle sequencing. Technical report, IBM Ltd., 1966.

[13] G. Navarro and K. Sadakane. Fully functional static and dynamic succinct

trees. TALG, 10:16, 2014.

[14] M. Pˇatra¸scu. Succincter. In Proc. FOCS, pages 305–313, 2008.
[15] D. D. Sleator and R. E. Tarjan. A data structure for dynamic trees. JCSS,

26:362–391, 1983.

[16] P. Venkat and D. M. Mount. A succinct, dynamic data structure for proximity

queries on point sets. In Proc. CCCG, 2014. To appear.

10

