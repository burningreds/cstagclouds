Directly Addressable Variable-Length Codes(cid:2)

Nieves R. Brisaboa1, Susana Ladra1, and Gonzalo Navarro2

1 Universidade da Coru˜na, Spain

{brisaboa,sladra}@udc.es

2 Dept. of Computer Science, Univ. of Chile

gnavarro@dcc.uchile.cl

Abstract. We introduce a symbol reordering technique that implicitly
synchronizes variable-length codes, such that it is possible to directly
access the i-th codeword without need of any sampling method. The
technique is practical and has many applications to the representation of
ordered sets, sparse bitmaps, partial sums, and compressed data structures 
for suﬃx trees, arrays, and inverted indexes, to name just a few. We
show experimentally that the technique oﬀers a competitive alternative
to other data structures that handle this problem.

1 Introduction

Variable-length coding is at the heart of Data Compression [23,21]. It is used,
for example, by statistical compression methods, which assign shorter codewords
to more frequent symbols. It also arises when representing integers from an
unbounded universe: Well-known codes like γ-codes and δ-codes are used when
smaller integers are to be represented using fewer bits.

A problem that frequently arises when variable-length codes are used is that
it is not possible to access directly the i-th encoded element, because its position 
in the encoded sequence depends on the sum of the lengths of the previous
codewords. This is not an issue if the data is to be decoded from the beginning,
as in many compression methods. Yet, the issue arises recurrently in the ﬁeld
of compressed data structures, where the compressed data should be accessible
and manipulable in compressed form. A partial list of structures where the need
to directly access variable-length codes arises includes Huﬀman and other similar 
encodings of text collections [14,15,1], compression of inverted lists [23,4],
compression of suﬃx trees and arrays (for example the Ψ function [20] and the
LCP array [7]), compressed sequence representations [19,6], partial sums [13],
sparse bitmaps [19,18,3] and its applications to handling sets over a bounded
universe supporting predecessor and successor search, and a long so on. It is
indeed a common case that an array of integers contains mostly small values,
but the need to handle a few large values makes programmers opt for allocating
the maximum space instead of seeking for a more sophisticated solution.

(cid:2) Funded in part (for the Spanish group) by MEC grant (TIN2006-15071-C03-03); and

for the third author by Fondecyt Grant 1-080019, Chile.

J. Karlgren, J. Tarhio, and H. Hyyr¨o (Eds.): SPIRE 2009, LNCS 5721, pp. 122–130, 2009.
c(cid:2) Springer-Verlag Berlin Heidelberg 2009

Directly Addressable Variable-Length Codes

123

The typical solution to provide direct access to a variable-length encoded
sequence is to regularly sample it and store the position of the samples in the
encoded sequence, so that decompression from the last sample is necessary. This
introduces a space and time penalty to the encoding that often hinders the use
of variable-length coding in many cases where it would be beneﬁcial.

In this paper we show that, by properly reordering the target symbols of a
variable-length encoding of a sequence, direct access to any codeword (achieving 
constant time per symbol of the target alphabet) is easy and fast. This is
a kind of implicit data structure that introduces synchronism in the encoded
sequence without using asymptotically any extra space. We show some experiments 
demonstrating that the technique is not only simple, but also competitive
in time and space with existing solutions in several applications.

2 Basic Concepts

Statistical encoding. Let X = x1x2 . . . xn be a sequence of symbols to represent.
A way to compress X is to order the distinct symbol values by frequency, and
identify each value xi with its position pi in the ordering, so that smaller positions
occur more frequently. Hence the problem is how to encode the pis into variablelength 
bit streams ci, giving shorter codewords to smaller values. Huﬀman coding
[11] is the best code (i.e., achieving the minimum total length for encoding X)
such that (1) assigns the same codeword to every occurrence of the same symbol
and (2) is a preﬁx code.
Coding integers. In other applications, the xis are directly the numbers pi to be
encoded, such that the smaller values are assumed to be more frequent. One can
still use Huﬀman, but if the set of distinct numbers is too large, the overhead
of storing the Huﬀman code may be prohibitive. In this case one can directly
encode the numbers with a ﬁxed preﬁx code that gives shorter codewords to
smaller numbers. Well-known examples are γ-codes and δ-codes [23,21].
Vbyte coding. [22] is a particularly interesting code for this paper. In its general
variant, the code splits the (cid:2)log(pi + 1)(cid:3) bits needed to represent pi by splitting
it into blocks of b bits and storing each block into a chunk of b + 1 bits. The
highest bit is 0 in the chunk holding the most signiﬁcant bits of pi, and 1 in the
rest of the chunks. For clarity we write the chunks from most to least signiﬁcant,
just like the binary representation of pi. For example, if pi = 25 = 110012 and
b = 3, then we need two chunks and the representation is 0011 1001.
Compared to an optimal encoding of (cid:2)log(pi + 1)(cid:3) bits, this code loses one
bit per b bits of pi, plus possibly an almost empty ﬁnal chunk. Even when the
best choice for b is used, the total space achieved is still worse than δ-encoding’s
performance. In exchange, Vbyte codes are very fast to decode.
Partial sums are an extension of our problem when X is taken as a sequence of
nonnegative diﬀerences between consecutive values of sequence Y = y1, y2, . . . yn,
so that yi = sum(i) =
1≤j≤i pj. Hence, X is a compressed representation of Y
that exploits the fact that consecutive diﬀerences are small numbers. We are then

(cid:2)

124

N.R. Brisaboa, S. Ladra, and G. Navarro

interested in obtaining eﬃciently yi = sum(i). Sometimes we are also interested
in ﬁnding the largest yi ≤ v given v, that is, search(v) = max{i, sum(i) ≤ v}.
Let us call S = sum(n) from now on.

3 Previous Work

From the previous section, we end up with a sequence of n concatenated variablelength 
codes. Being usually preﬁx, there is no problem in decoding them in
sequence. We now outline several solutions to the problem of giving direct access
to them, that is, extracting any pi eﬃciently, given i. Let us call N the length
in bits of the encoded sequence.

The classical solution samples the sequence and stores absolute pointers only to
the sampled elements, that is, to each h-th element of the sequence. Access to
the (h + d)-th element, for 0 ≤ d < h, is done by decoding d codewords starting
from the h-th sample. This involves a space overhead of (cid:5)n/h(cid:6)(cid:5)log N(cid:6) bits and
a time overhead of O(h) to access an element, assuming we can decode each
symbol in constant time. The partial sums problem is also solved by storing
some sampled yi values, which are directly accessed for sum or binary searched
for search, and then summing up the pis from the last sample.

A dense sampling is used by Ferragina and Venturini [6]. It represents pi using
just its (cid:2)log(pi + 1)(cid:3) bits, and sets pointers to every element in the encoded
sequence, giving the ending points of the codewords. By using two levels of
pointers (absolute ones every Θ(log N) values and relative ones for the rest) the
extra space for the pointers is O( n log log N
log N ), and constant-time access is possible.
Sparse bitmaps solve the direct access and partial sums problems when the
diﬀerences are strictly positive. The bitmap B[1, S] has a 1 at positions yi.

We make use of two complementary operations that can operate in constant
time after building o(S)-bit directories on top of B [12,2,16]: rank(B, i) is the
number of 1s in B[1, i], and select(B, i) is the position in B of the ith 1 (similarly,
select0(B, i) ﬁnds the ith 0). Then yi = select(B, i) and search(v) = rank(B, v)
easily solve the partial sums problem, whereas xi = select(B, i)− select(B, i−1)
solves our original access problem. We can also accommodate zero-diﬀerences
by setting bits i + yi in B[1, S + n], so yi = select(B, i) − i, search(v) =
rank(B, select0(B, v)), and xi = select(B, i) − select(B, i − 1) − 1.

A drawback of this solution is that it needs to represent B explicitly, thus it
requires S + o(S) bits, which can be huge. There has been much work on sparse
bitmap representations that can lighten space requirements [19,10,18].

4 Our Technique: Reordered Vbytes

We make use of the generalized Vbyte coding described in Section 2. We ﬁrst
encode the pis into a sequence of (b+1)-bit chunks. Next we separate the diﬀerent
chunks of each codeword. Assume pi is assigned a codeword Ci that needs r

Directly Addressable Variable-Length Codes

125

(cid:38)(cid:32)(cid:3)(cid:3) (cid:38)(cid:20)(cid:15)(cid:21)(cid:3)(cid:38)(cid:20)(cid:15)(cid:20)(cid:3) (cid:38)(cid:21)(cid:15)(cid:20)(cid:3) (cid:38)(cid:22)(cid:15)(cid:22)(cid:3)(cid:38)(cid:22)(cid:15)(cid:21)(cid:3)(cid:38)(cid:22)(cid:15)(cid:20)(cid:3) (cid:38)(cid:23)(cid:15)(cid:21)(cid:3)(cid:38)(cid:23)(cid:15)(cid:20)(cid:3) (cid:38)(cid:24)(cid:15)(cid:20)(cid:3)(cid:171)(cid:17)(cid:3)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:58)(cid:72)(cid:3)(cid:71)(cid:72)(cid:81)(cid:82)(cid:87)(cid:72)(cid:3)(cid:72)(cid:68)(cid:70)(cid:75)(cid:3)(cid:3)(cid:38)(cid:76)(cid:15)(cid:15)(cid:77)(cid:3)(cid:32)(cid:3)(cid:37)(cid:76)(cid:15)(cid:77)(cid:3)(cid:29)(cid:3)(cid:36)(cid:76)(cid:15)(cid:77)(cid:3)

(cid:36)(cid:24)(cid:15)(cid:20)(cid:3)
(cid:19)(cid:3)

(cid:171)(cid:17)(cid:3)
(cid:171)(cid:17)(cid:3)

(cid:36)(cid:20)(cid:3) (cid:36)(cid:20)(cid:15)(cid:20)(cid:3)
(cid:20)(cid:3)

(cid:38)(cid:20)(cid:3) (cid:37)(cid:20)(cid:3)

(cid:36)(cid:21)(cid:3) (cid:36)(cid:20)(cid:15)(cid:21)(cid:3)
(cid:19)(cid:3)

(cid:38)(cid:21)(cid:3) (cid:37)(cid:21)(cid:3)

(cid:36)(cid:22)(cid:3) (cid:36)(cid:22)(cid:15)(cid:22)(cid:3)
(cid:19)(cid:3)

(cid:38)(cid:22)(cid:3) (cid:37)(cid:22)(cid:3)

(cid:36)(cid:21)(cid:15)(cid:20)(cid:3)
(cid:19)(cid:3)

(cid:36)(cid:22)(cid:15)(cid:21)(cid:3)
(cid:20)(cid:3)

(cid:171)(cid:17)(cid:3)
(cid:171)(cid:17)(cid:3)

(cid:36)(cid:22)(cid:15)(cid:20)(cid:3)
(cid:20)(cid:3)

(cid:36)(cid:23)(cid:15)(cid:21)(cid:3)
(cid:19)(cid:3)

(cid:36)(cid:23)(cid:15)(cid:20)(cid:3)
(cid:20)(cid:3)

(cid:171)(cid:17)(cid:3)
(cid:171)(cid:17)(cid:3)

(cid:3)

(cid:3)

Fig. 1. Example of reorganization of the chunks of each codeword

chunks Ci,r, . . . , Ci,2, Ci,1. A ﬁrst stream, C1, will contain the n1 = n least
signiﬁcant chunks (i.e., rightmost) of every codeword. A second one, C2, will
contain the n2 second chunks of every codeword (so that there are only n2
codewords using more than one chunk). We proceed similarly with C3, and so
on. As the pis add up to S, we need at most (cid:5) log S
b (cid:6) streams Ck (usually less).
Each stream Ck will be separated into two parts. The lowest b bits of the
chunks will be stored contiguously in an array Ak (of b · nk bits), whereas the
highest bits will be concatenated into a bitmap Bk of nk bits. Figure 1 shows
the reorganization of the diﬀerent chunks of a sequence of ﬁve codewords. The
bits in each Bk identify whether there is a chunk of that codeword in Ck+1.

We set up rank data structures on the Bk bitmaps, which answer rank in
constant time using O( nk log log N
) extra bits of space, being N the length in
bits of the encoded sequence1. Solutions to rank are rather practical, obtaining
excellent times using 37.5% extra space on top of Bk, and decent ones using up
to 5% extra space [8,18].

log N

The overall structure is composed by the concatenation of the Bks, that of
the Aks, and pointers to the beginning of the sequence of each k. These pointers
need at most (cid:5) log S
b (cid:6)(cid:5)log N(cid:6) bits overall, which is negligible. In total there are
(cid:2)
b+1 chunks in the encoding (note N is a multiple of b + 1), and thus

k nk = N

the extra space for the rank data structures is just O( N log log N

Extraction of the i-th value of the sequence is carried out as follows. We start
with i1 = i and get its ﬁrst chunk b1 = B1[i1] : A1[i1]. If B1[i1] = 0 we are
done with pi = A1[i1]. Otherwise we set i2 = rank(B1, i1), which sends us to
the correct position of the second chunk of pi in B2, and get b2 = B2[i2] : A2[i2].
If B2[i2] = 0, we are done with pi = A1[i1] + A2[i2] · 2b. Otherwise we set
i3 = rank(B2, i2) and so on2.
nb(cid:6) accesses; the worst case is
at most (cid:5) log S
b (cid:6) accesses. Thus, in case the numbers to represent come from
a statistical variable-length coding, and the sequence is accessed at uniformly

Extraction of a random codeword requires (cid:5) N

b log N ).

1 This is achieved by using blocks of 1
2 To avoid the loss of a value in the highest chunk we use in our implementation the

2 log N bits in the rank directories [12,2,16].

variant of Vbytes we designed for text compression called ETDC [1].

126

N.R. Brisaboa, S. Ladra, and G. Navarro

distributed positions, we have the additional beneﬁt that shorter codewords are
accessed more often and are cheaper to decode.

4.1 Partial Sums
The extension to partial sums is as for the classical method: We store in a vector
Y [0, n/s] the accumulated sum at regularly sampled positions (say every hth
position). We store in Y [j] the accumulated sum up to phj. The extra space
required by Y is thus (cid:5)n/h(cid:6)(cid:5)log S(cid:6) bits. With those samples we can easily solve
the two classic operations sum(i) and search(v).
We compute sum(i) by accessing the last sampled Y [j] before pi, that is
j = (cid:2)i/h(cid:3) and adding up all the values between phj+1 and pi. To add those values
we ﬁrst sequentially add all the values between A1[hj +1] and A1[i]. We compute
s1 = hj + 1 and e1 = i and Acc1 =
A1[r]; then we compute s2 =
(cid:2)
rank(B1, s1 − 1) + 1 and e2 = rank(B1, e1) and again Acc2 =
A2[r];
s2≤r≤e2
(cid:2)
Acck · 2b(k−1).
and so on for the following levels. The ﬁnal result is Y [j] +
Notice that for a sampling step h this operation costs at most O( h log S

s1≤r≤e1

(cid:2)

To perform search(v) we start with a binary search for v in vector Y . Once we
ﬁnd the sample Y [j] with the largest value not exceeding v, we start a sequential
scanning and addition of the codewords until we reach v. That is, we start with
total = Y [j], b1 = hj +1, b2 = rank(B1, b1−1)+1, b3 = rank(B2, b2−1)+1 and
so on. The value of each new codeword is computed using its diﬀerent chunks
at levels k = 1, 2, . . ., adding Ak[bk] · 2b(k−1) and incrementing bk, as long as
k = 1 or Bk−1[bk−1 − 1] = 1. Once computed, the value is added to total until
we exceed the desired value v; then search(v) = b1 − 1. Notice that we compute
only one rank operation per sequence Bk, as the next chunks to read in each Bk
follow the current one. The total cost for a search operation is O(log n
h ) for the
binary search in the samples array plus O( h log S
) for the sequential addition of
the codewords following the selected sample Y [j].

b

).

b

5 Applications and Experiments

We detail now some applications of our scheme, and compare it with the current
solutions used in those applications. This section is not meant to be exhaustive,
but rather a proof of concept, illustrative of the power and ﬂexibility of our idea.
We implemented our technique with b values chosen manually for each level
(in many cases the same b for all). We prefer powers of 2 for b, so that faster
aligned accesses are possible. We implemented rank using the 37.5%-extra space
data structure by Gonz´alez et al. [8] (this is space over the Bk bitmaps).

Our machine is an Intel Core2Duo E6420@2.13Ghz, with 32KB+32KB L1
Cache, 4MB L2 Cache, and 4GB of DDR2-800 RAM. It runs Ubuntu 7.04 (kernel
2.6.20-15-generic). We compiled with gcc version 4.1.2 and the options -m32 -09.

5.1 High-Order Compressed Sequences
Ferragina and Venturini [6] gave a simple scheme (FV) to represent a sequence
of symbols S = s1s2 . . . sn so that it is compressed to its high-order empirical

Directly Addressable Variable-Length Codes

127

Table 1. Space for encoding the 2-byte blocks and individual access time

Method
Dense sampling (FV, c = 20)
Sparse sampling (h = 14)
Vbyte (b = 7) sampling (h = 14)
Ours (b = 8)

Space (% of original ﬁle) Time (nanosec per extraction)

94.34%
68.44%
75.90%
68.46%

298.4
557.2
305.7
216.1

entropy and any O(log n)-bit substring of S can be decoded in constant time.
This is extremely useful because it permits replacing any sequence by its compressed 
variant, and any kind of access to it under the RAM model of computation 
retains the original time complexity.

The idea is to split S into blocks of 1

2 log n bits, and then sort the blocks by
frequency. Once the sequence of their positions pi is obtained, it is stored using
a dense sampling, as explained in Section 3. We compare their dense sampling
proposal with our own representation of the pi numbers, as well as a classical
variant using sparse sampling (also explained in Section 3).

We took the ﬁrst 512 MB of the concatenations of collections FT91 to FT94
(Financial Times) from trec-2 (http://trec.nist.gov), and chose 2-byte
blocks, thus n = 229 and our block size is 16 bits.

We implemented scheme FV, and optimized it for this scenario. There are
5,426 diﬀerent blocks, and thus the longest block description has 12 bits. We
stored absolute 32-bit pointers every c = 20 blocks, and relative pointers of
(cid:5)log((c − 1) · 12)(cid:6) = 8 bits for each block. This was the setting giving the best
space, and let us manage pointers using integers and bytes, which is faster.

We also implemented the classical alternative of Huﬀman-encoding the different 
blocks, and setting absolute samples every h codewords. This gives us a
space-time tradeoﬀ, which we set to h = 14 to achieve space comparable to our
alternative. In addition, we implemented a variant with the same parameters
but using Vbyte-encoding, with b = 7 (i.e., using bytes as chunks).

We used our technique with b = 8, which lets us manipulate bytes and thus

is faster. The space was almost the same with b = 4, but time was worse.

Table 1 shows the results. We measure space as a fraction of the size of the
original 512 MB text, and time as nanoseconds per extraction, where we average
over the time to extract all the blocks of the sequence in random order.

The original FV method poses much space overhead (achieving almost no
compression). This, as expected, is alleviated by the sparse sampling, but the
access times increase considerably. Yet, our technique achieves much better space
and noticeable better access times than FV. When using the same space of a
sparse sampling, on the other hand, our technique is three times faster. Sparse
sampling can achieve 54% space (just the bare Huﬀman encoding), at the price of
higher access times. The Vbyte alternative is both larger and slower than ours. In
fact, the Vbyte-encoding itself, without the sampling overhead, occupies 67.8%
of the original sequence, very close to our representation (which will be similar
to an Vbyte encoding using b = 8).

128

N.R. Brisaboa, S. Ladra, and G. Navarro

Table 2. Space for encoding the diﬀerential Ψ array and individual sum time under
diﬀerent schemes. The b sequences refer to the (diﬀerent) consecutive b values used in
∗
the arrays C1, C2, etc. “Ours

” uses 5% extra space for rank on the bitmaps.

Method
Sadakane’s
Ours b = 8
Ours b = 4
Ours b = 2
Ours b = 0, 2, 4, 8
∗ b = 0, 2, 4, 8
Ours
Ours b = 0, 4, 8

Space (% of original ﬁle) Time (nanosec per Ψ computation)

66.72%
148.06%
103.44%
85.14%
73.96%
67.88%
76.85%

645.5
629.0
675.6
919.8
757.1
818.7
742.7

5.2 Compressed Suﬃx Arrays

Sadakane [20] proposed to represent the so-called Ψ array, useful to compress
suﬃx arrays [9,17], by encoding its consecutive diﬀerences along the large areas 
where Ψ is increasing. A γ-encoding is used to gain space, and the classical
alternative of sampling plus decompression is used in the practical implementation.
 We compare now this solution to our proposal, using the implementation 
obtained from Pizza&Chili site3 and setting one absolute sample every 128
values.
We took trec-2 collection CR, of about 47 MB, generated its Ψ array, and
measured the time to compute Ψ i(x), for 1 ≤ i < n, where x is the suﬃx array
position pointing to the ﬁrst text character. This simulates extracting the whole
text by means of function Ψ without having the text at hand.

As the diﬀerences are strictly positive, we represent in our method the diﬀerences 
minus 1 (so access to Ψ[i] is solved via sum(i) + i). This time we use b = 0
for the ﬁrst level of our structure, and other b values for the rest. This seemingly
curious choice lets us spend one bit (in B1, as A1 is empty) to represent all the
areas of Ψ where the diﬀerences are 1. This is known to be the case on large
areas of Ψ for compressible texts [17], and is also a good reason for Sadakane to
have chosen γ-codes. We set one absolute sample every 128 values for our sum.
Apart from the usual rank version that uses 37.5% of space over the bitmaps,
we tried a slower one that uses just 5% [8].

Table 2 shows the results. We measure space as a fraction of the size of
the original text, and time as nanoseconds per sum, as this is necessary to
obtain the original Ψ values from the diﬀerential version. We only show some
examples of ﬁxed b, and how using diﬀerent b values per level can achieve better
results.

This time our technique does not improve upon Sadakane’s representation,
which is carefully designed for this speciﬁc problem and known to be one of the
best implementations [5]. Nevertheless, it is remarkable that we get rather close
(e.g., same space and 27% slower, or 15% worse in space and time) with a general
and elegant technique. It is also a good opportunity to illustrate the ﬂexibility
of our technique, which lets us use diﬀerent b values per level.

3 Mirrors http://pizzachili.dcc.uchile.cl and http://pizzachili.di.unipi.it.

Directly Addressable Variable-Length Codes

129

6 Conclusions

We have introduced a data reordering technique that, when applied to a particular 
class of variable-length codes, enables easy and direct access to any codeword,
bypassing the heavyweight methods used in current schemes. This is an important 
achievement because the need of random access to variable-length codes is
ubiquitous in many sorts of applications.

We have shown experimentally that our technique competes successfully, in
several immediate applications. We have also compared our proposal with the
best solutions for sparse bitmaps [18], but we have omitted it in Section 5 due
to space limitations: Except for the search operation, we achieved better space
and time results when the distribution of the gaps was skewed, and comparable
performance otherwise (uniform distribution).

We have used the same b for every level, or manually chose it at each level to
ﬁt our applications. This could be reﬁned and generalized to use the best b at
each level, in terms of optimizing compression. The optimization problem can
be easily solved by dynamic programming in just O(n log S) time.

References

1. Brisaboa, N., Fari˜na, A., Navarro, G., Param´a, J.: Lightweight natural language

text compression. Information Retrieval 10, 1–33 (2007)

2. Clark, D.: Compact Pat Trees. PhD thesis, University of Waterloo, Canada (1996)
3. Claude, F., Navarro, G.: Practical rank/select queries over arbitrary sequences. In:
Amir, A., Turpin, A., Moﬀat, A. (eds.) SPIRE 2008. LNCS, vol. 5280, pp. 176–187.
Springer, Heidelberg (2008)

4. Culpepper, J., Moﬀat, A.: Compact set representation for information retrieval.
In: Ziviani, N., Baeza-Yates, R. (eds.) SPIRE 2007. LNCS, vol. 4726, pp. 137–148.
Springer, Heidelberg (2007)

5. Ferragina, P., Gonz´alez, R., Navarro, G., Venturini, R.: Compressed text indexes:

From theory to practice. ACM JEA 13, article 12, 30 pages (2009)

6. Ferragina, P., Venturini, R.: A simple storage scheme for strings achieving entropy

bounds. In: Proc. 18th SODA, pp. 690–696 (2007)

7. Fischer, J., M¨akinen, V., Navarro, G.: An(other) entropy-bounded compressed sufﬁx 
tree. In: Ferragina, P., Landau, G.M. (eds.) CPM 2008. LNCS, vol. 5029, pp.
152–165. Springer, Heidelberg (2008)

8. Gonz´alez, R., Grabowski, S., M¨akinen, V., Navarro, G.: Practical implementation

of rank and select queries. In: Poster Proc. 4th WEA, pp. 27–38 (2005)

9. Grossi, R., Vitter, J.: Compressed suﬃx arrays and suﬃx trees with applications

to text indexing and string matching. In: Proc. 32nd STOC, pp. 397–406 (2000)

10. Gupta, A., Hon, W.-K., Shah, R., Vitter, J.: Compressed dictionaries: Space measures,
 data sets, and experiments. In: Proc. 5th WEA, pp. 158–169 (2006)

11. Huﬀman, D.: A method for the construction of minimum-redundancy codes. Proceedings 
of the I.R.E. 40(9), 1090–1101 (1952)

12. Jacobson, G.: Space-eﬃcient static trees and graphs. In: Proc. 30th FOCS, pp.

549–554 (1989)

13. M¨akinen, V., Navarro, G.: Dynamic entropy-compressed sequences and full-text

indexes. ACM Transactions on Algorithms (TALG) 4(3), 1–38 (2008)

130

N.R. Brisaboa, S. Ladra, and G. Navarro

14. Moﬀat, A.: Word-based text compression. Software Practice and Experience 19(2),

185–198 (1989)

15. Moura, E., Navarro, G., Ziviani, N., Baeza-Yates, R.: Fast and ﬂexible word searching 
on compressed text. ACM TOIS 18(2), 113–139 (2000)

16. Munro, I.: Tables. In: Chandru, V., Vinay, V. (eds.) FSTTCS 1996. LNCS,

vol. 1180, pp. 37–42. Springer, Heidelberg (1996)

17. Navarro, G., M¨akinen, V.: Compressed full-text indexes. ACM Computing Surveys 
39(1), article 2 (2007)

18. Okanohara, D., Sadakane, K.: Practical entropy-compressed rank/select dictionary.

In: Proc. 9th ALENEX (2007)

19. Raman, R., Raman, V., Rao, S.: Succinct indexable dictionaries with applications
to encoding k-ary trees and multisets. In: Proc. 13th SODA, pp. 233–242 (2002)

20. Sadakane, K.: New text indexing functionalities of the compressed suﬃx arrays.

Journal of Algorithms 48(2), 294–313 (2003)

21. Solomon, D.: Variable-length codes for data compression. Springer, Heidelberg

(2007)

22. Williams, H.E., Zobel, J.: Compressing integers for fast ﬁle access. COMPJ: The

Computer Journal 42(3), 193–201 (1999)

23. Witten, I., Moﬀat, A., Bell, T.: Managing Gigabytes, 2nd edn. Morgan Kaufmann

Publishers, New York (1999)

