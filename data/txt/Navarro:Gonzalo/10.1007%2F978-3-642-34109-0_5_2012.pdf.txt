Compressed Suﬃx Trees for Repetitive Texts(cid:2)

Andr´es Abeliuk1,2 and Gonzalo Navarro1

1 Department of Computer Science, University of Chile

2 Instituto de Filosof´ıa y Ciencias de la Complejidad, IFICC

{aabeliuk,gnavarro}@dcc.uchile.cl

Abstract. We design a new compressed suﬃx tree speciﬁcally tailored
to highly repetitive text collections. This is particularly useful for sequence 
analysis on large collections of genomes of the close species. We
build on an existing compressed suﬃx tree that applies statistical compression,
 and modify it so that it works on the grammar-compressed
version of the longest common preﬁx array, whose diﬀerential version
inherits much of the repetitiveness of the text.

1

Introduction

The suﬃx tree [27,20] is arguably the most beautiful and relevant data structure
for string analysis. It has been said to have a myriad virtues [1], and it has a
myriad applications in many areas, particularly bioinformatics [11]. A suﬃx tree
built on a text T supports pattern matching in T in time proportional to the
length of the pattern. In addition, many complex sequence analysis problems are
solved through sophisticated traversals over the suﬃx tree. Thus, a suﬃx tree
implementation must support a variety of navigation operations. These involve
not only the classical tree navigation (parent, child) but also speciﬁc ones such
as suﬃx links and lowest common ancestors.

One of the main drawbacks of suﬃx trees is their considerable space requirement,
 which is usually close to 20n bytes for a text of n symbols, and at the very
least 10n bytes [14]. For example, the human genome, containing approximately
3 billion bases, could easily ﬁt in the main memory of a desktop computer (as
each DNA symbol needs just 2 bits). However, its suﬃx tree would require 30
to 60 gigabytes, too large to ﬁt in normal main memories. A way to reduce
this space to about 4 bytes per symbol is to use a simpliﬁed structure called a
suﬃx array [18], which still oﬀers pattern matching but misses important suﬃx
tree operations such as suﬃx links and lowest common ancestor operations. This
reduces the relevance of suﬃx arrays in many biological problems, whereas in
many other areas suﬃx arrays are suﬃcient.

Much research on compressed representations of suﬃx trees and arrays, which
operate in compressed form, has been pursued. Progress has been made in terms
of the statistical compressibility of the text collection, that is, how biased are
the symbol frequencies given a short context of k symbols around them.
(cid:2) Partially funded by Fondecyt grant 1-080019 and Millennium Institute for Cell Dynamics 
and Biotechnology (ICDB), Grant ICM P05-001-F, Mideplan, Chile.

L. Calder´on-Benavides et al. (Eds.): SPIRE 2012, LNCS 7608, pp. 30–41, 2012.
c(cid:2) Springer-Verlag Berlin Heidelberg 2012

Compressed Suﬃx Trees for Repetitive Texts

31

A recent challenge raised by the sharply falling costs of sequencing1 is the
growth of large sequence databases formed by genomes of individuals of the
same or closely related species. In a few years, biologists will need to handle
databases of thousands to millions of genomes: sequencing machines are already
producing the equivalent of thousands of genomes per day2. These requirements
dwarf the current challenges of indexing one genome in main memory, and could
never be tackled with statistical compression based techniques.

Fortunately, these huge databases have a feature that renders them tractable:
they are highly repetitive. Two human genomes share 99.9% of their sequences,
for example. Such features are not captured by statistical compression methods
(i.e., the frequencies of symbols stay roughly the same in a database of many
near-copies of the same sequence). Rather, we need repetition aware compression
methods. Although this kind of compression is well-known (e.g., grammar-based
and Ziv-Lempel-based compression), only recently there have appeared compressed 
suﬃx arrays and other indexes capable of pattern searching that take
advantage of repetitiveness [17,5,4,13]. Yet, none of the existing compressed sufﬁx 
trees [26,8,7,23,25,9], is tailored to repetitive text collections.

Our contribution is to present the ﬁrst compressed suﬃx tree with full functionality,
 whose compression eﬀectiveness is related to the repetitiveness of the
text collection. While our operations are much slower than existing compressed
suﬃx trees, the space required is also much lower on repetitive collections.

2 Our Contribution in Context

Most of the research in this area is focused on compressed suﬃx arrays [22]
(CSAs, generically), whose functionality is not suﬃcient for many computational
biology problems. There are, however, various recent results showing how to
enhance a CSA in order to obtain a compressed suﬃx tree (CST, generically)
[26,8,7,23,25,9]. Essentially, they show that if one adds longest common preﬁx
(LCP) information, one can obtain a CST from a CSA.

The ﬁrst CST was Sadakane’s [26]. Apart from the CSA, it used 2n bits to
represent the LCP, plus other 4n bits to support navigation. Most operations
are supported in constant time. The best existing implementation [9] shows that
it uses about 13 bpc (bits per character) and very few microseconds per query.
The second proposal was by Russo et al. [25]. It requires only o(n) bits on
top of a CSA, and the operations are supported in polylogarithmic time. The
implementation achieved very little space, around 4.5 bpc. However, operations
take order of milliseconds.

A third proposal by Fischer et al. [8], later improved by Fischer [7], achieves
(1/)nHk extra bits, where Hk is the k-th order empirical entropy of T (a measure 
of statistical compressibility [19]), for any constant  > 0. Operation times

1 See http://www.guardian.co.uk/world/feedarticle/10038353
2 See http://www.nytimes.com/2011/12/01/business/dna-sequencing-caught-in-

deluge-of-data.html? r=2

32

A. Abeliuk and G. Navarro

 n). Diﬀerent practical variants of this structure were deare 
of the form O(log
signed and implemented by C´anovas and Navarro [3] and by Ohlebusch, Gog
and Fischer [23,9]. The best implementations use as little as 10 bpc and execute
operations in a few microseconds (but usually slower than Sadakane’s CST).

We introduce a new CST that, for current repetitive biological collections,
reaches 1.3–1.5 bpc, in exchange for operation times in the order of milliseconds.
This large space diﬀerence with previous CSTs should widen on larger repetitive
collections (i.e., thousands or more similar sequences, as opposed to tens in our
test collections), whereas current CSTs would simply grow linearly in size.

Our result is built on three blocks, which will be detailed in the next sections:

1. We build on the only existing CSA that is tailored to repetitive collections,
the Run-Length CSA (RLCSA) [17]. The size of the RLCSA is a function of
the number of runs in Ψ , a concept that will be explained in Section 4 and
that is related to the repetitiveness of the text collection. The RLCSA will
be responsible for most of the ﬁnal space, 0.85–0.95 bpc.

2. We use a base 2n-bit LCP representation that was initially proposed by
Sadakane [26]. Fischer et al. [8] showed that this sequence could be represented 
using a number of bits that is, again, related to the runs in Ψ (see
Section 3. C´anovas and Navarro [3] showed empirically that the compression 
achieved was insigniﬁcant on non-repetitive collections. In this paper
we show that, on repetitive collections, this idea does pay oﬀ, adding just
0.2–0.25 bpc to the space.

3. Fischer et al. [8] show how to map all the CST operations into three queries
over the LCP array: range minimum queries (RMQ) and a new primitive
called next/previous smaller value (NSV/PSV), see Section 3. We design a
novel index on the LCP to answer those queries, inspired on that of C´anovas
and Navarro [3], but whose size depends on the number of runs in Ψ . Inspired
in a local suﬃx array compression method [10], we grammar-compress the
diﬀerential LCP array and replace the regular tree structure used by C´anovas
and Navarro by a (pruned) grammar tree resulting from the LCP compression.
 This index adds about 0.2–0.3 further bpc to the space.

3 Our Base Compressed Suﬃx Tree

A suﬃx array [18] over a text T [1, n] is an array A[1, n] of the positions in T ,
lexicographically sorted by the suﬃx starting at the corresponding position of T .
That is, T [A[i], n] < T [A[i + 1], n] for all 1 ≤ i < n. Note that every substring
of T is the preﬁx of a suﬃx, and that all suﬃxes starting with a given pattern
P appear consecutively in A, hence a couple of binary searches ﬁnd the area
A[sp, ep] containing all the positions where P occurs in T .

There are several compressed suﬃx arrays (CSAs) [22,6], which oﬀer essentially 
the following functionality: (1) Given a pattern P [1, m], ﬁnd the interval 
A[sp, ep] of the suﬃxes starting with P ; (2) obtain A[i] given i; (3) obtain 
A−1[j] given j. An important function the CSAs implement is Ψ (i) =

Compressed Suﬃx Trees for Repetitive Texts

33
A−1[(A[i] mod n) + 1] and its inverse, usually much faster than computing A
and A−1 . This function lets us move virtually in the text, from the suﬃx i that
points to text position j = A[i], to the one pointing to j + 1 = A[Ψ (i)].

A suﬃx tree [27,20,1] is a compact trie (or digital tree) storing all the suﬃxes
of T . This is a labeled tree where each text suﬃx is read in a root-to-leaf path,
and the children of a node are labeled by diﬀerent characters. Leaves are formed
when the preﬁx of the corresponding suﬃx is already unique. Here “compact”
means that unary paths are converted into a single edge, labeled by the string
formed by concatenating the involved character labels. If the children of each
node are ordered lexicographically by their string label, then the leaves of the
suﬃx tree form the suﬃx array of T .

In order to get a suﬃx tree from a suﬃx array, one needs the longest common
preﬁx (LCP) information, that is, LCP [i] is the length of the longest common
preﬁx between suﬃxes T [A[i − 1], n] and T [A[i], n], for i > 1, and LCP [1] = 0
(or, seen another way, the length of the string labeling the path from the root
to the lowest common ancestor node of suﬃx tree leaves i and i − 1). The suﬃx
tree topology is implicit if we identify each suﬃx tree node with the suﬃx array
interval containing the leaves that descend from it. This range uniquely identiﬁes
the node because there are no unary nodes in a suﬃx tree. A compressed suﬃx
tree (CST) is obtained by enriching a CSA with some representation of the LCP
data, plus some extra space to support fast queries.

Sadakane [26] showed how to compress the LCP array to just 2n bits by
noticing that, if sorted by text order rather than suﬃx array order, the LCP
numbers decrease by at most 1. Let P LCP be the permuted LCP array, then
P LCP [j + 1] ≥ P LCP [j] − 1. Thus the numbers can be diﬀerentially encoded,
h[j + 1] = P LCP [j + 1] − P LCP [j] + 1 ≥ 0, and then represented in unary
over a bitmap H[1, 2n] = 0h[1]10h[2] . . . 10h[n]1. Then, to obtain LCP [i], we look
for P LCP [A[i]], and this is extracted from H via rank/select operations. Here
rankb(H, i) counts the number of bits b in H[1, i] and selectb(H, i) is the position
of the i-th b in H. Both can be answered in constant time using o(n) extra bits
of space [21]. Then P LCP [j] = select1(H, j) − 2j, assuming P LCP [0] = 0.

Fischer et al. [8] prove that array H is compressible, as it has at most 2r runs of
0s or 1s. Here, r is the number of runs in Ψ , which is related to the repetitiveness
of T and will be discussed in Section 4 (the more repetitive T , the lower is r). Let
z1, z2 . . . zr the lengths of the runs of 0s and o1, o2 . . . or those of the runs of 1s.
They create arrays Z = 10z1−110z2−1 . . . and O = 10o1−110o2−1 . . ., which have
overall 2r 1s out of 2n, and hence can be compressed to 2r log n
r + O(r) + o(n)
bits with constant-time rank and select [24].

While Sadakane [26] represented explicitly the suﬃx tree topology using 4n
bits, Fischer et al. showed that all the operations can be simulated with suﬃx
array ranges, by means of three operations on LCP : (1) RM Q(i, j) gives the
position of the minimum in LCP [i, j]; (2) P SV (i) ﬁnds the last value smaller
than LCP [i] in LCP [1, i − 1]; and (3) N SV (i) ﬁnds the ﬁrst value smaller than
LCP [i] in LCP [i + 1, n]. All these could easily be solved in constant time using

34

A. Abeliuk and G. Navarro

O(n) extra bits of space on top of the LCP representation, but Fischer et al.
give sublogarithmic-time algorithms to solve them with only o(n) extra bits.

C´anovas and Navarro [3] implemented a practical solution to solve the operations 
NSV/PSV/RMQ. They divided the LCP array into blocks of length L
and formed a hierarchy of blocks, where they store the minimum LCP value of
each block i in an array m[i]. The array uses n
L log n bits. On top of array m,
they construct a perfect L-ary tree Tm where the leaves are the elements of m
and each internal node stores the minimum of the values stored in its children.
The total space needed for Tm is n
L log n(1 + O(1/L)) bits, so if L = ω(log n),
the space used is o(n) bits. To answer the queries with this structure one computes 
a minimal cover in Tm of the range of interest of LCP and ﬁnds the node
of the cover containing the answer. Then one moves down from the node until
ﬁnding the right leaf of Tm. Finally, the corresponding LCP block is sequentially
scanned to ﬁnd the exact position, which is the heaviest part in practice. To
answer RMQ queries faster they store for every node of Tm the local position in
the children where the minimum occurs, so there in no need to scan the child
blocks when going down the tree. The extra space incurred is still o(n) bits. If
the access to LCP cells is done via P LCP , then the overall cost of the operations
is dominated by O(L) times the cost of accessing a suﬃx array cell A[i].

4 Re-pair and Repetition-Aware CSAs

Re-Pair [15] is a grammar-based compression method that factors out repetitions
in a sequence. This method is based on the following heuristic: (1) Find the most
repeated pair ab in the sequence; (2) Replace all its occurrences by a new symbol
s; (3) Add a rule s → ab to a dictionary R; (4) Iterate until every pair is unique.
The result of the compression of a text T over a alphabet Σ of size σ, is
the dictionary R and the remaining sequence C, containing new symbols (s)
and symbols in Σ. Every sub-sequence of C can be decompressed locally by the
following procedure: Check if C[i] < σ; if so the symbol is original, else look in
R for rule C[i] → ab, and recursively continue expanding with the same steps.
The dictionary R corresponds to a context free grammar, and the sequence C
to the initial symbols of the derivation tree that represents T . The ﬁnal structure
can be regarded as a sequence of binary trees with roots C[i], see Figure 1 (left).
Gonz´alez and Navarro [10] used Re-Pair to compress the diﬀerentially encoded
[i] = A[i]−A[i−1]. They showed that Re-Pair achieves |R|+|C| =
suﬃx array, A(cid:3)
O(r log n
, r being the number of runs in Ψ . A run in Ψ is a maximal
contiguous area where Ψ [i + 1] = Ψ [i] + 1. It was shown that the number of runs
in Ψ is r ≤ nHk + σk for any k [16]. More importantly, repetitions in T induce
long runs in Ψ , and hence a smaller r [17]. An exact bound has been elusive, but
M¨akinen et al. [17] gave an average-case upper bound for r: if T is formed by a
random base sequence of length n(cid:3) (cid:5) n and then other sequences that have m
random mutations (which include indels, replacements, block moves, etc.) with
respect to the base sequence, then r is at most n(cid:3)

+ O(m logσ n) on average.

r ) on A(cid:3)

The RLCSA [17] is a CSA where those runs in Ψ are factored out, to achieve
O(r) cells of space. More precisely, the size of the RLCSA is r(2 log(n/r) +

Compressed Suﬃx Trees for Repetitive Texts

35

LCP = 0 1 0 3 2 3 3 2 2 3 2 5 4 5 5 5 4 5 4 5 5 5 4 5 4 5 5

T= LCP’ = 0 1 -1 3 -1 1 0 -1 0 1 -1 3 -1 1 0 0 -1 1 -1 1 0 0 -1 1 -1 1 0

RePair

C = h -1 h f f

R =

NPR-RP

C =

h

-1

h

f

f

a → (−1, 1)
b → (a, 0)
c → (1,−1)
d → (0, a)
e → (3, b)
f → (d, b)
g → (0, c)
h → (g, e)

h

f

g

e

d

b

g

e

0

c

3

b

d

b

0

a

a

0

1

-1

a

0

-1

1

-1

1

-1

1

3

b

Rule cover min lmin sum

b
d
e
f
g
h

3
3
4
6
3
7

-1
-1
2
-1
0
0

1
2
2
2
1
1

0
0
3
0
0
3

Fig. 1. On the left, example of the Re-Pair compression of a sequence T . We show
R in array form and also in tree form. On the right, our NPR-RP construction over
LCP (cid:2)
= T , pruning with t = 4. We show how deep can the symbols of C be expanded
after the pruning.

log σ)(1 + o(1)) bits, where r is the number of runs in Ψ [17]. It supports accesses
to A in time O(s log n), with O((n/s) log n) extra bits for a sampling of A.

5 Our Repetition-Aware CST

As explained in the Introduction, we use the RLCSA [17] as the base CSA of our
repetition-aware CST. We also use the compressed representation of P LCP [8].
Since in our case r (cid:5) n, we use a compressed bitmap representation that is useful
for very sparse bitmaps [13]: We δ-encode the runs of 0s between consecutive
1s, and store absolute pointers to the representation of every sth 1. This is very
eﬃcient in space and solves select1 queries in time O(s), which is the operation
needed to compute a P LCP value.

The main issue is how to support fast operations using the RLCSA and our
LCP representation. As already explained, we choose to support all the operations 
using RMQ/PSV/NSV [8], and in turn follow the scheme of C´anovas and
Navarro [3] to support these using the tree Tm. A problem is that this tree is of
size O((n/L) log n) bits, insensitive to the repetitiveness of the text.

Our main idea is to replace the regular structure of tree Tm by the parsing tree
obtained by a grammar compressor of the sequence LCP . We will now explain
this idea in detail.

5.1 Grammar-Compressing the LCP Array

The following fact motivates grammar-compressing the LCP array.

36

A. Abeliuk and G. Navarro

Fact 1. If i − 1 and i are within a run of Ψ and T [A[i − 1]] = T [A[i]], then
LCP [i] = LCP [Ψ (i)] + 1.
Proof. Let j = Ψ (i) and j(cid:3)
1]] = T [A[i]], it holds T [A[i− 1], n] = x· T [A[j(cid:3)
thus LCP [i] = (cid:8) + 1.

= Ψ (i− 1), and call (cid:8) = LCP [j]. Then, if x = T [A[i−
], n] and T [A[i], n] = x· T [A[j], n],

, deﬁned as LCP (cid:3)

[i] = LCP [i]−LCP [i−1] if i > 1, and LCP (cid:3)

This means that, except for the σ places of A[1, n] where the ﬁrst characters of
suﬃxes change, runs in Ψ correspond to areas of LCP that are oﬀ by 1 with
respect to other areas of LCP . This is the same situation detected by Gonz´alez
and Navarro [10] on A[1, n]. Following their idea, we can grammar-compress array
LCP (cid:3)
[1] = LCP [1].
This diﬀerential LCP array contains now O(r) areas that are exact repetitions of
others, and a RePair-based compression of it yields |R| +|C| = O(r log n
r ) words
[10]. We note, however, that the compression achieved in this way [10] is modest:
we guarantee O(r log n
r ) words, whereas the RLCSA and PLCP representations
require basically O(r log n
r ) bits. Thus we do not apply this idea directly, but
rather truncate the parsing tree of the grammar, and use it as a device to speed
up computations that would otherwise require expensive accesses to P LCP .
Let R and C be the results of compressing LCP (cid:3)
with RePair. Every nonterminal 
i of R expands to a substring S[1, t] of LCP (cid:3)
. No matter where S appears
in LCP (cid:3)
(indeed, it must appear more than once), we can store some values that
are intrinsic to S. Let us deﬁne a relative sequence of values associated to S,
[j − 1]. Then, we deﬁne the following
as follows: S(cid:3)
variables associated to the nonterminal:
– mini = min1≤j≤t S(cid:3)
– lmini and rmini are the leftmost and rightmost positions j where S(cid:3)
– sumi = S(cid:3)
– coveri = t is the number of values in S(cid:3)
As most of these values are small, we encode them with Directly Addressable
Codes [2], which use less space for short numbers while providing fast access
(rmin stored as the diﬀerence with lmin).

1≤j≤t S[j] is the sum of the values S[j].

[j] is the minimum value in S(cid:3)

[0] = 0 and S(cid:3)

[j] = S[j] + S(cid:3)

mini, respectively.
(cid:2)

[t] =

.

[j] =

.

To reduce space, we prune the grammar by deleting the nonterminals i such
that coveri < t, where t will be a space/time tradeoﬀ parameter (recall that the
grammar is superﬂuous, as we have access to LCP via P LCP , so we use it only
to speed up computations). However, “short” nonterminals that are mentioned
in sequence C are not deleted.

This ensures that we can skip Ω(t) symbols of LCP with a single access to
the corresponding nonterminal in C, except for the short nonterminals (and ter-
minals) that are retained in C. To speed up traversals on C, we join together
maximal consecutive subsequences of nonterminals and terminals in C that sum
up a total cover < t: we create a new nonterminal rule in R (for which we precompute 
the variables above) and replace it in C, deleting those nonterminals
that formed the new rule and do not appear anymore in C. This will also guarantee 
that no more than O(t) accesses to LCP are needed to solve queries. Note

Compressed Suﬃx Trees for Repetitive Texts

37

that we could have built a hierarchy of new nonterminals by recursively grouping
t consecutive symbols of C, achieving logarithmic operation times just as with
tree Tm [3], but this turned out to be counterproductive in practice. Figure 1
(right) gives an example.
position C[c · j], stores:
– P os[j] = 1 +

Finally, sampled pointers are stored to every c-th cell of C. Each sample for

coverC[k], that is, the ﬁrst position LCP [i] corre-

(cid:2)

sumC[k], that is, the value LCP [i].

1≤k≤cj−1

sponding to C[c · j].
1≤k≤cj−1

– V al[j] =

(cid:2)

5.2 Computing NSV, PSV, and RMQ

such that P os[j(cid:3)

To answer N SV (i), we ﬁrst look for the rule C[j] that contains LCP [i + 1]: we
] ≤ i + 1 and then sequenbinary 
search P os for the largest j(cid:3)
tially advance on C[cj(cid:3)..j] until ﬁnding the largest j such that pos = P os[j] +
(cid:2)
] +
(cid:2)

cj(cid:2)≤k<j coverC[k] ≤ i + 1. At the same time, we compute (cid:8) = V alue[j(cid:3)
cj(cid:2)≤k<j sumC[k].
Now, if (cid:8) + minC[j] < LCP [i], it is possible that N SV (i) is within the same
C[j]. In this case, we search recursively the tree expansion with root C[j] for
the leftmost value to the right of i and smaller than LCP [i]: Let C[j] → ab
in the grammar. We recursively visit child a if (cid:8) + mina < LCP [i] and pos +
covera > i + 1. If we ﬁnd no answer there, or we had decided not to visit a,
then we set (cid:8) = (cid:8) + suma and pos = pos + covera and recursively visit child
b if (cid:8) + minb < LCP [i] and pos + coverb > i + 1. If we also ﬁnd no answer
inside b, or we had decided not to visit b, we return with no value. On the other
hand, if we reach a leaf l during the recursion, we sequentially scan the array
LCP [pos, pos + coverl − 1], updating (cid:8) = (cid:8) + LCP [k] and increasing pos. If at
some position we ﬁnd a value smaller than LCP [i], we report the position pos.
If we return with no value from the ﬁrst recursive call at C[j], it was because
the only values smaller than LCP [i] were to the left of i. In this case, or if we
had decided not to enter into C[j] because (cid:8) + minC[j] ≥ LCP [i], we sequentially
scan C[j, n], while updating (cid:8) = (cid:8) + sumC[k] and pos = pos + coverC[k], until
ﬁnding the ﬁrst k such that (cid:8) + minC[k] < LCP [i]. Once we ﬁnd such k, we are
sure that the answer is inside C[k]. Thus we enter into C[k] with a procedure
very similar to the one for C[j] (albeit slightly simpler as we know that all the
positions are larger than i). In this case, as the LCP values are discrete, we know
that if (cid:8) + minC[k] = LCP [i]− 1, there is no smaller value to the left of the min
value, so in this case we directly answer the corresponding lmin value, without
accessing the LCP array. The solution to P SV (i) is symmetric.
To answer RM Q(x, y), we ﬁnd the rules C[i] and C[j] containing x and y,
respectively. We sequentially scan C[i+1, j−1] and store the smallest (cid:8)+minC[k]

value found (in case of ties, the leftmost). If the minimum is smaller than the
corresponding values (cid:8) + minC[i] and (cid:8) + minC[j], we directly return the value
pos+lminC[k] corresponding to position C[k]. Else, if the global minimum in C[i]
is equal to or less than the minimum for i < k < j, we must examine C[i] to ﬁnd

38

A. Abeliuk and G. Navarro

the smallest value to the right of x − 1. Assume C[i] → ab. We recursively enter
into a if pos + covera > x, otherwise we skip it. Then, we update (cid:8) = (cid:8) + suma
and pos = pos + covera, and enter into b if pos < x, otherwise we directly
consider (cid:8) + minb as a candidate for the minimum sought. Finally, if we arrive
at a leaf we scan it, updating (cid:8) and pos, and consider all the values (cid:8) where
pos ≥ x as candidates to the minimum. The minimum for C[i] is the smallest
among all candidates to minimum considered, and with pos + lminb or the leaf
scanning process we know its global position. This new minimum is compared
with the minimum of C[k] for i < k < j. Symmetrically, in case k = j contains
a value smaller than the minimum for i ≤ j < j, we have to examine C[j] for
the smallest value to the left of y + 1.

6 Experimental Evaluation

We used various DNA collections from the Repetitive Corpus at PizzaChili
(http://pizzachili.dcc.uchile.cl/repcorpus, created and thoroughly studied
by Kreft [12]). We took DNA collections Para and Influenza, which are the
most repetitive ones, and Escherichia, a less repetitive one. These are collections 
of genomes of various bacteria of those species. We also use DNA, which
is plain DNA from PizzaChili, as a non-repetitive baseline. On the other hand,
in order to show how results scale with repetitiveness, and although it is not a
biological collection, we also included Einstein, corresponding to the Wikipedia
versions of the article about Einstein in German.

All experimental results were performed on a 8 GB RAM computer with Intel
Core2 Duo, each processor of 3 GHz, 6 MB cache. Our implementation will be
publicly available at the ICDB Web page (http://www.icdb.cl/software.html).
For the RLCSA we used a ﬁxed sampling that gave reasonable performance:
one absolute value out of 32 is stored to access Ψ (i), and one text position every
128 is sampled to compute A[i]. Similarly, we used sampling step 32 for the
δ-encoding of the bitmaps Z and O that encode P LCP .

Table 1 shows the resulting sizes. The bpc of the CST is partitioned into those
for the RLCSA, for the PLCP, and for NPR, which stands for the data structure 
that solves N SV /P SV /RM Q queries. For the latter we used the smallest
setting that oﬀered answers within 2 milliseconds (msec). It can be seen that

Table 1. Text sizes, size of our CST (which replaces the text), bpc for the diﬀerent
components, and total bpc of the diﬀerent collections considered. The NPR structure
is the smallest setting between NPR-RP and NPR-RPBal for that particular text.

Name
Para
Influenza
Escherichia
DNA
Einstein

Text MB CST MB RLCSA (P)LCP NPR Total
1.30
1.47
3.58
9.83
0.28

0.26
0.21
0.92
3.62
0.01

0.20
0.30
0.20
0.30
0.10

410
148
108
50
89

67
27
48
61
3

0.84
0.96
2.46
5.91
0.17

Compressed Suﬃx Trees for Repetitive Texts

39

NPR-RPBal

NPR-RPBal

NPR-CN

NPR-CN

Left  y-axis:

Right y-axis:

NPR-RP

NPR-RP

NSV on Para.400MB 

400

350

300

250

200

150

100

50

0

200

150

100

50

 
s
e
s
s
e
c
c
a
P
C
L
 
f
o
#

 

 

 
s
e
s
s
e
c
c
a
P
C
L
 
f
o
#

 

 

 
s
e
s
s
e
c
c
a
P
C
L
 
f
o
#

 

 

 
s
e
s
s
e
c
c
a
P
C
L
 
f
o
#

 

 

350

300

250

200

150

100

50

0

300

250

200

150

100

50

0

RMQ on Para.400MB 

0

0,5

1

1,5

2

2,5

Bits per character 

RMQ on Influenza.150MB 

400

350

300

250

200

150

100

50

0

200

150

100

50

 
s
e
s
s
e
c
c
a
P
C
L
 
f
o
#

 

 

 
s
e
s
s
e
c
c
a
P
C
L
 
f
o
#

 

 

0

0,5

1

Bits per character 

RMQ on Escherichia.108MB 

0

1,5

300

250

200

150

100

50

0

 
s
e
s
s
e
c
c
a
P
C
L
 
f
o
#

 

 

0

0,5

1

1,5

2

Bits per character 

RMQ on DNA.50MB 

350

300

250

200

150

100

50

0

 
s
e
s
s
e
c
c
a
P
C
L
 
f
o
#

 

 

0

0,2

0,4

0,6

0,8

1

1,2

1,4

Bits per character 

RMQ on Einstein.90MB 

0

0,2

0,4

0,6

0,8

 
)
s
d
n
o
c
e
s
i
l
l
i

m

(
 
n
o
i
t
a
r
e
p
o
 
r
e
p
e
m
T

i

 

 
)
s
d
n
o
c
e
s
i
l
l
i

m

(
 
n
o
i
t
a
r
e
p
o
 
r
e
p
e
m
T

i

 

25

20

15

10

5

0

16

14

12

10

8

6

4

2

0

 
)
s
d
n
o
c
e
s
i
l
l
i

m

(
 
n
o
i
t
a
r
e
p
o
 
r
e
p
e
m
T

i

 

 
)
s
d
n
o
c
e
s
i
l
l
i

m

(
 
n
o
i
t
a
r
e
p
o
 
r
e
p
e
m
T

i

 

 
)
s
d
n
o
c
e
s
i
l
l
i

25

20

15

10

5

0

16

14

12

10

8

6
4
2

0

16

8

4

2

1

m

(
 
n
o
i
t
a
r
e
p
o
 
r
e
p
e
m
T

i

 

0,5

300

250

200

150

100

50

0

 
s
e
s
s
e
c
c
a
P
C
L
 
f
o
#

 

 

0,25

Bits per character 

 
)
s
d
n
o
c
e
s
i
l
l
i

m

(
 
n
o
i
t
a
r
e
p
o
 
r
e
p
e
m
T

i

 

 
)
s
d
n
o
c
e
s
i
l
l
i

m

(
 
n
o
i
t
a
r
e
p
o
 
r
e
p
e
m
T

i

 

 
)
s
d
n
o
c
e
s
i
l
l
i

m

(
 
n
o
i
t
a
r
e
p
o
 
r
e
p
e
m
T

i

 

 
)
s
d
n
o
c
e
s
i
l
l
i

m

(
 
n
o
i
t
a
r
e
p
o
 
r
e
p
e
m
T

i

 

25

20

15

10

5

0

16

14

12

10

8

6

4

2

0

25

20

15

10

5

0

18
16
14
12
10
8
6
4
2
0

16

 
)
s
d
n
o
c
e
s
i
l
l
i

m

(
 
n
o
i
t
a
r
e
p
o
 
r
e
p
e
m
T

i

 

0

0,5

1

1,5
Bits per character 

2

2,5

NSV on Influenza.150MB 

0

0,5

1

Bits per character 

NSV on Escherichia.108MB 

0

1,5

250

200

150

100

50

0

 
s
e
s
s
e
c
c
a
P
C
L
 
f
o
#

 

 

0

0,5

1

1,5

2

Bits per character 

NSV on DNA.50MB 

0

0,2

0,4

0,6

0,8

1

1,2

1,4

Bits per character 

NSV on Einstein.90MB 

8

4

2

1

0

0,2

0,4

0,6

0,8

0,5

Bits per character 

Fig. 2. Space/time performance of NPR operations. The x-axis shows the size of the
NPR structure. Note the logscale on y for Einstein.

40

A. Abeliuk and G. Navarro

we obtain, overall, 1.3–1.5 bpc for the most repetitive DNA collections. This
value deteriorates until approaching, for non-repetitive DNA, the same 10 bpc
that are reported in the literature for existing CSTs. Thus our data structure
adapts smoothly to the repetitiveness (or lack of it) of the collection. On the
other hand, on Einstein, which is much more repetitive, the space gets as low as
0.28 bpc. This is a good indication of what we can expect on future databases
with thousands of individuals of the same species, as opposed to these testbeds
with a few tens of individuals, or with more genetic variation.

Let us discuss the NPR operations now. We used a public Re-Pair compressor
by ourselves (http://www.dcc.uchile.cl/gnavarro/software), which oﬀers two
alternatives when dealing with symbols of the same frequencies. The basic one,
that we will call NPR-RP, stacks the symbols, whereas the second one, NPRRPBal,
 enqueues them and obtains much more balanced grammars in practice.
For our structure we tested values t = c = 64, 128, 256, 512. We also include the
basic regular structure of C´anovas and Navarro [3] (running over our RLCSA and
PLCP representations), to show that our grammar-based version oﬀers better
space/time tradeoﬀs than their regular tree Tm. For this version, RP-CN, we
used values L = 36, 64, 128, 256, 512.

We measure the times of operations N SV (as P SV is symmetric) and RM Q
following the methodology of C´anovas and Navarro [3]. We choose 10,000 random 
suﬃx tree leaves (corresponding to uniformly random suﬃx array intervals 
[vl, vr] = [v, v], v ∈ [1, n]) and navigate towards the root using operation
parent (vl, vr) = [P SV (vl), N SV (vr)]. At each such node, we also measure the
string depth, which corresponds to query strdep(vl, vr) = LCP [RM Q(vl +1, vr)].
We average the times of all the N SV and RM Q queries performed.

Figure 2 shows the space/time performance of NPR-CN, NPR-RP, and NPRRPBal.
 In addition, Figure 2 shows the number of explicit accesses to LCP made
per NPR operation, showing that in practice the main cost of the NPR operations
lies in retrieving the LCP values. Clearly, NPR-RP and NPR-RPBal dominate
the space/time map for all queries. They always make better use of the space
than the regular tree of NPR-CN. NPR-RPBal is usually better than NPR-RP,
especially in RMQ queries, where NPR-RP suﬀers from extremely unbalanced
trees that force the algorithm to examine many nodes, one by one. There are
some particular cases, like N SV on Escherichia, where NPR-RP is the fastest.

References

1. Apostolico, A.: The myriad virtues of subword trees, pp. 85–96. Combinatorial

Algorithms on Words. NATO ISI Series. Springer (1985)

2. Brisaboa, N.R., Ladra, S., Navarro, G.: Directly Addressable Variable-Length
Codes. In: Karlgren, J., Tarhio, J., Hyyr¨o, H. (eds.) SPIRE 2009. LNCS, vol. 5721,
pp. 122–130. Springer, Heidelberg (2009)

3. C´anovas, R., Navarro, G.: Practical Compressed Suﬃx Trees. In: Festa, P. (ed.)

SEA 2010. LNCS, vol. 6049, pp. 94–105. Springer, Heidelberg (2010)

4. Claude, F., Fari˜na, A., Mart´ınez-Prieto, M., Navarro, G.: Compressed q-gram indexing 
for highly repetitive biological sequences. In: Proc. 10th BIBE, pp. 86–91
(2010)

Compressed Suﬃx Trees for Repetitive Texts

41

5. Claude, F., Navarro, G.: Self-indexed Text Compression Using Straight-Line Programs.
 In: Kr´aloviˇc, R., Niwi´nski, D. (eds.) MFCS 2009. LNCS, vol. 5734, pp.
235–246. Springer, Heidelberg (2009)

6. Ferragina, P., Gonz´alez, R., Navarro, G., Venturini, R.: Compressed text indexes:

From theory to practice. ACM J. Exp. Algor. 13, article 12 (2009)

7. Fischer, J.: Wee LCP. Inf. Proc. Lett. 110, 317–320 (2010)
8. Fischer, J., M¨akinen, V., Navarro, G.: Faster entropy-bounded compressed suﬃx

trees. Theor. Comp. Sci. 410(51), 5354–5364 (2009)

9. Gog, S.: Compressed Suﬃx Trees: Design, Construction, and Applications. Ph.D.

thesis, Univ. of Ulm, Germany (2011)

10. Gonz´alez, R., Navarro, G.: Compressed Text Indexes with Fast Locate. In: Ma, B.,
Zhang, K. (eds.) CPM 2007. LNCS, vol. 4580, pp. 216–227. Springer, Heidelberg
(2007)

11. Gusﬁeld, D.: Algorithms on Strings, Trees and Sequences: Computer Science and

Computational Biology. Cambridge University Press (1997)

12. Kreft, S.: Self-index based on LZ77. Master’s thesis, Univ. of Chile (2010),

arXiv:1112.4578v1

13. Kreft, S., Navarro, G.: Self-indexing Based on LZ77. In: Giancarlo, R., Manzini,

G. (eds.) CPM 2011. LNCS, vol. 6661, pp. 41–54. Springer, Heidelberg (2011)

14. Kurtz, S.: Reducing the space requirements of suﬃx trees. Report 98-03, Technische

Fakult¨at, Univ. Bielefeld, Germany (1998)

15. Larsson, J., Moﬀat, A.: Oﬀ-line dictionary-based compression. Proc. of the

IEEE 88(11), 1722–1732 (2000)

16. M¨akinen, V., Navarro, G.: Succinct suﬃx arrays based on run-length encoding.

Nordic J. Comp. 12(1), 40–66 (2005)

17. M¨akinen, V., Navarro, G., Sir´en, J., V¨alim¨aki, N.: Storage and retrieval of highly

repetitive sequence collections. J. Comp. Biol. 17(3), 281–308 (2010)

18. Manber, U., Myers, E.: Suﬃx arrays: a new method for on-line string searches.

SIAM J. Comp., 935–948 (1993)

19. Manzini, G.: An analysis of the Burrows-Wheeler transform. J. ACM 48(3), 407–

430 (2001)

20. McCreight, E.: A space-economical

suﬃx tree construction algorithm. J.

ACM 32(2), 262–272 (1976)

21. Munro, I.: Tables. In: Chandru, V., Vinay, V. (eds.) FSTTCS 1996. LNCS,

vol. 1180, pp. 37–42. Springer, Heidelberg (1996)

22. Navarro, G., M¨akinen, V.: Compressed full-text indexes. ACM Comp. Surv. 39(1),

article 2 (2007)

23. Ohlebusch, E., Fischer, J., Gog, S.: CST++. In: Chavez, E., Lonardi, S. (eds.)

SPIRE 2010. LNCS, vol. 6393, pp. 322–333. Springer, Heidelberg (2010)

24. Raman, R., Raman, V., Rao, S.: Succinct indexable dictionaries with applications

to encoding k-ary trees and multisets. In: Proc. 13th SODA, pp. 233–242 (2002)

25. Russo, L., Navarro, G., Oliveira, A.: Fully-compressed suﬃx trees. ACM Trans.

Alg. 7(4), article 53 (2011)

26. Sadakane, K.: Compressed suﬃx trees with full

functionality. Theor. Comp.

Sys. 41(4), 589–607 (2007)

27. Weiner, P.: Linear pattern matching algorithms. In: IEEE Symp. Swit. and Aut.

Theo., pp. 1–11 (1973)

