6
1
0
2

 
r
a

 

M
3
2

 
 
]
S
D
.
s
c
[
 
 

2
v
9
3
9
6
0

.

1
0
6
1
:
v
i
X
r
a

Simple and Eﬃcient Fully-Functional Succinct Trees

Department of Computer Science, University of Chile

Joshimar Cordova and Gonzalo Navarro
{jcordova|gnavarro}@dcc.uchile.cl

March 24, 2016

Abstract

The fully-functional succinct tree representation of Navarro and Sadakane (ACM Transactions 
on Algorithms, 2014) supports a large number of operations in constant time using
2n + o(n) bits. However, the full idea is hard to implement. Only a simpliﬁed version with
O(lg n) operation time has been implemented and shown to be practical and competitive. We
describe a new variant of the original idea that is much simpler to implement and has worst-case
time O(lg lg n) for the operations. An implementation based on this version is experimentally
shown to be superior to existing implementations.

1

Introduction

(cid:0)2n−2

(cid:1), and its

n

n−1

Combinatorial arguments show that it is possible to represent any ordinal tree of n nodes using less
than 2n bits of space: the number of such trees is the (n − 1)th Catalan number, 1
logarithm (in base 2 and written lg across this paper) is 2n − Θ(lg n). A simple way to encode any
ordinal tree in 2n bits is the so-called balanced parentheses (BP) representation: traverse the tree
in depth-ﬁrst order, writing an opening parenthesis upon reaching a node, and a closing one upon
deﬁnitely leaving it. Much more challenging is, however, to eﬃciently navigate the tree using that
representation.
The interest in navigating a 2n-bit representation of a tree, compared to using a classical O(n)-
pointers representation, is that those succinct data structures allow ﬁtting much larger datasets
in the faster and smaller levels of the memory hierarchy, thereby improving the overall system
performance. Note that compression is not suﬃcient; it must be possible to operate the data in
its compressed form. The succinct representation of ordinal trees is one of the most clear success
stories in this ﬁeld. Table 1 lists the operations that can be supported in constant time within
2n + o(n) bits of space. These form a rich set that suﬃces for most applications.

The story starts with Jacobson [10], who proposed a simple levelwise representation called
LOUDS, which reduced tree navigation to two simple primitives on bitvectors: rank and select
(all these primitives will be deﬁned later). However, the repertoire of tree operations was limited.
Munro and Raman [15] used for the ﬁrst time the BP representation, and showed how three basic
primitives on the parentheses: open, close, and enclose, plus rank and select, were suﬃcient to
support a signiﬁcantly wider set of operations. The operations were supported in constant time,
however the solution was quite complex in practice. Geary et al. [8] retained constant times with
a much simpler solution to open, close, and enclose, based on a two-level recursion scheme. Still,

1

Table 1: Operations on ordinal trees, where i and j are node identiﬁers.

operation
root
preorder (i) / postorder (i)
preorderselect(k) / postorderselect(k)
isleaf (i)
isancestor (i, j)
depth(i)
parent(i)
fchild (i) / lchild (i)
nsibling(i) / psibling(i)
subtree(i)
levelancestor (i, d)
levelnext(i) / levelprev (i)
levelleftmost(d) / levelrightmost(d)
lca(i, j)
deepestnode(i)
height(i)
degree(i)
child (i, q)
childrank (i)
leafrank (i)
leafselect(k)
numleaves(i)
leftmostleaf (i) / rightmostleaf (i)

description
the tree root
preorder/postorder rank of node i
the node with preorder/postorder k
whether the node is a leaf
whether i is an ancestor of j
depth of node i
parent of node i
ﬁrst/last child of node i
next/previous sibling of node i
number of nodes in the subtree of node i
ancestor j of i such that depth(j) = depth(i) − d
next/previous node of i with the same depth
leftmost/rightmost node with depth d
the lowest common ancestor of two nodes i, j
the (ﬁrst) deepest node in the subtree of i
the height of i (distance to its deepest node)
q = number of children of node i
q-th child of node i
q = number of siblings to the left of node i
number of leaves to the left and up to node i
kth leaf of the tree
number of leaves in the subtree of node i
leftmost/rightmost leaf of node i

not all the operations of Table 1 were supported. Missing ones were added one by one: children [5],
levelancestor [14], child , childrank , height, and lca [13]. Each such addition involved extra o(n)-bit
substructures that were also hard to implement.

An alternative to BP, called DFUDS, was introduced by Benoit et al. [4].

It also used 2n
balanced parentheses, but they had a diﬀerent interpretation. Its main merit was to support child
and related operations very easily and in constant time. It did not support, however, operations
childrank , depth, levelancestor , and lca, which were added later [9, 11], again each using o(n) bits
and requiring a complex implementation to achieve constant time.

Navarro and Sadakane [16] introduced a new representation based on BP, said to be fullyfunctional 
because it supported all of the operations in Table 1 in constant time and using a
single set of structures. This was a signiﬁcant simpliﬁcation of previous results and enabled the
development of an eﬃcient implementation. The idea was to reduce all the tree operations to a
small set of primitives over parentheses:
fwdsearch, bwdsearch, rmq, and a few variations. The
main structure to implement those primitives was the so-called range min-max tree (rmM-tree),
which is a balanced tree of arity lg n (for a constant 0 <  < 1) that supports the primitives
in constant time on buckets of O(polylog n) parentheses. To handle queries that were not solved
within a bucket, other structures had to be added, and these were far less simple.
A simple O(lg n)-time implementation using a single binary range min-max tree for the whole
sequence [1] was shown to be faster (or use much less space, or both) than other implementable
constant-time representations [8] in several real-life trees and navigation schemes. Only the LOUDS
representation was shown to be competitive, within its limited functionality. While the O(lg n)

2

growth was shown to be imperceptible in many real-life traversals, some stress tests pursued later
[12] showed that it does show up in certain plausible situations.

No attempt was made to implement the actual constant-time proposal [16]. The reason is
that, while constant-time and o(n)-bit space in theory, the structures used for inter-bucket queries,
as well as the variant of rmM-trees that operates in constant time, involve large constants and
include structures that are known to be hard to implement eﬃciently, such as fusion trees [7] and
compressed bitvectors with optimal redundancy [18]. Any practical implementation of these ideas
leads again to the O(lg n) times already obtained with binary rmM-trees.
In this paper we introduce an alternative construction that builds on binary rmM-trees and is
simple to implement. It does not reach constant times, but rather O(lg lg n) time, and requires
2n + O(n/ lg n) bits of space. We describe a new implementation building on these ideas, and
experimentally show that it outperforms a state-of-the-art implementation of the O(lg n)-time solution,
 both in time and space, and therefore becomes the new state-of-the art implementation of
fully-functional succinct trees.

2 Basic Concepts

2.1 Bits and balanced parentheses

Given a bitvector B[1, 2n], we deﬁne rank t(i) as the number of occurrences of the bit t in B[1, i].
We also deﬁne select t(k) as the position in B of the kth occurrence of the bit t. Both primitives can
be implemented in constant time using o(n) bits on top of B [6]. Note that rank 1(i) + rank 0(i) = i
and rank t(select t(k)) = k.

A sequence of 2n parentheses will be represented as a bitvector B[1, 2n] by interpreting ‘(’ as a 1
and ‘)’ as a 0. On such a sequence we deﬁne the operation excess(i) as the number of opening minus
closing parentheses in B[1, i], that is, excess(i) = rank 1(i) − rank 0(i) = 2rank 1(i) − i. We say that
B is balanced if excess(i) ≥ 0 for all i, and excess(2n) = 0. Note that excess(i) = excess(i − 1) ± 1.
In a balanced sequence, every opening parenthesis at B[i] has a matching closing parenthesis
at B[j] for j > i, and every other parenthesis opening inside B[i + 1, j − 1] has its matching
parenthesis inside B[i + 1, j − 1] as well. Thus the parentheses deﬁne a hierarchy. Moreover, we
have excess(j) = excess(i) − 1 and excess(m) ≥ excess(i) for all i < m < j. This motivates the
deﬁnition of the following primitives on parentheses [15]:

close(i): the position of the closing parenthesis that matches B[i] = 1, that is, the smallest j > i

such that excess(j) = excess(i) − 1.

open(i): the position of the opening parenthesis that matches B[i] = 0, that is, the largest j < i

such that excess(j − 1) = excess(i).

enclose(i): the opening parenthesis of the smallest matching pair that contains position i, that is,

the largest j < i such that excess(j − 1) = excess(i) − 2.

It turns out that a more general set of primitives is useful to implement a large number of tree

operations [16], which look forward or backward for an arbitrary relative excess:

fwdsearch(i, d) = min{j > i, excess(j) = excess(i) + d},
bwdsearch(i, d) = max{j < i, excess(j) = excess(i) + d}.

3

Figure 1: An ordinal tree on the left (the node identiﬁers are their preorder numbers) and its BP
representation on the right, indicating which parentheses represent each node. For example the
node with preorder 5 has identiﬁer 7, which is its position in the sequence of parentheses.

In particular, we have close(i) = fwdsearch(i,−1), open(i) = bwdsearch(i, 0) + 1, and enclose(i) =
bwdsearch(i,−2) + 1.

To implement other tree operations, we also need the following primitives, which refer to minimum 
and maximum excess in a range of B:

rmq(i, j): position of the leftmost minimum in excess(i), excess(i + 1), . . . , excess(j).

rMq(i, j): position of the leftmost maximum in excess(i), excess(i + 1), . . . , excess(j).

mincount(i, j): number of occurrences of the minimum in excess(i), excess(i + 1), . . . , excess(j).

minselect(i, j, q): position of the qth minimum in excess(i), excess(i + 1), . . . , excess(j).

2.2 BP representation of ordinal trees

As said in the Introduction, an ordinal tree of n nodes is represented with 2n parentheses by
opening a parenthesis when we arrive at a node and closing it when we leave the node. The
resulting sequence is balanced, and the hierarchy it deﬁnes corresponds to subtree containment.
Let us identify each node with the position of its opening parenthesis in the sequence B. See
Figure 1.

Many tree operations are immediately translated into the primitives we have deﬁned [15]:
root = 1, depth(i) = excess(i), parent(i) = enclose(i), isleaf (i) iﬀ B[i + 1] = 0, fchild (i) =
i + 1 (if i is not a leaf), nsibling(i) = close(i) + 1 (if the result j holds B[j] = 0 then i has
no next sibling), psibling(i) = open(i − 1) (if B[i − 1] = 1 then i has no previous sibling),
lchild (i) = open(close(i) − 1) (if i is not a leaf), preorder (i) = rank 1(i), preorderselect(k) =
select 1(k), postorder (i) = rank 0(close(i)), postorderselect(k) = open(select 0(k)), isancestor (i, j) iﬀ
i ≤ j < close(i), and subtree(i) = (close(i) − i + 1)/2.
The primitives fwdsearch and bwdsearch yield other tree operations [16]: levelancestor (i, d) =
bwdsearch(i,−d− 1) + 1, levelnext(i) = fwdsearch(close(i), 1), levelprev (i) = open(bwdsearch(i, 0) +
1), levelleftmost(d) = fwdsearch(0, d), and levelrightmost(d) = open(bwdsearch(2n + 1, d)). The
other primitives yield the remaining operations: degree(i) = mincount(i+1, close(i)−1), child (i, q) =
minselect(i + 1, close(i) − 1, q − 1) + 1 for q > 1 (for q = 1 it is fchild (i)), childrank (i) =

4

)(((())(()))()((()()))10117643529811234567891011Figure 2: The rmM-tree of the parentheses sequence of Figure 1. The numbers below are excess(i).

mincount(parent(i) + 1, i) + 1 unless B[i − 1] = 1 (in which case childrank (i) = 1), lca(i, j) =
parent(rmq(i, j) + 1) unless isancestor (i, j) (so lca(i, j) = i) or isancestor (j, i) (so lca(i, j) = j),
deepestnode(i) = rMq(i, close(i)), and height(i) = excess(deepestnode(i)) − excess(i).

Finally, the operations on leaves are solved by extending the bitvector rank and select primitives
to count the occurrences of pairs 10 (which represent tree leaves, ‘()’): rank 10(i) is the number
of occurrences of 10 starting in B[1, i] and select 10(k) is the position of the kth occurrence of
10 in B. These are easily implemented as extensions of the basic rank and select primitives,
adding other o(n) bits on top of B. Then leafrank (i) = rank 10(i), leafselect(k) = select 10(k),
numleaves(i) = leafrank (close(i)) − leafrank (i − 1), leftmostleaf (i) = leafselect(leafrank (i − 1) + 1)
and ﬁnally rightmostleaf (i) = leafselect(leafrank (close(i))).

Therefore, all the operations of Table 1 are supported via the primitives fwdsearch, bwdsearch,

rmq, rMq, mincount, and minselect. We also need rank and select on 0, 1, and 10.

2.3 Range min-max trees

We describe the simple version of the structure used by Navarro and Sadakane [16] to solve the
primitives. We choose a block size b. Then, the (binary) range min-max tree, or rmM-tree, of
B[1, 2n] is a complete binary tree where the kth leaf covers B[(k− 1)b + 1, kb]. Each rmM-tree node
v stores the following ﬁelds: v.e is the total excess of the area covered by v, v.m is the minimum
excess in this area, v.M is the maximum excess in the area, and v.n is the number of times the
minimum excess occurs in the area. Since the rmM-tree is complete, it can be stored without
pointers, like a heap. See Figure 2.
Then, an operation like fwdsearch(i, d) is solved as follows. First, the block number k = (cid:100)i/b(cid:101) is
scanned from position i + 1 onwards, looking for the desired excess. If not found, then we reset the
desired relative excess to d ← d − (excess(kb) − excess(i)) and consider the leaf v of the rmM-tree

5

M=3n=1e=2m=1(()()))M=0n=1e=−2m=−2M=−1n=1e=−2m=−2M=2n=1e=2m=−1M=−1n=1e=−2m=−3M=2n=1e=2m=0M=4n=1e=4m=1M=0n=2e=0m=−3M=0n=1e=−4m=−4M=4n=3e=4m=1M=0n=1e=−4m=−4M=4n=1e=0m=0((()123232343212123434321()(()))())(0the ﬁelds v.m of the O(cid:16)

(cid:17)

b

that covers block k. Now we move upwards from v, looking for its nearest ancestor that contains
the answer. At every step, if v is a right child, we move to its parent.
If it is a left child, we
see if v(cid:48).m ≤ d ≤ v(cid:48).M , where v(cid:48) is the (right) sibling of v. If d is not in the range, then update
d ← d−v(cid:48).e and move to the parent of v. At some point in the search, we ﬁnd that v(cid:48).m ≤ d ≤ v(cid:48).M
for the sibling v(cid:48) of v, and then start descending from v(cid:48). Let vl and vr be its left and right children,
respectively. If vl.m ≤ d ≤ vl.M , then we descend to vl. Otherwise, we update d ← d − vl.e and
descend to vr. Finally, we arrive at a leaf, and scan its block until ﬁnding the excess d. Operation
bwdsearch(i, d) is analogous; we scan in the other direction.

lg j−i

For rmq(i, j), we scan the blocks of i and j and, if there are blocks in between, we consider
maximal nodes that cover the leaves contained in B[i, j]. Then we
identify the minimum excess in B[i, j] as the minimum found across the scans and the maximal
nodes. If the ﬁrst occurrence of the minimum is inside the scanned blocks, that position is rmq(i, j).
Otherwise, we must start from the node v that contained the ﬁrst occurrence of the minimum and
traverse downwards, looking if the ﬁrst occurrence was to the left or to the right (by comparing
the ﬁelds v.m). Operation rMq(i, j) is analogous. For mincount(i, j) we retraverse the blocks and
nodes, adding up the ﬁelds v.n of the nodes where v.m is the minimum. Finally, for minselect(i, j, q),
we do the same counting but traverse downward from the node v where the qth occurrence is found,
to ﬁnd its position.
Finally, for primitives rank t(i) and select t(k), we can compute on the ﬂy the number of 1s
inside any node v as v.r = (|v| + v.e)/2, where |v| is the size of the area of B covered by v. For
rank 1(i), we count the 1s in the block of i and then climb upwards from the leaf v covering i, adding
up v(cid:48).r for each left sibling of v found towards the root. For rank 0(i) we compute i − rank 1(i).
For select 1(k), we start from the root node v, going to the left child vl if vl.r ≥ k, and otherwise
updating k ← k − vl.r and going to the right child. For select 0(k) we proceed analogously, but
using |vl| − vl.r instead of vl.r. Finally, rank and select on 10 is implemented analogously, but we
need to store a ﬁeld v.rr storing the number of 10s.

By using small precomputed tables that allow us to scan any block of c = (lg n)/2 bits in
constant time (i.e., computing the analogous to ﬁelds e, m, M , and n for any chunk of c bits), the
total time of the operations is O(b/c + lg n) bits. The extra space of the rmM-tree over the 2n bits
of B is O((n/b) lg n) bits. For example, we can use a single rmM-tree for the whole B, set b = lg2 n,
and thus have all the operations implemented in time O(lg n) within 2n + O(n/ lg n) bits. This is
essentially the practical solution implemented for this structure [1].

3 An O(lg lg n) Time Solution
Now we show how to obtain O(lg lg n) worst-case time, still within O(n/ lg n) extra bits. The
main idea (still borrowing from the original solution [16]) is to cut B[1, 2n] into n(cid:48) = 2n/β buckets
of β = Θ(lg3 n) bits. We maintain one (binary) rmM-tree for each bucket. The block size of
the rmM-trees is set to b = lg n lg lg n. This maintains the extra space of each rmM-tree within
O((β/b) lg β) bits, adding up to O((n/b) lg β) = O(n/ lg n) bits. Their operation times also stay
O(b/c + lg β) = O(lg lg n).
Therefore, the operations that are solved within a bucket take O(lg lg n) time. The diﬃcult part
is how to handle the operations that span more than one bucket: a fwdsearch(i, d) or bwdsearch(i, d)
whose answer is not found within the bucket of i, or a rmq(i, j) or similar operation where i and j
are in diﬀerent buckets.

6

For each bucket k, we will store an entry e[k] = excess(kβ) with the excess at its end, and
entries m[k] = min(k−1)β<i≤kβ excess(i) and M [k] = max(k−1)β<i≤kβ excess(i) with the minimum
and maximum absolute excess reached inside the bucket. These entries require just O(n(cid:48) lg n) =

O(cid:0)n/ lg2 n(cid:1) bits of space. Heavier structures will be added for each operation, as described next.

3.1 Forward and backward searching

The solution for these queries is similar to the original one [16], but we can simplify it and make it
more practical by allowing us to take O(lg lg n) time to solve the operation. We describe its details
for completeness.
We ﬁrst try to solve fwdsearch(i, d) inside the bucket of i, k∗ = (cid:100)i/β(cid:101). If the answer is found in
there, we have completed the query in O(lg lg n) time. Otherwise, after scanning the block, we have
computed the new relative excess sought d (which is the original one minus excess(k∗β)−excess(i)).
This is converted into absolute with d ← d + e[k∗].
Now we have to ﬁnd the answer in the buckets k∗ + 1 onwards. We have to ﬁnd the smallest
k > k∗ with m[k] ≤ d ≤ M [k], and then ﬁnd the answer inside bucket k. Let us ﬁrst consider
the next bucket. If m[k∗ + 1] ≤ d ≤ M [k∗ + 1], then the desired excess is reached inside the next
bucket, and therefore we complete the query by running fwdsearch(0, d− e[k∗]) inside the rmM-tree
of bucket k∗ + 1. Otherwise, either d < m[k∗ + 1] or d > M [k∗ + 1]. Let us consider the ﬁrst case,
as the other is symmetric (and requires other similar data structures). The query bwdsearch(i, d)
works similarly, except that we look towards the left, therefore it is also analogous.
Since the excess changes by ±1 from one parenthesis to the next, it must hold M [k+1] ≥ m[k]−1
for all k, that is, there are no holes in the ranges [m[k], M [k]] of consecutive buckets. Therefore,
if d < m[k∗ + 1], then we simply look for the smallest k > k∗ + 1 such that m[k] ≤ d. Note that
for this search we would like to consider, given a k where m[k] > d, only the smallest k(cid:48) > k such
that m[k(cid:48)] < m[k], as those values m[k + 1], . . . , m[k(cid:48) − 1] ≥ m[k] are not the solution. If we deﬁne
a tree where k(cid:48) is the parent of k, then we are looking for the nearest ancestor k(cid:48)(cid:48) of node k where
m[k(cid:48)(cid:48)] < d.

The solution builds on a well-known problem called level-ancestor queries (an operation we have
already considered for our succinct trees). Given a node v and a distance t, we want the ancestor at
distance t from v. In the classical scenario, there is an elegant and simple solution to this problem

[3]. It requires O(cid:0)n(cid:48) lg2 n(cid:48)(cid:1) bits of space, but this is just O(n/ lg n). The idea is to extract the

longest root-to-leaf path and write it on an array called a ladder. Extracting this path disconnects
the tree into several subtrees. Each disconnected subtree is processed recursively, except that each
time we write a path p[1, (cid:96)] of nodes into a new ladder, we continue writing the ancestors up to
other (cid:96) nodes. That is, a path p[1, (cid:96)] is converted into a ladder of 2(cid:96) nodes (or less if we reach the
global root). Thus the ladders add up to at most 2n(cid:48) cells.

In the ladders, each node has a primary copy, corresponding to the path p[1, (cid:96)] where it belongs,
and zero or more secondary copies, corresponding to paths that are extended in other ladders. We
store a pointer to the primary copy of each node, and the id of its ancestors at distances t = 2l, for
l = 0, 1, . . .. This is where the n(cid:48) lg n(cid:48) words of space are used.
Now, to ﬁnd the tth ancestor of d, we compute l = (cid:98)lg t(cid:99), and ﬁnd in the tables the ancestor
u at distance 2l of v. Then we go to the ladder where the primary copy of u is written. Because
we extract the longest paths, since u has height at least 2l, the path p[1, (cid:96)] where it belongs must
be of length at least 2l, and therefore the ladder is of length at least 2(cid:96) ≥ 2 · 2l. Therefore, the

7

ladder contains the ancestors of u up to distance at least 2l, and thus the one we want, at distance
t − 2l < 2l, is written in the ladder. Thus we just read the answer in that ladder and ﬁnish.
We must extend this solution so that we ﬁnd the ﬁrst ancestor u with m[u] ≤ d. Recall that
the values m[u] form a decreasing sequence as we move higher in the sequence of ancestors, and
within any ladder. First, we can ﬁnd the appropriate l value with a binary search in the ancestors
at distance 2l, so that l is the smallest one such that the ancestor u at distance 2l still has m[u] > d.
This takes O(lg lg n(cid:48)) time.
Now, in the ladder of u, we must ﬁnd the ﬁrst cell u(cid:48) to its right with m[u(cid:48)] ≤ d. We solve this
by representing all the m[u(cid:48)] values as B[m[u(cid:48)]] = 1 in a bitvector B created for that ladder. Then
rank 1(B, d) is the distance from the end of the ladder to the position of the desired ancestor u(cid:48).
A useful bitvector representation for this matter is the sarray by Okanohara and Sadakane [17,
r + O(r)
Sec. 6].1 If the ladder contains r elements and the maximum value is µ, then it takes r lg µ
bits of space (which adds up to just O(n(cid:48) lg n) bits overall, since µ ≤ n is the maximum excess).

(cid:1) if we represent its internal bitvector H of O(r) bits with

a structure that solves rank and select in constant time [6]. Note that, since the excess changes
by ±1 across positions, it changes by ±β across buckets, and thus consecutive elements in the
ladder diﬀer by at most β. Therefore, it must be µ ≤ rβ, and the time for the rank operation is
O(lg β) = O(lg lg n).

It solves rank 1 queries in time O(cid:0)lg µ

r

3.2 Range minima and maxima

If both i and j fall inside the same bucket, then operations rmq(i, j) and relatives are solved
inside their bucket. Otherwise, the minimum might fall in the bucket of i, k1 = (cid:100)i/β(cid:101), in that
of j, k2 = (cid:100)j/β(cid:101), or in a bucket in between. Using the rmM-trees of buckets k1 and k2, we ﬁnd
the minimum µ1 in the range [i − (k1 − 1)β, β] of bucket k1, and convert it to a global excess,
µ1 ← µ1 + e[k1 − 1]. We also ﬁnd the minimum µ2 in the range [1, j − (k2 − 1)β] of bucket k2,
and convert it to µ2 ← µ2 + e[k2 − 1]. The problem is to ﬁnd the minimum in the intermediate
buckets, µ3 ← mink1<k<k2 m[k]. Once we have this, we easily solve rmq(i, j) as the position of µ1
if µ1 ≤ min(µ3, µ2), otherwise as the position of µ3 if µ3 ≤ µ2, and otherwise as the position of µ2
(recall that we want the leftmost position of the minimum).
In the original work [16], they use the most well-known classical solution to range minimum
queries [2]. While it solves the problem for query rmq, it decomposes the query range m[k1+1, k2−1]
into overlapping subintervals, and thus it cannot be used to solve the other related queries, such
as counting the number of occurrences of the minimum or ﬁnding its qth occurrence. As a result,
they resort to complex ﬁxes to handle each of the other related operations in constant time.
If we can allow ourselves to use O(lg lg n) time for the operations, then a much simpler and
elegant solution is possible, using a less known data structure for range minimum queries [19]. It
uses O(n(cid:48) lg n(cid:48)) words, which is O(n/ lg n) bits, and solves queries in constant time. The most
relevant feature of this solution is that it reduces the query on interval m[k1 + 1, k2 − 1] to disjoint
subintervals, which allows solving the related queries we are interested in.
Assume n(cid:48) is a power of 2 and consider a perfect binary tree on top of array m[1, n(cid:48)], of height
(cid:100)lg n(cid:48)(cid:101). The tree nodes with height h cover disjoint areas of m, of length 2h. The tree is stored as
a heap, so we identify the nodes with their position in the heap, starting from 1, and the children
of the node at position v are at positions 2v and 2v + 1.

1Other compressed representations use o(u) further bits, which make them unsuitable for us.

8

For each node v covering m[s, e], we store two arrays with the left-to-right and right-to-left
minima in m[s, e], that is, we store L[v][p] = min{m[s], . . . , m[s + p]} and R[v][p] = min{m[e −
p], . . . , m[e]} for all 0 ≤ p ≤ e − s. Their size adds up to O(n(cid:48) lg n(cid:48)) cells, or O(n(cid:48) lg n(cid:48) lg n) =
O(n/ lg n) bits.
Let us call k = k1 + 1 and k(cid:48) = k2 − 1. To ﬁnd the minimum in m[k, k(cid:48)], we compute the
lowest node v that covers [k, k(cid:48)]. Node v is found as follows: we compute the highest bit where the
numbers k − 1 and k(cid:48) − 1 diﬀer. If this is the hth bit (counting from the left), then node v is of
height h, and it covers the (cid:96)th area of m of size 2h (left-to-right), where (cid:96) = (cid:100)k/2h(cid:101). That is, it
holds v = n(cid:48)/2h + (cid:96) − 1 and the range it covers is m[s, e] = m[((cid:96) − 1)2h + 1, (cid:96) 2h].
The value of h can be computed as h = (cid:98)lg((k − 1) xor (k(cid:48) − 1))(cid:99)2. If operations lg and xor are
not allowed in the computation model, we can easily simulate them with small global precomputed
, which can process any sequence of lg(n(cid:48))/2 bits (note that computing lg

tables of size O(cid:16)√

n(cid:48)(cid:17)

requires just to ﬁnd the most signiﬁcant 1 in the computer word).
Now we have found the lowest node v that covers [s, e] ⊇ [k, k(cid:48)] in the perfect tree. Therefore,
for p = (s + e− 1)/2, the left child 2v of v covers m[s, p] and its right child 2v + 1 covers m[p + 1, e].
Then, the minimum of m[k, k(cid:48)] is either that of m[k, p] (which is available at R[2v][p − k]) or that
of m[p + 1, k(cid:48)] (available at L[2v + 1][k(cid:48) − p − 1]). We return the minimum of both.

This general mechanism is used to solve all the queries related to rmq, as we see next.

3.2.1 Solving rmq(i, j) and rMq(i, j)

The only missing piece for solving rmq(i, j) is to ﬁnd the leftmost position of the minimum in
m[k, k(cid:48)]. To do this we store other two arrays, Lp and Rp, with the leftmost positions of the
minima of the bucket ranges represented in L and R, respectively. That is, if v covers m[s, e], then
Lp[v][p] = rmq((s − 1)β + 1, (s + p)β) and Rp[v][p] = rmq((e − p)β + 1, eβ).
Thus, once we have the node v that covers [k, k(cid:48)], there are two choices: If R[2v][p − k] ≤
L[2v + 1][k(cid:48) − p − 1] (i.e., the minimum appears in the subrange m[k, p]), the leftmost position is
Rp[2v][p− k]. Otherwise (i.e., the minimum appears only in the subrange m[p + 1, k(cid:48)]) the leftmost
position is Lp[2v + 1][k(cid:48) − p − 1].

Note that any entry from the array L/R can be obtained on the ﬂy from the corresponding
entry of Lp/Rp and the bucket array m[], hence L/R are only conceptual and we do not store them.
Furthermore, the arrays Lp/Rp are only accessed by nodes that are the right/left children of their
parent, thus we only store one of them in each node.

Operation rMq(i, j) is solved analogously (needing similar structures R, L, Lp and Rp regarding

the maxima).

3.2.2 Solving mincount(i, j)

To count the number of times the minimum appears, we ﬁrst compute µ = min(µ1, µ2, µ3), and
then add up its occurrences in each of the three ranges: we add up mincount(i − (k1 − 1)β, β) in
bucket k1 if µ = µ1, mincount(1, j − (k2 − 1)β) in bucket k2 if µ = µ2, and the number of times
the minimum appears in [(k − 1)β + 1, k(cid:48)β] (i.e., inside buckets k to k(cid:48)) if µ = µ3. To compute
this last number, we store two new arrays, Ln and Rn, giving the number of times the minimum

2The xor operator takes two integers and performs the bitwise logical exclusive-or operation on them, that is, on

each pair of corresponding bits.

9

occurs in the corresponding areas of L and R, that is, Ln[v][p] = mincount((s − 1)β + 1, (s + p)β)
and Rn[v][p] = mincount((e − p)β + 1, eβ).
Thus, if R[2v][p − k] < L[2v + 1][k(cid:48) − p − 1], then the minimum appears only on the left, and
the count in buckets k to k(cid:48) is Rn[2v][p − k]. If R[2v][p − k] > L[2v + 1][k(cid:48) − p − 1], it appears only
on the right, and the count is Ln[2v + 1][k(cid:48) − p− 1]. Otherwise, it appears in both and the count is
Rn[2v][p − k] + Ln[2v + 1][k(cid:48) − p − 1]. Once again, a node needs to store only Ln or Rn, not both.

3.2.3 Solving minselect(i, j, q)

To solve minselect(i, j, q) we must see if q falls in the bucket of k1, in the bucket of k2, or in between.
We start by considering k1, if µ = µ1. In this case, we compute q1 = mincount(i − (k1 − 1)β, β),
the number of times µ occurs inside bucket k1. If q ≤ q1, then the qth occurrence is inside it, and
we answer minselect(i − (k1 − 1)β, β, q). If q > q1, then we continue, with q ← q − q1.
If µ appears between k1 and k2, that is, if µ = µ3, we compute q3 = mincount((k − 1)β + 1, k(cid:48)β)
as in Section 3.2.2. Again, if q ≤ q3, the answer is the qth occurrence of the minimum in buckets
k to k(cid:48). If q > q3, we just set q ← q − q3. Finally, if we have not yet solved the query, we return
minselect(1, j − (k2 − 1)β, q) within bucket k2.
To ﬁnd the qth occurrence of µ in the buckets k to k(cid:48), we make use of the arrays Ln and Rn. If
µ < R[2v][p−k], then the answer is to be found in the buckets p+1 to k(cid:48). If, instead, µ = R[2v][p−k],
then there are Rn[2v][p − k] occurrences of µ in m[k, p]. Thus, if q ≤ Rn[2v][p − k], we must ﬁnd
the qth occurrence of µ in buckets k to p. If instead q > Rn[2v][p− k], we set q ← q − Rn[2v][p− k]
and ﬁnd the qth occurrence of the minimum in buckets p + 1 to k(cid:48).

Let us ﬁnd the qth occurrence of µ in buckets k to p (the other case is symmetric, using L
instead of R). The minimum in m[k, p] is µ. It also holds that the minimum in m[k + l, p] is µ,
for all 0 ≤ l ≤ g, for some number g ≥ 0, and then m[k + g + 1, p] > µ. Those intervals are
represented in the cells R[2v][p− k] to R[2v][p− k− g], and the number of times µ occurs in them is
in Rn[2v][p− k] to Rn[2v][p− k− g]. Therefore, our search for the qth minimum spans a contiguous
area of Rn[2v]: we want to ﬁnd the largest l ≥ 0 such that Rn[2v][p − k − l] ≥ q. This means that
the qth occurrence of µ in buckets k to p is in bucket k + l, in whose rmM-tree we must return
minselect(1, β, Rn[2v][p − k − l] − q + 1).
To ﬁnd l fast, we record all the values Rn[2v][·] in complemented unary (i.e., number x ≥ 0
as 0x1) in a bitvector C. Then, each 0 counts an occurrence of the minimum and each 1 counts
a bucket. To ﬁnd l, we compute y = select 1(C, p − k) − (p − k), the sum of the values up to
Rn[2v][p − k], and then l(cid:48) = select 0(C, y − q + 1) − (y − q) is the desired cell Rn[2v][p − k − l], thus
l = p − k − l(cid:48).

We use again the sarray bitvector of Okanohara and Sadakane [17]. It solves select 1 in constant
time and select 0 in the same time as rank . There is a 1 per cell in Rn, so the global space is at
most (n(cid:48) lg n + O(n(cid:48))) lg n(cid:48) = O(n/ lg n) bits. Since the distance between consecutive 1s is at most
β, the time to compute select 0 is O(lg β) = O(lg lg n).
Note, in passing, that bitvector C can replace Rn[2v], as it can compute any cell Rn[2v][x] =
select 1(C, x)− select 1(C, x− 1)− 1 in constant time. Therefore we can use those bitvectors instead
of storing arrays Rn and Ln, thus avoiding to increase the space further.

10

3.3 Rank and select operations

The various basic and extended rank x and select x operations are implemented similarly as the
more complex operations. For rank x, we store the rank x value at the beginning of each bucket,
in an array rx[1, n(cid:48)], and then compute rank x(i) = rx[k] + rank x(i − (k − 1)β) inside the rmMtree 
of bucket k = (cid:100)i/β(cid:101). For select x(j), we store the rx[k] values in a bitvector Bx[1, n] with
Bx[k + rx[k]] = 1 for all k, then the bucket k where the answer lies is k = select 0(Bx, j) − j + 1,
inside whose rmM-tree we must solve select x(j − rx[k]). Again, with the bitvectors of Okanohara
and Sadakane [17], we do not need to store rx because its cells are computed in constant time as
and the time to compute select 0 is O(lg lg n) because there are at most β 0s per 1 in Bx.

rx[k] = select 1(Bx, k) − select 1(Bx, k − 1) − 1, the space used is n(cid:48) lg n + O(n(cid:48)) = O(cid:0)n/ lg2 n(cid:1) bits,

4

Implementation and Experimental Results

We now describe an engineered implementation based on our theoretical description, and experimentally 
evaluate it. Engineered implementations often replace solutions with guaranteed asymptotic 
complexity by simpler variants that perform better in most practical cases. Our new theoretical 
version is much simpler than the original [16], and thus most of it can be implemented
verbatim. Still, we further simplify some parts to speed them up in practice. As a result, our
implementation does not fully guarantee O(lg lg n) time complexity, but it turns out to be faster
than the state-of-the-art implementation that uses O(lg n) time. As this latter implementation
essentially uses one binary rmM-tree for the whole sequence, our experiments show that our new
way to handle inter-bucket queries is useful in practice, reducing both space and time.

4.1 Implementation

We use a ﬁxed bucket size of β = 215 parentheses (i.e., 4KB). Since the relative excess inside each
bucket are in the range [−215, 215] the ﬁelds of the nodes of each rmM-tree are stored using 16-bit
integers. To reduce space, we get rid of the v.e ﬁelds by storing v.m and v.M in absolute form, not
relative to their rmM-subtree.3 This is because the ﬁeld v.e is used only to convert relative values
to absolute.4 This reduces the space required by the rmM-tree nodes from 8 to 6 bytes (or 4 bytes
if the ﬁeld v.n is not required, as it is used only in the more complicated operations). The block
size of each rmM-tree, b, is parameterized and provides a space-time tradeoﬀ: the bigger the block
size, the more expensive it is to perform a full scan. The sequential scan of a block is performed
by lookup tables that handle chunks of either 8 or 16 bits. Preliminary tests yielded the following
values to be reasonable for b: 512 bits (with lookup tables of 8 bits) and 1024/2048 bits (with
lookup tables of 16 bits). In particular, for b = 1024 our rmM-trees have height h = lg(β/b) = 5
and a sequential scan of a block requires up to 64 table lookups.
The bucket arrays e[], m[] and M [] are stored in heap form, as described. The special tree T (cid:48) of
Section 3.1 is built using a stack-based folklore algorithm that ﬁnds the previous-smaller-value of
each element in array m[] in linear time and space (that is, O(n/β) words). The ladder decomposition 
and pointers to ancestors at distances 2k (for some k) in T (cid:48) are implemented verbatim. To
ﬁnd the target bucket for operation fwdsearch we sequentially iterate over k = 0, 1, . . . to ﬁnd an

3These values are absolute within their current bucket; they are still relative to the beginning of the bucket

(otherwise they would not ﬁt in 16 bits).

4Instead, relative values allow making the structure dynamic, as eﬃcient insertions/deletions become possible [16].

11

ancestor whose minimum excess is lower than the target, then we perform a sequential search in
its ladder to ﬁnd the target bucket. Although this implementation does not guarantee O(lg lg n)
worst case time, it is cache-friendly and faster than doing a binary search over the list of sampled
ancestors or using the sarray bitmap representation to accelerate the search. On the real datasets
that were used for the experiments, the height of T (cid:48) was in all cases less than 10, which fully
justiﬁes a sequential scan. A more sophisticated implementation could resort to the guaranteed
O(lg lg n)-time method when it detects that the ladder or the list of ancestors are long enough.

For operation rmq(i, j) and relatives, the perfect binary tree of Section 3.2 is implemented
verbatim, except that the bitvector C is not implemented; a sequential search in Rn/Ln is carried
out instead for minselect. The extended rank and select operations were not yet implemented.

4.2 Experiemental setup

To measure the performance of our new implementation we used two public datasets5: wiki, the
XML tree topology of a Wikipedia dump with 498, 753, 916 parentheses and prot, the topology of
the suﬃx tree of the Protein corpus from the Pizza&Chili repository6 with 670, 721, 008 parentheses.
We replicate the benchmark methodology used by Arroyuelo et al. [1]: we ﬁx a probability
p ∈ [0, 1] and generate a sample dataset of nodes by performing a depth-ﬁrst traversal of the tree
where we descend to a random child and also descend to each other child with probability p. All
datasets generated consist of at least 200, 000 nodes. Setting p = 0 emulates random root-to-leaf
paths while p = 1 provides a full traversal of the tree. Intermediate values of p emulate other tree
traversals that occur, for example, when solving XPath queries or performing approximate string
matching on suﬃx trees. We benchmark the operations open/close/enclose for p = 0.00, 0.25,
and 0.50. We also benchmark operation rmq(i, j) by choosing 200,000 pairs i < j at random and
classifying the results according to j − i.

All the experiments were ran on a Intel(R) Core(TM) i5 running at 2.7GHz with 8GB of RAM
running Mac OS X 10.10.5. Our implementation is single-threaded, written in C++, and compiled
with clang version 7.0.0 with the ﬂags -O3 and -DNDEBUG.
As a baseline we use the C++ implementation available in the Succinct Data Structures Library
7(SDSL), which provides an O(lg n)-time implementation based on the description of Arroyuelo et
al. [1]. This library is known for its excellent implementation quality. In particular, this implementation 
also stores the ﬁelds v.m and v.M in absolute form and discards v.e. It also does not store
v.n, as it does not implement the more complex operations associated with it. For this reason,
we will only compare the structures on the most basic primitives open/close/enclose/rmq that are
also implemented in SDSL. Also, for fairness, we do not account for the space of the ﬁeld v.n in
our structure.

4.3 Experimental results

Figures 3 and 4 (left) show the results for open/close/enclose operations with diﬀerent values of p.
The times reported are in microseconds and are the average obtained by performing the operation
over all the nodes of a dataset generated for a given parameter value p. The space is reported in bits
per node (bpn). The newpreﬁx 
refers to the implementation of our new structure, while sdslrefers


5Available at http://www.inf.udec.cl/~josefuentes/sea2015/
6Available at http://pizzachili.dcc.uchile.cl/
7Available at github.com/simongog/sdsl-lite

12

Figure 3: Space-time tradeoﬀs for our new implementation and the SDSL baseline, for operations
close (left) and open (right).

to the SDSL implementation. The three space-time tradeoﬀs shown in our new implementation
correspond to b = 512, 1024, and 2048 (a larger b obtains lower space and higher time).

For operation close, our implementation is considerably faster than SDSL, while using essentially
the same space. For p = 0.0, we are up to 4 times faster when using the least space. For larger p,
the operations becomes much faster due to the locality of the traversals, and the time diﬀerences
decrease, but it they are still over 10%.

Our implementation is still generally faster for open on prot, whereas on wiki SDSL takes over
for larger p values. The maximum advantage in our favor is seen on operation enclose, where our
implementation is 2–6 times faster when using the least space, with the only exception of prot
with p = 0.50, where we are only 30% faster.
For operation rmq we show the results classiﬁed by j − i, cut into 100 percentiles. Figure 4 (top
right) shows the results. Both structures use the same space, about 2.34 bits per node. On prot
we are signiﬁcantly faster in almost all the spectrum, while on wiki we are generally faster by a
small margin. The diﬀerence owes to the fact that the tree of prot is much deeper, and therefore
the traversals towards the rmq positions are more random and less cache-friendly. In wiki, the
root and the highest nodes are the answers to random rmqs in most cases, so their rmM-trees are
likely to be in cache from previous queries. On the other hand, we note that the times are basically
constant as a function of j − i.

13

�������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������Figure 4: Space-time tradeoﬀs for our new implementation and the SDSL baseline, for operation
enclose (left). On the right, the results as a function of the distance traversed in the parenthesis
sequence for rmq, close, and open.

The other plots on the right of Figure 4 we show how the times for operation close and open
evolve as a function of the diﬀerence between the position that is queried and the one where the
answer is found. We use the conﬁguration with about 2.34 bits per node for both implementations,
and average the query times over all the tree nodes. In general, only a slight increase is observed as
the distance grows. In the larger sequence prot, however, there is a sharp increase for the largest
distances. This is not because the number of operations grows sharply, but it rather owes to a 10X
increase in the number of cache misses: traversing the longest distances requires accessing various
rmM-tree nodes that no longer ﬁt in the cache. Note that the highest times, around 0.5 µs, are
indeed the typical times obtained in Figure 3 with p = 0.0, where most of the nodes traversed
produce cache misses.

5 Conclusions

We have described an alternative solution for representing ordinal trees of n nodes within 2n +
O(n/ lg n) bits of space, which solves a large number of queries in time O(lg lg n). While the original
solution upon which we build [16] obtains constant times, it is hard to implement and only variants
using O(lg n) time had been successfully implemented. We have presented a practical implemen14


�����������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������tation of our solution and have experimentally shown that, on real hundred-million node trees,
it achieves better space-time tradeoﬀs than current state-of-the-art implementations. This shows
that the new design has not only theoretical, but also practical value. Our new implementation is
publicly available at www.dcc.uchile.cl/gnavarro/software.

References

[1] D. Arroyuelo, R. C´anovas, G. Navarro, and K. Sadakane. Succinct trees in practice. In Proc.

12th Workshop on Algorithm Engineering and Experiments (ALENEX), pages 84–97, 2010.

[2] M. Bender and M. Farach-Colton. The LCA problem revisited. In Proc. 4th Latin American

Theoretical Informatics Symposium (LATIN), LNCS 1776, pages 88–94, 2000.

[3] M. Bender and M. Farach-Colton. The level ancestor problem simpliﬁed. Theoretical Computer

Science, 321(1):5–12, 2004.

[4] D. Benoit, E. D. Demaine, J. I. Munro, R. Raman, V. Raman, and S. S. Rao. Representing

trees of higher degree. Algorithmica, 43(4):275–292, 2005.

[5] Y. T. Chiang, C. C. Lin, and H. I. Lu. Orderly spanning trees with applications. SIAM Journal

on Computing, 34(4):924–945, 2005.

[6] D. Clark. Compact PAT Trees. PhD thesis, University of Waterloo, Canada, 1996.

[7] M. Fredman and D. Willard. Surpassing the information theoretic bound with fusion trees.

Journal of Computer and Systems Science, 47(3):424–436, 1993.

[8] R. F. Geary, N. Rahman, R. Raman, and V. Raman. A simple optimal representation for

balanced parentheses. Theoretical Computer Science, 368(3):231–246, 2006.

[9] R. F. Geary, R. Raman, and V. Raman. Succinct ordinal trees with level-ancestor queries.

ACM Transactions on Algorithms, 2(4):510–534, 2006.

[10] G. Jacobson. Space-eﬃcient static trees and graphs.

In Proc. 30th IEEE Symposium on

Foundations of Computer Science (FOCS), pages 549–554, 1989.

[11] J. Jansson, K. Sadakane, and W.-K. Sung. Ultra-succinct representation of ordered trees with

applications. Journal of Computer and System Sciences, 78(2):619–631, 2012.

[12] S. Joannou and R. Raman. Dynamizing succinct tree representations. In Proc. 11th International 
Symposium on Experimental Algorithms (SEA), LNCS 7276, pages 224–235, 2012.

[13] H. Lu and C. Yeh. Balanced parentheses strike back. ACM Transactions on Algorithms,

4(3):1–13, 2008.

[14] J. I. Munro, R. Raman, V. Raman, and S. S. Rao. Succinct representations of permutations

and functions. Theoretical Computer Science, 438:74–88, 2012.

[15] J. I. Munro and V. Raman. Succinct representation of balanced parentheses and static trees.

SIAM Journal on Computing, 31(3):762–776, 2001.

15

[16] G. Navarro and K. Sadakane. Fully-functional static and dynamic succinct trees. ACM Transactions 
on Algorithms, 10(3):article 16, 2014.

[17] D. Okanohara and K. Sadakane. Practical entropy-compressed rank/select dictionary. In Proc.

9th Workshop on Algorithm Engineering and Experiments (ALENEX), pages 60–70, 2007.

[18] M. P˘atra¸scu. Succincter. In Proc. 49th Annual IEEE Symposium on Foundations of Computer

Science (FOCS), pages 305–313, 2008.

[19] H. Yuan and M. J. Atallah. Data structures for range minimum queries in multidimensional
arrays. In Proc. 21st Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages
150–160, 2010.

16

