Optimal Dynamic Sequence Representations ∗

Gonzalo Navarro†

Yakov Nekrich‡

3
1
0
2

 

b
e
F
1

 

 
 
]
S
D
.
s
c
[
 
 

2
v
2
8
9
6

.

6
0
2
1
:
v
i
X
r
a

Abstract

We describe a data structure that supports access, rank and select queries, as well as symbol
insertions and deletions, on a string S[1, n] over alphabet [1..σ] in time O(lg n/ lg lg n), which
is optimal even on binary sequences and in the amortized sense. Our time is worst-case for the
queries and amortized for the updates. This complexity is better than the best previous ones
by a Θ(1 + lg σ/ lg lg n) factor. We also design a variant where times are worst-case, yet rank
and updates take O(lg n) time. Our structure uses nH0(S) + o(n lg σ) + O(σ lg n) bits, where
H0(S) is the zero-order entropy of S. Finally, we pursue various extensions and applications
of the result.

1

Introduction

String representations supporting rank and select queries are fundamental in many data structures,
 including full-text indexes [GGV03, FMMN07, GMR06], permutations [GMR06, BGNN10],
inverted indexes [BFLN08, BGNN10], graphs [CN10], document retrieval indexes [VM07], labeled
trees [GMR06, BHMR11], XML indexes [GHSV07, FLMM09], binary relations [BHMR11], and
many more. The problem is to encode a string S[1, n] over alphabet Σ = [1..σ] so as to support
the following queries:

ranka(S, i) = number of occurrences of a ∈ Σ in S[1, i], for 1 ≤ i ≤ n.
selecta(S, i) = position in S of the i-th occurrence of a ∈ Σ, for 1 ≤ i ≤ ranka(S, n).
access(S, i) = S[i].

There exist various static representations of S (i.e., S cannot change) that support these operations 
[GGV03, GMR06, FMMN07, BGNN10, BN12]. The most recent one [BN12] shows a
lower bound of Ω(lg lg σ
lg w ) time for operation rank on a RAM machine with w-bit words, using any
O(n lgO(1) n) space.
It also provides a matching upper bound that in addition achieves almost
constant time for select and access, using compressed space. Thus the problem is essentially closed.
However, various applications need dynamism, that is, the ability to insert and delete symbols
in S. A lower bound for this case, in order to support operations rank, insertions and deletions,
even for bitmaps (σ = 2) and in the amortized sense, is Ω(lg n/ lg lg n) [FS89]. On the other
hand the best known upper bound [HM10, NS10] is O((1 + lg σ/ lg n) lg n/ lg lg n), that is, a factor

∗Partially funded by Fondecyt grant 1-110066, Chile. An early partial version of this paper appeared in Proc.

SODA’13.

†Department of Computer Science, University of Chile. gnavarro@dcc.uchile.cl.
‡Department

Engineering

Electrical

of

and

Computer

Science,

University

of

Kansas.

yakov.nekrich@googlemail.com.

1

Θ(lg σ/ lg lg n) away from the lower bound for alphabets larger than polylogarithmic. Their space
is nH0(S) + o(n lg σ), where H0(S) = Pa∈[1..σ](na/n) lg(n/na) ≤ lg σ is the zero-order entropy of
S, where na is the number of occurrences of a in S.
In this paper we close this gap by providing an optimal-time dynamic representation of sequences.
 Our representation takes O(lg n/ lg lg n) time for all the operations, worst-case for the
three queries and amortized for insertions and deletions. We present a second variant achieving
worst-case bounds for all the operations, O(lg n/ lg lg n) for select and access and O(lg n) for rank,
insertions and deletions. The space is also nH0(S) + o(n lg σ). This O(lg n) is still faster than
previous work for lg σ = Ω((lg lg n)2). This gets much closer to closing this problem under the
dynamic scenario as well.

We then show how to handle general alphabets, such as Σ = R, or Σ = Γ∗ for a symbol
alphabet Γ, in optimal time. For example, in the comparison model for Σ = R, the time is
O(lg σ + lg n/ lg lg n), where σ is the number of distinct symbols that appear in S; in the case
Σ = Γ∗ for general Γ, the time is O(|a| + lg γ + lg n/ lg lg n), where |a| is the length of the involved
symbol (a string) and γ the number of distinct symbols of Γ that appear in the elements of S.
Previous dynamic solutions have assumed that the alphabet [1..σ] was static.

At the end we describe several applications where our result oﬀers improved time/space tradeoﬀs.
 These include compressed indexes for dynamic text collections, construction of the BurrowsWheeler 
transform [BW94] and static compressed text indexes within compressed space, as well as
compressed representations of dynamic binary relations, directed graphs, and inverted indexes.

We start with an overview of the state of the art, putting our solution in context, in Section 2.
We review the wavelet tree data structure [GGV03], which is at the core of our solution (and most
previous ones) in Section 3. In Section 4 we describe the core of our amortized solution, deferring
to Section 5 the management of deletions and its relation with a split-ﬁnd data structure needed
for rank and inserts. Section 6 deals with the changes in lg n and how we obtain times independent
of σ, and concludes with Theorem 1, our result on uncompressed sequences. Then Section 7 shows
how to improve the data encoding to obtain compressed space in Theorem 2, and Section 8 how
to obtain worst-case times, Theorem 3. Finally, Section 10 describes some extensions and several
applications of our results. We conclude in Section 11. An important technical part of the paper,
describing the structure of blocks that handle subsequences of polylogarithmic size, is deferred to
Section 9 to avoid distractions.

2 Related Work

With one exception [GHSV07], all the previous work on dynamic sequences build on the wavelet
tree structure [GGV03]. The wavelet tree decomposes S hierarchically. In a ﬁrst level, it separates
larger from smaller symbols, by marking in a bitmap which symbols of S were larger and which
were smaller. The two subsequences of S are recursively separated. The lg σ levels of bitmaps
describe S, and access, rank and select operations on S are carried out via lg σ rank and select
operations on the bitmaps (see Section 3 for more details).

In the static case, rank and select operations on bitmaps take constant time, and therefore
access, rank and select on S takes O(lg σ) time [GGV03]. This can be reduced to O(1+ lg σ/ lg lg n)
by using multiary wavelet trees [FMMN07]. These separate the symbols into ρ = o(lg n) ranges,
and instead of a bitmap store a sequence over an alphabet of size ρ.

Insertions and deletions in S can also be carried out by inserting and deleting bits from lg σ

2

bitmaps. However, the operations on dynamic bitmaps are bound to be slower. Fredman and Saks
[FS89] show that Ω(lg n/ lg lg n) time is necessary, even in the amortized sense, to support rank,
insert and delete operations on a bitmap. By using dynamic bitmap solutions [HSS03, CHL04,
BB04, CHLS07, HSS11] on the wavelet tree levels, one immediately obtains a dynamic sequence
representation, where the space and the time of the dynamic bitmaps solution is multiplied by
lg σ (the sum of the zero-order entropies of the bitmaps also adds up to nH0(S) [GGV03]). With
this combination one can obtain times as good as O(lg σ lg n/ lg lg n) (and n lg σ + o(n lg σ) bits)
[CHLS07] and spaces as good as O(nH0(S)) (and O(lg σ lg n) time) [BB04].

M¨akinen and Navarro [MN06, MN08] made the above combination explicit and obtained compressed 
bitmaps with logarithmic-time operations, yielding O(lg σ lg n) time for all the sequence
operations and the best compressed space until then, nH0(S) + o(n lg σ) bits. They also obtained
O((1 + lg σ/ lg lg n) lg n) query time, but with an update time of O(lg σ lg1+ε n), for any constant
0 < ε < 1. This was achieved by replacing binary with multiary wavelet trees, and obtaining
O(lg n) query time for the operations on sequences over a small alphabet of size o(lg n).

Lee and Park [LP07, LP09] pursued this path further, obtaining the O(1+lg σ/ lg lg n) lg n) time
for queries and update operations, yet the space was not compressed, n lg σ + o(n lg σ), and update
times were amortized. Shortly after, Gonz´alez and Navarro [GN08, GN09] obtained the best of both
worlds, making all the times worst-case and compressing the space again to nH0(S) + o(n lg σ) bits.
Both solutions managed to solve all query and update operations in O(lg n) time on sequences over
small alphabets of size o(lg n).

Finally, almost simultaneously, He and Munro [HM10] and Navarro and Sadakane [NS10] obtained 
the currently best result, O((1 + lg σ/ lg lg n) lg n/ lg lg n) time, still within the same compressed 
space. They did so by improving the times of the dynamic sequences on small alphabets
to O(lg n/ lg lg n), which is optimal even on bitmaps and in the amortized sense [FS89].

As mentioned, the solution by Gupta et al. [GHSV07] deviates from this path and is a general
framework for using any static data structure and periodically rebuilding it. By using it over a
given representation [GMR06], it achieves O(lg lg n) query time and O(nε) amortized update time.
It would probably achieve compressed space if combined with more recent static data structures
[BGNN10]. This shows that query times can be signiﬁcantly smaller if one allows for much higher
update times. In this paper, however, we focus in achieving similar times for all the operations.

Table 1 gives more details on previous and our new results. Wavelet trees can also be used to
model n × n grids of points, in which case σ = n. Bose et al. [BHMM09] used a wavelet-tree-like
structure to solve range counting in optimal static time O(lg n/ lg lg n), using operations slightly
more complex than rank on the wavelet tree levels. It is conceivable that this can be turned into an
O((lg n/ lg lg n)2) time algorithm using dynamic sequences on the wavelet tree levels. On the other
hand, Ω((lg n/ lg lg n)2) is a lower bound for dynamic range counting in two dimensions [Pat07].
This seems to suggest that it is unlikely to obtain better results than those previously known for
dynamic wavelet trees.

In this paper we show that this dead-end can be broken by abandoning the implicit assumption
that, to provide access, rank and select on S, we must provide rank and select on the bitmaps (or
sequences over [1..ρ]). We show that all what is needed is to track positions of S downwards and
upwards along the wavelet tree. It turns out that this tracking can be done in constant time per
level, breaking the Θ(lg n/ lg lg n) per-level barrier.

As a result, we obtain the optimal time complexity O(lg n/ lg lg n) for all the queries (worst-case)
and update operations (amortized), independently of the alphabet size. This is Θ(1 + lg σ/ lg lg n)

3

Table 1: History of results on managing dynamic sequences S[1, n] over alphabet [1..σ], assuming σ = o(n/ lg n) to simplify. Some
results [HSS03, BB04, CHL04] were presented only for binary sequences and the result we give is obtained by using them in
combination with wavelet trees. Column W/A tells whether the update times are (W)orst-case or (A)mortized.

Space (bits)

Query time

Update time

W/A

n lg σ + O(n lg σ(lg lg n)2/ lg n)

O(lg σ lg n/ lg lg n)

O(lg σ(lg n/ lg lg n)2)

Ω(lg n/ lg lg n) for rank and indels

4

Source
[FS89]
[HSS03, HSS11]
[CHL04]
[BB04]
[MN06, MN08]

[CHLS07]
[GHSV07]
[LP07, LP09]
[GN08, GN09]
[HM10]
[NS10]
Ours
Ours

O(n lg σ)

O(nH0(S) + lg n)

nH0(S) + O(n lg σ/√lg n)

nH0(S) + O(n lg σ/ lg1/2−ε n)

O(n lg σ)

n lg σ + O(n lg σ/ lg lg σ)

n lg σ + O(n lg σ/√lg n) + O(n)

nH0(S) + O(n lg σ/√lg n)
nH0(S) + O(n lg σ/√lg n)

nH0(S) + O(n lg σ/(ε lg1−ε n))

nH0(S) + O(n lg σ/ lg1−ε n)

nH0(S) + O(nH0(S)/ lg lg n) + O(n lg σ/ lg1−ε n) O( 1

O(lg σ lg n)
O(lg σ lg n)
O(lg σ lg n)
ε lg σ/ lg lg n) lg n)

O((1 + 1

O(lg σ lg n/ lg lg n)
O( 1
ε lg lg n + lg lg σ)

O((1 + lg σ/ lg lg n) lg n)
O((1 + lg σ/ lg lg n) lg n)

O((1 + lg σ/ lg lg n) lg n/ lg lg n)
O((1 + 1
ε lg σ/ lg lg n) lg n/ lg lg n)
O( 1

ε2 lg n/ lg lg n)

ε2 lg n/ lg lg n), O(lg n) for rank

O(lg σ lg n)
O(lg σ lg n)
O(lg σ lg n)
ε lg σ lg1+ε n)
O( 1
O(lg σ lg n/ lg lg n)

A
A
W
W
W
W
W
A
A
W
W
ε lg σ/ lg lg n) lg n/ lg lg n) W
O( 1
A
W

ε2 lg n/ lg lg n)

O(lg n)

O( 1

ε nε)

O((1 + lg σ/ lg lg n) lg n)
O((1 + lg σ/ lg lg n) lg n)

O((1 + lg σ/ lg lg n) lg n/ lg lg n)
O((1 + 1

times faster than what was believed to be the “ultimate” solution. Our space is nH0(S) + o(n lg σ)
bits, similar to previous solutions. We develop, alternatively, a data structure achieving worst-case
time for all the operations, yet this raises to O(lg n) for rank, insertions and deletions.

Among the many applications of this result, it is worth mentioning that any dynamic sequence
representation supporting rank and insertions in O(t(n)) amortized time can be used to compute
the Burrows-Wheeler transform (BWT) [BW94] of a sequence S[1, n] in worst-case time O(nt(n)).
Thus our results allow us to build the BWT in O(n lg n/ lg lg n) time and compressed space. The
best existing space-time tradeoﬀs are by Okanohara and Sadakane [OS09], who achieve optimal
O(n) time within O(n lg σ lg lgσ n) bits, and Hon et al. [HSS09], who achieve O(n lg lg σ) time with
O(n lg σ) bits. K¨arkk¨ainen [K¨ar07] had obtained before O(n lg n + nv) time and O(n lg n/√v) extra
bits for a parameter v. Using less space allows us to improve BWT-based compressors (like Bzip2)
by allowing them to cut the sequence into larger blocks, given a ﬁxed amount of main memory for
the compressor. Many other results will be mentioned in Section 10.

3 The Wavelet Tree

Let S be a string over alphabet Σ = [1..σ]. We associate each a ∈ Σ to a leaf va of a full balanced
binary tree T . The essential idea of the wavelet tree structure [GGV03] is the representation of
elements from a string S by bit sequences stored in the nodes of tree T . We associate a subsequence
S(v) of S with every node v of T . For the root vr, S(vr) = S. In general, S(v) consists of all the
occurrences of symbols a ∈ Σv in S, where Σv = {a ∈ Σ, va descends from v}. The wavelet tree
does not store S(v) explicitly, but just a bit vector B(v). We set B(v)[i] = t if the i-th element
of S(v) also belongs to S(vt), where vt is the t-th child of v (the left child corresponds to t = 0
and the right to t = 1). This data structure (i.e., T and bit vectors B(v)) is called a wavelet tree.
Since T has O(σ) nodes and ⌈lg σ⌉ levels, and the bitmaps at each level add up to length n, it
requires n⌈lg σ⌉ + O(σ lg n) bits of space. If the bitmaps B(v) are compressed to |B(v)|H0(B(v)
bits, the total size adds up to nH0(S) + O(σ lg n) bits [GGV03]. Various surveys on wavelet trees
are available, for example [NM07, Nav12].

For any symbol S[i] and every internal node v such that S[i] ∈ Σv, there is exactly one bit bv
in B(v) that indicates in which child of v the leaf vS[i] is stored. We will say that such bv encodes
S[i] in B(v); we will also say that bit bv from B(v) corresponds to a bit bu from B(u) if both bv
and bu encode the same symbol S[i] in two nodes v and u. Identifying the positions of bits that
encode the same symbol plays a crucial role in wavelet trees. Other, more complex, operations rely
on the ability to navigate in the tree and keep track of bits that encode the same symbol.

The wavelet tree encodes S, in the sense that it allows us to extract any S[i]. To implement
access(S, i) we traverse a path from the root to the leaf vS[i]. In each visited node we read the bit
bv that encodes S[i] and proceed to the corresponding bit in the bv-th child of v. Upon arriving to
a leaf va we answer access(S, i) = a.

The wavelet tree also implements operations rank and select. To compute selecta(S, i), we start
at the position of S(va)[i] and identify the corresponding bit bv in the parent v of va. We continue
this process to the root until reaching a position B(vr)[j]. Then the answer is selecta(S, i) = j.
Finally, to compute ranka(S, i), we traverse the wavelet tree from B(vr)[i] to the leaf va. At each
element bv of each node v in the path, we identify the last bit b′ that precedes bv and encodes an
a. Then we move to the bit bu corresponding to b′ in the b′-th child of v. Upon arriving at position
B(va)[j], the answer is rank(S, i) = j.

5

The standard method used in wavelet trees for identifying the corresponding bits is to maintain
rank/select data structures on the bit vectors B(v). Let B(v)[e] = t; we can ﬁnd the oﬀset of the
corresponding bit in the child of v by answering a query rankt(B(v), e). If v is the t-th child of a
node u, we can ﬁnd the oﬀset of the corresponding bit in u by answering a query selectt(B(u), e).
Finally, the more complicated process of ﬁnding b′ needed for ranka(S, i) is easily solved using binary
rank: if bv is at position e in B(v), then without need of ﬁnding b′ we know that its corresponding
position in the b′-th child of v is rankb′(B(v), e). This approach leads to O(lg σ) query times in
the static case because rank/select queries on a bit vector B(v) can be answered in constant time
and |B(v)| + o(|B(v)|) bits of space [Mun96, Cla96], and even using |B(v)|H0(B(v)) + o(|B(v)|)
bits [RRR07]. However, we need Ω(lg n/ lg lg n) time to support rank/select and updates on a bit
vector [FS89], which multiplies the operation times in the dynamic case.

An improvement (for both static and dynamic wavelet trees) can be achieved by increasing the
fan-out of the wavelet tree to ρ = Θ(lgε n) for a constant 0 < ε < 1: as before, B(v)[e] = t if the e-th
element of S(v) also belongs to S(vt) for the t-th child vt of v. This enables us to reduce the height
of the wavelet trees and the query time by a Θ(lg lg n) factor, because the rank/select times over
alphabet [1..ρ] is still constant in the static case [FMMN07] and O(lg n/ lg lg n) in the dynamic case
[HM10, NS10]. However, it seems that further improvements that are based on dynamic rank/select
queries in every node are not possible.

In this paper we use a diﬀerent approach to identifying the corresponding elements. We partition
sequences B(v) into blocks, which are stored in compact list structures L(v). Pointers from selected
positions in L(v) to the structure L(u) in children nodes u (and vice versa) enable us to navigate
between nodes of the wavelet tree in constant time. We extend the idea to multiary wavelet
trees. While similar techniques have been used in some geometric data structures [Nek11, Ble08],
applying them on compressed data structures where the bit budget is severely limited is much more
challenging. We describe our new solution next.

4 Basic Structure

We start by describing the main components of our modiﬁed wavelet tree. Then, we show how
our structure supports access(S, i) and selecta(S, i). In the third part of this section we describe
additional structures that enable us to answer ranka(S, i). Finally, we show how to support updates.

4.1 Structure
We assume that the wavelet tree T has node degree ρ = Θ(lgε n). We divide sequences B(v) into
blocks and store those blocks in a doubly-linked list L(v). Each block Gj(v) contains Θ(lg3 n/ lg ρ)
consecutive elements from B(v), except the last, which can be smaller. That is, |Gj(v)| = O(lg3 n) if
measured in bits. For each Gj(v) we maintain a data structure Rj(v) that supports access, rank and
select queries on elements of Gj(v). Since a block contains a poly-logarithmic number of elements
over an alphabet of size ρ, we can answer those queries in O(1) time using O(|Gj(v)|/ lg1−ε n)
additional bits (see Section 9 for details).
A pointer to an element B(v)[e] consists of two parts: a unique id of the block Gj(v) that
contains the oﬀset e and the index of e in Gj(v). Such a pair (block id, local index) will be called
the position of e in v.

We maintain pointers between selected corresponding elements in L(v) and its children. If an

6

element B(v)[e] = t is stored in a block Gj(v) and B(v)[e′] 6= t for all e′ < e in Gj(v) (i.e., B(v)[e]
is the ﬁrst occurrence of t in its block), then we store a pointer from e to the oﬀset et of the
corresponding element B(vt)[et] in L(vt), where vt is the t-th child of v. Pointers are bidirectional,
that is, we also store a pointer from et to e. In addition, if B(v)[e] is the ﬁrst oﬀset in its block
and B(u)[e′] corresponds to B(v)[e] in the parent u of v, then we store a pointer from e to e′ and,
by bidirectionality, from e′ to e. All these pointers will be called inter-node pointers. We describe
how they are implemented later in this section.

It is easy to see that the number of inter-node pointers from e in L(v) to et in L(vt), for
any ﬁxed t, is Θ(g(v)), where g(v) is the number of blocks in L(v). Hence, the total number of
pointers that point down from a node v is bounded by O(g(v)ρ). Additionally, there are O(g(v))
pointers up to the parent of v. Thus, the total number of pointers in the wavelet tree equals
O(Pv∈T g(v)ρ) = O(n lg σ/ lg3−ε n + σ lgε n). Note that the term σ lgε n is only necessary to
account for nodes that have just one block (with o(lg3 n) bits). Since the children of those nodes
must also have just one block, we avoid storing their pointers, as we know that all point to the
same block and their index inside the block can be found with constant-time rank/select operations
inside the block from where pointers leave. Their upwards pointer to their parent, if the parent has
more than one block, can be represented and charged to the space of the parent. This yields the
cleaner expression O(n lg σ/ lg3−ε n) for the number of pointers.

The pointers from a block Gj(v) are stored in a data structure Fj(v). Using Fj(v) we can ﬁnd,
for any oﬀset e in Gj(v) and any 1 ≤ t ≤ ρ, the last e′ ≤ e in Gj(v) such that there is a pointer
from e′ to an oﬀset e′
t in L(vt). We describe in Section 9 how Fj(v) implements the queries and
updates in constant time.

For the root node vr, we store a dynamic searchable partial-sum data structure K(vr) that
contains the number of positions in each block of L(vr). Using K(vr), we can ﬁnd the block Gj(vr)
that contains the i-th element of S(vr) = S (query search on the partial sums), as well as the number
of elements in all the blocks that precede a given block Gj(vr) (operation sum on the partial sums).
Both operations can be supported in O(lg n/ lg lg n) time and linear space [NS10, Lem. 1]. The
same data structures K(va) are also stored in the leaves va of T . Since g(vr) = O(n lg ρ/ lg3 n), and
also Pa∈Σ g(va) = O(n lg ρ/ lg3 n), we store O(n lg lg n/ lg3 n) elements in the partial sums K(vr)
and K(va), for an overall size of O(n lg lg n/ lg2 n) bits.

We observe that we do not store a sequence B(va) in a leaf node va, only in internal nodes.
Nevertheless, we divide the (implicit) sequence B(va) into blocks and store the number of positions
in each block in K(va); we maintain K(va) only if L(va) consists of more than one block. Moreover
we store inter-node pointers from the parent of va to va and vice versa. Pointers in a leaf are
maintained using the same rules of any other node.

For future reference, we provide the list of secondary data structures in Table 2.

4.2 Access and Select Queries

Assume the position of an element B(v)[e] = t in L(v) is known, and let iv be the index of oﬀset e
in its block Gj(v). Then the position of the corresponding oﬀset et in L(vt) is computed as follows.
v of the largest e′ ≤ e in Gj(v) such that there is a pointer from
Using Fj(v), we ﬁnd the index i′
t in L(vt). Due to our construction, such e′ must exist (it may be e itself). Let Gℓ(vt)
e′ to some e′
denote the block that contains e′
t, and let i′
t in Gℓ(vt). Due to our rules to
deﬁne pointers, et also belongs to Gℓ(vt), since if it belonged to another block Gm(vt) the upward
pointer from Gm(vt) would point between e′ and e, and since pointers are bidirectional, this would

t be the index of e′

7

Table 2: Structures inside any node v of the wavelet tree T , or only in the root node vr and the
leaves va. The third column gives the extra space in bits, on top of the data, for the whole structure;
here |data| is n lg σ in the uncompressed case and nH0(S) in the compressed case.

Structure Meaning
L(v)
Gj(v)
Rj(v)
Fj(v)
Hj(v)
Pt(v)
K(v)
Dj(v)
DEL

List of blocks storing B(v)
j-th block of list L(v)
Supports rank/select/access inside Gj(v)
Pointers leaving from Gj(v)
Pointers arriving at Gj(v)
Predecessor in L(v) containing symbol t
Partial sums on block lengths for vr and va
Deleted elements in Gj(v), for vr and va
Global list of deleted elements in S

Extra space in bits

O(n lg σ/ lg2 n + σ lg n)

O(|data| lg lg n/ lg n + n lg σ/ lg n + σ lg n)

O(|data|/ lg1−ε n)
O(n lg σ/ lg2−ε n)
O(n lg σ/ lg2 n)
O(n lg σ/ lg2−ε n)
O(n lg lg n/ lg2 n)
O(n(lg lg n)2/ lg n)

O(n/ lg n + n lg σ/ lg2 n)

contradict the deﬁnition of e′. Furthermore, let rv = rankt(Gj(v), iv) and r′
Then the index of et is i′
of B(v)[e] = t is known.

v).
v). Thus we can ﬁnd the position of et in O(1) time if the position

v = rankt(Gj (v), i′

t + (rv − r′

Analogously, assume we know a position B(vt)[et] at Gj(vt) and want to ﬁnd the position of the
corresponding oﬀset e in its parent node v. Using Fj(vt) we ﬁnd the last e′
t ≤ et in Gj(vt) that has
a pointer to its parent, which exists by construction (it can be the upward pointer from the ﬁrst
index in Gj(vt) or the reverse of some pointer from v to vt). Let e′
v in a
block Gℓ(v). Then, by our construction, e is also in Gℓ(v), since if it belonged to a diﬀerent block
Gm(v), then the ﬁrst occurrence of t in Gm(v) would point between e′
t and et, and its bidirectional
version would contradict the deﬁnition of e′
t and et
in Gj(vt), respectively. Then the index of e is selectt(Gℓ(v), rankt(Gℓ(v), i′

t and it be the indexes of e′

t point to e′, with index i′

t. Furthermore, let i′

v) + (it − i′

t)).

To solve access(S, i), we visit the nodes v0 = vr, v1 . . . vh = va, where h = lgρ σ is the height of
T , vk is the tk-th child of vk−1 and B(vk−1)[ek−1] = tk encodes S[i]. We do not ﬁnd out the oﬀsets
e1, . . . , eh, but just their positions. The position of e0 = i is found in O(lg n/ lg lg n) time using the
partial-sums structure K(vr). If the position of ek−1 is known, we can ﬁnd that of ek in O(1) time,
as explained above. When a leaf node vh = va is reached, we know that S[i] = a.

To solve selecta(S, i), we set eh = i and identify its position in the list L(va) of the leaf va,
using structure K(va). Then we traverse the path vh, vh−1, . . . , v0 = vr where vk−1 is the parent of
vk, until the root node is reached. In every node vk, we ﬁnd the position of ek−1 in L(vk−1) that
corresponds to ek as explained above. Finally, we compute the number of elements that precede e0
in L(vr) using structure K(vr).

Thus access and select require O(lgρ σ + lg n/ lg lg n) = O((lg σ + lg n)/ lg lg n) worst-case time.

4.3 Rank Queries

We need some additional data structures for the eﬃcient support of rank queries. In every node
v such that L(v) consists of more than one block, we store a data structure P (v). Using P (v) we
can ﬁnd, for any 1 ≤ t ≤ ρ and for any block Gj(v), the last block Gℓ(v) that precedes Gj(v) and

8

contains an element B(v)[e] = t. P (v) consists of ρ predecessor data structures Pt(v) for 1 ≤ t ≤ ρ.
We describe in Section 5 a way to support these predecessor queries in constant time in our scenario.
Let the position of oﬀset e be the i-th element in a block Gj(v). P (v) enables us to ﬁnd the
position of the last e′ ≤ e such that B(v)[e′] = t. First, we use Rj(v) to compute r = rankt(Gj (v), i).
If r > 0, then e′ belongs to the same block as e and its index in the block Gj(v) is selectt(Gj(v), r).
Otherwise, we use Pt(v) to ﬁnd the last block Gℓ(v) that precedes Gj(v) and contains an element
B(v)[e′] = t. We then ﬁnd the last such element in Gℓ(v) using Rℓ(v).

Now we are ready to describe the procedure to answer ranka(S, i). The symbol a is represented
as a concatenation of symbols t0 ◦ t1 ◦ . . . ◦ th, where each tk is between 1 and ρ. We traverse the
path from the root vr = v0 to the leaf va = vh. We ﬁnd the position of e0 = i in vr using the
data structure K(vr). In each node vk, 0 ≤ k < h, we identify the position of the last element
B(vk)[e′
k] = tk that precedes ek, using Ptk (vk). Then we ﬁnd the oﬀset ek+1 in the list L(vk+1) that
corresponds to e′
k.

When our procedure reaches the leaf node vh, the element B(vh)[eh] encodes the last symbol
a that precedes S[i]. We know the position of oﬀset eh, say index ih in its block Gℓ(vh). Then
we ﬁnd the number r of elements in all the blocks that precede Gℓ(vh) using K(vh). Finally,
ranka(S, i) = r + ih.

Since structures Pt answer queries in constant time, the overall time for rank is O(lgρ σ +

lg n/ lg lg n) = O((lg σ + lg n)/ lg lg n).

4.4 Updates

Now we describe how inter-node pointers are implemented. We say that an element of L(u) is
pointed if there is a pointer to its oﬀset. Unfortunately, we cannot store the local index of a pointed
element in the pointer: when a new element is inserted into a block, the indexes of all the elements
that follow it are incremented by 1. Since a block can contain Θ(lg3 n/ lg ρ) pointed elements, we
would have to update that many pointers after each insertion and deletion.

Therefore we resort to the following two-level scheme. Each pointed element in a block is
assigned a unique id. When a new element is inserted, we assign it the id max−id + 1, where
max−id is the maximum id value used so far. We also maintain a data structure Hj(v) for each
block Gj(v) that enables us to ﬁnd the position of a pointed element if its id in Gj(v) is known.
Implementation of Hj(v) is based on standard word RAM techniques and a table that contains ids
of the pointed elements; details are given in Section 9.

We describe now how to insert a new symbol a into S at position i. Let e0, e1, . . . , eh be the
oﬀsets of the elements that will encode a = t0 ◦ . . . ◦ th in vr = v0, v1, . . . , vh = va. We can ﬁnd
the position of e0 = i in L(vr) in O(lg n/ lg lg n) time using K(vr), and insert t0 at that position,
B(vr)[e0] = t0. Now, given the position of ek, in L(vk), where B(vk)[ek] = tk has just been inserted,
we ﬁnd the position of the last e′
k] = tk, in the same way as for rank queries.
Once we know the position of e′
k+1 in L(vk+1) that corresponds
to e′
k+1, at position
ek+1 = e′′

k. The element tk+1 must then be inserted into L(vk+1) immediately after e′′

k < ek such that B(vk)[e′
k in L(vk), we ﬁnd the position of e′′

k+1 + 1.

The insertion of a new element B(vk)[ek] = t into a block Gj(vk) is handled by structure Rj(vk)
and the memory manager of the block. We must also update structures Fj(vk) and Hj(vk) to keep
the correct alignments, and possibly to create and destroy a constant number inter-node pointers
to maintain our invariants. Also, since pointers are bidirectional, a constant number of inter-node
pointers in the parent and children of node vk may be updated. All those changes can be done in

9

O(1) time; see Section 9 for the details. Insertions may also require updating structures Pt(vk),
which require O(1) amortized time, see Section 5. Finally, if vk is the root node or a leaf, we also
update K(vk). This update is only by ±1, so it requires just O(lg n/ lg lg n) time [NS10, Lem. 1].
If the number of elements in Gj(vk) exceeds 2 lg3 n, we split Gj(vk) evenly into two blocks,
Gj1(vk) and Gj2(vk). Then, we rebuild the data structures R, F and H for the two new blocks.
Note that there are inter-node pointers to Gj(vk) that now could become dangling pointers, but all
those can be known from Fj(vk), since pointers are bidirectional, and updated to point to the right
places in Gj1(vk) or Gj2(vk). Finally, if vk is the root or a leaf, then K(vk) is updated, meaning
that we replace an existing element by two.

The total cost of splitting a block is dominated by that of building the new data structures R,
F and H. These are easily built in O(lg3 n/ lg ρ) time. Since we split a block Gj(v) at most once
per sequence of Θ(lg3 n) insertions in Gj(v), the amortized cost incurred by splitting a block is
o(1). Therefore the total cost of an insertion in L(v) is O(1). The insertion of a new symbol leads
to O(lgρ σ) insertions into lists L(v).

Our partial-sums structures K(vr) and K(va) do not support updates with large values. Inserting 
a new value for Gj2(vk) and moving part of the value of Gj(vk) to Gj2(vk) can be done
in O(lg3 n/ lg lg n) time by subtracting O(lg n) units from the value for Gj(v) until it becomes
|Gj1(vk)|, then inserting a number after it with value zero and increasing it by O(lg n) units until it
becomes |Gj2(vk)|. Each such increment/decrement and insertion takes O(lg n/ lg lg n) time [NS10,
Lem. 1] and we carry it out O(lg2 n/ lg ρ) times. Still this total cost amortizes to o(1) per operation.

Hence, the total cost of an insertion is O(lgρ σ + lg n/ lg lg n) = O((lg σ + lg n)/ lg lg n).
We describe next how deletions are handled, where we also describe the data structure P (v).

5 Lazy Deletions and Data Structure P (v)

We do not process deletions immediately, but in lazy form: we do not maintain exactly S but a
supersequence S of it. When a symbol S[i] = a is deleted from S, we retain it in S but take a
notice that S[i] = a is deleted. When the number of deleted symbols exceeds a certain threshold,
we expunge from the data structure all the elements marked as deleted. We deﬁne B(v) and the
list L(v) for the sequence S in the same way as B(v) and L(v) are deﬁned for S.

Since elements of L(v) are never removed, we can implement P (v) as an insertion-only data
structure. For any t, 1 ≤ t ≤ ρ, we store information about all the blocks of a node v in a
data structure Pt(v). Pt(v) contains one element for each block Gj(v) and is implemented as an
incremental split-ﬁnd data structure that supports insertions and splitting in O(1) amortized time
and queries in O(1) worst-case time [IA84]. The splitting positions in Pt(v) are the blocks Gj(v)
that contain an occurrence of t, so the operation “ﬁnd” in Pt(v) allows us to locate, for any Gj(v),
the last block preceding Gj(v) that contains an occurrence of t.

The insertion of a symbol t in L(v) may induce a new split in Pt(v). Furthermore, overﬂows in
a block Gj(v), which convert it into two blocks Gj1(v) and Gj2(v), induce insertions in Pt(v). Note
that an overﬂow in Gj(v) triggers ρ insertions in the Pt(v) structures, but this O(ρ) time amortizes
to o(1) because overﬂows occur every Θ(lg3 n/ lg ρ) operations.

Structures Pt(v) do not support “unsplitting” nor removals. The replacement of Gj(v) by Gj1(v)
and Gj2(v) is implemented as leaving in Pt(v) the element corresponding to Gj(v) and inserting
one corresponding to either Gj1(v) or Gj2(v). If Gj(v) contained t, then at least one of Gj1(v) and
Gj2(v) contain t, and the other can be inserted as a new element (plus possibly a split, if it also

10

contains t).

We need some additional data structures to support lazy deletions. A data structure K(v)
stores the number of non-deleted elements in each block of L(v) and supports partial-sum queries.
We will maintain K(v) in the root of the wavelet tree and in all leaf nodes. Moreover, we maintain
a data structure Dj(v) for every block Gj(v), where v is either the root or a leaf node. Dj(v) can
be used to count the number of deleted and non-deleted elements before the i-th element in a block
Gj(v) for any query index i, as well as to ﬁnd the index in Gj(v) of the i-th non-deleted element.
The implementation of Dj(v) is described in Section 9. We can use K(v) and Dj(v) to ﬁnd the
index i in L(v) where the i-th non-deleted element occurs, and to count the number of non-deleted
elements that occur before the index i in L(v).

We also store a global list DEL that contains, in any order, all the deleted symbols that have
not yet been expunged from the wavelet tree. For any symbol S[i] in the list DEL we store a pointer
to the oﬀset e in L(vr) that encodes S[i]. Pointers in list DEL are implemented in the same way
as inter-node pointers.

5.1 Queries

Queries are answered very similarly to Section 4. The main idea is that we can essentially ignore
deleted elements except at the root and at the leaves.

access(S, i): Exactly as in Section 3, except that e0 encodes the i-th non-deleted element in L(vr),

and is found using K(vr) and Dj(vr).

selecta(S, i): We ﬁnd the position of the oﬀset eh of the i-th non-deleted element in L(vh), where
vh = va, using K(va) and some Dj(va). Then we move up in the tree exactly as in Section 4.
When the root node v0 = vr is reached, we count the number of non-deleted elements that
precede oﬀset e0 using K(vr).

ranka(S, i): We ﬁnd the position of the oﬀset e0 of the i-th non-deleted element in L(vr). Let
vk, tk be deﬁned as in Section 4. In every node vk, we ﬁnd the last oﬀset e′
k ≤ ek such that
B(vk)[e′
k] = tk. Note that this element may be a deleted one, but it still drives us to the
correct position in L(vk+1). We proceed exactly as in Section 4 until we arrive at a leaf
vh = va. At this point, we count the number of non-deleted elements that precede oﬀset eh
using K(va) and Dj(va).

5.2 Updates

Insertions are carried out just as in Section 4. The only diﬀerence is that we also update the
data structure Dj(vk) when an element B(vk)[ek] that encodes the inserted symbol a is added
to a block Gj(vk). When a symbol S[i] = a is deleted, we append it to the list DEL of deleted
symbols. Then we visit each block Gj(vk) containing the element B(vk)[ek] that encodes S[i] and
update the data structures Dj(vk). Finally, K(vr) and K(va) are also updated. This takes in total
O(lgρ σ + lg n/ lg lg n) time.

When the number of symbols in the list DEL reaches n/ lg2 n, we perform a cleaning procedure
and get rid of all the deleted elements. Therefore DEL never requires more than O(n/ lg n) bits,
and the overhead due to storing deleted symbols is O(n lg σ/ lg2 n) bits.

11

Let B(vk)[ek], 0 ≤ k ≤ h, be the sequence of elements that encode a symbol S[i] ∈ DEL. The
method for tracking the elements B(vk)[ek], removing them from their blocks Gj(vk), and updating
the block structures, is symmetric to the insertion procedure described in Section 4. In this case
we do not need the predecessor queries to track the symbol to delete, as the procedure is similar to
that for accessing S[i]. When the size of a block Gj(vk) falls below (lg3 n)/2 and it is not the last
block of L(vk), we merge it with Gj+1(vk), and then split the result if its size exceeds 2 lg3 n. This
retains O(1) amortized time per deletion in any node vk, including the updates to K(vk) structures,
and this adds up to O((lg σ + lg n)/ lg lg n) amortized time per deleted symbol.

Once all the pointers in DEL are processed, we rebuild from scratch the structures P (v) for all
nodes v. The total size of all the P (v) structures is O(ρn lg σ/ lg3 n) elements. Since a data structure
for incremental split-ﬁnd is constructed in linear time, all the P (v)s are rebuilt in O(n lg σ/ lg3−ε n)
time. Hence the amortized time to rebuild the P (v)s is O(lg σ/ lg1−ε n), which does not aﬀect the
amortized time O((lg σ + lg n)/ lg lg n) to carry out the eﬀective deletions.

6 Changes in lg n and Alphabet Independence

Note that our structures depend on the value of w = lg n, so they should be rebuilt when lg n
changes. We use w = ⌈lg n⌉ as a ﬁxed value and rebuild the structure from scratch when n reaches
another power of two (more precisely, we use words of w = ⌈lg n⌉ bits until ⌈lg n⌉ increases by 1
or decreases by 2, and only then update w and rebuild). These reconstructions do not aﬀect the
amortized complexities, and the slightly larger words waste an O(1/ lg n) extra space factor in the
redundancy.

We take advantage of using a ﬁxed w value to get rid of the alphabet dependence. If lg σ ≤ w,
our time complexities are the optimal O(lg n/ lg lg n). However, if σ is larger, this means that not
all the alphabet symbols can appear in the current sequence (which contains at most n ≤ 2w < σ
distinct symbols). Therefore, in this case we create the wavelet tree for an alphabet of size s = 2w,
not σ (this wavelet tree is created when w changes). We also set up a mapping array SN [1, σ] that
will tell to which value in [1..s] is a symbol mapped, and a reverse mapping N S[1, s] that tells to
which original symbol in [1..σ] does a mapped symbol correspond. Both SN and N S are initialized
in constant time [Meh84, Section III.8.1] and require O(σ lg n + n lg σ) bits of space. Since this is
used only when σ > n, the space is O(σ lg n).

Upon operations ranka(S, i) and selecta(S, j), the symbol a is mapped using SN (the answer
is obvious if a does not appear in SN ) in constant time. The answer of operation access(S, i) is
mapped using N S in constant time as well. Upon insertion of a, we also map a using SN . If not
present in SN , we ﬁnd a free slot N S[i] (we maintain a list of free slots) and assign N S[i] = a and
SN [a] = i. When the last occurrence of a symbol a is deleted we return its slot to the free list and
unitialize its entry in SN . In this way, when lg σ > lg n, we can support all the operations in time
O(lg s/ lg lg s) = O(lg n/ lg lg n).

We are ready to state a ﬁrst version of our result, not yet compressing the sequence.

In
Section 9 it is seen that the time for the operations is O(1/ε). Since the height of the wavelet tree
is lgρ min(σ, s) = O( 1

As for the space, we show in Section 9 how to manage the data in blocks Gj(v) so that all
the elements stored in lists L(v) use n lg σ bits, plus the overhead O(n lg σ lg lg n/ lg n + σ lg n)
of the data organization and the memory manager. The internal structures Rj(v) add up to
O(n lg σ/ lg1−ε n) extra bits. Since there are O(n lg σ/ lg3 n + σ) blocks overall, all the pointers

ε lg n/ lg lg n), then we have O( 1

ε2 lg n/ lg lg n) time for all the operations.

12

between blocks of the same lists add up to O(n lg σ/ lg2 n+σ lg n) bits. All the data structures K(v)
add up to O(n lg ρ/ lg2 n) bits. We have shown that there are O(n lg σ/ lg3−ε n) inter-node pointers,
hence all inter-node pointers (i.e., Fj and Hj structures) use O(n lg σ/ lg2−ε n) bits. Structures
Pt(v) use O(n lg σ/ lg2−ε n) bits as they have ρ integers per block, and DEL takes O(n/ lg n)
bits plus the overhead of O(n lg σ/ lg2 n) of keeping deleted elements. The overall space is then
n lg σ + O(n lg σ/ lg1−ε n) + O(σ lg n) bits. (Note that when σ > n we use an alphabet of size O(n),
but then still we need the SN mapping, that takes O(σ lg n) bits.) This gives our ﬁrst result.

Theorem 1 A dynamic string S[1, n] over alphabet [1..σ] can be stored in a structure using n lg σ +
O(n lg σ/ lg1−ε n) + O(σ lg n) bits, for any 0 < ε < 1, and supporting queries access, rank and select
in time O( 1
ε2 lg n/ lg lg n)
amortized time.

ε2 lg n/ lg lg n). Insertions and deletions of symbols are supported in O( 1

7 Compressed Space

We now compress the space of the data structure to zero-order entropy (nH0(S) plus redundancy).
We show how a diﬀerent encoding of the bits within the blocks reduces the n lg σ to nH0(S) in the
space without aﬀecting the time complexities.

Raman et al. [RRR07] describe an encoding for a bitmap B[1, n] that occupies nH0(B) +
O(n lg lg n/ lg n) bits of space. It consists of cutting the bitmap into chunks of length b = (lg n)/2
and encoding each chunk i as a pair (ci, oi): ci is the class, which indicates how many 1s are there
in the chunk, and oi is the oﬀset, which is the index of this particular chunk within its class. The
ci components add up to O(n lg lg n/ lg n) bits, whereas the oi components add up to nH0(B).
Navarro and Sadakane [NS10, Sec. 8] describe a technique to maintain a dynamic bitmap in this
format. They allow the chunk length b to vary, so they encode triples (bi, ci, oi) maintaining the
invariant that bi + bi+1 > b for any i. They show that this retains the same space, and that each
update aﬀects O(1) chunks.

We extend this encoding to handle an alphabet [1..ρ] [FMMN07], so that b = (lgρ n)/2 symbols,
and each chunk is encoded as a tuple (bi, c1
i counts the occurrences of t in the
block. The classes (bi, c1
i ) use O(ρn lg lg n/ lg n) bits, and the oﬀsets still add up to nH0(B).
Blocks are encoded/decoded in O(1) time, as the class takes O(ρ lg lg n) = o(lg n) bits and the
block encoding requires at most O(lg n) bits. In Section 9 we show how using compressed chunks
does not aﬀect their handling inside blocks.

i , oi) where ct

i , . . . , cρ

i , . . . , cρ

The sum of the local entropies of the chunks, across the whole L(v), adds up to nH0(Bv),
and these add up to nH0(S) [GGV03]. The redundancy over the entropy is O(ρ lg lg n) bits per
miniblock, adding up to O(nH0(S) lg lg n/ lg1−ε n) bits, and we have also a ﬁxed redundancy of
O(n lg σ/ lg n + n(lg lg n)2/ lg n + σ lg n), according to Section 9. The fact that we store S instead
of S, with up to O(n/ lg2 n) spurious symbols, can increase nH0(S) up to nH0(S) ≤ nH0(S) +
O(n/ lg n) bits. Thus we get the following result, for any desired 0 < ε < 1.

Theorem 2 A dynamic string S[1, n] over alphabet [1..σ] can be stored in a structure using nH0(S)+
O(nH0(S)/ lg1−ε n + n lg σ/ lg n + n(lg lg n)2/ lg n + σ lg n) = nH0(S) + O(n lg σ/ lg1−ε n + σ lg n)
bits, for any 0 < ε < 1, and supporting queries access, rank and select in time O( 1
ε2 lg n/ lg lg n).
Insertions and deletions of symbols are supported in O( 1
ε2 lg n/ lg lg n) amortized time.

13

8 Worst-Case Complexities

While in previous sections we have obtained optimal time and compressed space, the time for the
update operations is amortized. In this section we derive worst-case time complexities, at the price
of losing the time optimality, which will now become logarithmic for some operations. Along the
rest of the section we remove the various sources of amortization in our solution.

8.1 Block Splits and Merges

Our amortized solution splits overﬂowing blocks and rebuilds the two new blocks from scratch
(Section 4.4). Similarly, it merges underﬂowing blocks (as a part of the cleaning of the global DEL
list in Section 5.2). This gives good amortized times but in the worst case the cost is Ω(lg3 n/ lg lg n).
We use a technique [GN09] that avoids global rebuildings. A block is called dense if it contains
at least lg3 n bits, and sparse otherwise. While sparse blocks of any size (larger than zero) are
allowed, we maintain the invariant that no two consecutive sparse blocks may exist. This retains
the fact that there are O(n lg σ/ lg3 n + σ) blocks in the data structure. The maximum size of a
block will be 2 lg3 n bits. When a block overﬂows due to an insertion, we move its last element
to the beginning of the next block. If the next block would also overﬂow, then we are entitled to
create a new sparse block between both dense blocks, containing only that element. Analogously,
when a deletion converts a dense block into sparse (i.e., it falls below length lg3 n), we check if both
neighbors are dense. If they are, the current block can become sparse. If, instead, there is a sparse
neighbor, we move its ﬁrst/last element into the current block to avoid it becoming sparse. If this
makes that sparse neighbor block become of size zero, we remove it.

Therefore, we only create and destroy empty blocks, and move a constant number of elements to
neighboring blocks. This can be done in constant worst-case time. It also simpliﬁes the operations
on the partial-sum data structures K(v), since now only updates by ±1 and insertions/deletions of
elements with value zero are necessary, and these are carried out in O(lg n/ lg lg n) worst case time
[NS10, Lem. 1]. Recall that lg n is ﬁxed in each instance of our data structure, so the deﬁnition of
sparse and dense is static.

8.2 Split-Find Data Structure and Lazy Deletions

The split-ﬁnd data structure [IA84] we used in Section 5 to implement the Pt structures has constant
amortized insertion time. We replace it by another one [Mor03, Thm 4.1] achieving O(lg lg n) worstcase 
time. Their structure handles a list of colored elements (list nodes), where each element can
have O(1) colors (each color is a positive integer bounded by O(logε n) for a constant 0 < ε < 1).
We will only use list nodes with 0 or 1 color. The operations of interest to us are: creating a new
list node without colors, assigning or removing a color to/from a list node, and ﬁnding the last
list node preceding a given node and having some given color. Node deletions are not supported.
The number of list nodes must be smaller than a certain upper bound n′, and the operations cost
O(lg lg n′). In our case, since lg n is ﬁxed, we can use n′ = 2w = O(n) as the upper bound.

We use ρ colors, one per symbol in the sequences. Each time we create a block, we add a new
uncolored node to the list, with a bidirectional pointer to the block. Each time we insert a symbol
t ∈ [1..ρ] for the ﬁrst time in a block, we add a new node colored t to the list, right after the
uncolored element that represents the block, and also set a bidirectional pointer between this node
and the block.

14

We cannot use the lazy deletions mechanism of Section 5, as it gives only good amortized
complexity. We carry out the deletions immediately in the blocks, as said in Section 8.1. Each
time the last occurrence of a symbol t ∈ [1..ρ] is deleted from a block, we remove the color from
the corresponding list node (if the symbol reappears later, we reuse the same node and color it,
instead of creating a new one).

Therefore, ﬁnding the last block where a symbol t appears, as needed by the rank query and
for insertions, corresponds to ﬁnding the last list node colored t and preceding the uncolored node
that represents the current block.

Since list nodes cannot be deleted, when a block disappears its (uncolored) list nodes are left
without an associated block. This does not alter the result of queries, but there is the risk of
maintaining too many useless nodes. We permanently run an incremental list “copying” process,
traversing the current list of blocks and inserting the corresponding nodes into a new list. This
new list is also updated, together with the current list, on operations concerning the blocks already
copied. When the new list is ready it becomes the current list and the previous list is incrementally
deleted. In O(nρ lg ρ/ lg3 n) steps we have copied the current list; by this time the number of useless
nodes is at most O(nρ lg ρ/ lg3 n) and just poses O(n lg lg n/ lg2−ε n) bits of space overhead.

Note that blocks must manage the sets of up to ρ pointers to their colored nodes. This is easily

handled in constant time with the same techniques used for structure Fj(v) in Section 9.

Since the colored list data structure requires O(lg lg n) time, operations rank and insert take

worst-case time O( 1

ε lg n), whereas access, select and delete still stay in O( 1

ε2 lg n/ lg lg n).

8.3 Changes in lg n

As an alternative to reconstructing the whole structure when n doubles or halves, M¨akinen and
Navarro [MN08] describe a way to handle this problem without aﬀecting the space nor the time
complexities, in a worst-case scenario. The sequence is cut into a preﬁx, a middle part, and a suﬃx.
The middle part uses a ﬁxed value ⌈lg n⌉, the preﬁx uses ⌈lg n⌉ − 1 and the suﬃx uses ⌈lg n⌉ + 1.
Insertions and deletions trigger slight expansions and contractions in this separation, so that when
n doubles all the sequence is in the suﬃx part, and when n halves all the sequence is in the preﬁx
part, and we smoothly move to a new value of lg n. This means that the value of lg n is ﬁxed for
any instance of our data structure. Operations access, rank and select, as well as insertions and
deletions, are easily adapted to handle this split string.

Actually, to have suﬃcient time to build universal tables of size O(nα) for 0 < α < 1, the
solution [MN08] maintains the sequence split into ﬁve, not three, parts. This gives also suﬃcient
time to build any universal table we need to handle block operations in constant time, as well as
to build the wavelet tree structures of the new partitions.

8.4 Memory Management Inside Blocks

The EAs of Lemma 1 (Section 9) have amortized times to grow and shrink. Converting those to
worst-case time requires a constant space overhead factor. While this is acceptable for the EAs of
structures T bl in Section 9, they raise the overall space to O(nH0(S)) bits if used to maintain the
main data. Instead, we get rid completely of the EA mechanism to maintain the data, and use a
single large memory area for all the miniblocks of Section 9, using Munro’s technique [Mun86].

The problem of using a single memory area is that the pointers to the miniblocks require
Θ(lg n) bits, which is excessive because miniblocks are also of Θ(lg n) bits. Instead, we use slightly

15

larger miniblocks, of Θ(lg n lg lg n) bits. This makes the overhead due to pointers to miniblocks
O(|Gj(v)|/ lg lg n), adding up to additional O(nH0(S)/ lg lg n + n lg σ/ lg1−ε n) = o(n lg σ) bits.
The price of using larger miniblocks is that now the operations on blocks are not anymore
constant time because they need to traverse a miniblock, which takes time O(lg lg n). We can still
retain constant time for the query operations, by considering logical miniblocks of Θ(lg n) bits,
which are stored in physical areas of Θ(lg lg n) miniblocks. However, update operations like insert
and delete must shift all the data in the miniblock area and possibly relocate it in the memory
manager, plus updating pointers to all the logical miniblocks displaced or relocated. This costs
O(lg lg n) time per insertion and deletion. This completes our result.

Theorem 3 A dynamic string S[1, n] over alphabet [1..σ] can be stored in a structure using nH0(S)+
O(nH0(S)/ lg lg n) + O(n lg σ/ lg1−ε n) + O(σ lg n) = nH0(S) + o(n lg σ) + O(σ lg n) bits, for any
constant 0 < ε < 1, and supporting queries access and select in worst-case time O( 1
ε2 lg n/ lg lg n),
and query rank, insertions and deletions in worst-case time O( 1

ε lg n).

9 Data Structures for Handling Blocks

We describe the way the data is stored in blocks Gj(v), as well as the way the various structures
inside blocks operate. All the data structures are based on the same idea: We maintain a tree
with node degree lgδ n and leaves that contain O(lg n) bits. Since elements within a block can
be addressed with O(lg lg n) bits, each internal node and each leaf ﬁts into one machine word.
Moreover, we can support searching and basic operations in each node in constant time.

9.1 Data Organization

The block data is physically stored as a sequence of miniblocks of Θ(lgρ n) symbols, or Θ(lg n) bits.
Thus there are O(lg2 n) miniblocks in a block. These miniblocks will be the leaves of a τ -ary tree
T , for τ = Θ(lgδ n) and some constant 0 < δ < 1. The height of this tree is constant, O(1/δ).
Each node of T stores τ counters telling the number of symbols stored at the leaves that descend
from each child. This requires just O(τ lg lg n) = o(lg n) bits. To access any position of Gj(v), we
descend in T , using the counters to determine the correct child. When we arrive at a leaf, we know
the local oﬀset of the desired symbol within the leaf, and can access it directly. Since the counters
ﬁt in less than a machine word, a small universal table gives the correct child in constant time,
therefore we have O(1) time access to any symbol (actually to any Θ(lgρ n) consecutive symbols).
Upon insertions or deletions, we arrive at the correct leaf, insert or delete the symbol (in
constant time because the leaf contains Θ(lg n) bits overall), and update the counters in the path
from the root (in constant time as they have o(lg n) bits). The leaves may have lg n to 2 lg n bits.
Splits/merges upon overﬂows/underﬂows are handled as usual, and can be solved in a constant
number of O(1)-time operations (T operates as a B-tree; internal nodes may have τ to 2τ children).
The space overhead due to the nodes of T is O(|Gj(v)| lg lg n/ lg n) bits, where we measure
|Gj(v)| in bits, not symbols. The factor τ disappears because each leaf of T has τ miniblocks.
We consider now the space used by the data itself. In order not to waste space, the miniblock
leaves are stored using a memory management structure by Munro [Mun86]. For our case, it
allows us to allocate, free, and access miniblocks of length up to 2 lg n in constant time.
Its
space waste, given that our pointers are internal to blocks and require O(lg lg n) bits, is O(lg lg n)
per allocated miniblock, which adds up to O(|Gj(v)| lg lg n/ lg n), plus a global redundancy of

16

O(lg2 n) bits. If we used one allocation structure per block, handling its miniblocks, the global
redundancy of O(lg2 n) bits per block would add O(n lg σ/ lg n + σ lg2 n) bits overall. This is
reduced to O(n lg σ/ lg2 n + σ lg n) by using one allocation structure per group of lg n blocks. This
reduces the overhead of the structures and the address space is still of size O(lg4 n), so pointers
can still be of length O(lg lg n).

Each allocation structure uses a memory area of ﬁxed-size cells (inside which the variable-length
miniblocks are stored) that grows or shrinks at the end as miniblocks are created or destroyed. A
structure to store those memory areas with ﬁxed-size cells and allowing them to grow and shrink is
the extendible array (EA) [RR03]. We need to handle a set of O(n lg σ/ lg4 n + σ/ lg n) EAs, what
is called a collection of extendible arrays. It supports accessing any cell of any EA, letting any EA
grow or shrink by one cell, and create and destroy EAs. The following lemma, simpliﬁed from the
original [RR03, Lemma 1], and using words of lg n bits, is useful.
Lemma 1 A collection of a EAs of total size s bits can be represented using s + O(a lg n +√sa lg n)
bits of space, so that the operations of creation of an empty EA and access take constant worst-case
time, whereas grow/shrink take constant amortized time. An EA of s′ bits can be destroyed in time
O(s′/ lg n).

In our case a = O(n lg σ/ lg4 n + σ/ lg n) and s = O(n lg σ), so the space overhead posed by the

EAs is O(n lg σ/ lg3 n + σ + n lg σ/ lg3/2 n + √nσ lg σ) = O(n lg σ/ lg n + σ lg n).

When we store the miniblocks in compressed form, in Section 7, they could use as little as
O(lgε n lg lg n) bits, and thus we could store up to Θ(lg1−ε n/ lg lg n) miniblocks in a single leaf
of T . This can still can be handled in constant time using (more complicated) universal tables
[MN08], and the counters and pointers of O(lg lg n) bits are still large enough.

9.2 Structure Rj(v)

To support rank and select we enrich T with further information per node. We store ρ counters 
with the number of occurrences of each symbol in the subtree of each child. The node
size becomes O(τ ρ lg lg n) = O(lgε+δ n lg lg n) = o(lg n) as long as ε + δ < 1. This adds up to
O(|Gj(v)|ρ lg lg n/ lg n) bits because the leaves of T handle τ miniblocks.
With this information on the nodes we can easily solve rank and select in constant time, by
descending on T and determining the correct child (and accumulating data on the leftward children)
in O(1) time using universal tables. Nodes can also be updated in constant time even upon splits
and merges, since all the counters can be recomputed in O(1) time.

9.3 Structure Fj(v)

This structure stores all the inter-node pointers leaving from block Gj(v), to its parent and to any
of the ρ children of node v.

The structure is a tree Tf very similar in spirit to T . The pointers are stored at the leaves of
Tf , in increasing order of their source position inside Gj(v). The pointers stored are inter-node,
and thus require Θ(lg n) bits. Thus we store a constant number of pointers per leaf of Tf . For
each pointer we store the position in Gj(v) holding the pointer (relative to the starting position of
the leaf node inside Gj(v)) and the target position (as an absolute pointer to another Gℓ(u)). The
internal nodes, of arity τ , maintain information on the number of positions of Gj(v) covered by
each child, and the number of pointers of each kind (1 + ρ counters) stored in the subtree of each

17

child. This requires O(τ ρ lg lg n) = o(lg n) bits, as before. To ﬁnd the last position before i holding
a pointer of a certain kind (parent or t-th wavelet tree child, for any 1 ≤ t ≤ ρ), we traverse Tf
from the root looking for position i. At each node x, it might be that the child y where we have
to enter holds pointers of that kind, or not. If it does, then we ﬁrst enter into child y. If we return
with an answer, we recursively return it. If we return with no answer, or there are no pointers
of the desired kind below y, we enter into the last sibling to the left of y that holds a pointer of
the desired kind, and switch to a diﬀerent mode where we simply go down the tree looking for the
rightmost child with a pointer of the desired kind. It is not hard to see that this procedure visits
O(1/δ) nodes, and thus it is constant-time because all the computations inside nodes can be done
in O(1) time with universal tables. When we arrive at the leaf, we scan for the desired pointer in
constant time.

The tree Tf must be updated when a symbol t is inserted before any other occurrence of t in
Gj(v), when a symbol is inserted at the ﬁrst position of Gj(v) and, similarly, when symbols are
deleted from Gj(v). The needed queries are easily answered with tree T . Moreover, due to the
bidirectionality, we must also update Tf when pointers to Gj(v) are created from the parent or
a child of v, or when they are deleted. Those updates work just like on the tree T . Tf is also
updated upon insertions and deletions of symbols, even if they do not change pointers, to maintain
the positions up to date. In this case we traverse Tf looking for the position of the update, change
the oﬀsets stored at the leaf, and update the subtree sizes stored at the nodes.

9.4 Structure Hj(v)

This structure manages the inter-node pointers that point inside Gj(v). As explained in Section 4.4,
we give a handle to the outside nodes, that does not change over time, and Hj(v) translates handles
to positions in Gj(v).

We store a tree Th that is just like Tf , where the incoming pointers are stored. Th is simpler,
however, because at each node we only need to store the number of positions covered by the subtree
of each child. It must also be possible to traverse Th from a leaf to the root.

In addition, we manage a table T bl so that T bl[h] points to the leaf of Th where the pointer
corresponding to handle h is stored. T bl is also managed as a tree similar to Tf , with pointers
sorted by id, where a constant number of ids h are stored at the leaves together with their pointers
to the leaves of Th (note that there are O(lg3 n/ lg ρ) ids at most, so we need O(lg lg n) bits for both
ids and their pointers to Th). Each internal node in T bl maintains the maximum id stored at its
leaves and the number of ids stored at its leaves. Thus one can in constant time ﬁnd the pointer
to Th corresponding to a given id, and also ﬁnd the smallest unused id when a fresh one is needed
(by looking for the ﬁrst leaf of T bl where the maximum id is larger than the number of ids).

At the leaves of Th we store, for each pointer, a backpointer to the corresponding leaf of T bl
and the position in Gj(v) (in relative form). Given a handle h, we ﬁnd using T bl the corresponding
position in the leaf of Th, and move upwards up to the root of Th, adding to the leaf oﬀset the
number of positions covered by the leftward children of each node. At the end we have obtained
the position in constant time.

When pointers to Gj(v) are created or destroyed, we insert or remove pointers in Th. This
requires traversing it top-down to ﬁnd the appropriate leaf position and returning back to the root
updating oﬀsets. Backpointers to T bl are used to adjust a constant number of positions in the leaf
of Th. We must also update Th upon symbol insertions and deletions in Gj(v), to maintain the
positions up to date. When a leaf splits or merges, we also update the pointers from a constant

18

number of positions in T bl, found with the backpointers. Similarly, the insertion and deletion of
pointers from outside require updating T bl, and the backpointers from Th are maintained up to
date using the pointers from T bl to Th.

T bl may contain up to Θ(lg3 n/ lg ρ) pointers of O(lg lg n) bits, which can be signiﬁcant for some
blocks. However, across the whole structure there can be only O(ρn lg σ/ lg3 n) pointers, adding
up to s = O(ρn lg σ lg lg n/ lg3 n) bits, spread across a = O(n lg σ/ lg3 n) tables T bl. Using again
Lemma 1, a collection of EAs poses an overhead of O(n lg σ/ lg2 n) bits.

9.5 Structure Dj(v) and the Final Result

Structure Dj(v) is implemented as a tree Td analogous to T , storing at each node the number of positions 
and the number of non-deleted positions below each child. It requires O(|Gj(v)| lg lg n/ lg n)
bits. Since these are stored only for the root vr and the leaves va of T , its space adds up to
O(n lg ρ lg lg n/ lg n) = O(n(lg lg n)2/ lg n) bits.
While the raw data adds up to n lg σ bits, the space overhead adds up to O(n lg σ lgε n lg lg n/ lg n)
for all the pointers plus O(n lg σ/ lg n + σ lg n) for the memory management overhead. We can use,
say, δ = ε and then have O(1/ε) time and O(n lg σ/ lg1−ε n + σ lg n) bits for any 0 < ε < 1 (renaming 
2ε as ε). However, when the data is compressed (Section 7), the sum of all the |Gj(v)| terms in
the space is nH0(S) + O(n lg σ lg lg n/ lgε n). This makes the space overhead related to the memory
management and of Rj(v) structures add up to O(nH0(S)/ lg1−ε n + n lg σ/ lg n + σ lg n) bits.

10 Extensions and Applications

We ﬁrst describe an extension of our results to handling general alphabets, and then various applications 
of the original and the extended results.

10.1 Handling General Alphabets

Our time results do not depend on the alphabet size σ, yet our space does, in a way that ensures
that σ gives no problems as long as σ = o(n) (so σ lg n = o(n lg σ)).

Let us now consider the case where the alphabet Σ is much larger than the eﬀective alphabet
of the string, that is, the set of symbols that actually appear in S at a given point in time. Let
us now use s ≤ n to denote the eﬀective alphabet size. Our aim is to maintain the space within
nH0(S) + o(n lg s) + O(s lg n) bits, even when the symbols come from a large universe Σ = [1..|Σ|],
or even from a general ordered universe such as Σ = R or Σ = Γ∗ (i.e., Σ are words over another
alphabet Γ).

Our mappings SN and N S of Section 6 give a simple way to handle a sequence over an unbounded 
ordered alphabet. By changing SN to a custom structure to search Σ, and storing elements
of Σ in array N S, we obtain the following results, using respectively Theorems 2 and 3.

Theorem 4 A dynamic string S[1, n] over a general alphabet Σ can be stored in a structure using
nH0(S) + o(n lg s) + O(s lg n) + S(s) bits and supporting queries access, rank and select in time
O(T (s) + lg n/ lg lg n). Insertions and deletions of symbols are supported in O(U (s) + lg n/ lg lg n)
amortized time. Here s ≤ n is the number of distinct symbols of Σ occurring in S, S(s) is the
number of bits used by a dynamic data structure to search over s elements in Σ plus to refer to s

19

elements in Σ, T (s) is the worst-case time to search for an element among s of them in Σ, and
U (s) is the amortized time to insert/delete symbols of Σ in the structure.
Theorem 5 A dynamic string S[1, n] over a general alphabet Σ can be stored in a structure using
nH0(S) + o(n lg s) + O(s lg n) +S(s) bits and supporting queries access and select in time O(T (s) +
lg n/ lg lg n) and rank in time O(T (s) + lg n). Insertions and deletions of symbols are supported in
O(U (s) + lg n) time. Here s ≤ n is the number of distinct symbols of Σ occurring in S, S(s) is the
number of bits used by a dynamic data structure to search over s elements in Σ plus to refer to s
elements in Σ, T (s) is the time to search for an element among s of them in Σ, and U (s) is the
time to insert/delete symbols of Σ in the structure. All times are worst-case

Using general and dynamic alphabets had not been achieved in previous dynamic sequence data
structures, because the wavelet has a static shape (and changing it is costly). These results open
the door to using these solutions in various scenarios where alphabet dynamism is essential. We
examine a few interesting particular cases:

• We can handle a sequence of arbitrary real numbers in the comparison model, by using a
balanced tree for the alphabet data structure. If Σ = R we have O(lg s + lg n/ lg lg n) times
using Theorem 4 and O(lg n) worst-case times using Theorem 5. Those complexities are
optimal in the comparison model.

• We can handle a sequence of strings, that is, Σ = Γ∗ on a general alphabet Γ. Here we can
store the eﬀective set of strings in a data structure by Franceschini and Grossi [FG04], so
that operations involving a string a take O(|a| + lg γ + lg n/ lg lg n), where γ is the number of
symbols of Γ actually in use. With Theorem 5 we obtain worst-case times O(|a| + lg γ + lg n).
• If Σ = [1..|Σ|] is a large integer range, we can obtain time O(lg lg |Σ| + lg n/ lg lg n), or worstcase 
times O(lg lg |Σ| + lg n), and the space increases by O(s lg |Σ|) bits, by using y-fast tries
[Wil83] to handle the alphabet.

• Another important particular case is when we maintain a contiguous eﬀective alphabet [1..s],
and only insert new symbols σ +1. This is the case where the symbol identities themselves are
not important. In this case there is no time penalty for letting the alphabet grow dynamically.

10.2 Dynamic Sequence Collections

A landmark application of dynamic sequences, stressed out in several papers along time [CHL04,
MN06, CHLS07, MN06, LP07, MN08, GN08, LP09, GN09, HM10, NS10], is to maintain a collection
C of texts, where one can carry out indexed pattern matching, as well as inserting and deleting
texts from the collection. Plugging in our new representation we can signiﬁcantly improve the time
and space of previous work, with an amortized and with a worst-case update time, respectively.

Theorem 6 There exists a data structure for handling a collection C of texts over an alphabet [1..σ]
within size nHh(C) + o(n lg σ) + O(σh+1 lg n + m lg n) bits, simultaneously for all h. Here n is the
length of the concatenation of m texts, C = T1 ◦ T2 ··· ◦ Tm, and we assume that the alphabet size
is σ = o(n). The structure supports counting of the occurrences of a pattern P in O(|P| lg n/ lg lg n)
time. After counting, any occurrence can be located in time O(lgσ n lg n). Any substring of length
ℓ from any T in the collection can be displayed in time O((ℓ/ lg lg n + lgσ n) lg n). Inserting or

20

deleting a text T takes O(lg n + |T| lg n/ lg lg n) amortized time. For 0 ≤ h ≤ (α lgσ n) − 1, for any
constant 0 < α < 1, the space simpliﬁes to nHh(C) + o(n lg σ) + O(m lg n) bits.
Theorem 7 There exists a data structure for handling a collection C of texts over an alphabet
[1..σ] within size nHh(C) + o(n lg σ) + O(σh+1 lg n + m lg n) bits, simultaneously for all h. Here n is
the length of the concatenation of m texts, C = T1 ◦ T2 ··· ◦ Tm, and we assume that the alphabet
size is σ = o(n). The structure supports counting of the occurrences of a pattern P in O(|P| lg n)
time. After counting, any occurrence can be located in time O(lgσ n lg n lg lg n). Any substring of
length ℓ from any T in the collection can be displayed in time O((ℓ + lgσ n lg lg n) lg n). Inserting
or deleting a text T takes O(|T| lg n) time. For 0 ≤ h ≤ (α lgσ n) − 1, for any constant 0 < α < 1,
the space simpliﬁes to nHh(C) + o(n lg σ) + O(m lg n) bits.

The theorems refer to Hh(C), the h-th order empirical entropy of sequence C [Man01]. This is
a lower bound to any semistatic statistical compressor that encodes each symbol as a function of
the h preceding symbols in the sequence, and it holds Hh(C) ≤ Hh−1(C) ≤ H0(C) ≤ lg σ for any
h > 0. To oﬀer search capabilities, the Burrows-Wheeler Transform (BWT) [BW94] of C, Cbwt, is
represented, not C; then access and rank operations on Cbwt are used to support pattern searches
and text extractions. K¨arkk¨ainen and Puglisi [KP11] showed that, if Cbwt is split into superblocks
of size Θ(σ lg2 n), and a zero-order compressed representation is used for each superblock, the total
bits are nHh(C) + o(n).
We use their partitioning, and Theorems 2 or 3 to represent each superblock. For Theorem 6,
the superblock sizes are easily maintained upon insertions and deletions of symbols, by splitting
and merging superblocks and rebuilding the structures involved, without aﬀecting the amortized
time per operation. They [KP11] also need to manage a table storing the rank of each symbol up
to the beginning of each superblock. This is arranged, in the dynamic scenario, with σ partial sum
data structures containing O(n/(σ lg2 n)) elements each, plus another one storing the superblock
lengths. This adds O(n/ lg n) bits and O(lg n/ lg lg n) time per operation [NS10, Lem. 1]. Upon
blocks splits and merges, we use the same techniques used for K structures described in Section 4.4.
For Theorem 7 we use the smooth block size management algorithm described in Section 8.1
for the superblocks, which guarantees worst-case times and the same space redundancy. Then
partial-sum data structures are used without problems.

Finally, the locating and displaying overheads are obtained by marking one element out of
lgσ n lg lg n, so that the space overhead of o(n lg σ) is maintained. Other simpler data structures
used in previous work [MN08], such as mappings from document identiﬁers to their position in Cbwt
and the samplings of the suﬃx array, can easily be replaced by O(lg n/ lg lg n) time partial-sums
data structures and simpler structures to maintain dictionaries of values [NS10, Lem. 1].

10.3 Burrows-Wheeler Transform

Another application of dynamic sequences is to build the BWT of a text T , T bwt, within compressed
space, by starting from an empty sequence and inserting each new character, T [n], T [n − 1], . . .,
T [1], at the proper positions. Equivalently, this corresponds to initializing an empty collection
and then inserting a single text T using Theorem 6. The result is also stated as the compressed
construction of a static FM-index [FMMN07], a compressed index that consists essentially of a
(static) wavelet tree of T bwt. Our new representation improves upon the best previous result on
compressed space [NS10].

21

Theorem 8 The Alphabet-Friendly FM-index [FMMN07], as well as the BWT [BW94], of a text
T [1, n] over an alphabet of size σ, can be built using nHh(T ) + o(n lg σ) bits, simultaneously for all
1 ≤ h ≤ (α lgσ n) − 1 and any constant 0 < α < 1, in time O(n lg n/ lg lg n). It can also be built
within the same time and nH0(T ) + o(n lg σ) + O(σ lg n) bits, for any alphabet size σ.

We are using Theorem 6 for the case h > 0, and Theorem 2 to obtain a less alphabet-restrictive
result for h = 0 (in this case, we do not split the text into superblocks of O(σ lg2 n) symbols, but
just use a single sequence). Note that, although insertion times are amortized in those theorems,
this result is worst-case because we compute the sum of all the insertion times.

This is the ﬁrst time that o(n lg n) time complexity is obtained within compressed space. Other
space-conscious results that achieve better time complexity (but more space) are Okanohara and
Sadakane [OS09], who achieved optimal O(n) time within O(n lg σ lg lgσ n) bits, and Hon et al.
[HSS09], who achieved O(n lg lg σ) time and O(n lg σ) bits. Older results, like K¨arkk¨ainen’s [K¨ar07],
are superseded.

10.4 Binary Relations

Barbay et al. [BGMR07] show how to represent a binary relation of t pairs relating n “objects”
with σ “labels” by means of a string of t symbols over alphabet [1..σ] plus a bitmap of length t + n.
The idea is to traverse the matrix, say, object-wise, and write down in a string the labels of the
pairs found. Meanwhile we append a 1 to the bitmap each time we ﬁnd a pair and a 0 each time
we move to the next object. Then queries like: ﬁnd the objects related to a label, ﬁnd the labels
related to an object, and tell whether an object and a label are related, are answered via access,
rank and select operations on the string and the bitmap.

A limitation in the past to make this representation dynamic was that creating or removing
labels implied changing the alphabet of the string. Now we can use Theorem 4 to obtain a fully
dynamic representation. We illustrate the case where labels and objects are contiguous values in
integer intervals [1..σ] and [1..n], respectively. We note that the structure on the sequence of labels
is so fast that the bitmap, which is longer, dominates the times.

Theorem 9 A dynamic binary relation consisting of t pairs relating n objects with σ labels can
support the operations of counting and listing the objects related to a given label, counting and listing
the labels related to a given object, and telling whether an object and a label are related, all in time
O(lg(n + t)/ lg lg(n + t)) per delivered datum. Pairs, objects and labels can also be added and deleted
in amortized time O(lg(n + t)/ lg lg(n + t)). The space required is tH + o(t lg σ) + n lg n + σ lg σ +
O(t + n + σ lg t) bits, where H = P1≤i≤σ(ti/t) lg(t/ti) ≤ lg σ, where ti is the number of objects
related to label i. Only labels and objects with no related pairs can be deleted.

Theorem 10 A dynamic binary relation consisting of t pairs relating n objects with σ labels can
support the operations of counting and listing the objects related to a given label, counting and listing
the labels related to a given object, and telling whether an object and a label are related, all in time
O(lg(n + t)) per delivered datum. Pairs, objects and labels can also be added and deleted in time
O(lg(n + t)). The space required is tH + o(t lg σ) + n lg n + σ lg σ + O(t + n + σ lg t) bits, where
H = P1≤i≤σ(ti/t) lg(t/ti) ≤ lg σ, where ti is the number of objects related to label i. Only labels
and objects with no related pairs can be deleted.

22

The careful reader may notice that we have uniformized the times of all the operations for
simplicity, yet some can be slightly faster. For example, listing the m labels related to a given
object requires only O(lg(n + t)/ lg lg(n + t) + m lg t/ lg lg t) time. Also, obviously, we can exchange
labels and objects if desired.

10.5 Directed Graphs

A particularly interesting and general binary relation is a directed graph with n nodes and e edges.
Our binary relation representation allows one to navigate a directed graph in forward and backward
direction, and modify it, within about the space needed by a classical adjacency list representation,
and even less.

Theorem 11 A dynamic directed graph consisting of n nodes and e edges can support the operations 
of counting and listing the neighbors pointed from a node, counting and listing the reverse
neighbors pointing to a node, and telling whether there is a link from one node to another, all in
time O(lg(n + e)/ lg lg(n + e)) per delivered datum. Nodes and edges can be added and deleted in
amortized time O(lg(n + e)/ lg lg(n + e)). The space used is eH + o(e lg n) + n lg n + O(e + n lg e)
bits, where H = P1≤i≤n(ei/e) lg(e/ei) ≤ lg n and ei is the outdegree of node i.
Theorem 12 A dynamic directed graph consisting of n nodes and e edges can support the operations 
of counting and listing the neighbors pointed from a node, counting and listing the reverse
neighbors pointing to a node, and telling whether there is a link from one node to another, all in time
O(lg(n + e)) per delivered datum. Nodes and edges can be added and deleted in time O(lg(n + e)).
The space used is eH + o(e lg n) + n lg n + O(e + n lg e) bits, where H = P1≤i≤n(ei/e) lg(e/ei) ≤ lg n
and ei is the outdegree of node i.

Note also that we can change “outdegree” by “indegree” in the theorem by representing the
transposed graph, as operations are symmetric. Our ability to handle dynamic alphabets is essential
here to allow node insertions and deletions in the graph.

10.6

Inverted Indexes

Finally, we consider an application where the symbols are strings. Take a text T as a sequence of n
words, which are strings over a set of letters Γ. The alphabet Γ is integer and ﬁxed, of size γ. The
alphabet of T is Σ = Γ∗, and its eﬀective alphabet is called the vocabulary V of T , of size |V | = σ.
A positional inverted index is a data structure that, given a word w ∈ V , returns the positions in
T where w appears [BYR11].
A well-known way to simulate a positional inverted index within no extra space on top of the
compressed text is to use a compressed sequence representation for T (over alphabet Σ), so that
operation selectw(T, i) simulates access to the ith position of the list of word w, whereas access to
the original T is provided via access(T, i). Operation rank can be used to emulate various inverted
index algorithms, particularly for intersections [BN09]. The space is the zero-order entropy of the
text seen as a sequence of words, which is very competitive in practice [BYR11]. Our new technique
permits modifying the underlying text, that is, it simulates a dynamic inverted index. For this sake
we use Theorem 4 and compact tries to handle a vocabulary over a ﬁxed alphabet.

23

Theorem 13 A text of n words with a vocabulary of σ words and total length ν over a ﬁxed alphabet
Γ of size γ can be represented within nH0(T ) + o(n lg σ) + O(ν lg γ + σ lg n) bits of space, where
H0(T ) is the word-wise entropy of T . The representation outputs any word T [i] = w given i, ﬁnds
the position of the ith occurrence of any word w, and tells the number of occurrences of any word
w up to position i, all in time O(|w| + lg n/ lg lg n). A word w can be inserted or deleted at any
position in T in amortized time O(|w| + lg n/ lg lg n).
Theorem 14 A text of n words with a vocabulary of σ words and total length ν over a ﬁxed alphabet
Γ of size γ can be represented within nH0(T ) + o(n lg σ) + O(ν lg γ + σ lg n) bits of space, where
H0(T ) is the word-wise entropy of T . The representation outputs any word T [i] = w given i, and
ﬁnds the position of the ith occurrence of any word w, in time O(|w| + lg n/ lg lg n). It tells the
number of occurrences of any word w up to position i, and supports the insertion or deletion of any
word w in T , in time O(|w| + lg n).

We remark that σ and ν are assumed to be O(nα) for some 0 < α < 1 in information retrieval

models [BYR11]. Under this assumption the space is just nH0(T ) + o(n lg σ).

Another kind of inverted index, a non-positional one, relates each word with the documents
where it appears (not to the exact positions). This can be seen as a direct application of our binary
relation representation [BCN10], and our dynamization theorems apply to it as well.

11 Conclusions and Further Challenges

We have obtained O(lg n/ lg lg n) time for all the operations that handle a dynamic sequence on an
arbitrary alphabet [1..σ], matching lower bounds that apply to binary alphabets [FS89], and using
zero-order compressed space. Our structure is faster than the best previous work [HM10, NS10] by
a factor of Θ(lg σ/ lg lg n) when the alphabet is larger than polylogarithmic. The query times are
worst-case, yet the update times are amortized. We also show that it is possible to obtain worstcase 
for all the operations, although times for rank and updates raises to O(lg n). We also show
how to handle general and inﬁnite alphabets. Our result can be applied to a number of problems
and improve previous upper bounds on those; we have described several ones.

We remark that the lower bounds [FS89] are valid also for amortized times, so our amortized
solution is optimal, yet it is not known whether our worst-case solution is optimal. Thus the main
remaining challenge is whether it is possible to attain the optimal O(lg n/ lg lg n) worst-case time
for all the operations.

Another interesting challenge is to support a stronger set of update operations, such as block
edits, concatenations and splits in the sequences. Navarro and Sadakane [NS10] support those
operations within time O(σ lg1+ε n). While it seems feasible to achieve, in our structure, O(σ lg n)
time by using blocks of Θ(lg2 n) bits, the main hurdle is the diﬃculty of mimicking the same splits
and concatenations on the list maintenance data structures we use [IA84, Mor03].

References

[BB04]

D. Blandford and G. Blelloch. Compact representations of ordered sets. In Proc. 15th
SODA, pages 11–19, 2004.

24

[BCN10]

J. Barbay, F. Claude, and G. Navarro. Compact rich-functional binary relation representations.
 In Proc. 9th LATIN, LNCS 6034, pages 170–183, 2010.

[BFLN08] N. Brisaboa, A. Fari˜na, S. Ladra, and G. Navarro. Reorganizing compressed text. In

Proc. 31st SIGIR, pages 139–146, 2008.

[BGMR07] J. Barbay, A. Golynski, I. Munro, and S. Srinivasa Rao. Adaptive searching in succinctly 
encoded binary relations and tree-structured documents. Theoretical Computer
Science, 387(3):284–297, 2007.

[BGNN10] J. Barbay, T. Gagie, G. Navarro, and Y. Nekrich. Alphabet partitioning for compressed

rank/select and applications. In Proc. 21st ISAAC, pages 315–326 (part II), 2010.

[BHMM09] P. Bose, M. He, A. Maheshwari, and P. Morin. Succinct orthogonal range search
structures on a grid with applications to text indexing. In Proc. 11th WADS, pages
98–109, 2009.

[BHMR11] J. Barbay, M. He, I. Munro, and S. Srinivasa Rao. Succinct indexes for strings, binary
relations and multi-labeled trees. ACM Transactions on Algorithms, 7(4):article 52,
2011.

[Ble08]

[BN09]

[BN12]

Guy E. Blelloch. Space-eﬃcient dynamic orthogonal point location, segment intersection,
 and range reporting. In Proc. 19th SODA, pages 894–903, 2008.

J. Barbay and G. Navarro. Compressed representations of permutations, and applications.
 In Proc. 26th STACS, pages 111–122, 2009.

D. Belazzougui and G. Navarro. New lower and upper bounds for representing sequences.
 In Proc. 20th ESA, LNCS 7501, pages 181–192, 2012.

[BW94]

M. Burrows and D. Wheeler. A block sorting lossless data compression algorithm.
Technical Report 124, Digital Equipment Corporation, 1994.

[BYR11]

R. Baeza-Yates and B. Ribeiro. Modern Information Retrieval. Addison-Wesley, 2nd
edition, 2011.

[CHL04]

H.-L. Chan, W.-K. Hon, and T.-W. Lam. Compressed index for a dynamic collection
of texts. In Proc. 15th CPM, LNCS 3109, pages 445–456, 2004.

[CHLS07] H. Chan, W.-K. Hon, T.-H. Lam, and K. Sadakane. Compressed indexes for dynamic

text collections. ACM Transactions on Algorithms, 3(2):article 21, 2007.

[Cla96]

D. Clark. Compact Pat Trees. PhD thesis, University of Waterloo, Canada, 1996.

[CN10]

[FG04]

F. Claude and G. Navarro. Extended compact Web graph representations. In Algorithms 
and Applications (Ukkonen Festschrift), pages 77–91. Springer, 2010.

G. Franceschini and R. Grossi. A general technique for managing strings in comparisondriven 
data structures. In Proc. 31st ICALP, LNCS 3142, pages 606–617, 2004.

[FLMM09] P. Ferragina, F. Luccio, G. Manzini, and S. Muthukrishnan. Compressing and indexing

labeled trees, with applications. Journal of the ACM, 57(1), 2009.

25

[FMMN07] P. Ferragina, G. Manzini, V. M¨akinen, and G. Navarro. Compressed representations
of sequences and full-text indexes. ACM Transactions on Algorithms, 3(2):article 20,
2007.

[FS89]

M. Fredman and M. Saks. The cell probe complexity of dynamic data structures. In
Proc. 21st STOC, pages 345–354, 1989.

[GGV03]

R. Grossi, A. Gupta, and J. Vitter. High-order entropy-compressed text indexes. In
Proc. 14th SODA, pages 841–850, 2003.

[GHSV07] A. Gupta, W.-K. Hon, R. Shah, and J. Vitter. A framework for dynamizing succinct

data structures. In Proc. 34th ICALP, pages 521–532, 2007.

[GMR06] A. Golynski, J. I. Munro, and S. S. Rao. Rank/select operations on large alphabets: a

tool for text indexing. In Proc. 17th SODA, pages 368–373, 2006.

[GN08]

[GN09]

[HM10]

R. Gonz´alez and G. Navarro. Improved dynamic rank-select entropy-bound structures.
In Proc. 8th LATIN, LNCS 4957, pages 374–386, 2008.

R. Gonz´alez and G. Navarro. Rank/select on dynamic compressed sequences and
applications. Theoretical Computer Science, 410:4414–4422, 2009.

M. He and I. Munro. Succinct representations of dynamic strings. In Proc. 17th SPIRE,
pages 334–346, 2010.

[HSS03] W.-K. Hon, K. Sadakane, and W.-K. Sung. Succinct data structures for searchable

partial sums. In Proc. 14th ISAAC, pages 505–516, 2003.

[HSS09] W.-K. Hon, K. Sadakane, and W.-K. Sung. Breaking a Time-and-Space Barrier in
Constructing Full-Text Indices. SIAM Journal of Computing, 38(6):2162–2178, 2009.

[HSS11] W.-K. Hon, K. Sadakane, and W.-K. Sung. Succinct data structures for searchable
partial sums with optimal worst-case performance. Theoretical Computer Science,
412(39):5176–5186, 2011.

[IA84]

[K¨ar07]

[KP11]

[LP07]

[LP09]

H. Imai and T. Asano. Dynamic segment intersection search with applications.
Proc. 25th FOCS, pages 393–402, 1984.

In

Juha K¨arkk¨ainen. Fast BWT in small space by blockwise suﬃx sorting. Theoretical
Computer Science, 387(3):249–257, 2007.

J. K¨arkk¨ainen and S. J. Puglisi. Fixed block compression boosting in FM-indexes. In
Proc. 18th SPIRE, LNCS 7024, pages 174–184, 2011.

S. Lee and K. Park. Dynamic rank-select structures with applications to run-length
encoded texts. In Proc. 18th CPM, LNCS 4580, pages 95–106, 2007.

S. Lee and K. Park. Dynamic rank/select structures with applications to run-length
encoded texts. Theoretical Computer Science, 410(43):4402–4413, 2009.

[Man01]

G. Manzini. An analysis of the Burrows-Wheeler transform. Journal of the ACM,
48(3):407–430, 2001.

26

[Meh84]

K. Mehlhorn. Data Structures and Algorithms 1: Sorting and Searching. EATCS
Monographs on Theoretical Computer Science. Springer-Verlag, 1984.

[MN06]

[MN08]

[Mor03]

V. M¨akinen and G. Navarro. Dynamic entropy-compressed sequences and full-text
indexes. In Proc. 17th CPM, LNCS 4009, pages 307–318, 2006.

V. M¨akinen and G. Navarro. Dynamic entropy-compressed sequences and full-text
indexes. ACM Transactions on Algorithms, 4(3):article 32, 2008.

C.W. Mortensen. Fully-dynamic two dimensional orthogonal range and line segment
intersection reporting in logarithmic time. In Proc. 14th SODA, pages 618–627, 2003.

[Mun86]

J. I. Munro. An implicit data structure supporting insertion, deletion, and search in
O(log n) time. Journal of Computer and Systems Sciences, 33(1):66–74, 1986.

[Mun96]

I. Munro. Tables. In Proc. 16th FSTTCS, LNCS 1180, pages 37–42, 1996.

[Nav12]

G. Navarro. Wavelet trees for all. In Proc. 23rd CPM, LNCS 7354, pages 2–26, 2012.

[Nek11]

[NM07]

[NS10]

[OS09]

[Pat07]

[RR03]

Yakov Nekrich. A dynamic stabbing-max data structure with sub-logarithmic query
time. In Proc. 22nd ISAAC, pages 170–179, 2011.

G. Navarro and V. M¨akinen. Compressed full-text indexes. ACM Computing Surveys,
39(1):article 2, 2007.

G. Navarro and K. Sadakane. Fully-functional static and dynamic succinct trees. CoRR,
abs/0905.0768v5, 2010. To appear in ACM Transactions on Algorithms.

D. Okanohara and K. Sadakane. A linear-time Burrows-Wheeler transform using induced 
sorting. In Proc. 16th SPIRE, LNCS 5721, pages 90–101, 2009.

M. Patrascu. Lower bounds for 2-dimensional range counting. In Proc. 39th STOC,
pages 40–46, 2007.

R. Raman and S. Srinivasa Rao. Succinct dynamic dictionaries and trees. In Proc.
30th ICALP, pages 357–368, 2003.

[RRR07]

R. Raman, V. Raman, and S. S. Rao. Succinct indexable dictionaries with applications
to encoding k-ary trees, preﬁx sums and multisets. ACM Transactions on Algorithms,
3(4):article 8, 2007.

[VM07]

[Wil83]

N. V¨alim¨aki and V. M¨akinen. Space-eﬃcient algorithms for document retrieval.
Proc. 18th CPM, pages 205–215, 2007.

In

D. Willard. Log-logarithmic worst-case range queries are possible in space θ(n). Information 
Processing Letters, 17(2):81–84, 1983.

27

