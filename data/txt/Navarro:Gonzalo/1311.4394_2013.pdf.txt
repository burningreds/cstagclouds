3
1
0
2

 

v
o
N
8
1

 

 
 
]
S
D
.
s
c
[
 
 

1
v
4
9
3
4

.

1
1
3
1
:
v
i
X
r
a

Encoding Range Minimum Queries∗

Pooya Davoodi†

Polytechnic Institute of New York University, United States

pooyadavoodi@gmail.com

Gonzalo Navarro‡

Department of Computer Science, University of Chile, Chile

gnavarro@dcc.uchile.cl

Rajeev Raman

Department of Computer Science, University of Leicester, UK

rr29@leicester.ac.uk

S. Srinivasa Rao§

School of Computer Science and Engineering, Seoul National

University, Republic of Korea
ssrao@cse.snu.ac.kr

Abstract

We consider the problem of encoding range minimum queries (RMQs): given
an array A[1..n] of distinct totally ordered values, to pre-process A and create a
data structure that can answer the query RMQ(i, j), which returns the index containing 
the smallest element in A[i..j], without access to the array A at query
time. We give a data structure whose space usage is 2n + o(n) bits, which is
asymptotically optimal for worst-case data, and answers RMQs in O(1) worstcase 
time. This matches the previous result of Fischer and Heun, but is obtained

∗An extended abstract of some of the results in Sections 1 and 2 appeared in Proc. 18th Annual International 
Conference on Computing and Combinatorics (COCOON 2012), Springer LNCS 7434, pp. 396–407.
†Research supported by NSF grant CCF-1018370 and BSF grant 2010437.
‡Partially funded by Millennium Nucleus Information and Coordination in Networks ICM/FIC P10-024F,
§Research partly supported by Basic Science Research Program through the National Research Foundation 
of Korea funded by the Ministry of Education, Science and Technology (Grant number 2012-0008241).

Chile.

1

in a more natural way. Furthermore, our result can encode the RMQs of a random 
array A in 1.919n + o(n) bits in expectation, which is not known to hold
for Fischer and Heun’s result. We then generalize our result to the encoding range
top-2 query (RT2Q) problem, which is like the encoding RMQ problem except that
the query RT2Q(i, j) returns the indices of both the smallest and second-smallest
elements of A[i..j]. We introduce a data structure using 3.272n+o(n) bits that answers 
RT2Qs in constant time, and also give lower bounds on the effective entropy
of RT2Q.

1 Introduction
Given an array A[1..n] of elements from a totally ordered set, the range minimum
query (RMQ) problem is to pre-process A and create a data structure so that the query
RMQ(i, j), which takes two indices 1 ≤ i ≤ j ≤ n and returns argmini≤k≤jA[k],
is supported efﬁciently (both in terms of space and time). We consider the encoding
version of this problem: after pre-processing A, the data structure should answer RMQs
without access to A; in other words, the data structure should encode all the information
about A needed to answer RMQs.
In many applications that deal with storing and
indexing massive data, the values in A have no intrinsic signiﬁcance and A can be
discarded after pre-processing (for example, A may contain scores that are used to
determine the relative order of documents returned in response to a search query). As
we now discuss, the encoding of A for RMQs can often take much less space than A
itself, so encoding RMQs can facilitate the efﬁcient in-memory processing of massive
data.

It is well known [7] that the RMQ problem is equivalent to the problem of supporting 
lowest common ancestor (LCA) queries on a binary tree, the Cartesian tree of A
[21]. The Cartesian tree of A is a binary tree with n nodes, in which the root is labeled
by i where A[i] is the minimum element in A; the left subtree of of the root is the
Cartesian tree of A[1..i − 1] and the right subtree of the root is the Cartesian tree of
A[i + 1..n]. The answer to RMQ(i, j) is the label of the LCA of the nodes labeled by i
and j. Thus, knowing the topology of the Cartesian tree of A sufﬁces to answer RMQs
on A.

Farzan and Munro [4] showed that an n-node binary tree can be represented in
2n + o(n) bits, while supporting LCA queries in O(1) time1. Unfortunately, this does
not solve the RMQ problem. The difﬁculty is that nodes in the Cartesian tree are labelled 
with the index of the corresponding array element, which is equal to the node’s
rank in the inorder traversal of the Cartesian tree. A common feature of succinct tree
representations, such as that of [4], is that they do not allow the user to specify the
numbering of nodes [19], and while existing succinct binary tree representations support 
numberings such as level-order [13] and preorder [4], they do not support inorder.
Indeed, for this reason, Fischer and Heun [5] solved the problem of optimally encoding
RMQ via an ordered rooted tree, rather than via the more natural Cartesian tree.

1The time complexity of this result assumes the word RAM model with logarithmic word size, as do all

subsequent results in this paper.

2

Our ﬁrst contribution is to describe how, using o(n) additional bits, we can add the

functionality below to the 2n + o(n)-bit representation of Farzan and Munro:

• node-rankinorder(x): returns the position in inorder of node x.
• node-selectinorder(y): returns the node z whose inorder position is y.

Here, x and z are node numbers in the node numbering scheme of Farzan and Munro,
and both operations take O(1) time. Using this, we can encode RMQs of an array A
using 2n + o(n) bits, and answer RMQs in O(1) time as follows. We represent the
Cartesian tree of A using the representation of Farzan and Munro, augmented with the
above operations, and answer RMQ(i, j) as

RMQ(i, j) = node-rankinorder(LCA(node-selectinorder(i), node-selectinorder(j))).

We thus match asymptotically the result of Fischer and Heun [5], while using a more
direct approach. Furthermore, using our approach, we can encode RMQs of a random
permutation using 1.919n + o(n) bits in expectation and answer RMQs in O(1) time.
It is not clear how to obtain this result using the approach of Fischer and Heun.

Our next contribution is to consider a generalization of RMQs, namely, to preprocess 
a totally ordered array A[1..n] to answer range top-2 queries (RT2Q). The
query RT2Q(i, j) returns the indices of the minimum as well as the second minimum
values in A[i..j]. Again, we consider the encoding version of the problem, so that the
data structure does not have access to A when answering a query. Encoding problems,
such as the RMQ and RT2Q, are fundamentally about determining the effective entropy
of the data structuring problem [9]. Given the input data drawn from a set of inputs S,
and a set of queries Q, the effective entropy of Q is (cid:100)log2 |C|(cid:101), where C is the set of
equivalence classes on S induced by Q, whereby two objects from S are equivalent if
(cid:0)2n
they provide the same answer to all queries in Q. For the RMQ problem, it is easy to
see that every binary tree is the Cartesian tree of some array A. Since there are Cn =
2n − O(log n) bits.

(cid:1) n-node binary trees, the effective entropy of RMQ is exactly (cid:100)log2 Cn(cid:101) =

The effective entropy of the more general range top-k problem, or ﬁnding the indices 
of the k smallest elements in a given range A[i, j], was recently shown to be
Ω(n log k) bits by Grossi et al. [11]. However, for k = 2, their approach only shows
that the effective entropy of RT2Q is ≥ n/2 – much less than the effective entropy of
RMQ. Using an encoding based upon merging paths in Cartesian trees, we show that
the effective entropy of RT2Q is at least 2.638n − O(log n) bits. We show that this
effective entropy applies also to the (apparently) easier problem of returning just the
second minimum in an array interval, R2M(i, j). We complement this result by giving
a data structure for encoding RT2Qs that takes 3.272n + o(n) bits and answers queries
in O(1) time. This structure builds upon our new 2n + o(n)-bit RMQ encoding by
adding further functionality to the binary tree representation of Farzan and Munro. We
note that the range top-k encoding of Grossi et al. [11] builds upon a encoding that
answers RT2Q in O(1) time, but their encoding for this subproblem uses 6n + o(n)
bits.

1

n+1

n

3

1.1 Preliminaries
Given a bit vector B[1..m], rankB(1, i) returns the number of 1s in B[1..i], and selectB(1, i)
returns the position of the ith 1 in B. The operations rankB(0, i) and selectB(0, i) are
deﬁned analogously for 0s. A data structure that supports the operations rank and select 
is a building block of many succinct data structures. The following lemma states a
rank-select data structure that we use to obtain our results.
Lemma 1. [1, 16] Given a bit vector B[1..m], there exists a data structure of size
m + o(m) bits that supports rankB(1, i), rankB(0, i) selectB(1, i), and selectB(0, i)
in O(1) time.

We also utilize the following lemma, which states a more space-efﬁcient rank-select

data structure that assumes the number of 1s in B is known.
Lemma 2. [18] Given a bit vector B[1..m] that contains n 1s, there exists a data struc-

(cid:1) + o(m) bits, that supports rankB(1, i), rankB(0, i) selectB(1, i),

ture of size log(cid:0)m

and selectB(0, i) in O(1) time.

n

2 Representation Based on Tree Decomposition

We now describe a succinct representation of binary trees that supports a comprehensive 
list of operations [12, 3, 4].2. The structure of Farzan and Munro [4] supports 
multiple orderings on the nodes of the tree including preorder, postorder, and
DFUDS order by providing the operations node-rankpreorder(v), node-selectpreorder(v),
node-rankpostorder(v), node-selectpostorder(v), node-rankDFUDS(v), and node-selectDFUDS(v).
We provide two additional operations node-rankinorder(v) and node-selectinorder(v) thereby
also supporting inorder numbering on the nodes.

Our data structure consists of two parts: (a) the data structure of Farzan and

Munro [4], and (b) an additional structure we construct to speciﬁcally support node-rankinorder
and node-selectinorder. In the following, we outline the ﬁrst part (refer to Farzan and
Munro [4] for more details), and then we explain in detail the second part.

2.1 Succinct cardinal trees of Farzan and Munro [4]
Farzan and Munro [4] reported a succinct representation of cardinal trees (k-ary trees).
Since binary trees are a special case of cardinal trees (when k = 2), their data structure
can be used as a succinct representation of binary trees. The following lemma states
their result for binary trees:
Lemma 3. [4] A binary tree with n nodes can be represented using 2n + o(n) bits of
space, while a comprehensive list of operations [4, Table 2] (or see Footnote 2) can be
supported in O(1) time.

2 This list includes left-child(v), right-child(v), parent(v), child-rank(v), degree(v), subtree-size(v),
depth(v), height(v), left-most-leaf(v), right-most-leaf(v), leaf-rank(v), leaf-select(j), level-ancestor(v, i),
LCA(u, v),
and
level-predecessor(v), where v denotes a node, i denotes a level, and j is an integer. Refer to the
original articles [12, 4] for the deﬁnition of these operations.

level-right-most(i),

level-successor(v),

level-left-most(i),

distance(u, v),

4

This data structure is based on a tree decomposition similar to previous ones [8,
12, 17]. An input binary tree is ﬁrst partitioned into O(n/ log2 n) mini-trees each of
size at most (cid:100)log2 n(cid:101), that are disjoint aside from their roots. Each mini-tree is further
8 (cid:101), which are also
partitioned (recursively) into O(log n) micro-trees of size at most (cid:100) lg n
disjoint aside from their roots. A non-root node in a mini-tree t, that has a child located
in a different mini-tree, is called a boundary node of t (similarly for micro-trees).

The decomposition algorithm achieves the following prominent property: each
mini-tree has at most one boundary node and each boundary node has at most one child
located in a different mini-tree (similar property holds for micro-trees). This property
implies that aside from the edges on the mini-tree roots, there is at most one edge in
each mini-tree that connects a node of the mini-tree to its child in another mini-tree.
These properties also hold for micro-trees.

It is well-known that the topology of a tree with k nodes can be described with
a ﬁngerprint of size 2k bits. Since the micro-trees are small enough, the operations
within the micro-trees can be performed by using a universal lookup-table of size o(n)
bits, where the ﬁngerprints of micro-trees are used as indexes into the table.

The binary tree representation consists of the following parts (apart from the lookup-
table): 1) representation of each micro-tree: its size and ﬁngerprint; 2) representation
of each mini-tree: links between the micro-trees within the mini-tree; 3) links between
the mini-trees. The overall space of this data structure is 2n + o(n) bits [4].

2.2 Data structure for node-rankinorder and node-selectinorder
We present a data structure that is added to the structure of Lemma 3 in order to support
node-rankinorder and node-selectinorder. This additional data structure contains two separate 
parts, each to support one of the operations. In the following, we describe each of
these two parts. Notice that we have access to the succinct binary tree representation
of Lemma 3.

2.2.1 Operation node-rankinorder
We present a data structure that can compute the inorder number of a node v, given its
preorder number. To compute the inorder number of v, we compute two values c1(v)
and c2(v) deﬁned as follows. Let c1(v) be the number of nodes that are visited before
v in inorder traversal and visited after v in preorder traversal; and let c2(v) be the
number of nodes that are visited after v in inorder traversal and visited before v in
preorder traversal (our method below to compute c2(v) is also utilized in Section 3 to
perform an operation called Ldepth(v), which computes c2(v) for any given node v).
Observe that the inorder number of v is equal to its preorder number plus c1(v)−c2(v).
The nodes counted in c1(v) are all the nodes located in the left subtree of v, which
can be counted by subtree size of the left child of v. The nodes counted in c2(v)
are all the ancestors of v whose left child is also on the v-to-root path, i.e., c2(v) is
the number of left-turns in the v-to-root path. We compute c2(v) in a way similar
to computing the depth of a node as follows. For the root rm of each mini-tree, we
precompute and store c2(rm) which requires O((n/ log2 n) log n) = o(n) bits. Let
mini-c2(v) and micro-c2(v) be the number of left turns from a node v up to only the

5

root of respectively the mini-tree and micro-tree containing v. For the root rµ of each
micro-tree, we precompute and store mini-c2(rµ). We use a lookup table to compute
micro-c2(v) for every node v.

Finally, to compute c2(v), we simply calculate c2(rm)+mini-c2(rµ)+micro-c2(v),
where rm and rµ are the root of respectively the mini-tree and micro-tree containing v.
The data structure of Lemma 3 can be used to ﬁnd rm and rµ and the calculation can
be done in O(1) time.

2.2.2 Operation node-selectinorder
We present a data structure that can compute the preorder number of a node v, given
its inorder number. To compute the preorder number of v, we compute 1) the preorder
number of the root rm of the mini-tree containing v; and 2) c(v, rm): the number of
nodes that are visited after rm and before v in preorder traversal, which may include
nodes both within and outside the mini-tree rooted at rm. Observe that the preorder
number of v is equal to the preorder number of rm plus c(v, rm). In the following, we
explain how to compute these two quantities:
(1) We precompute the preorder numbers of all the mini-tree roots and store them
in P [0..nm−1] in some arbitrary order deﬁned for mini-trees, where nm = O(n/ log2 n)
is the number of mini-trees. Notice that each mini-tree now has a rank from [0..nm−1].
Later on, when we want to retrieve the preorder number of the root of the mini-tree containing 
v, we only need to determine the rank i of the mini-tree and read the answer
from P [i]. In the following, we explain a data structure that supports ﬁnding the rank
of the mini-tree containing any given node v.

In the preprocessing, we construct a bit-vector A and an array B of mini-tree ranks,
which are initially empty, by traversing the input binary tree in inorder as follows (see
Figure 1 for an example):

For A, we append a bit for each visited node and thus the length of A is n. If
the current visited node and the previous visited node are in two different mini-trees,
then the appended bit is 1, and otherwise 0; if a mini-tree root is common among two
mini-trees, then its corresponding bit is 0 (i.e., the root is considered to belong to the
mini-tree containing its left subtree since a common root is always visited after its left
subtree is visited); the ﬁrst bit of A is always a 1.

For B, we append the rank of each visited mini-tree; more precisely, if the current
visited node and the previous visited node are in two different mini-trees, then we
append the rank of the mini-tree containing the current visited node, and otherwise we
append nothing. Similarly, a common root is considered to belong to the mini-tree
containing its left subtree; the ﬁrst rank in B is the rank of the mini-tree containing the
ﬁrst visited node.

We observe that a node v with inorder number i belongs to the mini-tree with rank
B[rankA(1, i + 1)], and thus P [B[rankA(1, i + 1)]] contains the preorder number of the
root of the mini-tree containing v.

We represent A using the data structure of Lemma 2, which supports rank in constant 
time. In order to analyze the space, we prove that the number of 1s in A is at
most 2nm: each mini-tree has at most one edge leaving the mini-tree aside from its
root, which means that the traversal can enter or re-enter a mini-tree at most twice.

6

Figure 1: Figure depicts a part of a binary tree where t1, t2, and t3 are its three minitrees.
 Node labels are in inorder ordering of these nodes. Each node has a corresponding 
bit in A and each mini-tree has one or two corresponding labels ((cid:96)i is the label of
ti) in B.

Therefore, the space usage is lg(cid:0) n

(cid:1) + o(n) = o(n) bits, as nm = O(n/ log2 n).

2nm

We store P and B explicitly with no preprocessing on them. The length of B is also at
most 2nm by the same argument. Thus, both P and B take O(n/ log2 n·log n) = o(n)
bits.
(2) Let S be the set of nodes that are visited after rm and before v in the preorder
traversal of the tree. Notice that c(v, rm) = |S|. Let tm and tµ be respectively the
mini-tree and micro-tree containing v. We note that S = S1 ∪ S2 ∪ S3, where S1
contains the nodes of S that are not in tm, S2 contains the nodes of S that are in tµ,
and S3 contains the nodes that are in tm and not in tµ. Observe that S1, S2, and S3 are
mutually disjoint. Therefore, c(v, rm) = |S1| + |S2| + |S3|. We now describe how to
compute each size.
S1: If tm has a boundary node which is visited before the root of tµ, then |S1| is
the subtree size of the child of the boundary node that is out of tm; otherwise |S1| = 0.
S2: Since these nodes are within a micro-tree, |S2| can be computed using a lookuptable.

S3: The local preorder number of the root of tµ, which results from traversing tm
while ignoring the edges leaving tm, is equal to |S3|. We precompute the local preorder

7

12345678910111213141516171819202122231234567891011121314151617181920212223...10000010000000101000001...t1t2t3t1t1t2t3A:nodelabels:B:...‘1‘2‘1‘3...numbers of all the micro-tree roots. The method to store these local preorder numbers
and the data structure that we construct in order to efﬁciently retrieve these numbers
is similar to the part (1), whereas here a mini-tree plays the role of the input tree and
micro-trees play the role of the mini-trees. In other words, we construct P , A, and B
of part (1) for each mini-tree. The space usage of this data structure is o(n) bits by the
same argument, regarding the fact that each local preorder number takes O(log log n)
bits.

Theorem 1. A binary tree with n nodes can be represented with a succinct data structure 
of size 2n + o(n) bits, which supports node-rankinorder, node-selectinorder, plus a
comprehensive set of operations [4, Table 2], all in O(1) time.

2.3 RMQs on Random Inputs
The following theorem gives a slight generalization of Theorem 1, which uses entropy
coding to exploit any differences in frequency between different types of nodes (Theorem 
1 corresponds to choosing all the αis to be 1/4 in the following):
Theorem 2. For any positive constants α0, αL, αR and α2, such that α0 + αL + αR +
α2 = 1, a binary tree with n0 leaves, nL (nR) nodes with only a left (right) child and n2
nodes with both children can be represented using
+ o(n)
bits of space, while a full set of operations [4, Table 2] including node-rankinorder,
node-selectinorder and LCA can be supported in O(1) time.

i∈{0,L,R,2} ni lg(1/αi)

(cid:16)(cid:80)

(cid:17)

(cid:16)(cid:80)

Proof. We proceed as in the proof of Theorem 1, but if α = mini∈{0,L,R,2} αi, we
choose the size of the micro-trees to be at most µ = lg n
2 lg(1/α) = Θ(log n). The 2nbit 
term in the representation of [4] comes from the representation of the microtrees.
Given a micro-tree with µi nodes of type i, for i ∈ {0, L, R, 2} we encode it by writing
the node types in level order (cf. [13]) and encoding this string using arithmetic coding
with the probability of a node of type i taken to be αi. The size of this encoding is
+ 2 bits, from which the theorem follows. Note
at most
that our choice of µ guarantees that each microtree ﬁts in lg n
2 bits and thus can still be
manipulated using universal look-up tables.
Corollary 1. If A is a random permutation over {1, . . . , n}, then RMQ queries on A
can be answered using ( 1

3 + lg 3)n + o(n) < 1.919n + o(n) bits in expectation.

i∈{0,L,R,2} µi lg(1/αi)

(cid:17)

Proof. Choose α0 = α2 = 1/3 and αR = αL = 1/6. The claim follows from the fact
that αin is the average value of ni on random binary trees, for any i ∈ {0, L, R, 2}
[10, Theorem 1].

While both our representation and that of Fischer and Heun [5] solve RMQs in
O(1) time and use 2n + o(n) bits in the worst case, ours allows an improvement in
the average case. However, we are unable to match the expected effective entropy of
RMQs on random arrays A, which is ≈ 1.736n + O(log n) bits [9, Thm. 1] (see also
[15]).

8

a na lg n
na

(cid:80)
103 to 107 and measured the entropy (cid:80)

It is natural to ask whether one can obtain improvements for the average case via
Fischer and Heun’s approach [5] as well. Their approach ﬁrst converts the Cartesian 
tree to an ordinal tree (an ordered, rooted tree) using the textbook transformation 
[2]. To the best of our knowledge, the only ordinal tree representation able to
use (2 − Θ(1))n bits is the so-called ultra-succinct representation [14], which uses
+ o(n) bits, where na is the number of nodes with a children. Our empirical 
simulations suggest that the combination of [5] with [14] would not use (2−Ω(1))n
bits on average on random permutations. We generated random permutations of sizes
on the resulting Cartesian trees.
The results, averaged over 100 to 1,000 iterations, are 1.991916, 1.998986, 1.999869,
1.999984 and 1.999998, respectively. The results appear as a straight line on a loglog 
plot, which suggests a formula of the form 2n − f (n) for a very slowly growing
function f (n). Indeed, using the model 2n − O(log n) we obtain the approximation
2n − 0.81 lg n with a mean squared error below 10−9.

a na lg n
na

To understand the observed behaviour, ﬁrst note that when the Cartesian tree is
converted to an ordinal tree, the arity of each ordinal tree node u turns out to be, in
the Cartesian tree, the length of the path from the right child v of u to the leftmost
descendant of u (i.e., the node representing u + 1 if we identify Cartesian tree nodes
with their positions in A). This is called ru (or Lv) in the next section. Next, note that:
Fact 1. The probability that a node v of the Cartesian tree of a random permutation
has a left child is 1
2 .
Proof. Consider the values A[v − 1] and A[v]. If A[v] < A[v − 1], then RMQ(v −
1, v) = v = LCA(v − 1, v), thus v − 1 descends from v and hence v has a left child.
If A[v] > A[v − 1], then RMQ(v − 1, v) = v − 1 = LCA(v − 1, v), thus v descends
from v − 1 and hence v is the leftmost node of the right subtree of v − 1, and therefore
v cannot have a left child. Therefore v has a left child iff A[v] < A[v − 1], which
happens with probability 1

2 in a random permutation.

a

E(na) lg

be(cid:80)

nE(na) =(cid:80)

gard Lv as a geometric variable with parameter 1
would be E(na) = n

Thus, if we disregarded the dependencies between nodes in the tree, we could re2,
 and thus the expected value of na
2a+1 . Taking the expectation as a ﬁxed value, the space would
2a+1 = 2n. Although this is only a heuristic argument 
(as we are ignoring both the dependencies between tree nodes and the variance
of the random variables), our empirical results nevertheless suggest that this simpliﬁed
model is asymptotically accurate, and thus, that no space advantage is obtained by representing 
random Cartesian trees, as opposed to worst-case Cartesian trees, using this
scheme.

n(a+1)

a≥0

3 Range Top-2 Queries

In this section we consider a generalization of the RMQ problem. Again, let A[1..n] be
an array of elements from a totally ordered set. Let R2M(i, j), for any 1 ≤ i < j ≤ n,
denote the position of the second smallest value in A[i..j]. More formally:

R2M(i, j) = argmin{A[k] : k ∈ ([i..j] \ RMQ(i, j))} .

9

The encoding RT2Q problem is to preprocess A into a data structure that, given i, j,
returns RT2Q(i, j) = (RMQ(i, j), R2M(i, j)), without accessing A at query time.

The idea is to augment the Cartesian tree of A, denoted TA, with some information
that allows us to answer R2M(i, j). If h is the position of the minimum element in
A[i..j] (i.e., h = RMQ(i, j)), then h divides [i..j] into two subranges [i..h − 1] and
[h + 1..j], and the second minimum is the smaller of the elements A[RMQ(i, h − 1)]
and A[RMQ(h + 1, j)]. Except for the case where one of the subranges is empty,
the answer to this comparison is not encoded in TA. We describe how to succinctly
encode the ordering between the elements of A that are candidates for R2M(i, j). Our
data structure consists of this encoding together with the encoding of TA using the
representation of Theorem 1 (along with the operations mentioned in Section 2).

We deﬁne the left spine of a node u to be the set of nodes on the downward path
from u (inclusive) that follows left children until this can be done no further. The right
spine of a node is deﬁned analogously. The left inner spine of a node u is the right spine
of u’s left child. If u does not have a left child then it has an empty left inner spine.
The right inner spine is deﬁned analogously. We use the notation lspine(v)/rspine(v),
lispine(v)/rispine(v), Lv/Rv and lv/rv to denote the left/right spines of v, the left/right
inner spines of v, and the number of nodes in the spines and inner spines of v respectively.
 We also assume that nodes are numbered in inorder and identify node names
with their inorder numbers.

As previously mentioned, our data structure encodes the ordering between the candidates 
for R2M(i, j). We ﬁrst identify locations for these candidates:
Lemma 4. In TA, for any i, j ∈ [1..n], i < j, R2M(i, j) is located in lispine(v) or
rispine(v), where v = RMQ(i, j).

Proof. Let v = RMQ(i, j). The second minimum clearly lies in one of two subranges
[i..v− 1] and [v + 1..j], and it must be equal to either RMQ(i, v− 1) or RMQ(v + 1, j).
W.l.o.g. assume that [i..v − 1] is non-empty: in this case the node v − 1 is the bottommost 
node on lispine(v). Furthermore, since v = RMQ(i, j), i must lie in the left
subtree of v. Since the LCA of the bottom-most node on lispine(v) and any other node
in the left subtree of v is a node in lispine(v), RMQ(i, v − 1) is in lispine(v). The
analogous statement holds for rispine(v).

Thus, for any node v, it sufﬁces to store the relative order between nodes in lispine(v)
and rispine(v) to ﬁnd R2M(i, j) for all queries for which v is the answer to the RMQ
query. As TA determines the ordering among the nodes of lispine(v) and also among
the nodes of rispine(v), we only need to store the information needed to merge lispine(v)
and rispine(v). We will do this by storing mv = max(lv + rv − 1, 0) bits associated
with v, for all nodes v, as explained later. We need to bound the total space required
for the ‘merging’ bits, as well as to space-efﬁciently realize the association of v with
the the mv merging bits associated with it. For this, we need the following auxiliary
lemmas:
Lemma 5. Let T be a binary tree with m nodes (of which m0 are leaves) and root u.

v∈T (lv + rv) = 2m − Lu − Ru, and(cid:80)

v∈T mv ≤ m − Lu − Ru + m0.

Then,(cid:80)

10

Proof. The ﬁrst part follows from the fact that the Ru nodes in rspine(u) do not appear
in lispine(v) for any v ∈ T , and all the other nodes in T appear exactly once in a left
inner spine. Similarly, the Lu nodes in lspine(u) do not appear in rispine(v) for any
v ∈ T , and the other nodes in T appear exactly once in a right inner spine. Then the
second part follows from the fact that mv = lv + rv − 1 iff lv + rv > 0, that is, v is
not a leaf. If v is a leaf, then lv + rv = 0 = mv. Thus we must subtract m − m0 from
the previous formula, which is the number of non-leaf nodes in T .

In the following lemma, we utilize two operations Ldepth(v) and Rdepth(v) which
compute the number of nodes that have their left and right child, respectively, in the
path from root to v (recall that Ldepth(v) computes c2(v) deﬁned in Section 2).
Lemma 6. Let T be a binary tree with m nodes and root τ. Suppose that the nodes of
T are numbered 0, . . . , m − 1 in inorder. Then, for any 0 ≤ u < m:

(lj + rj) = 2u − Lτ − lu + Ldepth(u) − Rdepth(u) + 1.

(cid:88)

j<u

Proof. The proof is by induction on m. For the base case m = 1, τ = u = 0 is
the only possibility and the formula evaluates to 0 as expected: lu = Ldepth(u) =
Rdepth(u) = 0 and Lτ = 1 (recall that τ is included in lspine(τ )).

Now consider a tree T with root τ and m > 1 nodes. We consider the three cases
u = τ, u < τ and u > τ in that order. If u = τ then Ldepth(τ ) = Rdepth(τ ) = 0. If
τ has no left child, the situation is the same as when m = 1. Else, letting v be the left
child of τ, note that Lv = Lτ − 1 and since lispine(τ ) = rspine(v), lτ = Rv. As the
subtree rooted at v has size exactly τ, the formula can be rewritten as 2τ − Lv − Rv,
its correctness follows from Lemma 5 without recourse to the inductive hypothesis.
If u < τ then by induction on the subtree rooted at the left child v of τ, the formula
gives 2u − Lv − lu + Ldepth(cid:48)
(u) + 1, where Rdepth(cid:48) and Ldepth(cid:48) are
measured with respect to v. As Ldepth(cid:48)
(u) = Rdepth(u)
and Lv = Lτ − 1, this equals 2u − Lτ − lu + Ldepth(u) − Rdepth(u) + 1 as required.
Finally we consider the case u > τ. Letting v and w be the left and right children
of τ, and u(cid:48) = u − τ − 1, we note that u(cid:48) is the inorder number of u in the subtree
rooted at w. Applying the induction hypothesis to the subtree rooted at w, we get that:

(u) = Ldepth(u)−1, Rdepth(cid:48)

(u) − Rdepth(cid:48)

(lj + rj) = 2u

(cid:48) − Lw − lu + Ldepth(cid:48)

(u) − Rdepth(cid:48)

(u) + 1,

(cid:88)

τ <j<u

where Rdepth(cid:48) and Ldepth(cid:48) are measured with respect to w. Simplifying:

j<u(lj + rj) = (cid:80)
(cid:80)

j<τ (lj + rj) + lτ + rτ +(cid:80)

τ <j<u(lj + rj)

= 2τ − Lv − Rv + lτ + rτ + 2u
= 2τ − Lv − Rv + lτ + rτ + 2u
= 2τ − Lv + 2u
= 2τ − (Lτ − 1) + 2(u − τ − 1) − lu + Ldepth(u) − Rdepth(u) + 2
= 2u − Lτ − lu + Ldepth(u) − Rdepth(u) + 1

(cid:48) − Lw − lu + Ldepth(cid:48)
(u) + 1
(cid:48) − Lw − lu + Ldepth(u) − Rdepth(u) + 2

(cid:48) − lu + Ldepth(u) − Rdepth(u) + 2

(u) − Rdepth(cid:48)

11

Here we have made use (in order) of Lemma 5 and the facts Ldepth(cid:48)
and Rdepth(cid:48)
Corollary 2. In the same scenario of Lemma 6, we have

(u) = Rdepth(u)−1; Lw = rτ and Rv = lτ ; and ﬁnally Lv = Lτ −1.

(u) = Ldepth(u)

mj = 2u − Lτ − lu + Ldepth(u) − Rdepth(u) + 1 − Lleaves(u),

(cid:88)

j<u

where Lleaves(u) is the number of leaves to the left of node u.

Proof. Trivially follows from Lemma 6 and the same considerations as in the proof of
Lemma 5.

The Data Structure. For each node u in TA, we create a bit sequence Mu of length
mu to encode the merge order of lispine(u) and rispine(u). Mu is obtained by taking
the sequence of all the elements of lispine(u) ∪ rispine(u) sorted in decreasing order,
and replacing each element of this sorted sequence by 0 if the element comes from
lispine(u) and by 1 if the element comes from rispine(u) (the last bit is omitted, as it
is unnecessary). We concatenate the bit sequences Mu for all u ∈ TA considered in
inorder and call the concatenated sequence M.

The data structure comprises M, augmented with rank and select operations and
a data structure for TA. If we use Theorem 1, then TA is represented in 2n + o(n) bits,
and the (augmented) M takes at most 1.5n + o(n) bits by Lemmas 5 and 1, since there
are at most (n+1)/2 leaves in an n-node binary tree. This gives a representation whose
space is 3.5n + o(n) bits. A further improvement can be obtained by using Theorem 2
as follows. For some real parameter 0 < x < 1, consider the concave function:

H(x) = 2x lg

1
x

+ 2

(1 − 2x)

2

lg

2

1 − 2x

+ x + 1.

√

2/2 ≈ 0.293, and attains a maximum value of γ = 2 + lg(1 +

Differentiating and simplifying, we get the maximum of H(x) as the solution to the
equation 2(lg(1 − 2x) − lg x) = 1, from which we get that H(x) is maximized at
x = 1−√
2) < 3.272.
Now let n0, nL(nR) and n2 be the numbers of leaves, nodes with only a left (right)
child and nodes with both children in TA. Letting x = n0/n, we apply Theorem 2 to
represent TA, using the parameters α0 = α2 to be equal to x, but capped to a minimum
of 0.05 and a maximum of 0.45, i.e. α0 = α2 = max{min{0.45, x}, 0.05}, and αL =
(cid:17)
αR = (1 − 2α0)/2. Observe that the capping means that αL and αR lie in the range
[0.05, 0.45] as well, thus satisfying the condition in Theorem 2 requiring the αi’s to be
constant. Then the space used by the representation is
+
n + n0 + o(n) bits. Provided capping is not applied, and since n0 = n2 + 1 and αL =
αR, this is easily seen to be nH(x) + o(n) bits, and is therefore bounded by γn + o(n)
bits. If x > 0.45, then the representation takes 2n0 lg(1/0.45)+(n−2n0) lg(1/0.05)+
n+n0+o(n) bits. Since 2 lg(1/0.45)−2 lg(1/0.05)+1 < 0, this is maximized with the
least possible n0 = 0.45n, where the space is precisely nH(0.45)+o(n) < γn+o(n).
Similarly, for x < 0.05 the space is less than nH(0.05) + o(n) < γn + o(n) bits.

i∈{0,L,R,2} ni lg(1/αi)

(cid:16)(cid:80)

12

We now explain how this data structure can answer RT2Q in constant time. We utilize 
the data structure of Theorem 2 constructed on TA in order to ﬁnd u = LCA(i, j) =
RMQ(i, j). Subsequently:

1. We determine the start of Mu within M by calculating(cid:80)

j<u mj.

2. We locate the appropriate nodes from lispine(u) and rispine(u) and the corresponding 
bits within Mu and make the required comparison.

We now explain each of these steps. For step (1), we use Corollary 2. When evaluating
the formula, we use the O(1)-time support for Ldepth(u) and Rdepth(u) given by the
data structure of Section 2; there we explain Ldepth(u) indeed computes c2(u) and we
describe how to compute c2(u) in constant time (computing Rdepth(u) can be done
analogously). This leaves only the computation of lu and Lleaves(u). The former is
done as follows. We check if u has a left child: if not, then lu = 0. Otherwise, if v is u’s
left child, then v and u− 1 are respectively the topmost and lowest nodes in lispine(u).
We can then obtain lu in O(1) time as depth(v)−depth(u) in O(1) time by Theorem 2.
On the other hand, Lleaves(u) can be computed as leaf-rank(v(cid:48) +subtree-size(v(cid:48))− 1),
where v(cid:48) = node-selectinorder(v) and v is the left child of u. If v does not exist then
Lleaves(u) = leaf-rank(u(cid:48)), where u(cid:48) = node-selectinorder(u). All those operations
take O(1) time by Theorem 2.
For step (2) we use Lemma 4 to locate the two candidates from A[i..u − 1] and
A[u + 1..j] (assuming that i < u < j, if not, the problem is easier) in O(1) time as v =
LCA(i, u − 1) and w = LCA(u + 1, j). Next we obtain the rank ρv of v in lispine(u)
in O(1) time as depth(u − 1) − depth(v). The rank ρw of w in rispine(u) is obtained
j<u(lj +rj), we compare selectM (0, rankM (0, ∆)+ρv)
and selectM (1, rankM (1, ∆) + ρw) in O(1) time to determine which of v and w is
smaller and return that as the answer to R2M(i, j).3 We have thus shown:
Theorem 3. Given an array of n elements from a totally ordered set, there exists a
data structure of size at most γn + o(n) bits that supports RT2Qs in O(1) time, where
γ = 2 + lg(1 +

similarly. Now, letting ∆ =(cid:80)

2) < 3.272.

√

Note that γn is a worst-case bound. The size of the encoding can be less for other
values of n0. In particular, since H(x) is convex and the average value of n0 on random
permutations is n/3 [10, Theorem 1], we have by Jensen’s inequality that the expected
size of the encoding is below H(1/3) = lg(3) + 5

3 < 3.252.

4 Effective Entropy of RT2Q and R2M

In this section we lower bound the effective entropy of RT2Q, that is, the number of
equivalence classes C of arrays distinguishable by RT2Qs. For this sake, we deﬁne
extended Cartesian trees, in which each node v indicates a merging order between its

left and right internal spines, using a number in a universe of size(cid:0)lv+rv

(cid:1). We prove

rv

3If we select the last (non-represented) bit of Mu, the result will be out of the Mu area of M, but

nevertheless the result of the comparison will be correct.

13

that any distict extended Cartesian tree can arise for some input array, and that any two
distinct extended Cartesian trees give a different answer for at least some RT2Q. Then
we aim to count the number of distinct extended Cartesian trees.

While unable to count the exact number of extended Cartesian trees, we provide
a lower bound by unrolling their recurrence a ﬁnite number of times (precisely, up to
7 levels). This effectively limits the lengths of internal spines we analyze, and gives
0.160646n θ(n) for a polynomial θ(n), from
us a number of conﬁgurations of the form
where we obtain a lower bound of 2.638n − O(log n) bits on the effective entropy of
RT2Q.

1

We note that our bound on RT2Qs also applies to the weaker R2M operation, since
any encoding answering R2Ms has enough information to answer RT2Qs. Indeed, it
is easy to see that RMQ(i, j) is the only position that is not the answer of any query
R2M(i(cid:48), j(cid:48)) for any i ≤ i(cid:48) < j(cid:48) ≤ j. Then, with RMQ and R2M, we have RT2Q.
Therefore we can give our result in terms of the weaker R2M.

Theorem 4. The effective entropy of R2M (and RT2Q) over an array A[1, n] is at least
2.638n − O(log n).

n

rv

14

1

n+1

n

rv

lg

4.1 Modeling the Effective Entropy of R2M
Recall that to show that the effective entropy of RMQ is 2n − O(log n) bits, we argue
that (i) any two Cartesian trees will give a different answer to at least one RMQ(i, j);
(ii) any binary tree is the Cartesian tree of some permutation A[1, n]; (iii) the number 
of binary trees of n nodes is

(cid:0)2n
(cid:1), thus in the worst case one needs at least

= 2n − O(log n) bits to distinguish among them.

(cid:16) 1
(cid:1)(cid:17)
(cid:0)2n
(cid:1)]. The number M (v) identiﬁes one particular merging order between the
[1..(cid:0)lv+rv
nodes in lispine(v) and rispine(v), and(cid:0)lv+rv
(cid:1) is the exact number of different merg-

n+1
A similar reasoning can be used to establish a lower bound on the effective entropy 
of RT2Q. We consider an extended Cartesian tree T of size n, where for any
node v having both left and right children we store a number M (v) in the range

ing orders we can have.
Now we follow the same steps as before. For (i), let T and T (cid:48) be Cartesian trees
extended with the corresponding numbers M (v) for v ∈ T and M(cid:48)(v(cid:48)) for v(cid:48) ∈ T (cid:48). We
already know that if the topologies of T and T (cid:48) differ, then there exists an RMQ(i, j)
that gives different results on T and T (cid:48). Assume now that the topologies are equal,
but there exists some node v where M (v) differs from M(cid:48)(v). Then there exists an
RT2Q(i, j) where the extended trees give a different result. W.l.o.g., let i and j be the
ﬁrst positions of lispine(v) and rispine(v), respectively, where vl = lispine(v)[i] goes
before vr = rispine(v)[j] according to M (v), but after according to M(cid:48)(v). Then T
answers R2M(v1, v2) = v1 and T (cid:48) answers R2M(v1, v2) = v2 (we interpret v1 and v2
as inorder numbers here).

As for (ii), let T be an extended Cartesian tree, where u is the (inorder number
of the) root of T . Then we build a permutation A[1, n] whose extended tree is T
as follows. First, we set the minimum at A[u] = 0. Now, we recursively build the
ranges A[1, u − 1] (a permutation in with values in [0..u − 1]) and A[u + 1, n] (a

permutation with values in [0..n− u− 1]) for the left and right child of T , respectively.
Assume, inductively, that the permutations already satisfy the ordering given by the
M (v) numbers for all the nodes v within the left and right children of u. Now we are
free to map the values of A \ A[u] to the interval [1, n − 1] in any way that maintains
the relative ordering within A[1, u − 1] and A[u + 1, n]. We do so in such a way that
the elements of lispine(u) and rispine(u) compare according to M (u). This is always
possible: We sort A[1, u − 1] and A[u + 1, n] from smallest to largest values, let A[ai]
be the ith smallest cell of A[1, u − 1] and A[bj] the ith smallest cell of A[u + 1, n].
Also, we set cursors at lispine(u)[l] and rispine(u)[r], initially l = r = 1, and set
c = i = j = 1. At each step, if M (u) indicates that lispine(u)[l] comes before
rispine(u)[r], we reassign A[ai] = c and increase i and c, until (and including) the
reassignment of ai = lispine(u)[l], then we increase l; otherwise we reassign A[bj] = c
and increase j and c, until (and including) the reassignment of bj = rispine(u)[r], then
we increase r. We repeat the process until reassigning all the values in A \ A[u].

For (iii), next we will lower bound the total number of extended Cartesian trees.

4.2 Lower Bound on Effective Entropy
As explained, we have been unable to come up with a general counting for the lower
bound, yet we give a method that can be extended with more and more effort to reach
higher and higher lower limits. The idea is to distinguish the ﬁrst steps in the generation

of the Cartesian tree out of the root node, and charge the minimum value of(cid:0)lv+rv

(cid:1) we

rv

can ensure in each case. Let

(cid:88)

n>0

T (x) =

t(n)xn

where t(n) is the number of extended Cartesian trees with n nodes, counted using some
simple lower-bounding technique. For example, if we consider the simplest model for
T (x), we have that a (nonempty) tree is a root v either with no children, with a left
child rooting a tree, with a right child rooting a tree, or with left and right children

rooting trees, this time multiplied by 2 to account for(cid:0)lv+rv

(cid:1) (see the levels 0

(cid:1) ≥(cid:0)2

rv

1

and 1 in Figure 2). Then T (x) satisﬁes

T (x) = x + xT (x) + xT (x) + 2xT (x)2 = x + 2xT (x) + 2xT (x)2,

which solves to

T (x) =

which has two singularities at x =
2
Thus it follows that t(n) is of the form

(cid:16)(cid:16) 2√

1 − 2x − √
(cid:17)n
(cid:16) 2√
−1±√

2

(cid:17)n

1 − 4x − 4x2
4x

,

(cid:17)

. The one closest to the origin is x =

.
θ(n) for some polynomial θ(n) [20],
n−O(log n) ≥ 2.271n−

(cid:16) 2√

(cid:17)

2−1

√

2−1
2

and thus we need at least lg
O(log n) bits to represent all the possible extended Cartesian trees.

θ(n)

= lg

2−1

2−1

This result can be improved by unrolling the recurrence of T further, that is, replacing 
each T by its four possible alternatives in the basic deﬁnition. Then the lower
bound improves because some left and right internal spines can be seen to have length

15

Level
# of cases
1
4
2
25
3
675
∼ 4.6 × 105
4
5 ∼ 2.1 × 1011
6 ∼ 4.4 × 1022
7 ∼ 1.9 × 1045

# of terms
3
9
63
119
479
1951
7935

degree
2
4
8
16
32
64
128

singularity
0.207107
0.190879
0.179836
0.172288
0.167053
0.163343
0.160646

lower bound

2.271n − O(log n)
2.389n − O(log n)
2.474n − O(log n)
2.537n − O(log n)
2.581n − O(log n)
2.621n − O(log n)
2.638n − O(log n)

Table 1: Our results for increasing number of levels. The second column gives the
number of cases generated, the third the number of terms in the resulting polynomial,
the fourth the degree of the polynomial in x and T , the ﬁfth the x value of the singularity 
found, and the last column gives the implied lower bound.

two or more. The results do not admit easy algebraic solutions anymore, but we can
numerically analyze the resulting functions with Maple and establish a safe numeric
threshold from where higher lower bounds can be derived. For example by doing a
ﬁrst level of replacement in the simple recurrence, we obtain a recurrence with 25
cases, which yields
T (x) = x+2x2 +4x2T (x)+4x2T (x)2 +2x3 +10x3T (x)+26x3T (x)2 +36x3T (x)3 +24x3T (x)4;

(see level 2 in Figure 2) which Maple is able to solve algebraically, although the
formula is too long to display it here. While Maple could not algebraically ﬁnd the
singularities of T (x), we analyzed the result numerically and found a singularity at
x = 0.190879... Therefore, we conclude that t(n) ≥
0.190880n θ(n), and thus that a
lower bound is n lg

0.190880 − O(log n) ≥ 2.389n − O(log n).

1

1

To ﬁnd the singularity we used the result [6, Thm. VII.3] that, under certain conditions 
that are met in our case, the singularities of an equation of the form T (x) =
G(x, T (x)) can be found by numerically solving the system formed by the equation
T = G(x, T ) and its derivative, 1 = ∂G(x,T )
If the positive solution is found at
(x = r, T = γ), then there is a singularity at x = r. If, further, T (x) is aperiodic (as
rn θ(n) for some
in our case), then r is the unique dominant singularity and t(n) = 1
polynomial θ(n).

To carry the idea further, we wrote a program that generates all the combinations
of any desired level, and builds a recurrence to feed Maple with. We use the program
to generate the recurrences of level 3 onwards. Table 1 shows the results obtained up
to level 7, which is the one yielding the lower bound 2.638n− O(log n) of Theorem 4.
This was not without challenges; we describe the details in the Appendix.

∂T

.

5 Conclusions

We obtained a succinct binary tree representation that extends the representation of
Farzan and Munro [4] by supporting navigation based on the inorder numbering of the
nodes, and a few additional operations. Using this representation, we describe how to
encode an array in optimal space in a more natural way than the existing structures,

16

to support RMQs in constant time. In addition, this representation reaches 1.919n +
o(n) bits on random permutations, thus breaking the worst-case lower bound of 2n −
O(log n) bits. This is not known to hold on any alternative representation. It is an open
question to ﬁnd a data structure that answers RMQs in O(1) time using 2n + o(n) bits
in the worst case, while also achieving the expected effective entropy bound of about
1.736n bits for random arrays A.

Then, we obtain another structure that encodes an array of n elements from a total
order using 3.272n + o(n) bits to support RT2Qs in O(1) time. This uses almost half
of the 6n + o(n) bits used for this problem in the literature [11]. Our structure can
possibly be plugged in their solution, thus reducing their space.
While the effective entropy of RMQs is known to be precisely 2n − O(log n) bits,
the effective entropy for range top-k queries is only known asymptotically: it is at least
n lg k−O(n) bits, and at most O(n log k) bits [11]. We have shown that, for k = 2, the
effective entropy is at least 2.638n − O(log n) bits. Determining the precise effective
entropy for k ≥ 2 is an open question.

Acknowledgements

Many thanks to Jorge Olivos and Patricio Poblete for discussions (lectures) on extracting 
asymptotics from generating functions.

References
[1] D. Clark. Compact Pat Trees. PhD thesis, University of Waterloo, Canada, 1996.

[2] Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein.

Introduction to Algorithms. The MIT Press, 2 edition, 2001.

[3] Arash Farzan and J. Ian Munro. A uniform approach towards succinct representation 
of trees. In Proc. 11th Scandinavian Workshop on Algorithm Theory, volume
5124 of LNCS, pages 173–184. Springer-Verlag, 2008.

[4] Arash Farzan and J. Ian Munro. A uniform paradigm to succinctly encode various

families of trees. Algorithmica, to appear, 2012.

[5] Johannes Fischer and Volker Heun. Space-efﬁcient preprocessing schemes for
range minimum queries on static arrays. SIAM Journal on Computing, 40(2):465–
492, 2011.

[6] P. Flajolet and R. Sedgewick. Analytic Combinatorics. Cambridge University

Press, 2009.

[7] Harold N. Gabow, Jon Louis Bentley, and Robert E. Tarjan. Scaling and related
In Proc. 16th annual ACM Symposium on

techniques for geometry problems.
Theory of Computing, pages 135–143. ACM Press, 1984.

17

[8] Richard F. Geary, Rajeev Raman, and Venkatesh Raman. Succinct ordinal trees
with level-ancestor queries. ACM Transactions on Algorithms, 2(4):510–534,
2006.

[9] M. Golin, J. Iacono, D. Krizanc, R. Raman, S. Srinivasa Rao, and S. Shende.

Encoding 2D range maximum queries. CoRR, 1109.2885v2, 2012.

[10] M. J. Golin, John Iacono, Danny Krizanc, Rajeev Raman, and S. Srinivasa Rao.
Encoding 2D range maximum queries. In Proc. 22nd International Symposium on
Algorithms and Computation, volume 7074 of LNCS, pages 180–189. SpringerVerlag,
 2011.

[11] R. Grossi, J. Iacono, G. Navarro, R. Raman, and S. Srinivasa Rao. Encodings for
range selection and top-k queries. In Proc. 21st Annual European Symposium on
Algorithms (ESA), LNCS 8125, pages 553–564, 2013.

[12] Meng He, J. Ian Munro, and S. Srinivasa Rao. Succinct ordinal trees based on
tree covering. In Proc. 34th International Colloquium on Automata, Languages
and Programming, pages 509–520. Springer-Verlag, 2007.

[13] Guy Jacobson. Succinct Static Data Structures. PhD thesis, Carnegie Mellon

University, Pittsburgh, PA, USA, 1989.

[14] J. Jansson, K. Sadakane, and W.-K. Sung. Ultra-succinct representation of ordered 
trees. In Proc. 18th Annual ACM-SIAM Symposium on Discrete Algorithms
(SODA), pages 575–584, 2007.

[15] John C. Kieffer, En-Hui Yang, and Wojciech Szpankowski. Structural complexity
of random binary trees. In Proc. IEEE International Symposium on Information
Theory (ISIT), pages 635–639, 2009.

[16] I. Munro. Tables. In Proc. 16th Conference on Foundations of Software Technology 
and Theoretical Computer Science (FSTTCS), LNCS 1180, pages 37–42,
1996.

[17] J. Ian Munro, Venkatesh Raman, and Adam J. Storm. Representing dynamic
binary trees succinctly. In Proc. 12th Annual ACM-SIAM Symposium on Discrete
Algorithms, pages 529–536. SIAM, 2001.

[18] Rajeev Raman, Venkatesh Raman, and Srinivasa Rao Satti. Succinct indexable
dictionaries with applications to encoding k-ary trees, preﬁx sums and multisets.
ACM Transactions on Algorithms, 3(4):Article 43, 2007.

[19] Rajeev Raman and Srinivasa Rao Satti. Succinct representations of ordinal trees.
In Proc. Conference on Space Efﬁcient Data Structures, Streams and Algorithms,
volume 8066 of LNCS, pages 319–332. Springer-Verlag, 2013.

[20] R. Sedgewick and P. Flajolet. An Introduction to the Analysis of Algorithms.

Addison-Wesley, 1995.

[21] Jean Vuillemin. A unifying look at data structures. Communications of the ACM,

23(4):229–239, 1980.

18

where the x stands for a node and T for any subtree. We indicate the numbers(cid:0)lv+rv

Figure 2: Our scheme to enumerate extended Cartesian trees T with increasing detail,

below nodes having left and right internal spines. Level 0 corresponds just to T (x). In
level 1 we have four possibilities, which lead to the equation T (x) = x + 2xT (x) +
2xT (x)2. For level 2, each of the T s in level 1 is expanded in all the four possible
ways, leading to 25 possibilities and to the equation T (x) = x + 2x2 + 4x2T (x) +
4x2T (x)2 + 2x3 + 10x3T (x) + 26x3T (x)2 + 36x3T (x)3 + 24x3T (x)4.

(cid:1)

rv

19

TTTTTTTT22TTTTTTTTTTTT2222233332TTTTTTTTTTTTTTTTTTTT222222333366663xxxT3xxxTxxxTT6xxxTT66xxxTTTxxxTT6xxxTTT12xxxTTT12xxxTT6TTTT2xxTTTxTx+++2level 1Tlevel 03xxxTx+xx+xxT+xxT+2xxTT+xx+xxT+xxT+TxTx2+2xxx++2xxxT+2xxxT+3xxxTT+2xxxTT+++++++++level 2TTxxxTTT24T+A Unrolling the Lower Bound Recurrence

The main issue to unroll further levels of the recurrence is that it grows very fast.
The largest tree at level (cid:96) has 2(cid:96) leaves labeled T . Each such leaf is expanded in 4
possible ways to obtain the trees of the next level. Let A((cid:96)) be the number of trees
If all the A((cid:96) − 1) trees had 2(cid:96)−1 leaves labeled T , then we
generated at level (cid:96).
would have A((cid:96)) = A((cid:96) − 1) · 42(cid:96)−1 ≤ 22(cid:96)+1. If we consider just one tree of level
(cid:96) with 2(cid:96) leaves labeled T , we have A((cid:96)) = 42(cid:96)−1
= 22(cid:96). Thus the number of trees
to generate is 22(cid:96) ≤ A((cid:96)) ≤ 22(cid:96)+1. For levels 3 and 4 we could just generate and
add up all the trees, but from level 5 onwards we switched to a dynamic programming
based counting that performs O((cid:96)4 · 16(cid:96)) operations, which completed level 5 in 40
seconds instead of 4 days of the basic method. It also completed level 6 in 20 minutes
and level 7 in 10 hours. We had to use unbounded integers,4 since 64-bit numbers
overﬂow already in level 5 and their width doubles every new level. Apart from this,
the degree of the generated polynomials doubles at every new level and the number of
terms grows by a factor of up to 4, putting more pressure on Maple. In level 3, with
polynomials of degree 8, Maple is already unable to algebraically solve the equations
related to G(x, T ), but they can still be solved numerically. Since level 5, Maple was
unable to solve the system of two equations, and we had to ﬁnd the singularity by
plotting the implicit function and inspecting the axis x ∈ [−1, 1].5 Since level 6, Maple
could not even plot the implicit function, and we had to manually ﬁnd the solution of
the two equations on G(x, T ). At this point even loading the equation into Maple is
troublesome; for example in level 7 we had to split the polynomial into 45 chunks to
avoid Maple to crash.

For level 8, our generation program would take nearly two weeks. It is likely that
Maple would also give problems with the large number of terms in the polynomial
(expected to be near 32000). For level 9 (expected to take more than one year), we
cannot compile as we reach an internal limit of the library to handle large integers: The
space usage of the dynamic programming tables grows as O((cid:96)2 · 4(cid:96)) and for level 9
it surpasses 230 large integers. Thus we are very close to reaching various limits of
practical applicability of this technique. A radically different model is necessary to
account for every possible internal spine length and thus obtain the exact lower bound.

4With the GNU Multiple Precision Arithmetic Library (GMP), at http://gmplib.org.
5Note that, in principle, there is a (remote) chance of us missing the dominant singularity by visual inspection,
 ﬁnding one farther from the origin instead. Even in this case, each singularity implies a corresponding
exponential term in the growth of the function, and thus we would ﬁnd a valid lower bound.

20

