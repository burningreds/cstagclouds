Colored Range Queries and Document Retrieval

Travis Gagie1,(cid:2), Gonzalo Navarro1,(cid:2), and Simon J. Puglisi2

1 Dept. of Computer Science, Univt. of Chile

{tgagie,gnavarro}@dcc.uchile.cl

2 School of Computer Science and Information Technology

Royal Melbourne Institute of Technology

simon.puglisi@rmit.edu.au

Abstract. Colored range queries are a well-studied topic in computational 
geometry and database research that, in the past decade, have
found exciting applications in information retrieval. In this paper we give
improved time and space bounds for three important one-dimensional
colored range queries — colored range listing, colored range top-k queries
and colored range counting — and, thus, new bounds for various document 
retrieval problems on general collections of sequences. Speciﬁcally,
we ﬁrst describe a framework including almost all recent results on colored 
range listing and document listing, which suggests new combinations 
of data structures for these problems. For example, we give the
fastest compressed data structures for colored range listing and document 
listing, and an eﬃcient data structure for document listing whose
size is bounded in terms of the high-order entropies of the library of documents.
 We then show how (approximate) colored top-k queries can be
reduced to (approximate) range-mode queries on subsequences, yielding
the ﬁrst eﬃcient data structure for this problem. Finally, we show how
a modiﬁed wavelet tree can support colored range counting in logarithmic 
time and space that is succinct whenever the number of colors is
superpolylogarithmic in the length of the sequence.

1 Introduction

A range query on a sequence S[1, n] of elements in [1, σ] takes as arguments two
indices i and j and returns information about S[i, j]. This information could
be, for example, the minimum or maximum value in S[i, j] [12], the element
with a speciﬁed rank in sorted order [15] (e.g., the median [7]), the mode [17], a
complete list of the distinct elements [31], the frequencies of the elements [35],
a list of the k most frequent elements for a given k [20], or the number of distinct 
elements [6]. In this paper, motivated by problems in document retrieval,
we consider the latter three kinds of problems, which are often referred to as
“colored” range queries: colored range listing (with or without color frequen-
cies), colored range top-k queries, and colored range counting. These have been

(cid:2) Partially funded by the Millennium Institute for Cell Dynamics and Biotechnology

(ICDB), Grant ICM P05-001-F, Mideplan, Chile.

E. Chavez and S. Lonardi (Eds.): SPIRE 2010, LNCS 6393, pp. 67–81, 2010.
c(cid:2) Springer-Verlag Berlin Heidelberg 2010

68

T. Gagie, G. Navarro, and S.-J. Puglisi

associated, respectively, to very relevant document retrieval queries on general
texts [31,35,37,20,15,12,9]: listing the documents where a pattern appears (possibly 
computing term frequencies), ﬁnding the most relevant documents to a query
(under a tf × idf scheme, for example), and computing document frequencies.
Such techniques have been shown to be competitive [9], even beating classical
inverted indexes on natural-language texts.

In Section 2 we describe a framework that includes almost all recent results on
colored range listing and the related problem of document listing. This framework 
suggests new combinations of data structures that yield interesting new
bounds, including the fastest compressed data structures for colored range listing 
and an eﬃcient data structure for document listing whose space occupancy
is bounded in terms of the higher-order entropies of the library of documents.
In Section 3 we describe what seems to be the ﬁrst data structure to support
eﬃcient, general approximate colored range top-k queries. By “approximate” we
mean that we are given an  > 0 with S and we guarantee that no element
we do not list occurs more than 1 +  times more often in the range than any
element we list. Finally, in Section 4 we describe a new solution to the colored 
range counting problem, reducing the space bound from O(n log n) bits
to n log σ + O(n log log n) bits without changing the O(log n) time bound. The
improvements for colored range queries we present in Sections 3 and 4 are not
competitive with the state of the art when mapped to the more speciﬁc problem
of document retrieval. However, as we discuss in Section 5, data structures for
general colored range queries can be applied to information retrieval scenarios
that specialized document-retrieval data structures cannot.

2 Listing

Related work. The problem of colored range listing (CRL) is to preprocess a
given sequence S[1, n] over [1, σ] such that later, given a range S[i..j], we can
quickly list all the distinct elements (“colors”) in that range. Almost all recent
data structures for CRL (and the related problem of document listing) are based
on a key idea by Muthukrishnan [31] (see [23] for older work). He deﬁned C[1, n]
to be the array in which C[j] is the largest value i < j such that S[i] = S[j], or 0
if there is no such i, so that S[(cid:4)] is the ﬁrst occurrence of a color in S[i..j] if and
only if i ≤ (cid:4) ≤ j and C[(cid:4)] < i. He showed how, if we store C in an O(n log n)-bit
data structure due to Gabow, Bentley and Tarjan [14] that supports O(1)-time
range-minimum queries (RMQs), we can quickly ﬁnd all the values in C[i..j] less
than i and, thus, list all the colors in S[i..j]. To do this, we ﬁnd the minimum
value C[(cid:4)] in C[i..j]; if it is less than i, then we output S[(cid:4)] and recurse on
S[i..(cid:4)− 1] and S[(cid:4) + 1..j]. Altogether, Muthukrishnan’s CRL data structure uses
O(n log n) bits and O(1) time per color reported.

Muthukrishnan gave his solution to the CRL problem as part of a solution
to the problem of document listing (DL), in which we are given a library of
documents and asked to preprocess them such that later, given a pattern, we can
quickly list all the distinct documents containing that pattern (see [29] for older
work). Let T [1, n] be the concatenation of the D documents. Muthukrishnan

Colored Range Queries and Document Retrieval

69

deﬁned the array E[1, n] such that E[i] is the document containing the starting
position of the lexicographically ith suﬃx in T . If we store a suﬃx tree [38,1] for
T then, given a pattern, we can quickly ﬁnd the lexicographic ranks i and j of
the ﬁrst and last suﬃxes starting with the pattern. This is equivalent to ﬁnding
the range A[i..j] in the suﬃx array [27] A for T that lists the starting positions
of all the suﬃxes of T that start with the pattern. Once we know i and j, we can
implement a DL query as a CRL query on E[i..j]. Altogether, Muthukrishnan’s
DL data structure uses O(n log n) bits and O(m + ndoc) time to list the ndoc
documents containing a pattern of length m.

Sadakane [35] gave a slower but smaller version of Muthukrishnan’s DL data
structure, in which he replaced Gabow, Bentley and Tarjan’s data structure by
a 4n + o(n) bit index that, given a range C[i..j], in O(1) time and without
consulting C returns the position of the minimum value in that range (but not
the value itself). He also replaced the suﬃx tree by a compressed suﬃx array
(CSA) for T and showed how the CSA and a bit vector V [1, n] can simulate
access to E: 1s in V mark the positions in T where the documents start; then,
for 1 ≤ (cid:4) ≤ n, E[(cid:4)] = rank1(V, CSA[(cid:4)]), where rank1(V, r) is the number of 1s in
V [1..r]. It takes D log(n/D)+O(D)+o(n) bits to store V such that a rank query
takes O(1) time [33]. Sadakane did not store C at all so, when listing the distinct
documents containing a pattern, he used a D-bit string to mark which documents
he had already listed. He used a recursion similar to Muthukrishnan’s, stopping
whenever it ﬁnds a document already reported.
Altogether, Sadakane’s DL data structure uses |CSA| + 4n + D log(n/D) +
O(D) + o(n) bits and O(search(m) + ndoc · lookup(n)) time, where search(m) is
the time to ﬁnd the range CSA[i..j] containing the starting positions of suﬃxes
beginning with the pattern and lookup(n) is the time to compute CSA[(cid:4)] for any
(cid:4). (There are a number of CSA implementations, allowing various space/time
tradeoﬀs [32].) He used |CSA| + 4n + o(n) additional bits for data structures to
compute the pattern’s frequency in each document, increasing the time bound
to O(search(m) + ndoc(lookup(n) + log log ndoc)) (assuming lookup(n) is also the
time to ﬁnd CSA−1[(cid:4)], where CSA−1 is the inverse permutation).
V¨alim¨aki and M¨akinen [37] gave an alternative slower-but-smaller version of
Muthukrishnan’s CRL data structure, in which they used a 2n + o(n) bit, O(1)
time RMQ succinct index due to Fischer and Heun [13] that requires access to C.
V¨alim¨aki and M¨akinen showed how access to C can be implemented by rank and
select queries on S; speciﬁcally, for 1 ≤ (cid:4) ≤ n, C[(cid:4)] = selectS[(cid:3)](S, rankS[(cid:3)](S, (cid:4))−
1), where selecta(S, r) is the position of the rth occurrence of a in S. V¨alim¨aki and
M¨akinen stored S in a multiary wavelet tree [10], which takes nH0(S)+o(n) log σ
bits and O(1 + log σ/ log log n) time; when σ is polylogarithmic in n, it takes
nH0(S) + o(n) bits and O(1) time. The 0-th order empirical entropy H0(S) =
(cid:2)
occ(a,S), where occ(a, S) is the number of times element a occurs

in S, is the Shannon entropy of the distribution of elements in S.
Altogether, their CRL data structure takes nH0(S) + 2n + o(n) log σ bits and
O(1 + log σ/ log log n) time per reported color. Combining this data structure
with a CSA yields a DL data structure that takes |CSA|+n log D+2n+o(n) log D

occ(a,S)

a

n

log

n

70

T. Gagie, G. Navarro, and S.-J. Puglisi

bits and O(search(m) + ndoc(1 + log D/ log log n)) time. They also showed how
to compute the pattern’s frequency in a document d using two rank queries on E,
rankd(E, j) − rankd(E, i − 1). Since multiary wavelet trees support rank queries
in the same time as accesses, it follows that reporting the pattern’s frequency
in all the documents does not aﬀect their time and space bounds. Finally, they
noted that, using one select query per occurrence, they can list the positions of
the pattern’s occurrences in a speciﬁed document.
Gagie, Puglisi and Turpin [15] showed that a binary wavelet tree [18] can be
used to compute range quantile queries on S in O(log σ) time, and that these
queries can be used to enumerate the distinct elements in S[i..j], eliminating
the need for RMQs. A binary wavelet tree for S takes nH0(S) + o(n) log σ bits
and supports access, rank and select in O(log σ) time; therefore, by itself it is a
CRL data structure that takes O(log σ) time per reported element. Combining
a wavelet tree for E with a CSA for T , we obtain a DL data structure that takes
|CSA| + n log D + o(n) log D bits and O(search(m) + ndoc log D) time.

Hon, Shah and Vitter [20] described a solution to DL similar to Sadakane’s
but removing the Θ(n)-bit space term. They pack log n consecutive cells of
C into a block and build the RMQ data structure on the block minima (so it
takes O(n/ log n) bits of space), and tries to report (avoiding repetitions) all
the documents in the block that holds the minimum. Their whole data structure 
takes |CSA| + D log(n/D) + O(D) + o(n) bits and answers queries in time
O(search(m) + ndoc log n · lookup(n)), for any constant  > 0.

They can also return the number of times the pattern occurs in any document 
by using, like Sadakane, one CSAd local to each document d. These add
up to other |CSA| extra bits. To ﬁnd out how many times document d = E[(cid:4)],
i ≤ (cid:4) ≤ j, appears in E[i..j], it maps (cid:4) to position p = CSA[(cid:4)] − select1(V, d) + 1
within document d, and then to (cid:4)(cid:3) = CSA−1
d [p]. This is the ﬁrst lexicographic
occurrence of the pattern in CSAd. The last occurrence is found by an exponential 
search and then binary search on CSAd[(cid:4)(cid:3)..], for the largest c such that
CSA−1[CSAd[(cid:4)(cid:3) + c] + select1(V, d) − 1)] ≤ j. Then the answer, c + 1, is obtained
in time O(lookup(n) log c) = O(lookup(n) log n).
New tradeoﬀs. All the previous solutions have essentially the same ingredients:
for CRL, access to S, distinct color enumeration on S (implemented via RMQs
on C or range quantile queries on S) and, to count the number of times each
color occurs, rank on S; for DL, a suﬃx tree or CSA for T , access to E, distinct
document enumeration on E and, to report the pattern’s frequency in each
document, rank on E. Solutions for CRL can be used for DL with the addition
of a CSA for T , setting S = E and σ = D. Recall that Sadakane’s [35] and Hon,
Shah and Vitter’s [20] solutions for DL implement access to E using a CSA and
bit vector V on T , so they cannot be used for general CRL.

Our main contribution in this section is the observation that, using new data
structures for access, color enumeration and rank, we can obtain interesting new
bounds for both CRL and DL. This is formalized in the next theorem.

Theorem 1. Suppose we are given a sequence S[1, n] over [1, σ] and we store any
data structure supporting access on S in time tacc and any structure supporting

Colored Range Queries and Document Retrieval

71

distinct enumeration in a range of S in time tenum per element (and any structure
supporting rank on S in time trank if computing frequencies is desired). Then later,
given i and j, we can list the distinct elements in S[i..j] in time O(tacc + tenum) per
reported element, plus O(trank) to list its frequency in S[i..j].
Corollary 1. Given a concatenation T [1, n] of D documents, we can store either

– the CSA for T and data structures supporting access, enumeration and rank
– the CSA for T , a bit vector occupying D log(n/D) + O(D) + o(n) bits, and

on the corresponding array E[1, n] in times tacc, tenum and trank, or

data structures supporting enumeration and rank on E as above,

such that, given a pattern of length m, we can list the distinct documents containing 
that pattern in time O(search(m)) plus O(tacc + tenum + trank) per reported
document, where tacc = lookup(n) in the second case and trank is required only in
order to list the frequencies of the documents.

A selection of these data structures is shown in Table 1. If we choose a set of rows
covering support for access and enumeration (and rank) then we can answer CRL
queries (and return the frequency of each color). The space bound is the sum of
the space bounds and the time bound per reported color is O(tacc + tenum + trank),
the latter term for computing frequencies. For example,

2+9: is V¨alim¨aki and M¨akinen’s scheme [37].
1: is the scheme by Gagie, Puglisi, and Turpin [15].
3+9+10: combining Ferragina and Venturini’s [11] data structure with Fis-
cher’s [12] succinct index for RMQ and Grossi, Orlandi and Raman’s [19]
succinct index for rank gives a solution for CRL that takes nHk(S) + 2n +
o(n) log σ + n o(log σ) bits and O(1) time per reported color, matching the
time of Muthukrishnan’s O(n log n)-bit space solution [31]. The k-th order
empirical entropy Hk(S) measures the compressibility of S when we use
contexts of length k; see [28] for details. The frequency of any color can be
obtained in time O(log log σ).

6+9: is similar to the above but the n o(log σ) space term is avoided, as the
structure by Barbay, Gagie, Navarro and Nekrich [4] computes rank as well.
This becomes the least-space reported solution to CRL, listing in O(1) time.
(4 or 5)+9: combining Barbay et al.’s [4] access and rank data structure
with Fischer’s [12] succinct index for RMQ gives a solution for CRL that
takes nH0(S) + 2n + o(n)(H0(S) + 1) bits and O(log log σ) bits per reported 
color and its frequency (variant 4), which is the fastest compressed
solution when we want all the frequencies; or O(1) per reported color and
O(log log σ log log log σ) per reported frequency (variant 5), which trades frequency 
reporting time for constant-time listing.

[35]+9: replacing Sadakane’s [35] RMQ data structure with the one by Fischer 
[12] improves Sadakane’s space bound by 2n bits.

[20]+10: replacing Hon, Shah and Vitter’s [20] CSAd structures by that of Grossi
et al. [19] speeds up counting document frequencies (here tacc = lookup(n)).

72

T. Gagie, G. Navarro, and S.-J. Puglisi

Table 1. Space and time bounds for some data structures supporting operations on
S[1, n] over [1, σ]. The O(σ log n) extra bits of wavelet trees [18,10] can be avoided [26]
so we have not included it. The space bound in rows 3 and 6 holds for k = o(logσ n).
In rows 7 and 8, g is the size (in bits) of a given context-free grammar generating S
and only S and α is the inverse Ackermann function. The succinct index for RMQ in
row 9 does not need access to the underlying data (i.e., C), but the succinct index for
rank in row 10 does (i.e., S), hence the time of the latter depends on tacc. Due to space
constraints, here we write log[2] and log[3] for log log and log log log.

row

source

space (in bits)

1

2

3

4

5

6

7

8

9

[18,15]

nH0(S) + o(n) log σ

[10, Cor. 3.3]

nH0(S) + o(n) log σ

[11]

nHk(S) + o(n) log σ

[4, Thm. 1]

nH0(S) + o(n)(H0(S) + 1)

[4, Thm. 1]

nH0(S) + o(n)(H0(S) + 1)

[4, Thm. 2]

nHk(S) + o(n) log σ

[5, Thm. 1]

[5, Thm. 1]

O(g α(g))

O(g)

[12, Thm. 1]

2n + o(n)

10 [19, Thm. 5(a)]

n o(log σ)

tacc

O(log σ)

O(cid:3)

(cid:4)

log

1 + log σ
[2] n
O(1)

(cid:4)

O(cid:3)

log[2] σ
O(1)
O(1)
O(log n)

O(cid:3)

(cid:4)

log n log[2] n

tenum
O(log σ)

trank

O(log σ)

O(cid:3)

1 + log σ
[2] n

log

(cid:4)

O(cid:3)

log[2] σ

(cid:4)

log[2] σ log[3] σ

(cid:4)

(cid:4)

(log[2] σ)2 log[3] σ

O(cid:3)
O(cid:3)

O(1)

O(cid:3)

tacc log[2] σ

(cid:4)

The |CSA| space is exchanged by n o(log d) bits, which can be less or more.
We can then also discard the D-bit string marking documents used by both
solutions [35,20] and replace it with rank queries on E.

(7 or 8)+9+10: combines Bille, Landau and Weimann’s [5] grammar-based
data structure for access, Fischer’s [12] succinct index for RMQ, and Grossi
et al.’s [19] succinct index for rank. Gonz´alez and Navarro [16] showed how
to build a grammar generating an array that, together with some other small
data structures, gives access to the suﬃx array (SA) A. Building Bille, Landau 
and Weimann’s data structure for this grammar, we obtain a O(log n)-
time data structure for DL whose size is bounded in terms of the high-order
entropies of the library of documents. This is described next.

Theorem 2. Given a concatenation T [1, n] of D documents, we can store T in

|CSA| + 2n + o(n) + n o(log D)+
(cid:6)
(cid:7)
n min(Hk(T ), 1) + D

O

(cid:5)

(cid:5)

log

1

min(Hk(T ), 1) + D/n

(cid:8)

(cid:8)

α(n) log n

bits, for any k ≤ α logτ n, constant 0 < α < 1 and τ the size of the alphabet of
T . Then given a pattern of length m, we can list the distinct documents containColored 
Range Queries and Document Retrieval

73

ing that pattern in time O(search(m)) plus O(log n) to list each document, plus
O(log n log log D) to give its frequency.
Proof. Gonz´alez and Navarro’s algorithm takes advantage of the so-called runs
of the SA, that is, areas A[i..i + (cid:4)] such that there is some other area A[j..j + (cid:4)]
where A[j + k] = A[i + k] + 1 for all 0 ≤ k ≤ (cid:4). Let R be the number of runs with
which the SA can be covered; it is known that R ≤ min(n, nHk(T ) + σk) for any
k [25]. Gonz´alez and Navarro represent the SA diﬀerentially so that these areas
become true repetitions, and use a grammar-based compression algorithm that
represents A using at most R log(n/R) rules. We note that, in E, those SA runs
become identical areas E[i..i + (cid:4)] = E[j..j + (cid:4)] except for at most D cells where
the document number can change when we advance one text position. It follows
that, by applying the same compression algorithm [16] to E we obtain at most
(cid:3)(cid:4)
(R + D) log(n/(R + D)) rules and hence the space given in the theorem.

As a ﬁnal note applying only to document collections, Sadakane’s CSA [34]
essentially represents a function Ψ such that A[Ψ(i)] = A[i] + 1, which is stored
in compressed form and any value computed in constant time. Thus one advances
virtually in the text by successively applying Ψ. Now assume we sample E with
a step r such that, for any i, E[Ψ j(i)] is sampled for some 0 ≤ j < r. Then one
computes any E[i] value in time O(r) by following Ψ until hitting a sampled
entry, whose value will be the same as E[i] if we also sample every document
end in the text collection. The space is O((n/r) log r) + (n/r) log D for a bitmap
marking the sampled cells and an array with the sampled values, respectively.
For example, using r = log D yields access to E (though not rank nor select on
it) in the same time of a binary wavelet tree, within bit space n+o(n). Depending
on the relation between n and D, this can be an interesting alternative to using
lookup and marking the document beginnings [35].

3 Top-k Queries

(cid:7)

n log2 n

Improving the current-best solution for documents. Recently, Hon, Shah
and Wu [21] described a data structure that stores a library T of D documents of
total length n in O(cid:6)
bits such that later, given a pattern of length m and
an integer k ≥ 1, we can ﬁnd the k documents that contain that pattern most
frequently, in O(m + log n log log n + k) time. We call this the document top-k
problem (DTK). Hon, Shah and Vitter [20] gave solutions for DTK that store
T in O(n log n) bits and answer queries in O(m + k log k) time, or in 2|CSA| +
o(n) + D log(n/D) + O(D) bits and O(cid:6)
time.
The last solution consists of a tree τk built for each k power of 2. For τk they
divide E into blocks of size z = k log2+ n, and τk consists of the suﬃx tree nodes
that are lowest common ancestors (lca) of end points of blocks, and transitively
all the lcas of pairs of those nodes. At each node, τk stores the k most frequent
documents within the whole blocks it contains, and their frequencies. Thus each
τk requires O((n/z)k log n) = O(cid:6)
bits, and all the trees together add
up to O(n/ log n) bits. At query time, to ﬁnd the top-k documents in E[i..j],

search(m) + k log3+ n · lookup(n)

n/ log1+ n

(cid:7)

(cid:7)

74

T. Gagie, G. Navarro, and S.-J. Puglisi

they increase k to the next power of 2 and ﬁnd the highest node of τk whose
range [i(cid:3)..j(cid:3)] is contained in [i..j]. They show that i(cid:3) − i ≤ z and j − j(cid:3) ≤ z by
the lca properties. Then the query is answered by considering the k candidates
given by τk and the O(z) further candidates found at positions of E[i..i(cid:3)− 1] and
E[j(cid:3) + 1..j], for each of which they compute the frequency. The total time, considering 
priority queue operations, is O(search(m) + z(trank + log k) + k log k) =
O(cid:6)
(cid:7)
search(m) + k log3+ n · lookup(n)
. This time bound can be improved to
O(cid:6)
(cid:7)
search(m) + k log D log(D/k) log1+ n · lookup(n)
by noticing that (a) one
needs only O(log D) powers of 2 for k since k ≤ D; (b) one can store the top-k
elements in the τk trees and not their frequency. The k frequencies can be computed 
at query time without changing the time complexity since k = o(z). Thus
the k documents out of D can be stored in increasing order and as gammaencoded 
diﬀerences, taking O(k log(D/k)) bits. Therefore we can use smaller
blocks of size z = k log D log(D/k) log n, which are processed faster, and still
have O(n/ log n) = o(n) space for the structure.
In addition, as shown in Section 2, by replacing the |CSA| bits of their solution 
for computing frequencies, by Grossi et al.’s [19] succinct index for rank, we
achieve a new space bound of |CSA| + o(n) + D log(n/D) + O(D) + n o(log D)
bits, which can be better or worse than before, but the time is reduced
to O(search(m) + k log D log(D/k) log n · lookup(n)), for any  (log-logarithmic
terms disappear by adjusting ).

An approximate solution to the general problem. We now give a solution
to the approximate colored range top-k problem (CRTK), which asks us to
preprocess a given sequence S such that later, given a range S[i..j] and an
integer k ≥ 1, we can return an approximate list of the k elements (“colors”) that
occur most frequently in that range. We do not know of any previous eﬃcient
solutions to this problem, although ﬁnding the k most frequent or important
items in various data sets and models is a well studied problem and there has
been work on interesting special cases (see, e.g., [22,24]).
Greve, Jørgensen, Larsen and Truelsen [17] recently gave a data structure
that, for any  > 0, stores S in O((n/) log n) bits such that we can ﬁnd an
element such that no element is more than 1 +  times more frequent in S[i, j],
in O(log(1/)) time. Thus, their data structure solves the approximate CRTK
problem for k = 1, which is called the approximate range-mode problem. We can
assume their data structure also returns the frequency of the approximate mode
in S[i..j], since adding a rank data structure for S allows us to compute this and
does not change their space bound. We show how to use their data structure as
a building block to store S in O((n/)(H0(S) + 1) log n) bits such that, given
an integer k, we can approximately list the k most common elements and their
frequencies in O(k log σ log(1/)) time.

We ﬁrst build a binary wavelet tree for S [18]. This is a balanced tree where each
node represents a range of [1, σ]: the root represents the full range, the leaves the
individual symbols, and the children of a node represent the left and right halves
of the node’s interval. For each node v, let Sv be the subsequence of S consisting
of characters labelling the leaves in v’s subtree. The original wavelet tree does not

Colored Range Queries and Document Retrieval

75

store Sv, but just a bitmap Bv of length |Sv| telling whether each Sv[i] went to the
left or right child. Rank and select over those bitmaps allow accessing any S[i], as
well as computing ranka(S, i) and selecta(S, i), in time O(log σ), and the overall
space is n log σ(1 + o(1)). It can also track any range S[i..j] down to any node [26].
Here we do store each subsequence Sv in an instance of Greve et al.’s approximate 
range-mode data structure. For now, assume [i, j] = [1, n] and that Greve
et al.’s data structure returns the exact mode, rather than an approximation.
Notice that, if a1, . . . , ak(cid:2) are the k(cid:3) most frequent elements and v is an ancestor
of the leaf labelled ak(cid:2) but not of those labelled a1, . . . , ak(cid:2)−1, then ak(cid:2) is the
mode in Sv. Let V be the set of ancestors of a1, . . . , ak(cid:2)−1 and let V (cid:3) be the set
of nodes whose siblings are in V but who are not in V themselves; V (cid:3) contains
the root of the tree if V is empty. We can ﬁnd ak(cid:2) by ﬁnding the mode of Sv for
each v ∈ V (cid:3), ﬁnding their frequencies in S, and taking the most frequent.
We keep the modes for each v ∈ V (cid:3) in a priority queue, ordered by their
frequencies and with the corresponding nodes of the wavelet tree as auxiliary
data. Notice ak(cid:2) is the head of the queue, so we can ﬁnd and output it in O(1)
time; let v be the corresponding node, i.e., the node in V (cid:3) such that the mode
of Sv is ak(cid:2). To update the queue, we delete ak(cid:2), perform range-mode queries on
the siblings of nodes on the path from v to the leaf labelled ak(cid:2), and add the
modes to the queue. There are always O(k log σ) nodes in the queue (the tree is of
height O(log σ)) so, if we use a priority queue allowing O(log(k log σ)) = O(log σ)
time deletion and O(1) time insertion [8], then we can ﬁnd the k most frequent
elements in S in O(k log σ log(1/)) time. We can deal with general i and j by
using the wavelet tree to compute the appropriate range in each subsequence
[26]. As for the approximation, it is clear that, whenever we output an element,
none of the elements not output yet can be more than 1+ times more frequent.
If we use a Huﬀman-shaped wavelet tree, then calculation shows that our space
usage is O((n/)(H0(S) + 1) log n) bits. However, since a Huﬀman tree can be
very deep (height n − 1 for a very skewed distribution), this would compromise
our time bound. Therefore, we use a an O(log σ)-restricted Huﬀman tree [30],
which yields both the space and time bounds we want.
Theorem 3. Given a sequence S[1, n] over an alphabet of size σ and a constant
 > 0, we can store S in O((n/)(H0(S) + 1) log n) bits such that, given i, j and
k, we can list k distinct elements such that no element is more than 1 +  times
more frequent in S[i..j] than any of the ones we list, in O(k log σ log(1/)) time.
This (1 + )-approximation makes sense in information retrieval scenarios, where
top-k queries are understood to be just approximations to the ideal answer.
Corollary 2. Given a set of D documents of total length n and a constant  > 0,
we can store them in O((n/) log n log D) bits such that, given a pattern of length
m and k, we can list k distinct documents such that no document contains that
pattern more than 1 +  times as often as any of the ones we list, in a total of
O(m + k log D log(1/)) time.
Although Corollary 2 is weaker than Hon, Shah and Vitter’s uncompressed result,
 our approach applies to the general colored range query problem, and may

76

T. Gagie, G. Navarro, and S.-J. Puglisi

be faster than what the upper bound suggests. For example, if the documents are
webpages sorted lexicographically by URL, then it is more likely that interesting
patterns will occur often in clusters of documents than widely spread out [36,39].
In this case, leaves in a balanced wavelet tree for E that are labelled with the k
distinct documents that contain the pattern most often, are likely to share many
ancestors; if so, our data structure can speed up to O(m + k log k log(1/)).
The K-mining problem. Muthukrishnan [31] deﬁned (document) K-mining
(DKM) as the problem of ﬁnding all the documents in the library that contain
a given pattern at least K times. He gave an O(cid:6)
(cid:7)
-bit data structure
that, given K and a pattern of length m, answers queries in O(m) time plus
O(1) time per reported document. Hon, Shah and Wu [21] noted that we can
use binary search with a DTK data structure to solve DKM, with an O(log n)
(cid:7)
slowdown for the queries. They then showed how we can use an O(cid:6)
-bit
data structure to ﬁnd the largest k such that k documents contain the pattern
K times, in O(search(m) + log n log log n) time. Hon, Shah and Vitter [20] gave
an O(n log n)-bit data structure that answers K-mine queries in time O(m)
plus O(1) per reported document. They also showed how to improve the space
bound to 2|CSA| + o(n) + D log(n/D) bits at the cost of increasing the time
(cid:7)
O(cid:6)
search(m) + k log3+ n · lookup(n)
, which we can improve similarly as before.
Neither of these solutions applies to general colored range queries, however.

n log2 n

n log2 n

Since our CRTK data structure outputs elements in (approximately) nonincreasing 
order by frequency in the range, it also solves (approximately) the
natural generalization of DKM: i.e., the colored range K-mine (CRKM) problem,
which asks us to report all the elements that occur at least K times in S[i..j]. If
we query our data structure until the next element it would report occurs fewer
than (1 + )K times, then we use O(log σ log(1/)) time per reported element,
but we may miss some elements that occur between K and (1 + )K times.
Alternatively, if we query our data structure until the next element it would
report occurs fewer than K/(1 + ) times, then we ﬁnd all the elements that
occur at least K times, but we can bound our time only in terms of the number
of elements that occur at least K/(1 + ) times.

4 Counting

Given a wavelet tree for the array C we described in Section 2, and positions i
and j, it is not diﬃcult to count the number of values less than i in C[i..j] [26],
which is the number of distinct elements in S[i..j] [31]. The wavelet tree for C
takes O(n log n) bits and does this counting in time proportional to its height,
O(log n). This already matches the best known solution, due to Bozanis, Kitsios,
Makris and Tsakalidis [6]. In the rest of this section we show how to reduce the
space bound to n log σ + O(n log log n) bits.
Theorem 4. We can represent a sequence S[1, n] over alphabet [1, σ] in n log σ+
O(n log log n) bits of space so as to count the number of distinct elements in any
interval S[i..j] in time O(log n).

Colored Range Queries and Document Retrieval

77

Proof. Our structure represents C[1, n] using a wavelet tree. We have already
explained how to attain the given time bound. The remaining problem is that
the wavelet tree for C requires n log n(1 + o(1)) bits. We reduce the space to
n log σ + O(n log log n) as follows. Note that each symbol c ∈ [1, σ] that appears
at positions c1 < c2 < . . . < cnc, S[c1] = S[c2] = . . . = S[cnc] = c, induces a
chain in C of the form C[c1] = 0, C[c2] = c1, C[c3] = c2, . . ., C[cnc] = cnc−1. Now
consider the middle point n/2 of C. For any c, let us call mc the last value such
that cmc < n/2. Then for any c and any k ≤ mc it holds C[ck] < n/2, and for any
k > mc it holds C[ck] ≥ n/2. Thus C[cmc+1] = cmc ≥ n/2 and C[cmc] < n/2,
and i = mc is the only value satisfying this for c. Thus all the sequence values are
C[i] < n/2 for i < n/2. For i ≥ n/2 there are at most σ positions i = mc ≥ n/2
such that C[mc] < n/2, and all the rest are C[i] ≥ n/2. Thus there are at most
σ positions in C where C[i] < n/2 and C[i + 1] ≥ n/2, and at most σ positions
where C[i] ≥ n/2 and C[i + 1] < n/2. Since the root bitmap Bv satisﬁes Bv[i] = 0
iﬀ C[i] < n/2, there are at most σ transitions from 0 to 1 in Bv, and at most σ
transitions from 1 to 0. Both children of v may contain the σ subsequences and
thus each may contain up to σ transitions again. Thus, there are at most 2dσ 0/1
and 1/0 transitions among all the bitmaps at depth d of the wavelet tree.
For d ≥ log(n/σ) this upper bound is useless, so we may assume that bitmaps
at depths log(n/σ) to log n − 1 are incompressible. These add up to n(log n −
log(n/σ)) = n log σ bits, plus o(n log σ) to provide rank and select capabilities
to those bitmaps. For smaller d, we introduce a compression scheme. Consider
the concatenation Bd of all the bitmaps at depth d. Then Bd contains at most
2dσ runs of 0s and 2dσ runs of 1s. We represent Bd using two sparse bitmaps.
A bitmap Rd[1, n] will mark with a 1 the beginning of each run of 0s or 1s. Let
o1, o2, . . . the lengths of the runs of 1s. A second bitmap Od[1, rank1(Bd, n)] will
have a 1 at positions 1, 1+o1, 1+o1+o2, . . .. Then rank1(Bd, i) can be computed
as follows. First we compute x = rank1(Rd, i). Because C[i] < i, the ﬁrst run of
Bd is a 0-run, thus if x is odd then i is within a 0-run and otherwise within a
1-run. If x is odd, then we must count the 1s in the ﬁrst (x − 1)/2 1-runs of Bd,
that is, rank1(Bd, i) = select1(Od, (x + 1)/2) − 1. If, instead, x is even, then we
must count the 1s in the ﬁrst x/2 − 1 1-runs and add the 1s in the current run.
This is rank1(B, di) = select1(Od, x/2) + i − select1(Rd, x).

(cid:7)

2dσ

2dσ + O(cid:6)

2dσ

(cid:7)

2dσ log n

2dσ + O(cid:6)

+ o(n) = O(n) + o(n log(n/σ)) .

We represent Rd with Raman et al.’s technique [33]. If Rd has m 1s, then
m + O(m) + o(n) bits. At level d we have
the representation takes m log n
m ≤ 2dσ, thus Rd requires at most 2dσ log n
+ o(n) bits (that
o(n) is O(n log log n/ log n)). Added over all the compressible levels we have
(cid:2)
log(n/σ)−1
Analogously, the Od bitmap takes O(n) + o(n log(n/σ)) bits. Added to the
d=0
incompressible levels, we have n log σ + o(n log n) bits of space, or more precisely,
n log σ + O(n log log n). The preprocessing time is the same as for a classical
(cid:3)(cid:4)
wavelet tree over alphabet [0, n − 1].
On the other hand, the array C can also provide access to S as follows. Sample
the tth occurrence of each color c, say at S[i] = c, in a bitmap B[1, n], that is
B[i] = 1, and store the samples at W [rank1(B, i)] = c. Then, we can ﬁnd out

78

T. Gagie, G. Navarro, and S.-J. Puglisi

any S[j] without storing S by repeatedly asking whether B[i] = 1, B[C[i]] = 1,
B[C[C[i]]] = 1, and so on until ﬁnding a sampled value, in time O(t log n). The
extra space is n + o(n) + O((n/t) log σ), so we can set t = O(log n) for any
constant  > 0 to make it n + o(n) log σ. Therefore, our representation replaces
(cid:7)
S, as it can compute any S[i] in time O(cid:6)
. Its space occupancy, n log σ +
o(n) log σ + O(n log log n), makes the representation succinct (i.e., |S|(1 + o(1))
bits) whenever σ is more than polylogarithmic in n.

log1+ n

Theorem 4, applied over sequence S = E, lets us compute document frequencies 
for arbitrary patterns. Find the suﬃx array interval CSA[i..j] corresponding
to the pattern, and then count the diﬀerent values in E[i..j]. For this particular
case, however, there is a better solution [35] using 2n + o(n) bits and constant
time, yet it does not generalize to colored range counting. On the other hand,
(cid:7)
since our representation provides access to E in time tacc = O(cid:6)
log1+ n
, it can
be regarded within the framework of Section 2.

5 Further Applications to Information Retrieval

We have presented new and eﬃcient solutions for three natural colored range
queries: colored range listing, colored range top-k queries, and colored range
counting. Our solutions for colored range listing lead to the fastest compressed
data structures for that problem and for document listing; our (approximate)
solution for colored range top-k queries is, as far as we know, the ﬁrst eﬃcient
data structure for that problem; and our solution for colored range counting
reduces the space bound from O(n log n) bits to n log σ + O(n log log n) bits
while maintaining O(log n) query time. Although our solutions for colored range
top-k queries and colored range counting do not give improved bounds for the
corresponding document retrieval problems, our more general data structures
may ﬁnd applications to other information retrieval scenarios beyond ranges
induced by searching for exact patterns in suﬃx trees or arrays.

A simple example of natural queries not ﬁtting in the restricted model are
lexicographic range queries. Imagine we look for patterns lexicographically in
the range ["1969", "2010"] in documents; the result is a suﬃx array range that
does not correspond to any suﬃx tree node. In this case, existing techniques for
document retrieval based on suﬃx tree properties (such as for computing top-k
queries [20] and for computing document frequencies [35]) will not work. The
general techniques we have introduced in this article do.

Yet another scenario that is not captured by the suﬃx tree model is inverted
indices for natural language text (as opposed to the general texts addressed
in this paper) [3]. Consider that we store the list of documents where each
vocabulary word appears, consecutively according to the order of the words
in the vocabulary. If queries are simple words, then all the document retrieval
problems we have considered are easily solved by storing the documents of each
list ordered by decreasing term frequency. Yet, imagine we wish to provide also
the same functionality on stemmed searching, upon user request at query time.
One solution is to group together the vocabulary words sharing the same stem

Colored Range Queries and Document Retrieval

79

so that, while individual word queries can be handled as usual, stemmed queries
are handled by considering the concatenation of the lists of the words sharing
the same stem. Then we can regard the concatenation of all inverted lists as
the array E and use the general techniques developed in this paper to answer
various document queries on stems: Document listing and counting algorithms
apply verbatim, while those involving frequencies pose further challenges as each
entry in the inverted lists is weighted by the term frequency of the word in the
document. Other query operations, from case folding to thesauri expansion, can
also be reduced to a proper grouping of lists.

Finally, there are information retrieval scenarios completely diﬀerent from
the text search framework. For example, colored range queries seem a natural
tool for query mining [2], where logs of queries posed to search engines are
recorded over periods of time, and then analyzed to discover trends in user
behavior. By considering that each diﬀerent query is a color, we can ﬁnd the
most popular queries or the number of distinct queries within any given time
period, among many other potential queries of interest, which could in turn
become new challenging colored range queries.

References

1. Apostolico, A.: The myriad virtues of subword trees. In: Combinatorial Algorithms

on Words. NATO ISI Series, pp. 85–96. Springer, Heidelberg (1985)

2. Baeza-Yates, R.: Applications of web query mining. In: Losada, D.E., Fern´andezLuna,
 J.M. (eds.) ECIR 2005. LNCS, vol. 3408, pp. 7–22. Springer, Heidelberg
(2005)

3. Baeza-Yates, R., Ribeiro, B.: Modern Information Retrieval. AW (1999)
4. Barbay, J., Gagie, T., Navarro, G., Nekrich, Y.: Alphabet partitioning for compressed 
rank/select with applications. Technical Report 0911.4981, arXiv (2010)

5. Bille, P., Landau, G.M., Weimann, O.: Random access to grammar compressed

strings. Technical Report 1001.1565, arXiv (2010)

6. Bozanis, P., Kitsios, N., Makris, C., Tsakalidis, A.K.: New upper bounds for generalized 
intersection searching problems. In: F¨ul¨op, Z., Gecseg, F. (eds.) ICALP
1995. LNCS, vol. 944, pp. 464–474. Springer, Heidelberg (1995)

7. Brodal, G.S., Gfeller, B., Jørgensen, A.G., Sanders, P.: Towards optimal range

medians. Theoretical Computer Science (to appear)

8. Carlsson, S., Munro, J.I., Poblete, P.V.: An implicit binomial queue with constant
insertion time. In: Karlsson, R., Lingas, A. (eds.) SWAT 1988. LNCS, vol. 318, pp.
1–13. Springer, Heidelberg (1988)

9. Culpepper, J.S., Navarro, G., Puglisi, S.J., Turpin, A.: Top-k ranked document

search in general text databases. In: Proc. ESA (2010) (to appear)

10. Ferragina, P., Manzini, G., M¨akinen, V., Navarro, G.: Compressed representations
of sequences and full-text indexes. ACM Transactions on Algorithms (TALG), 3(2),
article 20 (2007)

11. Ferragina, P., Venturini, R.: A simple storage scheme for strings achieving entropy

bounds. Theoretical Computer Science 371(1), 115–121 (2007)

12. Fischer, J.: Optimal succinctness for range minimum queries. In: Proc. LATIN, pp.

158–169 (2010)

80

T. Gagie, G. Navarro, and S.-J. Puglisi

13. Fischer, J., Heun, V.: A new succinct representation of RMQ-information and
improvements in the enhanced suﬃx array. In: Chen, B., Paterson, M., Zhang, G.
(eds.) ESCAPE 2007. LNCS, vol. 4614, pp. 459–470. Springer, Heidelberg (2007)
14. Gabow, H.N., Bentely, J.L., Tarjan, R.E.: Scaling and related techniques for geometry 
problems. In: Proc. STOC, pp. 135–143 (1984)

15. Gagie, T., Puglisi, S.J., Turpin, A.: Range quantile queries: Another virtue of
wavelet trees. In: Karlgren, J., Tarhio, J., Hyyr¨o, H. (eds.) SPIRE 2009. LNCS,
vol. 5721, pp. 1–6. Springer, Heidelberg (2009)

16. Gonz´alez, R., Navarro, G.: Compressed text indexes with fast locate. In: Ma, B.,
Zhang, K. (eds.) CPM 2007. LNCS, vol. 4580, pp. 216–227. Springer, Heidelberg
(2007)

17. Greve, M., Jørgensen, A.G., Larsen, K.D., Truelsen, J.: Cell probe lower bounds
and approximations for range mode. In: Gavoille, C. (ed.) ICALP 2010, Part I.
LNCS, vol. 6198, pp. 605–616. Springer, Heidelberg (2010)

18. Grossi, R., Gupta, A., Vitter, J.S.: High-order entropy-compressed text indexes.

In: Proc. SODA, pp. 636–645 (2003)

19. Grossi, R., Orlandi, A., Raman, R.: Optimal trade-oﬀs for succinct string indexes.

In: Proc. ICALP, pp. 678–689 (2010)

20. Hon, W.-K., Shah, R., Vitter, J.: Space-eﬃcient framework for top-k string retrieval

problems. In: Proc. FOCS, pp. 713–722 (2009)

21. Hon, W.-K., Shah, R., Wu, S.-B.: Eﬃcient index for retrieving top-k most frequent 
documents. In: Karlgren, J., Tarhio, J., Hyyr¨o, H. (eds.) SPIRE 2009. LNCS,
vol. 5721, pp. 182–193. Springer, Heidelberg (2009)

22. Ilyas, I.F., Beskales, G., Soliman, M.A.: A survey of top-K query processing techniques 
in relational database systems. ACM Computing Surveys 40(4) (2008)

23. Janardan, R., Lopez, M.A.: Generalized intersection searching problems. International 
Journal of Computational Geometry and Applications 3(1), 39–69 (1993)

24. Karpinski, M., Nekrich, Y.: Top-K color queries for document retrieval. Technical

Report 1007.1361, arXiv (2010)

25. M¨akinen, V., Navarro, G.: Succinct suﬃx arrays based on run-length encoding.

Nordic Journal of Computing 12(1), 40–66 (2005)

26. M¨akinen, V., Navarro, G.: Rank and select revisited and extended. Theoretical

Computer Science 387(3), 332–347 (2007)

27. Manber, U., Myers, G.: Suﬃx arrays: a new method for on-line string searches.

SIAM Journal on Computing 22(5), 935–948 (1993)

28. Manzini, G.: An analysis of the Burrows-Wheeler transform. Journal of the

ACM 48(3), 407–430 (2001)

29. Matias, Y., Muthukrishnan, S., Sahinalp, S.C., Ziv, J.: Augmenting suﬃx trees,
with applications. In: Bilardi, G., Pietracaprina, A., Italiano, G.F., Pucci, G. (eds.)
ESA 1998. LNCS, vol. 1461, pp. 67–78. Springer, Heidelberg (1998)

30. Milidi´u, R.L., Laber, E.S.: Bounding the ineﬃciency of length-restricted preﬁx

codes. Algorithmica 31(4), 513–529 (2001)

31. Muthukrishnan, S.: Eﬃcient algorithms for document retrieval problems. In: Proc.

SODA, pp. 657–666 (2002)

32. Navarro, G., M¨akinen, V.: Compressed full-text indexes. ACM Computing Surveys,

39(1), article 2 (2007)

33. Raman, R., Raman, V., Rao, S.: Succinct indexable dictionaries with applications

to encoding k-ary trees and multisets. In: Proc. SODA, pp. 233–242 (2002)

34. Sadakane, K.: New text indexing functionalities of the compressed suﬃx arrays.

Journal of Algorithms 48(2), 294–313 (2003)

Colored Range Queries and Document Retrieval

81

35. Sadakane, K.: Succinct data structures for ﬂexible text retrieval systems. Journal

of Discrete Algorithms 5(1), 12–22 (2007)

36. Silvestri, F.: Sorting out the document identiﬁer assignment problem. In: Amati,
G., Carpineto, C., Romano, G. (eds.) ECiR 2007. LNCS, vol. 4425, pp. 101–112.
Springer, Heidelberg (2007)

37. V¨alim¨aki, N., M¨akinen, V.: Space-eﬃcient algorithms for document retrieval. In:
Ma, B., Zhang, K. (eds.) CPM 2007. LNCS, vol. 4580, pp. 205–215. Springer,
Heidelberg (2007)

38. Weiner, P.: Linear pattern matching algorithm. In: Proc. IEEE Symp. on Switching

and Automata Theory, pp. 1–11 (1973)

39. Yan, H., Ding, S., Suel, T.: Inverted index compression and query processing with

optimized document ordering. In: Proc. WWW, pp. 401–410 (2009)

