2
1
0
2

 
r
p
A
0
1

 

 
 
]

G
C
.
s
c
[
 
 

1
v
4
3
0
2

.

4
0
2
1
:
v
i
X
r
a

Adaptive Techniques to ﬁnd Optimal Planar Boxes

J. Barbay∗

G. Navarro†

P. P´erez-Lantero‡

July 16, 2018

Abstract

Given a set P of n planar points, two axes and a real-valued score function f ()
on subsets of P , the Optimal Planar Box problem consists in ﬁnding a box (i.e.
axis-aligned rectangle) H maximizing f (H ∩ P ). We consider the case where f () is
there exists a composition function g() monotone in
monotone decomposable, i.e.
its two arguments such that f (A) = g(f (A1), f (A2)) for every subset A ⊆ P and
every partition {A1, A2} of A. In this context we propose a solution for the Optimal
Planar Box problem which performs in the worst case O(n2 lg n) score compositions
and coordinate comparisons, and much less on other classes of instances deﬁned by
various measures of diﬃculty. A side result of its own interest is a fully dynamic MCS
Splay tree data structure supporting insertions and deletions with the dynamic ﬁnger
property, improving upon previous results [Cort´es et al., J.Alg. 2009].

1

Introduction

Consider a set P of n planar points, and two axes x and y forming a base of the plane,
such that the points are in general position (i.e. no pair of points share the same x or y
coordinate). We say that a real-valued function f () on subsets of P is decomposable [2,8] if
there exists a composition function g() such that f (A) = g(f (A1), f (A2)) for every subset
A ⊆ P and every partition {A1, A2} of A. Without loss of generality, we extend f () to
P such that f (p) = f ({p}). A decomposable function is monotone if the corresponding
composition function g() is monotone in its two arguments. A box is a rectangle aligned
to the axes, and given a monotone decomposable function f (), such a box is f ()-optimal
if it optimizes f (H ∩ P ). Without loss of generality, we assume that we want to maximize
f () and that its composition function g() is monotone increasing in its two arguments.
Given a monotone decomposable function f () well deﬁned for the empty set ∅, a point
p of P is positive if f (p) > f (∅). Otherwise, this point p is negative. Observe that if p
is positive then f (A ∪ {p}) = g(f (A), f (p)) > g(f (A), f (∅)) = f (A) by monotonicity of
g(): hence a point p is positive if and only if f (A ∪ {p}) > f (A) for every subset A ⊂ P
not containing p. A stripe is an area delimited by two lines parallel to the same axis. A
positive stripe (resp. negative stripe) is one which contains only positive (resp. negative)
points. A monochromatic stripe is a stripe in which all points have the same sign.

∗Department of Computer Science, University of Chile, Chile
†Department of Computer Science, University of Chile, Chile. Partially supported by Millennium

Institute for Cell Dynamics and Biotechnology (ICDB), Grant ICM P05-001-F, Mideplan, Chile.

‡Escuela de Ingenier´ıa Civil en Inform´atica, Universidad de Valpara´ıso, Chile. Partially supported by

grant FONDECYT 11110069 and project MEC MTM2009-08652.

1

Given a set of planar points, a simple example of such monotone decomposable functions
is counting the number of points contained in the box. Further examples include counting
the number of blue points; returning the diﬀerence between the number of blue points and
the number of red points contained; returning the number of blue points in the box or
−∞ if it contains some red points; summing the weights of the points contained; taking
the maximum of the weights of contained points; etc.

Given a set P of n planar points and a real-valued function f () on subsets of P , the
Optimal Planar Box problem consists in ﬁnding an f ()-optimal box. Depending of
the choice of the function f (), this geometric optimization problem has various practical
applications, from identifying rectangular areas of interest in astronomical pictures to the
design of optimal rectangular forest cuts or the analysis of medical radiographies. We
present various adaptive techniques for the Optimal Planar Box problem:

• In the worst case over instances composed of n points, our algorithm properly generalizes 
Cort´es et al.’s solution [6] for the Maximum Weight Box problem, within
the same complexity of O(n2 lg n) score compositions.

• For any δ ∈ [1..n] and n1, . . . , nδ ∈ [1..n] summing to n, in the worst case over
instances composed of δ monochromatic stripes of alternating signs when the points
are sorted by their y-coordinates, such that the i-th stripe contains ni points, our
algorithm executes O(δn(1 + H(n1, . . . , nδ))) ⊂ O(δn lg(δ + 1)) score compositions
i=1(ni/n) lg(n/ni) is the usual entropy
function.

(Theorem 4.1), where H(n1, . . . , nδ) = (cid:80)δ

• Assuming the same y-coordinate order, for any λ ∈ [0..n2], in the worst case over
instances where λ is the sum of the distances between the insertion positions of the
consecutive points according to their x-coordinate, our algorithm executes O(n2(1 +
lg(1+λ/n)) score compositions (Lemma 5.1). Measure λ relates to the local insertion
sort complexity [13] of the sequence of x-coordinates. It holds λ ∈ O(n+ Inv), where
Inv is the number of disordered pairs in the sequence. When the points are grouped
into δ monochromatic stripes, this complexity drops to O(nδ(1 + lg(1 + Inv/n))
(Theorem 6.1).

• Assuming the same y-coordinate order, for a minimal cover of the same sequence of
x-coordinates into ρ ≤ n runs (i.e. contiguous increasing subsequences) of lengths
r1, . . . , rρ, our algorithm executes O(n2(1 + H(r1, . . . , rρ))) ⊂ O(n2 lg(ρ + 1)) score
compositions (Lemma 5.3). When the points can be grouped into δ monochromatic
stripes, this complexity decreases to O(nδ(1 + H(r1, . . . , rρ))) ⊂ O(nδ lg(ρ + 1))
(Theorem 6.1 again).

• In the case where subsets of points are clustered along the diagonal, our algorithm
reduces to the corresponding sub-instances in linear time via a clever partitioning
strategy (Theorem 7.7), applying previous techniques on those sub-instances directly
or indirectly (Theorem 8.3).

We describe our algorithm progressively as follows:

1. After describing the existing solutions to more speciﬁc problems, we present a solution 
for the Optimal Planar Box problem performing O(n lg n) coordinate comparisons 
and O(n2 lg n) score compositions in the worst case over instances composed
of n points, in Section 2.

2

2. We describe a truly dynamic MCS tree data structure, supporting in good amortized
time the insertion and deletion of points (as opposed to their mere activation, as in
the original variant described by Cort´es et al. [6]) and of sequences of points ordered
by their coordinates along one axis, such as to support in particular the Dynamic
Finger property on score compositions and comparisons, in Section 3.

3. Observing that an instance composed mostly of positive or negative points is easier
than a general instance, we describe a technique to detect and take advantage of
monochromatic stripes, hence taking advantage of the sign distribution of the values
of f (), in Section 4.

4. Observing that the problem can be solved in O(n lg n) coordinate comparisons and
O(n) score compositions when the points are projected in the same order on both
axis, we describe in Section 5 a variant of the algorithm which takes advantage of the
relative insertion positions of the consecutive points into a sorted sequence, by using
our new dynamic MCS tree data structure. The resulting cost is related to that of
local insertion sorting [10, 13, 14], and we relate it with other measures of disorder
of permutations, such as the number of inversions and the number of consecutive
increasing runs [10, 14].

5. Observing that sorting the points within each monochromatic stripe can only further 
reduce the inversions and runs complexity between the orders of the points,
we combine the two previous techniques into a single algorithm, whose complexity
improves upon both techniques, in Section 6.

6. Observing that one dimensional instances can be solved in linear time even when
plunged in the plane, whereas the techniques described above have quadratic complexity 
on such instances, we describe how to decompose instances where the points
are even only partially aligned into diagonals of blocks, which yields an algorithm
whose best complexity is linear and degrades smoothly to the previous worst case
complexity as the points lose their alignment.

7. Finally, observing that sub-instances which cannot be decomposed into diagonals
of blocks have a speciﬁc “windmill” shape, we show how to take advantage of this
shape in order to still take advantage of particular instances.

The proofs of the most technical results are presented in the appendix, following an exact
(automatic) copy of the statement of the corresponding result.

2 Optimal Boxes and Related Problems

Given a set P of n weighted planar points, in which the weight of a point can be either
positive or negative, the Maximum Weight Box problem [6] consists in ﬁnding a box
R maximizing the sum of the weights of the points in R ∩ P . Cort´es et al. [6] gave an
algorithm solving this problem in time O(n2 lg n) using O(n) space, based on MCS trees,
a data structure supporting in O(lg n) time the dynamic computation of the MaximumSum 
Consecutive Subsequence problem [3] (hence the name “MCS”). Their solution
applies to other similar problems, such as the Maximum Subarray problem [16] which,
given a two-dimensional array, consists in ﬁnding a subarray maximizing the sum of its
elements.

3

The Maximum Weight Box problem [6] and, by successive reductions, the Maximum
Subarray problem [16], the Maximum Box problem [6, 9, 12], and the Maximum Discrepancy 
Box problem [6, 7] can all be reduced to a ﬁnite number of instances of the
Optimal Planar Box problem by choosing adequate deﬁnitions for the score functions
f () to optimize.

Cort´es et al.’s algorithm [6] ﬁrst sorts the points by their y-coordinate in O(n lg n) time
and then traverses the resulting sequence of points p1, p2, . . . pn as follows. For each pi,
it sets an MCS tree (described in more details in Section 3) with points pi, . . . pn, where
the key is their x-coordinate xi, and all have value f (∅). It then successively activates
points pj for j ∈ [i..n], setting its weight to value f (pj), updating the MCS tree so that
to compute the optimal box contained between the y-coordinate of pi to that of pj. The
whole algorithm executes in time O(n2 lg n), corresponding to n2 activations in the tree,
each performed in time O(log n).

3 Fully Dynamic MCS Trees

w(xk) is updated, a consecutive subsequence (xi)i∈[l..r] of S maximizing(cid:80)
Cort´es et al. [6] deﬁned the MCS tree as an index for a ﬁxed sequence S = (xi)i∈[1..n] of n
elements, where each element xk of S has a weight w(xk) ∈ R, so that whenever a weight
i∈[l..r] w(xi) is
obtained (or recomputed) in O(lg n) time. This behavior is dynamic in the sense that it
allows modiﬁcation of element weights, yet it is only partially dynamic in the sense that
it admits neither addition of new elements nor deletion of existing elements.

Existing dynamic data structures can be easily adapted into a truly dynamic data structure
with the same functionalities as MCS trees. We start by generalizing Cort´es et al.’s
algorithm and data-structure [6] from mere additive weights to monotone decomposable
score functions in Lemma 3.1. We further generalize this solution to use an AVL tree [1]
in Lemma 3.2 and a Splay tree [15] in Lemma 3.3, whose “ﬁnger search” property will
yield the results of Sections 4 and 5.

Lemma 3.1 Let S be a static sequence of n elements, and f () be a monotone decomposable 
score function receiving as argument any subsequence of S, deﬁned through the
activation and deactivation of each element of S. There exists a semi-dynamic data structure 
for maintaining S using linear space that supports the search for an element in O(lg n)
comparisons; the activation or deactivation of an element in O(lg n) score compositions;
and f ()-optimal sub range queries in O(lg n) comparisons and score compositions.

Proof. Consider a monotone decomposable score function f () on sequences (a particular
case of monotone decomposable score functions on point sets described in Section 1) and
its corresponding composition function g().

Given a ﬁxed sequence S = (xi)i∈[1..n] of n elements, each element xi of S is associated
to a score f (xi) (a more general notion than the weight previously considered, in that
f (x1, . . . , xn) can be computed more generally than by merely the sum of the individual
scores f (xi)).

Then the MCS tree data structure consists of a binary balanced tree T with n leaves,
in which the leaves of T from left to right store the elements x1, x2, . . . , xn of S. The
term interval is used to denote the ﬁrst and last indices of a consecutive subsequence of

4

S joint with the score of the corresponding sequence. We denote by [xl, xr] the interval
corresponding to the subsequence (xi)i∈[l..r], and [xk, xk] by [xk]. Let ∅ denote the empty
interval whose score is equal to zero, and given intervals I1, . . . , It let max{I1, . . . , It} be
the interval of maximum score among I1, . . . , It. Each node v of T stores the following
four maximum-score intervals, where xl, xl+1, . . . , xr are, from left to right, the leaves of
T descendant of v:

of the sequence (where L stands for “Left”);

• I(v) = [xl, xr], the information about the full sequence;
• L(v) = max{[xl, xl], [xl, xl+1], . . . , [xl, xr], ∅}, the information about the best preﬁx
• R(v) = max{[xl, xr], [xl+1, xr], . . . , [xr, xr], ∅}, the information about the best suﬃx
• M (v) = max{max{[xl(cid:48), xr(cid:48)] : l ≤ l(cid:48)
≤ r}, ∅}, the information about the best

of the sequence (where R stands for “Right”); and ﬁnally

factor of the sequence (where M stands for “Middle”).

≤ r(cid:48)

Then the maximum-score consecutive subsequence of S is given by the interval M () of the
root node of T . Given two contiguous intervals [xl, xk] and [xk+1, xr], let [xl, xk]+[xk+1, xr]
denote the interval [xl, xr], of score obtained through the combination of the scores of of
[xl, xk] and [xk+1, xr]. Consider I + ∅ = ∅ + I = I for every interval I. The key
observation in the MCS tree is that if we have computed I(v1), L(v1), R(v1), M (v1),
I(v2), L(v2), R(v2), and M (v2), where nodes v1 and v2 are the left and right children
of node v, respectively, then I(v), L(v), R(v), and M (v) can be computed through a
constant number of operations: I(v) = I(v1) + I(v2), L(v) = max{L(v1), g(I(v1), L(v2))},
R(v) = max{g(R(v1), I(v2)), R(v2)}, and M (v) = max{M (v1), M (v2), g(R(v1), L(v2))}.
These observations can be used, whenever the score of an element xi of S is modiﬁed, to
update through a constant number of operation each node in the path from the leaf of T
storing the score of a sequence containing xi to the root. Since T is a balanced tree this
corresponds to O(lg n) operations. The MCS tree data structure also supports optimal
subrange queries through O(lg n) operations, that is, given a range R of sequence S reports
a subrange R(cid:48) of R such that the score of S in R(cid:48) is maximized.
2

The MCS tree data structure can be converted into a truly dynamic data structure supporting 
both insertions and deletions of elements. This data structure can be used to
index a dynamic sequence S = (xi)i∈[1..n] of n elements so that whenever an element is
inserted or removed, a consecutive subsequence S(cid:48) = (xi)i∈[l..r] of S optimizing f (S(cid:48)) can
be (re)computed in O(lg n) score compositions and comparisons. The following lemma
establishes the property of this data structure, which we call MCS AVL tree.

Lemma 3.2 Let S be a dynamic sequence of n elements, and f () be a monotone decomposable 
score function receiving as argument any consecutive subsequence of S. There
exists a fully dynamic data structure for maintaining S using linear space that supports
the search for an element in O(lg n) comparisons; the update of the weight of a point in
O(lg n) score compositions, the insertion or deletion of an element in O(lg n) comparisons 
and score compositions; and f ()-optimal subrange queries in O(lg n) comparisons
and score compositions.

Proof. Let S = (xi)i∈[1..n], f () the monotone decomposable score function, and g() its
composition function. Consider an AVL tree T of n nodes such that for i ∈ [1..n] the i-th

5

node in the in-order traversal of T stores element xi of S. We now generalize the MCS
data structure.

Each node v of T , in which the in-order traversal of the subtree rooted at v reports the
elements xl, xl+1, . . . , xr of S, is augmented with the same four intervals I(v), L(v), R(v),
and M (v) used by the MCS data structure. If node v stores the element xk and if the
intervals I(v1), L(v1), R(v1), M (v1), I(v2), L(v2), R(v2), and M (v2) of the left (v1) and
right (v2) children of v have been computed; then I(v), L(v), R(v), and M (v) can be
computed in constant number of score compositions as follows:

• I(v) = I(v1) + [xk] + I(v2)
• L(v) = max{L(v1), I(v1) + [xk] + L(v2)}
• R(v) = max{R(v1) + [xk] + I(v2), R(v2)}
• M (v) = max{M (v1), M (v2), R(v1) + [xk] + L(v2)}

In this computation the value f () of every interval of the form I1 + [xk] + I2 is equal to
g(g(f (S1), f (xk)), f (S2)), where S1 and S2 are the subsequences corresponding to I1 and
I2, respectively. For empty (or null) nodes v, I(v) = L(v) = R(v) = M (v) = ∅. This
states how these intervals are computed for leaves and one-child nodes. We show that the
computation of L(v) is correct by considering separately the case where L(v) contains xk
and the case where it does not. If xk is not in L(v), then L(v) must consider only the
elements xl, xl+1, . . . , xk−1, and is thus equal to L(v1). Otherwise, if xk belongs to L(v),
then L(v) must have the form I(v1) + [xk] + I2, where I2 is either ∅ or an interval of the
form [xk+1, xj], j ∈ [k + 1..r]. Observe that I(v2) has the same form as I2 and maximizes
f (). Since g() is monotone increasing in its ﬁrst and second arguments, then

f (I(v1) + [xk] + I2) = g (g(f (I(v1)), f (xk)), f (I2))

≤ g(g(f (I(v1)), f (xk)), f (I(v2)))
= f (I(v1) + [xk] + I(v2)).

Therefore, I(v1) + [xk] + I(v2) is a correct choice for L(v) if xk belongs to L(v). Similar
arguments can be given for R(v) and M (v).

If the elements of S are ordered by some key, then T is a binary search tree on that key, and
insertions and deletions are performed by using the binary search criterion. Otherwise, if
elements of S are not ordered, we further augment every node v of T with the number
of nodes in the subtree rooted at v in order to perform both insertions and deletions
by rank [5].
In either case, whenever we insert or delete an element we spend O(lg n)
comparisons. Furthermore, when a leaf node is inserted, or a leaf or one-child node is
deleted, we perform a bottom-up update of all nodes of T in the path from that node to
the root. Let v be any of those nodes. If the update at v does not require a rotation,
then the augmented information of v is computed in O(1) score compositions from the
same information of its two children nodes, which are already computed. Otherwise, if
the update v requires a rotation, once the rotation is performed, we update bottom-up
the augmented information of the nodes changed in the rotation (they include v and the
node getting the position of v) from the same information of their new children nodes.
This is done again in O(1) score compositions since there are at most three aﬀected nodes
in any rotation and the augmented information of their new children nodes is already
computed. The update of v always requires O(1) score compositions, and then O(lg n)

6

score compositions are used in total at each insertion or deletion. The subsequence S(cid:48)
corresponding to the interval M () of the root of T is the subsequence of S that maximizes
f (), and is updated at each insertion or deletion.

Optimal subrange queries on T can be performed as follows. Let xl and xr be the ﬁrst
and last elements of the input range R, respectively. Using O(lg n) comparisons we ﬁnd:
the node vl storing xl, the node vr storing xr, and the least common ancestor node vl,r
of vl and vr. Then the intervals Rl, Ml, Lr, and Mr are computed: We ﬁrst initialize
Rl := Ml := ∅, and after that, for each node v taken bottom-up from vl to the left
child of vl,r, such that element xk stored in v belongs to R, we perform the updates
Rl := max{Rl + [xk] + I(v2), R(v2)} and Ml := max{Ml, M (v2), Rl + [xk] + L(v2)} in
this order, where v2 is the right child of v. Similarly, we initialize Rr := Mr := ∅ and
for each node v taken bottom-up from vr to the right child of vl,r, such that element
xk stored in v belongs to R, we do Lr := max{L(v1), I(v1) + [xk] + Lr} and Mr :=
max{M (v1), Mr, R(v1) + [xk] + Lr}, where v1 is the left child of v. Finally, the optimal
subrange of R is equal to max{Ml, Mr, Rl + [xl,r] + Lr}, where xl,r is the element of S
stored in vl,r. The whole computation requires O(lg n) score compositions.

Therefore, the tree T satisﬁes all conditions of the lemma.

2

sequence of m accesses on an arbitrary n-node Splay tree costs O(m + n +(cid:80)m

The Splay tree is a self-adjusting binary search tree created by Sleator and Tarjan [15].
It supports the basic operations search, insert and delete, all of them called accesses, in
O(lg n) amortized time. For many sequences of accesses, splay trees perform better than
other search trees, even when the speciﬁc pattern of the sequences are unknown. Among
other properties of Splay trees, we are particularly interested in the Dynamic Finger
Property, conjectured by Sleator and Tarjan [15] and proved by Cole et al. [4]: every
j=1 lg(dj + 1))
rotations where, for j = 1..m, the j-th and (j − 1)-th accesses are performed on elements
whose ranks among the elements stored in the Splay tree diﬀer by dj. For j = 0, the j-th
element is the element stored at the root. It is easy to see that in the MCS AVL tree we
can replace the underlying AVL tree by a Splay tree, and obtain then the next lemma,
which describes the MCS Splay tree data structure.

Lemma 3.3 Let S be a dynamic sequence of n elements and f () be a monotone decomposable 
function receiving as argument any consecutive subsequence of S. There exists a
data structure for maintaining S that uses linear space and supports the search in O(lg n)
amortized comparisons, the update of the weight of a point in O(lg n) amortized score compositions,
 and the insertion and deletion of elements in O(lg n) amortized comparisons and
score compositions. Joint with the insertion or deletion of any element, the consecutive
subsequence S(cid:48) of S maximizing f (S(cid:48)) is recomputed. The Dynamic Finger Property is
also satisﬁed for each operation (search, insertion and deletion), both for the number of
comparisons and for the number of score compositions performed.

Proof. The complexities of the accesses, and also the Dynamic Finger Property, follow
from the Splay tree properties and from the fact that the augmented information in each
node can be computed in a constant number of score compositions and comparisons, from
the corresponding information stored in its children nodes. Since after each rotation the
augmented information of the aﬀected nodes of the rotation is updated, the consecutive
subsequence S(cid:48) of S maximizing f (S(cid:48)), which is stored at the root, is recomputed.

As in the case of the MCS AVL tree, updating the weight of an element can be reduced

7

to removing and inserting it with its new weight, so that its support in O(lg n) amortized
comparisons and score compositions is a simple consequence of the support of insertions
and deletions. Obviously, in practice the update operator can be implemented much more
simply but still requires a rebalancing of the tree on the element accessed in order to yield
2
the amortized complexity.

4 Taking Advantage of Monochromatic Stripes

Consider an instance where positive and negative points can be clustered into δ positive
and negative stripes along one given axis, of cardinalities n1, . . . , nδ. On such instances
one does not need to consider boxes whose borders are in the middle of such stripes: all
optimal boxes will start at the edge of a stripe; speciﬁcally, the top (resp. bottom) of an
optimal box will align with a positive point at the top (resp. bottom) of a positive stripe.

This very simple observation not only limits the number of boxes for which we need to
compute a score, but also it makes it easier to compute the score of each box: adding
the ni points of the i-th stripe in increasing order of their coordinates in a MCS Splay
i=1 ni lg(n/ni)) coordinate comparisons and score
compositions. The reason is that the ni distances dj + 1 of Lemma 3.3 telescope to at
j=1 lg(dj + 1)) is upper

tree of ﬁnal size n amortizes to O(n +(cid:80)δ
most n + ni within stripe i, and thus by convexity the cost O(n +(cid:80)n

bounded by

(cid:32)

δ(cid:88)

i=1

O

n +

ni lg(1 + n/ni)

(cid:33)

(cid:33)

(cid:32)

δ(cid:88)

i=1

n +

ni lg(n/ni)

⊂ O
= O(n(1 + H(n1, . . . , nδ)))
⊂ O(n lg(δ + 1))

Combining this with the fact that the top of an optimal box is aligned with a positive
point at the top of a positive stripe yields the following result.

Theorem 4.1 For any δ ∈ [1..n] and n1, . . . , nδ ∈ [1..n] summing to n, in the worst
case over instances composed of δ stripes of alternating signs over an axis such that
the i-th stripe contains ni points, there exists an algorithm that ﬁnds an f ()-optimal
box in O(δn(1 + H(n1, . . . , nδ))) ⊂ O(δn lg(δ + 1)) score compositions and O(δn(1 +
H(n1, . . . , nδ)) + n lg n) ⊂ O(δn lg(δ + 1) + n lg n) coordinate comparisons.

5 Taking Advantage of Point Alignments

Running the algorithm outlined in the ﬁrst paragraph of Section 4 over the MCS Splay tree
has further consequences. In this section we show how it makes the algorithm adaptive to
local point alignments.

Imagine that, once the points p1, p2, . . . , pn have been sorted by y-coordinate, they also
become sorted by x-coordinate. By hypothesis, no pair of points are aligned to any axis,
so that the second order is a permutation of the ﬁrst one. Call π the permutation that
rearranges the points p1, p2, . . . , pn from an ordering sorted by x-coordinates to an ordering
by y-coordinates.

8

Then, when we insert the points in the MCS Splay tree, the distance from each insertion
to the previous one is equal to 1, and therefore the overall cost of the algorithm is O(n2)
score composition (recall the description of Cort´es algorithm in Section 2).

j > 1, according to Lemma 3.3. Let us deﬁne λ =(cid:80)n

More generally, assume that the x-coordinate xj of pj falls at position rj in the sorted set
(cid:80)n
of previous x-coordinates {x1, x2, . . . , xj−1}. Then we have d1 = 0 and dj = |rj − rj−1| for
j=2 |rj − rj−1| as the local insertion
j=2 |πj − πj−1|,
complexity of the sequence [13]. Note that a simple upper bound is λ ≤
as the latter refers to the ﬁnal positions of the elements, whereas λ refers to the moves
needed in the current preﬁx of the permutation.

is O(n +(cid:80)n
logarithm and because(cid:80)n

The cost of our algorithm using the MCS Splay tree can then be upper bounded as
follows. When we insert the points in the MCS Splay tree starting from p1, the total cost
j=1 lg(dj + 1)) ⊂ O(n + n lg(1 + λ/n)) score compositions, by convexity of the
j=1 dj + 1 ≤ λ + n. A simple upper bound when considering all

the n passes of the algorithm is obtained as follows.

Lemma 5.1 Let P be a set of n points in the plane. Let f () be a monotone decomposable
score function receiving as argument any subset of P . There exists an algorithm that ﬁnds
an f ()-optimal box in O(n2(1 + lg(1 + λ/n))) score compositions and O(n2(1 + lg(1 +
λ/n)) + n lg n) coordinate comparisons, where λ ≤ n2 is the local insertion complexity of
the sequence of x-coordinates of the points sorted by y-coordinates.

In the worst case this boils down to the O(n2 lg n)-worst-case algorithm, whereas in the
best case λ = 0 and the cost corresponds to O(n2) operations.

We can upper bound λ by using other measures of disorder in permutations. For example,
let us consider Inv, the number of inversions in the permutation π, or said another way, the
number of pairs out of order in the sequence [14]. The measure Inv corresponds to a cost
where the “ﬁnger” is always at the end of the sequence. This can be as small as (λ− n)/2,
for example consider the permutation π = (m, m − 1, m + 1, m − 2, m + 2, . . . , 1, 2m − 1)
for m = (n + 1)/2 and odd n. However, Inv can be much larger than λ because it is
not symmetric on decreasing sequences, for example when the points are semi-aligned in
a decreasing diagonal and the permutation is π = (n, n − 1, n − 2, . . . , 1). Thus replacing
λ by Inv in Lemma 5.1 yields a valid upper bound in terms of big-O complexity.

Lemma 5.2 lemma:relacionInvLambda For any permutation π of local insertion complexity 
λ and presenting Inv inversions,

Inv ≥ λ/2 − n.

Proof. For each xj, λ increases by |πj − πj−1|, whereas Inv increases by j − πj. Since
πj ≤ j, if πj > πj−1, we can use that (j − 1) − πj−1 + 1 ≥ πj − πj−1, or else we use
j − πj ≥ πj−1 − πj. So either the contribution of xj−1 plus 1, or that of xj, to Inv, upper
bounds the contribution of xj to λ. By summing up both contributions (that of xj−1
plus 1, and that of xj) for each j we have 2 · Inv + n and this upper bounds λ. See also
2
Estivill-Castro and Wood’s survey [10].

Another well-known measure of permutation complexity is the number of increasing runs ρ,
that is, the minimum number of contiguous monotone increasing subsequences that cover

9

of the dj values. Therefore(cid:80)n

π [11]. Let r1, . . . , rρ be the lengths of the runs, computed in O(n) comparisons. Then the
sum of the values |πj+1−πj| within the i-th run telescopes to at most n, and so does the sum
i=1 ri lg(n/ri)
by convexity. This leads to the following alternative upper bound.

(cid:80)ρ
i=1 ri lg(1 + n/ri) ≤ n +(cid:80)ρ

j=1 lg(dj + 1) ≤

Lemma 5.3 Let P be a set of n points in the plane. Let f () be a monotone decomposable
function receiving as argument any subset of P . There exists an algorithm that ﬁnds
an f ()-optimal box in O(n lg n) coordinate comparison and O(n2(1 + H(r1, . . . , rρ)) ⊂
O(n2 lg(ρ+1)) score compositions, where r1, . . . , rρ are the lengths of ρ maximal contiguous
increasing subsequences that cover the sequence of x-coordinates of the points sorted by ycoordinate.


6 Taking Advantage of both Stripes and Alignments

The combination of the techniques of Sections 4 and 5 can be elegantly analyzed. A simple
result is that we need to start only from δ diﬀerent pi values, and therefore an upper bound
to our complexity is O(nδ((1+lg(1+λ/n))). We can indeed do slightly better by sorting the
points by increasing x-coordinates within each monochromatic stripe. While the measure
λ(cid:48) resulting from this reordering may be larger than λ, the upper bounds related to Inv
and ρ, namely Inv(cid:48), ρ(cid:48), and H(n(cid:48)
ρ(cid:48)), do not increase. In particular it is easy to see
that the upper bound of Theorem 4.1 is dominated by the combination since ρ(cid:48)
≤ δ and
H(r(cid:48)
ρ(cid:48)) ≤ H(n1, . . . , nδ) (because no run will cut a monochromatic stripe once the

1, . . . , n(cid:48)

1, . . . , r(cid:48)

latter is reordered).

Theorem 6.1 Let P be a set of n points in the plane. Let f () be a monotone decomposable 
function receiving as argument any subset of P . There exists an algorithm that
ﬁnds an f ()-optimal box in O(n lg n) coordinate comparisons and O(nδ(1 + min(lg(1 +
Inv/n),H(r1, . . . , rρ)))) ⊂ O(nδ lg(ρ + 1)) score compositions, where δ is the minimum
number of monochromatic stripes in which the points, sorted by increasing y-coordinate,
can be partitioned; X is the corresponding sequence of x-coordinates once we (re-)sort
by increasing x-coordinate the points within each monochromatic stripe; Inv ≤ n2 is the
number of out-of-order pairs in X; and r1, . . . , rρ are the lengths of the minimum number
ρ ≤ δ of contiguous increasing runs that cover X. A similar result holds by exchanging x
and y axes.

Note that if these new measures are not particularly favorable, the formula boils down to
the O(nδ lg δ) time complexity of Section 4.

7 Taking Advantage of Diagonals of Blocks

In this section we take advantage of the relative positions of the points to obtain a diﬀerent
adaptive algorithm. We will consider partitions of the point set into two subsets. These
partitions are induced by two lines which are parallel to the axes and perpendicular each
other. A combination of optimal boxes of each of the subsets will lead to the optimal box
of the whole point set.
For any subset A ⊆ P , a diagonalization of A is a partition {A1, A2} of A induced by two
lines (cid:96)1 and (cid:96)2, respectively parallel to axes x and y, so that the elements of A1 and the

10

elements of A2 belong to opposite quadrants with respect to the point (cid:96)1∩(cid:96)2. Figure 1 gives
some example of diagonalization. In particular, assuming that the points of A1 are to the
left of the points of A2, we call the diagonalization bottom-up (Figure 1a) if the elements
of A1 are all below the elements of A2 and top-down (Figure 1b) otherwise. Note that if
p1, p2, . . . , pm denote the elements of A sorted by x-coordinate, then any diagonalization
of A has the form {{p1, . . . , pk},{pk+1, . . . , pm}} for some index k ∈ [1..m − 1].

Figure 1: A diagonalization {A1, A2} of the point set A = A1 ∪ A2.

Given any bounded set S ⊂ R2, let Box(S) denote the smallest enclosing box of S and let

the extreme points of A be those belonging to the boundary of Box(A).

Not all point sets can be decomposed into diagonals, the simplest case being a set of four
points placed at the four corners of a square which sides are slightly rotated from the axes
x and y. We call such a point set a windmill, for the characteristic position of its extreme
points. See Figure 2 for an example.

Figure 2: Windmills.

Lemma 7.1 Let A be a point set that does not admit a diagonalization. Then A has
exactly four extreme points. Furthermore, A has a windmill which contains at least one
extreme point of A.

Proof. The ﬁrst part of the lemma is easy to show. We proceed to prove the second part.
Let {p, q, r, s} be the extreme points of A.
If they form a windmill then we are done.
Otherwise, assume without loss of generality that their relative positions are as depicted
in Figure 3a. If there is an element q(cid:48) of A in region R1 then {p, q(cid:48), r, s} is a windmill
and we are done. Analogously, if there is a point s(cid:48) of A in region R2 then {p, q, r, s(cid:48)
} is a
windmill and we are done. Assume then that R1 ∪ R2 is empty of elements of A. Since A
does not admit a diagonalization then R3∪ R4 must contain a point of A. Assume without
loss of generality that R3 contains a point. Let t be the rightmost point of R3 creating
regions R5 and R6 as shown in Figure 3b. If there is a point u in R5 then set {p, q, t, u} is
a windmill and we are done. Analogously, if there is a point u in R6 then set {p, t, r, u} is

11

ℓ1ℓ2(a)A1A2ℓ1ℓ2(b)A1A2(a)(b)Figure 3: Proof of Lemma 7.1. In each ﬁgure the shaded area is empty of points of set A.

a windmill and we are done. Then assume R5 ∪ R6 is empty of elements of A. Now, since
A does not admit a diagonalization, region R4 must contain a point of A. Let then t(cid:48) be
the bottom-most point of R4, which induces the regions R6 and R7 depicted in Figure 3c.
We now proceed with t(cid:48), similar as we did with t. If region R7 contains a point u of A then
{p, t, u, t(cid:48)
} is a windmill and the result follows. Otherwise, region R8 must contain a point
u(cid:48) of A, because in the contrary case A would have a diagonalization. Then {t, u(cid:48), s, t(cid:48)
} is
2

a windmill. Therefore, the result follows.

Deﬁnition 7.2 A diagonalization tree of P , D-tree, is a binary tree such that: (i) each
leaf u contains a subset S(u) ⊆ P which does not admit a diagonalization, (ii) set
{S(u) | u is a leaf } is a partition of P , and (iii) each internal node v has exactly two
children v1 (the left one) and v2 (the right one) and satisﬁes that {A(v1), A(v2)} is a diagonalization 
of A(v), where for each node v A(v) denotes the union of the sets S(u) for
all leaves u descendant of v (See Figure 4).

Figure 4: A D-tree of the point set {p1, . . . , p13}.

12

qprsR1R2R3R4qprsR1R2R5R4tR6qprsR1R2R5R7tR6t′R8(a)(b)(c)p1p2p3p4p5p6p7p8p9p10p11p12p13p3,...,p7p1p2p8p9p10,...,p13Lemma 7.3 Let P be a set of n points in the plane. Every D-tree of P has the same
number of leaves. Furthermore, the i-th leaves from left to right of any two D-trees of P
contain the same subset S(·) of P .

Proof. We use induction on the number of elements of P . If P contains only one element,
or P does not admit a diagonalization, then we are done. Otherwise, assume that P has
exactly k ≥ 1 diﬀerent diagonalization, all of which must be of the same type (bottomup 
or top-down). Then P is partitioned into k + 1 non-empty sets denoted from left
i=j+1 Pi} for
to right P1, P2, . . . , Pk+1 such that the k diagonalizations are {
j = 1 . . . k. Observe then that no set Pi admits a bottom-up diagonalization and that any
D-tree of P can be obtained by building a binary tree, whose leaves from left to right are
P1, P2, . . . , Pk+1 and each internal node has two children, and replacing every leaf Pi by a
D-tree of Pi. Applying the induction hypothesis for P1, P2, . . . , Pk+1 the result follows. 2

(cid:83)j
i=1 Pi,(cid:83)k+1

From Lemma 7.3 we can conclude that every D-tree T of P induces the same partition of
P , which is equal to {S(u1), . . . , S(uβ)}, where u1, . . . , uβ are the leaf nodes of T.

Lemma 7.4 Let P be a set of n points in the plane. A D-tree of P requires O(n) space
and can be built in O(n lg n) comparisons.

Proof. Let p1, p2, . . . , pn be the elements of P sorted by x-coordinate, and let pπ1, pπ2, . . . ,
pπn be the elements of P sorted by y-coordinate. Both orders can be obtained in O(n lg n)
coordinate comparisons.

Considering the computation of permutation π as a preprocessing, we now show that:
If P admits a diagonalization {{p1, . . . , pk},{pk+1, . . . , pn}} then it can be determined in
O(min{k, n − k}) comparisons. Otherwise, if P does not admit a diagonalization, then it
can be decided in O(n) comparisons.
For each index i ∈ [1..n], let ML(i) = maxj∈[1..i] πj, mL(i) = minj∈[1..i] πj, MR(i) =
maxj∈[i..n] πj, and mR(i) = minj∈[i..n] πj.
Observe that if {{p1, . . . , pk},{pk+1, . . . , pn}} is a diagonalization of P , then index k ∈
[1..n − 1] satisﬁes ML(k) = k or mL(k) = n − k + 1. Furthermore, ML(k) = k and
mL(k) = n − k + 1 are equivalent to mR(k + 1) = k + 1 and MR(k + 1) = n − k,
respectively.
Then we can determine a diagonalization of P , if it exists, as follows: For j = 1..(cid:98)n/2(cid:99)
decide if {{p1, . . . , pj},{pj+1, . . . , pn}} is a diagonalization (i.e. ML(j) = j or mL(j) =
n− j + 1) or {{p1, . . . , pn−j},{pn−j+1, . . . , pn}} is a diagonalization (i.e. MR(n− j + 1) = j
or mR(n − j + 1) = n − j + 1). Note that if j > 1 then ML(j), mL(j), MR(n − j + 1),
and mR(n − j + 1) can all be computed in O(1) comparisons from ML(j − 1), mL(j − 1),
πj, MR(n − j + 2), mR(n − j + 2), and πn−j+1. Therefore, if there is a diagonalization
{{p1, . . . , pk},{pk+1, . . . , pn}} of P it is decided for j = min{k, n − k} ≤ (cid:98)n/2(cid:99), and
If no diagonalization is found for any value of
thus determined in O(j) comparisons.
j ∈ [1..(cid:98)n/2(cid:99)], then the algorithm spends O(n) comparisons in total.
We can then build a D-tree of P recursively as follows. Run the above algorithm for P .
If a diagonalization {{p1, . . . , pk},{pk+1, . . . , pn}} of P exists, which was determined in
O(t) comparisons where t = min{k, n − k}, then create a root node and set as left child a
D-tree of {p1, . . . , pk} and as right child a D-tree of {pk+1, . . . , pn}. Otherwise, if P does

13

not admit a diagonalization, which was decided in O(n) comparisons, then create a leaf
node whose set S(·) is equal to P . This results in the next recurrence equation for the
total number T (n) of comparisons, where 1 ≤ t ≤ (cid:98)n/2(cid:99):

(cid:26) O(t) + T (t) + T (n − t)

O(n)

T (n) =

n > 1, a diagonalization exists

otherwise.

W.l.o.g. assume that the constants in O(t) and O(n) in the recurrence are equal to one.
Then we prove by induction that T (n) ≤ n + n lg n. The base case of the induction is the
second line of the recurrence equation, where n ≤ n + n lg n always holds. In the inductive
case, we have:

T (n) = t + T (t) + T (n − t)

≤ n + t + t lg t + (n − t) lg(n − t)
= n + t + t lg t + (n − t) lg(n − t) + (n lg n − t lg n − (n − t) lg n)
= n + n lg n + t − t lg(n/t) − (n − t) lg(n/(n − t))
= n + n lg n + n(t/n − H(t/n))
≤ n + n lg n

The second line uses the inductive hypothesis. In the third line we add and subtract n lg n
written in two diﬀerent forms. In the fourth line we regroup terms. In the ﬁfth line we
introduce the binary entropy function H(x) = x lg(1/x) + (1 − x) lg(1/(1 − x)), where
x = t/n. Finally, in the last line we apply the analytic inequality x ≤ H(x), which holds
at least for x ≤ 1/2. Thus T (n) ≤ n + n lg n and then T (n) is O(n lg n). One can see that
this solution is tight by considering the case t = n/2.

It is easy to see that any D-tree of P requires O(n) space. The result follows.

2

Deﬁnition 7.5 For any non-empty subset A ⊆ P the set of “ten” f ()-optimal boxes of
A, denoted by Ten(A), consists of the following f ()-optimal boxes of A, all contained in
Box(A):

1. Box(A).

2. Bopt(A), an f ()-optimal box.

3. B1(A), an f ()-optimal box containing the bottom-left vertex of Box(A).

4. B2(A), an f ()-optimal box containing the bottom-right vertex of Box(A).

5. B3(A), an f ()-optimal box containing the top-right vertex of Box(A).

6. B4(A), an f ()-optimal box containing the top-left vertex of Box(A).

7. B1,2(A), an f ()-optimal box containing the bottom vertices of Box(A).

8. B2,3(A), an f ()-optimal box containing the right vertices of Box(A).

9. B3,4(A), an f ()-optimal box containing the top vertices of Box(A).

10. B4,1(A), an f ()-optimal box containing the left vertices of Box(A).

Lemma 7.6 For any non-empty subset A ⊆ P and any diagonalization {A1, A2} of A,
Ten(A) can be computed in O(1) score compositions from Ten(A1) and Ten(A2).

14

Proof. Suppose without loss of generality that {A1, A2} is bottom-up . Let u2 and u4
be the bottom-right and top-left vertices of Box(A), respectively. Let max{H1, . . . , Ht}
denote the f ()-optimal box for A among the boxes H1, . . . , Ht. Observe that:

• Box(A) = Box(A1 ∪ A2) = Box(Box(A1) ∪ Box(A2)).
• Bopt(A) = max{Bopt(A1), Bopt(A2), Box(B3(A1) ∪ B1(A2))}.
• B1(A) = max{B1(A1), Box(Box(A1) ∪ B1(A2))}.
• B2(A) = Box(max{B2(A1), B2(A2), Box(B2,3(A1) ∪ B1,2(A2))} ∪ {u2}).
• B3(A) = max{Box(B3(A1) ∪ Box(A2)), B3(A2)}.
• B4(A) = Box(max{B4(A1), B4(A2), Box(B3,4(A1) ∪ B4,1(A2))} ∪ {u4}).
• B1,2(A) = max{Box(B1,2(A1) ∪ {u2}), Box(Box(A1) ∪ B1,2(A2))}
• B2,3(A) = max{Box(B2,3(A2) ∪ {u2}), Box(Box(A2) ∪ B2,3(A1))}
• B3,4(A) = max{Box(B3,4(A2) ∪ {u4}), Box(Box(A2) ∪ B3,4(A1))}
• B4,1(A) = max{Box(B4,1(A1) ∪ {u4}), Box(Box(A1) ∪ B4,1(A2))}

Since each of the elements of Ten(A) can be obtained from a constant number of pairwise

∩ A)) = g(f (H ∩ A), f (H(cid:48)

disjoint boxes of Ten(A1) ∪ Ten(A2), and for any two disjoint boxes H and H(cid:48) we have
f ((H ∪ H(cid:48)) ∩ A) = f ((H ∩ A) ∪ (H(cid:48)
∩ A)), the result follows. 2
ﬁnds an f ()-optimal box of P in O(n lg n +(cid:80)β
Theorem 7.7 Let P be a set of n points in the plane. Let f () be a monotone decom-
indices) and O((cid:80)β
posable function receiving as argument any subset of P . There exists an algorithm that
i=1 hc(ni)) comparisons (on coordinates and
i=1 hs(ni) + β) score compositions, where {P1, . . . , Pβ} is the partition of
P induced by any D-tree of P and β is the size of this partition, ni is the cardinality of Pi,
and hc(ni) and hs(ni) are the numbers of coordinate comparisons and score compositions
used, respectively, to compute the “ten” f ()-optimal boxes of Pi.

Proof. Build a D-tree T of P in O(n lg n) comparisons (Lemma 7). Let u1, . . . , uβ be
the leaves of T which satisfy S(ui) = Pi for all i ∈ [1..n]. Compute the set Ten(S(ui)) =
Ten(Pi) in hc(ni) coordinate comparisons and hs(ni) score compositions. By using a postorder 
traversal of T , for each internal node v of T compute Ten(A(v)) from Ten(A(v1))
and Ten(A(v2)), where v1 and v2 are the children nodes of v, in O(1) score compositions
(Lemma 7.6). The f ()-optimal box of P is the box Bopt(A(r)), where r is the root node of T
i=1 hc(ni) = O(n lg n+
i=1 hs(ni) + β)
2

and satisﬁes A(r) = P . In total, this algorithm runs in O(n lg n)+(cid:80)β
(cid:80)β
i=1 hc(ni)) coordinate comparisons and(cid:80)β

i=1 O(1) = O((cid:80)β

i=1 hs(ni) +(cid:80)β−1

score compositions.

Observe that the best case of the algorithm of Theorem 7.7 is when |S(ui)| is O(1) for each
leaf node ui of the D-tree of P . It yields a complexity of O(n lg n) coordinate comparisons
and O(n) score compositions. The worst case is when P does not admit a diagonalization,
but even in this case the additional O(n lg n) coordinate comparisons, performed by the
algorithm while trying to diagonalize, do not add to the overall asymptotic complexity.
Hence the worst case complexity of the diagonalization algorithm reduces to the case studied 
in the previous sections, with a complexity of at most O(n lg n) coordinate comparisons
and O(n2 lg n) score compositions.

15

8 Dealing with Windmills

In this section we use Lemma 7.1 to obtain a variant of the algorithm in Theorem 7.7.
The set S(u) of every leaf node u of any D-tree of P does not admit a diagonalization and
has a windmill containing an extreme point of S(u). The idea is to remove the extreme
points of S(u) and then recursively build a D-tree of the remaining points. This approach
yields a diagonalization in depth of the point set, potentially reducing the number of score
compositions.

Deﬁnition 8.1 An extended diagonalization tree of P , D∗-tree, is deﬁned recursively as
follows: Each leaf node u of a D-tree of P satisfying |S(u)| > 1 is replaced by a node u(cid:48)
containing the set X(u) of the four extreme points of S(u), and if the set S(u) \ X(u) is
not empty then u(cid:48) has as its only one child a D∗-tree of S(u) \ X(u).
Lemma 8.2 Let P be a set of n points in the plane. Every D∗-tree of P has the same
number σ of one-child nodes, contains n − 4σ leaves nodes, and every leaf node u satisﬁes
|S(u)| = 1 or |S(u)| = 4. A D∗-tree of P requires O(n) space and can be built in O(n lg n+
σn) comparisons.

Proof. The ﬁrst part of the proof can be seen from Lemma 7.3 and Deﬁnition 8.1. A
D∗-tree of P can be built in O(n lg n + σn) comparisons by following the same algorithm
to build a D-tree of P until ﬁnding a leaf node u such that S(u) does not admit a diagonalization.
 At this point we pay O(n) comparisons in order to continue the algorithm
with the set S(u) \ X(u) according to Deﬁnition 8.1. Since this algorithm ﬁnds σ nodes
u, the total comparisons are O(n lg n + σn). The D∗-tree has n nodes of bounded degree
2
and hence can be encoded in linear space.

Theorem 8.3 Let P be a set of n points in the plane. Let f () be a monotone decomposable
function receiving as argument any subset of P . There exists an algorithm that ﬁnds an
f ()-optimal box of P in O(n lg n + σn) coordinate comparisons and O(n + σn lg n) score
compositions, where σ is the number of one-child nodes of every D∗-tree of P .

Proof. Build a D∗-tree T of P in O(n lg n + σn) comparisons (Lemma 8.2). For each
of the n − 4σ leaves nodes u of T compute Ten(S(u)) in constant score compositions.
Then, using a post-order traversal of T , compute Ten(S(u)) for each internal node u as
follows: If v has two children v1 (the left one) and v2 (the right one), then {A(v1), A(v2)}
is a diagonalization of A(v) and Ten(A(v)) can be computed in O(1) score compositions
from Ten(A(v1)) and Ten(A(v2)) (Lemma 7.6). Otherwise, if v is one of the σ one-child
nodes, then Ten(A(v)) can be computed in O(n lg n) worst-case comparisons and score
compositions. Namely, if a box of Ten(A(v)) contains a at least one point of X(u) in
the boundary then it can be found in O(n lg n) comparisons and score compositions [6].
Otherwise, it is a box of Ten(A(v(cid:48))), where v(cid:48) is the child of v. We pay O(1) score
compositions for each of the O(n) two-child nodes and O(n lg n) score compositions for
each of the σ one-child nodes. Then the total score compositions is O(n + σn lg n). The
2
result follows.

16

9 Conclusions

Cort´es et al. [6] proposed a solution for the Maximum Weight Box problem based on
a data structure maintaining the scores of a hierarchy of segments, the MCS Tree. We
extended this solution in several directions:

1. we showed how to replace the sum operator by a monotone decomposable function
f (), so that Cort´es et al.’s algorithm [6] can optimize more general score functions
than merely the sum of the weights of the points;

2. we extended the MCS tree data structure to make it fully dynamic, supporting the
insertion and removal of points in time logarithmic in the number of points in the
structure and in their relative insertion ranks;

3. we described adaptive techniques to take advantage of various particularities of instances,
 such as the clustering of positive and negative points or the relative positions
of the points, without sacriﬁcing worst case complexity.

Whereas we showed many techniques to take advantage of particular instances of the
Optimal Planar Box problem, other types of instances are left to study, such as more
complex forms of clustering, and a uniﬁed analysis of all the adaptive techniques presented.
Other directions of research are the generalization of the problem to higher dimension, and
to other shapes than boxes, such as convex polygons.

References

[1] G. Adelson-Velskii and E. M. Landis. An algorithm for the organization of information.
 In Proceedings of the USSR Academy of Sciences, volume 146, pages 263–266,
1962. (Russian) English translation by M. J. Ricci in Soviet Math. Doklady, 3:12591263,
 1962.

[2] C. Bautista-Santiago, J. M. D´ıaz-B´a˜nez, D. Lara, P. P´erez-Lantero, J. Urrutia, and

I. Ventura. Computing optimal islands. Oper. Res. Lett., 39(4):246–251, 2011.

[3] J. Bentley. Programming pearls: algorithm design techniques. Commun. ACM,

27(9):865–873, 1984.

[4] R. Cole, B. Mishra, J. Schmidt, and A. Siegel. On the dynamic ﬁnger conjecture for
splay trees. Part I: Splay sorting log n-block sequences. SIAM J. Comp., 30(1):1–43,
2000.

[5] T. H. Cormen, C. Stein, R. L. Rivest, and C. E. Leiserson. Introduction to Algorithms.

McGraw-Hill Higher Education, 2nd edition, 2001.

[6] C. Cort´es, J. M. D´ıaz-B´a˜nez, P. P´erez-Lantero, C. Seara, J. Urrutia, and I. Ventura.
Bichromatic separability with two boxes: A general approach. J. Algorithms, 64(2-
3):79–88, 2009.

[7] D. P. Dobkin, D. Gunopulos, and W. Maass. Computing the maximum bichromatic
discrepancy, with applications to computer graphics and machine learning. J. Comput.
 Syst. Sci., 52(3):453–470, 1996.

17

[8] D. P. Dobkin and S. Suri. Dynamically computing the maxima of decomposable

functions, with applications. In FOCS, pages 488–493, 1989.

[9] J. Eckstein, P. Hammer, Y. Liu, M. Nediak, and B. Simeone. The maximum box
problem and its application to data analysis. Comput. Optim. App., 23(3):285–298,
2002.

[10] V. Estivill-Castro and D. Wood. A survey of adaptive sorting algorithms. ACM

Comp. Surv., 24(4):441–476, 1992.

[11] D.E. Knuth. The Art of Computer Programming, volume 3. Addison-Wesley, 1968.

[12] Y. Liu and M. Nediak. Planar case of the maximum box and related problems. In

CCCG, pages 14–18, 2003.

[13] Heikki Mannila. Measures of presortedness and optimal sorting algorithms. In IEEE

Trans. Comput., volume 34, pages 318–325, 1985.

[14] A. Moﬀat and O. Petersson. An overview of adaptive sorting. Australian Comp. J.,

24(2):70–77, 1992.

[15] D. D. Sleator and R. E. Tarjan. Self-adjusting binary search trees. J. ACM, 32(3):652–

686, 1985.

[16] T. Takaoka. Eﬃcient algorithms for the maximum subarray problem by distance
matrix multiplication. Electronic Notes in Theoretical Computer Science, 61:191–
200, 2002. CATS’02, Computing: the Australasian Theory Symposium.

18

