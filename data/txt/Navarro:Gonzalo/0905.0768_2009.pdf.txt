0
1
0
2

 

p
e
S
4
2

 

 
 
]
S
D
.
s
c
[
 
 

5
v
8
6
7
0

.

5
0
9
0
:
v
i
X
r
a

Fully-Functional Static and Dynamic Succinct Trees ∗

Gonzalo Navarro†

Kunihiko Sadakane‡

Abstract

We propose new succinct representations of ordinal trees, which have been studied extensively.

It is known that any n-node static tree can be represented in 2n + o(n) bits and a
number of operations on the tree can be supported in constant time under the word-RAM
model. However the data structures are complicated and diﬃcult to dynamize. We propose a
simple and ﬂexible data structure, called the range min-max tree, that reduces the large number 
of relevant tree operations considered in the literature to a few primitives that are carried
out in constant time on suﬃciently small trees. The result is extended to trees of arbitrary
size, achieving 2n + O(n/polylog(n)) bits of space, which is optimal for some operations. The
redundancy is signiﬁcantly lower than any previous proposal. For the dynamic case, where
insertion/deletion of nodes is allowed, the existing data structures support very limited operations.
 Our data structure builds on the range min-max tree to achieve 2n + O(n/ log n) bits
of space and O(log n) time for all the operations. We also propose an improved data structure
using 2n + O(n log log n/ log n) bits and improving the time to the optimal O(log n/ log log n)
for most operations. We extend our support to forests, where whole subtrees can be attached
to or detached from others, in time O(log1+ n) for any  > 0.
Our techniques are of independent interest. An immediate derivation gives improved solution
to range minimum/maximum queries where consecutive elements diﬀer by ±1, achieving O(n +
n/polylog(n)) bits of space. A second one stores an array of numbers supporting operations
sum and search and limited updates, in optimal time O(log n/ log log n). A third one allows
representing dynamic bitmaps and sequences supporting rank/select and indels, within zeroorder 
entropy bounds and optimal time O(log n/ log log n) for all operations on bitmaps and
polylog-sized alphabets, and O(log n log σ/(log log n)2) on larger alphabet sizes σ. This improves
upon the best existing bounds for entropy-bounded storage of dynamic sequences, compressed
full-text self-indexes, and compressed-space construction of the Burrows-Wheeler transform.

1

Introduction

Trees are one of the most fundamental data structures, needless to say. A classical representation
of a tree with n nodes uses O(n) pointers or words. Because each pointer must distinguish all the
nodes, it requires log n bits1 in the worst case. Therefore the tree occupies Θ(n log n) bits. This
causes a space problem for storing a large set of items in a tree. Much research has been devoted to

Institute for Cell Dynamics and Biotechnology (ICDB), Grant ICM P05-001-F, Mideplan, Chile.

∗A preliminary version of this paper appeared in Proc. SODA 2010, pp. 134–149.
†Department of Computer Science, University of Chile. gnavarro@dcc.uchile.cl. Funded in part by Millennium
‡Principles of Informatics Research Division, National Institute of Informatics, 2-1-2 Hitotsubashi, Chiyoda-ku,
Tokyo 101-8430, Japan. sada@nii.ac.jp. Work supported in part by the Grant-in-Aid of the Ministry of Education,
Science, Sports and Culture of Japan.

1The base of logarithm is 2 throughout this paper.

1

reducing the space to represent static trees [27, 34, 35, 37, 19, 20, 6, 12, 9, 10, 29, 25, 3, 21, 48, 28, 11]
and dynamic trees [36, 46, 8, 1], achieving so-called succinct data structures for trees.

(cid:1)/(2n − 1) = 22n/Θ(n

n−1

bits because there exist (cid:0)2n−1

A succinct data structure stores objects using space close to the information-theoretic lower
bound, while simultaneously supporting a number of primitive operations on the objects in constant 
time. Here the information-theoretic lower bound for storing an object from a universe with
cardinality L is log L bits because in the worst case this number of bits is necessary to distinguish
any two objects.
In this paper we are interested in ordinal trees, in which the children of a node are ordered. The
information-theoretic lower bound for representing an ordinal tree with n nodes is 2n − Θ(log n)
3
2 ) such trees [34]. The size of a succinct data
structure storing an object from the universe is typically (1 + o(1)) log L bits. We assume that the
computation model is the word RAM with word length Θ(log n) in which arithmetic and logical
operations on Θ(log n)-bit integers and Θ(log n)-bit memory accesses can be done in constant time.
Basically there exist three types of succinct representations of ordinal trees: the balanced parentheses 
sequence (BP) [27, 34], the level-order unary degree sequence (LOUDS) [27, 10], and the
depth-ﬁrst unary degree sequence (DFUDS) [6, 28]. An example of them is shown in Figure 1.
LOUDS is a simple representation, but it lacks many basic operations, such as the subtree size of
a given node. Both BP and DFUDS build on a sequence of balanced parentheses, the former using
the intuitive depth-ﬁrst-search representation and the latter using a more sophisticated one. The
advantage of DFUDS is that it supports a more complete set of operations by simple primitives,
most notably going to the i-th child of a node in constant time. In this paper we focus on the BP
representation, and achieve constant time for a large set of operations, including all those handled
with DFUDS. Moreover, as we manipulate a sequence of balanced parentheses, our data structure
can be used to implement a DFUDS representation as well.

1.1 Our contributions

We propose new succinct data structures for ordinal trees encoded with balanced parentheses, in
both static and dynamic scenarios.

Static succinct trees. For the static case we obtain the following result.

Theorem 1 For any ordinal tree with n nodes, all operations in Table 1 except insert and delete
are carried out in constant time O(c) with a data structure using 2n +O(n/ logc n) bits of space on
a Θ(log n)-bit word RAM, for any constant c > 0. The data structure can be constructed from the
balanced parentheses sequence of the tree, in O(n) time using O(n) bits of space.

The space complexity of our data structures signiﬁcantly improves upon the lower-order term
achieved in previous representations. For example, the extra data structure for level ancestor requires 
O(n log log n/
log n) bits [37], or O(n(log log n)2/ log n) bits2 [28], and that for child requires
O(n/(log log n)2) bits [29]. Ours requires O(n/ logc n) bits for all of the operations. We show in
the Conclusions that this redundancy is optimal for some operations.

√

The simplicity and space-eﬃciency of our data structures stem from the fact that any query
operation in Table 1 is reduced to a few basic operations on a bit vector, which can be eﬃciently

2This data structure is for DFUDS, but the same technique can be also applied to BP.

2

solved by a range min-max tree. This approach is diﬀerent from previous studies in which each
operation needs distinct auxiliary data structures. Therefore their total space is the summation
of all the data structures. For example, the ﬁrst succinct representation of BP [34] supported
only ﬁndclose, ﬁndopen, and enclose (and other easy operations) and each operation used diﬀerent
data structures. Later, many further operations such as lmost leaf [35], lca [48], degree [9], child
and child rank [29], level ancestor [37], were added to this representation by using other types of
data structures for each. There exists another elegant data structure for BP supporting ﬁndclose,
ﬁndopen, and enclose [19]. This reduces the size of the data structure for these basic operations,
but still has to add extra auxiliary data structures for other operations.

Dynamic succinct trees. Our approach is suitable for the dynamic maintenance of trees. Former 
approaches in the static case use two-level data structures to reduce the size, which causes
diﬃculties in the dynamic case. On the other hand, our approach using the range min-max tree
is easily applied in this scenario, resulting in simple and eﬃcient dynamic data structures. This is
illustrated by the fact that all the operations are supported. The following theorem summarizes
our results.

Theorem 2 On a Θ(log n)-bit word RAM, all operations on a dynamic ordinal tree with n nodes
can be carried out within the worst-case complexities given in Table 1, using a data structure that
requires 2n + O(n log log n/ log n) bits. Alternatively, the operations of the table can be carried out
in O(log n) time using 2n + O(n/ log n) bits of space.

Note we achieve time complexity O(log n/ log log n) for most operations, including insert and
delete, if we solve degree, child, and child rank naively. Otherwise we can achieve O(log n) complexity 
for these, yet also for insert and delete.3 The time complexity O(log n/ log log n) is optimal:
Chan et al. [8, Thm. 5.2] showed that just supporting the most basic operations of Table 1 (ﬁndopen,
ﬁndclose, and enclose, as we will see) plus insert and delete, requires this time even in the amortized
sense, by a reduction from Fredman and Saks’s lower bounds on rank queries [17].
Moreover, we are able to attach and detach whole subtrees, in time O(log1+ n) for any constant
 > 0 (see Section 2.3 for the precise details). These operations had never been considered before
in succinct tree representations.

Byproducts. Our techniques are of more general interest. A subset of our data structure is able to
solve the well-known “range minimum query” problem [4]. In the important case where consecutive
elements diﬀer by ±1, we improve upon the best current space redundancy of O(n log log n/ log n)
bits [14].
Corollary 1 Let E[0, n−1] be an array of numbers with the property that E[i]−E[i−1] ∈ {−1, +1}
for 0 < i < n, encoded as a bit vector P [0, n − 1] such that P [i] = 1 if E[i] − E[i − 1] = +1 and
P [i] = 0 otherwise. Then, in a RAM machine we can preprocess P in O(n) time and O(n) bits
such that range maximum/minimum queries are answered in constant O(c) time and O(n/ logc n)
extra bits on top of P .

3In the conference version of this paper [50] we erroneously aﬃrm we can obtain O(log n/ log log n) for all these
operations, as well as level ancestor, level next/level prev, and level lmost/level rmost, for which we can actually
obtain only O(log n).

3

Another direct application, to the representation of a dynamic array of numbers, yields an
improvement to the best current alternative [30] by a Θ(log log n) time factor.
If the updates
are limited, further operations sum (that gives the sum of the numbers up to some position) and
search (that ﬁnds the position where a given sum is exceeded) can be supported, and our complexity
matches the lower bounds for searchable partial sums by Pˇatra¸scu and Demaine [41] (if the updates
are not limited one can still use previous results [30], which are optimal in that general case). We
present our result in a slightly more general form.

|xi| = O(log n), can be stored within ((cid:80)|xi|)(1 + o(1)) bits of space, so that we can (i) compute

Lemma 1 A sequence of n variable-length constant-time self-delimiting4 bit codes x1 . . . xn, where
any sequence of codes xi, . . . , xj, (ii) update any code xi ← y, (iii) insert a new code z between any
pair of codes, and (iv) delete any code xd from the sequence, all in O(log n/ log log n) time (plus
j − i for (i)). Moreover, let f (xi) be a nonnegative integer function computable in constant time
from the codes. If the updates and indels are such that |f (y) − f (xi)|, f (z), f (xd) = O(log n), then
j=1 f (xi) and search(s) = max{i, sum(i) ≤ s} within

we can also support operations sum(i) = (cid:80)i

the same time.

For example we can store n numbers 0 ≤ ai < 2k within kn + o(kn) bits, by using their k-bit
binary representation [ai]2 as the code, and their numeric value as f ([ai]2) = ai, so that we support
sum and search on the sequence of numbers. If the numbers are very diﬀerent in magnitude we

can δ-encode them to achieve ((cid:80) log ai)(1 + o(1)) +O(n) bits of space. We can also store bits, seen

as 1-bit codes, in n + o(n) bits and and carry out sum = rank and search = select, insertions and
deletions, in O(log n/ log log n) time.

A further application of our results to the compressed representation of sequences achieves a

result summarized in the next theorem.
Theorem 3 Any sequence S[0, n−1] over alphabet [1, σ] can be stored in nH0(S)+O(n log σ/ log n+
σ log n) bits of space, for any constant 0 <  < 1, and support the operations rank, select, insert,
. For polylogarithmic-sized alphabets, this is the

and delete, all in time O(cid:16) log n

(cid:16)

1 + log σ
log log n
optimal O(log n/ log log n); otherwise it is O

log log n

(cid:17)(cid:17)
(cid:16) log n log σ

(log log n)2

(cid:17)

.

This time complexity slashes the the best current result [22] by a Θ(log log n) factor. The
optimality of the polylogarithmic case stems again from Fredman and Saks’ lower bound on rank
on dynamic bitmaps [17]. This result has immediate applications to building compressed indexes
for text, building the Burrows-Wheeler transform within compressed space, and so on.

1.2 Organization of the paper

In Section 2 we review basic data structures used in this paper. Section 3 describes the main ideas
for our new data structures for ordinal trees. Sections 4 and 5 describe the static construction. In
Sections 6 and 7 we give two data structures for dynamic ordinal trees. In Section 8 we derive our
new results on compressed sequences and applications. In Section 9 we conclude and give future
work directions.

4This means that one can distinguish the ﬁrst code xi from a bit stream xiα in constant time.

4

Table 1: Operations supported by our data structure. The time complexities are for the dynamic
case; in the static case all operations are performed in constant time.The ﬁrst group is composed
of basic operations, used to implement the others, but which could have other uses.

operation

description

inspect(i)
ﬁndclose(i)/ﬁndopen(i)
enclose(i)
rank( (i)/rank) (i)
select( (i)/select) (i)
rmqi(i, j)/RMQi(i, j)
pre rank (i)/post rank (i)
pre select(i)/post select(i)
isleaf (i)
isancestor (i, j)
depth(i)
parent(i)
ﬁrst child(i)/last child(i)
next sibling(i)/prev sibling(i)
subtree size(i)
level ancestor(i, d)
level next(i)/level prev(i)
level lmost(d)/level rmost(d)
lca(i, j)
deepest node(i)
height(i)
degree(i)
child(i, q)
child rank(i)
in rank (i)
in select(i)
leaf rank (i)
leaf select(i)
lmost leaf (i)/rmost leaf (i)
insert(i, j)
delete(i)

variant 1 | variant 2

time complexity
O(log n/ log log n)
O(log n/ log log n)
O(log n/ log log n)
O(log n/ log log n)
O(log n/ log log n)
O(log n/ log log n)
O(log n/ log log n)
O(log n/ log log n)
O(log n/ log log n)
O(log n/ log log n)
O(log n/ log log n)
O(log n/ log log n)
O(log n/ log log n)
O(log n/ log log n)
O(log n/ log log n)

P [i]
position of parenthesis matching P [i]
position of tightest open parent. enclosing i
number of open/close parentheses in P [0, i]
position of i-th open/close parenthesis
position of min/max excess value in range [i, j]
preorder/postorder rank of node i
the node with preorder/postorder i
whether P [i] is a leaf
whether i is an ancestor of j
depth of node i
parent of node i
ﬁrst/last child of node i
next/previous sibling of node i
number of nodes in the subtree of node i
ancestor j of i s.t. depth(j) = depth(i) − d
next/previous node of i in BFS order
leftmost/rightmost node with depth d
the lowest common ancestor of two nodes i, j
the (ﬁrst) deepest node in the subtree of i
the height of i (distance to its deepest node)
q = number of children of node i
q-th child of node i
q = number of siblings to the left of node i
inorder of node i
node with inorder i
number of leaves to the left of leaf i
i-th leaf
leftmost/rightmost leaf of node i
insert node given by matching parent. at i and j O(log n/ log log n)
O(log n/ log log n)
delete node i

O(log n)
O(log n)
O(log n)

O(log n/ log log n)
O(log n/ log log n)
O(log n/ log log n)

O(q log n/ log log n) O(log n)
O(q log n/ log log n) O(log n)
O(q log n/ log log n) O(log n)

O(log n/ log log n)
O(log n/ log log n)
O(log n/ log log n)
O(log n/ log log n)
O(log n/ log log n)

O(log n)
O(log n)

5

Figure 1: Succinct representations of trees.

2 Preliminaries

Here we describe the balanced parentheses sequence and basic data structures used in this paper.

m

2.1 Succinct data structures for rank/select
Consider a bit string S[0, n− 1] of length n. We deﬁne rank and select for S as follows. rankc(S, i) is
the number of occurrences c ∈ {0, 1} in S[0, i], and selectc(S, i) is the position of the i-th occurrence
of c in S. Note that rankc(S, selectc(S, i)) = i and selectc(S, rankc(S, i)) ≤ i.
There exist many succinct data structures for rank/select [27, 33, 45]. A basic one uses n + o(n)
bits and supports rank/select in constant time on the word RAM with word length O(log n).
The space can be reduced if the number of 1’s is small. For a string with m 1’s, there exists a
data structure for constant-time rank/select using nH0(S) + O(n log log n/ log n), where H0(S) =
m +O(m) is called the empirical zero-order entropy of the sequence.
n log n
The space overhead on top of the entropy has been recently reduced [43] to O(n tt/ logt n + n3/4)
bits, while supporting rank and select in O(t) time. This can be built in linear worst-case time5.
A crucial technique for succinct data structures is table lookup. For small-size problems we
construct a table which stores answers for all possible sequences and queries. For example, for rank
and select, we use a table storing all answers for all 0,1 patterns of length 1
2 log n. Because there
n diﬀerent patterns, we can store all answers in a universal table (i.e., not
exist only 2
n · polylog(n) = o(n/polylog(n)) bits, which can be
depending on the bit sequence) that uses
accessed in constant time on a word RAM with word length Θ(log n).

n−m = m log n

m + n−m

n log n

√

1

2 log n =

√

(cid:80)

1≤c≤σ

nc
n log n
nc

The deﬁnition of rank and select on bitmaps generalizes to arbitrary sequences over an integer
alphabet [1, σ], as well as the deﬁnition of zero-order empirical entropy of sequences, to H0(S) =
, where c occurs nc times in S. A compressed representation of general sequences
that supports rank/select is achieved through a structure called a wavelet tree [23]. This is a
complete binary tree that partitions the alphabet [1, σ] into contiguous halves at each node. The
node then stores a bitmap telling which branch did each letter go. The tree has height (cid:100)log σ(cid:101),
and it reduces rank and select operations to analogous operations on its bitmap in a root-to-leaf
or leaf-to-root traversal. If the bitmaps are represented within their zero-order entropy, the total
space adds up to nH0(S) + o(n log σ) and the operations are supported in O(log σ) time. This can
log log n(cid:101)), while maintaining the same asymptotic space, by using a multiary
be improved to O((cid:100) log σ

5They use a predecessor structure by Pˇatra¸scu and Thorup [42], more precisely their result achieving time

“lg (cid:96)−lg n

a

”, which is a simple modiﬁcation of van Emde Boas’ data structure.

6

26817354Ordinal tree((()()())(()()))1((()((())))(()))234567812345678BPDFUDS11101110110000002345678LOUDS√
wavelet tree of arity Θ(
still can answer rank/select in constant time [13].

log n), and replacing the bitmaps by sequences over small alphabets, which

2.2 Succinct tree representations
A rooted ordered tree T , or ordinal tree, with n nodes is represented by a string P [0, 2n − 1] of
balanced parentheses of length 2n. A node is represented by a pair of matching parentheses ( . . . )
and all subtrees rooted at the node are encoded in order between the matching parentheses (see
Figure 1 for an example). A node v ∈ T is identiﬁed with the position i of the open parenthesis
P [i] representing the node.

There exist many succinct data structures for ordinal trees. Among them, the ones with maximum 
functionality [11] support all the operations in Table 1, except insert and delete, in constant
time using 2n + O(n log log log n/ log log n)-bit space. Our static data structure supports the same
operations and reduces the space to 2n + O(n/polylog(n)) bits.

2.3 Dynamic succinct trees

We consider insertion and deletion of internal nodes or leaves in ordinal trees. In this setting, there
exist no data structures supporting all the operations in Table 1. The data structure of Raman
and Rao [46] supports, for binary trees, parent, left and right child, and subtree size of the current
node in the course of traversing the tree in constant time, and updates in O((log log n)1+) time.
Note that this data structure assumes that all traversals start from the root. Chan et al. [8] gave a
dynamic data structure using O(n) bits and supporting ﬁndopen, ﬁndclose, enclose, and updates,
in O(log n/ log log n) time. They also gave another data structure using O(n) bits and supporting
ﬁndopen, ﬁndclose, enclose, lca, leaf rank, leaf select, and updates, in O(log n) time.

Furthermore, we consider the more sophisticated operation (which is simple on classical trees)
of attaching a new subtree as the new child of a node, instead of just a leaf. The model is that this
new subtree is already represented with our data structures. Both trees are thereafter blended and
become a unique tree. Similarly, we can detach any subtree from a given tree so that it becomes an
independent entity represented with our data structure. This allows for extremely ﬂexible support
of algorithms handling dynamic trees, far away from the limited operations allowed in previous
work. This time we have to consider a maximum possible value for log n (say, w, the width of the
system-wide pointers). Then we require 2n + O(n log w/w +
2w) bits of space and carry out the
queries in time O(w/ log w) or O(w), depending on the tree. Insert or delete takes O(w1+) for any
constant  > 0 if we wish to allow attachment and detachment of subtrees, which then can also be
carried out in time O(w1+).

√

2.4 Dynamic compressed bitmaps and sequences
Let B[0, n − 1] be a bitmap. We want to support operations rank and select on B, as well as
operations insert(B, i, b), which inserts bit b between B[i] and B[i + 1], and delete(B, i), which
deletes position B[i] from B. Chan et al. [8] handle all these operations in O(log n/ log log n) time
(which is optimal [17]) using O(n) bits of space (actually, by reducing the problem to a particular
√
dynamic tree). M¨akinen and Navarro [30] achieve O(log n) time and nH0(B)+O(n log log n/
log n)
√
bits of space. The results can be generalized to sequences. Gonz´alez and Navarro [22] achieve
nH0 + O(n log σ/
log log n )) time to handle all the operations

log n) bits of space and O(log n(1 + log σ

7

on a sequence over alphabet [1, σ]. They give several applications to managing dynamic text
collections, construction of static compressed indexes within compressed space, and construction
of the Burrows-Wheeler transform [7] within compressed space. We improve all these results in
this paper, achieving the optimal O(log n/ log log n) on polylog-sized alphabets and reducing the
lower-order term in the compressed space by a Θ(log log n) factor.

3 Fundamental concepts

In this section we give the basic ideas of our ordinal tree representation. In the next sections we
build on these to deﬁne our static and dynamic representations.
We represent a possibly non-balanced6 parentheses sequence by a 0,1 vector P [0, n − 1] (P [i] ∈
{0, 1}). Each opening/closing parenthesis is encoded by ( = 1, ) = 0.

First, we remind that several operations of Table 1 either are trivial in a BP representation, or

are easily solved using enclose, ﬁndclose, ﬁndopen, rank, and select [34]. These are:

inspect(i) = P [i] (or rank1(P, i) − rank1(P, i − 1) if there is no access to P [i]
isleaf (i) = [P [i + 1] = 0]

isancestor (i, j) = i ≤ j ≤ f indclose(P, i)
depth(i) = rank1(P, i) − rank0(P, i)
parent(i) = enclose(P, i)

pre rank (i) = rank1(P, i)
pre select(i) = select1(P, i)
post rank (i) = rank0(P, i)
post select(i) = select0(P, i)
ﬁrst child(i) = i + 1 (if P [i + 1] = 1, else i is a leaf)
last child(i) = ﬁndopen(P, ﬁndclose(P, i) − 1) (if P [i + 1] = 1, else i is a leaf)

next sibling(i) = f indclose(i) + 1 (if P [f indclose(i) + 1] = 1, else i is the last sibling)
prev sibling(i) = f indopen(i − 1) (if P [i − 1] = 0, else i is the ﬁrst sibling)
subtree size(i) = (f indclose(i) − i + 1)/2

Hence the above operations will not be considered further in the paper. Let us now focus on a
small set of primitives needed to implement most of the other operations. For any function g(·) on
{0, 1}, we deﬁne the following.
Deﬁnition 1 For a 0,1 vector P [0, n − 1] and a function g(·) on {0, 1},

j(cid:88)

sum(P, g, i, j)

def=

g(P [k])

fwd search(P, g, i, d)

bwd search(P, g, i, d)

k=i
def= min
j≥i
def= max
j≤i

{j | sum(P, g, i, j) = d}
{j | sum(P, g, j, i) = d}

6As later we will use these constructions to represent arbitrary segments of a balanced sequence.

8

rmq(P, g, i, j)

def= min
i≤k≤j

rmqi(P, g, i, j)

RMQ(P, g, i, j)

RMQi(P, g, i, j)

{sum(P, g, i, k)}
{sum(P, g, i, k)}
{sum(P, g, i, k)}

def= argmin
i≤k≤j
def= max
i≤k≤j

def= argmax
i≤k≤j

{sum(P, g, i, k)}

The following function is particularly important.

Deﬁnition 2 Let π be the function such that π(1) = 1, π(0) = −1. Given P [0, n − 1], we deﬁne
the excess array E[0, n − 1] of P as an integer array such that E[i] = sum(P, π, 0, i).

Note that E[i] stores the diﬀerence between the number of opening and closing parentheses in
P [0, i]. When P [i] is an opening parenthesis, E[i] = depth(i) is the depth of the corresponding
node, and is the depth minus 1 for closing parentheses. We will use E as a conceptual device in our
discussions, it will not be stored. Note that, given the form of π, it holds that |E[i + 1] − E[i]| = 1
for all i.

The above operations are suﬃcient to implement the basic navigation on parentheses, as the
next lemma shows. Note that the equation for ﬁndclose is well known, and the one for level ancestor
has appeared as well [37], but we give proofs for completeness.
Lemma 2 Let P be a BP sequence encoded by {0, 1}. Then ﬁndclose, ﬁndopen, enclose, and
level ancestor can be expressed as follows.

ﬁndclose(i) = fwd search(P, π, i, 0)

ﬁndopen(i) = bwd search(P, π, i, 0)

enclose(i) = bwd search(P, π, i, 2)

level ancestor(i, d) = bwd search(P, π, i, d + 1)

Proof. For ﬁndclose, let j > i be the position of the closing parenthesis matching the opening
parenthesis at P [i]. Then j is the smallest index > i such that E[j] = E[i] − 1 = E[i − 1] (because
of the node depths). Since by deﬁnition E[k] = E[i − 1] + sum(P, π, i, k) for any k > i, j is the
smallest index > i such that sum(P, π, i, j) = 0. This is, by deﬁnition, fwd search(P, π, i, 0).
For ﬁndopen, let j < i be the position of the opening parenthesis matching the closing parenthesis 
at P [i]. Then j is the largest index < i such that E[j − 1] = E[i] (again, because of the node
depths)7. Since by deﬁnition E[k − 1] = E[i] − sum(P, π, k, i) for any k < i, j is the largest index
< i such that sum(P, π, j, i) = 0. This is bwd search(P, π, i, 0).
For enclose, let j < i be the position of the opening parenthesis that most tightly encloses the
opening parenthesis at P [i]. Then j is the largest index < i such that E[j − 1] = E[i] − 2 (note
that now P [i] is an opening parenthesis). Now we reason as for ﬁndopen to get sum(P, π, j, i) = 2.
Finally, the proof for level ancestor is similar to that for enclose. Now j is the largest index < i
such that E[j − 1] = E[i] − d − 1, which is equivalent to sum(P, π, j, i) = d + 1.
(cid:117)(cid:116)

7Note E[j] − 1 = E[i] could hold at incorrect places, where P [j] is a closing parenthesis.

9

We also have the following, easy or well-known, equalities:

lca(i, j) = max(i, j), if isancestor(i, j) or isancestor(j, i)

parent(rmqi(P, π, i, j) + 1), otherwise [47]

deepest node(i) = RMQi(P, π, i, ﬁndclose(i))

height(i) = depth(deepest node(i)) − depth(i)

level next(i) = fwd search(P, π, ﬁndclose(i), 0)

level prev(i) = ﬁndopen(bwd search(P, π, i, 0))

level lmost(d) = fwd search(P, π, 0, d)
level rmost(d) = ﬁndopen(bwd search(P, π, n − 1,−d))

We also show that the above functions unify the algorithms for computing rank/select on 0,1
vectors and those for balanced parenthesis sequences. Namely, let φ, ψ be functions such that
φ(0) = 0, φ(1) = 1, ψ(0) = 1, ψ(1) = 0. Then the following equalities hold.

Lemma 3 For a 0,1 vector P ,

rank1(P, i) = sum(P, φ, 0, i)
select1(P, i) = fwd search(P, φ, 0, i)
rank0(P, i) = sum(P, ψ, 0, i)
select0(P, i) = fwd search(P, ψ, 0, i)

Therefore,

in principle we must focus only on the following set of primitives:
bwd search, sum, rmqi, RMQi, degree, child, and child rank, for the rest of the paper.

fwd search,

Our data structure for queries on a 0,1 vector P is basically a search tree in which each leaf
corresponds to a range of P , and each node stores the last, maximum, and minimum values of
preﬁx sums for the concatenation of all the ranges up to the subtree rooted at that node.
Deﬁnition 3 A range min-max tree for a vector P [0, n−1] and a function g(·) is deﬁned as follows.
Let [(cid:96)1, r1], [(cid:96)2, r2], . . . , [(cid:96)q, rq] be a partition of [0, n − 1] where (cid:96)1 = 0, ri + 1 = (cid:96)i+1, rq = n − 1.
Then the i-th leftmost leaf of the tree stores the sub-vector P [(cid:96)i, ri], as well as e[i] = sum(P, g, 0, ri),
m[i] = e[i−1]+rmq(P, g, (cid:96)i, ri) and M [i] = e[i−1]+RMQ(P, g, (cid:96)i, ri). Each internal node u stores in
e[u]/m[u]/M [u] the last/minimum/maximum of the e/m/M values stored in its child nodes. Thus,
the root node stores e = sum(P, g, 0, n − 1), m = rmq(P, g, 0, n − 1) and M = RMQ(P, g, 0, n − 1).

Example 1 An example of range min-max tree is shown in Figure 2. Here we use g = π, and thus
the nodes store the minimum/maximum values of array E in the corresponding interval.

4 A simple data structure for polylogarithmic-size trees

Building on the previous ideas, we give a simple data structure to compute fwd search, bwd search,
and sum in constant time for arrays of polylogarithmic size. Then we consider further operations.
Let g(·) be a function on {0, 1} taking values in {1, 0,−1}. We call such a function ±1
function. Note that there exist only six such functions where g(0) (cid:54)= g(1), which are indeed
φ,−φ, ψ,−ψ, π,−π.

10

Figure 2: An example of the range min-max tree using function π, and showing the m/M values.

Let w be the bit length of the machine word in the RAM model, and c ≥ 1 any constant. We
have a (not necessarily balanced) parentheses vector P [0, n − 1], of moderate size n ≤ N = wc.
Assume we wish to solve the operations for an arbitrary ±1 function g(·), and let G[i] denote
sum(P, g, 0, i), analogously to E[i] for g = π.
2 w.
We imaginarily divide vector P into (cid:100)n/s(cid:101) chunks of length s. These form the partition alluded in
Deﬁnition 3: (cid:96)i = s · (i − 1). Thus the values m[i] and M [i] correspond to minima and maxima of
G within each chunk, and e[i] = G[ri].

Our data structure is a range min-max tree TmM for vector P and function g(·). Let s = 1

Furthermore, the tree will be k-ary and complete, for k = Θ(w/(c log w)). Thus the leaves store
all the elements of arrays m and M . Because it is complete, the tree can be represented just by
three integer arrays e(cid:48)[0,O(n/s)], m(cid:48)[0,O(n/s)], and M(cid:48)[0,O(n/s)], like a heap.
1)(cid:101) = O(nc log w/w) bits each. The depth of the tree is (cid:100)logk(n/s)(cid:101) = O(c).

Because −wc ≤ e(cid:48)[i], m(cid:48)[i], M(cid:48)[i] ≤ wc for any i, arrays e(cid:48), m(cid:48) and M(cid:48) occupy k

k−1 · n

s ·(cid:100)log(2wc +

The following fact is well known; we reprove it for completeness.

Lemma 4 Any range [i, j] ⊆ [0, n − 1] in TmM is covered by a disjoint union of O(ck) subranges
where the leftmost and rightmost ones may be subranges of leaves of TmM , and the others correspond
to whole nodes of TmM .
Proof. Let a be the smallest value such that i ≤ ra and b be the largest such that j ≥ (cid:96)b. Then
the range [i, j] is covered by the disjoint union [i, j] = [i, ra][(cid:96)a+1, ra+1] . . . [(cid:96)b, j] (we can discard the
special case a = b, as in this case we have already one leaf covering [i, j]). Then [i, ra] and [(cid:96)b, j]
are the leftmost and rightmost leaf subranges alluded in the lemma; all the others are whole tree
nodes.
It remains to show that we can reexpress this disjoint union using O(ck) tree nodes. If all the
k children of a node are in the range, we replace the k children by the parent node, and continue
recursively level by level. Note that if two parent nodes are created in a given level, then all the
other intermediate nodes of the same level must be created as well, because the original/created
nodes form a range at any level. At the end, there cannot be more than 2k − 2 nodes at any level,
because otherwise k of them would share a single parent and would have been replaced. As there
are c levels, the obtained set of nodes covering [i, j] is of size O(ck).
(cid:117)(cid:116)

11

1212343432321232321210(()((()())())(()())())E1/22/43/42/31/32/31/20/0m/M1/41/30/20/4PabcdefghijklExample 2 In Figure 2 (where s = k = 3), the range [3, 18] is covered by [3, 5], [6, 8], [9, 17], [18, 18].
They correspond to nodes d, e, f , and a part of leaf k, respectively.

Computing fwd search(P, g, i, d) is done as follows (bwd search is symmetric). First we check if
the chunk of i, [(cid:96)k, rk] for k = (cid:98)i/s(cid:99), contains fwd search(P, g, i, d) with a table lookup using vector
√
P , by precomputing a simple universal table of 2s log s = O(
2w log w) bits8 If so, we are done.
Else, we compute the global target value we seek, d(cid:48) = G[i − 1] + d = e[k] − sum(P, g, i, rk) + d
(again, the sum inside the chunk is done in constant time using table lookup). Now we divide the
range [rk + 1, n − 1] into subranges I1, I2, . . . represented by range min-max tree nodes u1, u2, . . .
as in Lemma 4 (note these are simply all the right siblings of my parent, all the right siblings of
my grandparent, and so on). Then, for each Ij, we check if the target value d(cid:48) is between m[uj]
and M [uj], the minimum and maximum values of subrange Ij. Let Ik be the ﬁrst j such that
m[uj] ≤ d(cid:48) ≤ M [uj], then fwd search(P, g, i, d) lies within Ik. If Ik corresponds to an internal tree
node, we iteratively ﬁnd the leftmost child of the node whose range contains d(cid:48), until we reach a
leaf. Finally, we ﬁnd the target in the chunk corresponding to the leaf by table lookups, using P
again.

Example 3 In Figure 2, where G = E and g = π, computing ﬁndclose(3) = fwd search(P, π, 3, 0) =
12 can be done as follows. Note this is equivalent to ﬁnding the ﬁrst j > 3 such that E[i] =
E[3− 1] + 0 = 1. First examine the node (cid:98)3/s(cid:99) = 1 (labeled d in the ﬁgure). We see that the target
1 does not exist within d after position 3. Next we examine node e. Since m[e] = 3 and M [e] = 4, e
does not contain the answer either. Next we examine the node f . Because m[f ] = 1 and M [f ] = 3,
the answer must exist in its subtree. Therefore we scan the children of f from left to right, and
ﬁnd the leftmost one with m[·] ≤ 1, which is node h. Because node h is already a leaf, we scan the
segment corresponding to it, and ﬁnd the answer 12.

The sequence of subranges arising in this search corresponds to a leaf-to-leaf path in the range
min-max tree, and it contains O(ck) ranges according to Lemma 4. We show now how to carry out
this search in time O(c) rather than O(ck).
According to Lemma 4, the O(ck) nodes can be partitioned into O(c) sequences of sibling nodes.
We will manage to carry out the search within each such sequence in O(1) time. Assume we have
to ﬁnd the ﬁrst j ≥ i such that m[uj] ≤ d(cid:48) ≤ M [uj], where u1, u2, . . . , uk are sibling nodes in TmM .
We ﬁrst check if m[ui] ≤ d(cid:48) ≤ M [ui]. If so, the answer is ui. Otherwise, if d(cid:48) < m[ui], the answer
is the ﬁrst j > i such that m[uj] ≤ d(cid:48), and if d(cid:48) > M [ui], the answer is the ﬁrst j > i such that
M [uj] ≥ d(cid:48).
Lemma 5 Let u1, u2, . . . a sequence of TmM nodes containing consecutive intervals of P . If g(·) is
a ±1 function and d < m[u1], then the ﬁrst j such that d ∈ [m[uj], M [uj]] is the ﬁrst j > 1 such
that d ≥ m[uj]. Similarly, if d > M [u1], then it is the ﬁrst j > 1 such that d ≤ M [uj].
Proof. Since g(·) is a ±1 function and the intervals are consecutive, M [uj] ≥ m[uj−1] − 1 and
m[uj] ≤ M [uj−1] + 1. Therefore, if d ≥ m[uj] and d < m[uj−1], then d < M [uj] + 1, thus
d ∈ [m[uj], M [uj]]; and of course d (cid:54)∈ [m[uk], M [uk]] for any k < j as j is the ﬁrst index such that
d ≥ m[uj]. The other case is symmetric.
(cid:117)(cid:116)

8Using integer division and remainder a segment within a chunk can be isolated and padded in constant time.

√
2ww2 log w) bits, which will not change our ﬁnal results.

Otherwise the table is slightly larger, 2ss2 log s = O(

12

Thus the problem is reduced to ﬁnding the ﬁrst j > i such that m[j] ≤ d(cid:48), among (at most) k
sibling nodes (the case M [j] ≥ d(cid:48) is symmetric). We build a universal table with all the possible
sequences of k values m[·] and all possible −wc ≤ d(cid:48) ≤ wc values, and for each such sequence and
d(cid:48) we store the ﬁrst j in the sequence such that m[j] ≤ d(cid:48) (or we store a mark telling that there is
no such position in the sequence). Thus the table has (2wc + 1)k+1 entries, and log(k + 1) bits per
entry. By choosing the constant of k = Θ(w/(c log w)) so that k ≤
2 log(2wc+1) − 1, the total space
√
is O(
2w log w) (and the arguments for the table ﬁt in a machine word). With the table, each
search for the ﬁrst node in a sequence of siblings can be done in constant rather than O(k) time,
and hence the overall time is O(c) rather than O(ck). Note that we store the m(cid:48)[·] values in heap
order, and therefore the k sibling values to input to the table are stored in contiguous memory,
thus they can be accessed in constant time. We use an analogous universal table for M [·].
Finally, the process to solve sum(P, g, i, j) in O(c) time is simple. We descend in the tree up
to the leaf [(cid:96)k, rk] containing j. We obtain sum(P, g, 0, (cid:96)k − 1) = e[k − 1] and compute the rest,
sum(P, g, (cid:96)k, j), in constant time using a universal table we have already introduced. We repeat
the process for sum(P, g, 0, i − 1) and then subtract both results.

w

We have proved the following lemma.

Lemma 6 In the RAM model with w-bit word size, for any constant c ≥ 1 and a 0,1 vector P of
length n < wc, and a ±1 function g(·), fwd search(P, g, i, j), bwd search(P, g, i, j), and sum(P, g, i, j)
can be computed in O(c) time using the range min-max tree and universal lookup tables that require
√
O(

2w log w) bits.

4.1 Supporting range minimum queries

Next we consider how to compute rmqi(P, g, i, j) and RMQi(P, g, i, j).
Lemma 7 In the RAM model with w-bit word size, for any constant c ≥ 1 and a 0,1 vector P of
length n < wc, and a ±1 function g(·), rmqi(P, g, i, j) and RMQi(P, g, i, j) can be computed in O(c)
√
time using the range min-max tree and universal lookup tables that require O(

2w log w) bits.

Proof. Because the algorithm for RMQi is analogous to that for rmqi, we consider only the latter.
From Lemma 4, the range [i, j] is covered by a disjoint union of O(ck) subranges, each corresponding
to some node of the range min-max tree. Let µ1, µ2, . . . be the minimum values of the subranges.
Then the minimum value in [i, j] is the minimum of them. The minimum values in each subrange
are stored in array m(cid:48), except for at most two subranges corresponding to leaves of the range
√
min-max tree. The minimum values of such leaf subranges are found by table lookups using P ,
by precomputing a universal table of O(
2w log w) bits. The minimum value of a subsequence
√
µ(cid:96), . . . , µr which shares the same parent in the range min-max tree can be also found by table
lookups. The size of such universal table is O((2wc + 1)kk log k) = O(
2w) bits (the k factor is
to account for queries that span less than k values, so we can specify the query length). Hence we
ﬁnd the node containing the minimum value µ among µ1, µ2, . . ., in O(c) time. If there is a tie, we
choose the leftmost one.

If µ corresponds to an internal node of the range min-max tree, we traverse the tree from the
node to a leaf having the leftmost minimum value. At each step, we ﬁnd the leftmost child of the
current node having the minimum, in constant time using our precomputed table. We repeat the
process from the resulting child, until reaching a leaf. Finally, we ﬁnd the index of the minimum

13

value in the leaf, in constant time by a lookup on our universal table for leaves. The overall time
(cid:117)(cid:116)
complexity is O(c).

4.2 Other operations

The previous development on fwd search, bwd search, rmqi, and RMQi, has been general, for any
g(·). Applied to g = π, they solve a large number of operations, as shown in Section 3. For the
remaining ones we focus directly on the case g = π.
It is obvious how to compute degree(i), child(i, q) and child rank(i) in time proportional to the
degree of the node. To compute them in constant time, we add another array n(cid:48)[·] to the data
structure. In the range min-max tree, each node stores the minimum value of a subrange for the
node. In addition to this, we store in n(cid:48)[·] the number of the minimum values of each subrange in
the tree.

Lemma 8 The number of children of node i is equal to the number of occurrences of the minimum
value in E[i + 1, ﬁndclose(i) − 1].
Proof. Let d = E[i] = depth(i) and j = ﬁndclose(i). Then E[j] = d − 1 and all excess values
in E[i + 1, j − 1] are ≥ d. Therefore the minimum value in E[i + 1, j − 1] is d. Moreover, for
the range [ik, jk] corresponding to the k-th child of i, E[ik] = d + 1, E[jk] = d, and all the values
between them are > d. Therefore the number of occurrences of d, which is the minimum value in
E[i + 1, j − 1], is equal to the number of children of i.
(cid:117)(cid:116)

√

Now we can compute degree(i) in constant time. Let d = depth(i) and j = ﬁndclose(i). We
partition the range E[i + 1, j − 1] into O(ck) subranges, each of which corresponds to a node of the
range min-max tree. Then for each subrange whose minimum value is d, we sum up the number
of occurrences of the minimum value (n(cid:48)[·]). The number of occurrences of the minimum value in
leaf subranges can be computed by table lookup on P , with a universal table using O(
2w log w)
bits. The time complexity is O(c) if we use universal tables that let us process sequences of (up to)
k children at once, that is, telling the minimum m[·] value within the sequence and the number of
times it appears. This table requires O((2wc + 1)kk log k) = O(
Operation child rank(i) can be computed similarly, by counting the number of minima in
E[parent(i), i − 1]. Operation child(i, q) follows the same idea of degree(i), except that, in the
node where the sum of n(cid:48)[·] exceeds q, we must descend until the range min-max leaf that contains
the opening parenthesis of the q-th child. This search is also guided by the n(cid:48)[·] values of each
node, and is done also in O(c) time. Here we need another universal table that tells at which
position the number of occurrences of the minimum value exceeds some threshold, which requires
O((2wc + 1)k(2wc + 1) log k) = O(
For operations leaf rank , leaf select, lmost leaf and rmost leaf , we deﬁne a bit-vector P1[0, n−
1] such that P1[i] = 1 ⇐⇒ P [i] = 1 ∧ P [i + 1] = 0. Then leaf rank (i) = rank1(P1, i)
and leaf select(i) = select1(P1, i) hold. The other operations are computed by lmost leaf (i) =
select1(P1, rank1(P1, i − 1) + 1) and rmost leaf (i) = select1(P1, rank1(P1, ﬁndclose(i))).

2w log w) bits.

√

2w) bits.

√

We recall the deﬁnition of inorder of nodes, which is essential for compressed suﬃx trees.

Deﬁnition 4 ([48]) The inorder rank of an internal node v is deﬁned as the number of visited
internal nodes, including v, in a left-to-right depth-ﬁrst traversal, when v is visited from a child of
it and another child of it will be visited next.

14

Note that an internal node with q children has q − 1 inorders, so leaves and unary nodes have
no inorder. We deﬁne in rank (i) as the smallest inorder value of internal node i.
To compute in rank and in select, we use another bit-vector P2[0, n − 1] such that P2[i] =
1 ⇐⇒ P [i] = 0 ∧ P [i + 1] = 1. The following lemma gives an algorithm to compute the inorder of
an internal node.

Lemma 9 ([48]) Let i be an internal node, and let j = in rank (i), so i = in select(j). Then

in rank (i) = rank1(P2, ﬁndclose(P, i + 1))
in select(j) = enclose(P, select1(P2, j) + 1)

Note that in select(j) will return the same node i for any its degree(i) − 1 inorder values.

Note that we need not to store P1 and P2 explicitly; they can be computed from P when needed.
We only need the extra data structures for constant-time rank and select, which can be reduced to
the corresponding sum and fwd search operations on the virtual P1 and P2 vectors.

4.3 Reducing extra space
Apart from vector P [0, n−1], we need to store vectors e(cid:48), m(cid:48), M(cid:48), and n(cid:48). In addition, to implement
rank and select using sum and fwd search, we would need to store vectors e(cid:48)
ψ, M(cid:48)
φ,
and M(cid:48)
ψ which maintain the corresponding values for functions φ and ψ. However, note that
sum(P, φ, 0, i) and sum(P, ψ, 0, i) are nondecreasing, thus the minimum/maximum within the chunk
is just the value of the sum at the beginning/end of the chunk. Moreover, as sum(P, π, 0, i) =
sum(P, φ, 0, i) − sum(P, ψ, 0, i) and sum(P, φ, 0, i) + sum(P, ψ, 0, i) = i, it turns out that both
eφ[i] = (ri + e[i])/2 and eψ[i] = (ri − e[i])/2 are redundant. Analogous formulas hold for internal
nodes. Moreover, any sequence of k consecutive such values can be obtained, via table lookup, from
the sequence of k consecutive values of e[·], because the ri values increase regularly at any node.
Hence we do not store any extra information to support φ and ψ.
If we store vectors e(cid:48), m(cid:48), M(cid:48), and n(cid:48) naively, we require O(nc log w/w) bits of extra space on

ψ, m(cid:48)

φ, m(cid:48)

φ, e(cid:48)

top of the n bits for P .
The space can be largely reduced by using a recent technique by Pˇatra¸scu [43]. They deﬁne
an aB-tree over an array A[0, n − 1], for n a power of B, as a complete tree of arity B, storing B
consecutive elements of A in each leaf. Additionally, a value ϕ ∈ Φ is stored at each node. This
must be a function of the corresponding elements of A for the leaves, and a function of the ϕ values
of the children and of the subtree size, for internal nodes. The construction is able to decode the
B values of ϕ for the children of any node in constant time, and to decode the B values of A for
the leaves in constant time, if they can be packed in a machine word.

In our case, A = P is the vector, B = k = s is our arity, and our trees will be of size
N = Bc, which is slightly smaller than the wc we have been assuming. Our values are tuples
ϕ ∈ (cid:104)−Bc,−Bc, 0,−Bc(cid:105) . . .(cid:104)Bc, Bc, Bc, Bc(cid:105) encoding the m, M , n, and e values at the nodes,
respectively. We give next their result, adapted to our case.
Lemma 10 (adapted from Thm. 8 in [43]) Let |Φ| = (2B + 1)4c, and B be such that (B +
1) log(2B + 1) ≤ w
c log w )). An aB-tree of size N = Bc with values in Φ can be
stored using N + 2 bits, plus universal lookup tables of O(
2w) bits. It can obtain the m, M , n or
√
e values of the children of any node, and descend to any of those children, in constant time. The
structure can be built in O(N + w3/2) time, plus O(

2wpoly(w)) for the universal tables.

8c (thus B = Θ( w

√

15

The “+w3/2” construction time comes from a fusion tree [18] that is used internally on O(w)
values. It could be reduced to w time for any constant  > 0 and navigation time O(1/), but we
prefer to set c > 3/2 so that N = Bc dominates it.
These parameters still allow us to represent our range min-max trees while yielding the complexities 
we had found, as k = Θ(w/(c log w)) and N ≤ wc. Our accesses to the range min-max
tree are either (i) partitioning intervals [i, j] into O(ck) subranges, which are easily identiﬁed by
navigating from the root in O(c) time (as the k children are obtained together in constant time);
or (ii) navigating from the root while looking for some leaf based on the intermediate m, M , n, or
e values. Thus we retain all of our time complexities.

√
The space, instead, is reduced to N +2+O(

2w), where the latter part comes from our universal
tables and those of Lemma 10 (our universal tables become smaller with the reduction from w and
s to B). Note that our vector P must be exactly of length N ; padding is necessary otherwise. Both
the padding and the universal tables will lose relevance for larger trees, as seen in the next section.

The next theorem summarizes our results in this section.

Theorem 4 On a w-bit word RAM, for any constant c > 3/2, we can represent a sequence P of
N = Bc parentheses, for suﬃciently small B = Θ( w
c log w ), computing all operations of Table 1 in
O(c) time, with a data structure depending on P that uses N + 2 bits, and universal tables (i.e.,
√
not depending on P ) that use O(
2wpoly(w)) (the
latter being needed only once for universal tables) and its working space is O(N ) bits.

2w) bits. The preprocessing time is O(N +

√

In case we need to solve the operations that build on P1 and P2, we need to represent their
corresponding φ functions (as ψ is redundant). This can still be done with Lemma 10 using
Φ = (2B + 1)6c and (B + 1) log(2B + 1) ≤ w

12c . Theorem 4 applies verbatim.

5 A data structure for large trees

w logk n) = O(

In practice, one can use the solution of the previous section for trees of any size, achieving
O( k log n
log w−log log n ) = O(log n) time (using k = w/ log n) for all operations with
an extremely simple and elegant data structure (especially if we choose to store arrays m(cid:48), etc. in
simple form). In this section we show how to achieve constant time on trees of arbitrary size.

log n

For simplicity, let us assume in this section that we handle trees of size wc in Section 4. We

comment at the end the diﬀerence with the actual size Bc handled.

For large trees with n > wc nodes, we divide the parentheses sequence into blocks of length wc.
Each block (containing a possibly non-balanced sequence of parentheses) is handled with the range
min-max tree of Section 4.
Let m1, m2, . . . , mτ ; M1, M2, . . . , Mτ ; and e1, e2, . . . , eτ ; be the minima, maxima, and excess of
the τ = (cid:100)2n/wc(cid:101) blocks, respectively. These values are stored at the root nodes of each TmM tree
and can be obtained in constant time.

5.1 Forward and backward searches on π

We consider extending fwd search(P, π, i, d) and bwd search(P, π, i, d) to trees of arbitrary size. We
focus on fwd search, as bwd search is symmetric.
We ﬁrst try to solve fwd search(P, π, i, d) within the block j = (cid:98)i/wc(cid:99) of i. If the answer is within
block j, we are done. Otherwise, we must look for the ﬁrst excess d(cid:48) = ej−1 + sum(P, π, 0, i − 1 −

16

Figure 3: A tree representing the lrm(j) sequences of values m1 . . . m9.

wc · (j − 1)) + d in the following blocks (where the sum is local to block j). Then the answer
lies in the ﬁrst block r > j such that mr ≤ d(cid:48) ≤ Mr. Thus, we can apply again Lemma 5,
(cid:54)∈ [mj+1, Mj+1], we must either ﬁnd the ﬁrst r > j + 1 such that
starting at [mj+1, Mj+1]: If d(cid:48)
mr ≤ j, or such that Mr ≥ j. Once we ﬁnd such block, we complete the operation with a local
fwd search(P, π, 0, d(cid:48) − er−1) query inside it.

The problem is how to achieve constant-time search, for any j, in a sequence of length τ . Let

us focus on left-to-right minima, as the others are similar.
Deﬁnition 5 Let m1, m2, . . . , mτ be a sequence of integers. We deﬁne for each 1 ≤ j ≤ τ the
left-to-right minima starting at j as lrm(j) = (cid:104)j0, j1, j2, . . .(cid:105), where j0 = j, jr < jr+1, mjr+1 < mjr ,
and mjr+1 . . . mjr+1−1 ≥ mjr .

The following lemmas are immediate.

Lemma 11 The ﬁrst element ≤ x after position j in a sequence of integers m1, m2, . . . , mτ is mjr
for some r > 0, where jr ∈ lrm(j).
Lemma 12 Let lrm(j)[pj] = lrm(j(cid:48))[pj(cid:48)]. Then lrm(j)[pj + i] = lrm(j(cid:48))[pj(cid:48) + i] for all i > 0.

That is, once the lrm sequences starting at two positions coincide in a position, they coincide
thereafter. Lemma 12 is essential to store all the τ sequences lrm(j) for each block j, in compact
form. We form a tree Tlrm, which is essentially a trie composed of the reversed lrm(j) sequences.
The tree has τ nodes, one per block. Block j is a child of block j1 = lrm(j)[1] (note lrm(j)[0] =
j0 = j), that is, j is a child of the ﬁrst block j1 > j such that mj1 < mj. Thus each j-to-root path
spells out lrm(j), by Lemma 12. We add a ﬁctitious root to convert the forest into a tree. Note
this structure is called 2d-Min-Heap by Fischer [14], who shows how to build it in linear time.
Example 4 Figure 3 illustrates the tree built from the sequence (cid:104)m1 . . . m9(cid:105) = (cid:104)6, 4, 9, 7, 4, 4, 1, 8, 5(cid:105).
Then lrm(1) = (cid:104)1, 2, 7(cid:105), lrm(2) = (cid:104)2, 7(cid:105), lrm(3) = (cid:104)3, 4, 5, 7(cid:105), and so on.

If we now assign weight mj − mj1 to the edge between j and its parent j1, the original problem
of ﬁnding the ﬁrst jr > j such that mjr ≤ d(cid:48) reduces to ﬁnding the ﬁrst ancestor jr of node j such
that the sum of the weights between j and jr exceeds d(cid:48)(cid:48) = mj − d(cid:48). Thus we need to compute
weighted level ancestors in Tlrm. Note that the weight of an edge in Tlrm is at most wc.

17

512345678997441846logt(τ W )

+ (τ W )3/4) bits.

Lemma 13 For a tree with τ nodes where each edge has an integer weight in [1, W ], after O(τ log1+ τ )
time preprocessing, a weighted level-ancestor query is solved in O(t + 1/) time on a Ω(log(τ W ))-bit
word RAM. The size of the data structure is O(τ log τ log(τ W ) + τ W tt
Proof. We use a variant of Bender and Farach’s (cid:104)O(τ log τ ),O(1)(cid:105) algorithm [5]. Let us ignore
weights for a while. We extract a longest root-to-leaf path of the tree, which disconnects the tree
into several subtrees. Then we repeat the process recursively for each subtree, until we have a set
of paths. Each such path, say of length (cid:96), is extended upwards, adding other (cid:96) nodes towards the
root (or less if the root is reached). The extended path is called a ladder, and its is stored as an
array so that level-ancestor queries within a ladder are trivial. This partitioning guarantees that
a node of height h has also height h in its path, and thus at least its ﬁrst h ancestors are in its
ladder. Moreover the union of all ladders has at most 2τ nodes and thus requires O(τ log τ ) bits.
For each tree node v, an array of its (at most) log τ ancestors at depths depth(v) − 2i, i ≥ 0, is
stored (hence the O(τ log τ )-words space and time). To solve the query level ancestor(v, d), where
d(cid:48) = depth(v) − d, the ancestor v(cid:48) at distance d(cid:48)(cid:48) = 2(cid:98)log d(cid:48)(cid:99) from v is computed. Since v(cid:48) has height
at least d(cid:48)(cid:48), it has at least its ﬁrst d(cid:48)(cid:48) ancestors in its ladder. But from v(cid:48) we need only the ancestor
at distance d(cid:48) − d(cid:48)(cid:48) < d(cid:48)(cid:48), so the answer is in the ladder.
To include the weights, we must be able to ﬁnd the node v(cid:48) and the answer considering the
weights, instead of the number of nodes. We store for each ladder of length (cid:96) a sparse bitmap of
length at most (cid:96)W , where the i-th 1 left-to-right represents the i-th node upwards in the ladder, and
the distance between two 1s, the weight of the edge between them. All the bitmaps are concatenated
into one (so each ladder is represented by a couple of integers indicating the extremes of its bitmap).
This long bitmap contains at most 2τ 1s, and because weights do not exceed W , at most 2τ W 0s.
Using Pˇatra¸scu’s sparse bitmaps [43], it can be represented using O(τ log W + τ W tt
+ (τ W )3/4)
bits and do rank/select in O(t) time.
In addition, we store for each node the log τ accumulated weights towards ancestors at distances
2i, using fusion trees [18]. These can store z keys of (cid:96) bits in O(z(cid:96)) bits and, using O(z5/6(z1/6)4) =
O(z1.5) preprocessing time, answer predecessor queries in O(log(cid:96) z) time (via an (cid:96)1/6-ary tree). The
1/6 can be reduced to achieve O(z1+) preprocessing time and O(1/) query time for any desired
constant 0 <  ≤ 1/2.
In our case this means O(τ log τ log(τ W )) bits of space, O(τ log1+ τ ) construction time, and
O(1/) access time. Thus we can ﬁnd in constant time, from each node v, the corresponding
weighted ancestor v(cid:48) using a predecessor query. If this corresponds to (unweighted) distance 2i,
then the true ancestor is at distance < 2i+1, and thus it is within the ladder of v(cid:48), where it is
found using rank/select on the bitmap of ladders (each node v has a pointer to its 1 in the ladder
(cid:117)(cid:116)
corresponding to the path it belongs to).
To apply this lemma for our problem of computing fwd search outside blocks, we have W = wc
+ n3/4). By choosing

and τ = n
 = min(1/2, 1/c), the query time is O(c + t) and the preprocessing time is O(n) for c > 3/2.

logt(τ W )

wc . Then the size of the data structure becomes O( n log2 n

wc + n tt
logt n

5.2 Other operations

For computing rmqi and RMQi, we use a simple data structure [4] on the mr and Mr values, later
improved to require only O(τ ) bits on top of the sequence of values [47, 15]. The extra space is
thus O(n/wc) bits, and it solves any query up to the block granularity. For solving a general query

18

[i, j] we should compare the minimum/maximum obtained with the result of running queries rmqi
and RMQi within the blocks at the two extremes of the boundary [i, j].

We consider all pairs (i, j) of matching parentheses (j = ﬁndclose(i)) such that i and j belong
to diﬀerent blocks. If we deﬁne a graph whose vertices are blocks and the edges are the pairs of
parentheses considered, the graph is outer-planar since the parenthesis pairs nest [27], yet there are
multiple edges among nodes. To remove these, we choose the tightest pair of parentheses for each
pair of vertices. These parentheses are called pioneers. Since they correspond to edges of a planar
graph, the number of pioneers is O(n/wc).

For computing child, child rank, and degree, it is enough to consider only nodes which completely
include a block (otherwise the query is solved in constant time by considering just two adjacent
blocks; we can easily identify such nodes using ﬁndclose). Furthermore, among them, it is enough
to consider pioneers: Assume (i, i(cid:48)) contains a whole block but is not a pioneer pair of parentheses.
Then there exists a pioneer pair (j, j(cid:48)) contained in (i, i(cid:48)) where j is in the same block of i and j(cid:48)
is in the same block of i(cid:48). Thus the block contains no children of (i, i(cid:48)) as all descend from (j, j(cid:48)).
Moreover, all the children of (i, i(cid:48)) start either in the block of i or in the block of i(cid:48), since (j, j(cid:48)) or an
ancestor of it is a child of (i, i(cid:48)). So again the operations are solved in constant time by considering
two blocks. Such cases can be identiﬁed by doing ﬁndclose on the last child of i starting in its block
and seeing if that child closes in the block of i(cid:48).
Let us call marked the nodes to consider (that is, pioneers that contain a whole block). There
are O(n/wc) marked nodes, thus for degree we can simply store the degrees of marked nodes using
O( n log n
For child and child rank, we set up a bitmap C[0, 2n − 1] where marked nodes v are indicated
with C[v] = 1, and preprocess C for rank queries so that satellite information can be associated
to marked nodes. Using again Pˇatra¸scu’s result [43], vector C can be represented in at most
+ n3/4) bits, so that access and operation rank can be computed in O(t) time.
2n
We will focus on children of marked nodes placed at the blocks fully contained in the nodes, as
the others are in at most the two extreme blocks and can be dealt with in constant time. Note a
block is fully contained in at most one marked node.

) bits of space, and the others are computed in constant time as explained.

wc

wc log(wc) +O( n tt

logt n

For each marked node v we store a list formed by the blocks fully contained in v, and the
marked nodes children of v, in left-to-right order of P . The blocks store the number of children of
v that start within them, and the children marked nodes store simply a 1 (indicating they contain
1 child of v). All also store their position inside the list. The length of all the sequences adds up
to O(n/wc) because each block and marked node appears in at most one list. Their total sum of
children is at most n, for the same reason. Thus, it is easy to store all the number of children as
gaps between consecutive 1s in a bitmap, which can be stored within the same space bounds of the
other bitmaps in this section (O(n) bits, O(n/wc) 1s).

Using this bitmap, child and child rank can easily be solved using rank and select. For child(v, q)
on a marked node v we start using p = rank1(Cv, select0(Cv, q)) on the bitmap Cv of v. This tells
the position in the list of blocks and marked nodes of v where the q-th child of v lies. If it is a
marked node, then that node is the child. If instead it is a block v(cid:48), then the answer corresponds
to the q(cid:48)-th minimum within that block, where q(cid:48) = q − rank0(select1(Cv, p)). (Recall that we ﬁrst
have to see if child(v, q) lies in the block of v or in that of ﬁndclose(v), using a within-block query
in those cases, and otherwise subtracting from q the children that start in the block of v.)

For child rank(u), we can directly store the answers for marked blocks u. Else, it might be
that v = parent(u) starts in the same block of u or that ﬁndclose(v) is in the same block of

19

ﬁndclose(u), in which case we solve child rank(u) with an in-block query and the help of degree(v).
Otherwise, the block where u belongs must be in the list of v, say at position pu. Then the answer
is rank0(Cv, select1(Cv, pu)) plus the number of minima in the block of u until u − 1.

Finally, the remaining operations require just rank and select on P , or the virtual bit vectors
P1 and P2. For rank it is enough to store the answers at the end of blocks, and ﬁnish the query
within a single block. For select1(P, i) (and similarly for select0 and for P1 and P2), we make up a
sequence with the accumulated number of 1s in each of the τ blocks. The numbers add up to O(n)
and thus can be represented as gaps of 0s between consecutive 1s in a bitmap S[0,O(n)], which can
be stored within the previous space bounds. Computing x = rank1(S, select0(S, i)), in time O(t),
lets us know we must ﬁnish the query in block x, using its range min-max tree with the local value
i(cid:48) = select0(S, i) − select1(S, x).

Bc

+ n tt
logt n

).

√

c log w ). The sum
2w)

5.3 The ﬁnal result
Recall from Theorem 4 that we actually use blocks of size Bc, not wc, for B = O( w
of the space for all the block is 2n + O(n/Bc), plus shared universal tables that add up to O(
bits. Padding the last block to size exactly Bc adds up another negligible extra space.
On the other hand, in this section we have extended the results to larger trees of n nodes,
adding time O(t) to the operations. By properly adjusting w to B in these results, the overall extra
√
space added is O( n(c log B+log2 n)
2w + n3/4) bits. Using a computer word of w = log n
+
bits, setting t = c, and expanding B = O(
c log log n ), we get that the time for any operation is O(c)
log n
and the total space simpliﬁes to 2n + O( n(c log log n)c
logc−2 n
Construction time is O(n). We now analyze the working space for constructing the data structure.
 We ﬁrst convert the input balanced parentheses sequence P into a set of aB-trees, each of
which represents a part of the input of length Bc. The working space is O(Bc) from Theorem 4.
Next we compute marked nodes: We scan P from left to right, and if P [i] is an opening parenthesis,
we push i in a stack, and if it is closing, we pop an entry from the stack. At this point it is very
easy to spot marked nodes. Because P is nested, the values in the stack are monotone. Therefore
we can store a new value as the diﬀerence from the previous one using unary code. Thus the values
in the stack can be stored in O(n) bits. Encoding and decoding the stack values takes O(n) time
in total. Once the marked nodes are identiﬁed, Pˇatra¸scu’s compressed representation [43] of bit
vector C is built in O(n) space too, as it also cuts the bitmap into polylog-sized aB-trees and then
computes some directories over just O(n/polylog(n)) values.
The remaining data structures, such as the lrm sequences and tree, the lists of the marked
nodes, and the Cv bitmaps, are all built on O(n/Bc) elements, thus they need at most O(n) bits
of space for construction.
By rewriting c − 2 − δ as c, for any constant δ > 0, we get our main result on static ordinal

trees, Theorem 1.

6 A simple data structure for dynamic trees

In this section we give a simple data structure for dynamic ordinal trees. In addition to the previous
query operations, we add now insertion and deletion of internal nodes and leaves.

20

6.1 Memory management
We store a 0,1 vector P [0, 2n − 1] using a dynamic min-max tree. Each leaf of the min-max tree
stores a segment of P in verbatim form. The length (cid:96) of each segment is restricted to L ≤ (cid:96) ≤ 2L
for some parameter L > 0.

If insertions or deletions occur, the length of a segment will change. We use a standard technique
for dynamic maintenance of memory cells [32]. We regard the memory as an array of cells of length
2L each, hence allocation is easily handled in constant time. We use L + 1 linked lists sL, . . . , s2L
where si stores all the segments of length i. All the segments with equal length i are packed
consecutively, without wasting any extra space in the cells of linked list si (except possibly at the
head cell of each list). Therefore a cell (of length 2L) stores (parts of) at most three segments,
and a segment spans at most two cells. Tree leaves store pointers to the cell and oﬀset where its
segment is stored. If the length of a segment changes from i to j, it is moved from si to sj. The
space generated by the removal is ﬁlled with the head segment in si, and the removed segment is
stored at the head of sj.
With this scheme, scanning any segment takes O(L/ log n) time, by processing it by chunks of
Θ(log n) bits. This is also the time to compute operations fwd search, bwd search, rmqi, etc. on the
segment, using universal tables. Migrating a node to another list is also done in O(L/ log n) time.
If a migration of a segment occurs, pointers to the segment from a leaf of the tree must change.
For this sake we store back-pointers from each segment to its leaf. Each cell stores also a pointer
to the next cell of its list. Finally, an array of pointers for the heads of sL, . . . , s2L is necessary.
Overall, the space for storing a 0,1 vector of length 2n is 2n + O( n log n

L ) bits.

The rest of the dynamic tree will use sublinear space, and thus we allocate ﬁxed-size memory

cells for the internal nodes, as they will waste at most a constant fraction of the allocated space.

6.2 A dynamic tree

We give a simple dynamic data structure representing an ordinal tree with n nodes using 2n +
O(n/ log n) bits, and supporting all query and update operations in O(log n) worst-case time.
We divide the 0,1 vector P [0, 2n−1] into segments of length from L to 2L, for L = log2 n. We use
a balanced binary tree for representing the range min-max tree. If a node of the tree corresponds
to a vector P [i, j], the node stores i and j, as well as e = sum(P, π, i, j), m = rmq(P, π, i, j),
M = RMQ(P, π, i, j), and n, the number of minimum values in P [i, j] regarding π. (Data on φ for
the virtual vectors P1 and P2 is handled analogously.)
It is clear that fwd search, bwd search, rmqi, RMQi, rank, select, degree, child and child rank
can be computed in O(log n) time, by using the same algorithms developed for small trees in
Section 4. These operations cover all the functionality of Table 1. Note the values we store are
local to the subtree (so that they are easy to update), but global values are easily derived in a
top-down traversal. For example, to solve fwd search(P, π, i, d) starting at the min-max tree root v
with children vl and vr, we ﬁrst see if j(vl) ≥ i, in which case try ﬁrst on vl. If the answer is not
there or j(vl) < i, we try on vr, changing d to d − e(vl). This will only traverse O(log n) nodes,
as seen in Section 4. As another example, to compute depth(i) from v we ﬁrst see if j(vl) ≥ i, in
which case we continue at vl, otherwise we continue at vr and add e(vl) to that result.
Because each node uses O(log n) bits, and the number of nodes is O(n/L), the total space
is 2n + O(n/ log n) bits. This includes the extra O( n log n
L ) term for the leaf data. Note that
we need to maintain several universal tables that handle chunks of 1
2 log n bits. These require

21

√
O(

n · polylog(n)) extra bits, which is negligible.
If insertion/deletion occurs, we update a segment, and the stored values in the leaf for the

segment. From the leaf we step back to the root, updating the values as follows:

i(v), j(v) = i(vl), j(vr)

e(v) = e(vl) + e(vr)
m(v) = min(m(vl), e(vl) + m(vr))
M (v) = max(M (vl), e(vl) + M (vr))
n(v) = n(vl) if m(vl) < e(vl) + m(vr),
n(vr) if m(vl) > e(vl) + m(vr),
n(vl) + n(vr) otherwise.

If the length of the segment exceeds 2L, we split it into two and add a new node. If, instead,
the length becomes shorter than L, we ﬁnd the adjacent segment to the right. If its length is L,
we concatenate them; otherwise move the leftmost bit of the right segment to the left one. In this
manner we can keep the invariant that all segments have length L to 2L. Then we update all the
values in the ancestors of the modiﬁed leaves, as explained. If a balancing operation occurs, we also
update the values in nodes. All these updates are carried out in constant time per involved node,
as their values are recomputed using the formulas above. Thus the update time is also O(log n).
When (cid:100)log n(cid:101) changes, we must update the allowed values for L, recompute universal tables,
change the width of the stored values, etc. M¨akinen and Navarro [30] have shown how to do this for
a very similar case (dynamic rank/select on a bitmap). Their solution of splitting the bitmap into
three parts and moving border bits across parts to deamortize the work applies verbatim to our
sequence of parentheses, thus we can handle changes in (cid:100)log n(cid:101) without altering the space nor the
time complexity (except for O(w) extra bits in the space due to a constant number of system-wide
pointers, a technicism we ignore). We have one range min-max tree for each of the three parts and
adapt all the algorithms in the obvious manner9.

√

L

√
√

√

log n ≤ k ≤ 2

7 A faster dynamic data structure

Instead of the balanced binary tree, we use a B-tree with branching factor Θ(
log n), as in previous
work [8]. Then the depth of the tree is O(log n/ log log n). The lengths of segments is L to 2L
for L = log2 n/ log log n. The required space for the range min-max tree and the vector is now
2n +O(n log log n/ log n) bits (the internal nodes use O(log3/2 n) bits but there are only O(
)
of them). Now each leaf can be processed in time O(log n/ log log n).

n
log n

Each internal node v of the range min-max tree has k children, for

log n
(we relax the constants later). Let c1, c2, . . . , ck be the children of v, and [(cid:96)1, r1], . . . , [(cid:96)k, rk] be
their corresponding subranges. We store (i) the children boundaries (cid:96)i, (ii) sφ[1, k] and sψ[1, k]
storing sφ/ψ[i] = sum(P, φ/ψ, (cid:96)1, ri), (iii) e[1, k] storing e[i] = sum(P, π, (cid:96)1, ri), (iv) m[1, k] storing
m[i] = e[i − 1] + rmq(P, π, (cid:96)i, ri), M [1, k] storing M [i] = e[i − 1] + RMQ(P, π, (cid:96)i, ri), and (v) n[1, k]
storing in n[i] the number of times the minimum excess within the i-th child occurs within its

9One can act as if one had a single range min-max tree where the ﬁrst two levels were used to split the three parts
(these ﬁrst nodes would be special in the sense that their handling of insertions/deletions would reﬂect the actions
on moving bits between the three parts).

22

subtree. Note that the values stored are local to the subtree (as in the simpler balanced binary
tree version, Section 6) but cumulative with respect to previous siblings. Note also that storing
sφ, sψ and e is redundant, as noted in Section 4.3, but we need sφ/ψ in explicit form to achieve
constant-time searching into their values, as it will be clear soon.

Apart from simple accesses to the stored values, we need to support the following operations

within any node:

• p(i): the largest j such that (cid:96)j−1 ≤ i (or j = 1).
• wφ/ψ(i): the largest j such that sφ/ψ[j − 1] ≤ i (or j = 1).
• f (i, d): the smallest j ≥ i such that m[j] ≤ d ≤ M [j].
• b(i, d): the largest j ≤ i such that m[j] ≤ d ≤ M [j].
• r(i, j): the smallest x such that m[x] is minimum in m[i, j].
• R(i, j): the smallest x such that m[x] is maximum in m[i, j].
• n(i, j): the number of times the minimum within the subtrees of children i to j occurs within

that range.

• r(i, j, t): the x such that the t-th minimum within the subtrees of children i to j occurs within

the x-th child.

• update: updates the data structure upon ±1 changes in some child.
Simple operations involving rank and select on P are carried out easily with O(log n/ log log n)
applications of p(i) and wφ/ψ(i). For example depth(i) is computed, starting from the root node,
by ﬁnding the child j = p(i) to descend, then recursively computing depth(i− (cid:96)j) on the j-th child,
and ﬁnally adding e[j − 1] to the result. Handling φ for P1 and P2 is immediate; we omit it.
Operations fwd search/bwd search can be carried out via O(log n/ log log n) applications of
√
f (i, d)/b(i, d). Recalling Lemma 4, the interval of interest is partitioned into O(
log n·log n/ log log n)
nodes of the B-tree, but these can be grouped into O(log n/ log log n) sequences of consecutive siblings.
 Within each such sequence a single f (i, d)/b(i, d) operation is suﬃcient. For example, for
fwd search(i, d), let us assume d is a global excess to ﬁnd (i.e., start with d ← d + depth(i)− 1). We
start at the root v of the range min-max tree, and compute j = p(i), so the search starts at the
j-th child, with the recursive query fwd search(i − (cid:96)j, d − e[j − 1]). If the answer is not found in
that child, query j(cid:48) = f (j + 1, d) tells that it is within child j(cid:48). We then enter recursively into the
j(cid:48)-th child of the node with fwd search(i − (cid:96)j(cid:48), d − e[j(cid:48) − 1]), where the answer is sure to be found.
Operations rmqi and RMQi are solved in very similar fashion, using O(log n/ log log n) applications 
of r(i, j)/R(i, j). For example, to compute rmq(i, i(cid:48)) (the extension to rmqi is obvious) we
start with j = p(i) and j(cid:48) = p(i(cid:48)). If j = j(cid:48) we answer with e[j − 1] + rmq(i − (cid:96)j, i(cid:48) − (cid:96)j) on the j-th
child of the current node. Otherwise we recursively compute e[j − 1] + rmq(i − (cid:96)j, (cid:96)j+1 − (cid:96)j − 1),
e[j(cid:48) − 1] + rmq(0, i(cid:48) − (cid:96)j(cid:48)) and, if j + 1 < j(cid:48), m[r(j + 1, j(cid:48) − 1)], and return the minimum of the two
or three values.

For degree we partition the interval as for rmqi and then use m[r(i, j)] in each node to identify
those holding the global minimum. For each node holding the minimum, n(i, j) gives the number
of occurrences of the minimum in the node. Thus we apply r(i, j) and n(i, j) O(log n/ log log n)

23

times. Operation child rank is very similar, by changing the right end of the interval of interest, as
before. Finally, solving child is also similar, except that when we exceed the desired rank in the sum
(i.e., in some node n(i, j) ≥ t, where t is the local rank of the child we are looking for), we ﬁnd the
desired min-max tree branch with r(i, j, t), and continue on the child with t ← t− n(i, r(i, j, t)− 1),
using one r(i, j, t) operation per level.

7.1 Dynamic partial sums

Let us now face the problem of implementing the basic operations. Our ﬁrst tool is a result by
Raman et al., which solves several subproblems of the same type.

constant time: sum(i) =(cid:80)i

Lemma 14 ([44]) Under the RAM model with word size Θ(log n), it is possible to maintain a
sequence of log n nonnegative integers x1, x2, . . . of log n bits each, for any constant 0 ≤  < 1,
such that the data structure requires O(log1+ n) bits and carries out the following operations in
j=1 xj, search(s) = max{i, sum(i) ≤ s}, and update(i, δ), which sets
xi ← xi + δ, for − log n ≤ δ ≤ log n. The data structure also uses a precomputed universal table of
) bits for any ﬁxed (cid:48) > 0. The structure can be built in O(log n) time except the table.
size O(n(cid:48)

Then we can store (cid:96), sφ, and sψ in diﬀerential form, and obtain their values via sum. The
same can be done with e, provided we ﬁx the fact that it can contain negative values by storing
e[i] + 2(cid:100)log n(cid:101) · i (this works for constant-time sum, yet not for search). Operations p and wφ/ψ are
then solved via search on (cid:96) and s, respectively. Moreover we can handle ±1 changes in the subtrees
in constant time as well. In addition, we can store m[i]− e[i− 1] and M [i]− e[i− 1], which depend
only on the subtree, and reconstruct the values in constant time using sum on e, which eliminates
the problem of propagating changes in e[i] to m[i + 1, k] and M [i + 1, k]. Local changes to m[i] or
M [i] can be applied directly.

7.2 Cartesian trees

Our second tool is the Cartesian tree [51, 49]. A Cartesian tree for an array B[1, k] is a binary
tree in which the root node stores the minimum value B[µ], and the left and the right subtrees are
Cartesian trees for B[1, µ − 1] and B[µ + 1, k], respectively. If there exist more than one minimum
value position, then µ is the leftmost. Thus the tree shape has enough information to determine the
position of the leftmost minimum in any range [i, j]. As it is a binary tree of k nodes, a Cartesian
tree can be represented within 2k bits using parentheses and the bijection with general trees. It
can be built in O(k) time.
√
We build Cartesian trees for m[1, k] and for M [1, k] (this one taking maxima). Since 2k =
O(
log n), universal tables let us answer in constant time any query of the form r(i, j) and R(i, j),
√
as these depend only on the tree shape as explained. All the universal tables we will use on Cartesian
trees take O(2O(

log n) · polylog(n)) = o(nα) for any constant 0 < α < 1.

We also use Cartesian trees to solve operations f (i, d) and b(i, d). However, these do not
depend only on the tree shape, but on the actual values m[i, k]. We focus on f (i, d) since b(i, d) is
symmetric. Following Lemma 5, we ﬁrst check whether m[i] ≤ d ≤ M [i], in which case the answer
is i. Otherwise, the answer is either the next j such that m[j] ≤ d (if d < m[i]), or M [j] ≥ d (if
d > M [i]). Let us focus on the case d < m[i], as the other is symmetric. By Lemma 11, the answer
belongs to lrm(i), where the sequence is m[1, k].

24

Lemma 15 Let C be the Cartesian tree for m[1, k]. Then lrm(i) is the sequence of nodes of C in
the upward path from i to the root, which are reached from the left child.

Proof. The left and right children of node i contain values not smaller than i. All the nodes in
the upward path are equal to or smaller than i. Those reached from the right must be at the left
of position i, as they must be either to the left or to the right of all the nodes already seen, and i
has been seen. Their left children are also to the left of i. Ancestors j reached from the left are
strictly smaller than i and, by the previous argument, to the right of i, thus they belong to lrm(i).
Finally, the right descendants of those j are not in lrm(i) because they are after j and equal to or
(cid:117)(cid:116)
larger than m[j].

The Cartesian tree can have precomputed lrm(i) for each i, as this depends only on the tree
shape, and thus are stored in universal tables. This is the sequence of positions in m[1, k] that must
be considered. We can then binary search this sequence, using the technique described to retrieve
any desired m[j], to compute f (i, d) in O(log k) = O(log log n) time.

7.3 Complete trees

We arrange a complete binary tree on top of the n[1, k] values, so that each node of the tree records
(i) one leaf where the subtree minimum is attained, and (ii) the number of times the minimum
arises in its subtree. This tree is arranged in heap order and requires O(log3/2 n) bits of space.
A query n(i, j) is answered essentially as in Section 6: We ﬁnd the O(log k) nodes that cover
[i, j], ﬁnd the minimum m[·] value among the leaves stored in (i) for each covering node (recall
we have constant-time access to m), and add up the number of times (ﬁeld (ii)) the minimum of
m[i, j] occurs. This takes overall O(log k) time.

A query r(i, j, t) is answered similarly, stopping at the node where the left-to-right sum of the
ﬁelds (ii) reaches t, and then going down to the leaf x where t is reached. Then the t-th occurrence
of the minimum in subtrees i to j occurs within the x-th subtree.

When an m[i] or n[i] value changes, we must update the upward path towards the root of the
complete tree, using the update formula for n(v) given in Section 6. This is also suﬃcient when e[i]
changes: Although this implicitly changes all the m[i + 1, k] values, the local subtree data outside
the ancestors of i are unaﬀected. Then the root n(v) value will become an n[i(cid:48)] value at the parent
of the current range min-max tree node (just as the minimum of m[1, k], maximum of M [1, k],
excess e[k], etc., which can be computed in constant time as we have seen).
Since these operations take time O(log k) = O(log log n) time, the time complexity of degree,
child, and child rank is O(log n). Update operations (insert and delete) also require O(log n) time,
as we may need to update n[·] for one node per tree level. However, as we see later, it is possible
to achieve time complexity O(log n/ log log n) for insert and delete for all the other operations.
Therefore, we might choose not to support operations n(i, j) and r(i, j, t) to retain the lower update
complexity. In this case, operations degree, child, and child rank can only be implemented naively
using ﬁrst child, next sibling, and parent.

7.4 Updating Cartesian trees

We already solved some simple cases of update, but not yet how to maintain Cartesian trees. When
a value m[i] or M [i] changes (by ±1), the Cartesian trees might change their shape. Similarly, a
±1 change in e[i] induces a change in the eﬀective value of m[i + 1, k] and M [i + 1, k]. We store m

25

Figure 4: Rotations between i and its parent j when m[i] decreases by 1. The edges between any x
and its parent are labeled with d[x] = m[x] − m[Cparent(x)], if these change during the rotation.
The d[·] values have already been updated. On the left, when i < j, on the right, when i > j.

and M in a way independent of e, but the Cartesian trees are built upon the actual values of m
and M . Let us focus on m, as M is similar. If m[i] decreases by 1, we need to determine if i should
go higher in the tree. We compare i with its Cartesian tree parent j = Cparent(i) and, if (a) i < j
and m[i] − m[j] = 0, or if (b) i > j and m[i] − m[j] = −1, we must carry out a rotation with i
and j. Figure 4 shows the two cases. As it can be noticed, case (b) may propagate the rotations
towards the new parent of i, as it generates a new distance d − 1 that is smaller than before.
In order to carry out those propagations in constant time, we store an array d[1, k], so that
d[i] = m[i]− m[Cparent(i)] if this is ≤ k + 2, and k + 2 otherwise. Since d[1, k] requires O(k log k) =
√
O(
log n log log n) = o(log n) bits of space, it can be manipulated in constant time using universal
tables: With d[1, k] and the current Cartesian tree as input, a universal table can precompute the
outcome of the changes in d[·] and the corresponding sequence of rotations triggered by the decrease
of m[i] for any i, so we can obtain in constant time the new Cartesian tree and the new table d[1, k].
The limitation of values up to k + 2 is necessary for the table ﬁtting in a machine word, and its
consequences will be discussed soon.

Similarly, if m[i] increases by 1, we must compare i with its two children: (a) the diﬀerence
with its left child cannot fall below 1 and (b) the diﬀerence with its right child cannot fall below
0. Otherwise we must carry out rotations as well, depicted in Figure 5. While it might seem that
case (b) can propagate rotations upwards (due to the d− 1 at the root), this is not the case because
d had just been increased as m[i] grew by 1. In case both (a) and (b) arise simultaneously, we
must apply the rotation corresponding to (b) and then that of (a). No further propagation occurs.
Again, universal tables can precompute all these updates.

For changes in e[i], the universal tables have precomputed the eﬀect of carrying out all the
changes in m[i + 1, k] , updating all the necessary d[1, k] values and the Cartesian tree. This is

26

iijjjjCiiABABAAdBC010−1BCCd−1Figure 5: Rotations between i and its children when m[i] increases by 1. The edges between any x
and its parent are labeled with d[x] = m[x] − m[Cparent(x)], if these change during the rotation.
The d[·] values have already been updated. On the left (right), when the edge to the left (right)
child becomes invalid after the change in d[·].

equivalent to precomputing the eﬀect of a sequence of k − i successive changes in m[·].

Our array d[1, k] distinguishes values between 0 and k + 2. As the changes to the structure of
the Cartesian tree only depend on whether d[i] is 0, 1, or larger than 1, and all the updates to d[i]
are by ±1 per operation, we have suﬃcient information in d[·] to correctly predict any change in
the Cartesian tree shape for the next k updates. We refresh table d[·] fast enough to ensure that
no value of d[·] is used for more than k updates without recomputing it, as then its imprecision
could cause a ﬂaw. We simply recompute cyclically the cells of d[·], one per update. That is,
at the i-th update arriving at the node, we recompute the cell i(cid:48) = 1 + (i mod k), setting again
d[i(cid:48)] = min(k + 2, m[i(cid:48)] − m[Cparent(i(cid:48))]); note that Cparent(i(cid:48)) is computed from the Cartesian
tree shape in constant time via table lookup. Note the values of m[·] are always up to date because
we do not keep them in explicit form but with e[i − 1] subtracted (and in turn e is not maintained
explicitly but via partial sums).

7.5 Handling splits and merges

In case of splits or merges of segments or internal range min-max tree nodes, we must insert or
delete children in a node. To maintain the range min-max tree dynamically, we use Fleischer’s data
structure [16]. This is an (a, 2b)-tree (for a ≤ 2b) storing n numeric keys in the leaves, and each
leaf is a bucket storing at most 2 loga n keys. It supports constant-time insertion and deletion of a
key once its location in a leaf is known.

Each leaf owns a cursor, which is a pointer to a tree node. This cursor traverses the tree
upwards, looking for nodes that should be split, moving one step per insertion received at the leaf.
When the cursor reaches the root, the leaf has received at most loga n insertions and thus it is
split. Both new leaves are born with their cursor at their common parent. In addition some edges

27

jjiiii1d’−1>=0d−1ABCj−1dd’>=1AjBCABAB00CCmust be marked. Marks are considered when splitting nodes (see Fleischer [16] for details). The
insertion steps are as follows:

1. Insert the new key into the leaf B. Let v be the current node where the cursor of B points.

2. If v has more than b children, split it into v1 and v2, and unmark all edges leaving from those

nodes. If the parent of v has more than b children, mark the edges to v1 and v2.

3. If v is not the root, set the cursor to the parent of v. Otherwise, split B into two halves, and

let the cursor of both new buckets point to their common parent.

√

√
log n, b = 2

To apply this to our data structure, let a =

log n. Then the height of the
tree is O(log n/ log log n), and each leaf should store Θ(log n/ log log n) keys. Instead our structure
stores Θ(log2 n/ log log n) bits in each leaf. If Fleischer’s structure handles O(log n)-bit numbers,
it turns out that the leaf size is the same in both cases. The diﬀerence is that our insertions
are bitwise, whereas Fleischer’s insertions are number-wise (that is, in packets of O(log n) bits).
Therefore we can use their same structure, yet the cursor will return to the leaf Θ(log n) times
more frequently than necessary. Thus we only split a leaf when the cursor returns to it and it has
actually exceeded size 2L. This means leaves can actually reach size L(cid:48) = 2L +O(log n/ log log n) =
2L(1 + O(1/ log n)), which is not a problem. Marking and unmarking of children edges is easily
handled in constant time by storing a bit-vector of length 2b in each node.

Fleischer’s update time is constant. Ours is O(

log n) because, if we split a node into two,
√
we fully reconstruct all the values in those two nodes and their parent. This can be done in
O(k) = O(
log n) time, as the structure of Lemma 14, the Cartesian trees, and the complete trees
can be built in linear time. Nevertheless this time is dominated by the O(log n/ log log n) cost of
inserting a bit at the leaf.

√

√

log n.

Deletion of children of internal nodes may make the node arity fall below a. This is handled as
in Fleischer’s structure, by deamortized global rebuilding. This increases only the sublinear size of
the range min-max tree; the leaves are not aﬀected. As a consequence, our tree arities are in the
range 1 ≤ k ≤ 4
Deletions at leaves, however, are handled as before, ensuring that they have always between L
and L(cid:48) bits. This may cause some abrupt growth in their length. The most extreme case arises
when merging an underﬂowing leaf of L − 1 bits with its sibling of length L. In this case the result
is of size 2L − 1, close to overﬂowing, it cannot be split, and the cursor may be far from the root.
This is not a problem, however, as we have still suﬃcient time before the merged leaf reaches size
L(cid:48).

7.6 The ﬁnal result

We have obtained the following result.
Lemma 16 For a 0,1 vector of length 2n, there exists a data structure using 2n+O(n log log n/ log n)
bits supporting fwd search and bwd search in O(log n) time, and updates and all other queries except
degree, child, and child rank, in O(log n/ log log n) time. Alternatively, degree, child, child rank,
and updates can be handled in O(log n) time.

The complexity of fwd search and bwd search is not completely satisfactory, as we have reduced
many operators to those. To achieve better complexities, we note that most operators that reduce

28

to fwd search and bwd search actually reduce to the less general operations ﬁndclose, ﬁndopen, and
enclose on parentheses. Those three operations can be supported in time O(log n/ log log n) by
√
adapting the technique of Chan et al. [8]. They use a tree of similar layout as ours: leaves storing
Θ(log2 n/ log log n) parentheses and internal nodes of arity k = Θ(
log n), where Lemma 14 is used
to store seven arrays of numbers recording information on matched and unmatched parentheses on
the children. Those are updated in constant time upon parenthesis insertions and deletions, and
are suﬃcient to support the three operations. They report O(n) bits of space because they do not
use a mechanism like the one we describe in Section 6.1 for the leaves; otherwise their space would
be 2n + O(n log log n/ log n) as well. Note, on the other hand, that they do not achieve the times
we oﬀer for lca and related operations.

This completes the main result of this section, Theorem 2.

7.7 Updating whole subtrees

We face now the problem of attaching and detaching whole subtrees. Now we assume log n is
ﬁxed to some suﬃciently large value, for example log n = w, the width of the systemwide pointers.
Hence, no matter the size of the trees, they use segments of the same length, and the times are a
function of w and not of the actual tree size.

Now we cannot use Fleischer’s data structure [16], because a detached subtree could have
dangling cursors pointing to the larger tree it belonged. As a result, the time complexity for insert
or delete changes to O(log3/2 n/ log log n). To improve it, we change the degree of nodes in the
√
log n) to Θ(log n) for any ﬁxed constant  > 0. This makes the
range min-max tree from Θ(
complexity of insert and delete O( 1
 log1+ n/ log log n) = O(log1+ n), and multiplies all query
time complexities by the constant O(1/).
First we consider attaching a tree T1 to another tree T2, that is, T1 becomes a subtree rooted at
a node v of T2. Here v can be either an internal node or a leaf. Let P1[0, 2n1−1] and P2[0, 2n2−1] be
the BP sequence of T1 and T2, respectively. Then this attaching operation corresponds to creating
a new BP sequence P (cid:48) = P2[0, p]P1[0, 2n1 − 1]P2[p + 1, 2n2 − 1] where p and p + 1 are positions of
parentheses for siblings of the root of T1 in the new tree if v is an internal node, or p and p + 1 are
the positions for v if v is a leaf.

If p and p + 1 belong to the same segment, we cut the segment into two, say Pl = P [l, p] and
Pr = P [p + 1, r]. If the length of Pl (Pr) is less than L, we concatenate it to the left (right) segment
of it. If its length exceeds 2L, we split it into two. We also update the upward paths from Pl and
Pr to the root of the range min-max tree for T2 to reﬂect the changes done at the leaves.

Now we merge the range min-max trees for T1 and T2 as follows. Let h1 be the height of the
range min-max tree of T1, and h2 be the height of the lca, say v, between Pl and Pr in the range
min-max tree of T2. If h2 > h1 then can simply concatenate the root of T1 at the right of the
ancestor of Pl of height h1, then split the node if it has overﬂowed, and ﬁnish.
If h2 ≤ h1, we divide v into vl and vr, so that the rightmost child of vl is an ancestor of Pl and
the leftmost child of vr is an ancestor of Pr. We do not yet care about vl or vr being too small.
We repeat the process on the parent of v until reaching the height h2 = h1 + 1. Let us call u
the ancestor where this height is reached (we leave for later the case where we split the root of T2
without reaching the height h1 + 1).

Now we add T1 as a child of u, between the child ancestor of Pl and that ancestor of Pr. All the
leaves have the same depth, but the ancestors of Pl and of Pr at heights h2 to h1 might be underfull
as we have cut them arbitrarily. We glue the ancestor of height h of Pl with the leftmost node of

29

height h of T1, and that of Pr with the rightmost node of T1, for all h2 ≤ h ≤ h1. Now there are
no underfull nodes, but they can have overﬂowed. We verify the node sizes in both paths, from
height h = h2 to h1 + 1, splitting them as necessary. At height h2 the node can be split into two,
adding another child to its parent, which can thus be split into three, adding in turn two children
to its parent, but from there on nodes can only be split into three and add two more children to
their parent. Hence the overall process of ﬁxing arities takes time O( 1

 log1+ n/ log log n).

If node u does not exist, then T1 is not shorter than T2. In this case we have divided T2 into a
left and right part. Let h2 be the height of T2. We attach the left part of T2 to the leftmost node
of height h2 in T1, and the right part of T2 to the rightmost node of height h2 in T1. Then we ﬁx
arities in both paths analogously as before.

Detaching is analogous as well. After splitting the leftmost and rightmost leaves of the area to
be detached, let Pl and Pr the leaves of T preceding and following the leaves that will be detached.
We split the ancestors of Pl and Pr until reaching their lca, let it be v. Then we can form a new
tree with the detached part and remove it from the original tree T . Again, the paths from Pl and
Pr to v may contain underfull nodes. But now Pl and Pr are consecutive leaves, so we can merge
their ancestor paths up to v and then split as necessary.

Similarly, the leftmost and rightmost path of the detached tree may contain underfull nodes.
We merge each node of the leftmost (rightmost) path with its right (left) sibling, and then split
if necessary. The root may contain as few as two children. Overall the process takes O(log1+ n)
time.

8

Improving dynamic compressed sequences

The techniques we have developed along the paper are of independent interest. We illustrate
this point by improving the best current results on sequences of numbers with sum and search
operations, dynamic compressed bitmaps, and their many byproducts.

8.1 Codes, Numbers, and Partial Sums

We prove now Lemma 1 on sequences of codes and partial sums, this way improving previous results
by M¨akinen and Navarro [30] and matching lower bounds [41].
Section 7 shows how to maintain a dynamic bitmap P supporting various operations in time
O(log n/ log log n), including insertion and deletion of bits (parentheses in P ). This bitmap P will
now be the concatenation of the (possibly variable-length) codes xi. We will ensure that each leaf
contains a sequence of whole codes (no code is split at a leaf boundary). As these are of O(log n)
bits, we only need to slightly adjust the lower limit L to enforce this: After splitting a leaf of length
2L, one of the two new leaves might be of size L − O(log n).
√
We process a leaf by chunks of b = 1
2 log n bits: A universal table (easily computable in
O(
n polylog(n)) time and space) can tell us how many whole codes are there in the next b
bits, how much their f (·) values add up to, and where the last complete code ends (assuming we
start reading at a code boundary). Note that the ﬁrst code could be longer than b, in which case
the table lets us advance zero positions. In this case we decode the next code directly. Thus in
constant time (at most two table accesses plus one direct decoding) we advance in the traversal
by at least b bits. If we surpass the desired position with the table we reprocess the last O(log n)
√
codes using a second table that advances by chunks of O(
log n) bits, and ﬁnally process the last

30

√
O(
log n) codes directly. Thus in time O(log n/ log log n) we can access a given code in a leaf
(and subsequent ones in constant time each), sum the f (·) values up to some position, and ﬁnd the
position where a given sum s is exceeded. We can also easily modify a code or insert/delete codes,
by shifting all the other codes of the leaf in time O(log n/ log log n).

In internal nodes of the range min-max tree we will use the structure of Lemma 14 to maintain
the number of codes stored below the subtree of each child of the node. This allows determining
in constant time the child to follow when looking for any code xi, thus access to any codes xi . . . xj
is supported in time O(log n/ log log n + j − i).

When a code is inserted/deleted at a leaf, we must increment/decrement the number of codes in
the subtree of the ancestors up to the root; this is supported in constant time by Lemma 14. Splits
and merges can be caused by indels and by updates. They force the recomputation of their whole
parent node, and Fleischer’s technique is used to ensure a constant number of splits/merges per
update. Note we are inserting not individual bits but whole codes of O(log n) bits. This can easily
be done, but now O(log n/ log log n) insertions/updates can double the size of a leaf, and thus we
must consider splitting the leaf every time the cursor returns to it (as in the original Fleischer’s
proposal, not every log n times as when inserting parentheses), and we must advance the cursor
upon insertions and updates.
Also, we must allow leaves of sizes between L and L(cid:48) = 3L (but still split them as soon as they
exceed 2L bits). In this way, after a merge produces a leaf of size 2L− 1, we still have time to carry
out L = O(log n/ log log n) further insertions before the cursor reaches the root and splits the leaf.
(Recall that if the merge produces a leaf larger than 2L we can immediately split it, so 2L is the
worst case we must handle.)
For supporting sum and search we also maintain at each node the sum of the f (·) values of
the codes stored in the subtree of each child. Then we can determine in constant time the child
to follow for search, and the sum of previous subtrees for sum. However, insertions, deletions and
updates must alter the upward sums only by O(log n) so that the change can be supported by
Lemma 14 within the internal nodes in constant time.

8.2 Dynamic bitmaps

Apart from its general interest, handling a dynamic bitmap in compressed form is useful for maintaining 
satellite data for a sample of the tree nodes. A dynamic bitmap B could mark which nodes
are sampled, so if the sampling is sparse enough we would like B to be compressed. A rank on
this bitmap would give the position in a dynamic array where the satellite information for the
sampled nodes would be stored. This bitmap would be accessed by preorder (pre rank) on the
dynamic tree. That is, node v is sampled iﬀ B[pre rank (v)] = 1, and if so, its data is at position 
rank1(B, pre rank (v)) in the dynamic array of satellite data. When a tree node is inserted or
deleted, we need to insert/delete its corresponding bit in B.

In the following we prove the next lemma, which improves and indeed simpliﬁes previous results

[8, 30]; then we explore several byproducts.10
Lemma 17 We can store any bitmap B[0, n−1] within nH0(B)+O(n log log n/ log n) bits of space,
while supporting the operations rank, select, insert, and delete, all in time O(log n/ log log n). We

10Very recently, He and Munro [24] achieved results similar to Lemma 17, Theorem 3 and Theorem 5 (excluding
√
attachment and detachment of sequences), independently and with a diﬀerent technique. The results diﬀer in the
space redundancy, which in their case is O(1/

log n) times the sequence, versus our O(log log n/ log n).

31

can also support attachment and detachment of contiguous bitmaps within time O(log1+ n) for any
constant  > 0, yet now log n is a maximum ﬁxed value across all the operations.

To achieve zero-order entropy space, we use Raman et al.’s (c, o) encoding [45]: The bits are
grouped into small chunks of b = log n
2 bits, and each chunk is represented by two components: the
class ci, which is the number of bits set, and the oﬀset oi, which is an identiﬁer of that chunk within
those of the same class. Raman et al. show that, while the |ci| lengths add up to O(n log log n/ log n)
extra bits, the |oi| =

components add up to nH0(B) + O(n/ log n) bits.

(cid:108)
log(cid:0) b

(cid:1)(cid:109)

ci

ci

ci

m

We plan to store whole chunks in leaves of the range min-max tree. A problem is that the
insertion or even deletion of a single bit in Raman et al.’s representation can up to double the size
of the compressed representation of the segment, because it can change all the alignments. This
occurs for example when moving from 0b 1b 0b 1b . . . to 10b−1 01b−1 10b−1 01b−1 . . ., where we switch
from all ci = 0/b and |oi| = 0, to all ci = 1/b − 1, and |oi| = (cid:100)log b(cid:101). This problem can be dealt
with (laboriously) on binary trees [30, 22], but not on our k-ary tree, because Fleischer’s scheme
does not allow leaves being partitioned often enough.
We propose a diﬀerent solution that ensures that an insertion cannot make the leaf’s physical
size grow by more than O(log n) bits. Instead of using the same b value for all the chunks, we
allow any 1 ≤ bi ≤ b. Thus each chunk is represented by a triple (bi, ci, oi), where oi is the oﬀset
of this chunk among those of length bi having ci bits set. To ensure O(n log log n/ log n) space
overhead over the entropy, we state the invariant that any two consecutive chunks i, i + 1 must
satisfy bi + bi+1 > b. Thus there are O(n/b) chunks and the overhead of the bi and ci components,
representing each with (cid:100)log(b + 1)(cid:101) bits, is O(n log b/b). It is also easy to see that the inequality

(cid:1) + O(n/ log n) = nH0(B) + O(n/ log n)

(cid:1) + O(n/ log n) ≤ log(cid:0) n

[39](cid:80)|oi| =(cid:80)(cid:100)log(cid:0)bi

(cid:1)(cid:101) = log Π(cid:0)bi

i, c(cid:48)

i, o(cid:48)
i).

holds, where m is the number of 1s in the bitmap.
To maintain the invariant, the insertion of a bit us processed as follows. We ﬁrst identify the
chunk (bi, ci, oi) where the bit must be inserted, and compute its new description (b(cid:48)
If
i/2± 1. Now we check left
b(cid:48)
i > b, we split the chunk into two, (bl, cl, ol) and (br, cr, or), for bl, br = b(cid:48)
and right neighbors (bi−1, ci−1, oi−1) and (bi+1, ci+1, oi+1) to ensure the invariant on consecutive
chunks holds. If bi−1 + bl ≤ b we merge these two chunks, and if br + bi+1 ≤ b we merge these two
as well. Merging is done in constant time by obtaining the plain bitmaps, concatenating them, and
reencoding them, using universal tables (which we must have for all 1 ≤ bi ≤ b). Deletion of a bit
i + bi+1 ≤ b. It
is analogous; we remove the bit and then consider the conditions bi−1 + b(cid:48)
is easy to see that no insertion/deletion can increase the encoding by more than O(log n) bits.
Now let us consider codes xi = (bi, ci, oi). These are clearly constant-time self-delimiting and
|xi| = O(log n), so we can directly use Lemma 1 to store them in a range min-max tree within
n(cid:48) + O(n(cid:48) log log n(cid:48)/ log n(cid:48)) bits, where n(cid:48) = nH0(B) + O(n log log n/ log n) is the size of our compressed 
representation. Since n(cid:48) ≤ n + O(n log log n/ log n), we have O(n(cid:48) log log n(cid:48)/ log n(cid:48)) =
O(n log log n/ log n) and the overall space is as promised in the lemma. We must only take care of
checking the invariant on consecutive chunks when merging leaves, which takes constant time.
Now we use the sum/search capabilities of Lemma 1. Let fb(bi, ci, oi) = bi and fc(bi, ci, oi) = ci.
As both are always O(log n), we can have sum/search support on them. With search on fb we
can reach the code containing the jth bit of the original sequence, which is key for accessing an
arbitrary bit. For supporting rank we need to descend using search on fb, and accumulate the
sum on the fc values of the left siblings as we descend. For supporting select we descend using
search on fc, and accumulate the sum on the fb values. Finally, for insertions and deletions of
bits we ﬁrst access the proper position, and then implement the operation via a constant number

i ≤ b and b(cid:48)

32

of updates, insertions, and deletions of codes (for updating, splitting, and merging our triplets).
Thus we implement all the operations within time O(log n/ log log n).
We can also support attachment and detachment of contiguous bitmaps, by applying essentially
the same techniques developed in Section 7.7. We can have a bitmap B(cid:48)[0, n(cid:48) − 1] and insert it
between B[i] and B[i+1], or we can detach any B[i, j] from B and convert it into a separate bitmap
that can be handled independently. The complications that arise when cutting the compressed
segments at arbitrary positions are easily handled by splitting codes. Zero-order compression is
retained as it is due to the sum of the local entropies of the chunks, which are preserved (small
resulting segments after the splits are merged as usual).

We divide the sequence into chunks of maximum size b = 1

8.3 Sequences and Text Indices
We now aim at maintaining a sequence S[0, n− 1] of symbols over an alphabet [1, σ], so that we can
insert and delete symbols, and also compute symbol rankc(S, i) and selectc(S, i), for 1 ≤ c ≤ σ. This
has in particular applications to labeled trees: We can store the sequence S of the labels of a tree
in preorder, so that S[pre rank (i)] is the label of node i. Insertions and deletions of nodes must be
accompanied with insertions and deletions of their labels at the corresponding preorder positions,
and this can be extended to attaching and detaching subtrees. Then we not only have easy access
to the label of each node, but can also use rank and select on S to ﬁnd the r-th descendant node
labeled c, or compute the number of descendants labeled c. If the balanced parentheses represent
the tree in DFUDS format [6], we can instead ﬁnd the ﬁrst child of a node labeled c using select.
2 logσ n symbols and store them
using an extension of the (ci, oi) encoding for sequences [13]. Here ci = (c1
is
the number of occurrences of character a in the chunk. For this code to be of length O(log n) we
need σ = O(log n/ log log n); more stringent conditions will arise later. To this code we add the bi
component as in Section 8.2. This takes nH0(S)+O( nσ log log n
) bits of space. In the range min-max
tree nodes, which we again assume to hold Θ(log n) children for some constant 0 <  < 1, instead
of a single fc function as in Section 8.2, we must store one fa function for each a ∈ [1, σ], requiring
extra space O( nσ log log n
). Symbol rank and select are easily carried out by considering the proper
fa function. Insertion and deletion of symbol a is carried out in the compressed sequence as before,
and only fb and fa sums must be incremented/decremented along the path to the root.
In case a leaf node splits or merges, we must rebuild the partial sums for all the σ functions
fa (and the single function fb) of a node, which requires O(σ log n) time. In Section 7.5 we have
shown how to limit the number of splits/merges to one per operation, thus we can handle all the
operations within O(log n/ log log n) time as long as σ = O(log1− n/ log log n). This, again, greatly
simpliﬁes the solution by Gonz´alez and Navarro [22], which used a collection of searchable partial
sums with indels.

i ), where ca
i

i , . . . , cσ

log n

log n

Up to here, the result is useful for small alphabets only. Gonz´alez and Navarro [22] handle
larger alphabets by using a multiary wavelet tree (Section 2.1). Recall this is a complete r-ary
tree of height h = (cid:100)logr σ(cid:101) that stores a string over alphabet [1, r] at each node. It solves all the
operations (including insertions and deletions) by h applications of the analogous operation on the
sequences over alphabet [1, r].
Now we set r = log1− n/ log log n, and use the small-alphabet solution to handle the sequences
. The
zero-order entropies of the small-alphabet sequences add up to that of the original sequence and the

stored at the wavelet tree nodes. The height of the wavelet tree is h = O(cid:16)

(1−) log log n

(cid:17)

1 +

log σ

33

(cid:17)

redundancies add up to O(cid:16)

(cid:17)(cid:17)
.
By slightly altering , we achieve the ﬁrst part of Theorem 3, where the O(σ log n) term owes to
representing the wavelet tree itself, which has O(σ/r) nodes.
For the second part, the arity of the nodes ﬁxed to Θ(log n) allows us attach and detach
substrings in time O(r log1+ n) on a sequence with alphabet size r. This has to be carried out on
each of the O(σ/r) wavelet tree nodes, reaching overall complexity O(σ log1+ n).

. The operations time is O(cid:16) log n

(1−) log n log log n

(1−) log log n

log log n

(cid:16)

n log σ

1 +

log σ

The theorem has immediate application to the handling of compressed dynamic text collections,
construction of compressed static text collections within compressed space, and construction of the
Burrows-Wheeler transform (BWT) within compressed space. We state them here for completeness;
for their derivation refer to the original articles [30, 22].

The ﬁrst result refers to maintaining a collection of texts within high-entropy space, so that one
can perform searches and also insert and delete texts. Here Hh refers to the h-th order empirical
entropy of a sequence, see e.g. Manzini [31]. We use sampling step logσ n log log n to achieve it.
Theorem 5 There exists a data structure for handling a collection C of texts over an alphabet [1, σ]
within size nHh(C) + o(n log σ) +O(σh+1 log n + m log n + w) bits, simultaneously for all h. Here n
is the length of the concatenation of m texts, C = 0 T10 T2 ··· 0 Tm, and we assume that σ = o(n) is
the alphabet size and w = Ω(log n) is the machine word size under the RAM model. The structure
supports counting of the occurrences of a pattern P in O(|P|
log log n )) time, and inserting
and deleting a text T in O(log n +|T|
log log n )) time. After counting, any occurrence can
be located in time O( log2 n
log σ )). Any substring of length (cid:96) from any T in the collection
can be displayed in time O( log2 n
log log n )). For h ≤ (α logσ n) − 1, for
any constant 0 < α < 1, the space complexity simpliﬁes to nHh(C) + o(n log σ) + O(m log n + w)
bits.

log log n (1 + log log n

log log n (1 + log log n

log log n (1 + log σ

log log n (1 + log σ

log log n (1+ log σ

log σ ) + (cid:96) log n

log n

log n

The second result refers to the construction of the most succinct self-index for text within the
same asymptotic space required by the ﬁnal structure. This is tightly related to the construction
of the BWT, which has many applications.
Theorem 6 The Alphabet-Friendly FM-index [13], as well as the BWT [7], of a text T [0, n − 1]
over an alphabet of size σ, can be built using nHh(T ) + o(n log σ) bits, simultaneously for all
h ≤ (α logσ n) − 1 and any constant 0 < α < 1, in time O(n log n

log log n (1 + log σ

log log n )).

On polylog-sized alphabets, we build the BWT in o(n log n) time. Even on a large alphabet
σ = Θ(n), we build the BWT in o(n log2 n) time. This slashes by a log log n factor the corresponding
previous result [22]. Other previous results that focus in using little space are as follows. Okanohara
and Sadakane [38] achieved optimal O(n) construction time with O(n log σ log logσ n) bits of extra
space (apart from the n log σ bits of the sequence). Hon et al. [26] achieve O(n log log σ) time and
O(n log σ) bits of extra space. Ours is the fastest construction within compressed space.

9 Concluding remarks

We have proposed ﬂexible and powerful data structures for the succinct representation of ordinal 
trees. For the static case, all the known operations are done in constant time using 2n +

34

O(n/polylog(n)) bits of space, for a tree of n nodes and a polylog of any degree. This signiﬁcantly 
improves upon the redundancy of previous representations. The core of the idea is the range
min-max tree. This simple data structure reduces all of the operations to a handful of primitives,
which run in constant time on polylog-sized subtrees. It can be used in standalone form to obtain
a simple and practical implementation that achieves O(log n) time for all the operations. We then
show how constant time can be achieved by using the range min-max tree as a building block for
handling larger trees.

The simple implementation using one range min-max tree has actually been implemented and
compared with the state of the art over several real-life trees [2]. It has been shown that it is by
far the smallest and fastest representation in most cases, as well as the one with widest coverage
of operations. It requires around 2.37 bits per node and carries out most operations within the
microsecond on a standard PC.

For the dynamic case, there have been no data structures supporting several of the usual tree
operations. The data structures of this paper support all of the operations, including node insertion
and deletion, in O(log n) time, and a variant supports most of them in O(log n/ log log n) time,
which is optimal in the dynamic case even for a very reduced set of operations. They are based on
dynamic range min-max trees, and especially the former is extremely simple and implementable. We
expect a performance similar to that of the static version in practice. Their ﬂexibility is illustrated
by the fact that we can support much more complex operations, such as attaching and detaching
whole subtrees.

Our work contains several ideas of independent interest. An immediate application to storing
a dynamic sequence of numbers supporting operations sum and search achieves optimal time
O(log n/ log log n). Another application is the storage of dynamic compressed sequences achieving
zero-order entropy space and improving the redundancy of previous work.
It also improves the
times for the operations, achieving the optimal O(log n/ log log n) for polylog-sized alphabets. This
in turn has several applications to compressed text indexing.
Pˇatra¸scu and Viola have recently shown that n + n/wΘ(c) bits are necessary to compute rank or
select on bitmaps in time O(t) in the worst case [40]. This lower bound holds also in the subclass
of balanced bitmaps11 (i.e., those corresponding to balanced parenthesis sequences), which makes
our redundancy on static trees optimal as well, at least for some of the operations: Since rank or
select can be obtained from any of the operations depth, pre rank, post rank, pre select, post select,
any balanced parentheses representation supporting any of these operations in time O(c) requires
2n+2n/wΘ(c) bits of space. Still, it would be good to show a lower bound for the more fundamental
set of operations ﬁndopen, ﬁndclose, and enclose.
On the other hand, the complexity O(log n/ log log n) is known to be optimal for several basic
dynamic tree operations, but not for all. It is also not clear if the redundancy O(n/r) achieved for
the dynamic trees, r = log n for the simpler structure and r = log log n/ log n for the more complex
one, is optimal to achieve the corresponding O(r) operation times. Finally, it would be good to
achieve O(log n/ log log n) time for all the operations or prove it impossible.

Acknowledgments

We thank Mihai Pˇatra¸scu for conﬁrming us the construction cost of his aB-tree and rank/select
data structure [43]. And we thank him again for conﬁrming us that the lower bound [40] holds for

11M. Pˇatra¸scu, personal communication.

35

balanced sequences.

References

[1] D. Arroyuelo. An improved succinct representation for dynamic k-ary trees. In Proc. 19th
Annual Symposium on Combinatorial Pattern Matching (CPM), LNCS 5029, pages 277–289,
2008.

[2] D. Arroyuelo, R. C´anovas, G. Navarro, and K. Sadakane. Succinct trees in practice. In Proc.
11th Workshop on Algorithm Engineering and Experiments (ALENEX), pages 84–97. SIAM
Press, 2010.

[3] J. Barbay, J. I. Munro, M. He, and S. S. Rao. Succinct indexes for strings, binary relations
and multi-labeled trees. In Proc. 18th Annual ACM-SIAM Symposium on Discrete Algorithms
(SODA), pages 680–689, 2007.

[4] M. Bender and M. Farach-Colton. The LCA problem revisited. In Proc. 4th Latin American

Symposium on Theoretical Informatics (LATIN), LNCS 1776, pages 88–94, 2000.

[5] M. Bender and M. Farach-Colton. The level ancestor problem simpliﬁed. Theoretical Computer

Science, 321(1):5–12, 2004.

[6] D. Benoit, E. D. Demaine, J. I. Munro, R. Raman, V. Raman, and S. S. Rao. Representing

trees of higher degree. Algorithmica, 43(4):275–292, 2005.

[7] M. Burrows and D.J. Wheeler. A block sorting data compression algorithm. Technical report,

Digital Systems Research Center, 1994.

[8] H.-L. Chan, W.-K. Hon, T.-W. Lam, and K. Sadakane. Compressed indexes for dynamic text

collections. ACM Transactions on Algorithms, 3(2):article 21, 2007.

[9] Y.-T. Chiang, C.-C. Lin, and H.-I. Lu. Orderly spanning trees with applications. SIAM Journal

on Computing, 34(4):924–945, 2005.

[10] O. Delpratt, N. Rahman, and R. Raman. Engineering the LOUDS succinct tree representation.
In Proc. 5th Workshop on Eﬃcient and Experimental Algorithms (WEA), pages 134–145.
LNCS 4007, 2006.

[11] A. Farzan and J. I. Munro. A uniform approach towards succinct representation of trees. In
Proc. 11th Scandinavian Workshop on Algorithm Theory (SWAT), LNCS 5124, pages 173–184,
2008.

[12] P. Ferragina, F. Luccio, G. Manzini, and S. Muthukrishnan. Structuring labeled trees for
optimal succinctness, and beyond. In Proc. 46th IEEE Annual Symposium on Foundations of
Computer Science (FOCS), pages 184–196, 2005.

[13] P. Ferragina, G. Manzini, V. M¨akinen, and G. Navarro. Compressed Representations of Sequences 
and Full-Text Indexes. ACM Transactions on Algorithms, 3(2):No. 20, 2007.

36

[14] J. Fischer. Optimal succinctness for range minimum queries. In Proc. 9th Symposium on Latin

American Theoretical Informatics (LATIN), LNCS 6034, pages 158–169, 2010.

[15] J. Fischer and V. Heun. A new succinct representation of RMQ-information and improvements
in the enhanced suﬃx array. In Proc. 1st International Symposium on Combinatorics, Algorithms,
 Probabilistic and Experimental Methodologies (ESCAPE), LNCS 4614, pages 459–470,
2007.

[16] R. Fleischer. A simple balanced search tree with O(1) worst-case update time. International

Journal of Foundations of Computer Science, 7(2):137–149, 1996.

[17] M. Fredman and M. Saks. The Cell Probe Complexity of Dynamic Data Structures. In Proc.

21st Annual ACM Symposium on Theory of Computing (STOC), pages 345–354, 1989.

[18] M. Fredman and D. Willard. Surpassing the information theoretic bound with fusion trees.

Journal of Computer and Systems Science, 47(3):424–436, 1993.

[19] R. F. Geary, N. Rahman, R. Raman, and V. Raman. A simple optimal representation for
balanced parentheses. In Proc. 15th Annual Symposium on Combinatorial Pattern Matching
(CPM), LNCS 3109, pages 159–172, 2004.

[20] R. F. Geary, R. Raman, and V. Raman. Succinct ordinal trees with level-ancestor queries.
In Proc. 15th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1–10,
2004.

[21] A. Golynski, R. Grossi, A. Gupta, R. Raman, and S. S. Rao. On the size of succinct indices.
In Proc. 15th Annual European Symposium on Algorithms (ESA), pages 371–382. LNCS 4698,
2007.

[22] R. Gonz´alez and G. Navarro. Rank/select on dynamic compressed sequences and applications.

Theoretical Computer Science, 410:4414–4422, 2008.

[23] R. Grossi, A. Gupta, and J. S. Vitter. High-Order Entropy-Compressed Text Indexes. In Proc.
14th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 841–850, 2003.

[24] M. He and I. Munro. Succinct representations of dynamic strings. In Proc. 17th International
Symposium on String Processing and Information Retrieval (SPIRE), LNCS, 2010. To appear.

[25] M. He, J. I. Munro, and S. S. Rao. Succinct ordinal trees based on tree covering. In Proc. 34th
International Colloquium on Automata, Languages and Programming (ICALP), LNCS 4596,
pages 509–520, 2007.

[26] W. K. Hon, K. Sadakane, and W. K. Sung. Breaking a Time-and-Space Barrier in Constructing

Full-Text Indices. SIAM Journal on Computing, 38(6):2162–2178, 2009.

[27] G. Jacobson. Space-eﬃcient static trees and graphs. In Proc. 30th IEEE Annual Symposium

on Foundations of Computer Science (FOCS), pages 549–554, 1989.

[28] J. Jansson, K. Sadakane, and W.-K. Sung. Ultra-succinct representation of ordered trees. In
Proc. 18th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 575–584,
2007.

37

[29] H.-I. Lu and C.-C. Yeh. Balanced parentheses strike back. ACM Transactions on Algorithms,

4(3):article 28, 2008.

[30] V. M¨akinen and G. Navarro. Dynamic entropy-compressed sequences and full-text indexes.

ACM Transactions on Algorithms, 4(3):article 32, 2008.

[31] G. Manzini. An Analysis of the Burrows-Wheeler Transform. Journal of the ACM, 48(3):407–

430, 2001.

[32] J. I. Munro. An implicit data structure supporting insertion, deletion, and search in O(log n)

time. Journal of Computer System Sciences, 33(1):66–74, 1986.

[33] J. I. Munro. Tables. In Proc. 16th Foundations of Software Technology and Computer Science

(FSTTCS), LNCS 1180, pages 37–42, 1996.

[34] J. I. Munro and V. Raman. Succinct representation of balanced parentheses and static trees.

SIAM Journal on Computing, 31(3):762–776, 2001.

[35] J. I. Munro, V. Raman, and S. S. Rao. Space eﬃcient suﬃx trees. Journal of Algorithms,

39(2):205–222, 2001.

[36] J. I. Munro, V. Raman, and A. J. Storm. Representing dynamic binary trees succinctly. In

Proc. ACM-SIAM SODA, pages 529–536, 2001.

[37] J. I. Munro and S. S. Rao. Succinct representations of functions. In Proc. 31th International
Colloquium on Automata, Languages and Programming (ICALP), LNCS 3142, pages 1006–
1015, 2004.

[38] D. Okanohara and K. Sadakane. A linear-time burrows-wheeler transform using induced sorting.
 In Proc. 16th International Symposium on String Processing and Information Retrieval
(SPIRE), LNCS 5721, pages 90–101, 2009.

[39] R. Pagh. Low Redundancy in Static Dictionaries with Constant Query Time. SIAM Journal

on Computing, 31(2):353–363, 2001.

[40] M. Pˇatra¸scu and E. Viola. Cell-probe lower bounds for succinct partial sums. In Proc. 21st

ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 117–122, 2010.

[41] Mihai Pˇatra¸scu and Erik D. Demaine. Logarithmic lower bounds in the cell-probe model.

SIAM Journal on Computing, 35(4):932–963, 2006.

[42] M. P˘atra¸scu and M. Thorup. Time-space trade-oﬀs for predecessor search.

In Proc. 38th

Annual ACM Symposium on Theory of Computing (STOC), pages 232–240, 2006.

[43] M. Pˇatra¸scu. Succincter. In Proc. 49th IEEE Annual Symposium on Foundations of Computer

Science (FOCS), pages 305–313, 2008.

[44] R. Raman, V. Raman, and S. S. Rao. Succinct dynamic data structures. In Proc. 7th Annual

Workshop on Algorithms and Data Structures (WADS), LNCS 2125, pages 426–437, 2001.

38

[45] R. Raman, V. Raman, and S. S. Rao. Succinct indexable dictionaries with applications to
encoding k-ary trees and multisets. In Proc. 13th Annual ACM-SIAM Symposium on Discrete
Algorithms (SODA), pages 233–242, 2002.

[46] Rajeev Raman and S. Srinivasa Rao. Succinct dynamic dictionaries and trees.

In Annual
International Colloquium on Automata, Languages and Programming (ICALP), volume 2719
of Lecture Notes in Computer Science, pages 357–368. Springer-Verlag, 2003.

[47] K. Sadakane. Succinct Representations of lcp Information and Improvements in the Compressed 
Suﬃx Arrays. In Proc. 13th Annual ACM-SIAM Symposium on Discrete Algorithms
(SODA), pages 225–232, 2002.

[48] K. Sadakane. Compressed Suﬃx Trees with Full Functionality. Theory of Computing Systems,

41(4):589–607, 2007.

[49] K. Sadakane. Succinct data structures for ﬂexible text retrieval systems. Journal of Discrete

Algorithms, 5:12–22, 2007.

[50] K. Sadakane and G. Navarro. Fully-functional succinct trees. In Proc. 21st Annual ACM-SIAM

Symposium on Discrete Algorithms (SODA), pages 134–149, 2010.

[51] J. Vuillemin. A unifying look at data structures. Communications of the ACM, 23(4):229–239,

1980.

39

