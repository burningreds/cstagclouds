Position-Restricted Substring Searching

Veli M¨akinen1,(cid:1) and Gonzalo Navarro2,(cid:1)(cid:1)

1 Department of Computer Science, University of Helsinki, Finland

vmakinen@cs.helsinki.fi
2 Center for Web Research,

Dept. of Computer Science, University of Chile

gnavarro@dcc.uchile.cl

Abstract. A full-text index is a data structure built over a text string
T [1, n]. The most basic functionality provided is (a) counting how many
times a pattern string P [1, m] appears in T and (b) locating all those
occ positions. There exist several indexes that solve (a) in O(m) time
and (b) in O(occ) time. In this paper we propose two new queries, (c)
counting how many times P [1, m] appears in T [l, r] and (d) locating all
those occl,r positions. These can be solved using (a) and (b) but this
requires O(occ) time. We present two solutions to (c) and (d) in this
paper. The ﬁrst is an index that requires O(n log n) bits of space and
answers (c) in O(m + log n) time and (d) in O(log n) time per occurrence 
(that is, O(occl,r log n) time overall). A variant of the ﬁrst solution
answers (c) in O(m + log log n) time and (d) in constant time per occurrence,
 but requires O(n log1+ n) bits of space for any constant  > 0.
The second solution requires O(nm log σ) bits of space, solving (c) in
O(m(cid:1)log σ/ log log n(cid:2)) time and (d) in O(m(cid:1)log σ/ log log n(cid:2)) time per
occurrence, where σ is the alphabet size. This second structure takes
less space when the text is compressible.

Our solutions can be seen as a generalization of rank and select dictionaries,
 which allow computing how many times a given character c
appears in a preﬁx T [1, i] and also locate the i-th occurrence of c in
T . Our solution to (c) extends character rank queries to substring rank
queries, and our solution to (d) extends character select to substring select
queries.

As a byproduct, we show how rank queries can be used to implement
fractional cascading in little space, so as to obtain an alternative implementation 
of a well-known two-dimensional range search data structure
by Chazelle. We also show how Grossi et al.’s wavelet trees are suitable
for two-dimensional range searching, and their connection with Chazelle’s
data structure.

1 Introduction and Related Work

The indexed string matching problem is that of, given a long text T [1, n] over an
alphabet Σ of size σ, build a data structure called full-text index on it, to solve
(cid:2) Funded by the Academy of Finland under grant 108219.
(cid:2)(cid:2) Funded by Millennium Nucleus Center for Web Research, Grant P04-067-F, Mideplan,
 Chile.

J.R. Correa, A. Hevia, and M. Kiwi (Eds.): LATIN 2006, LNCS 3887, pp. 703–714, 2006.
c(cid:1) Springer-Verlag Berlin Heidelberg 2006

704

V. M¨akinen and G. Navarro

two types of queries: (a) Given a short pattern P [1, m] over Σ, count the occurrences 
of P in T ; (b) locate those occ positions in T . There are several classical
full-text indexes requiring O(n log n) bits of space which can answer counting
queries in O(m) time (like suﬃx trees [2]) or O(m + log n) time (like suﬃx arrays 
[14]). Both locate each occurrence in constant time once the counting is
done. Similar complexities are obtained with modern compressed data structures 
[5, 10, 7], requiring space nHk(T ) + o(n log σ) bits, where Hk(T ) ≤ log σ is
the k-th order empirical entropy of T .1

In this paper we introduce a new problem, position restricted substring searching,
 which consists of two new queries: (c) Given P [1, m] and two integers
1 ≤ l ≤ r ≤ n, count all the occurrences of P in T [l, r], and (d) locate those
occl,r occurrences. These queries are fundamental in many text search situations
where one wants to search only a part of the text collection, e.g. restricting the
search to a subset of dynamically chosen documents in a document database,
restricting the search to only parts of a long DNA sequence, and so on. Curiously,
 there seem to be no solutions to this problem apart from locating all the
occurrences and then ﬁlter those in the range [l, r]. This costs at least O(m+occ)
for (c) and (d) together.

We present several alternative structures to solve this problem. The best complexities 
are summarized in Table 1.

Table 1. Simpliﬁed complexities achieved for queries (c) and (d). Locating time is
given per occurrence.

Section

4
4
4
5

Bits of space
O(n log1+ n)

n log n(1 + o(1)) + O(nHk(T ) logγ n)

n log n(1 + o(1)) + nHk(T )

n m−1

k=0 Hk(T )

Counting time
O(m + log log n)

O(m(cid:1) log σ

O(m + log n)
O(m(cid:1) log σ
(cid:2))

log log n

(cid:2) + log n)

log log n

Locating time

O(1)

O(log n)
O(log n)
O(m(cid:1) log σ

log log n

(cid:2))

Interestingly, our solutions are also useful to solve a generalization of another
well-studied problem. Given a sequence S over an alphabet Σ of size σ and a
character c ∈ Σ, query rankc(S, i) returns the number of occurrences of c in
S[1, i], while selectc(S, j) returns the position of the j-th occurrence of c in S.
Both queries can be answered in constant time using data structures that require
nH0(S)+ o(n) bits of space if the alphabet of the sequence is σ = O(polylog(n)),
or in O(log σ/ log log n) time in general [9, 8]. They can also be solved in O(log σ)
time using wavelet trees [10, 11]. For the case of binary sequences, apart from
the simple n + o(n) bits data structures [12, 4, 16], there are others that answer
rank and select in constant time using nH0(S) + o(n) bits [18].

A natural generalization of the above problem is substring rank and select. For
a string s, ranks(S, i) is the number of occurrences of s in S[1, i], and selects(S, j)
is the starting position of the j-th occurrence of s in S. We can use the indexes
for position-restricted substring searching to answer ranks in the same time of
1 In this paper log stands for log2.

Position-Restricted Substring Searching

705

a counting query (type (c)), and selects in the same time of a counting query
plus the time to locate one occurrence (type (d)).

As a byproduct, we present a space-eﬃcient implementation of a well-known
two-dimensional range search data structure by Chazelle [3]. We show in particular 
how the fractional cascading information (which is simulated rather than
stored in Chazelle’s data structure) can be represented by constant-time rank
queries on bit arrays. We also show that Grossi et al.’s wavelet trees [10, 11]
are suitable for two-dimensional range searching, pointing out in particular their
connection with Chazelle’s data structure.

2 Two-Dimensional Range Searching

In this section we describe a range search data structure to query by rectangular
areas. The structure is a succinct variant of one from Chazelle [3, 13] where we
have completely removed binary searching and fractional cascading and have
replaced them by constant-time rank queries over bit arrays. Given a set of
points in [1, n] × [1, n], the data structure permits determining the number of
(cid:2)] in time O(log n), as well as retrieving
points that lie in a range [i, i
each of those points in O(log n) time. The structure can be implemented using
n log n(1 + o(1)) bits.

(cid:2)] × [j, j

Structure. We describe a slightly simpler version of the original structure [3],
which is suﬃcient for our problem. The simpliﬁcation is that our set of points
come from pairing two permutations of [1, n]. Therefore, no two diﬀerent points
share their same ﬁrst or second coordinates, that is, for every pair of points
(i, j) (cid:3)= (i
(cid:2). Moreover, there is a point with ﬁrst
coordinate i for any 1 ≤ i ≤ n and a point with second coordinate j for any
1 ≤ j ≤ n.

(cid:2)) it holds i (cid:3)= i

(cid:2) and j (cid:3)= j

, j

(cid:2)

(cid:2))/2(cid:5) + 1, i

(cid:2))/2(cid:5)] and [(cid:4)(i + i

(cid:2)] are associated to [i,(cid:4)(i + i

The structure is built as follows. First, sort the points by their j coordinate.
Then, form a perfect binary tree where each node handles an interval of the ﬁrst
coordinate i, and thus knows only the points whose ﬁrst coordinate falls in the
interval. The root handles the interval [1, n], and the children of a node handling
(cid:2)]. The leaves
interval [i, i
handle intervals for the form [i, i]. All those intervals will be called tree intervals.
Each node v contains a bitmap Bv so that Bv[r] = 0 iﬀ the r-th point handled
by node v (in the order given by the initial sorting by j coordinate) belongs to
the left child. Each of those bitmaps Bv is preprocessed for constant-time rank
queries [12, 4, 16]). The bitmaps with rank functionality give a space-eﬃcient way
to implement fractional cascading, and also avoid any need of binary searching.
Querying. We ﬁrst show how to track a particular point (i, j) as we go down
the tree. In the root, the position given by the sorting of coordinates is precisely
j, because there is exactly one point with second coordinate j for any j ∈ [1, n].
Then, if Broot[j] = 0, this means that point (i, j) is in the left subtree, otherwise 
it is in the right subtree. In the ﬁrst case, the new position of (i, j) in the left

706

V. M¨akinen and G. Navarro

(cid:1)], [j, j

(cid:1)], [ti, ti

(cid:1)])

(cid:1)] = ∅ then return 0;
(cid:1)] then return j
(cid:1))/2(cid:8);

Algorithm. RangeCount(v, [i, i
(1)
(2)
(3)
(4)
(5)
(6)
(7) return RangeCount(lef t(v), [i, i

(cid:1)
then return 0;
if j > j
(cid:1)] ∩ [i, i
if [ti, ti
(cid:1)] ⊆ [i, i
if [ti, ti
tm ← (cid:7)(ti + ti
[jl, j
[jr, j

(cid:1) − j + 1;
l] ← [rank0(Bv, j − 1) + 1, rank0(Bv, j
(cid:1)
r] ← [rank1(Bv, j − 1) + 1, rank1(Bv, j
(cid:1)
(cid:1)
r], [tm + 1, ti

(cid:1)], [jl, j
(cid:1)
l], [ti, tm]) +
(cid:1)], [jr, j

RangeCount(right(v), [i, i

(cid:1))];
(cid:1))];

(cid:1)]);

(cid:1)] on a tree structure
Fig. 1. Algorithm for counting the number of points in [i, i
rooted by v with nodes lef t(v) and right(v). The last argument is the tree interval
(cid:1)], [j, j
handled by node v. The ﬁrst invocation is RangeCount(root, [i, i

(cid:1)], [1, n]).

(cid:1)]× [j, j

Range searching for [i, i

subtree is j ← rank0(Broot, j), which is the number of points preceding (i, j)
in Broot which chose the left subtree. Similarly, the new position on the right
subtree it is j ← rank1(Broot, j).
(cid:2)] × [j, j
(cid:2)] is carried out as follows. Find in the tree
(cid:2)]. The answer is then the set
the O(log n) maximal tree intervals that cover [i, i
(cid:2)]. Those points
of points in those intervals whose second coordinate is in [j, j
form an interval in the B array of each of the nodes that form the cover of
(cid:2) coordinates as we descend by
[i, i
(cid:2)] ←
the tree. Every time we descend to the left child of a node v, we update [j, j
[rank0(Bv, j − 1) + 1, rank0(Bv, j
(cid:2))], and similarly with rank1 for a right child.
(cid:2)], the number of
When we arrive at a node whose interval is contained in [i, i
(cid:2)− j +1. Thus the whole procedure takes O(log n) time.
qualifying points is just j
Figure 1 shows the pseudocode.

(cid:2)]. However, we need to track those j and j

For retrieving the points, we start from each of the tree nodes that cover
(cid:2)] is tracked
(cid:2)]. Each point in the node whose second coordinate is in [j, j
[i, i
down in the tree until the leaves, so as to ﬁnd its ﬁrst coordinate i. This can
be done in O(log n) time per retrieved element. (For our application, we do not
describe how to associate the proper j value to each i coordinate found, but
it can be done by traversing the tree upwards from each leaf using select.) We
(cid:2)], as long as it has some
traverse the whole subtree of each node included in [i, i
(cid:2)]. The leaves found in this process are reported. Figure 2 gives the
point in [j, j
pseudocode.
Space. We do not need any pointer for this tree. We only need 1 + (cid:7)log n(cid:8)
bit streams, one per tree level. All the bit streams at level h of the tree are
concatenated into a single one, of length exactly n. A single rank structure is
computed for each whole level, totalizing n log n(1 + O(log log n/ log n)) bits.
Maintaining the initial position p of the sequence corresponding to node v at
level h is easy. There is only one sequence at the root, so p = 1 at level h = 1.
Now, assume that the sequence for v starts at position p (in level h), and we
move to a child (in level h + 1). Then the left child starts at the same position

Position-Restricted Substring Searching

707

(cid:1)], [ti, ti

(cid:1)])
then { output ti; return; }

(cid:1)
if ti = ti
(cid:1) then return;
if j > j
tm ← (cid:7)(ti + ti
[jl, j
[jr, j

Algorithm. RangeLocate(v, [j, j
(1)
(2)
(3)
(4)
(5)
(6) RangeLocate(lef t(v), [jl, j
(7) RangeLocate(right(v), [jr, j

l] ← [rank0(Bv, j − 1) + 1, rank0(Bv, j
(cid:1)
r] ← [rank1(Bv, j − 1) + 1, rank1(Bv, j
(cid:1)
(cid:1)]);

(cid:1)
r], [tm + 1, ti

(cid:1)
l], [ti, tm]);

(cid:1))/2(cid:8);

(cid:1))];
(cid:1))];

(cid:1) − j + 1 in line (3) of RangeCount,

Fig. 2. Algorithm to invoke instead of returning j
so as to locate occurrences instead of just counting them
p, while the right child starts at p + rank0(Bv,|Bv|). The length of the current
sequence |Bv| is also easy to maintain. The root sequence is of length n. Then
the left child of v is of length rank0(Bv,|Bv|) and the right child is of length
rank1(Bv,|Bv|). Finally, if we know that v starts at position p and we have the
whole-level sequence Bh instead of Bv, then rankb(Bv, j) = rankb(Bh, p − 1 +
j) − rankb(Bh, p − 1). Figure 3 shows again the counting algorithm, this time
over the real data structure.

(cid:1)], [j, j

(cid:1) − j + 1;

(cid:1)] = ∅ then return 0;
(cid:1)] then return j
(cid:1))/2(cid:8);

(cid:1)
then return 0;
if j > j
(cid:1)] ∩ [i, i
if [ti, ti
(cid:1)] ⊆ [i, i
if [ti, ti
tm ← (cid:7)(ti + ti
[jl, j
[jr, j
[(cid:3)l, (cid:3)r] ← [rank0(Bh, p, p − 1 + (cid:3)), rank1(Bh, p, p − 1 + (cid:3))]
(cid:1) ← p + rank0(Bh, (cid:3))

Algorithm. RangeCount(B, [i, i
(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8) p
(9) return RangeCount(B, [i, i
RangeCount(B, [i, i

l] ← [rank0(Bh, p, p − 1 + j − 1) + 1, rank0(Bh, p, p − 1 + j
(cid:1)
r] ← [rank1(Bh, p, p − 1 + j − 1) + 1, rank1(Bh, p, p − 1 + j
(cid:1)

(cid:1)
(cid:1)], [jl, j
l], h + 1, p, (cid:3)l, [ti, tm]) +
(cid:1)], [jr, j
(cid:1)
r], h + 1, p

, (cid:3)r, [tm + 1, ti

(cid:1)]);

(cid:1)

(cid:1)], h, p, (cid:3), [ti, ti

(cid:1)])

(cid:1))];
(cid:1))];

(cid:1)] × [j, j
Fig. 3. Algorithm for counting the number of points in [i, i
(cid:1)], [j, j
wise, structure. The ﬁrst invocation is RangeCount(B, [i, i
rankb(Bh, a, b) as shorthand for rankb(Bh, b) − rankb(Bh, a − 1).

(cid:1)] on the real, level-
(cid:1)], 1, 1, n, [1, n]). We use

Wavelet Trees. Wavelet trees [10, 11] are data structures for text indexing introduced 
by Grossi et al. The wavelet tree is a perfectly balanced tree of height
(cid:7)log σ(cid:8). Each tree node corresponds to a subinterval of [1, σ] and represents the
text subsequence of characters in that subinterval. At each node, the current
alphabet range is partitioned into two halves, and the corresponding alphabet
subintervals are assigned to the left and right child of the node. The only data
stored at a node is a bitmap where, for each character of the text it represents,
it is indicated whether that character went left or right.

708

V. M¨akinen and G. Navarro

Each bitmap is processed for rank and select queries. If one uses basic techniques 
for those queries [12, 4, 16] the wavelet tree takes n(cid:7)log σ(cid:8)+ O(n log log n/
logσ n) bits of space for a text T [1, n], that is, the same text size. With
more advanced techniques [18], the size of the wavelet tree achieves nH0(T ) +
O(n log log n/ logσ n) bits of space, where H0(T ) is the zero-order entropy of T .
In both cases, the wavelet tree solves in O(log σ) time the following queries: (a)
T [i], that is, ﬁnding the i-th character of T ; (b) rankc(T, i), that is, ﬁnding the
number of occurrences of c in T [1, i]; and (c) selectc(T, j), that is, ﬁnding the
position in T of the j-th occurrence of c.
We note now that wavelet trees have yet other applications not considered
before. Assume we have a set of points (i, j) ∈ [1, n]× [1, n] which is the product
of two permutations of [1, n] as explained in the beginning of this section. Call
i(j) the unique i value such that (i, j) is a point in the set. Then consider the
text T [1, n] = i(1)i(2)i(3) . . . i(n). Then, the wavelet tree of T is exactly the data
structure we have described in this section. This text has alphabet of size n and
its zero-order entropy is also log n, thus this wavelet tree takes n log n(1 + o(1))
bits as expected. Although the original wavelet-tree queries are not especially
interesting in this range search scenario, we have shown in this section that
the wavelet tree structure can indeed be used to solve two-dimensional range
search queries in O(log n) time, and report each occurrence in O(log n) time as
well.

3 A Simple O(m + log n) Time Solution

Our ﬁrst solution is composed of two data structures. The ﬁrst is the familiar
suﬃx array A[1, n] of T , enriched with longest common preﬁx (lcp) information
[14]. This structure needs 2n(cid:7)log n(cid:8) bits and permits determining the interval
A[sp, ep] of suﬃxes that start with P [1, m] in O(m + log n) time [14]. The
second is the range search data structure R described in Section 2, indexing
the points (i,A[i]). Both structures together require 3n log n(1 + o(1)) bits, or
3n + o(n) words.
To ﬁnd the number of occurrences of P [1, m] in T [l, r], we ﬁrst ﬁnd the interval
A[sp, ep] of the occurrences of P in T , and then count the number of points in
the range [l, r − m + 1] × [sp, ep] using R. This takes overall O(m + log n) time.
Additionally, each ﬁrst coordinate (that is, text position l ≤ i ≤ r − m + 1) of
an occurrence can be retrieved in O(log n) time, that is, the occl,r occurrences
can be located in O(occl,r log n) time.

A plus of the index is that, unlike plain suﬃx arrays, this structure locates
the occurrences in text position order, not in suﬃx array order. In order to ﬁnd
them in suﬃx array order, we should rather index points (A[i], i) and search
for the interval [sp, ep] × [l, r − m + 1]. Then R would retrieve the suﬃx array
positions i (in increasing order in A) such that A[i] is an occurrence.
Larger and faster. It is possible to improve the locating time to O(1) by using
slightly more space. Instead of the structure of Section 2, that of Alstrup et al.
[1] can be used to index the points (i,A[i]). This structure retrieves the occl,r

Position-Restricted Substring Searching

709

occurrences of a range query in O(log log n + occl,r) time. In exchange, it needs
O(n log1+ n) bits of space, for any constant 0 <  < 1. Thus, by using slightly
more space, we achieve O(m + log n) counting time and O(1) locating time per
occurrence.

Given the complexity O(log log n) for the range-search part of the counting
query, it makes sense to replace the suﬃx array by a suﬃx tree, so that we still
have O(n log1+ n) bits of space and can solve the counting query in O(m +
log log n) time, and the locating query in constant time per occurrence.
Smaller and slower. Alternatively, it is possible to replace the suﬃx array A
and its lcp information by any of the wealth of existing compressed data structures 
[17]. For example, by using the LZ-index of Ferragina and Manzini [6] we
obtain n log n(1 + o(1)) + O(nHk(T ) logγ n) bits of space (for any γ > 0 and
any k = O(logσ log n)) and the same time complexities. On the other hand,
we can use the alphabet-friendly FM-index of Ferragina et al. [7, 8] to obtain
n log n(1 + o(1)) + nHk(T ) bits of space (for any σ = o(n/ log log n) and any
k ≤ α logσ n for any constant 0 < α < 1). In this case the counting time raises
to O(m(cid:7)log σ/ log log n(cid:8) + log n). This is still O(m + log n) if σ = O(polylog(n)).

4 An O(m log σ) Time Solution

We present now a solution that, given a construction parameter t, requires
nt log σ(1+o(1)) bits of space and achieves O(m(cid:7)log σ/ log log n(cid:8)) time for counting 
the occurrences of any pattern of length m ≤ t. Likewise, each such occurrence 
can be located in O(m(cid:7)log σ/ log log n(cid:8)) time. For example, choosing
t = logσ n gives a structure using n log n(1 + o(1)) bits of space able to search
for patterns of length m ≤ logσ n.
Actually, we show that this structure can be smaller for compressible texts,
t−1
taking n
k=0 Hk(T ) instead of nt log σ, where Hk(T ) is the k-th order empirical
entropy of T [15, 10]. This is a lower bound to the number of bits per character
achievable by any compressor that considers contexts of length k to model T .

(cid:1)

Structure. Our structure indexes the positions of all the t-grams (substrings of
length t) of T . It can be tought of as an extension of the wavelet tree [10, 11] to
t-grams.

The structure is a perfectly balanced binary tree, which indexes the binary
representation of all the t-grams of T , and searches for the binary representation
of P . The binary representation b(s) of a string s over an alphabet σ is obtained
by expanding each character of s to the (cid:7)log σ(cid:8) bits necessary to code it. We
index n t-grams of T , namely b(T [1, t]), b(T [2, t + 1]), . . . , b(T [n, n + t− 1]). The
text T is padded with t − 1 dummy characters at the end.
The binary tree has (cid:6) = t(cid:7)log σ(cid:8) levels. Each tree node v is associated a binary
string s(v) according to the path from the root to v. That is, s(root) = ε and, if
vl and vr are the left and right children of v, respectively, then s(vl) = s(v)0 and
s(vr) = s(v)1. To each node v we also associate a subsequence of text positions
Sv = {i, s(v) is a preﬁx of b(T [i, i + t − 1])}.

710

V. M¨akinen and G. Navarro

Note that each i ∈ Sv will belong exactly to one of its two children, vl or vr.
At each internal node v we store a bitmap Bv of length nv = |Sv|, such that
Bv[i] = 0 iﬀ i ∈ Svl. Neither s(v) nor Sv are explicitly stored, only Bv is.
Querying. Given a text position i at the root node, we can track its corresponding
position in Bv for any node v such that i ∈ Sv. At the root, we start with
iroot = i. When we descend to the left child vl of a node v in the path, we
set ivl = rank0(Bv, i), and if we descend to the right child vr we set ivr =
rank1(Bv, i). Then we arrive with the proper iv value at any node v.
In order to search for P in the interval [l, r], we start at the root with lroot = l
and rroot = r − m + 1, and ﬁnd the tree node v such that s(v) = b(P ) (following
the bits of b(P ) to choose the path from the root). At the same time we obtain
the proper values lv and rv. Then the answer to the counting query is rv − lv + 1.
The process requires O(m log σ) time.
To locate each such occurrence lv ≤ iv ≤ rv, we must do the inverse tracking
upwards. If v is the left child of its parent vp, then the corresponding position in
vp is ivp = select0(Bvp, iv). If v is a right child, then ivp = select1(Bvp, iv).
The ﬁnal position in T is thus iroot. This takes O(m log σ) time for each
occurrence.
Space. The bulk of the space requirement corresponds to the overall size of bit
arrays Bv. Vectors Bv could be represented using the technique of Clark and
Munro [4, 16], which permits answering rank and select queries in constant time
over the bit arrays Bv using nv(1 + o(1)) bits. All the nv values at any depth
add up n, and since the tree height is (cid:6), we have nt(cid:7)log σ(cid:8)(1 + o(1)) bits overall.
The same technique used before to concatenate all the bitmaps at each level is
used here to ensure that o(1) is sublinear in n.

We show now that, by using more sophisticated techniques [18], the space
requirement may be reduced on compressible texts T . In that work they represent
bit array Bv using nvH0(Bv) + o(nv) bits, and answer rank and select queries
in constant time. As we already know that the o(nv) parts add up o(nm log σ)
bits (more precisely, O(nm log σ log log n/ log n) bits), we focus on the entropyrelated 
part. Let us assume for simplicity that σ is a power of 2.
Let us analyze all the nvH0(Bv) terms together. For a binary string s, let us
deﬁne ns = |{i, s is a preﬁx of b(T [i, i + t − 1])}|. Thus, if we consider vector
Broot, its representation takes nH0(Broot) = −n0 log n0
− n1 log n1
n .
Consider now the vectors B for the two children of the root. The entropy
−
− n10 log n10
part of their representations add up −n00 log n00
. We notice that n0 = n00 + n01 and n1 = n10 + n11. By adding up
n11 log n11
−
the size of representations of the root and its two children, we get −n00 log n00
n1
− n10 log n10
n01 log n01
n bits. This can be extended inductively to
n
log σ levels, so that the sum of all the representations from the root to level
log σ − 1 is

− n01 log n01

− n11 log n11

n1

n

n0

n0

n

n

− (cid:2)
s∈{0,1}log σ

ns log ns
n

= nH0(T )

where 0 log 0 = 0.

711
Similarly, starting from each node v such that s(v) ∈ {0, 1}log σ, we have that
, and all the B vectors in the next

Position-Restricted Substring Searching

− ns(v)1 log ns(v)1

ns(v)

nH0(Bv) = −ns(v)0 log ns(v)0
log σ levels of its subtree add up
− (cid:2)
s∈{0,1}log σ

ns(v)

ns(v)s log

ns(v)s
ns(v)

.

Summing this for all the nodes representing all the possible s(v) ∈ {0, 1}log σ,
we have

− (cid:2)
s,s(cid:1)∈{0,1}log σ

nss(cid:1) log nss(cid:1)
ns

= nH1(T ).

This can be continued inductively until level t log σ, to show that the overall

space is

t−1(cid:2)

k=0

n

Hk(T ) + O(nt log σ log log n/ log n)

bits. For incompressible texts this is nt log σ(1+ o(1)), but for compressible texts
it may be signiﬁcantly less.

Higher arity trees. A generalization of the rank/select data structures [18] permit
handling sequences with alphabets of size up to O(polylog(n)) with constant time
rankc and selectc [9, 8]. Instead of handling one bit of b(T [i, i + t− 1]) at a time,
we could handle a bits at a time. This way, our binary tree would be 2a-ary
instead of binary. Instead of a sequence of bits Bv at each node, we would store
a sequence Bv of integers in [0, a − 1]. As long as 2a = O(polylog(n)) (that is,
a = O(log log n)), we can index those sequences Bv with the generalized data
structure so as to answer in constant time the rank/select queries we need to
navigate the tree.
The search algorithm is adapted in the obvious way. When going down to
the d-th child of node v, 0 ≤ d < a, we update iv to ivd = rankd(Bv, iv) and,
similarly, when going up to v from child d, iv = selectd(Bv, ivd). Note that a must
divide log σ to ensure that any pattern search will arrive exactly at a tree node.
The overall time is O(m log(σ)/a) = O(m(cid:7)log σ/ log log n(cid:8)), either for counting
or for locating an occurrence. This is O(m) whenever σ = O(polylog(n)).

We note that it is necessary, again, to concatenate all sequences at each tree
level, so that the limit a = O(log log n) remains constant as we descend in the
tree. For space occupancy related to entropy, the analysis is very similar; we just
consider a bits at once.

Compared to the solution of the previous section requiring O(n log n) bits of
space and O(m + log n) counting time, we can use t = O(logσ n) to achieve the
same space complexity, so that any query of length up to t can be answered.
The structure of this section is faster than that of the previous section in this
range of m values. Compared to the faster structure requiring O(n log1+ n) bits
and O(m) counting time, our structure could answer in the same space counting
queries on patterns of length up to O(logσ n log n). The time for counting is
better than the previous structure for m = O(log log n).

712

V. M¨akinen and G. Navarro

5 Substring Rank and Select

The techniques developed for the problem of counting and locating the occurrences 
of a pattern P in T [l, r] can be used to solve the substring rank and substring 
select problem. As far as we know, this problem has not been addressed
before. It extends the rankc and selectc queries, c ∈ Σ, to strings over Σ. That
is, given s ∈ Σ
∗, ranks(T, i) is the number of occurrences of s in T [1, i], while
selects(T, j) is the initial position of the j-th occurrence of s in T .

Note that ranks(T, i) is just the number of occurrences of s in T [1, i], and
therefore it corresponds directly to a particular case of our counting queries.
On the other hand, selects(T, j) is solved by using the locating mechanism. We
detail this query now.

With the structure of Section 3 we must start with a counting query for s in
the interval [1, n]. Therefore, we end up at the unique interval [sp, ep] at the tree
root. Then, to solve selects(T, j) we must track down in the tree the position
sp + j − 1 at the tree root. Therefore, we need overall O(|s| + log n) time for
selects(T, j) (just as for ranks(T, i)), yet (cid:6) calls to selects cost O(|s| + (cid:6) log n).
It is not clear whether the more complicated O(n log1+ n) bits structure can
extract random occurrences in constant time.

Let us now consider the structure of Section 4. Once we search for s in the
tree starting with range [l, r] = [1, n], we end up at some node v (such that
s(v) = b(s)) with [lv, rv]. To solve selects(T, j) we take entry lv + j − 1 at node v
and walk the tree upwards until ﬁnding the position in the root node, and that
position is the answer. This takes overall time O(|s|(cid:7)log σ/ log log n(cid:8)) (just as for
ranks), and requires O(n|s| log σ) bits of space (or less if T is compressible).

6 A Small Experiment

We have implemented the simplest mechanism described in Section 3, and compared 
it against a brute-force solution, that is, use the plain suﬃx array to
discover the occ occurrences and then pass over those determining which are in
the right text range.

As the suﬃx array search is identical in both cases, we have focused on the
time to complete the process once the suﬃx array range is known. For counting,
the brute-force method has complexity O(occ), whereas our method in Section 3
requires O(log n) time. For locating the occurrences, the brute-force method is
still O(occ) time, while our method requires O(occl,r log n).

We tested over diﬀerent English texts, ranging from 1 to 100 megabytes.
We randomly generated subintervals of the suﬃx array of a ﬁxed length and
compared the time to pass over it counting/reporting the text positions within
some range, against using the generated suﬃx array interval as a key for the twodimensional 
search of our method. Note that the fact that the suﬃx array ranges
generated do not come from an actual search is irrelevant for the performance
of the process, and it permits us tight control over occ.

According to the experiments, our counting method becomes faster than brute
force approximately for occ > 1, 000. This did not depend signiﬁcatively on the

Position-Restricted Substring Searching

713

text size nor on the text interval [l, r] chosen. The two-dimensional search part
takes, in our method, time similar to the suﬃx array search.

For locating queries, on the other hand, our method was superior for occl,r

occ <
0.004. This is obtained when occ exceeds 1,000 by a suﬃcient margin (say, 10
times). The reason is that our method has a constant overhead that is independent 
of occl,r, so that even for occl,r = 0 the brute force method is faster for
occ < 1, 000.

The ranges of values obtained show that our method is reasonably practical,

and it wins when the query is suﬃciently selective, as expected.

7 Conclusions

We have addressed several important generalizations of well-studied problems in
string matching and succinct data structures. First, we generalized the indexed
string matching problem to position-restricted searching, where the search can
be done inside any text substring. We have obtained space and time complexities
close to those obtained for the basic problem. Second, we generalized rank and
select queries on sequences to substring rank and select, where the occurrences of
any substring s can be tracked instead of only characters. Our time complexities
are slightly over the ideal O(|s|).

It is an interesting open question whether we can close those small gaps, that
is (1) whether we can answer position-restricted counting queries in O(m) time
and locating each result in O(1) time with structures taking O(n log n) bits of
space, or even better, compressed data structures requiring O(nHk) bits of space;
and (2) whether we can answer rank and select queries for substring s in O(|s|)
time.

In addition, we have shown some interesting connections between well-known
two-dimensional range search data structures by Chazelle and recent data structures 
for compressed text indexing (the wavelet trees by Grossi et al.). We also
showed how rank queries permit implement Chazelle’s structure without using
any fractional cascading information nor binary searches.

References

1. S. Alstrup, G. Brodal, and T. Rahue. New data structures for orthogonal range
searching. In Proc. 41st IEEE Symposium on Foundations of Computer Science
(FOCS), pages 198–207, 2000.

2. A. Apostolico. The myriad virtues of subword trees. In Combinatorial Algorithms

on Words, NATO ISI Series, pages 85–96. Springer-Verlag, 1985.

3. B. Chazelle. A functional approach to data structures and its use in multidimensional 
searching. SIAM Journal on Computing, 17(3):427–462, 1988.

4. D. Clark. Compact Pat Trees. PhD thesis, University of Waterloo, 1996.
5. P. Ferragina and G. Manzini. Opportunistic data structures with applications. In
Proc. 41st IEEE Symposium on Foundations of Computer Science (FOCS), pages
390–398, 2000.

714

V. M¨akinen and G. Navarro

6. P. Ferragina and G. Manzini. Indexing compressed texts. Journal of the ACM,

52(4):552–581, 2005.

7. P. Ferragina, G. Manzini, V. M¨akinen, and G. Navarro. An alphabet-friendly
FM-index. In Proc. 11th International Symposium on String Processing and Information 
Retrieval (SPIRE), LNCS v. 3246, pages 150–160, 2004.

8. P. Ferragina, G. Manzini, V. M¨akinen, and G. Navarro. Compressed representation
of sequences and full-text indexes. Technical Report 2004-05, Technische Fakult¨at,
Universit¨at Bielefeld, Germany, December 2004. Submitted to a journal.

9. P. Ferragina, G. Manzini, V. M¨akinen, and G. Navarro. Succinct representation of
sequences. Technical Report TR/DCC-2004-5, Department of Computer Science,
University of Chile, Chile, August 2004.
ftp://ftp.dcc.uchile.cl/pub/users/gnavarro/sequences.ps.gz.

10. R. Grossi, A. Gupta, and J. Vitter. High-order entropy-compressed text indexes.
In Proc. 14th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA),
pages 841–850, 2003.

11. R. Grossi, A. Gupta, and J. Vitter. When indexing equals compression: ExperIn 
Proc. 15th Annual

iments with compressing suﬃx arrays and applications.
ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 636–645, 2004.

12. G. Jacobson. Space-eﬃcient static trees and graphs. In Proc. 30th IEEE Symp.

Foundations of Computer Science (FOCS’89), pages 549–554, 1989.

13. J. K¨arkk¨ainen. Repetition-based text indexes. PhD thesis, Dept. of Computer
Science, University of Helsinki, Finland, 1999. Also available as Report A-1999-4,
Series A.

14. U. Manber and G. Myers. Suﬃx arrays: a new method for on-line string searches.

SIAM Journal on Computing, pages 935–948, 1993.

15. G. Manzini. An analysis of the Burrows-Wheeler transform. Journal of the ACM,

48(3):407–430, 2001.

16. I. Munro. Tables. In Proc. 16th Foundations of Software Technology and Theoretical

Computer Science (FSTTCS’96), LNCS 1180, pages 37–42, 1996.

17. G. Navarro and V. M¨akinen. Compressed full-text indexes. Technical Report
TR/DCC-2005-7, Department of Computer Science, University of Chile, Chile,
June 2005. ftp://ftp.dcc.uchile.cl/pub/users/gnavarro/survcompr.ps.gz.
Submitted to a journal.

18. R. Raman, V. Raman, and S. Srinivasa Rao. Succinct indexable dictionaries with
applications to encoding k-ary trees and multisets. In Proc. 13th Annual ACMSIAM 
Symposium on Discrete Algorithms (SODA’02), pages 233–242, 2002.

