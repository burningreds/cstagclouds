5
1
0
2

 

n
u
J
 

9
2

 
 
]
S
D
.
s
c
[
 
 

2
v
8
3
4
3

.

0
1
4
1
:
v
i
X
r
a

Eﬃcient and Compact Representations of Preﬁx Codes ∗ † ‡

Travis Gagie(cid:63)

Gonzalo Navarro†

Yakov Nekrich‡

Alberto Ord´o˜nez+

(cid:63) Department of Computer Science, University of Helsinki, Finland

† Department of Computer Science, University of Chile, Chile

‡ David R. Cheriton School of Computer Science, University of Waterloo, Canada

+ Database Laboratory, Universidade da Coru˜na, Spain

Abstract

Most of the attention in statistical compression is given to the space used by the compressed
sequence, a problem completely solved with optimal preﬁx codes. However, in many applications,
 the storage space used to represent the preﬁx code itself can be an issue. In this paper we
introduce and compare several techniques to store preﬁx codes. Let N be the sequence length
and n be the alphabet size. Then a naive storage of an optimal preﬁx code uses O(n log n) bits.
Our ﬁrst technique shows how to use O(n log log(N/n)) bits to store the optimal preﬁx code.
Then we introduce an approximate technique that, for any 0 <  < 1/2, takes O(n log log(1/))
bits to store a preﬁx code with average codeword length within an additive  of the minimum.

Finally, a second approximation takes, for any constant c > 1, O(cid:0)n1/c log n(cid:1) bits to store a

preﬁx code with average codeword length at most c times the minimum. In all cases, our data
structures allow encoding and decoding of any symbol in O(1) time. We experimentally compare
our new techniques with the state of the art, showing that we achieve 6–8-fold space reductions,
at the price of a slower encoding (2.5–8 times slower) and decoding (12–24 times slower). The
approximations further reduce this space and improve the time signiﬁcantly, up to recovering
the speed of classical implementations, for a moderate penalty in the average code length. As
a byproduct, we compare various heuristic, approximate, and optimal algorithms to generate
length-restricted codes, showing that the optimal ones are clearly superior and practical enough
to be implemented.

1

Introduction

(where(cid:80)n

i=1 pi = 1), the binary empirical entropy of the text is H(P ) =(cid:80)n

Statistical compression is a well-established branch of Information Theory. Given a text T of length
N , over an alphabet of n symbols Σ = {a1, . . . , an} with relative frequencies P = (cid:104)p1, . . . , pn(cid:105) in T
i=1 pi lg(1/pi), where lg
denotes the logarithm in base 2. An instantaneous code assigns a binary code ci to each symbol ai
so that the symbol can be decoded as soon as the last bit of ci is read from the compressed stream.
∗Funded in part by Millennium Nucleus Information and Coordination in Networks ICM/FIC RC130003 for
Gonzalo Navarro, and by MINECO (PGE and FEDER) [TIN2013-46238-C4-3-R, TIN2013-47090-C3-3-P]; CDTI,
AGI, MINECO [CDTI-00064563/ITC-20133062]; ICT COST Action IC1302; Xunta de Galicia (co-founded with
FEDER) [GRC2013/053], and AP2010-6038 (FPU Program) for Alberto Ord´o˜nez.

†Early partial versions of this work appeared in Proc. SOFSEM 2010 [22] and Proc. DCC 2013 [44].
‡Corresponding author: Alberto Ord´o˜nez, email: alberto.ordonez@udc.es.

1

[28] ﬁnds a preﬁx-free set of codes ci of length (cid:96)i, such that its average length L(P ) =(cid:80)n

An optimal (or minimum-redundancy) instantaneous code (also called a preﬁx code) like Huﬀman’s
i=1 pi(cid:96)i is
minimal and satisﬁes H(P ) ≤ L(P ) < H(P ) + 1. This guarantees that the encoded text uses less
than N (H(P ) + 1) bits. Arithmetic codes achieve less space, NH(P ) + 2 bits, however they are
not instantaneous, which complicates and slows down both encoding and decoding.
In this paper we are interested in instantaneous codes. In terms of the redundancy of the code,
L(P ) − H(P ), Huﬀman codes are optimal and the topic can be considered closed. How to store
the preﬁx code itself, however, is much less studied. It is not hard to store it using O(n log n) bits,
and this is suﬃcient when n is much smaller than N . There are several scenarios, however, where
the storage of the code itself is problematic. One example is word-based compression, which is
a standard to compress natural language text [6, 37]. Word-based Huﬀman compression not only
performs very competitively, oﬀering compression ratios around 25%, but also beneﬁts direct access
[52], text searching [39], and indexing [8]. In this case the alphabet size n is the number of distinct
words in the text, which can reach many millions. Other scenarios where large alphabets arise are
the compression of East Asian languages and general numeric sequences. Yet another case arises
when the text is short, for example when it is cut into several pieces that are statistically compressed
independently, for example for compression boosting [15, 29] or for interactive communications or
adaptive compression [9]. The more eﬀectively the codes are stored, the ﬁner-grained can the text
be cut.

During encoding and decoding, the code must be maintained in main memory to achieve reasonable 
eﬃciency, whereas the plain or the compressed text can be easily read or written in streaming
mode. Therefore, the size of the code, and not that of the text, is what poses the main memory
requirements for eﬃcient compression and decompression. This is particularly stringent on mobile
devices, for example, where the supply of main memory is comparatively short. With the modern
trend of embedding small devices and sensors in all kinds of objects (e.g., the “Internet of Things”1),
those low-memory scenarios may become common.

In this paper we obtain various relevant results of theoretical and practical nature about how
to store a code space-eﬃciently, while also considering the time eﬃciency of compression and
decompression. Our speciﬁc contributions are the following.

1. In Section 3 we show that it is possible to store an optimal preﬁx code within O(n log (cid:96)max)
bits, where (cid:96)max = O(min(n, log N )) is the maximum length of a code (Theorem 1). Then
we reﬁne the space to O(n log log(N/n)) bits (Corollary 1). Within this space, encoding and
decoding are carried out in constant time on a RAM machine with word size w = Ω(log N ).
The result is obtained by using canonical Huﬀman codes [47], fast predecessor data structures
[18, 46] to ﬁnd code lengths, and multiary wavelet trees [25, 16, 5] to represent the mapping
between codewords and symbols.

2. In Section 4 we show that, for any 0 <  < 1/2, it takes O(n log log(1/)) bits to store a
preﬁx code with average codeword length at most L(P ) + . Encoding and decoding can be
carried out in constant time on a RAM machine with word size w = Ω(log n). Thus, if we
can tolerate a small constant additive increase in the average codeword length, we can store
a preﬁx code using only O(n) bits. We obtain this result by building on the above scheme,
where we use length-limited optimal preﬁx codes [35] with a carefully chosen (cid:96)max value.

1http://en.wikipedia.org/wiki/Internet of Things

2

3. In Section 5 we show that, for any constant c > 1, it takes O(cid:0)n1/c log n(cid:1) bits to store a preﬁx

code with average codeword length at most cL(P ). Encoding and decoding can be carried
out in constant time on a RAM machine with word size w = Ω(log n). Thus, if we can tolerate
a small constant multiplicative increase, we can store a preﬁx code in o(n) bits. To achieve
this result, we only store the codes that are shorter than about (cid:96)max/c, and use a simple code
of length (cid:96)max + 1 for the others. Then all but the shortest codewords need to be explicitly
represented.

4. In Section 6 we engineer and implement all the schemes above and compare them with careful
implementations of state-of-the-art optimal and suboptimal codes. Our model representations
are shown to use 6–8 times less space than classical ones, at the price of being several times
slower for compression (2.5–8 times) and decompression (12–24 times). The additive approximations 
reduce these spaces up to a half and the times by 20%–30%, at the expense
of a small increase (5%) in the redundancy. The multiplicative approximations can obtain
models of the same size of the additive ones, yet increasing the redundancy to around 10%.
In exchange, they are about as fast as the classical compression methods. If we allow them
increase the redundancy to 15%–20%, the multiplicative approximations obtain model sizes
that are orders of magnitude smaller than classical representations.

5. As a byproduct, Section 6 also compares varios heuristic, approximation, and exact algorithms
to generate length-restricted preﬁx codes. The experiments show that the optimal algorithm is
practical to implement and runs fast, while obtaining signiﬁcantly better average code lengths
than the heuristics and the approximations. A very simple-to-program approximation reaches
the same optimal average code length in our experiments, yet it runs signiﬁcantly slower.

Compared to early partial versions of this work [22, 44], this article includes more detailed explanations,
 better implementations of our exact scheme, the ﬁrst implementations of the approximate
schemes, the experimental study of the performance of algorithms that generate length-limited
codes, and stronger baselines to compare with.

2 Related Work
A simple pointer-based implementation of a Huﬀman tree takes O(n log n) bits, and it is not
diﬃcult to show this is an optimal upper bound for storing a preﬁx code with minimum average
codeword length. For example, suppose we are given a permutation π over n symbols. Let P be
the probability distribution that assigns probability pπ(i) = 1/2i for 1 ≤ i < n, and probability
pπ(n) = 1/2n−1. Since P is dyadic, every optimal preﬁx code assigns codewords of length (cid:96)π(i) = i,
for 1 ≤ i < n, and (cid:96)π(n) = n − 1. Therefore, given any optimal preﬁx code and a bit indicating
whether π(n − 1) < π(n), we can reconstruct π. Since there are n! choices for π, in the worst case
it takes Ω(log n!) = Ω(n log n) bits to store an optimal preﬁx code.

Considering the argument above, it is natural to ask whether the same lower bound holds for
probability distributions that are not so skewed, and the answer is no. A preﬁx code is canonical [47,
38] if a shorter codeword is always lexicographically smaller than a longer codeword. Given any
preﬁx code, we can always generate a canonical code with the same code lengths. Moreover, we
can reassign the codewords such that, if a symbol is lexicographically the jth with a codeword of
length (cid:96), then it is assigned the jth consecutive codeword of length (cid:96). It is clear that it is suﬃcient

3

√

to store the codeword length of each symbol to be able to reconstruct such a code, and thus the
code can be represented in O(n log (cid:96)max) bits.
There are more interesting upper bounds than (cid:96)max ≤ n. Katona and Nemetz [31] (see also
Buro [11]) showed that, if a symbol has relative frequency p, then any Huﬀman code assigns it a
codeword of length at most (cid:98)logφ(1/p)(cid:99), where φ = (1 +
5)/2 ≈ 1.618 is the golden ratio, and
thus (cid:96)max is at most (cid:98)logφ(1/pmin)(cid:99), where pmin is the smallest relative frequency in P . Note also
that, since pmin ≥ 1/N , it must hold (cid:96)max ≤ logφ N , therefore the canonical code can be stored in
O(n log log N ) bits.
Alternatively, one can enforce a value for (cid:96)max (which must be at least (cid:100)lg n(cid:101)) and pay a price
in terms of average codeword length. The same bound above [31] hints at a way to achieve any
desired (cid:96)max value: artiﬁcially increase the frequency of the least frequent symbols until the new
pmin value is over φ−(cid:96)max, and then an optimal preﬁx code built on the new frequencies will hold
the given maximum code length. Another simple technique (see, e.g., [3], where it was used for
Hu-Tucker codes) is to start with an optimal preﬁx code, and then spot all the highest nodes in
the code tree with depth (cid:96)max − d and more than 2d leaves, for any d. Then the subtrees of the
parents of those nodes are made perfectly balanced. A more sophisticated technique, by Milidi´u
and Laber [35], yields a performance guarantee. It ﬁrst builds a Huﬀman tree T1, then removes all
the subtrees rooted at depth greater than (cid:96)max, builds a complete binary tree T2 of height h whose
leaves are those removed from T1, ﬁnds the node v ∈ T1 at depth (cid:96)max − h − 1 whose subtree T3’s
leaves correspond to the symbols with minimum total probability, and ﬁnally replaces v by a new
node whose subtrees are T2 and T3. They show that the resulting average code length is at most
L(P ) + 1/φ(cid:96)max−(cid:100)lg(n+(cid:100)lg n(cid:101)−(cid:96)max)(cid:101)−1.
All these approximations require O(n) time plus the time to build the Huﬀman tree. A technique
to obtain the optimal length-restricted preﬁx code, by Larmore and Hirshberg [33], performs in
O(n (cid:96)max) time by reducing the construction to a binary version of the coin-collector’s problem.

The above is an example of how an additive increase in the average codeword length may yield
less space to represent the code itself. Another well-known additive approximation follows from
Gilbert and Moore’s proof [24] that we can build an alphabetic preﬁx code with average codeword
length less than H(P ) + 2, and indeed no more than L(P ) + 1 [41, 48]. In an alphabetic preﬁx
code, the lexicographic order of the codewords is the same as that of the source symbols, so we
need to store only the code tree and not the assignment of codewords to symbols. Any code tree,
of n − 1 internal nodes, can be encoded in 4n + o(n) bits so that it can be navigated in constant
time per operation [14], and thus encoding and decoding of any symbol takes time proportional to
its codeword length.

Multiplicative approximations have the potential of yielding codes that can be represented
within o(n) bits. Adler and Maggs [1] showed it generally takes more than (9/40)n1/(20c) lg n bits
to store a preﬁx code with average codeword length at most cH(P ). Gagie [19, 20, 21] showed that,

for any constant c ≥ 1, it takes O(cid:0)n1/c log n(cid:1) bits to store a preﬁx code with average codeword
cH(P ) + o(log n) in O(cid:0)n1/c−(cid:1) bits. Note that our result does not have the additive term “+2” in

length at most cH(P ) + 2. He also showed his upper bound is nearly optimal because, for any
positive constant , we cannot always store a preﬁx code with average codeword length at most

addition to the multiplicative term, which is very relevant on low-entropy texts.

4

(a)

(b)

Figure 1: An arbitrary canonical preﬁx code (a) and the result of sorting the source symbols at
each level (b).

3 Representing Optimal Codes

Figure 1(a) illustrates a canonical Huﬀman code. For encoding in constant time, we can simply use
an array like Codes, which stores at position i the code ci of source symbol ai, using (cid:96)max = O(log N )
bits for each. For decoding, the source symbols are written in an array Symb, in left-to-right order
of the leaves. This array requires n lg n bits. The access to this array is done via two smaller arrays,
which have one entry per level: sR[(cid:96)] points to the ﬁrst position of level (cid:96) in Symb, whereas ﬁrst[(cid:96)]

stores the ﬁrst code in level (cid:96). The space for these two arrays is O(cid:0)(cid:96)2

(cid:1) bits.

max

Then, if we have to decode the ﬁrst symbol encoded in a bitstream, we ﬁrst have to determine
its length (cid:96). In our example, if the bitstream starts with 0, then (cid:96) = 2; if it starts with 10, then
(cid:96) = 3, and otherwise (cid:96) = 4. Once the level (cid:96) is found, we read the next (cid:96) bits of the stream in ci,
and decode the symbol as ai = Symb[sR[(cid:96)] + ci − ﬁrst[(cid:96)]].
The problem of ﬁnding the appropriate entry in ﬁrst can be recast into a predecessor search
problem [23, 30]. We extend all the values ﬁrst[(cid:96)] by appending (cid:96)max − (cid:96) bits at the end. In our
example, the values become 0000 = 0, 1000 = 8, and 1100 = 12. Now, we ﬁnd the length (cid:96) of
the next symbol by reading the ﬁrst (cid:96)max bits from the stream, interpreting it as a binary number,
and ﬁnding its predecessor value in the set. Since we have only (cid:96)max = O(log N ) numbers in the
set, and each has (cid:96)max = O(log N ) bits, the predecessor search can be carried out in constant time

using fusion trees [18] (see also Patrascu and Thorup [46]), within O(cid:0)(cid:96)2

(cid:1) bits of space.

max

Although the resulting structure allows constant-time encoding and decoding, its space usage is
still O(n (cid:96)max) bits. In order to reduce it to O(n log (cid:96)max), we will use a multiary wavelet tree data
structure [25, 16]. In particular, we use the version that does not need universal tables [5, Thm. 7].
This structure represents a sequence L[1, n] over alphabet [1, (cid:96)max] using n lg (cid:96)max + o(n lg (cid:96)max)
bits, and carries out the operations in time O(log (cid:96)max/ log w). In our case, where (cid:96)max = O(w),

5

726841530000001011111111010111111100111010000101Codes 72684153SymbsR001001100firstCanonical Huffman Tree representationCanonical Huffman Tree276813450000001011111127681345001001100firstsRCanonical Huffman Tree with symbols sorted at each levelSymb42444323Lthe space is n lg (cid:96)max + o(n) bits and the time is O(1). The operations supported by wavelet trees
are the following: (1) Given i, retrieve L[i]; (2) given i and (cid:96) ∈ [1, (cid:96)max], compute rank(cid:96)(L, i),
the number of occurrences of (cid:96) in L[1, i]; (3) given j and (cid:96) ∈ [1, (cid:96)max], compute select(cid:96)(S, j), the
position in L of the j-th occurrence of (cid:96).
Assume that the symbols of the canonical Huﬀman tree are in increasing order within each
depth, as in Figure 1(b).2 Now, the key property is that Codes[i] = ﬁrst[(cid:96)] + rank(cid:96)(L, i) − 1, where
(cid:96) = L[i], which ﬁnds the code ci = Codes[i] of ai in constant time. The inverse property is useful for
decoding code ci of length (cid:96): the symbol is ai = Symb[sR[(cid:96)]+ci−ﬁrst[(cid:96)]] = select(cid:96)(L, ci−ﬁrst[(cid:96)]+1).
Therefore, arrays Codes, Symb, and sR are not required; we can encode and decode in constant
time using just the wavelet tree of L and ﬁrst, plus its predecessor structure. This completes the
result.

word size w = Ω((cid:96)max), we can store an optimal preﬁx code using n lg (cid:96)max + o(n) + O(cid:0)(cid:96)2

Theorem 1 Let P be the frequency distribution over n symbols for a text of length N , so that an
optimal preﬁx code has maximum codeword length (cid:96)max. Then, under the RAM model with computer
note that (cid:96)max ≤ logφ N . Within this space, encoding and decoding any symbol takes O(1) time.

(cid:1) bits,

max

Therefore, under mild assumptions, we can store an optimal code in O(n log log N ) bits, with
constant-time encoding and decoding operations. In the next section we reﬁne this result further.
On the other hand, note that Theorem 1 is also valid for nonoptimal preﬁx codes, as long as they
are canonical and their (cid:96)max is O(w).

We must warn the practice-oriented reader that Theorem 1 (as well as those to come) must be
understood as a theoretical result. As we will explain in Section 6, other structures with worse
theoretical guarantees perform better in practice than those chosen to obtain the best theoretical
results. Our engineered implementation of Theorem 1 reaches O(log log N ), and even O(log N ),
decoding time. It does, indeed, use much less space than previous model representations, but it is
also much slower.

4 Additive Approximation

In this section we exchange a small additive penalty over the optimal preﬁx code for an even more
space-eﬃcient representation of the code, while retaining constant-time encoding and decoding.
It follows from Milidi´u and Laber’s bound [35] that, for any  with 0 <  < 1/2, there is always
a preﬁx code with maximum codeword length (cid:96)max = (cid:100)lg n(cid:101) +(cid:100)logφ(1/)(cid:101) + 1 and average codeword
length within an additive

1

φ(cid:96)max−(cid:100)lg(n+(cid:100)lg n(cid:101)−(cid:96)max)(cid:101)−1

≤

1

φ(cid:96)max−(cid:100)lg n(cid:101)−1

≤

1

φlogφ(1/)

= 

n lg (cid:96)max + O(cid:0)n + (cid:96)2

(cid:1) bits, with constant-time encoding and decoding. In order to reduce the

of the minimum L(P ). The techniques described in Section 3 give a way to store such a code in
space, we note that our wavelet tree representation [5, Thm. 7] in fact uses nH0(L) + o(n) bits
when (cid:96)max = O(w). Here H0(L) denotes the empirical zero-order entropy of L. Then we obtain
the following result.

max

2In fact, most previous descriptions of canonical Huﬀman codes assume this increasing order, but we want to

emphasize that this is essential for our construction.

6

Theorem 2 Let L(P ) be the optimal average codeword length for a distribution P over n symbols.
Then, for any 0 <  < 1/2, under the RAM model with computer word size w = Ω(log n), we can
store a preﬁx code over P with average codeword length at most L(P ) + , using n lg lg(1/) +O(n)
bits, such that encoding and decoding any symbol takes O(1) time.

Proof. Our structure uses nH0(L) + o(n) + O(cid:0)(cid:96)2

(cid:1) bits, which is nH0(L) + o(n) because (cid:96)max =

O(log n). To complete the proof it is suﬃcient to show that H0(S) ≤ lg lg(1/) + O(1).
To see this, consider L as two interleaved subsequences, L1 and L2, of length n1 and n2,
with L1 containing those lengths ≤ (cid:100)lg n(cid:101) and L2 containing those greater. Thus nH0(L) ≤
n1H0(L1) + n2H0(L2) + n (from an obvious encoding of L using L1, L2, and a bitmap).

max

occurrences of symbol (cid:96). This completion cannot decrease n1H0(L1) =(cid:80)(cid:100)lg n(cid:101)

Let us call occ((cid:96), L1) the number of occurrences of symbol (cid:96) in L1. Since there are at most
2(cid:96) codewords of length (cid:96), assume we complete L1 with spurious symbols so that it has exactly 2(cid:96)
occ((cid:96),L1) ,
as increasing some occ((cid:96), L1) to occ((cid:96), L1) + 1 produces a diﬀerence of f (n1) − f (occ((cid:96), L1)) ≥ 0,
where f (x) = (x + 1) lg(x + 1) − x lg x is increasing. Hence we can assume L1 contains exactly 2(cid:96)
occurrences of symbol 1 ≤ (cid:96) ≤ (cid:100)lg n(cid:101); straightforward calculation then shows n1H0(L1) = O(n1).
On the other hand, L2 contains at most (cid:96)max − (cid:100)lg n(cid:101) distinct values, so H0(L2) ≤ lg((cid:96)max −
(cid:100)lg n(cid:101)), unless (cid:96)max = (cid:100)lg n(cid:101), in which case L2 is empty and n2H0(L2) = 0. Thus n2H0(L2) ≤
n2 lg((cid:100)logφ(1/)(cid:101) + 1) = n2 lg lg(1/) +O(n2). Combining both bounds, we get H0(L) ≤ lg lg(1/) +
O(1) and the theorem holds.
(cid:3)

(cid:96)=1 occ((cid:96), L1) lg

n1

In other words, under mild assumptions, we can store a code using O(n log log(1/)) bits at the
price of increasing the average codeword length by , and in addition have constant-time encoding
and decoding. For constant , this means that the code uses just O(n) bits at the price of an
arbitrarily small constant additive penalty over the shortest possible preﬁx code. Figure 2 shows
an example. Note that the same reasoning of this proof, applied over the encoding of Theorem 1,
yields a reﬁned upper bound.

Corollary 1 Let P be the frequency distribution of n symbols for a text of length N . Then, under
the RAM model with computer word size w = Ω(log N ), we can store an optimal preﬁx code for P

using n lg lg(N/n) + O(cid:0)n + log2 N(cid:1) bits, while encoding and decoding any symbol in O(1) time.

Proof. Proceed as in the proof of Theorem 2, using that (cid:96)max ≤ logφ N and putting inside L1 the
lengths up to (cid:100)logφ n(cid:101). Then n1H(L1) = O(n1) and n2H(L2) ≤ lg lg(N/n) + O(n2).
(cid:3)

5 Multiplicative Approximation

In this section we obtain a multiplicative rather than an additive approximation to the optimal
preﬁx code, in order to achieve a sublinear-sized representation of the code. We will divide the
alphabet into frequent and infrequent symbols, and store information about only the frequent ones.
Given a constant c > 1, we use Milidi´u and Laber’s algorithm [35] to build a preﬁx code
with maximum codeword length (cid:96)max = (cid:100)lg n(cid:101) + (cid:100)1/(c − 1)(cid:101) + 1 (our ﬁnal codes will have length
up to (cid:96)max + 1). We call a symbol’s codeword short if it has length at most (cid:96)max/c + 2, and long

otherwise. Notice there are S ≤ 2(cid:96)max/c+2 = O(cid:0)n1/c(cid:1) symbols with short codewords. Also, although

applying Milidi´u and Laber’s algorithm may cause some exceptions, symbols with short codewords

7

(a)

(b)

(c)

Figure 2: An example of Milidi´u and Laber’s algorithm [35]. In (a), a canonical Huﬀman tree. We
set lmax = 5 and remove all the symbols below that level (marked with the dotted line), which
yields three empty nodes (marked as black circles in the top tree). In (b), those black circles are
replaced by the deepest symbols below level lmax: 1, 8, and 10. The other symbols below (cid:96)max, 9,
13, 12 and 5, form a balanced binary tree that is hung from a new node created as the left child of
the root (in black in the middle tree). The former left child of the root becomes the left child of
this new node. Finally, in (c), we transform the middle tree into its cannonical form, but sorting
those symbols belonging to the same level in increasing order.

8

810112591371541432611810711541432611913125101578141461123591213are usually more frequent than symbols with long ones. We will hereafter call frequent/infrequent
symbols those encoded with short/long codewords.

Note that, if we build a canonical code, all the short codewords will precede the long ones. We
ﬁrst describe how to handle the frequent symbols. A perfect hash data structure [17] hash will
map the frequent symbols in [1, n] to the interval [1, S] in constant time. The reverse mapping is
done via a plain array ihash[1, S] that stores the original symbol that corresponds to each mapped
symbol. We use this mapping also to reorder the frequent symbols so that the corresponding preﬁx
in array Symb (recall Section 3) reads 1, 2, . . . , S. Thanks to this, we can encode and decode any
frequent symbol using just ﬁrst, sR, predecessor structures on both of them, and the tables hash
and ihash. To encode a frequent symbol ai, we ﬁnd it in hash, obtain the mapped symbol a(cid:48) ∈ [1, S],
ﬁnd the predecessor sR[(cid:96)] of a(cid:48) and then the code is the (cid:96)-bit integer ci = ﬁrst[(cid:96)] + a(cid:48) − sR[(cid:96)]. To
decode a short code ci, we ﬁrst ﬁnd its corresponding length (cid:96) using the predecessor structure
on ﬁrst, then obtain its mapped code a(cid:48) = sR[(cid:96)] + ci − ﬁrst[(cid:96)], and ﬁnally the original symbol is

i = ihash[a(cid:48)]. Structures hash and ihash require O(cid:0)n1/c log n(cid:1) bits, whereas sR and ﬁrst, together
with their predecessor structures, require less, O(cid:0)log2 n(cid:1) bits.

The long codewords will be replaced by new codewords, all of length (cid:96)max + 1. Let clong be the
ﬁrst long codeword and let (cid:96) be its length. Then we form the new codeword c(cid:48)
long by appending
(cid:96)max + 1 − (cid:96) zeros at the end of clong. The new codewords will be the ((cid:96)max+1)-bit integers
long + i − 1
long, c(cid:48)
c(cid:48)
(frequent symbols ai will leave unused symbols c(cid:48)

long + n − 1. An infrequent symbol ai will be mapped to code c(cid:48)

long + 1, . . . , c(cid:48)

long + i − 1). Figure 3 shows an example.

Since c > 1, we have n1/c < n/2 for suﬃciently large n, so we can assume without loss of
generality that there are fewer than n/2 short codewords,3 and thus there are at least n/2 long
codewords. Since every long codeword is replaced by at least two new codewords, the total number
long+n−1.
of new codewords is at least n. Thus there are suﬃcient slots to assign codewords c(cid:48)
To encode an infrequent symbol ai, we ﬁrst fail to ﬁnd it in table hash. Then, we assign it the
long + i − 1. To decode a long codeword, we ﬁrst read (cid:96)max + 1 bits
((cid:96)max+1)-bits long codeword c(cid:48)
into ci. If ci ≥ c(cid:48)
long, then the codeword is long, and corresponds to the source symbol aci−c(cid:48)
long+1.
Note that we use no space to store the infrequent symbols. This leads to proving our result.
Theorem 3 Let L(P ) be the optimal average codeword length for a distribution P over n symbols.
Then, for any constant c > 1, under the RAM model with computer word size w = Ω(log n), we can
such that encoding and decoding any symbol takes O(1) time.

store a preﬁx code over P with average codeword length at most cL(P ), using O(cid:0)n1/c log n(cid:1) bits,

long to c(cid:48)

Proof. Only the claimed average codeword length remains to be proved. By analysis of the algorithm
by Milidi´u and Laber [35] we can see that the codeword length of a symbol in their length-restricted
code exceeds the codeword length of the same symbol in an optimal code by at most 1, and only
when the codeword length in the optimal code is at least (cid:96)max−(cid:100)log n(cid:101)−1 = (cid:100)1/(c−1)(cid:101). Hence, the
codeword length of a frequent symbol exceeds the codeword length of the same symbol in an optimal
(cid:100)1/(c−1)(cid:101)+1
(cid:100)1/(c−1)(cid:101) ≤ c. Every infrequent symbol is encoded with a codeword of
code by a factor of at most
length (cid:96)max + 1. Since the codeword length of an infrequent symbol in the length-restricted code is
more than (cid:96)max/c + 2, its length in an optimal code is more than (cid:96)max/c + 1. Hence, the codeword
length of an infrequent symbol in our code is at most (cid:96)max+1
(cid:96)max/c+1 < c times greater than the codeword
3If this is not the case, then n = O(1), so we can use any optimal encoding: there will be no redundancy over

L(P ) and the asymptotic space formula for storing the code will still be valid.

9

(a)

(b)

(c)

Figure 3: An example of the multiplicative approximation, with n = 16 and c = 3. The tree
shown in (a) is the result of applying the algorithm of Milidi´u and Laber to a given set of codes.
Now, we set (cid:96)max = 6 according to our formula, and declare short those codewords of lengths up
to (cid:98)(cid:96)max/c(cid:99) + 2 = 4. Short codewords (above the dashed line on top) are stored unaltered but
with all symbols at each level sorted in increasing order (b). Long codewords (below the dashed
line) are extended up to length (cid:96)max + 1 = 7 and reassigned a code according to their values in the
contiguous slots of length 7 (those in gray in the middle). Thus, given a long codeword x, its code
is directly obtained as c(cid:48)
long = 11000002 is the ﬁrst code of length (cid:96)max + 1.
In (c), a representation of the hash and inverse hash to code/decode short codewords. We set the
hash size to m = 13 and h(x) = (5x + 7) mod m. We store the code associated with each cell.

long + x − 1, where c(cid:48)

10

1379164611518103215141261115810122347913141516110000081210151112345678910111206010101010111000100101100hash1261115810iHash000101000sRfirstlength of the same symbol in an optimal code. Hence, the average codeword length for our code is
(cid:3)
less than c times the optimal one.

c times the optimum, in O(cid:0)n1/c log n(cid:1) bits and allowing constant-time encoding and decoding.

Again, under mild assumptions, this means that we can store a code with average length within

6 Experimental Results

We engineer and implement the optimal and approximate code representations described above,
obtaining complexities that are close to the theoretical ones. We compare these with the best known
alternatives to represent preﬁx codes we are aware of. Our comparisons will measure the size of
the code representation, the encoding and decoding time and, in the case of the approximations,
the redundancy on top of H(P ).

6.1 Implementations

Our constant-time results build on two data structures. One is the multiary wavelet tree [16, 5].
A practical study [7] shows that multiary wavelet trees can be faster than binary ones, but require
signiﬁcantly more space (even with the better variants they design). To prioritize space, we will
use binary wavelet trees, which perform the operations in time O(log (cid:96)max) = O(log log N ).

The second constant-time data structure is the fusion tree [18], of which there are no practical
implementations as far as we know. Even implementable loglogarithmic predecessor search data
structures, like van Emde Boas trees [51], are worse than binary search for small universes like our
range [1, (cid:96)max] = [1,O(log N )]. With a simple binary search on ﬁrst we obtain a total encoding
and decoding time of O(log log N ), which is suﬃciently good for practical purposes. Even more,
preliminary experiments showed that sequential search on ﬁrst is about as good as binary search in
our test collections (this is also the case with classical representations [34]). Although sequential
search costs O(log N ) time, the higher success of instruction prefetching makes it much faster than
binary search. Thus, our experimental results use sequential search.
To achieve space close to nH0(L) in the wavelet tree, we use a Huﬀman-shaped wavelet tree
[43]. The bitmaps of the wavelet tree are represented in plain form and using a space overhead of
37.5% to support rank/select operations [42]. The total space of the wavelet tree is thus close to
1.375 · nH0(L) bits in practice. Besides, we enhance these bitmaps with a small additional index
to speed up select operations [45], which increases the constant 1.375 to at least 1.4, or more if we
want more speed. An earlier version of our work [44] recasts this wavelet tree into a compressed
permutation representation [4] of vector Symb, which leads to a similar implementation.

For the additive approximation of Section 4, we use the same implementation as for the exact
version, after modifying the code tree as described in that section. The lower number of levels will
automatically make sequence L more compressible and the wavelet tree faster.
For the multiplicative approximation of Section 5, we implement table hash with double hashing.
The hash function is of the form h(x, i) = (h1(x) + (i − 1) · h2(x)) mod m for the ith trial, where
h1(x) = x mod m, h2(x) = 1 + (x mod (m − 1)), where m is a prime number. Predecessor
searches over sR and ﬁrst are done via binary search since, as discussed above, theoretically better
predecessor data structures are not advantageous on this small domain.

11

Classical Huﬀman codes. As a baseline to compare with our encoding, we use the representation 
of Figure 1(a), using n (cid:96)max bits for Codes, n lg n bits for Symb, (cid:96)2
max bits for ﬁrst, and (cid:96)max lg n
bits for sR. For compression, the obvious constant-time solution using Codes is the fastest one. We
also implemented the fastest decompression strategies we are aware of, which are more sophisticated.
 The naive approach, dubbed TABLE in our experiments, consists of iteratively probing the
next (cid:96) bits from the compressed sequence, where (cid:96) is the next available tree depth. If the relative
numeric code resulting from reading (cid:96) bits exceeds the number of nodes at this level, we probe the
next level, and so on until ﬁnding the right length [47].

Much research has focused on impoving upon this naive approach [38, 36, 12, 49, 26, 34].
For instance, one could use an additional table that takes a preﬁx of b bits of the compressed
sequence and tells which is the minimum code length compatible with that preﬁx. This speeds up
decompression by reducing the number of iterations needed to ﬁnd a valid code. This technique
was proposed by Moﬀat and Turpin [38] and we call it TABLES in our experiments. Alternatively,
one could use a table that stores, for all the b-bit preﬁxes, the symbols that can be directly decoded
from them (if any) and how many bits those symbols use. Note this technique can be combined with
TABLES: if no symbol can be decoded, we use TABLES. In our experiments, we call TABLEE
the combination of these two techniques.

Note that, when measuring compression/decompression times, we will only consider the space
needed for compression/decompression (whereas our structure is a single one for both operations).

Hu-Tucker codes. As a representative of a suboptimal code that requires little storage space
[10], we also implement alphabetic codes, using the Hu-Tucker algorithm [27, 32]. This algorithm
takes O(n log n) time and yields the optimal alphabetic code, which guarantees an average code
length below H(P ) + 2. As the code is alphabetic, no permutation of symbols needs to be stored;
the ith leaf of the code tree corresponds to the ith source symbol. On the other hand, the tree shape
is arbitrary. We implement the code tree using succinct tree representations, more precisely the
so-called FF [2], which eﬃciently supports the required navigation operations. This representation
requires 2.37 bits per tree node, that is, 4.74n bits for our tree (which has n leaves and n − 1
internal nodes). FF represents general trees, so we convert the binary code tree into a general tree
using the well-known mapping [40]: we identify the left child of the code tree with the ﬁrst child in
the general tree, and the right child of the code tree with the next sibling in the general tree. The
general tree has an extra root node whose children are the nodes in the rightmost path of the code
tree.

With this representation, compression of symbol c is carried out by starting from the root and
descending towards the cth leaf. We use the number of leaves on the left subtree to decide whether
to go left or right. The left/right decisions made in the path correspond to the code. In the general
tree, we compute the number of nodes k in the subtree of the ﬁrst child, and then the number of
leaves in the code tree is k/2. For decompression, we start from the root and descend left or right
depending on the bits of the code. Each time we go right, we accumulate the number of leaves on
the left, so that when we arrive at a leaf the decoded symbol is the ﬁnal accumulated value plus 1.

6.2 Experimental Setup

We used an isolated AMD Phenom(tm) II X4 955 running at 800MHz with 8GB of RAM memory
and a ST3250318AS SATA hard disk. The operating system is GNU/Linux, Ubuntu 10.04, with
kernel 3.2.0-31-generic. All our implementations use a single thread and are coded in C++. The

12

Collection

EsWiki
EsInv
Indo

Length Alphabet Entropy Depth Level entr.
(H0(L))
2.24
2.60
2.51

(H(P ))
11.12
5.88
16.29

1,634,145
1,005,702
3,715,187

((cid:96)max)
28
28
27

(N )

(n)

200,000,000
300,000,000
120,000,000

Table 1: Main statistics of the texts used.

Collection

EsWiki
EsInv
Indo

Naive
(nw)

6.23 MB
3.83 MB
14.17 MB

Engineered Canonical
(n lg n)
4.02 MB
2.39 MB
9.67 MB

(n (cid:96)max)
5.45 MB
3.35 MB
11.96 MB

Ours

(nH0(L))
0.44 MB
0.31 MB
1.11 MB

Compressed

[50]

0.45 MB
0.33 MB
1.18 MB

Table 2: Rough minimum size of various model representations.

compiler is gcc version 4.6.3, with -O9 optimization. Time results refer to cpu user time. The
stream to be compressed and decompressed is read from and written to disk, using the buﬀering
mechanism of the operating system.

We use three datasets4 in our experiments. EsWiki is a sequence of word identiﬁers obtained 
by stemming the Spanish Wikipedia with the Snowball algorithm. Compressing natural
language using word-based models is a strong trend in text databases [37]. EsInv is the concatenation 
of diﬀerentially encoded inverted lists of a random sample of the Spanish Wikipedia.
These have large alphabet sizes but also many repetitions, so they are highly compressible. Finally,
 Indo is the concatenation of the adjacency lists of Web graph Indochina-2004 available at
http://law.di.unimi.it/datasets.php. Compressing adjacency lists to zero-order entropy is a
simple and useful tool for graphs with power-law degree distributions, although it is usually combined 
with other techniques [13]. We use a preﬁx of each of the sequences to speed up experiments.
Table 1 gives various statistics on the collections. Apart from N and n, we give the empirical
entropy of the sequence (H(P ), in bits per symbol or bps), the maximum length of a Huﬀman code
((cid:96)max), and the zero-order entropy of the sequence of levels (H0(L), in bps). It can be seen that
H0(L) is signiﬁcantly smaller than lg (cid:96)max, thus our compressed representation of L can indeed be
up to an order of magnitude smaller than the worst-case upper bound of n lg (cid:96)max bits.

Before we compare the exact sizes of diﬀerent representations, which depend on the extra data
structures used to speed up encoding and decoding, Table 2 gives the size of the basic data that
must be stored in each case. The ﬁrst column shows nw, the size of a naive model representation
using computer words of w = 32 bits. The second shows n (cid:96)max, which corresponds to a more
engineered representation where we use only the number of bits required to describe a codeword.
In these two, more structures are needed for decoding but we ignore them. The third column
gives n lg n, which is the main space cost of a canonical Huﬀman tree representation: basically
the permutation of symbols (diﬀerent ones for encoding and decoding). The fourth column shows
nH0(L), which is a lower bound on the size of our model representation (the exact value will depend
on the desired encoding/decoding speed). These raw numbers explain why our technique will be

4Made available in http://lbd.udc.es/research/ECRPC

13

much more eﬀective to represent the model than the typical data structures, and that we can expect
up to 7–9-fold space reductions (these will decrease to 6–8-fold on the actual structures). Indeed,
this entropy space is close to that of a sophisticated model representation [50] that can be used
only for transmitting the model in compressed form; this is shown in the last column.

6.3 Representing Optimal Codes

Figure 4 compares compression and decompression times, as a function of the space used by
the code representations, of our new data structure (COMPR) versus the table based representations 
described in Section 6.1 (TABLE, TABLES, and TABLEE). We used sampling periods of
{16, 32, 64, 128} for the auxiliary data structures added to the wavelet tree bitmaps to speed up
select operations [45], and parameter b = 14 for table based approaches (this gave us the best time
performance).

It can be seen that our compressed representations takes just around 12% of the space of
the table implementation for compression (an 8-fold reduction), while being 2.5–8 times slower.
Note that compression is performed by carrying out rank operations on the wavelet tree bitmaps.
Therefore, we do not consider the space overhead incurred to speed up select operations, and
we only plot a single point for technique COMPR at compression charts. Also, we only show
the simple (and most compact) TABLE variant, as the improvements of the others apply only to
decompression.

For decompression, our solution (COMPR) takes 17% to 45% of the space of the TABLE∗
variants (thus reaching almost a 6-fold space reduction), but it is also 12–24 times slower. This
is because our solution uses operation select for decompression, and this is slower than rank even
with the structures for speeding it up.

Overall, our compact representation is able to compress at a rate around 2.5–5 MB/sec and
decompress at 1 MB/sec, while using much less space than a classical Huﬀman implementation
(which compresses/decompresses at around 14–25 MB/sec).

Finally, note that we only need a single data structure to both compress and decompress, while
the naive approach uses diﬀerent tables for each operation. In the cases where both functionalities
are simultaneously necessary (as in compressed sequence representations [43]), our structure uses
as little as 7% of the space needed by a classical representation.

6.4 Length-Limited Codes

In the theoretical description, we refer to an optimal technique for limiting the length of the code
trees to a given value (cid:96)max ≥ (cid:100)lg n(cid:101) [33], as well as several heuristics and approximations:

• Milidi´u: the approximate technique proposed by Milidi´u and Laber [35] that nevertheless

guarantees the upper bound we have used in the paper. It takes O(n) time.

• Increase: inspired in the bounds of Katona and Nemetz [31], we start with f = 2 and set to f
the frequency of each symbol whose frequency is < f . Then we build the Huﬀman tree, and if
its height is ≤ (cid:96)max, we are done. Otherwise, we increase f by 1 and repeat the process. Since
the Huﬀman construction algorithm is linear-time once the symbols are sorted by frequency
O(n log n) time if we use exponential search to ﬁnd the correct f value. A close predecessor
of this method appears in Chapter 9 of Managing Gigabytes [52]. They use a multiplicative

and the process does not need to reorder them, this method takes O(cid:0)n log(nφ−(cid:96)max)(cid:1) =

14

Figure 4: Code representation size versus compression/decompression time for table based representations 
(TABLE, TABLES, and TABLEE) and ours (COMPR). Time (in logscale) is measured
in nanoseconds per symbol.

15

 10 100 1000 0 5 10 15 20 25 30ηs/symbolSpace (bits/alphabet symbol)Collection EsWikiCompression 10 100 1000 0 5 10 15 20 25 30ηs/symbolSpace (bits/alphabet symbol)Collection EsWikiDecompressionCOMPRTABLETABLESTABLEE 10 100 1000 0 5 10 15 20 25 30ηs/symbolSpace (bits/alphabet symbol)Collection EsInvCompression 10 100 1000 0 5 10 15 20 25 30ηs/symbolSpace (bits/alphabet symbol)Collection EsInvDecompression 10 100 1000 0 5 10 15 20 25 30ηs/symbolSpace (bits/alphabet symbol)Collection IndoCompression 10 100 1000 0 5 10 15 20 25 30ηs/symbolSpace (bits/alphabet symbol)Collection IndoDecompressioninstead of an additive approximation, so as to ﬁnd an appropriate f faster. Thus they may
ﬁnd a value of f that is larger than the optimal.

• Increase-A: analogous to Increase, but instead adds f to the frequency of each symbol.
• Balance: the technique (e.g., see [3]), that balances the parents of the maximal subtrees that,
even if balanced, exceed the maximum allowed height. It also takes O(n) time. In the case of
a canonical Huﬀman tree, this is even simpler, since only one node along the rightmost path
of the tree needs to be balanced.

• Optimal: the package-merge algorithm of Larmore and Hirshberg [33]. Its time complexity

is O(n (cid:96)max).

Figure 5 compares the techniques for all the meaningful (cid:96)max values, showing the additive redundancy 
they produce over H(P ).
It can be seen that the average code lengths obtained by
Milidi´u, although they have theoretical guarantees, are not so good in practice. They are comparable 
with those of Balance, a simpler and still linear-time heuristic, which however does not
provide any guarantee and sometimes can only return a completely balanced tree. On the other
hand, technique Increase performs better than or equal to Increase-A, and actually matches the
average code length of Optimal systematically in the three collections.

Techniques Milidi´u, Balance, and Optimal are all equally fast in practice, taking about 2 seconds 
to ﬁnd their length-restricted code in our collections. The time for Increase and Increase-A
depends on the value of (cid:96)max. For large values of (cid:96)max, they also take around 2 seconds, but this
raises up to 20 seconds when (cid:96)max is closer to (cid:100)lg n(cid:101) (and thus the value f to add is larger, up to
100–300 in our sequences).

In practice, technique Increase can be recommended for its extreme simplicity to implement
and very good approximation results. If the construction time is an issue, then Optimal should be
used. It performs fast in practice and it is not so hard to implement5. For the following experiments,
we will use the results of Optimal/Increase.

As a ﬁnal note, observe that by restricting the code length to, say, (cid:96)max = 22 on EsWiki
and EsInv and (cid:96)max = 23 on Indo, the additive redundancy obtained is below  = 0.6, and the
redundancy is below 5% of H(P ).

6.5 Approximations

Now we evaluate the additive and multiplicative approximations, in terms of average code length
L, compression and decompression performance. We compare them with two optimal model representations,
 OPT-T and OPT-C, which correspond to TABLE and COMPR of Section 6.3. The
additive approximations (Section 4) included, ADD+T and ADD+C, are obtained by restricting
the maximum code lengths to (cid:96)max and storing the resulting codes using TABLE or COMPR, respectively.
 We show one point per (cid:96)max = 22 . . . 27 on EsWiki and EsInv, and (cid:96)max = 22 . . . 26 on
Indo. For the multiplicative approximation (Section 5), we test the variants MULT-(cid:96)max, which
limit (cid:96)max to 25 and 26, and use c values 1.5, 1.75, 2, and 3. For all the solutions that use a wavelet
tree, we have ﬁxed a select sampling rate to 32.
of the code, measured as L(P )/H(P ).

Figure 6 shows the results in terms of bps for storing the model versus the resulting redundancy

5There are even some public implementations, for example https://gist.github.com/imaya/3985581

16

Figure 5: Comparison of the length-restricted approaches measured as their additive redundancy
(in logscale) over the zero-order empirical entropy, H(P ), for each value of (cid:96)max. We also include
Hu-Tucker and Huﬀman as reference points.

17

 0.01 0.1 1 102122232425262728293031L(P)-H(P)lmaxCollection EsWiki 0.01 0.1 1 10 22 23 24 25 26 27 28 29 30L(P)-H(P)lmaxCollection EsInv 0.01 0.1 1 10 22 23 24 25 26 27 28 29 30L(P)-H(P)lmaxCollection IndoMilidiúBalanceIncreaseIncrease-AOptimalHu-TuckerHuﬀmanFigure 6: Relation between the size of the model and the average code length. The x-axes are in
logscale for values smaller than 1 and in two linear scales for 1–5 and 5–35. The horizontal line
shows the limit (cid:100)lg n(cid:101)/H(P ), where no compression is obtained compared with a ﬁxed-length code.

18

11.11.53.001        .01  .1 1L(P)/H(P) 12345Model size (bps)Collection EsWiki5101520253035  11.11.53.001        .01  .1 1L(P)/H(P) 12345Model size (bps)Collection EsInv5101520253035  11.11.53.001        .01  .1 1L(P)/H(P) 12345Model size (bps)Collection indo5101520253035  OPT-TOPT-CADD+CADD+TMULT-26MULT-25The additive approximations have a mild impact when implemented in classical form. However,
the compact representation, ADD+C, reaches half the space of our exact compact representation,
OPT-C. This is obtained at the price of a modest redundancy, below 5% in all cases, if one uses
reasonable values for (cid:96)max.

With the larger c values, the multiplicative approach is extremely eﬃcient for storing the model,
reaching reductions up to 2 and 3 orders of magnitude with respect to the classic representations.
However, this comes at the price of a redundancy that can reach 50%. The redundancy may go
beyond (cid:100)lg n(cid:101)/H(P ), at which point it is better to use a plain code of (cid:100)lg n(cid:101) bits. Instead, with
value c = 1.75, the model size is still 20 times smaller than a classical representation, and 2–3 times
smaller than the most compact representation of additive approximations, with a redundancy only
slightly over 10%.

Figure 7 compares these representations in terms of compression and decompression performance.
 The numbers near each point show the redundancy (as a percentage over the entropy) of
the model representing that point. We use ADD+C with values (cid:96)max = 22 on EsWiki and EsInv
and (cid:96)max = 23 on Indo. For ADD+T, the decompression times are the same for all the tested
(cid:96)max values. In this ﬁgure we set the select samplings of the wavelet trees to (32, 64, 128). We also
include in the comparison the variant MULT-26 with c = 1.75 and 1.5.

It can be seen that the multiplicative approach is very fast, comparable to the table-based
approaches ADD+T and OPT-T: 10%–50% slower at compression and at most 20% slower at
decompression. Within this speed, if we use c = 1.75, the representation is 6–11 times smaller than
the classical one for compression and 5–9 times for decompression, at the price of about 10% of
redundancy. If we choose c = 1.5, the redundancy increases to about 20% but the model becomes
an order of magnitude smaller.

The compressed additive approach (ADD+C) achieves a smaller model than the multiplicative
one with c = 1.75 (it is 14 times smaller than the classical representation for compression and 11
times for decompression). This is achieved with signiﬁcantly less redundancy than the multiplicative
model, just 3%–5%. However, although ADD+C is about 20%–30% faster than the exact code
OPT-C, it is still signiﬁcantly slower than the table-based representations (2–5.5 times slower for
compression and 9–17 for decompression).

Finally, we can see that our compact implementation of Hu-Tucker codes achieves competitive
space, but it is an order of magnitude slower than our additive approximations, which can always use
simultaneously less space and time. With respect to the redundancy, Figure 5 shows that Hu-Tucker
codes are equivalent to our additive approximations with (cid:96)max = 23 on EsWiki, (cid:96)max = 22 on EsInv,
and (cid:96)max = 24 on Indo. This shows that the use of alphabetic codes as a suboptimal code to reduce
the model representation size is inferior, in all aspects, to our additive approximations. Figure 7
shows that Hu-Tucker is also inferior, in the three aspects, to our compact optimal codes, OPT-C.
We remark that alphabetic codes are interesting by themselves for other reasons, in contexts where
preserving the order of the source symbols is important.

7 Conclusions

We have explored the problem of providing compact representations of Huﬀman models. The
model size is relevant in several applications, particularly because it must reside in main memory
for eﬃcient compression and decompression.

We have proposed new representations achieving constant compression and decompression time

19

Figure 7: Space/time performance of the approximate and exact approaches. Times are in nanoseconds 
per symbol and in logscale. The numbers around the points are their redundancy as a percentage 
of the entropy.

20

 10 100 1000 10000 0 5 10 15 20 25 30ηs/symbolModel size (bps)Collection EsWikiCompression0.30.350.5121372 10 100 1000 10000 0 5 10 15 20 25ηs/symbolModel size (bps)Collection EsWikiDecompression0.30.350.51372 10 100 1000 10000 0 5 10 15 20 25 30ηs/symbolModel size (bps)Collection EsInvCompression0.30.330.511.517123 10 100 1000 10000 0 5 10 15 20 25ηs/symbolModel size (bps)Collection EsInvDecompression0.30.330.517123 10 100 1000 10000 0 5 10 15 20 25 30ηs/symbolModel size (bps)Collection IndoCompression0.10.130.5131581 10 100 1000 10000 0 5 10 15 20 25ηs/symbolModel size (bps)Collection IndoDecompression0.10.130.51581OPT-TOPT-CADD+C+22ADD+C+23ADD+T+23,24,25MULT-26 Hu-Tuckerper symbol while using O(n log log(N/n)) bits per symbol, where n is the alphabet size and N the
sequence length. This is in contrast to the (at least) O(n log n) bits used by previous representations.
In our practical implementation, the time complexities are O(log log N ) and even O(log N ), but
we do achieve 8-fold space reductions for compression and up to 6-fold for decompression. This
comes, however, at the price of increased compression and decompression time (2.5–8 times slower
at compression and 12–24 at decompression), compared to current representations. In low-memory
scenarios, the space reduction can make the diﬀerence between ﬁtting the model in main memory
or not, and thus the increased times are the price to pay.
We also showed that, by tolerating a small additive overhead of  on the average code length,
the model can be stored in O(n log log(1/)) bits, while maintaining constant compression and
decompression time. In practice, these additive approximations can halve our compressed model
size (becoming 11–14 times smaller than a classical representation), while incurring a very small
increase (5%) in the average code length. They are also faster, but still 2–5.5 times slower for
compression and 5–9 for decompression.

Finally, we showed that a multiplicative penalty in the average code length allows storing the
model in o(n) bits. In practice, the reduction in model size is sharp, while the compression and
decompression times are only 10%–50% and 0%–20% slower, respectively, than classical implementations.
 Redundancies are higher, however. With 10% of redundancy, the model size is close to
that of the additive approach, and with 20% the size decreases by another order of magnitude.

Some challenges for future work are:
• Adapt these representations to dynamic scenarios, where the model undergoes changes as
compression/decompression progresses. While our compact representations can be adapted
to support updates, the main problem is how to eﬃciently maintain a dynamic canonical
Huﬀman code. We are not aware of such a technique.

• Find more eﬃcient representations of alphabetic codes. Our baseline achieves reasonably good
space, but the navigation on the compact tree representations slows it down considerably. It is
possible that faster representations supporting left/right child and subtree size can be found.
• Find constant-time encoding and decoding methods that are fast and compact in practice.
Multiary wavelet trees [7] are faster than binary wavelet trees, but generally use much more
space. Giving them the shape of a (multiary) Huﬀman tree and using plain representations
for the sequences in the nodes could reduce the space gap with our binary Huﬀman-shaped
wavelet trees used to represent L. As for the fusion trees, looking for a practical implementation 
of trees with arity w, which outperforms a plain binary search, is interesting not only
for this problem, but in general for predecessor searches on small universes.

Acknowledgements. We thank the reviewers, whose comments helped improve the paper signiﬁcantly.


References

[1] M. Adler and B. M. Maggs. Protocols for asymmetric communication channels. Journal of

Computer and System Sciences, 63(4):573–596, 2001.

21

[2] D. Arroyuelo, R. C´anovas, G. Navarro, and K. Sadakane. Succinct trees in practice. In Proc.

11th Workshop on Algorithm Engineering and Experiments (ALENEX), pages 84–97, 2010.

[3] J. Barbay and G. Navarro. Compressed representations of permutations, and applications. In
Proc. 26th International Symposium on Theoretical Aspects of Computer Science (STACS),
pages 111–122, 2009.

[4] J. Barbay and G. Navarro. On compressing permutations and adaptive sorting. Theoretical

Computer Science, 513:109–123, 2013.

[5] D. Belazzougui and G. Navarro. New lower and upper bounds for representing sequences. In
Proc. 20th Annual European Symposium on Algorithms (ESA), LNCS 7501, pages 181–192,
2012.

[6] J. L. Bentley, D. D. Sleator, R. E. Tarjan, and V. K. Wei. A locally adaptive data compression

scheme. Communications of the ACM, 29(4), 1986.

[7] A. Bowe. Multiary Wavelet Trees in Practice. Honours thesis, RMIT University, Australia,

2010.

[8] N. Brisaboa, A. Fari˜na, S. Ladra, and G. Navarro. Implicit indexing of natural language text

by reorganizing bytecodes. Information Retrieval, 15(6):527–557, 2012.

[9] N. Brisaboa, A. Fari˜na, G. Navarro, and J. Param´a. Lightweight natural language text compression.
 Information Retrieval, 10:1–33, 2007.

[10] N. Brisaboa, G. Navarro, and A. Ord´o˜nez. Smaller self-indexes for natural language. In Proc.
19th International Symposium on String Processing and Information Retrieval (SPIRE), LNCS
7608, pages 372–378, 2012.

[11] M. Buro. On the maximum length of Huﬀman codes.

Information Processing Letters,

45(5):219–223, 1993.

[12] Y. Choueka, S. T. Klein, and Y. Perl. Eﬃcient variants of Huﬀman codes in high level
languages. In Proc. 8th Annual International ACM Conference on Research and development
in Information Retrieval (SIGIR), pages 122–130. ACM, 1985.

[13] F. Claude and G. Navarro. Fast and compact Web graph representations. ACM Transactions

on the Web, 4(4):article 16, 2010.

[14] P. Davoodi, R. Raman, and S. Rao Satti. Succinct representations of binary trees for range
minimum queries. In Proc. 18th Annual International Conference on Computing and Combinatorics 
(COCOON), LNCS 7434, pages 396–407, 2012.

[15] P. Ferragina, R. Giancarlo, G. Manzini, and M. Sciortino. Boosting textual compression in

optimal linear time. Journal of the ACM, 52(4):688–713, 2005.

[16] P. Ferragina, G. Manzini, V. M¨akinen, and G. Navarro. Compressed representations of sequences 
and full-text indexes. ACM Transactions on Algorithms, 3(2):article 20, 2007.

22

[17] M. L. Fredman, J. Koml´os, and E. Szemer´edi. Storing a sparse table with O(1) worst case

access time. Journal of the ACM, 31(3):538–544, 1984.

[18] M. L. Fredman and D. E. Willard. Surpassing the information theoretic bound with fusion

trees. Journal of Computer and System Sciences, 47(3):424–436, 1993.

[19] T. Gagie. Compressing probability distributions. Information Processing Letters, 97(4):133–

137, 2006.

[20] T. Gagie. Large alphabets and incompressibility. Information Processing Letters, 99(6):246–

251, 2006.

[21] T. Gagie. Dynamic asymmetric communication. Information Processing Letters, 108(6):352–

355, 2008.

[22] T. Gagie, G. Navarro, and Y. Nekrich. Fast and compact preﬁx codes. In Proc. 36th International 
Conference on Current Trends in Theory and Practice of Computer Science (SOFSEM),
LNCS 5901, pages 419–427, 2010.

[23] T. Gagie and Y. Nekrich. Worst-case optimal adaptive preﬁx coding. In Proc. 9th Symposium

on Algorithms and Data Structures (WADS), pages 315–326, 2009.

[24] E. N. Gilbert and E. F. Moore. Variable-length binary encodings. Bell System Technical

Journal, 38:933–967, 1959.

[25] R. Grossi, A. Gupta, and J. Vitter. High-order entropy-compressed text indexes. In Proc. 14th

ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 841–850, 2003.

[26] R. Hashemian. Memory eﬃcient and high-speed search Huﬀman coding. Communications,

IEEE Transactions on, 43(10):2576–2581, 1995.

[27] T. C. Hu and A. C. Tucker. Optimal computer search trees and variable-length alphabetical

codes. SIAM Journal of Applied Mathematics, 21(4):514–532, 1971.

[28] D. Huﬀman. A method for the construction of minimum-redundancy codes. Proceedings of

the I.R.E., 40(9):1090–1101, 1952.

[29] J. K¨arkk¨ainen and S. J. Puglisi. Fixed block compression boosting in FM-indexes. In Proc.
18th International Symposium on String Processing and Information Retrieval (SPIRE), pages
174–184, 2011.

[30] M. Karpinski and Y. Nekrich. A fast algorithm for adaptive preﬁx coding. Algorithmica,

55(1):29–41, 2009.

[31] G. O. H. Katona and T. O. H. Nemetz. Huﬀman codes and self-information. IEEE Transactions

on Information Theory, 22(3):337–340, 1976.

[32] D. E. Knuth. The Art of Computer Programming. Vol. 3: Sorting and Searching. AddisonWesley,
 1973.

[33] L. L. Larmore and D. S. Hirschberg. A fast algorithm for optimal length-limited Huﬀman

codes. Journal of the ACM, 37(3):464–473, 1990.

23

[34] M. Liddell and A. Moﬀat. Decoding preﬁx codes. Software: Practice and Experience,

36(15):1687–1710, 2006.

[35] R. L. Milidi´u and E. S. Laber. Bounding the ineﬃciency of length-restricted preﬁx codes.

Algorithmica, 31(4):513–529, 2001.

[36] R. L. Milidi´u, E. S. Laber, L. O. Moreno, and J. C Duarte. A fast decoding method for preﬁx

codes. In Proc. 13th Data Compression Conference (DCC), page 438, 2003.

[37] A. Moﬀat. Word-based text compression. Software Practice and Experience, 19(2):185–198,

1989.

[38] A. Moﬀat and A. Turpin. On the implementation of minimum-redundancy preﬁx codes. IEEE

Transactions on Communications, 45(10):1200–1207, 1997.

[39] E. Moura, G. Navarro, N. Ziviani, and R. Baeza-Yates. Fast and ﬂexible word searching on

compressed text. ACM Transactions on Information Systems, 18(2):113–139, 2000.

[40] J. I. Munro and V. Raman. Succinct representation of balanced parentheses and static trees.

SIAM Journal on Computing, 31(3):762–776, 2001.

[41] N. Nakatsu. Bounds on the redundancy of binary alphabetical codes. IEEE Transactions on

Information Theory, 37(4):1225–1229, 1991.

[42] G. Navarro. Implementing the LZ-index: Theory versus practice. ACM Journal of Experimental 
Algorithmics (JEA), 13:article 2, 2009.

[43] G. Navarro. Wavelet trees for all. Journal of Discrete Algorithms, 25:2–20, 2014.

[44] G. Navarro and A. Ord´o˜nez. Compressing Huﬀman models on large alphabets. In Proc. 23rd

Data Compression Conference (DCC), pages 381–390, 2013.

[45] G. Navarro and E. Providel. Fast, small, simple rank/select on bitmaps. In Proc. 11th International 
Symposium on Experimental Algorithms (SEA), LNCS 7276, pages 295–306, 2012.

[46] M. Patrascu and M. Thorup.

Time-space trade-oﬀs for predecessor search.

CoRR,

cs/0603043v1, 2008. http://arxiv.org/pdf/cs/0603043v1.

[47] E. S. Schwarz and B. Kallick. Generating a canonical preﬁx encoding. Communications of the

ACM, 7(3):166–169, 1964.

[48] D. Sheinwald. On binary alphabetic codes. In Proc. 2nd Data Compression Conference (DCC),

pages 112–121, 1992.

[49] A. Siemi´nski. Fast decoding of the Huﬀman codes. Information Processing Letters, 26(5):237–

241, 1988.

[50] A. Turpin and A. Moﬀat. Housekeeping for preﬁx coding. IEEE Transactions on Communications,
 48(4):622–628, 2000.

[51] P. van Emde Boas, R. Kaas, and E. Zijlstra. Design and implementation of an eﬃcient priority

queue. Mathematical Systems Theory, 10:99–127, 1977.

24

[52] I. H. Witten, A. Moﬀat, and T. C. Bell. Managing Gigabytes: Compressing and Indexing

Documents and Images. Morgan Kaufmann, 2nd edition, 1999.

25

