Algorithmica (2003) 35: 347–369
DOI: 10.1007/s00453-002-1005-2

Algorithmica

© 2003 Springer-Verlag New York Inc.

Approximate Matching of Run-Length

Compressed Strings

Veli M¨akinen,1 Gonzalo Navarro,2 and Esko Ukkonen1

Abstract. We focus on the problem of approximate matching of strings that have been compressed using
run-length encoding. Previous studies have concentrated on the problem of computing the longest common
subsequence (LCS) between two strings of length m and n, compressed to m
runs. We extend an
n + n
(cid:1)
m) complexity. Furthermore,
existing algorithm for the LCS to the Levenshtein distance achieving O(m
we extend this algorithm to a weighted edit distance model, where the weights of the three basic edit operations
can be chosen arbitrarily. This approach also gives an algorithm for approximate searching of a pattern of m
(cid:1)) time. Then we propose improvements for a greedy
letters (m
(cid:1)) expected case complexity.
algorithm for the LCS, and conjecture that the improved algorithm has O(m
Experimental results are provided to support the conjecture.

runs) in a text of n letters (n

and n

runs) in O(mm

n

(cid:1)

n

(cid:1)

(cid:1)

(cid:1)

(cid:1)

(cid:1)

(cid:1)

Key Words. Compressed pattern matching, Run-length encoding, Levenshtein distance, Longest common
subsequence, Weighted edit distance.

1. Introduction. The problem of compressed pattern matching is, given a compressed
text T and a (possibly compressed) pattern P, to ﬁnd all occurrences of P in T without
decompressing T (and P). The goal is to search faster than by using the basic scheme:
decompression followed by a search.

In the basic approach, we are interested in reporting only the exact occurrences, i.e.
the locations of the substrings of T that match pattern P exactly. We can loosen the
requirement of exact occurrences to approximate occurrences by introducing a distance
function to measure the similarity between P and its occurrence in T . Now, we want
to ﬁnd all the approximate occurrences of P in T , where the distance between P and
a substring of T is at most a given error threshold k. Often a suitable distance measure
between two strings is the edit distance, deﬁned as the minimum amount of character
insertions, deletions and substitutions that are needed to make the two strings equal. For
this distance we are interested in k < |P| errors.

Many studies have been made around the subject of compressed pattern matching
over different compression formats, starting with the work of Amir and Benson [1],
e.g. [2], [10], [15] and [16]. The only works addressing the approximate variant of the
problem have been [14], [20], [22], on Ziv–Lempel [27].

1 Department of Computer Science, P.O. Box 26 (Teollisuuskatu 23), FIN-00014 University of Helsinki,
Finland. {vmakinen,ukkonen}@cs.helsinki.ﬁ. Supported by the Academy of Finland under Grant 22584.
2 Center for Web Research, Department of Computer Science, University of Chile, Blanco Encalada 2120,
Santiago, Chile. gnavarro@dcc.uchile.cl. Supported by Millenium Nucleus Center for Web Research, Grant
P01-029-F, Mideplan, Chile.

Received September 10, 2001; revised July 5, 2002. Communicated by T. Nishizeki.
Online publication January 13, 2003.

348

V. M¨akinen, G. Navarro, and E. Ukkonen

(cid:1)

(cid:1)

and n

Our focus is approximate matching over run-length encoded strings. In run-length encoding,
 a string that consists of repetitions of letters is compressed by encoding each repetition 
as a pair (“letter,” “length of the repetition”). For example, string aaabbbbccaab
is encoded as a sequence (a, 3)(b, 4)(c, 2)(a, 2)(b, 1). This technique is widely used,
especially in image compression, where repetitions of pixel values are common. This is
particularly interesting for fax transmissions and bilevel images. Approximate matching
on images can be a useful tool to handle distortions. Even a one-dimensional compressed
approximate matching algorithm would be useful to speed up existing two-dimensional
approximate matching algorithms, e.g. [17] and [6].
(cid:1)+
Exact pattern matching over run-length encoded text can be done optimally in O(m
(cid:1)) time, where m
are the compressed sizes of the pattern and the text [1].
n
Approximate pattern matching over run-length encoded text has not been considered
before this study, but there has been work on the distance calculation, namely, given two
strings of length m and n that are run-length compressed to lengths m
, calculate
their distance using the compressed representations of the strings. This problem was ﬁrst
posed by Bunke and Csirik [7]. They considered the version of edit distance without
the replacement operation, which is related to the problem of calculating the longest
(cid:1)) time algorithm for a
common subsequence (LCS) of two strings. They gave an O(m
special case of the problem, where all run-lengths are of equal size. Later, they gave an
m) time algorithm for the general case [8]. A major improvement over the
O(m
(cid:1)))
previous results was due to Apostolico et al. [4]. They ﬁrst gave a basic O(m
(cid:1))). Mitchell [21] gave an algorithm
algorithm, and further improved it to O(m
n
with the same time complexity in the worst case, but faster with some inputs. Its time
complexity is O(( p + m
(cid:1))), where p is the amount of pairs
of compressed characters that match ( p equals the amount of equal letter boxes, see
the deﬁnition in Section 2.2). All these algorithms were limited to the LCS distance,
although Mitchell’s method [21] could be applied when different costs are assigned to
the insertion and deletion operations. It still remained an open question (as posed by
Bunke and Csirik) whether similar improvements could be found for a more general set
of edit operations and their costs.

n
(cid:1)) log( p + m

log(m
(cid:1) + n

n + n

(cid:1) + n

(cid:1)+n

and n

(cid:1)(m

n

n

(cid:1)

(cid:1)

(cid:1)

(cid:1)

(cid:1)

(cid:1)

(cid:1)

(cid:1)

(cid:1)

We give an algorithm for computing the Levenshtein distance [18] between two
strings. In the Levenshtein distance a unit cost is assigned to each of the three edit
operations. The algorithm is an extension of the O(m
m) algorithm of Bunke
and Csirik [8]. We keep the same cost but generalize the algorithm to handle a more
complex distance model. Independently from our work, Arbell et al. have found a similar
algorithm [5].

n + n

(cid:1)

(cid:1)

We manage to extend the O(m

m) algorithm also to a weighted edit distance

(cid:1)

n + n

(cid:1)

model, where the costs for the three operations can be chosen arbitrarily.

We modify our algorithm to work in a context of approximate pattern matching, and
(cid:1)) time for searching a pattern of length m that is run-length compressed

n

(cid:1)

achieve O(mm
to length m

(cid:1)

, in a run-length compressed text of length n

.

(cid:1)

(cid:1)

We also study the LCS calculation. First, we give a (partially) greedy algorithm for
(cid:1))) time. Adapting the well-known diagonal method
the LCS that works in O(m
(cid:1))) time, where
[24], we are able to improve the greedy method to work in O(d2 min(n
d is the edit distance between the two strings (under insertions and deletions with the
unit cost model).

(cid:1)+n

(cid:1), m

(cid:1)(m

n

Approximate Matching of Run-Length Compressed Strings

349

Then we present improvements for the greedy method for the LCS, which do not
however affect the worst case, but do have an effect on the average case. We end up
(cid:1)) time on average. As we are unable
conjecturing that our improved algorithm is O(m
to prove it, we provide instead experimental evidence to support the conjecture.

n

(cid:1)

This paper is an extended version of a conference paper [19]. The weighted edit
distance computation was developed after the conference version. Motivated by our open
question in that paper, Crochemore et al. [9] noticed that their subquadratic sequence
alignment algorithm for unrestricted cost matrices could be generalized to this problem;
they obtained the same O(m
m) bound using completely different techniques from
ours.

n+n

(cid:1)

(cid:1)

2. Edit Distance on Run-Length Compressed Strings

··· aik , where 1 ≤ i1 < i2 ··· < ik ≤ m.

2.1. Edit Distance. Let  be a ﬁnite set of symbols, called an alphabet. A string A of
length |A| = m is a sequence of symbols in , denoted by A = A1···m = a1a2 ··· am,
where ai ∈  for every i. If |A| = 0, then A = λ is an empty string. A subsequence of
A is any sequence ai1ai2
The edit distance D(A, B) can be used to measure the similarity between two strings
A = a1a2 ··· am and B = b1b2 ··· bn by calculating the minimum cost of edit operations
that are needed to convert A into B [18], [26], [23]. The usual edit operations are
substitution (convert ai into bj , denoted by ai → bj ), insertion (λ → bj ) and deletion
(ai → λ). Different costs for edit operations can be given depending on the letters
involved. We deﬁne a nonnegative function δ that assigns a cost to each of the above
operations. The cost to convert A into B must be a distance, which holds whenever δ
is strictly positive (δ(x → y) = 0 ⇔ x = y) symmetric (δ(x → y) = δ(y → x))
and satisﬁes the triangle inequality (δ(x → y) + δ(y → z) ≥ δ(x → z)) for every
x, y, z ∈  ∪ {λ}.
For the Levenshtein distance (denoted by DL(A, B)) [18], we assign costs δ(a →
a) = 0, δ(a → b) = 1, δ(a → λ) = 1 and δ(λ → a) = 1, for all a, b ∈ , a (cid:14)= b. If
substitutions are forbidden, i.e. δ(a → b) = ∞, we get the distance DID(A, B).
In general, the edit distance D(A, B) with arbitrary δ costs can be calculated by using
dynamic programming [23]; evaluating an (m + 1) × (n + 1) matrix (dij), 0 ≤ i ≤ m,
0 ≤ j ≤ n, using the initial value d0,0 = 0 and the recurrence
(1)
where d is assumed to take the value ∞ when accessed outside its bounds. The matrix
(dij) can be evaluated row-by-row or column-by-column in O(mn) time, and the value
dmn equals D(A, B).

di, j = min(di−1, j + δ(ai → λ), di, j−1 + δ(λ → bj ), di−1, j−1 + δ(ai , bj )),

The distance DL(A, B) is obtained as a particular case using recurrence:

(2)

di, j = min(di−1, j + 1, di, j−1 + 1, di−1, j−1 + if ai = bj

then 0 else 1).

The recurrence for DID(A, B) is

(3)

di, j = min(di−1, j + 1, di, j−1 + 1, di−1, j−1 + if ai = bj

then 0 else ∞).

350

V. M¨akinen, G. Navarro, and E. Ukkonen

The problem of calculating the longest common subsequence of strings A and B
(denoted by LCS(A, B)) is related to the distance DID(A, B). It is easy to see that
2∗|LCS(A, B)| = m + n − DID(A, B). Also, the sequence LCS(A, B) can be extracted
using recurrence (3) if the optimal path is stored in the matrix: in each cell dij a link is
stored to the cell that gives the minimum value in the recurrence (3). Now, LCS(A, B)
is the concatenation of symbols ai (or alternatively bj ) that correspond to cells dij in the
optimal path from d00 to dmn that have diagonalwise links. (In fact there may be more
than one optimal path yielding different LCSs of the same length.)

2.2. Dividing the Edit Distance Matrix into Boxes. A run-length encoding of the
string A = a1a2 ··· am is A
(cid:1) = (a1, p1)(ap1+1, p2)(ap1+ p2+1, p3)··· (am− pm(cid:1)+1, pm(cid:1) ) =
, p2)··· (aim(cid:1) , pm(cid:1) ), where (aik
=
(ai1
, p1)(ai2
of length|αk| = pk. We also call (aik
a pk
, pk ) a run of aik . String A is optimally run-length
(cid:14)= aik+1 for all 1 ≤ k < m
(cid:1)
ik
.
encoded if aik

, pk ) denotes a sequence αk = aik aik

··· aik

Let A

(cid:1) = (ai1

, p1)(ai2

In the next sections we show how to speed up the evaluation of values dmn for both
distances DL(A, B) and DID(A, B) when both strings A and B are run-length encoded.
We generalize to D(A, B) as well. In all the methods, we use the following notation to
divide the matrix (dij) into submatrices (see Figure 1).
, r2)··· (bjn(cid:1) , rn(cid:1) ) be
the run-length encoded representations of strings A and B. The rows and columns that
correspond to the ends of runs in A and B divide the edit distance matrix (dij) into
submatrices. To ease the notation later on, we deﬁne the submatrices so that they overlap
, r(cid:11)) deﬁnes a ( pk +1)× (r(cid:11)+1)
on the borders. Formally, each pair of runs (aik

, p2)··· (aim(cid:1) , pm(cid:1) ) and B

(cid:1) = (bj1

, pk ), (bj(cid:11)

, r1), (bj2

DP matrix

aaa

a b b b b b b c

c c c c

b b

11
d

21
d

31
d

...

a
a
a
b
b
b
b
b
a
a
a
a
a
a
b
b

12

d

22

d

32
d

...

13

d

23

d

33

d

...

...

...

...

...

overlapping borders of boxes
corners

one particular "box"

dkl
rl0

dkl
rl1
dkl
rl2

dkl
pk rl

pk +1

dkl
01

dkl
02

00dkl
dkl
10
dkl
20

kl

d

dkl
pk 0

pkdkl

1 dkl
pk 2

rl +1

equal letter box

different letter box

Fig. 1. A dynamic programming matrix split into run-length blocks.

Approximate Matching of Run-Length Compressed Strings

351

submatrix (dk,(cid:11)

s,t ) such that

(4)

= dik+s−1, j(cid:11)+t−1,

dk,(cid:11)
s,t

0 ≤ s ≤ pk ,

0 ≤ t ≤ r(cid:11).

We call submatrices (dk,(cid:11)

= bj(cid:11)), then (dk,(cid:11)

s,t ) boxes. If a pair of runs corresponding to a box contains equal
s,t ) is called an equal letter box. Otherwise we call (dk,(cid:11)
letters (i.e. aik
s,t )
a different letter box. Adjacent boxes can form runs of different letter boxes along rows
and columns. We assume that both strings are optimally run-length encoded, and hence
runs of equal letter boxes cannot occur. (If the strings are not optimally encoded, they
(cid:1)) time by joining adjacent
can easily be converted into optimally encoded in O(m
runs of equal letters. This cost is negligible compared with those of our algorithms.)

(cid:1) + n

(cid:1)

(cid:1) + m

3. An O(mn(cid:1) + m(cid:1)n) Algorithm for the Levenshtein Distance. Bunke and Csirik [8]
gave an O(mn
n) time algorithm for computing the LCS between two strings of
lengths n and m run-length compressed to n
. They pose it as an open problem
extending their algorithm to the Levenshtein distance. This is what we do in this section,
without increasing the complexity to compute the new distance DL. Arbell et al. [5] have
independently found a similar algorithm. Their solution is also based on the same idea
of extending the O(mn

n) LCS algorithm to the Levenshtein distance.

(cid:1) + m

and m

(cid:1)

(cid:1)

(cid:1)

Compared with the LCS-related distance DID, the Levenshtein distance DL permits an
additional character substitution operation, at cost 1. We compute DL(A, B) by ﬁlling all
the borders of all the boxes (dk,(cid:11)
s,t ) (see Figure 1). We manage to ﬁll each cell in constant
time, which adds up the promised O(mn
n) complexity. The space complexity can
be made O(n + m) by processing the matrix rowwise or columnwise.

(cid:1) + m

(cid:1)

3.1. Basic Algorithm. We start with two lemmas that characterize the relationships
between the border values in the boxes (dk,(cid:11)
s,t ). First, we consider the equal letter boxes:

LEMMA 1 [8]. The recurrences (2) and (3) can be replaced by

= if s ≤ t then dk,(cid:11)
(5)
where 1 ≤ s ≤ pk and 1 ≤ t ≤ r(cid:11), for values dk,(cid:11)

dk,(cid:11)
s,t

s,t

0,t−s else dk,(cid:11)
s−t,0

,

in an equal letter box.

Note that Lemma 1 holds for both Levenshtein and LCS distance models, because
formulas (2) and (3) are equal when ai = bj . Since we are computing all the cells in
the borders of the boxes, Lemma 1 permits computing new box borders in constant time
using those of previous boxes.

The difﬁcult part lies in the different letter boxes.

LEMMA 2. The recurrence (2) can be replaced by

dk,(cid:11)
s,t

= 1 + min(t − 1 +

dk,(cid:11)
(6)
q,0
where 1 ≤ s ≤ pk and 1 ≤ t ≤ r(cid:11), for values dk,(cid:11)

max(0,s−t )≤q≤s

min

s,t

, s − 1 +

min

max(0,t−s)≤q≤t

dk,(cid:11)
0,q

),

in a different letter box.

352

V. M¨akinen, G. Navarro, and E. Ukkonen
PROOF. We use induction on s + t. If s + t = 2, then formula (6) becomes dk,(cid:11)
=
1 + min(dk,(cid:11)
), which matches recurrence (2). In the inductive case we have

1,1

, dk,(cid:11)
1,0

, dk,(cid:11)
0,1

0,0

= 1 + min(dk,(cid:11)

s−1,t−1

dk,(cid:11)
s,t

, dk,(cid:11)
s−1,t

, dk,(cid:11)

s,t−1

)

by recurrence (2), and using the induction hypothesis we get
, s − 2 +

= 2 + min(min(t − 2 +

min

dk,(cid:11)
s,t

max(0,s−t )≤q≤s−1

min

max(0,t−s)≤q≤t−1

dk,(cid:11)
0,q

),

min(t − 1 +
min(t − 2 +

min

max(0,s−1−t )≤q≤s−1

min

max(0,s−t+1)≤q≤s

dk,(cid:11)
q,0

= 1 + min(t − 1 +

min

max(0,s−t )≤q≤s

dk,(cid:11)
q,0
, s − 2 +
dk,(cid:11)
q,0
, s − 1 +
, s − 1 +

dk,(cid:11)
q,0

min

max(0,t−s+1)≤q≤t

min

max(0,t−1−s)≤q≤t−1

dk,(cid:11)
0,q

),

dk,(cid:11)
0,q

))

min

max(0,t−s)≤q≤t

dk,(cid:11)
0,q

),

where we have used the property that consecutive cells in the (dij) matrix differ at most
by one [25]. Note that we have assumed s > 1 and t > 1. The particular cases s = 1 or
t = 1 are easily derived as well, for example for s = 1 and t > 1 we have
dk,(cid:11)
1,t

1,t−1

)

, dk,(cid:11)
0,t
, dk,(cid:11)
0,t

= 1 + min(dk,(cid:11)
0,t−1
= 1 + min(dk,(cid:11)
0,t−1
= 1 + min(dk,(cid:11)
0,t−1
= 1 + min(t − 1 + min(dk,(cid:11)

max(0,2−t )≤q≤1
, dk,(cid:11)
1,0
), min(dk,(cid:11)
0,t−1
which is the particularization of formula (6) for s = 1.

, dk,(cid:11)
, 1 + min(t − 2 +
, t − 1 + min(dk,(cid:11)

dk,(cid:11)
min
q,0
), 1 + min(dk,(cid:11)
0,t−2
, dk,(cid:11)
0,t

, dk,(cid:11)
1,0

, dk,(cid:11)
0,t

)),

0,0

0,0

,

min

max(0,t−2)≤q≤t−1

, dk,(cid:11)

0,t−1

))

dk,(cid:11)
0,q

))

Formula (6) relates the values at the right and bottom borders of a box to its left and
top borders. Yet it is not enough to compute the cells in constant time. Although we
cannot compute one cell in O(1) time, we can compute all the pk (or r(cid:11)) cells in overall
O( pk ) (or O(r(cid:11))) time.

Figure 2 shows the algorithm. We use a data structure (which in the pseudocode
is represented just as a set M∗) able to handle a multiset of elements starting with a
single element, adding and deleting elements, and delivering its minimum value at any
time. It will be used to maintain and update the minima minmax(0,s−t )≤q≤s dk,(cid:11)
q,0 and
minmax(0,t−s)≤q≤t dk,(cid:11)
0,q, used in formula (6). We see later that in our particular case all
those operations can be performed in constant time.

In the code we use drk,(cid:11)

s

s,r(cid:11) for the rightmost column and dbk,(cid:11)
t

bottom row. Their update formulas are derived from formula (6):

= dk,(cid:11)

pk ,t for the

= dk,(cid:11)

drk,(cid:11)

s

= 1 + min(r(cid:11) − 1 +
= 1 + min(t − 1 +

min

max(0,s−r(cid:11))≤q≤s

min

max(0,r(cid:11)−s)≤q≤r(cid:11)

q

drk,(cid:11)−1
drk,(cid:11)−1

, s − 1 +
, pk − 1 +

t

dbk,(cid:11)
The whole algorithm can be made to ﬁt in O(n + m) space by noting that in a
columnwise traversal we need, when computing box (k, l), to store only drk−1,(cid:11) and

max(0, pk−t )≤q≤ pk

max(0,t− pk )≤q≤t

min

min

).

q

q

),

q

dbk−1,(cid:11)
dbk−1,(cid:11)

Approximate Matching of Run-Length Compressed Strings

353

s

0

+ s

, r1)(bj2

(cid:1) = (bj1

← drk−1,0
pk−1

, r2)··· (bjn(cid:1) , rn(cid:1) ))

, p2)··· (aim(cid:1) , pm(cid:1) ), B

(cid:1) = (ai1
, p1)(ai2
/* We fill the topmost row and leftmost column first */
← 0, db0,0
← 0
for k ∈ 1··· m
(cid:1) do
for s ∈ 0··· pk do drk,0
← drk,0
dbk,0
for (cid:11) ∈ 0··· n
0
(cid:1) do
for t ∈ 0··· r(cid:11) do db0,(cid:11)
← db0,(cid:11)
dr0,(cid:11)
0
/* now we fill the rest of the matrix */
for (cid:11) ∈ 1··· m
for k ∈ 1··· n

(cid:1) do /* columnwise traversal */
(cid:1) do
if ak = b(cid:11) then /* equal letter box */

← db0,(cid:11)−1
r(cid:11)−1

+ t

pk

r(cid:11)

t

for s ∈ 1··· pk do
for t ∈ 1··· r(cid:11) do

if s ≤ r(cid:11) then drk,(cid:11)
if t ≤ pk then dbk,(cid:11)

← dbk−1,(cid:11)
← drk,(cid:11)−1
else /* different letter box */
}, Mt ← {dbk−1,(cid:11)
}

s

t

r(cid:11)−s else drk,(cid:11)
s

pk−t else dbk,(cid:11)

t

← drk,(cid:11)−1
s−r(cid:11)
← dbk−1,(cid:11)
t− pk

Levenshtein (A
1.
2. dr0,0
0
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
16.
17.
18.
19.
20.
21.
22.
23.
24.
25.
26.
27.
28.
29.
30.
31.
32.

return drm

s

r(cid:11)

}

}

Ml ← {drk,(cid:11)−1
← drk−1,(cid:11)
0
drk,(cid:11)
pk−1
for s ∈ 1··· pk do
0
Ml ← Ml ∪ {drk,(cid:11)−1
if s > r(cid:11) then Ml ← Ml − {drk,(cid:11)−1
s−r(cid:11)−1
}
if r(cid:11) ≥ s then Mt ← Mt ∪ {dbk−1,(cid:11)
r(cid:11)−s
drk,(cid:11)
Ml ← {drk,(cid:11)−1
s
← dbk,(cid:11)−1
pk
dbk,(cid:11)
r(cid:11)−1
for t ∈ 1··· r(cid:11) do
0
Mt ← Mt ∪ {dbk−1,(cid:11)
if t > pk then Mt ← Mt − {dbk−1,(cid:11)
t− pk−1
if pk ≥ t then Ml ← Ml ∪ {drk,(cid:11)−1
}
pk−t
dbk,(cid:11)

}, Mt ← {dbk−1,(cid:11)

}

}

0

t

}

← 1 + min(r(cid:11) − 1 + min(Ml ), s − 1 + min(Mt ))

← 1 + min(t − 1 + min(Ml ), pk − 1 + min(Mt ))
(cid:1)
(cid:1)
pm(cid:1) /* or dbm
rn(cid:1)

*/

n

n

(cid:1)

(cid:1)

t

Fig. 2. The O(m
run-length sequences of pairs (letter, run length).

(cid:1)

n + n

(cid:1)

m) time algorithm to compute the Levenshtein distance between A and B, coded as

dbk,(cid:11)−1, so the space is that for storing one complete column (m) and a row whose
width is one box (at most n). Our multiset data structure does not increase this space
complexity. Hence we have

THEOREM 3. Given strings A and B of lengths m and n that are run-length encoded
to lengths m
m) time
and O(m + n) space in the worst case.

, there is an algorithm to calculate DL(A, B) in O(m

n + n

and n

(cid:1)

(cid:1)

(cid:1)

(cid:1)

354

V. M¨akinen, G. Navarro, and E. Ukkonen

Fig. 3. Different cases along the computation of a different letter box.

This is a good point to give some intuition on the method. Figure 3 illustrates three
different points along our computation of a different letter box. In principle, to ﬁll the
cell (s, t ), we would need to consider all the cells (0··· s, t ) and (s, 0··· t ). However,
we have shown in Lemma 2 that some of these cells cannot inﬂuence the ﬁnal value of
the cell (s, t ). The reason is as follows. The cells in the gray areas need to reach cell
(s, t ) through a path of vertical, horizontal and diagonal moves, which correspond to
insertions, deletions and substitutions. Every such move costs 1, so the ﬁnal cost is s − 1
for every cell in the top gray area and t − 1 for every cell in the left gray area. These
costs are added to the original costs of the gray cells. Note that the optimal paths use
the diagonal moves as much as possible. The reason that permits not considering some
of the top and left cells is that their shortest paths to (s, t ) are longer than those of gray
cells, by an amount that equals their distance to the closest gray cell. Since neighboring
cells differ by at most one, a nongray area can never compensate its farther distance to
(s, t ) with a smaller cell value. Finally, those gray areas grow by one cell at a time and
we manage to maintain their minimum value in constant time.

3.2. Multiset Data Structure. What is left is to describe our data structure to handle 
a multiset of natural numbers. We exploit the fact that consecutive cells in (dij)
differ by at most one [25]. Our data structure represents the multiset S as a triple
(min(S), max(S), Vmin(S)··· max(S) → N). That is, we store the minimum and maximum
value of the multiset and a vector of counters V , which stores at Vi the number of elements 
equal to i in S. Given the property that consecutive cells differ by at most one, we
have that no value Vi is equal to zero. This is proved in the following lemma.
LEMMA 4. No value Vi for min(S) ≤ i ≤ max(S) is equal to zero when S is a set of
consecutive values in (dij) (i.e. S contains a contiguous part of a row or a column of the
matrix (dij)).
PROOF. The lemma is trivially true for the extremes i = min(S) and i = max(S). Let
us assume that the value min(S) is achieved at cell di, j and that the value max(S) is
achieved at cell di(cid:1), j(cid:1). Since all the intermediate cell values are also in S by hypothesis,
and consecutive cells differ by at most one, it follows that any value x between min(S)
and max(S) exists in a path that goes from di, j to di(cid:1), j(cid:1). Hence Vx > 0.

Approximate Matching of Run-Length Compressed Strings

355

Create (x)
1. return (x, x, Vx = 1)

Add ((minS, maxS, V ), y)
2. if y < minS then
minS ← y
3.
add new ﬁrst cell Vy = 0
4.
5. else if y > maxS then
maxS ← y
6.
add new last cell Vy = 0
7.
8. Vy ← Vy + 1
9. return (minS, maxS, V )

if z = minS then

Remove ((minS, maxS, V ), z)
10. Vz ← Vz − 1
11. if Vz = 0 then
12.
13.
14.
15.
16.
17.
18. return (minS, maxS, V )

else /* z = maxS */

remove ﬁrst cell from V
minS ← minS + 1

remove last cell from V
maxS ← maxS − 1

Min ((minS, maxS, V ))
19. return minS

Fig. 4. The multiset data structure implementation.

Figure 4 shows the detailed algorithms. When we initialize the data structure with
the single element S = {x} we represent the situation as (x, x, Vx = 1). When we have
to add an element y to S, we check whether y is outside the range min(S)··· max(S),
and in that case we extend the range. In any case we increment Vy. Note that the domain
is never extended by more than one cell, as empty cells cannot appear in between, by
Lemma 4. When we have to remove an element z from S we simply decrement Vz. If
Vz becomes zero, Lemma 4 implies that this is because z is either the minimum or the
maximum of the set. So we reduce the domain of V by one. Finally, obtaining min(S)
is trivial as we already have it precomputed.

It is easy to see that all the operations take constant time. As a practical matter, we
note that it is a good idea to keep V in a circular array so that it can grow and shrink
by any extreme. Its maximum size corresponds to pk (for Ml) or r(cid:11) (for Mt ), which are
known at the time of Create.

356

V. M¨akinen, G. Navarro, and E. Ukkonen

4. Extending the Algorithm to Weighted Edit Distance.
In this section we show
that the algorithm of Section 3 can be extended to handle an arbitrary cost function δ so
that the algorithm stays in O(mn

(cid:1) + m

n) time.

(cid:1)

The key fact is that, inside a given box (k, (cid:11)), the letters in A and B are the same all

the time, namely aik and bj(cid:11). Hence, there are only three different costs involved:

(7)

(insertion) Ci = δ(λ → bj(cid:11)
),
→ λ),
(deletion) Cd = δ(aik
(substitution) Cs = δ(aik
→ bj(cid:11)

),

where, since the triangle inequality holds, Cs ≤ Ci + Cd.
We will not differentiate between equal and different letter boxes in this section, since
Cs ≤ Ci + Cd in both cases. Also, to simplify the exposition, we assume without loss of
generality that the costs Ci and Cd are the same in all boxes (if this is not the case, we
can redeﬁne boxes not to overlap at borders and use the basic recurrence to compute the
values in the left and top borders inside the current box from the borders of preceding
boxes).

Several problems have to be dealt with. We ﬁrst consider how to compute the path
costs and how to determine the relevant cells, then how to update the path costs in
constant time, and ﬁnally how to handle our multiset under a more general scenario.

4.1. Determining Relevant Cells and Path Costs. The following lemma shows that the
path costs can still be computed in constant time and that the cells that are relevant to
the computation of dk,(cid:11)

s,t are exactly the same as for the Levenshtein distance.

LEMMA 5.

If δ is the cost function, then it holds that

(8)

dk,(cid:11)
s,t

= min(path(s − q, t ) +
path(s, t − q) +
where 1 ≤ s ≤ pk and 1 ≤ t ≤ r(cid:11), for values dk,(cid:11)

min

max(0,s−t )≤q≤s

dk,(cid:11)
q,0

,

dk,(cid:11)
0,q

),

min

max(0,t−s)≤q≤t

s,t . The function path is deﬁned as

path(d, r ) = Cs min(d, r ) + Cd max(d − r, 0) + Ci max(r − d, 0),

where Cd, Ci and Cs are as deﬁned in (7).

In order to determine the cost of a path from (s

(cid:1), t
PROOF.
the optimal path uses as many diagonal moves as possible, so its cost is (t − t
(cid:1))Cs + ((t − t
((s − s
The formula for path is easily derived from this observation.

(cid:1)) to (s, t ), we observe that
(cid:1))Cs +
(cid:1))− (s − s
(cid:1)))Ci otherwise.

(cid:1)))Cd if s − s

, and (s − s

(cid:1))− (t − t

(cid:1) ≥ t − t

(cid:1)

It remains to show that the cells not considered in the minimization are not necessary.
We ﬁrst assume that s > t and consider including the previous cell in the ﬁrst row of
the minimization formula (8). We call q0 = s − t the row index of the ﬁrst relevant cell
(cid:1) = s − t − 1 ≥ 0 that of the previous cell. Because of the formula to compute
and q

Approximate Matching of Run-Length Compressed Strings

357

+ δ(aik

(cid:1), t ) + dk,(cid:11)
q(cid:1),0

≤ dk,(cid:11)
q(cid:1),0
≥ path(s − q0, t ) + dk,(cid:11)

− Cd.
the extended edit distance (1), dk,(cid:11)
(cid:1), t ) = Cst + Cd = path(s − q0, t ) + Cd. Therefore,
On the other hand, path(s − q
q0,0
path(s − q
q0,0, which means that adding the previous
(cid:1), 0) in the minimization does not change the ﬁnal value. The argument can be
cell (q
inductively repeated with any previous cell. The case s ≤ t is trivial since there are no
previous cells. This proof applies equally to the second row of formula (8) for the cells
on the top.

→ λ), so dk,(cid:11)
q(cid:1),0

≥ dk,(cid:11)

q0,0

4.2. Updating Path Costs.
In the Levenshtein distance all the paths arriving at cell
(s, t ) cost the same (t − 1 or s − 1 depending on the case). So we maintain a set of cell
values, take the minimum and add the invariant path cost to them. Under weighted edit
distance, we need to add the path cost to the cell values before looking for the minimum.
Hence we store in our multiset the cell values with the path costs added.
The problem is that all the path costs change as we move from (s, t ) to (s + 1, t ) or
(s, t + 1) in the algorithm of Figure 2. To avoid the need of updating all the values of the
multiset, we store a different, invariant value that does not alter the order relationship
between cell costs (so the minimum will be the same, and after it is chosen we compute
its real value in constant time).
Let us consider the ﬁrst row of formula (8), that is, the values stored in the multiset Ml,
as Mt is analogous. The area of relevant cells in the leftmost column is max(0, s−t )··· s.
As in the algorithm in Figure 2, we need to consider two cases: (i) the area is extended
from pk ··· pk to max(0, pk − r(cid:11))··· pk by increasing t from 0 to r(cid:11); (ii) the area is
extended from 0··· 0 to max(0, pk − r(cid:11))··· pk by increasing s from 0 to pk. Let us
focus on (i). As t is increased by one, all the previous path costs must be incremented by
Ci because they use one more horizontal move (insertion). Instead of incrementing all
previous values by Ci, we can subtract an amount from each new value that is included
in the set of relevant values, so that the values in the set remain comparable. This amount
is t × Ci for the (t + 1)th element that is included in the set. Since the new value that
is included in the set uses only diagonal moves (substitutions), the absolute value that is
+ t × Cs − t × Ci. When
stored for the (t + 1)th cell included in the multiset is dk,(cid:11)
calculating the value of cell dk,(cid:11)
pk ,t , we can look for the minimum as before, and after it is
found, we can retrieve its original value by adding t × Ci.
We now consider case (ii). As we move from s to s + 1, all the previous paths gain
one diagonal move (substitution) and lose one horizontal move (insertion), so this time
the amount to subtract is s × (Cs − Ci) at the (s + 1)th step. This is also the value to add
to the minimum after it is retrieved. Again, we can see that the path of the new cell that
is included uses only horizontal moves (insertions), so the absolute value to store for it
is dk,(cid:11)
s,0
A related problem is how to determine the value of the cells that have to be removed
from the multiset, since we have stored it with the path cost added and the invariant
value subtracted. This corresponds to case (ii) only. As the amount by which all had
been shifted at the (x + 1)th step was x × (Cs − Ci), and the element removed is
(x, 0) := (s−r(cid:11), 0), we remove from the multiset the value dk,(cid:11)
+(r(cid:11)+x)×Ci−x×Cs =
dk,(cid:11)
s−r(cid:11),0

+ (r(cid:11) + s) × Ci − s × Cs.

+ s × Ci − (s − r(cid:11)) × Cs.

pk−t,0

x,0

358

V. M¨akinen, G. Navarro, and E. Ukkonen

4.3. Managing the Multiset. Now we are faced with the ﬁnal problem: how to handle
the multiset operations in constant time. The differences between consecutive cells need
not be in the set {−1, 0,+1}, so the preconditions for our previous multiset implementation 
do not hold anymore. We show that we can still implement the multiset in constant
time per operation.
We ﬁrst consider the case where δ gives integer values. The maximum difference
between two consecutive cell values is v∗ = max(Ci, Cd) = O(1). Since now we add
the path values to the cell costs before inserting them in the multiset, the difference
between consecutive values can be as large as 2v∗
, which is still constant. Hence we can
implement the multiset with an array V of counters as before. Unlike the Levenshtein
case, V will have zero entries, but there will never be more than 2v∗ − 1 consecutive
zero entries. Therefore, inserting a new entry may force us to initialize up to 2v∗
cells
(instead of only one as in lines 4 and 7 of Figure 4), and removing an entry may force
us to remove up to 2v∗
cells before ﬁnding the next nonzero entry (instead of only one
as in lines 13 and 16 of Figure 4). All this costs O(v∗), which is constant. Note also that
it is not true any longer, as assumed in line 15 of Figure 4, that cells that become zero
must necessarily lie at the limit of the multiset.

If the δ function delivers real values, the above solution does not work. Implementing
the set as a priority queue adds an O(log max(m, n)) factor to the time. However, we can
use min-deques [13] instead, which allow handling a queue of elements by adding and
removing elements from both ends, as well as taking the minimum over the queue. All
these operations can be performed in O(1) worst-case amortized time by using simple
techniques, and O(1) worst-case time per operation with more sophisticated ones. Hence
the total time stays the same.

An illustration of the algorithm is shown in Figure 5. The example shows how to
derive the values of the last row by using the values of the leftmost column. Note that
the values in the ﬁrst row should also be taken into account, but for brevity we only
consider the leftmost column. A transition from value t = 3 to value t = 4 is shown in
the example. First, value 3 is included in the set Ml after adding the path value t ×Cs = 4
and subtracting the value t × Ci = 8. The minimum of the set Ml is now −1, and the
value t × Ci = 8 is added to get the value of X.

Fig. 5. An example of the evaluation of the weighted edit distance.

Approximate Matching of Run-Length Compressed Strings

359

5. Approximate Searching. We now consider a problem related to computing the
LCS or the edit distance. Assume that string A is a short pattern and string B is a long
text (so m is much smaller than n), and that we are given a threshold parameter k. We
are interested in reporting all the “approximate occurrences” of A in B, that is, all the
positions of text substrings which are at distance k or less from the pattern A. In order
to ensure a linear size output, we content ourselves with reporting the ending positions
of the occurrences (which we call “matches”).
The classical algorithm to ﬁnd all the matches [23] computes a matrix exactly like
those of recurrences (3) and (2), with the only difference that d0, j = 0. This permits the
occurrences to start at any text position. The last row of the matrix, dm, j , is examined
and every text position j such that dm, j ≤ k is reported as a match.

Our goal now is to devise a more efﬁcient algorithm when pattern and text are run-
(cid:1) + R) algorithm (where R is the size of the output)
length compressed. A trivial O(m2n
is obtained as follows. We start ﬁlling the matrix only at beginnings of text runs, and
complete the ﬁrst m columns only (at O(m2) cost). The rest of the columns of the run
are equal to the mth because no optimal path can span more than m columns under the
LCS or Levenshtein models (m deletions are enough to convert an empty substring of
the text into the pattern). We later examine the last row of the matrix and report every
text position with value ≤ k. If the run is longer than m, then we have not produced
the whole last row but only the ﬁrst m cells of it. In this case we report the positions
m + 1··· r(cid:11) of the (cid:11)th run if and only if the position m was reported.
We now improve the trivial algorithm. A ﬁrst attempt is to apply our algorithms
directly using the new base value d0, j = 0. This change does not present complications.
m)
time, which may or may not be better than the trivial approach. The problem is that
(cid:1)), especially if n is much larger
O(m
than m. We seek an algorithm proportional to the compressed text size. We divide the
text runs into short (of length at most m) and long (longer than m) runs. We apply our
Levenshtein algorithm on the text runs, ﬁlling the matrix columnwise. If we have a short
, r(cid:11)), r(cid:11) ≤ m, we compute all the m
(cid:1) + 1 horizontal borders plus its ﬁnal vertical
run (bj(cid:11)
border (which becomes the initial border of the next column). The time to achieve this
r(cid:11) + m). For an additional O(r(cid:11)) cost we examine all the cells of the last row
(cid:1)
is O(m
and report all the text positions j(cid:11) + t such that dm

We ﬁrst concentrate on the Levenshtein distance. Our algorithm obtains O(m

n) may be too much in comparison to O(m2n

n+n

(cid:1)

(cid:1)

(cid:1)

pm(cid:1) ,t ≤ k.
(cid:1),(cid:11)

(cid:1)

If we have a long run (bj(cid:11)

, r(cid:11)), r(cid:11) > m, we limit its length to m and apply the same
m + m + m) cost. The columns m + 1··· r(cid:11) of that run are equal to
algorithm, at O(m
the mth, so we just need to examine the last row of the mth column, and report all the
text positions up to the end of the run, j(cid:11) + m + 1··· j(cid:11) + r(cid:11), if dm
m + R) time in the worst case, where R is the number
of occurrences reported. The space requirement is that to compute one text run limited
to length m, i.e. O(m
m). For the LCS model we have the same upper bound of m, so
(cid:1))) algorithm to be presented in
we achieve the same complexity. Our O(m
n
Section 6 does not yield good complexity here.

This algorithm takes O(n

(cid:1) + n

≤ k.

(cid:1),(cid:11)
pm(cid:1) ,m

(cid:1)(m

m

(cid:1)

(cid:1)

(cid:1)

(cid:1)

Note that if we are allowed to represent the occurrences as a sequence of runs of
consecutive text positions (all of which match), then the extra term R of the search cost
disappears.

360

V. M¨akinen, G. Navarro, and E. Ukkonen

THEOREM 6. Given a pattern A and a text B of lengths m and n that are run-length
, there is an algorithm to ﬁnd all the ending points of the
encoded to lengths m
approximate occurrences of A in B, either under the LCS or Levenshtein model, in
O(m

m) space in the worst case.

(cid:1)) time and O(m

and n

mn

(cid:1)

(cid:1)

(cid:1)

(cid:1)

The above result generalizes easily for the case of weighted edit distance using the
methods from Section 4. The limit between short and long runs depends in this case on
the δ values, but it is still O(m). For integer-valued costs for the edit operations, we have
the same bound as before, O(m

(cid:1)).

mn

(cid:1)

6. Improving a Greedy Algorithm for the LCS. The idea in our algorithm for the
Levenshtein distance DL in Section 3 was to ﬁll all the borders of all the boxes (dk,(cid:11)
s,t ).
The natural way to reduce the complexity would be to ﬁll only the corners of the boxes
(see Figure 1). For the DL distance this seems difﬁcult to obtain, but for the DID distance
there is an obvious greedy algorithm that achieves this goal: in different letter boxes, we
can calculate the corner values in constant time, and in equal letter boxes we can trace
(cid:1)) time. Thus, we can calculate all the corner
an optimal path to a corner in O(m
(cid:1))) time.3
values in O(m

(cid:1) + n

(cid:1) + n

(cid:1)(m

n

(cid:1)

(cid:1), m

It turns out that we can improve the greedy algorithm signiﬁcantly by fairly simple 
means. We notice that the diagonal method of [24] can be applied, and yields an
(cid:1))) algorithm, where d = DID(A, B). We also give other improvements
O(d2 min(n
that do not affect the worst case, but are signiﬁcant in the average case and in practice.
(cid:1)) time on the
We end the section conjecturing that our improved algorithm runs in O(m
average. As we are unable to prove this conjecture, we provide experimental evidence
to support it.

n

(cid:1)

6.1. Greedy Algorithm for the LCS. Calculating the corner value dk,(cid:11)
pk ,r(cid:11) in a different
=
letter box is easy, because it can be retrieved from the values dk,(cid:11)
pk−1,r(cid:11) and dk,(cid:11)
pk ,0
0,r(cid:11)
dk,(cid:11)−1
pk ,r(cid:11)−1, which are calculated earlier during the dynamic programming. This follows from
the lemma:

= dk−1,(cid:11)

LEMMA 7 [8]. The recurrence (3) can be replaced by the recurrence

(9)
where 1 ≤ s ≤ pk and 1 ≤ t ≤ r(cid:11), for values dk,(cid:11)

s,0

s,t

dk,(cid:11)
s,t

in a different letter box.

= min(dk,(cid:11)

+ t, dk,(cid:11)

0,t

+ s),

In contrast to the DL distance, the difﬁcult part in the DID distance lies in equal letter
boxes. As noted earlier, Lemma 1 also applies for the DID distance. From Lemma 1 we
can see that the corner values are retrieved along the diagonal, and those values may

(cid:1)

(cid:1)

(cid:1))) algorithm for the LCS, which they then improved
3 Apostolico et al. [4] also gave a basic O(m
(cid:1))). Their basic algorithm differs from our greedy algorithm in that they were using the
to O(m
recurrence for computing the LCS directly, and we are computing the distance DID. Furthermore, they traced a
(cid:1))) algorithm).
speciﬁc optimal path (which was the property that they could use to achieve the O(m

log(m

log(m

(cid:1)(m

(cid:1) + n

n

n

n

n

(cid:1)

n

(cid:1)

(cid:1)

(cid:1)

(cid:1)

Approximate Matching of Run-Length Compressed Strings

361

not have been calculated earlier. However, if pk = r(cid:11) in all equal letter boxes, then each
(cid:1)) algorithm for a
corner dk,(cid:11)
(very) special case, as previously noted in [7].

pk ,r(cid:11) can be calculated in constant time. This gives an O(m

n

(cid:1)

(cid:1)) time. The idea is to trace an optimal path to the cell dk,(cid:11)
0,r(cid:11)− pk
pk−r(cid:11),0 is symmetric). If k = 1, then the value d1,(cid:11)

What follows is an algorithm to retrieve the value dk,(cid:11)
(cid:1) + n
= dk,(cid:11)

pk ,r(cid:11) in an equal letter box in
pk ,r(cid:11). This can be done
O(m
= dk,(cid:11)
by using Lemmas 1 and 7 recursively. Assume that dk,(cid:11)
by Lemma 1 (case
pk ,r(cid:11)
dk,(cid:11)
0,r(cid:11)− pk
corresponds to a value
pk ,r(cid:11)
in the ﬁrst row (0) of the matrix (dij) which is known. Otherwise, the box (dk−1,(cid:11)
) is a
different letter box, and using the deﬁnition of overlapping boxes and Lemma 7 it holds
that

s,t

= dk−1,(cid:11)

= min(dk−1,(cid:11)

+ r(cid:11) − pk , dk−1,(cid:11)
0,r(cid:11)− pk

+ pk−1).

pk−1,0

pk−1,r(cid:11)− pk

dk,(cid:11)
0,r(cid:11)− pk
Now, the value dk−1,(cid:11)
pk−1,0 is calculated during the dynamic programming, so we can continue
tracing value dk−1,(cid:11)
0,r(cid:11)− pk
using Lemmas 1 and 7 recursively until we meet a value that has
already been calculated during dynamic programming (including the ﬁrst row and the
ﬁrst column of the matrix (dij)). The recursion never branches, because Lemma 1 deﬁnes
explicitly the next value to trace, and one of the two values (from which the minimum
is taken over in Lemma 7) is always known (that is because we enter the different letter
boxes at the borders, and therefore the other value is from a corner that is calculated
during the dynamic programming). We call the path described by the recursion a tracing
path.
(cid:1) + n
(cid:1)) time, because we
boxes in the tracing path.
(cid:1))) algorithm to calculate DID(A, B). A worst-case
(cid:1)), because we need to store only the
(cid:1)) space for the stack is not needed because

are skipping one box at a time, and there are at most m
Therefore, we get an O(m
example that actually achieves the bound is A = an and B = (ab)n/2.

pk ,r(cid:11) in an equal letter box may take O(m
(cid:1) + n

The space requirement of the algorithm is O(m

Tracing the value dk,(cid:11)

(cid:1) + n

(cid:1) + n

(cid:1)(m

n

n

(cid:1)

(cid:1)

(cid:1)

corner value in each box, and the O(m
the recursion does not branch.
(cid:1)

(cid:1)

n + n

We also achieve the O(m

m) bound, because the corner values dk,(cid:11)

pk ,r(cid:11) of equal
letter boxes deﬁne distinct tracing paths, and therefore each cell in the borders of the
boxes can be visited only once. To see this observe that each border cell reached by a
tracing path uniquely determines the border cell it comes from along the tracing path,
and therefore no two different paths can meet in a border cell. The only exception is a
corner cell, but in this case all the tracing paths end there immediately.

THEOREM 8. Given strings A and B of lengths m and n that are run-length encoded
(cid:1) +
(cid:1)
to lengths m
n + n
(cid:1)), m
(cid:1)
n

, there is an algorithm to calculate DID(A, B) in O(min(m

and n
m)) time and O(m

(cid:1)) space.

(cid:1)(m

n

n

(cid:1)

(cid:1)

(cid:1)

(cid:1)

6.2. Diagonal Algorithm. The diagonal method [24] provides an O(d min(m, n)) algorithm 
for calculating the distance d = DID(A, B) (or DL as well) between strings A and
B of length m and n, respectively. The idea is the following: the value dm,n = DID(A, B)
in the (di, j ) matrix of recurrence (3) deﬁnes a diagonal band, where the optimal path
must lie. Thus, if we want to check whether DID < k, we can limit the calculation
to the diagonal band deﬁned by value k (consisting of O(k) diagonals). Starting with

362

V. M¨akinen, G. Navarro, and E. Ukkonen

k = |n − m|+ 1, we can double the value k and run in each step recurrence (3) on the increasing 
diagonal band. As soon as dm,n < k, we have found DID(A, B) = dm,n, and we
can stop the doubling. The total number of diagonals evaluated is at most 2 DID(A, B),
and there are at most min(m, n) cells in each diagonal. Therefore, the total cost of the
algorithm is O(d min(m, n)), where d = DID(A, B).

We can use the diagonal method with our greedy algorithm as follows. We calculate
only the corner values that are inside the diagonal band deﬁned by value k in the above
doubling algorithm. The corner values in equal letter boxes inside the diagonal band can
be retrieved in O(k) time. That is because we can limit the length of the tracing paths
with the value 2k + 1 (between two equal letter boxes there is a different letter box that
contributes at least one to the value that we are tracing, and we are not interested in
(cid:1))),
corner values that are greater than k). Therefore, we get the total cost O(d2 min(m
where d = DID(A, B).

(cid:1), n

6.3. Faster on Average. There are some practical reﬁnements for the greedy algorithm
that do not improve its worst-case behavior, but do have an impact on its average case.

Skipping runs of different letter boxes in tracing paths. Consider two consecutive
). By Lemma 7 it holds for the values 1 ≤ t ≤ r(cid:11)
different letter boxes (dk,(cid:11)
that

s,t ) and (dk+1,(cid:11)

s,t

dk+1,(cid:11)
pk+1,t

0,t

pk+1,0

= min(dk+1,(cid:11)
= min(dk,(cid:11)
= min(dk,(cid:11)
= min(dk,(cid:11)

+ pk+1, dk+1,(cid:11)
+ t )
+ pk+1, dk+1,(cid:11)
+ t )
+ pk + pk+1, dk,(cid:11)
+ pk + pk+1, dk+1,(cid:11)

pk+1,0

pk ,0

pk ,t

0,t

pk+1,0

0,t

+ pk+1 + t, dk+1,(cid:11)
+ t ).

pk+1,0

+ t )

The above result can be extended to the following lemma by using induction:

(cid:1)+1,(cid:11)
(cid:1),(cid:11)
s,t ), (dk
s,t

LEMMA 9. Let ((dk
s,t )) be vertical 
and horizontal runs of different letter boxes. When 1 ≤ t ≤ r(cid:11) and 1 ≤ s ≤ pk,
recurrence (4) can be replaced by the recurrences

), . . . , (dk,(cid:11)

), . . . , (dk,(cid:11)

s,t

s,t ), (dk,(cid:11)(cid:1)+1
s,t )) and ((dk,(cid:11)(cid:1)
(cid:3)
(cid:3)

+ k(cid:2)
+ (cid:11)(cid:2)

s=k(cid:1)

ps

,

rt

,

t=(cid:11)(cid:1)

1 ≤ t ≤ r(cid:11),

1 ≤ s ≤ pk .

(cid:1)
(cid:1)

= min

dk,(cid:11)
pk ,t

= min

dk,(cid:11)
s,r(cid:11)

dk,(cid:11)
pk ,0

dk,(cid:11)
0,r(cid:11)

+ t, dk

(cid:1),(cid:11)
0,t

+ s, dk,(cid:11)(cid:1)

s,0

Now it is obvious how to speed up the retrieval of values dk,(cid:11)

pk ,r(cid:11) in the equal letter
boxes. During dynamic programming, we can maintain pointers in each different letter
box to the last equal letter box encountered in the direction of the row and the column.
When we enter a different letter box while tracing the value of dk,(cid:11)
pk ,r(cid:11) in an equal letter
box, we can use Lemma 9 to calculate the minimum over the run of different letter boxes
at once, and continue on tracing from the equal letter box preceding the run of different

Approximate Matching of Run-Length Compressed Strings

363

letter boxes. (Note that in order to use the summations of Lemma 9 we should store the
cumulative ik and j(cid:11) values instead of pk and r(cid:11).) Therefore we get the following result:

(cid:1)

THEOREM 10. Given strings A and B of lengths m and n that are run-length encoded
, such that the letters of the runs are independently and uniformly
to lengths m
distributed over an alphabet of size ||, there is an algorithm to calculate DID(A, B) in
(cid:1))/||2)) time on the average.
O(m

and n
(cid:1) + n

(cid:1)(1 + (m

n

(cid:1)

(cid:1)

(cid:1)

n

On the other hand, there are on the average O(m

PROOF. The ﬁrst part of the cost, O(m
of all the different letter boxes.

(cid:1)), comes from the constant time computation
(cid:1)/||) equal letter boxes. This can
be seen as follows: Consider the box model of Figure 1. The equal letter boxes in a row
of the matrix correspond to the same character, say σ ∈ . Let X j be a random variable
to denote the amount of different letter boxes between the jth and ( j − 1)th equal letter
box in a row (without lack of generality, we may assume that a row starts and ends with
an equal letter box). It is an easy exercise to see that the expected value of each X j is
|| − 1. We can use this to estimate the number of equal letter boxes in a row, denoted
by ϕ, because we can write

n

(cid:1)

(10)

j=1

ϕ(cid:2)

X j + 1 < n

(cid:1).

(cid:1)

(cid:1)

To get the claimed bound O(m

We are interested in the ﬁrst value of ϕ such that (10) does not hold. Using a result from
(cid:1)/||) (see p. 359 in [11]; the result
renewal theory, the expected value of such a ϕ is O(n
requires that the variables X j are independent, which is our case). Using the linearity of
expectation, the expected number of equal letter boxes in the whole matrix is just the
sum of the equal letter boxes in all rows, that is O(m
(cid:1))/||2)), it remains to show that the
(cid:1))/||). This is the
expected amount of calculation in an equal letter box is O((m
amount of equal letter boxes visited by a tracing path. We can use a similar argument as
when calculating the amount of equal letter boxes in a row. Let X j be a random variable
to denote the amount of different letter boxes between the jth and ( j − 1)th equal letter
box in a tracing path (again, we may assume that a tracing path starts and ends with an
equal letter box). Notice that the string that is the concatenation of the characters in a
tracing path has similar distribution as the strings A and B. Thus the expected value of
each X j is || − 1. As a tracing path can visit at most m

boxes, we can write

(cid:1)(1 + (m

(cid:1)/||).

(cid:1) + n

(cid:1) + n

(cid:1) + n

n

n

(cid:1)

(11)

j=1

ϕ(cid:2)

X j + 1 < m

(cid:1) + n

(cid:1),

where ϕ is the number of equal letter boxes in a tracing path. As previously, the expected
value of the ﬁrst ϕ such that (11) does not hold is O((n

(cid:1))/||).

(cid:1) + m

Using bridges to prune tracing paths. The second improvement to the greedy algorithm
is to limit the length of the tracing paths. In the greedy algorithm the tracing is continued
until a value is reached that has been calculated during the dynamic programming.

364

V. M¨akinen, G. Navarro, and E. Ukkonen

pk ,0

pk ,0

+ r(cid:11).

pk ,t , 1 ≤ t ≤ r(cid:11) (or symmetrically dk,(cid:11)

However, there are more known values than those that have been explicitly calculated.
s,r(cid:11), 1 ≤ s ≤ pk), in the border
Consider value dk,(cid:11)
+ t,
= dk,(cid:11)
of a different letter box. If dk,(cid:11)
pk ,r(cid:11)
otherwise we get a contradiction: dk,(cid:11)
pk ,r(cid:11)

+ r(cid:11), then it must hold that dk,(cid:11)
< dk,(cid:11)
pk ,0

pk ,t = dk,(cid:11)

We call the above situation a horizontal (vertical) bridge. Note that from Lemma 7 it
follows that there is either a vertical or a horizontal bridge in each different letter box.
When we enter a different letter box in the recursion, we can check whether the bridge
property holds at the border we entered, using the corner values that are calculated during
the dynamic programming. Thus, we can stop the recursion at the ﬁrst bridge encountered.
To combine this improvement with the algorithm that skips runs of different letter boxes,
we need Lemma 11 below that states that the bridges propagate along runs of different
letter boxes. Therefore we only need to check whether the last different letter box has
a bridge to decide whether we have to skip to the next equal letter box. The resulting
algorithm is given in pseudocode in Figure 6. An illustration of the algorithm is shown
in Figure 7.

LEMMA 11. Let ((dk
If there is a horizontal bridge dk
(cid:1) < k
(cid:1)(cid:1),(cid:11)
dk
pk(cid:1)(cid:1) ,r(cid:11)
of different letter boxes.

(cid:1)+1,(cid:11)
(cid:1),(cid:11)
s,t ), (dk
s,t
+ r(cid:11) for all k

= dk

(cid:1)(cid:1),(cid:11)
pk(cid:1)(cid:1) ,0

), . . . , (dk,(cid:11)
s,t )) be a vertical run of different letter boxes.
= dk
+ r(cid:11), then there is a horizontal bridge
(cid:1),(cid:11)
(cid:1),(cid:11)
pk(cid:1) ,r(cid:11)
pk(cid:1) ,0
(cid:1)(cid:1) ≤ k. The symmetric result holds for horizontal runs

PROOF. We use the counterargument that dk
(cid:1) < k
k

(cid:1)(cid:1) ≤ k. Then by Lemma 9 and by the bridge assumption it holds that

(cid:1)(cid:1),(cid:11)
pk(cid:1)(cid:1) ,r(cid:11)

= dk

(cid:1)(cid:1),(cid:11)
pk(cid:1)(cid:1) ,0

+ r(cid:11) does not hold for some

On the other hand, using the counterargument and the fact that consecutive cells in the
(dij) matrix differ at most by one [25], we get

which is a contradiction and so the original proposition holds.

Lemma 11 has a corollary: if the last different letter box in a run does not have a
horizontal (vertical) bridge, then none of the boxes in the same run have a horizontal
(vertical) bridge and, on the other hand, all the boxes in the same run must have a vertical
(horizontal) bridge.

Now, if two tracing paths cross inside a box (or run thereof), then one of them
necessarily meets a bridge. In the average case, there are many crossings of the tracing
paths and the total cost for tracing the values in equal letter boxes decreases signiﬁcantly.
Another way to consider the average length of a tracing path is to think that every time
a tracing path enters a different letter box, it has some probability to hit a bridge. If the

(cid:1)(cid:1),(cid:11)
dk
pk(cid:1)(cid:1) ,r(cid:11)

= dk

(cid:1)+1,(cid:11)
0,r(cid:11)

ps = dk

(cid:1)+1,l
0,0

(cid:1)(cid:1)(cid:2)
+ k

s=k(cid:1)+1

(cid:1)(cid:1),(cid:11)
dk
pk(cid:1)(cid:1) ,r(cid:11)

< dk

(cid:1)(cid:1),(cid:11)
pk(cid:1)(cid:1) ,0

+ r(cid:11) ≤ dk

(cid:1)+1,(cid:11)
0,0

+

(cid:1)(cid:1)(cid:2)
+ r(cid:11) + k

ps .

s=k(cid:1)+1

(cid:1)

(cid:1)(cid:1)(cid:2)

k

(cid:3)

ps

s=k(cid:1)+1

+ r(cid:11),

Approximate Matching of Run-Length Compressed Strings

365

pk ,r(cid:11) */

, r1)(bj2

, p1)(ai2

(cid:4)
(cid:4)(cid:11)

s,t ) as follows: */

(cid:14)= bj(cid:11) then
(cid:14)= bj(cid:11) then

, r2)··· (bjn(cid:1) , rn(cid:1) ))

, p2)··· (aim(cid:1) , pm(cid:1) ), B

k
t=dk,(cid:11).jumptop+1 pt */
t=dk,(cid:11).jumpleft+1 rt */

(cid:1) = (ai1
(cid:1) = (bj1
/* We use structure d k,(cid:11) to denote a box (d k,(cid:11)
/* d k,(cid:11).corner := d k,(cid:11)
/* d k,(cid:11).jumptop := location of the next equal letter box above */
/* d k,(cid:11).jumpleft := location of the next equal letter box in the left */
/* d k,(cid:11).sumtop := if aik
/* d k,(cid:11).sumleft := if aik
/* Initialize ﬁrst row and column (let ai0
for k ∈ 1··· m
for (cid:11) ∈ 1··· n
compute all the values d k,(cid:11).(jumptop, jumpleft, sumtop, sumleft)
/* now we fill the rest of the corner values */
for k ∈ 1··· m
(cid:1) do
for (cid:11) ∈ 1··· n
(cid:1) do
(cid:1), (cid:11)(cid:1), p, r, sum, d k,(cid:11).corner) ← (false, k, (cid:11), pk , r(cid:11), 0,∞)
(bridge, k
(cid:14)= bj(cid:11) then /* different letter box */
if aik
d k,(cid:11).corner ← min(d k−1,(cid:11).corner + pk , d k,(cid:11)−1.corner + r(cid:11))
else while bridge = false do

(cid:1) do d k,0.corner ← d k−1,0.corner + pk
(cid:1) do d 0,(cid:11).corner ← d 0,(cid:11)−1.corner + r(cid:11)

= λ, p0 = r0 = 1) */

= bj0

/* equal letter box, trace d k,(cid:11).corner */
if p = r then /* straight from the diagonal */
(cid:1)−1,(cid:11)(cid:1)−1.corner)

d k,(cid:11).corner ← min(d k,(cid:11).corner, sum + d k
bridge ← true

else if p < r then /* diagonal up */

(cid:1)) ← (r − p, k
.corner = d k
(cid:1),(cid:11)(cid:1)

(cid:1) − 1)
(r, k
(cid:1),(cid:11)(cid:1)−1.corner + r )
d k,(cid:11).corner ← min(d k,(cid:11).corner, sum + d k
(cid:1),(cid:11)(cid:1)−1.corner + r(cid:11)(cid:1) then bridge ← true
if d k
else /* jump to the next equal letter box */

(cid:1)) ← (sum + d k

(cid:1),(cid:11)(cid:1)

.sumtop, d k

.jumptop)

(cid:1),(cid:11)(cid:1)

LCS (A
1.
2.
3.
4.
5.
6.
7.
8. d 00.corner ← 0
9.
10.
11.
12.
13.
14.
15.
16.
17.
18.
19.
20.
21.
22.
23.
24.
25.
26.
27.
28.
29.
30.
31.

32.
33.
34.

(sum, k
p ← pk(cid:1)
if k

(cid:1) = 0 then /* first row */
d k,(cid:11).corner ← min(d k,(cid:11).corner,
bridge ← true

sum + d k

(cid:1),(cid:11)(cid:1)−1.corner + r )

else . . . /* diagonal left similarly*/

return (m + n − d m

(cid:1)

(cid:1),n

.corner)/2 /* return the length of the LCS */

Fig. 6. The improved greedy algorithm for computing the LCS between A and B, coded as run-length sequences
of pairs (letter, run length).

bridges were placed randomly in the different letter boxes, then the probability to hit a
bridge would be 1
2 . This would immediately give a constant expected length for a tracing
path. However, the placing of the bridges depends on the computation of recurrence (3),
and this makes the probabilistic reasoning much more complex. We are still conﬁdent
that the following conjecture holds, although we have not been able to prove it.

366

V. M¨akinen, G. Navarro, and E. Ukkonen

Fig. 7. Evaluating the LCS between strings A = aaabbbbaaaa and B = aaaaabbbbccccaa using the
algorithm in Figure 6. The gray values denote the bridges, thus these values are not explicitly computed, but
they can be deduced from the corner values.

(cid:1)

(cid:1)

CONJECTURE 12. Let A and B be strings that are run-length encoded to lengths m
, such that the lengths of the runs are equally distributed in both strings. Under
and n
these assumptions the expected running time of the algorithm in Figure 6 for calculating
DID(A, B) is O(m

(cid:1)).

n

(cid:1)

6.4. Experimental Results. To test Conjecture 12, we ran the algorithm in Figure 6
with the following settings:

1. m

(cid:1) ∈ {1, 50, 100, 500, 1000, 1500, 2000},|| = 2, runs in [1, 1000].

(cid:1) = 2000,|| ∈ {2, 4, 8, 16, 32, 64, 128, 256}, runs in [1, 1000].

(cid:1) = 2000, || = 2, runs in [1, x], x ∈ {1, 10, 100, 1000, 10,000, 100,000,

(cid:1) = n
1,000,000}.
(cid:1) = 2000, n
2. m
(cid:1) = n
3. m
4. String A was as in item 1 with runs in [1, 1000]. String B was generated by applying k
random insertions/deletions on A, where k ∈ {0, 1, 10, 100, 1000, 10,000, 100,000}.
5. Real data: three different black/white images (printed lines from a book draft (187 ×
591), technical drawing (160 × 555) and a signature (141 × 362)). We ran the LCS
algorithm on all pairs of lines in each image.

Table 6.4 shows the results. Different parameter choices are listed in the order they
appear in the above listing (e.g. setting 1 in test 1 corresponds to x = 1, setting 2
corresponds to x = 10, etc.).

(cid:1)

The average length L of a tracing path (i.e. the amount of equal letter boxes visited by
a tracing path) was smaller than two in tests 1–4 (slightly greater in test 5). That is, the
(cid:1)) with a very small constant factor. Test 1 showed
running time was in practice O(m
that when the mean length of the runs increases, then L also increases, but not exceeding
two (L ∈ [1, 1.99]). In test 2, the worst situation was with n
(L = 1.98). We
tested the effect of the alphabet in test 3, and the worst was || = 2 (L = 1.99) and
the best was || = 256 (L = 1.13). Test 4 was used to simulate a typical situation, in
which the distance between the strings is small. The amount of errors did not have much

(cid:1) = m

n

(cid:1)

Approximate Matching of Run-Length Compressed Strings

367

Table 1. The average length and the maximum length (in parentheses) of a tracing path

∗
Test

was measured in different test settings.

Setting 1, setting 2, . . .

1 (1), 1.71 (18), 1.96 (28), 1.98 (27), 1.98 (32), 1.99 (29), 1.98 (25)
1.73 (5), 1.77 (10), 1.74 (13), 1.80 (21), 1.90 (30), 1.97 (35), 1.98 (38)
1.99 (30), 1.77 (20), 1.60 (14), 1.45 (14), 1.33 (9), 1.24 (7), 1.17 (6), 1.13 (6)
1.71 (9), 1.71 (8), 1.71 (7), 1.71 (10), 1.72 (9), 1.72 (10), 1.72 (12)
2.00 (35), 2.34 (146), 2.32 (31)

Test 1
Test 2
Test 3
Test 4
Test 5
∗
The values of tests 1–4 are averages over 10–10,000 trials (e.g. on small values of n
in test 2, more
trials were needed because of high variance, whereas otherwise the variance was small). Test 5 was
deterministic (i.e. the values are from one trial).

(cid:1)

inﬂuence (L ∈ [1.71, 1.72]). In real data (test 5), there were pairs close to the worst case
(A = an, B = (ab)n/2), and therefore the results were slightly worse than with randomly
generated data: L ∈ {2.00, 2.34, 2.31} with the three images. Of course real data does
not need to ﬁt the hypothesis of our conjecture.

7. Conclusions. We have presented new algorithms for approximate matching of runlength 
compressed strings. The previous algorithms [8], [4] permit computing their LCS.
We have presented a new LCS algorithm with improved average complexity. We have
also extended an LCS algorithm [8] to a more general weighted edit distance model
(in particular to the Levenshtein distance) without increasing its complexity. Finally, we
have presented an algorithm with nontrivial complexity for approximate searching of a
run-length compressed pattern on a run-length compressed text under either model.

A possible application for the edit distance would be the comparison of images.
Several models to compare images permitting not only differences in the pixel values
but also distortions have been proposed [17], [6]. When considering color images, a
natural choice is that the cost to change one pixel by another has a cost related to the
absolute difference of their colors. Once a suitable cost for insertions and deletions of
pixels is chosen, the problem is how to compute the best alignment between two images or
ﬁnd the places in a large image where a small image pattern aligns best. The algorithms
depicted in [17] and [6] need O(n4) time to compare two n × n images. They also
give fast ﬁltration methods to search for patterns inside large images. In several cases,
these algorithms resort to one-dimensional weighted edit distance or one-dimensional
approximate searching algorithms. These could be signiﬁcantly improved if the images
were run-length compressed prior to the computation and our algorithms were used for
those subproblems. Some recent algorithms searching for rotated image patterns inside
a large image [12] could be extended as well: their matching model does not permit
insertions or deletions of pixels, so they could be integrated with other approaches such
as [6]. Again, it would be possible to speed up the comparison process by run-length
compressing the image and the pattern, the latter at several rotations.

With respect to the original models, an interesting question is whether an algorithm can
be obtained whose cost is just the product of the compressed lengths. Indeed, this seems
possible in the average case, as demonstrated by the experiments with our improved

368

V. M¨akinen, G. Navarro, and E. Ukkonen

algorithm for the LCS. Finally, a combination of a two-dimensional approximate pattern
matching algorithm with two-dimensional run-length compression [17], [6], [1], [3]
seems interesting.

References

[1] A. Amir and G. Benson. Efﬁcient two-dimensional compressed matching. In Proc. 2nd IEEE Data

Compression Conference (DCC’92), pages 279–288, 1992.

[2] A. Amir, G. Benson, and M. Farach. Let sleeping ﬁles lie: pattern matching in Z-compressed ﬁles.

Journal of Computer and Systems Sciences, 52(2):299–307, 1996.

[3] A. Amir, G. Landau, and D. Sokol. Inplace run-length 2d compressed search. In Proc. 11th Symposium

on Discrete Algorithms (SODA’00), pages 817–818, 2000.

[4] A. Apostolico, G. Landau, and S. Skiena. Matching for run-length encoded strings. Journal of Complexity,
 15:4–16, 1999.

[5] O. Arbell, G. Landau, and J. Mitchell. Edit distance of run-length encoded strings.

Information

Processing Letters, 83(6):307–314, 2002.

[6] R. Baeza-Yates and G. Navarro. New models and algorithms for multidimensional approximate pattern

matching. Journal of Discrete Algorithms, 1(1):21–49, 2000. Special issue on Matching Patterns.

[7] H. Bunke and J. Csirik. An algorithm for matching run-length coded strings. Computing, 50:297–314,

1993.

[8] H. Bunke and J. Csirik. An improved algorithm for computing the edit distance of run-length coded

strings. Information Processing Letters, 54(2):93–96, 1995.

[9] M. Crochemore, G. M. Landau, and M. Ziv-Ukelson. A sub-quadratic sequence alignment algorithm
for unrestricted cost matrices. In Proc. 13th Annual ACM–SIAM Symposium on Discrete Algorithms
(SODA’02), pages 679–688, 2002 (the citation is to the revised report 2001-08 at Institut GaspardMonge,
 Universit´e de Marne-la-Vall´ee).

[10] M. Farach and M. Thorup. String matching in Lempel–Ziv compressed texts. Algorithmica, 20:388–404,

1998.

[11] W. Feller. An Introduction to Probability Theory and Its Applications, Vol. II. Wiley, New York, 1966.
[12] K. Fredriksson. Rotation invariant histogram ﬁlters for similarity and distance measures between
digital images. In Proc. 7th Symposium on String Processing and Information Retrieval (SPIRE’00),
pages 105–115. IEEE Computer Society Press, Los Alamitos, CA, 2000.

[13] H. Gajewska and R. Tarjan. Deques with heap order. Information Processing Letters, 12(4):197–200,

[14]

1986.
J. K¨arkk¨ainen, G. Navarro, and E. Ukkonen. Approximate string matching over Ziv–Lempel compressed
text. In Proc. 11th Annual Symposium on Combinatorial Pattern Matching (CPM’00), pages 195–209.
LNCS 1848, Springer-Verlag, Berlin, 2000. Extended version to appear in the Journal of Discrete
Algorithms.

[15] T. Kida, Y. Shibata, M. Takeda, A. Shinohara, and S. Arikawa. A unifying framework for compressed
pattern matching. In Proc. 6th Symposium on String Processing and Information Retrieval (SPIRE’99),
pages 89–96. IEEE Computer Society Press, Los Alamitos, CA, 1999.

[16] T. Kida, M. Takeda, A. Shinohara, M. Miyazaki, and S. Arikawa. Multiple pattern matching in LZW

compressed text. In Proc. 8th IEEE Data Compression Conference (DCC’98), 1998.

[17] K. Krithivasan and R. Sitalakshmi. Efﬁcient two-dimensional pattern matching in the presence of errors.

Information Sciences, 43:169–184, 1987.

[18] V. Levenshtein. Binary codes capable of correcting deletions, insertions and reversals. Soviet Physics

Doklady, 6:707–710, 1966.

[19] V. M¨akinen, G. Navarro, and E. Ukkonen. Approximate matching of run-length compressed strings.
In Proc. 12th Annual Symposium on Combinatorial Pattern Matching (CPM’01), pages 31–49. LNCS
2089, Springer-Verlag, Berlin, 2001.

[20] T. Matsumoto, T. Kida, M. Takeda, A. Shinohara, and S. Arikawa. Bit-parallel approach to approximate
string matching. In Proc. 7th Symposium on String Processing and Information Retrieval (SPIRE’00),
pages 221–228. IEEE Computer Society Press, Los Alamitos, CA, 2000.

Approximate Matching of Run-Length Compressed Strings

369

[21]

J. Mitchell. A geometric shortest path problem, with application to computing a longest common
subsequence in run-length encoded strings. Technical Report, Department of Applied Mathematics,
SUNY Stony Brook, NY, 1997.

[22] G. Navarro, T. Kida, M. Takeda, A. Shinohara, and S. Arikawa. Faster approximate string matching
over compressed text. In Proc. 11th IEEE Data Compression Conference (DCC’01), pages 459–468,
2001.

[23] P. Sellers. The theory and computation of evolutionary distances: pattern recognition. Journal of

Algorithms, 1(4):359–373, 1980.

[24] E. Ukkonen. Algorithms for approximate string matching. Information and Control, 64(1–3):100–118,

1985.

[25] E. Ukkonen. Finding approximate patterns in strings. Journal of Algorithms, 6(1–3):132–137, 1985.
[26] R. Wagner and M. Fisher. The string-to-string correction problem. Journal of the ACM, 21(1):168–173,

[27]

1974.
J. Ziv and A. Lempel. A universal algorithm for sequential data compression. IEEE Transactions on
Information Theory, 23:337–343, 1977.

