Alphabet Partitioning for Compressed

Rank/Select and Applications

J´er´emy Barbay1, Travis Gagie1,(cid:2), Gonzalo Navarro1,(cid:2), and Yakov Nekrich2

1 Department of Computer Science, University of Chile

{jbarbay,tgagie,gnavarro}@dcc.uchile.cl

2 Department of Computer Science, University of Bonn

yasha@cs.uni-bonn.de

Abstract. We present a data structure that stores a string s[1..n] over
the alphabet [1..σ] in nH0(s) + o(n)(H0(s)+1) bits, where H0(s) is the
zero-order entropy of s. This data structure supports the queries access
and rank in time O (lg lg σ), and the select query in constant time. This
result improves on previously known data structures using nH0(s) +
o(n lg σ) bits, where on highly compressible instances the redundancy
o(n lg σ) cease to be negligible compared to the nH0(s) bits that encode
the data. The technique is based on combining previous results through
an ingenious partitioning of the alphabet, and practical enough to be
implementable. It applies not only to strings, but also to several other
compact data structures. For example, we achieve (i) faster search times
and lower redundancy for the smallest existing full-text self-index; (ii)
compressed permutations π with times for π() and π−1() improved to
log-logarithmic; and (iii) the ﬁrst compressed representation of dynamic
collections of disjoint sets.

1 Introduction

Search queries on strings have many important applications, to the point that
one is willing to sacriﬁce some additional space to index the string in order to
support the queries in less time. The most important queries serve as primitives
to implement many other operations, in particular pattern matching in fulltext 
databases (see, e.g., [18,7,14,19] for recent discussions): given a string s,
s.access(i) returns the ith character of s, which we denote s[i]; s.ranka(i) returns
the number of occurrences of the character a up to position i; and s.selecta(i)
returns the position of the ith occurrence of a in s.

Wavelet trees [11] represent a string s[1..n] over alphabet [1..σ] within n lg σ +
o(n lg σ) bits, where lg denotes the logarithm in base two. The indexing space
in o(n lg σ) is considered asymptotically “negligible” compared to the n lg σ bits
required to hold the main data, while providing support for the queries in time
O (lg σ). Later results [10] improved the times to O (lg lg σ).

Regularities in the string permit further reductions in the space, from n lg σ
bits down to nHk(s) bits, where Hk(s) denotes the kth-order empirical entropy

(cid:2) Funded in part by the Millennium Institute for Cell Dynamics and Biotechnology

(ICDB), Grant ICM P05-001-F, Mideplan, Chile.

O. Cheong, K.-Y. Chwa, and K. Park (Eds.): ISAAC 2010, Part II, LNCS 6507, pp. 315–326, 2010.
c(cid:2) Springer-Verlag Berlin Heidelberg 2010

316

J. Barbay et al.

Table 1. Recent bounds and our new ones for data structures supporting access, rank
and select. The ﬁrst row holds for σ = O (polylog(n)) and the second for σ = o(n). The
space bound in the sixth row holds for k = o(logσ n). The times of our Thm. 1 can be
reﬁned into a more complicated formula (see also Cor. 1).

[8, Thm. 3.2]

[8, Cor. 3.3]

[10, Thm. 2.2]

[10, Thm. 2.2]

[3, Lem. 4.1]

[3, Thm. 4.2]

O

space (bits)

nH0(s) + o(n)

nH0(s) + o(n lg σ)
n lg σ + o(n lg σ)
n lg σ + o(n lg σ)
nH0(s) + o(n lg σ)
nHk(s) + o(n lg σ)

(cid:2)

access
O (1)
1 + lg σ
lg lg n
O (lg lg σ)

O (1)

O (lg lg σ)

O (1)

(cid:3)

O

(cid:3)

(cid:2)

O

(cid:2)

rank
O (1)
1 + lg σ
lg lg n
O (lg lg σ)

(cid:3)

select
O (1)
1 + lg σ
lg lg n
O (1)

O (lg lg σ)

O (1)

O (lg lg σ lg lg lg σ)
O (cid:4)

O (lg lg σ)

(lg lg σ)2 lg lg lg σ

(cid:5) O (lg lg σ lg lg lg σ)

Thm 1

Thm 1

nH0(s) + o(n)(H0(s) + 1)
nH0(s) + o(n)(H0(s) + 1)

O (lg lg σ)

O (1)

O (lg lg σ)

O (lg lg σ lg lg lg σ)

O (1)

O (lg lg σ)

of s (i.e., the minimum self-information of s with respect to a kth-order Markov
source; see Manzini [15] for a deﬁnition and discussion). The challenge of compressing 
the string while still supporting the queries eﬃciently was also achieved,
using as little as nH0(s) + o(n lg σ) [11,8,3] and even nHk(s) + o(n lg σ) bits [3]
(for any k = o(logσ n)) while retaining the time complexities.

One problem with such space is that, on highly compressible data, the o(n lg σ)
bits of the index are not always negligible compared to the space used to encode
the compressed data. Hence the challenge is to retain the eﬃcient support for
the queries while compressing the index redundancy as well. In this paper we
solve this challenge in the case of zero-order entropy compression, that is, the
redundancy of our data structure is asymptotically negligible compared to the
zero-order entropy of the text, plus o(n) bits.
For comparison, the representation by Golynski et al. [10] does not compress1
s and uses additional O
= o(n lg σ) bits, but oﬀers log-logarithmic times
(cid:3)
for the queries. Ferragina et al.’s wavelet tree [8] achieves zero-order compression
plus O
time. Barbay et al. [3] obtain zero-order space and log-logarithmic times, but
their redundancy is still o(n lg σ). See Table 1 for a summary of our bounds and
previous ones.2

= o(n lg σ) bits, supporting the queries in O

1 + lg σ
lg lg n

n lg σ lg lg n

n lg σ
lg lg σ

(cid:3)

(cid:2)

(cid:2)

(cid:2)

(cid:3)

lg n

In Section 2 we show how to combine the strengths of these data structures,
obtaining not only zero-order compressed space and log-logarithmic times, but

1 In terms of the usual entropy measures. It compresses to the k-th order entropy of

a diﬀerent sequence (A. Golynski, personal communication).

2 When we write o(n lg σ) bits we mean o(n) lg σ. Although in some cases [10,3] the
results are actually n o(lg σ), we point out that this can be taken as o(n) lg σ because,
if σ = O (polylog(n)), one can use a structure by Ferragina et al. [8, Thm. 3.2] that
solves access, rank, and select in constant time using nH0(s) + o(n) bits. Thus one
can assume σ = ω(1) at the very least. See also Footnote 6 of Barbay et al. [3].

Alphabet Partitioning for Compressed Rank/Select and Applications

317

also compressed redundancy. The technique can be summarized as partitioning
the alphabet into sub-alphabets according to the characters’ frequencies in s,
storing in a multiary wavelet tree [8] the string that results from replacing the
characters in s by identiﬁers of their sub-alphabets, and storing separate strings,
each the projection of s to the characters of s belonging to each sub-alphabet,
this time using Golynski et al.’s [10] structure for large alphabets. We achieve a
data structure that stores a string s[1..n] in nH0(s) + o(n)(H0(s) + 1) bits, thus
guaranteeing that the redundancy stays negligible even when the text is very
compressible. It supports queries in the times shown in Table 1 (rows 7 and 8
give two alternatives).

Then we consider various extensions and applications of our main result. In
Section 3 we show how our result can be used to improve an existing text index
that achieves k-th order entropy [8,3], so as to improve its redundancy and
query times. This way we achieve the ﬁrst self-index with space bounded by
nHk(s)+ o(n)(Hk(s)+ 1) bits, able of counting and locating pattern occurrences
and extracting any substring of s, within the time complexities achieved by either
of its predecessors. In Sections 4 and 5, respectively, we show how to apply our
data structure to store a compressed permutation, a compressed function and
a compressed dynamic collection of disjoint sets, while supporting a rich set of
operations on those. This improves or gives alternatives to the best previous
results [4,17,12]. We have approached these applications in such a way that an
improvement to our main result, however achieved, translates into improved
bounds for them as well.

2 Alphabet Partitioning

Let s[1..n] be a sequence over eﬀective alphabet [1..σ], i.e., every character appears 
in s, so σ ≤ n. (At the end of the section we handle the case of large
|s|a
n lg n|s|a , where
alphabets.) The zero-order entropy of s is H0(s) =
|s|a is the number of occurrences of the character a in s. Note that by convexity
we have nH0(s) ≥ (σ − 1) lg n + (n− σ + 1) lg(n/(n− σ + 1)), a property we will
use later.
m[1..σ] be the sequence assigning to each character a ∈ [1..σ] the value

Our results are based on the following alphabet partitioning scheme. Let

a∈[1..σ]

(cid:6)

1..

Let t[1..n] be the string over
obtained from s by replacing each
occurrence of a by m[a], for 1 ≤ a ≤ σ. For 0 ≤ (cid:3) ≤ (cid:5)lg2 n(cid:6), let σ(cid:4) be the number
of occurrences of (cid:3) in m or, equivalently, the number of distinct characters of
s replaced by (cid:3) in t. Finally, let s(cid:4)[1..|t|(cid:4)] be the string over [1..σ(cid:4)] deﬁned by
s(cid:4)[t.rank(cid:4)(i)] = m.rank(cid:4)(s[i]).
Notice that, if both a and b are replaced by the same number in t, then
lg(n/|s|b) − lg(n/|s|a) < 1/ lg n and so |s|a/|s|b < 21/ lg n. It follows that, if a is
replaced by (cid:3) in t, then σ(cid:4) < 21/ lg n|s(cid:4)|/|s|a (by ﬁxing a and summing over all
those b replaced by (cid:3)). Since

m[a] = (cid:5)lg(n/|s|a) lg n(cid:6) ≤ (cid:7)

(cid:9)

(cid:8)(cid:10)

(cid:7)
lg2 n

(cid:8)

lg2 n

.

318

J. Barbay et al.

(cid:11)

(cid:4)lg(n/|s|a) lg n(cid:5)=(cid:4)

|s|a = |s(cid:4)|

and

(cid:11)

a

|s|a =

(cid:11)

(cid:4)

|s(cid:4)| = n ,

we have

(cid:11)

(cid:4)

|s(cid:4)| lg σ(cid:4)
(cid:11)

nH0(t) +
(cid:11)

|s(cid:4)| lg(n/|s(cid:4)|) +

(cid:11)

(cid:2)
lg n|s(cid:4)|/|s|a
2 1

(cid:3)

|s|a lg

<

=

(cid:4)

(cid:11)

a

(cid:4)

(cid:4)lg(n/|s|a) lg n(cid:5)=(cid:4)

|s|a lg(n/|s|a) + n/ lg n

= nH0(s) + o(n) .

In other words, if we represent t with H0(t) bits per symbol and each s(cid:4) with
lg σ(cid:4) bits per symbol, we achieve a good overall compression. Thus we can obtain
a very compact representation of a string s by storing a compact representation
of t and storing each s(cid:4) as an “uncompressed” string over an alphabet of size σ(cid:4).
Now we show how our approach can be used to obtain a fast and compact
rank/select data structure. Suppose we have a data structure T that supports
access, rank and select queries on t; another structure M that supports the same
queries on m; and data structures S1, . . . , S(cid:4)lg2 n(cid:5) that support the same queries
on s1, . . . , s(cid:4)lg2 n(cid:5). With these data structures we can implement
s.access(i) = m.select(cid:4)(s(cid:4).access(t.rank(cid:4)(i))), where (cid:3) = t.access(i);

s.ranka(i) = s(cid:4).rankc(t.rank(cid:4)(i)), where (cid:3) = m.access(a) and c = m.rank(cid:4)(a);

s.selecta(i) = t.select(cid:4)(s(cid:4).selectc(i)) where (cid:3) = m.access(a) and c = m.rank(cid:4)(a).

(cid:2)

(cid:2)

(cid:3)

n(lg lg n)2

lg n

(cid:2)

We implement T and M as multiary wavelet trees [8]; we implement each
S(cid:4) as either a multiary wavelet tree or an instance of Golynski et al.’s [10,
Thm. 2.2] access/rank/select data structure, depending on whether σ(cid:4) ≤ lg n
or not. The wavelet tree for T uses at most nH0(t) + O
bits and
(cid:3)
operates in constant time, because its alphabet size is polylogarithmic. If S(cid:4) is
implemented as a wavelet tree, it uses at most |s(cid:4)|H0(s(cid:4)) + O
bits3 and operates in constant time for the same reason; otherwise it uses at most
|s(cid:4)| lg σ(cid:4) + O
(cid:3)
bits (the latter because σ(cid:4) >
lg n). Thus in either case the space for s(cid:4) is bounded by |s(cid:4)| lg σ(cid:4) +O
|s(cid:2)| lg |s(cid:2)|
lg lg lg n
bits. Finally, since M is a sequence of length σ over an alphabet of size (cid:5)lg2 n(cid:6),
the wavelet tree for M takes O (σ lg lg n) bits. Because of the property we referred 
to in the beginning of this section, nH0(s) ≥ (σ − 1) lg n, this space is
3 This is achieved by using block sizes of length lg n
storing universal tables of size O (
o(·) expressions involving n and other variables will be asymptotic in n.

, at the price of
n polylog(n)) = o(n) bits. Therefore all of our

≤ |s(cid:4)| lg σ(cid:4) + O

2 and not lg |s(cid:2)|

2

|s(cid:2)| lg σ(cid:2)
lg lg lg n

|s(cid:2)| lg |s(cid:2)| lg lg n

lg n

|s(cid:2)| lg σ(cid:2)
lg lg σ(cid:2)

(cid:2)

(cid:3)

(cid:3)

(cid:2)

√

Alphabet Partitioning for Compressed Rank/Select and Applications
(cid:2)

(cid:3)
. By these calculations, the space for T , M and the S(cid:4)’s adds

H0(s)O
up to nH0(s) + o(n)H0(s) + o(n), where the o(n) term is O

n lg lg n

319

lg n

(cid:2)

(cid:3)

n

.

lg lg lg n

min

(cid:2)

(cid:2)

min

(cid:3)(cid:3)

1 + lg σ(cid:2)

Depending on which time tradeoﬀ we use for Golynski et al.’s data structure,
we obtain the results of Table 1. We can reﬁne the time complexity by noticing
that the only non-constant times are due to operating on some string s(cid:4), where
the alphabet is of size σ(cid:4) < 21/ lg n|s(cid:4)|/|s|a, where a is the character in question,
thus lg lg σ(cid:4) = O (lg lg min(σ, n/|s|a)).
Theorem 1. We can store s[1..n] over eﬀective alphabet [1..σ] in nH0(s) +
o(n)(H0(s) + 1) bits and support access, rank and select queries in O (lg lg σ),
O (lg lg σ), and O (1) time, respectively (variant (i)). Alternatively, we can support 
access, rank and select queries in O (1), O (lg lg σ lg lg lg σ) and O (lg lg σ)
time, respectively (variant (ii)). Any of the σ terms in these time complexities
is actually min(σ, n/|s|a), where a stands for s[i] in the time of the access query,
and for the character argument in the time of the rank and select query.
Moreover, by implementing S(cid:4) as a wavelet tree whenever σ(cid:4) ≤ (lg n)lg lg lg n,
we ensure to achieve the complexities of wavelet trees if those are better than
the ones given above. That is, for example, O
instead
of just O (lg lg σ(cid:4)). We can similarly match the complexity O (lg lg σ(cid:4) lg lg lg σ(cid:4)).
(cid:3)
Note that, if we do this, the complexities that were O (1) become O
.
Corollary 1. All the time complexities up to O (lg lg σ) in variants (i) or (ii)
of Theorem 1 can be made O
1 + lg σ
. Alternatively, all time
(cid:2)
(cid:3)(cid:3)
complexities in variant (ii) can be made O
.
lg lg n , lg lg σ lg lg lg σ
As in Theorem 1, the σ term is actually min(σ, n/|s|a).
In the most general case, s is a sequence over an alphabet Σ which is not an
(cid:6) be the set of elements
eﬀective alphabet, and σ symbols from Σ occur in s. Let Σ
(cid:6) to elements of [1..σ] by replacing
that occur in s; we can map characters from Σ
each a ∈ Σ
(cid:6) are stored in the indexed
dictionary data structure described by Raman et al. [20], so that the following
queries are supported in constant time: for any a ∈ Σ
(cid:6) can be
(cid:6) the answer is −1); for any i ∈ [1..σ] the i-th smallest
found (for any a (cid:7)∈ Σ
(cid:6) can be found. The indexed dictionary of Raman et al. [20] uses
element in Σ
σ lg(eμ/σ) + o(σ) + O (lg lg μ) bits of space, where e is the base of the natural
(cid:6); the value of μ can be speciﬁed
logarithm and μ is the maximal element in Σ
with additional O (lg μ) bits. We replace every element in s by its rank in Σ
(cid:6),
and the resulting string is stored using Theorem 1. Hence, in the general case the
space usage is increased by σ lg(eμ/σ) + o(σ) + O (lg μ) bits and the asymptotic
time complexity of queries remains unchanged.

(cid:6) with its rank in Σ

lg lg n , lg lg σ(cid:4)
(cid:2)

(cid:6). All elements of Σ

(cid:6) its rank in Σ

lg lg n , lg lg σ
(cid:2)

1 + lg σ(cid:2)
lg lg n

1 + lg σ

(cid:3)(cid:3)

min

(cid:2)

(cid:2)

320

J. Barbay et al.

3 Reduced Redundancy on Self-indexes

Our result can be readily carried over self-indexes. These also represent a sequence,
but they support other operations related to text searching. A well known selfindex 
[8] achieves k-th order entropy space by partitioning the Burrows-Wheeler
transform [6] of the sequence and encoding each partition to its zero-order entropy.
Those partitions must support queries access and rank. By using Theorem 1(i)
to represent such partitions, we achieve the following result, improving previous
ones [8,10,3].
Theorem 2. Let s[1..n] be a string over alphabet4 [1..σ]. Then we can represent 
s using nHk(s) + o(n)(Hk(s) + 1) bits of space, for any k ≤ (α logσ n) − 1
and constant 0 < α < 1, while supporting the following queries: (i) count the
number of occurrences of a pattern p[1..m] in s, in time O (m lg lg σ); (ii) locate 
any such occurrence in time O (lg n lg lg lg n lg lg σ); (iii) extract s[l, r] in
time O ((r − l) lg lg σ + lg n lg lg lg n lg lg σ). The lg lg σ times can be reduced to
O

(cid:2)

(cid:3)

if convenient.

1 + lg σ
lg lg n

For these particular locating and extracting times we are sampling one out of
every lg n lg lg lg n text positions, which maintains our lower-order space term
o(n) at O (n/ lg lg lg n). Compared to Theorem 4.2 of Barbay et al. [3], we reduce
the redundancy from o(n) lg σ to o(n)(Hk(s) + 1). Our improved locating times,
however, just owe to the denser sampling, which they could also use.

4 Compressing Permutations

n lg n
ni

(cid:6) ni

We now show how to use access/rank/select data structures to store a compressed 
permutation. We follow Barbay and Navarro’s notation [4] and improve
their space and, especially, their time performance. They measure the compressibility 
of a permutation π in terms of the entropy of the distribution of the
lengths of runs of diﬀerent kinds. Let π be covered by ρ runs (using any of the
previous deﬁnitions of runs [13,4,16]) of lengths runs(π) = (cid:8)n1, . . . , nρ(cid:9). Then
≤ lg ρ is called the entropy of the runs (and, because
H(runs(π)) =
ni ≥ 1, it also holds nH(runs(π)) ≥ (ρ − 1) lg n). We ﬁrst consider permutations 
which are interleaved sequences of increasing or decreasing values as ﬁrst
deﬁned by Levcopoulos et al. [13] for adaptive sorting, and later on for compression 
[4], and then give improved results for more speciﬁc classes of runs. In both
cases we consider ﬁrst the application of the permutation π() and its inverse,
−1(), to show later how to extend the support to the iterated applications of
π
the permutation, πk(), extending and improving previous results [17].
Theorem 3. Let π be a permutation on n elements that consists of ρ interleaved 
increasing or decreasing runs, of lengths runs(π). We can store π in
−1() queries
2nH(runs(π)) + o(n)(H(runs(π)) + 1) bits and perform π() and π
in O

(cid:2)
1 + lg ρ

time.

(cid:3)(cid:3)

(cid:2)

min

lg lg n , lg lg ρ

4 Again, [1..σ] does not need to be the eﬀective alphabet (see paragraph after Thm. 1).

Alphabet Partitioning for Compressed Rank/Select and Applications

321

Proof. We ﬁrst replace all the elements of the rth run by r, for 1 ≤ r ≤ ρ.
(cid:6) be s permuted according to π, that is,
Let s be the resulting string and let s
(cid:6)[π(i)] = s[i]. We store s and s
(cid:6) using Theorem 1(i) and store ρ bits indicating
s
whether each run is increasing or decreasing. Notice that, if π(i) is part of an
increasing run, then s

.ranks[i](π(i)) = s.ranks[i](i), so

(cid:6)

(cid:6)

π(i) = s

.selects[i]

(cid:4)
(cid:5)
s.ranks[i](i)
.ranks[i](π(i)) = s.ranks[i](n) + 1 −
(cid:6)

;

if π(i) is part of a decreasing run, then s
s.ranks[i](i), so

(cid:6)

π(i) = s

.selects[i]

(cid:4)
(cid:5)
s.ranks[i](n) + 1 − s.ranks[i](i)

.

−1() query is symmetric. The space of the bitmap is ρ ∈ o(n)H(runs(π))
(cid:10)(cid:11)

A π
because nH(runs(π)) ≥ (ρ − 1) lg n.

We now consider the case of runs restricted to be strictly incrementing (+1) or
decrementing (−1), while still letting them be interleaved: such runs were not
directly considered before.

(cid:2)

min

(cid:3)(cid:3)

time and π

lg lg n , lg lg ρ

(cid:2)
1 + lg ρ

−1() queries in O (1/) time.

Theorem 4. Let π be a permutation on n elements that consists of ρ interleaved
strictly incrementing or decrementing runs. For any constant  > 0, we can store
π in nH(runs(π)) + o(n)(H(runs(π)) + 1) +O (ρn) bits and perform π() queries
in O
Proof. We ﬁrst replace all the elements of the rth run by r, for 1 ≤ r ≤ ρ,
considering the runs in order by minimum element. Let s ∈ {1, . . . , ρ}n be the
resulting string. We store s using Theorem 1(i); we also store an array containing
the runs’ lengths, directions (incrementing or decrementing), and minima, in
order by minimum element; and store a predecessor data structure containing the
runs’ minima as keys with their positions in the array as auxiliary information.
The predecessor data structure is based on Lemma 4 of Andersson’s paper [1].
It is an n-ary trie where the keys are sought considering  lg n bits per trie
node, and hence found in O (1/) time. Each of the ρ elements may require
O ((1/)n lg n) bit space for the n-size children arrays along its O (1/)-length
path. By slightly adjusting  the space is O (ρn) bits. With these data structures,
we can retrieve a run’s data given either its array index or any of its elements.
If π(i) is the jth element in an incrementing run whose minimum element
is m, then π(i) = m + j − 1; on the other hand, if π(i) is the jth element of a
decrementing run of length l whose minimum element is m, then π(i) = m+l−j.
It follows that, given i, we can compute π(i) by using the query j = s.ranks[i](i)
and then an array lookup at position s[i] to ﬁnd m, l and the direction, ﬁnally
computing π(i) from them. Also, given π(i), we can compute i by ﬁrst using a
predecessor query to ﬁnd the run’s array position r, then an array lookup to
ﬁnd m, l and the direction, then computing j = π(i) − m + 1 (increasing) or
(cid:10)(cid:11)
j = m + l − π(i) (decreasing), and ﬁnally using the query i = s.selectr(j).

322

J. Barbay et al.

(cid:3)(cid:3)

time.

(cid:2)

(cid:2)

min

1 + lg ρ

lg lg n , lg lg ρ

−1() queries in O

−1
Notice that, if π consists of ρ contiguous increasing or decreasing runs, then π
consists of ρ interleaved incrementing or decrementing runs. Therefore, Theorem 
4 applies to such permutations as well, with the time bounds for π() and
−1() queries reversed, which yields the following corollary:
π
Corollary 2. Let π be a permutation on n elements that consists of ρ contiguous 
increasing or decreasing runs. For any constant  > 0, we can store π in
nH(runs(π)) + o(n)(H(runs(π)) + 1) + O (ρn) bits and perform π() queries in
O (1/) time and π
If π’s runs are both contiguous and incrementing or decrementing, then so are
−1. In this case we can store π in O (ρn) bits and answer π() and
the runs of π
−1() queries in O (1) time. To do this, we use two predecessor data structures:
π
for each run, in one of the data structures we store the position j in π of the ﬁrst
element of the run, with π(j) as auxiliary information; in the other, we store
π(j), with j as auxiliary information. To perform a query π(i), we use the ﬁrst
predecessor data structure to ﬁnd the starting position j of the run containing i,
and return π(j)+ i− j. A π
−1() query is symmetric. Decreasing runs are handled
as before.
Corollary 3. Let π be a permutation on n elements that consists of ρ contiguous
incrementing or decrementing runs. For any constant  > 0, we can store π in
O (ρn) bits and perform π() and π
−k(i)) within compressed
We now show how to achieve exponentiation (πk(i), π
space. Munro et al. [17] reduced the problem of supporting exponentiation on
a permutation π to the support of the direct and inverse application of another
permutation, related but with quite distinct runs than π. Expressing their result
as a succinct index and combining it with any of our results does yield a compression,
 but one where the space depends of the lengths of both the runs and
cycles of π. The following construction, extending the technique from Munro et
al. [17], retains the compressibility properties of π by building a companion data
structure that uses small space to support the exponentiation, thus allowing the
compression of the main data structure with any of our results.
Theorem 5. Suppose we have a data structure D that stores a permutation π
on n elements and supports queries π() in time g(π). Then for any t ≤ n, we can
build a succinct index that takes O ((n/t) lg n) bits and, when used in conjunction
with D, supports πk() and π

−k() queries in O (t g(π)) time.

−1() queries in O (1/) time.

Proof. We decompose π into its cycles and, for every cycle of length at least t,
store the cycle’s length and an array containing pointers to every tth element
in the cycle, which we call ‘marked’. We also store a compressed binary string,
aligned to π, indicating the marked elements. For each marked element, we record
to which cycle it belongs and its position in the array of that cycle.

Alphabet Partitioning for Compressed Rank/Select and Applications

323

To compute πk(i), we repeatedly apply π() at most t times until we either
loop (in which case we need apply π() at most t more times to ﬁnd πk(i) in the
loop) or we ﬁnd a marked element. Once we have reached a marked element,
we use its array position and cycle length to ﬁnd the pointer to the last marked
element in the cycle before πk(i), and the number of applications of π() needed
−k query is similar (note that it does not
to map that to πk(i) (at most t). A π
(cid:10)(cid:11)
need to use π
As an example, given a constant  > 0 and a value t ≤ n, we can combine
Corollary 2 and Theorem 5 to obtain a data structure that stores Sadakane’s Ψ
function [21] for s in nH0(s) + o(n)(H0(s) + 1) + O (σn + (n/t) lg n) bits and
−k() queries in O (1/ + t) time; these queries are useful
supports Ψ k() and Ψ
when working on compressed suﬃx arrays and trees.

−1()).

5 Compressing Functions and Dynamic Collections of

Disjoint Sets

Hreinsson, Krøyer and Pagh [12] recently showed how, given X = {x1, . . . , xn} ⊆
[U] and f : [U] → [1..σ], where [U] is the set of numbers whose binary representations 
ﬁt in a machine word, they can store f restricted to X in compressed 
form with constant-time evaluation. Their representation uses at most
(1 + δ)nH0(f) + n min(pmax + 0.086, 1.82(1− pmax)) + o(σ) bits, where δ > 0 is a
given constant and pmax is the relative frequency of the most common function
value. We note that this bound holds even when σ (cid:14) n.
Notice that, in the special case where X = [1..n] and σ ≤ n, we can achieve
constant-time evaluation and a better space bound using Theorem 1. We can
also ﬁnd all the elements in [1..n] that f maps to a given element in [1..σ] (using
select), ﬁnd an element’s rank among the elements with the same image, or the
size of the preimage (using rank), etc.
Theorem 6. Let f : [1..n] → [1..σ] be a surjective function.5 We can represent
f using nH0(f) + o(n)(H0(f) + 1) bits so that any f(i) can be computed in O (1)
−1(a) can be computed in O (lg lg σ) time, and
time. Moreover, each element of f
−1(a)| requires time O (lg lg σ lg lg lg σ). Alternatively we can compute f(i) and
|f
|f
−1(a)| in time O (lg lg σ) and deliver any element of f
We omit the other improvements of Theorem 1 and Corollary 1 for conciseness.
We can also achieve interesting results with our theorems from Section 4, as runs
arise naturally in many real-life functions. For example, suppose we decompose
f(1), . . . , f(n) into ρ interleaved non-increasing or non-decreasing runs. Then
we can store it as a combination of the permutation π that stably sorts the
values f(i), plus a compressed rank/select data structure storing a binary string
b[1..n + σ + 1] with σ + 1 bits set to 1: if f maps i values in [1..n] to a value j in

−1(a) in O (1) time.

5 So that [1..σ] is the eﬀective alphabet size of string f . General functions with image
of size σ(cid:2) < σ require O (σ(cid:2)
)) + o(σ) extra bits, or we can handle them using
O (σ lg lg n) bits with our structure M .

lg(σ/σ(cid:2)

324

J. Barbay et al.

[1..σ] then, in b, there are i bits set to 0 between the jth and (j + 1)th bits set
to 1. Therefore,

f(i) = b.rank1(b.select0(π(i)))

(cid:5)

(cid:5)

σ lg n
σ

σ lg n
σ

+ o(n) bits [20].

−1(a) is
−1() to the area b.rank0(b.select1(a))+1 . . . b.rank0(b.select1
−1(a)| is computed in O (1) time. Notice that H(runs(π)) =

and the theorem below follows immediately from Theorem 3. Similarly, f
obtained by applying π
(a + 1)), and |f
H(runs(f)) ≤ H0(f), and that b can be stored in O (cid:4)
Corollary 4. Let f : [1..n] → [1..σ] be a surjective function6 with f(1), . . . , f(n)
store f in 2nH(runs(f)) + o(n)(H(runs(f)) + 1) + O (cid:4)
consisting of ρ interleaved non-increasing or non-decreasing runs. Then we can
bits and compute
−1(a), in O (lg lg ρ) time. The size
any f(i), as well as retrieve any element in f
|f
−1(a)| can be computed in O (1) time.
We can obtain a more competitive result if f is split into contiguous runs, but
their entropy is no longer bounded by the zero-order entropy of string f.
Corollary 5. Let f : [1..n] → [1..σ] be a surjective function with f(1), . . . , f(n)
represent f in nH(runs(f))+o(n)(H(runs(f))+1)+O (ρn)+O (cid:4)
consisting of ρ contiguous non-increasing or non-decreasing runs. Then we can
bits, for
any constant  > 0, and compute any f(i) in O (lg lg σ) time, as well as retrieve
−1(a)| can be computed in
any element in f
O (1) time.
Finally, we now give what is, to the best of our knowledge, the ﬁrst result about
storing a compressed collection of disjoint sets. The key point in the next theorem
is that, as the sets in the collection C are merged, our space bound shrinks with
the entropy of the distribution sets(C) of elements to sets.

−1(a) in O (1/) time. The size |f

(cid:5)

σ lg n
σ

Theorem 7. Let C be a collection of disjoint sets whose union is [1..n]. For
any constant  > 0, we can store C in (1 + )nH(sets(C)) + O (|C| lg n) + o(n)
bits and perform any sequence of m union and ﬁnd operations in a total of
O ((1/)(m + n) lg lg n) time.
Proof. We ﬁrst use Theorem 1 to store the string s[1..n] in which each s[i] is
the representative of the set containing i. We then store the representatives in a
standard disjoint-set data structure D [22]. Together, our data structures take
nH(sets(C)) + O (|C| lg n) + o(n)(H(sets(C)) + 1) bits. We can perform a query
ﬁnd(i) on C by performing D.ﬁnd(s[i]), and perform a union(i, j) operation on
C by performing D.union(D.ﬁnd(s[i]), D.ﬁnd(s[j])).

For our data structure to shrink as we union sets, we keep track of H(sets(C))
and, whenever it shrinks by a factor of 1+ , we rebuild our entire data structure
on the updated values s[i] ← ﬁnd(s[i]). First, note that all those ﬁnd operations
take O (n) time because of path-compression [22]: Only the ﬁrst time one accesses
a node v ∈ C it may occur that the representative is not directly v’s parent.
6 Otherwise we proceed as usual to map the domain to the eﬀective one.

Alphabet Partitioning for Compressed Rank/Select and Applications

325

Reconstructing the structure of Theorem 1 takes also O (n) time: As we need
just access on s, we need only rank and access on our multiary wavelet tree and
access on the s(cid:4) sequences. Thus the latter are implemented simply as arrays
(cid:5)
Since H(sets(C)) is always less than lg n, we rebuild only O (cid:4)
and the former are also easily built in linear time for these two queries [8].
=
log1+ lg n
O ((1/) lg lg n) times. Finally, the space term o(n)H(sets(C)) is absorbed by
(cid:10)(cid:11)
H(sets(C)) by slightly adjusting .

6 Conclusions and Future Work

We have presented the ﬁrst zero-order compressed representation of strings efﬁciently 
supporting queries access, rank, and select, so that the redundancy of
the compressed representation is also compressed. That is, our space for string
s[1..n] over alphabet [1..σ] is nH0(s) + o(n)(H0(s) + 1) instead of the usual
nH0(s) + o(n) lg σ bits. This is very important in many practical applications
where the data is highly compressible and the redundancy would otherwise dominate 
the overall space.
In the full paper we will work on several improvements and further applications.
 First, we can reduce the dependence on the alphabet size from O (σ lg lg n)
to O (σ) by storing a length-restricted Shannon code in O (σ) bits [9] instead
of the data structure M. To avoid the O (1) extra redundancy per character
associated with using a length-restricted preﬁx code, we replace each character 
in s whose codeword length is at most lg lg n by a distinct number in t.
This increases the alphabet size of t by at most lg n; calculation shows that
our space bound increases by an O (1 + 1/ lg lg n)-factor and, thus, remains at
most nH0(s) + o(n)(H0(s) + 1). Second, given any constant c, we can reduce the
min(σ, n/|s|a) in our time bounds by a factor of (lg n)c; to do this, we further
partition each sub-alphabet into (lg n)c sub-sub-alphabets. Third, our alphabet
partitioning techniques yields a compressed representation of posting lists of
sizes (n1, . . . , nσ) which supports access, rank and select on the rows in time
O (lg lg σ), and uses total space for data and index proportional to the entropy
H(n1, . . . , nσ) of the distribution of those sizes (if the posting lists refer to the
words of a text, this is also the zero-order word-based entropy of the text). This
is achieved by encoding the string of labels encountered during a row-ﬁrst traversal,
 writing a special symbol (e.g. $) at each change of row. This improves the
space of previously known data structures [2], and improves the time complexity
of previous compression results [5].

Naturally, the next challenge ahead is to obtain a data structure using space
nHk(s)+o(n)(Hk(s)+1) bits rather than nHk(s)+o(n) lg σ, while still supporting
the queries access, rank, and select, in reasonable time. Note that Barbay et al. [3]
achieve nHk(s) + o(n) lg σ for such a structure: we have reduced the redundancy
to o(n)(Hk(s) + 1) for the case k = 0 and for self-indexes, but not for the basic
problem in the general case where k = o(logσ n).

Acknowledgments. Many thanks to Djamal Belazzougui for helpful comments
on a draft of this paper.

326

J. Barbay et al.

References

1. Andersson, A.: Sorting and searching revisited. In: Karlsson, R., Lingas, A. (eds.)

SWAT 1996. LNCS, vol. 1097, pp. 185–197. Springer, Heidelberg (1996)

2. Barbay, J., Golynski, A., Munro, J.I., Rao, S.S.: Adaptive searching in succinctly
encoded binary relations and tree-structured documents. Theoretical Computer
Science 387(3), 284–297 (2007)

3. Barbay, J., He, M., Munro, J.I., Rao, S.S.: Succinct indexes for strings, binary

relations and multi-labeled trees. In: Proc. 18th SODA, pp. 680–689 (2007)

4. Barbay, J., Navarro, G.: Compressed representations of permutations, and applications.
 In: Proc. 26th STACS, pp. 111–122 (2009)

5. Blandford, D., Blelloch, G.: Index compression through document reordering. In:

Proc. DCC, pp. 342–351 (2002)

6. Burrows, M., Wheeler, D.: A block sorting lossless data compression algorithm.

Technical Report 124, Digital Equipment Corporation (1994)

7. Claude, F., Navarro, G.: Practical rank/select queries over arbitrary sequences. In:
Amir, A., Turpin, A., Moﬀat, A. (eds.) SPIRE 2008. LNCS, vol. 5280, pp. 176–187.
Springer, Heidelberg (2008)

8. Ferragina, P., Manzini, G., M¨akinen, V., Navarro, G.: Compressed representations

of sequences and full-text indexes. ACM Transactions on Algorithms 3(2)

9. Gagie, T., Navarro, G., Nekrich, Y.: Fast and compact preﬁx codes. In: van
Leeuwen, J., Muscholl, A., Peleg, D., Pokorn´y, J., Rumpe, B. (eds.) SOFSEM
2010. LNCS, vol. 5901, pp. 419–427. Springer, Heidelberg (2010)

10. Golynski, A., Munro, J.I., Rao, S.S.: Rank/select operations on large alphabets: a

tool for text indexing. In: Proc. 17th SODA, pp. 368–373 (2006)

11. Grossi, R., Gupta, A., Vitter, J.: High-order entropy-compressed text indexes. In:

Proc. 14th SODA, pp. 841–850 (2003)

12. Hreinsson, J.B., Krøyer, M., Pagh, R.: Storing a compressed function with constant
time access. In: Fiat, A., Sanders, P. (eds.) ESA 2009. LNCS, vol. 5757, pp. 730–
741. Springer, Heidelberg (2009)

13. Levcopoulos, C., Petersson, O.: Sorting shuﬄed monotone sequences. Information

and Computation 112(1), 37–50 (1994)

14. M¨akinen, V., Navarro, G.: Rank and select revisited and extended. Theoretical

Computer Science 387(3), 332–347 (2007)

15. Manzini, G.: An analysis of the Burrows-Wheeler transform. Journal of the

ACM 48(3), 407–430 (2001)

16. Mehlhorn, K.: Sorting presorted ﬁles. In: Weihrauch, K. (ed.) GI-TCS 1979. LNCS,

vol. 67, pp. 199–212. Springer, Heidelberg (1979)

17. Munro, J.I., Raman, R., Raman, V., Rao, S.S.: Succinct representations of permutations.
 In: Baeten, J.C.M., Lenstra, J.K., Parrow, J., Woeginger, G.J. (eds.)
ICALP 2003. LNCS, vol. 2719, pp. 345–356. Springer, Heidelberg (2003)

18. Navarro, G., M¨akinen, V.: Compressed full-text indexes. ACM Computing Surveys 
39(1):article 2 (2007)

19. Rahman, N., Raman, R.: Rank and select operations on binary strings. In: Kao,

M.-Y. (ed.) Encyclopedia of Algorithms. Springer, Heidelberg (2008)

20. Raman, R., Raman, V., Rao, S.: Succinct indexable dictionaries with applications

to encoding k-ary trees and multisets. In: Proc. 13th SODA, pp. 233–242 (2002)

21. Sadakane, K.: New text indexing functionalities of the compressed suﬃx arrays.

Journal of Algorithms 48(2), 294–313 (2003)

22. Tarjan, R.E., van Leeuwen, J.: Worst-case analysis of set union algorithms. Journal

of the ACM 31(2), 245–281 (1984)

