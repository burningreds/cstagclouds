k2-Trees for Compact Web Graph Representation(cid:2)

Nieves R. Brisaboa1, Susana Ladra1, and Gonzalo Navarro2

1 Database Lab., Univ. of A Coru˜na, Spain

{brisaboa,sladra}@udc.es

2 Dept. of Computer Science, Univ. of Chile, Chile

gnavarro@dcc.uchile.cl

Abstract. This paper presents a Web graph representation based on
a compact tree structure that takes advantage of large empty areas of
the adjacency matrix of the graph. Our results show that our method is
competitive with the best alternatives in the literature, oﬀering a very
good compression ratio (3.3–5.3 bits per link) while permitting fast navigation 
on the graph to obtain direct as well as reverse neighbors (2–15
microseconds per neighbor delivered). Moreover, it allows for extended
functionality not usually considered in compressed graph representations.

1 Introduction

The World Wide Web structure can be regarded as a directed graph at several
levels, the ﬁnest grained one being pages that point to pages. Many algorithms
of interest to obtain information from the Web structure are essentially basic
algorithms applied over the Web graph [11, 16].

Running typical algorithms on those huge Web graphs is always a problem.
Even the simplest external memory graph algorithms, such as graph traversals,
are usually non disk-friendly [24]. This has pushed several authors to consider
compressed graph representations, which aim to oﬀer memory-eﬃcient graph
representations that still allow fast navigation without decompressing. The aim
of this research is to allow classical graph algorithms to be run in main memory
over much larger graphs than those aﬀordable with a plain representation.

The most famous representative of this trend is surely Boldi and Vigna’s WebGraph 
Framework [6]. The WebGraph compression method is indeed the most
successful member of a family of approaches to compress Web graphs based on
their statistical properties [1, 5, 7, 20, 21, 23]. It allows fast extraction of the
neighbors of a page while spending just a few bits per link (about 2 to 6, depending 
on the desired navigation performance). Their representation explicitly
exploits Web graph properties such as: (1) the power-law distribution of indegrees 
and outdegrees, (2) the locality of reference, (3) the “copy property” (the
set of neighbors of a page is usually very similar to that of some other page).

More recently, Claude and Navarro [10] showed that most of those properties
are elegantly captured by applying Re-Pair compression [17] on the adjacency

(cid:2) Funded in part (for the Spanish group) by MEC grant (TIN2006-15071-C03-03); and

for the third author by Fondecyt Grant 1-080019, Chile.

J. Karlgren, J. Tarhio, and H. Hyyr¨o (Eds.): SPIRE 2009, LNCS 5721, pp. 18–30, 2009.
c(cid:2) Springer-Verlag Berlin Heidelberg 2009

k2-Trees for Compact Web Graph Representation

19

lists, and that reverse navigation (ﬁnding the pages that point to a given page)
could be achieved by representing the output of Re-Pair using some more sophisticated 
data structures [9]. Reverse navigation is useful to compute several
relevance ranking on pages, such as HITS, PageRank, and others. Their technique 
oﬀers better space/time tradeoﬀs than WebGraph, that is, they oﬀer faster
navigation than WebGraph when both structures use the same space.

Asano et al. [2] achieve even less than 2 bits per link by explicitly exploiting
regularity properties of the adjacency matrix of the Web graphs, but their navigation 
time is substantially higher, as they need to uncompress full domains in
order to ﬁnd the neighbors of a single page.

In this paper we also aim at exploiting the properties of the adjacency matrix,
 yet with a general technique to take advantage of clustering rather than a
technique tailored to particular Web graphs. We introduce a compact tree representation 
of the matrix that not only is very eﬃcient to represent large empty
areas of the matrix, but at the same time allows eﬃcient forward and backward
navigation of the graph. An elegant feature of our solution is that it is symmetric,
 both navigations are carried out by similar means and achieve similar times.
In addition, our proposal allows some interesting operations that are not usually
present in alternative structures.

2 Our Proposal
The adjacency matrix of a Web graph of n pages is a square matrix {aij} of n×n,
where each row and each column represents a Web page. Cell aij is 1 if there
is a hyperlink in page i towards page j, and 0 otherwise. Page identiﬁers are
integers, which correspond to their position in an array of alphabetically sorted
URLs. This puts together the pages of the same domains, and thus locality of
reference translates into closeness of page identiﬁers. As on average there are
about 15 links per Web page, this matrix is extremely sparse. Due to locality
of reference, many 1s are placed around the main diagonal (that is, page i has
many pointers to pages nearby i). Due to the copy property, similar rows are
common in the matrix. Finally, due to skewness of distribution, some rows and
colums have many 1s, but most have very few.

We propose a compact representation of the adjacency matrix that exploits its
sparseness and clustering properties. The representation is designed to compress
large matrix areas with all 0s into very few bits.
We represent the adjacency matrix by a k2-ary tree, which we call k2-tree, of
height h = (cid:2)logk n(cid:3). Each node contains a single bit of data: 1 for the internal
nodes and 0 for the leaves, except for the last level, where all are leaves and
represent bit values of the matrix. The ﬁrst level (numbered 0) corresponds to
the root; its k2 children are represented at level 1. Each child is a node and
therefore it has a value 0 or 1. All internal nodes (i.e., with value 1) have exactly
k2 children, whereas leaves (with value 0 or at the last tree level) have no children.
Assume for simplicity that n is a power of k; we will soon remove this assumption.
 Conceptually, we start dividing the adjacency matrix following a MXQuadtree 
strategy [22, Section 1.4.2.1] into k2 submatrices of the same size, that

20

N.R. Brisaboa, S. Ladra, and G. Navarro

1

0

0

0

1

1

0

0

0

0

1

1

0

0

1

0

1

0

0

1

11

0 1

1 1 1

0

(a) First example.

Retrieving direct neighbors for page 10

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

1

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

1

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

1

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

1

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

1

1

1

1

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0
0

0
0

0
0

0
0

0
0

0
0

0
0

0
0

0
0

0
1

0
0

0

0

0

0

0

0
0

0
0

0
0

0
0

0
0

0
0

0
0

0
0

0
1

0
0

0
1

0

0

0

0

0

0
0

0
0

0
0

0
0

0
0

0
0

0
0

0
0

0
0

0
1

0
0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

page

10

1

0

1

1

1

1

0

1

0

1

0

1

1 0 0

1

0

0 0

0 0 0

1

0

1

0

1

0

1

1

0

0

0

1

1

0

0100 0011

0010

0010

1010 1000

0110 0010

0100

(b) Expansion, subdivision and navigation for k = 2.

Fig. 1. k2-tree examples

is, k rows and k columns of submatrices of size n2/k2. Each of the resulting k2
submatrices will be a child of the root node and its value will be 1 iﬀ in the cells
of the submatrix there is at least one 1. A 0 child means that the submatrix has
all 0s and hence the tree decomposition ends there.

The children of a node are ordered in the tree starting with the submatrices
in the ﬁrst (top) row, from left to right, then the submatrices in the second row
from left to right, and so on. Once the level 1, with the children of the root, has
been built, the method proceeds recursively for each child with value 1, until
we reach submatrices full of 0s, or we reach the cells of the original adjacency
matrix. In the last level of the tree, the bits of the nodes correspond to the
matrix cell values. Figure 1(a) illustrates a 22-tree for a 4 × 4 matrix.

A larger k induces a shorter tree, with fewer levels, but more children per
internal node. If n is not a power of k, we conceptually extend our matrix to the
right and bottom with 0s, making it of width n(cid:3) = k(cid:4)logk n(cid:5). This does not cause
a signiﬁcant overhead as our technique is eﬃcient to handle large areas of 0s.
Figure 1(b) shows an example of the adjacency matrix of a Web graph (we use
the ﬁrst 11 × 11 submatrix of graph CNR [6]), how it is expanded to an n(cid:3) × n(cid:3)
matrix (n(cid:3) power of k = 2) and its corresponding tree. Notice that its last level
represents cells in the original adjacency matrix, but most cells in the original
adjacency matrix are not represented in this level because, where a large area
with 0s is found, it is represented by a single 0 in a smaller level of the tree.

2.1 Navigating with a k2-Tree

To obtain the pages pointed by a speciﬁc page p, that is, to ﬁnd direct neighbors
of page p, we need to ﬁnd the 1s in row p of the matrix. We start at the root
and travel down the tree, choosing exactly k children of each node.
Example. We ﬁnd the pages pointed by the ﬁrst page in the example of
Figure 1(a), that is, ﬁnd the 1s of the ﬁrst matrix row. We start at the root
of the 22-tree and compute which children of the root overlap the ﬁrst row of
the matrix. These are the ﬁrst two children, to which we move:

– The ﬁrst child is a 1, thus it has children. To ﬁgure out which of its children
are useful we repeat the same procedure. We compute in the corresponding

k2-Trees for Compact Web Graph Representation

21

submatrix (the one at the top left corner) which of its children represent
cells overlapping the ﬁrst row of the original matrix. These are the ﬁrst and
the second children. They are leaf nodes and their values are 1 and 1.

– The second child of the root represents the second submatrix, but its value
is 0. This means that all the cells in the adjacency matrix in this area are 0.
Thus, the Web page represented by the ﬁrst row has links to itself and page 2.
Figure 1(b) shows this navigation for a larger example.
Reverse neighbors. An analogous procedure retrieves the list of reverse neighbors.
 To obtain which pages point to page q, we need to locate which cells have
a 1 in column q of the matrix. Thus, we carry out a symmetric algorithm, using
columns instead of rows.

Summarizing, searching for direct or for reverse neighbors in the k2-tree is
completely symmetric. The only diﬀerence is the formula to compute the children
of each node used in the next step. In either case we perform a top-down traversal
of the tree. If we want to search for direct(reverse) neighbors in a k2-tree, we go
down through k children forming a row(column) inside the matrix.

3 Data Structure and Algorithms

Our data structure is essentially a compact tree of N nodes. There exist several
such representations for general trees [4, 12, 14, 19], which asymptotically approach 
the information-theoretic minimum of 2N + o(N) bits. In our case, where
there are only arities k2 and 0, the information-theoretic minimum of N + o(N)
bits is achieved by a so-called “ultra-succinct” representation [15] for general
trees. Our representation is much simpler, and close to the so-called LOUDS
(level-ordered unary degree sequence) tree representation [14] (which would not
achieve N + o(N) bits if directly applied to our trees).

Our data structure can be regarded as a simpliﬁed variant of LOUDS for the
case where arities are just k2 and 0, which achieves the information-theoretic
minimum of N +o(N) bits, provides the traversal operations we require (basically
move to the i-th child, although also parent is easily supported) in constant time,
and is simple and practical.

3.1 Data Structure

We represent the whole adjacency matrix via the k2-tree using two bit arrays:

T (tree): stores all the bits of the k2-tree except those in the last level. The bits
are placed following a levelwise traversal: ﬁrst the k2 binary values of the
children of the root node, then the values of the second level, and so on.

L (leaves): stores the last level of the tree. Thus it represents the value of (some)

original cells of the adjacency matrix.

We create over T an auxiliary structure that enables us to compute rank
queries eﬃciently. Given an oﬀset i inside a sequence T of bits, rank(T, i) counts

22

N.R. Brisaboa, S. Ladra, and G. Navarro

the number of times the bit 1 appears in T [1, i]. This can be supported in constant 
time and fast in practice using sublinear space on top of the bit sequence
[14, 18]. In practice we use an implementation that uses 5% of extra space on
top of the bit sequence and provides fast queries, as well as another that uses
37.5% extra space and is much faster [13].

We do not need to perform rank over the bits in the last level of the tree;
that is the practical reason to store them in a diﬀerent bitmap (L). Thus the
space overhead for rank is paid only over T .
Analysis. Assume the graph has n pages and m links. Each link is a 1 in the
matrix, and in the worst case it induces the storage of one distinct node per
level, for a total of (cid:2)logk2(n2)(cid:3) nodes. Each such (internal) node costs k2 bits,
for a total of k2m(cid:2)logk n(cid:3) bits. However, especially in the upper levels, not all
the nodes in the path to each leaf can be diﬀerent. In the worst case, all the
nodes exist up to level (cid:4)logk2 m(cid:5) (only since that level there can be m diﬀerent
internal nodes at the same level). From that level, the worst case is that each of
the m paths to the leaves is unique. Thus, in the worst case, the total space is
(cid:2)(cid:6)logk2 m(cid:7)

k2(cid:2) + k2m((cid:2)logk2 n2(cid:3) − (cid:4)logk2 m(cid:5)) = k2m(logk2

n2
m + O(1)) bits.

(cid:2)=1
This shows that, at least in a worst-case analysis, a smaller k yields less space
n2
occupancy. For k = 2 the space is 4m(log4
m + O(m) bits,
which is asymptotically twice the information-theoretic minimum necessary to
represent all the matrices of n × n with m 1s. In the experimental section we
see that, on Web graphs, the space is much better than the worst case, as Web
graphs are far from uniformly distributed.

n2
m + O(1)) = 2m log2

Finally, the expansion of n to the next power of k can, in the horizontal
direction, force the creation of at most k(cid:2) new children of internal nodes at level
(cid:2) ≥ 1 (level (cid:2) = 1 is always fully expanded unless the matrix is all zeros). Each
such child will cost k2 extra bits. The total excess is O(k2 · k(cid:4)logk n(cid:5)−1) = O(k2n)
bits, which is usually negligible. The vertical expansion is similar.

3.2 Finding a Child of a Node

Our levelwise traversal satisﬁes the following property, which permits fast navigation 
to the i-th child of node x, childi(x) (for 0 ≤ i < k2):
Lemma 1. Let x be a position in T (the ﬁrst position being 0) such that T [x] =
1. Then childi(x) is at position rank(T, x) · k2 + i of T : L
Proof. T : L is formed by traversing the tree levelwise and appending the bits
of the tree. We can likewise regard this as traversing the tree levelwise and
appending the k2 bits of the childred of the 1s found at internal tree nodes. By
the time node x is found in this traversal, we have already appended k2 bits per
1 in T [1, x − 1], plus the k2 children of the root. As T [x] = 1, the children of x
are appended at positions rank(T, x) · k2 to rank(T, x) · k2 + (k2 − 1).
Example. To represent the 22-tree of Figure 1(b), arrays T and L are:

T = 1011 1101 0100 1000 1100 1000 0001 0101 1110,
L = 0100 0011 0010 0010 1010 1000 0110 0010 0100.

k2-Trees for Compact Web Graph Representation

23

Direct(n, p, q, x)
1.
2.
3.
4.
5.
6.
7.

If x ≥ |T| Then // leaf

If L[x − |T|] = 1 Then output q
If x = −1 or T [x] = 1 Then

Else // internal node
y = rank(T, x) · k2
For j = 0 . . . k − 1 Do

Direct(n/k, p mod (n/k),

q + (n/k) · j, y + j)

+ k · (cid:6)p/(n/k)(cid:7)

If x ≥ |T| Then // leaf

Reverse(n, q, p, x)
1.
2.
3.
4.
5.
6.
7.

If L[x − |T|] = 1 Then output p
If x = −1 or T [x] = 1 Then

Else // internal node
y = rank(T, x) · k2
For j = 0 . . . k − 1 Do

Reverse(n/k, q mod (n/k),

+ (cid:6)q/(n/k)(cid:7)

p +(n/k)·j, y + j·k)

Fig. 2. Returning direct(reverse) neighbors

In T each bit represents a node. First four bits represent nodes 0, 1, 2 and 3,
which are the children of the root. The following four bits represent the children
of node 0. There are no children for node 1 because it is a 0, then the children
of node 2 start at position 8 and those of node 3 start at position 12. The bit in
position 4, the ﬁfth bit of T , represents the ﬁrst child of node 0, and so on.

3.3 Navigation

To ﬁnd the direct(reverse) neighbors of a page p(q) we need to locate which
cells in row ap∗ (column a∗q) of the adjacency matrix have a 1. We have already
explained that these are obtained by a top-down tree traversal that chooses k
out of the k2 children of a node, and also gave the way to obtain the i-th child
of a node in our representation. The only missing piece is the formula that maps
global row numbers to the children number at each level.
Recall h = (cid:2)logk n(cid:3) is the height of the tree. Then the nodes at level (cid:2) represent
square submatrices of width kh−(cid:2), and these are divided into k2 submatrices of
width kh−(cid:2)−1. Cell (p(cid:2), q(cid:2)) at a matrix of level (cid:2) belongs to the submatrix at row
(cid:4)p(cid:2)/kh−(cid:2)−1(cid:5) and column (cid:4)q(cid:2)/kh−(cid:2)−1(cid:5).
Let us call p(cid:2) the relative row position of interest at level (cid:2). Clearly p0 =
p, and row p(cid:2) of the submatrix of level (cid:2) corresponds to children number k ·
(cid:4)p(cid:2)/kh−(cid:2)−1(cid:5) + j, for 0 ≤ j < k. The relative position in those children is p(cid:2)+1 =
p(cid:2) mod kh−(cid:2)−1. Similarly, column q corresponds q0 = q and, in level (cid:2), to children
j · k + (cid:4)q(cid:2)/kh−(cid:2)−1(cid:5), for 0 ≤ j < k, with relative position q(cid:2)+1 = q(cid:2) mod kh−(cid:2)−1.
The algorithms for extracting direct and reverse neighbors are described in
Figure 2. For direct neighbors it is called Direct(kh, p, 0,−1), where the parameters 
are: current submatrix size, row of interest in current submatrix, column
oﬀset of the current submatrix in the global matrix, and the position in T : L
of the node to process (the initial −1 is an artifact because our trees do not
represent the root node). Values T , L, and k are global. It is assumed that
n is a power of k and that rank(T,−1) = 0. For reverse neighbors it is called
Reverse(kh, q, 0,−1), where the parameters are the same except that the second
is the column of interest and the third is the row oﬀset of the current submatrix.

24

N.R. Brisaboa, S. Ladra, and G. Navarro

Analysis. Our navigation time has no worst-case guarantees better than O(n),
as a row p − 1 full of 1s followed by p full of 0s could force a Direct query on p
to go until the leaves across all the row, to return nothing.
However, this is unlikely. Assume the m 1s are uniformly distributed in the
matrix. Then the probability that a given 1 is inside a submatrix of size (n/k(cid:2))×
(n/k(cid:2)) is 1/k2(cid:2). Thus, the probability of entering the children of such submatrix
is (brutally) upper bounded by m/k2(cid:2). We are interested in k(cid:2) submatrices at
each level of the tree, and therefore the total work is on average upper bounded
by m·(cid:2)h−1
k(cid:2)/k2(cid:2) = O(m). This can be reﬁned because there are not m diﬀerent
submatrices in the ﬁrst levels of the tree. Assume we enter all the O(kt) matrices
of interest up to level t = (cid:4)logk2 m(cid:5), and from then on the sum above applies.
This is O(kt + m · (cid:2)h−1
m) time. This is not
the ideal O(m/n) (average output size), but much better than O(n) or O(m).

√
k(cid:2)/k2(cid:2)) = O(kt + m/kt) = O(

Again, if the matrix is clustered, the average performance is indeed better
than under uniform distribution: whenever a cell close to row p forces us to
traverse the tree down to it, it is likely that there is a useful cell at row p as well.

(cid:2)=0

(cid:2)=t+1

3.4 Construction
Assume our input is the n× n matrix. Construction of our tree is easily carried o
ut bottom-up in linear time and using the same space as the ﬁnal tree. If, instead,
we have an adjacency list representation of the matrix, we can still achieve the
same time by setting up n cursors, one per row, so that each time we have to
access apq we compare the current cursor of row p with value q.

In this case we could try to achieve time proportional to m, the number of 1s
in the matrix. For this sake we could insert the 1s one by one into an initially
empty tree, building the necessary part of the path from the root to the corresponding 
leaf. After the tree is built we can traverse it levelwise to build the
ﬁnal representation, or recursively to output the bits to diﬀerent sequences, one
n2
m )), that is, proper 
level, as before. The space could still be O(k2m(1 + logk2
portional to the ﬁnal tree size, if we used some dynamic compressed parentheses
representation of trees [8]. The total time would be O(log m) per bit of the tree.
As we produce each tree level and traverse each matrix row (or adjacency list)
sequentially, we can construct the tree on disk in optimal I/O time provided we
have main memory to maintain logk n disk blocks to output the tree, plus B
disk blocks (B being the disk page size in bits) for reading the matrix.

4 A Hybrid Approach

As we can notice, the greater k is, the more space L needs, because even though
there are fewer submatrices in the last level, they are larger. Hence we may spend
k2 bits to represent very few 1s. Notice for example that if k = 4 in Figure 1(b),
we will store some last-level submatrices containing a unique 1, spending 15 more
bits that are 0. On the contrary, when k = 2 we use fewer bits for that leaf level.
We can improve our structure if we use a larger k for the ﬁrst levels of the
tree and a small k for the last levels. This strategy takes advantage of the strong

k2-Trees for Compact Web Graph Representation

25

points of both approaches. Using large values of k for the ﬁrst levels, the tree is
shorter, so we will be able to obtain the list of neighbors faster, as we have fewer
levels to traverse. Using small values of k for the last levels we do not store too
many bits for each 1 of the adjacency matrix, as the submatrices are smaller.

5 Experimental Evaluation

We ran several experiments over some Web crawls from the WebGraph project.
Figure 3(a) gives the main characteristics of the graphs used: name (and ver-
sion) of the graph, number of pages and links and the size of a plain adjacency
list representation of the graphs (using 4-byte integers). The machine used in
our tests is a 2GHz Intel R(cid:2)
Xeon R(cid:2) (8 cores) with 16 GB RAM. It ran Ubuntu
GNU/Linux with kernel version 2.4.22-15-generic SMP (64 bits). The compiler
was gcc version 4.1.3 and -O9 compiler optimizations were set. Space is measured 
in bits per edge (bpe), by dividing the total space of the structure by the
number of edges (i.e., links) in the graph. Time results measure average cpu user
time per neighbor retrieved: We compute the time to search for the neighbors of
all the pages (in random order) and divide by the total number of edges in the
graph.

5.1 Comparison between Diﬀerent Alternatives
We ﬁrst study our approach with diﬀerent values of k. Figure 3(b) shows 8
diﬀerent alternatives of our method over the EU graph using diﬀerent values of
k. All build on the rank structure that uses 5% of extra space [13]. The ﬁrst
column names the approaches as follows: (cid:3)2× 2(cid:3), (cid:3)3× 3(cid:3) and (cid:3)4× 4(cid:3) stand for the
alternatives where we subdivide the matrix into 2×2, 3×3 and 4×4 submatrices,
respectively, in every level of the tree. On the other hand, we denote (cid:3)H − i(cid:3) the
hybrid approach where we use k = 4 up to level i of the tree, and then we
use k = 2 for the rest of the levels. The second and third columns indicate
the size, in bytes, used to store the tree T and the leaves L, respectively. The
fourth column shows the space needed in main memory by the structures (e.g.,
including the extra space for rank), in bits per edge. Finally, the last two columns
show the times to retrieve the direct (ﬁfth column) and reverse (sixth) neighbors,
measured in microseconds per link retrieved (μs/e). Note that, when we use a
ﬁxed k, we obtain better times when k is greater, because we are shortening the
height of the tree, but the compression ratio worsens, as the space for L becomes
dominant and many 0s are stored in there.
If we use a hybrid approach, we can maintain a compression ratio close to
that obtained by the (cid:3)2 × 2(cid:3) alternative while improving the time, until we get
close to the (cid:3)4 × 4(cid:3) alternative. The best compression is obtained for (cid:3)H − 3(cid:3),
even better than (cid:3)2 × 2(cid:3). Figure 3(c) shows similar results graphically, for the
three larger graphs, space on the left and time to retrieve direct neighbors on the
right. The space does not worsen much if we keep k = 4 up to a moderate level,
whereas times improve consistently. A medium value, say switching to k = 2 at
level 7, looks as a good compromise.

26

N.R. Brisaboa, S. Ladra, and G. Navarro

(a) Description of the graphs used.

(b) Diﬀerent approaches over graph EU.

File

CNR (2000)
EU (2005)
Indochina (2004)
UK (2002)

Links

Pages

Size
(millions) (millions) (MB)
14
3.216
77
19.235
194.109
769
298.113 1,208

0.325
0.862
7.414
18.520

(bytes)

Leaves
(bytes)

Variant Tree
2 × 2 6,860,436 5,583,076 5.21076 2.56
3 × 3 5,368,744 9,032,928 6.02309 1.78
4 × 4 4,813,692 12,546,092 7.22260 1.47
H − 1 6,860,432 5,583,100 5.21077 2.78
H − 3 6,860,412 5,583,100 5.21076 2.67
H − 5 6,864,404 5,583,100 5.21242 2.39
H − 7 6,927,924 5,583,100 5.23884 2.10
H − 9 8,107,036 5,583,100 5.72924 1.79

Space Direct Reverse
(μs/e) (μs/e)
(bpe)
2.47
1.71
1.42
2.62
2.49
2.25
1.96
1.67

Space (Hybrid approach)

Speed (Hybrid approach)

)
e
g
d
e

/
s
t
i

b
(
 

e
c
a
p
s

)
e
g
d
e

/
c
e
s
o
r
c
m

i

(
 

e
m

i
t

)
e
g
d
e

/
c
e
s
o
r
c
m

i

(
 

e
m

i
t

 6

 5.5

 5

 4.5

 4

 3.5

 3

 0

eu
indochina
uk

)
e
g
d
e

/
c
e
s
o
r
c
m

i

(
 

e
m

i
t

 2

 4

 6

 8

 10

 12

level of change

 25

 20

 15

 10

 5

 0

 0

eu
indochina
uk

 2

 4

 6

 8

 10

 12

level of change

(c) Space/time behavior of the hybrid approach varying the level where we change k.

UK - Direct Neighbors

UK - Reverse Neighbors

 25

 20

 15

 10

 5

 0

 4

 5

 4

 3

 2

 1

2x2
3x3
4x4
Hybrid5
Hybrid37
WebGraph x 2
WebGraph x 2 (RAM)
RePair x 2
RePair_both

)
e
g
d
e

/
c
e
s
o
r
c
m

i

(
 

e
m

i
t

 4.5

 5

 5.5

 6

 6.5

 7

 7.5

 8

 8.5

space (bits/edge)

 25

 20

 15

 10

 5

 0

 4

2x2
3x3
4x4
Hybrid5
Hybrid37
WebGraph x 2
WebGraph x 2 (RAM)
RePair x 2
RePair_both

 4.5

 5

 5.5

 6

 6.5

 7

 7.5

 8

 8.5

space (bits/edge)

(d) Space/time to retrieve direct and reverse neighbors.

EU - Direct Neighbors

2x2
3x3
4x4
Hybrid5
Hybrid37
WebGraph
WebGraph (RAM)
RePair

(f) Comparison with approach Smaller.

Space (bpe) Smaller Smaller × 2 Hybrid5
4.46
CNR
5.21
EU
Time (ms/p)
CNR
EU

0.048
0.099

2.34
28.72

3.98
5.56

1.99
2.78

 0

 4

 5

 6
 7
space (bits/edge)

 8

 9

(e) Retrieving only direct neighbors.

Fig. 3. Experimental evaluation

k2-Trees for Compact Web Graph Representation

27

5.2 Comparison with Other Methods

We ﬁrst compare graph representations that allow retrieving both direct and
reverse neighbors. Figure 3(d) shows the space/time tradeoﬀ for retrieving direct
and reverse neighbors, over the larger graph (UK), as it is representative of
the common behaviour of the other smaller graphs. We measure the average
time eﬃciency in μs/e as before. Representations providing space/time tuning
parameters appear as a line, whereas the others appear as a point.

We compare our compact representations with the proposal in [9, Chapter
7] that computes both direct and reverse neighbors (RePair both), as well as
the simpler representation in [10] (as improved in [9, Chapter 6], RePair) that
retrieves just direct neigbors. In this case we represent both the graph and its
transpose, in order to achieve reverse navigation as well (RePair × 2). We do the
same with Boldi and Vigna’s technique [6] (WebGraph), as it also allows for direct
neighbors retrieval only (we call it WebGraph × 2 when we add both graphs).
As this technique uses less space on disk than what the process needs to run, we
show in WebGraph (RAM) the minimum space needed to run (yet we keep the
best time it achieves with suﬃcient RAM space). All the implementations were
provided by their authors.
We include our alternatives 2 × 2, 3 × 3, 4 × 4, and Hybrid5, all of which use
the slower solution for rank that uses just 5% of extra space [13], and Hybrid37,
which uses the faster rank method that uses 37.5% extra space on top of T .
As we can see, our representations (particularly Hybrid5 and 2×2) achieve the
best compression (3.3 to 5.3 bpe, depending on the graph, 4.22 for graph UK)
among all the techniques that provide direct and reverse neighbor queries. The
only alternative that gets somewhat close is RePair both, but it is much slower
to retrieve direct neighbors. For reverse neighbors, instead, it is an interesting
alternative. Hybrid37 oﬀers relevant tradeoﬀs in some cases. Finally, WebGraph
× 2 and RePair × 2 oﬀer very attractive time performance, but they need
signiﬁcantly more space. As explained, using less space may make the diﬀerence
between being able of ﬁtting a large Web graph in main memory or not.

If, instead, we wished only to carry out forward navigation, alternatives RePair 
and WebGraph become preferable in most cases. Figure 3(e), however, shows
graph EU, where we still achieve signiﬁcantly less space than WebGraph.

We also compare our proposal with the method in [2] (Smaller). As we do not
have their code, we ran new experiments on a Pentium IV of 3.0 GHz with 4
GB of RAM, which resembles better the machine used in their experiments. We
used the smaller graphs, on which they have reported experiments. Figure 3(f)
shows the space and average time needed to retrieve the whole adjacency list of
a page, in milliseconds per page. As, again, their representation cannot retrieve
reverse neighbors, Smaller × 2 is an estimation of the space they would need to
represent both the normal and transposed graphs.
Our method is orders of magnitude faster to retrieve an adjacency list, while
the space is similar to Smaller × 2. The diﬀerence is so large that it could be
possible to be competitive even if part of our structure (e.g. L) was in secondary
memory (in which case our main memory space would be similar to just Smaller).

28

N.R. Brisaboa, S. Ladra, and G. Navarro

6 Extended Functionality

While alternative compressed graph representations [2, 6, 10] are limited to retrieving 
the direct, and sometimes the reverse, neighbors of a given page, and we
have compared our technique with those in these terms, we show now that our
representation allows for more sophisticated forms of retrieval than extracting
direct and reverse neighbors.

First, in order to determine whether a given page p points to a given page q,
most compressed (and even some classical) graph representations have no choice
but to extract all the neighbors of p (or a signiﬁcant part of them) and see if
q is in the set. We can answer such query in O(logk n) time, by descending to
exactly one child at each level of the tree. More precisely, at level (cid:2) we descend
to child k·(cid:4)p/kh−(cid:2)−1(cid:5) +(cid:4)q/kh−(cid:2)−1(cid:5), if it is not a zero, and compute the relative
position of cell (p, q) in the submatrix just as in Section 3.3. If we arrive at the
last level and ﬁnd a 1 at cell (p, q), then there is a link, otherwise there is not.
A second interesting operation is to ﬁnd the direct neighbors of page p that
are within a range of pages [q1, q2] (similarly, the reverse neighbors of q that are
within a range [p1, p2]). This is interesting, for example, to ﬁnd out whether p
points to a domain, or is pointed from a domain, in case we sort URLs in lexicographical 
order. The algorithm is similar to Direct and Reverse in Section 3.3,
except that we do not enter all the children 0 ≤ j < k of a row (or column), but
only from (cid:4)q1/kh−(cid:2)−1(cid:5) ≤ j ≤ (cid:4)q2/kh−(cid:2)−1(cid:5) (similarly for p1 to p2).

Yet a third operation of interest is to ﬁnd all the links from a range of pages
[p1, p2] to another [q1, q2]. This is useful, for example, to extract all the links
between two domains. The algorithm to solve this query indeed generalizes all
of the others we have seen. This gives times of O(n) for retrieving direct and
reverse neighbors (we made a ﬁner average-case analysis in Section 3.3), O(p2 −
p1 + logk n) or O(q2 − q1 + logk n) for ranges of direct or reverse neighbors, and
O(logk n) for queries on single links.

7 Conclusions

We have introduced a compact representation for Web graphs that takes advantage 
of the sparseness and clustering of their adjacency matrix. Our representation 
enables eﬃcient forward and backward navigation in the graph (a
few microseconds per neighbor found) within compact space (about 3 to 5 bits
per link). Our experimental results show that our technique oﬀers an attractive 
space/time tradeoﬀ compared to the state of the art. Moreover, we support
queries on the graph that extend the basic forward and reverse navigation.

More exhaustive experimentation and tuning is needed to exploit the full
potential of our data structure, in particular regarding the space/time tradeoﬀs
of the hybrid approach. We also plan to research and experiment more in depth
on the extended functionality supported by our representation.

The structure we have introduced can be of more general interest. It could be
fruitful, for example, to generalize it to binary relations, such as the one relating

k2-Trees for Compact Web Graph Representation

29

keywords with the Web pages, or more generally documents, where they appear.
Then one could retrieve not only the Web pages that contain a keyword, but also
the set of keywords present in a Web page, and thus have access to important
summarization data without accessing the page itself. Our range search could
permit searching within subcollections or subdirectories. Our structure could
become a relevant alternative to the current state of the art in this direction,
e.g. [3, 9]. Another example is the representation of discrete grids of points, for
computational geometry applications or geographic information systems.

References

1. Adler, M., Mitzenmacher, M.: Towards compressing Web graphs. In: Proc. 11th

DCC, pp. 203–212 (2001)

2. Asano, Y., Miyawaki, Y., Nishizeki, T.: Eﬃcient compression of web graphs. In:
Hu, X., Wang, J. (eds.) COCOON 2008. LNCS, vol. 5092, pp. 1–11. Springer,
Heidelberg (2008)

3. Barbay, J., He, M., Munro, I., Rao, S.S.: Succinct indexes for strings, binary relations 
and multi-labeled trees. In: Proc. 18th SODA, pp. 680–689 (2007)

4. Benoit, D., Demaine, E., Munro, I., Raman, R., Raman, V., Rao, S.S.: Representing

trees of higher degree. Algorithmica 43(4), 275–292 (2005)

5. Bharat, K., Broder, A., Henzinger, M., Kumar, P., Venkatasubramanian, S.: The
Connectivity Server: Fast access to linkage information on the Web. In: Proc. 7th
WWW, pp. 469–477 (1998)

6. Boldi, P., Vigna, S.: The WebGraph framework I: Compression techniques. In:

Proc. 13th WWW, pp. 595–601. ACM Press, New York (2004)

7. Broder, A., Kumar, R., Maghoul, F., Raghavan, P., Rajagopalan, S., Stata, R.,
Tomkins, A., Wiener, J.: Graph structure in the Web. Journal of Computer Networks 
33(1–6), 309–320 (2000); also in Proc. 9th WWW

8. Chan, H.-L., Hon, W.-K., Lam, T.-W., Sadakane, K.: Compressed indexes for dynamic 
text collections. ACM Transactions on Algorithms 3(2), article 21 (2007)

9. Claude, F.: Compressed data structures for Web graphs. Master’s thesis, Dept. of
Comp. Sci., Univ. of Chile, Advisor: Navarro, G., TR/DCC-2008-12 (August 2008)
10. Claude, F., Navarro, G.: A fast and compact Web graph representation. In: Ziviani,
N., Baeza-Yates, R. (eds.) SPIRE 2007. LNCS, vol. 4726, pp. 105–116. Springer,
Heidelberg (2007)

11. Donato, D., Millozzi, S., Leonardi, S., Tsaparas, P.: Mining the inner structure of

the Web graph. In: Proc 8th WebDB, pp. 145–150 (2005)

12. Geary, R., Rahman, N., Raman, R., Raman, V.: A simple optimal representation

for balanced parentheses. Theoretical Computer Science 368(3), 231–246 (2006)

13. Gonz´alez, R., Grabowski, S., M¨akinen, V., Navarro, G.: Practical implementation

of rank and select queries. In: Poster Proc. 4th WEA, pp. 27–38 (2005)

14. Jacobson, G.: Space-eﬃcient static trees and graphs. In: Proc. 30th FOCS, pp.

549–554 (1989)

15. Jansson, J., Sadakane, K., Sung, W.-K.: Ultra-succinct representation of ordered

trees. In: Proc. 18th SODA, pp. 575–584 (2007)

16. Kleinberg, J., Kumar, R., Raghavan, P., Rajagopalan, S., Tomkins, A.: The Web
as a graph: Measurements, models, and methods. In: Asano, T., Imai, H., Lee,
D.T., Nakano, S.-i., Tokuyama, T. (eds.) COCOON 1999. LNCS, vol. 1627, pp.
1–17. Springer, Heidelberg (1999)

30

N.R. Brisaboa, S. Ladra, and G. Navarro

17. Larsson, J., Moﬀat, A.: Oﬀ-line dictionary-based compression. Proc. IEEE 88(11),

1722–1732 (2000)

18. Munro, I.: Tables. In: Chandru, V., Vinay, V. (eds.) FSTTCS 1996. LNCS,

vol. 1180, pp. 37–42. Springer, Heidelberg (1996)

19. Munro, I., Raman, V.: Succinct representation of balanced parentheses and static

trees. SIAM Journal on Computing 31(3), 762–776 (2001)

20. Raghavan, S., Garcia-Molina, H.: Representing Web graphs. In: Proc. 19th ICDE,

p. 405 (2003)

21. Randall, K., Stata, R., Wickremesinghe, R., Wiener, J.: The LINK database: Fast

access to graphs of the Web. Technical Report 175, Compaq SRC (2001)

22. Samet, H.: Foundations of Multidimensional and Metric Data Structures. Morgan

Kaufmann Publishers Inc., San Francisco (2006)

23. Suel, T., Yuan, J.: Compressing the graph structure of the Web. In: Proc. 11th

DCC, pp. 213–222 (2001)

24. Vitter, J.: External memory algorithms and data structures: dealing with massive

data. ACM Computing Surveys 33(2), 209–271 (2001) (Version revised at 2007)

