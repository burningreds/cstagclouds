Fully-Compressed Suﬃx Trees

Lu´ıs M. S. Russo1,(cid:2), Gonzalo Navarro2,(cid:2)(cid:2), and Arlindo L. Oliveira1

1 INESC-ID, R. Alves Redol 9, 1000 LISBOA, Portugal

{lsr,aml}@algos.inesc-id.pt

2 Dept. of Computer Science, University of Chile

gnavarro@dcc.uchile.cl

Abstract. Suﬃx trees are by far the most important data structure
in stringology, with myriads of applications in ﬁelds like bioinformatics
and information retrieval. Classical representations of suﬃx trees require
O(n log n) bits of space, for a string of size n. This is considerably more
than the n log2 σ bits needed for the string itself, where σ is the alphabet
size. The size of suﬃx trees has been a barrier to their wider adoption
in practice. Recent compressed suﬃx tree representations require just
the space of the compressed string plus Θ(n) extra bits. This is already
spectacular, but still unsatisfactory when σ is small as in DNA sequences.
In this paper we introduce the ﬁrst compressed suﬃx tree representation 
that breaks this linear-space barrier. Our representation requires
sublinear extra space and supports a large set of navigational operations
in logarithmic time. An essential ingredient of our representation is the
lowest common ancestor (LCA) query. We reveal important connections
between LCA queries and suﬃx tree navigation.

1 Introduction and Related Work

Suﬃx trees are extremely important for a large number of string processing problems.
 Their many virtues have been described by Apostolico [1] and Gusﬁeld [2].
The combinatorial properties of suﬃx trees have a profound impact in the bioinformatics 
ﬁeld, which needs to analyze large strings of DNA and proteins with
no predeﬁned boundaries. This partnership has produced several important results,
 but it has also exposed the main shortcoming of suﬃx trees. Their large
space requirements, together with their need to operate in main memory to be
useful in practice, renders them inapplicable in the cases where they would be
most useful, that is, on large texts.

The space problem is so important that it has originated a plethora of research
results, ranging from space-engineered implementations [3] to novel data structures 
to simulate it, most notably suﬃx arrays [4]. Some of those space-reduced
variants give away some functionality in exchange. For example suﬃx arrays

(cid:2) Supported by the Portuguese Science and Technology Foundation by grants
SFRH/BD/12101/2003, SFRH/BPD/34373/2006 and projects DBYeast, POSI/
EIA/57398/2004 and ARN, PTDC/EIA/67722/2006.

(cid:2)(cid:2) Partially funded by Millennium Institute for Cell Dynamics and Biotechnology,

Grant ICM P05-001-F, Mideplan, Chile.

E.S. Laber et al. (Eds.): LATIN 2008, LNCS 4957, pp. 362–373, 2008.
c(cid:2) Springer-Verlag Berlin Heidelberg 2008

Fully-Compressed Suﬃx Trees

363

miss the important suﬃx link navigational operation. Yet, all these classical approaches 
require O(n log n) bits, while the indexed string requires only n log σ
bits1, n being the size of the string and σ the size of the alphabet. For example
the human genome requires 700 Megabytes, while even a space-eﬃcient suﬃx
tree on it requires at least 40 Gigabytes [5], and the reduced-functionality suﬃx
array requires more than 10 Gigabytes. This problem is particularly evident in
DNA because log σ = 2 is much smaller than log n.

These representations are also much larger than the size of the compressed
string. Recent approaches [6] combining data compression and succinct data
structures have achieved spectacular results for the pattern search problem. For
example Ferragina et al. [7] presented a compressed suﬃx array that requires
nHk + o(n log σ) bits and computes occ in time O(m(1 + (logσ log n)−1)). Here
nHk denotes the k-th order empirical entropy of the string [8], a lower bound
on the space achieved by any compressor using k-th order modeling.

It turns out that it is possible to use this kind of data structures, that we will
call compressed suﬃx arrays2, and, by adding a few extra structures, support all
the operations provided by suﬃx trees. Sadakane was the ﬁrst to present such a
result [5], adding 6n bits to the size of the compressed suﬃx array.

In this paper we break the Θ(n) extra-bits space barrier. We build a new
suﬃx tree representation on top of a compressed suﬃx array, so that we can
support all the navigational operations and our extra space ﬁts within the sublinear 
o(n log σ) extra bits of the compressed suﬃx array. Our central tools are
a particular sampling of suﬃx tree nodes, its connection with the suﬃx link and
the lowest common ancestor (LCA) query, and the interplay with the compressed
suﬃx array. We exploit the relationship between these actors and uncover some
relationships between them that might be of independent interest.

A comparison between Sadakane’s representation and ours is shown in Table 1.
The result for the time complexities is mixed. Our representation is faster for the
important Child operation, when log σ = o(log log n), yet Sadakane’s is usually 
faster on the rest. On the other hand, our representation requires much less
space. For DNA, assuming realistically that Hk ≈ 2, Sadakane’s approach requires
8n + o(n) bits, whereas our approach requires only 2n + o(n) bits. We choose a
compressed suﬃx array that has the best Letter time, for nHk + o(n log σ) bits.
Only when σ = ω(polylog(n)) and there are O(nH0) + o(n log σ) bits available is
Sadakane’s compressed suﬃx array [9] faster at computing the Letter operation.
In that case, using his compressed suﬃx array, Sadakane’s suﬃx tree would work
faster, while ours does not beneﬁt from that. As such, Table 1 shows the time complexities 
that can be obtained for suﬃx trees using the best asymptotic space achieved
for compressed suﬃx arrays alone. This space is optimal in the sense that no k-th
order compressor can achieve asymptotically less space to represent T .

There exists a previous description [10] of a technique based on interval representation 
and sampling of suﬃx tree. However it is extremely brief and no
theoretical bounds on the result are given.

1 In this paper log stands for log2.
2 These are also called compact suﬃx arrays, FM-indexes, etc., see [6].

364

L.M.S. Russo, G. Navarro, and A.L. Oliveira

Table 1. Comparing compressed suﬃx tree representations. The operations are deﬁned 
along Section 2. Time complexities, but not space, are big-O expressions. Notice
that Letter(v, i) can also be computed in O(iψ) time. Also Child can, alternatively,
be computed using FChild and at most σ times NSib. We give the generalized performance 
and an instantiation using δ = (logσ log n) log n, assuming σ = O(polylog(n)),
and using the FM-Index of Ferragina et al. [7] as the compressed suﬃx array (CSA).

Ours
|CSA| + O((n/δ) log n)

= nHk + o(n log σ)
= (logσ log n) log n
= 1
= (logσ log n) log n

= (logσ log n) log n Ψ δ

Space in bits

Sadakane’s
|CSA| + 6n + o(n)
= nHk + 6n + o(n log σ)
SDep/ Locate
Φ
Count/ Ancestor 1
Parent/ FChild/
1
NSib
SLink
SLinki
Letter(v, i)
LCA
Child

= 1 1
= 1 (Ψ + t)δ

= 1 (Ψ + t)δ

= (logσ log n) log n Φ + (Ψ + t)δ
= (logσ log n) log n Φ

Ψ
Φ
Φ
1
Φ log σ = (log log n) log n log σ + Φ log δ + (Ψ + t)δ

= 1 (Ψ + t)δ

= (logσ log n) log n
= (logσ log n) log n
= (logσ log n) log n
= (logσ log n) log n

TDep
LAQt
LAQs
WeinerLink

1
1

t

Not Supported

2 Basic Concepts

= (log log n)2 logσ n
= 1 (Ψ + t)δ2
= ((logσ log n) log n)2
= 1 log n + (Ψ + t)δ2 = ((logσ log n) log n)2
= (logσ log n) log n
= 1

log n + (Ψ + t)δ

= 1 t

(cid:3); by LCA(v, v

Figure 1 shows an example that illustrates the concepts in this section. We
denote by T a string; by Σ the alphabet of size σ; by T [i] the symbol at position
(cid:3) concatenation; by T = T [..i−1].T [i..j].T [j+1..] respectively
(i mod n); by T.T
a preﬁx, a susbtring and a suﬃx; by Parent(v) the parent node of node v; by
TDep(v) its tree-depth; by FChild(v) its ﬁrst child; by NSib(v) the next child
(cid:3))
of the same parent; by LAQt(v, d) its level-d ancestor; by Ancestor(v, v
(cid:3)) the lowest common ancestor.
whether v is an ancestor of v
The path-label of a node v in a labeled tree is the concatenation of the
edge-labels from the root down to v. We refer indiﬀerently to nodes and to
their path-labels, also denoted by v. The i-th letter of the path-label is denoted
as Letter(v, i) = v[i]. The string-depth of a node v, denoted by SDep(v),
is the length of its path-label. LAQs(v, d) is the highest ancestor of node v
with SDep ≥ d. Child(v, X) is the node that results of descending from v
by the edge whose label starts with symbol X, if it exists. The suﬃx tree
of T is the deterministic compact labeled tree for which the path-labels of the
leaves are the suﬃxes of T $, where $ is a terminator symbol not belonging to
Σ. We will assume n is the length of T $. For a detailed explanation see Gus-
ﬁeld’s book [2]. The suﬃx-link of a node v (cid:4)= Root of a suﬃx tree, denoted

Fully-Compressed Suﬃx Trees

365

a
b

b

b

b
a
b
$

2
0

b

a
b
$

5
2

b
a
b
$

6
1

a
b
$

4
3

$

3
5

$

0
6A:

$

1
4

Fig. 1. Suﬃx tree T of string abbbab, with the
leaves numbered. The arrow shows the SLink
between node ab and b. Below it we show the
suﬃx array. The portion of the tree corresponding 
to node b and respective leaves interval is
highlighted with a dashed rectangle. The sampled 
nodes have bold outlines.

1

2

i: 01 234 56 7890 12 345 67 8901
((0)((1)(2))((3)(4)((5)(6))))

(
i: 0

(3)(4)
1 23 4

)
5

Fig. 2. Parentheses representations 
of trees. The parentheses 
on top represent the
suﬃx tree and those on the
bottom represent the sampled
tree. The numbers are not part
of the representation; they are
shown for clarity. The rows labeled 
i: give the index of the
parentheses.

SLink(v), is a pointer to node v[1..]. Note that SDep(v) of a leaf v identiﬁes
the suﬃx of T $ starting at position n − SDep(v) = Locate(v). For example
T [Locate(ab$)..] = T [7−3..] = T [4..] = ab$. The suﬃx array A[0, n−1] stores
the Locate values of the leaves in lexicographicall order. The suﬃx tree nodes
can be identiﬁed with suﬃx array intervals: each node corresponds to the range of
leaves that descend from v. The node b corresponds to the interval [3, 6]. Hence
the node v will be represented by the interval [vl, vr]. Leaves are also represented
by their left-to-right index (starting at 0). For example by vl − 1 we refer to the
leaf immediately before vl, i.e. [vl − 1, vl − 1]. With this representation we can
Count in constant time the number of leaves that descend from v. The number
of leaves below b is 4 = 6 − 3 + 1. This is precisely the number of times that
the string b occurs in the indexed string T . We can also compute Ancestor in
O(1) time: Ancestor(v, v

(cid:3)) ⇔ vl ≤ v

≤ vr.

(cid:3)
l

≤ v

(cid:3)
r

3 Using Compressed Suﬃx Arrays

We are interested in compressed suﬃx arrays because they have very compact
representations and support partial suﬃx tree functionality (being usually more
powerful than the classical suﬃx arrays [6]). Apart from the basic functionality
of retrieving A[i] = Locate(i), state-of-the-art compressed suﬃx arrays support
operation SLink(v) for leaves v. This is called ψ(v) in the literature: A[ψ(v)] =
A[v] + 1, and thus SLink(v) = ψ(v), let its time complexity be O(Ψ). The
iterated version of ψ, denoted as ψi, can usually be computed faster than O(iΨ)
−1, let its time
with compressed indexes [6]. This is achieved with the A and A
complexity be O(Φ). It also supports the WeinerLink(v, a) operation [11] for
nodes v: WeinerLink(v, X) gives the suﬃx tree node with path-label X.v[0..].
This is called the LF mapping in compressed suﬃx arrays, and is a kind of

366

L.M.S. Russo, G. Navarro, and A.L. Oliveira

inverse of ψ, let its time complexity be O(t). Consider the interval [3, 6] that
represents the leaves whose path-labels start by b. In this case we have that
LF(a, [3, 6]) = [1, 2], i.e. by using the LF mapping with a we obtain the interval
of leaves whose path-labels start by ab. We use an extension of LF to strings,
LF(X.Y, v) = LF(X, LF(Y, v)).

Finally, compressed suﬃx arrays are usually self-indexes, meaning that they
replace the text: it is possible to extract any substring, of size (cid:9), of the indexed
text in O(Φ + (cid:9)Ψ) time. A particularly easy case that is solved in constant time
is to extract T [A[v]] for a suﬃx array cell v, that is, the ﬁrst letter of a given
suﬃx3. This corresponds to v[0], the ﬁrst letter of the path-label of leaf v.

As anticipated, our compressed suﬃx tree representation will consist of a
sampling of the suﬃx tree plus a compressed suﬃx array representation. A wellknown 
compressed suﬃx array is Sadakane’s CSA [9], which requires 1
 nH0 +
O(n log log σ) bits of space and has times Ψ = O(1), Φ = O(log n), and t =
O(log n), for any  > 0. For our results we favor a second compressed suﬃx array,
called the FM-index [7], which requires nHk+o(n log σ) bits, for any k ≤ α logσ n
and constant 0 < α < 1. Its complexities are Ψ = t = O(1 + (logσ log n)−1) and
Φ = O((logσ log n) log n).4 The instantiation in Table 1 is computed for the FMindex,
 but the reader can easily compute the result of using Sadakane’s CSA. In
that case the comparison would favor more Sadakane’s compressed suﬃx tree,
yet the space would be considerably higher.

4 The Sampled Suﬃx Tree

A pointer based implementation of suﬃx trees requires O(n log n) bits to represent 
a suﬃx tree of (at most) 2n nodes. As this is too much, we will store only a
few sampled nodes. We denote our sampling factor by δ, so that in total we sample
O(n/δ) nodes. Hence, provided δ = ω(logσ n), the sampled tree can be represented 
using o(n log σ) bits. To ﬁx ideas we can assume δ = (cid:7)(logσ log n) log n(cid:8).
In our running example we use δ = 4.
To understand the structure of the sampled tree notice that every tree with 2n
nodes can be represented in 4n bits as a sequence of parentheses (see Figures 1
and 2). The representation of the sampled tree can be obtained by deleting the
parentheses of the non-sampled nodes, as in Figure 2. For the sampled tree to
be representative of the suﬃx tree it is necessary that every node is, in some
sense, close enough to a sampled node.
Deﬁnition 1. A δ-sampled tree S of a suﬃx tree T with Θ(n) nodes is formed
by choosing O(n/δ) nodes of T so that for each node v of T there is an i < δ
such that node SLinki(v) is sampled.
3 This is computed in O(1) as the c ∈ Σ satisfying C[c] ≤ i < C[c + 1], see [6].
4 ψ(i) can be computed as selectT [A[i]](T bwt, T [A[i]]) using the multiary wavelet
tree [12]. The cost for Φ is obtained using a sampling step of (logσ log n) log n, so
that o(n log σ) stands for O((n log σ)/ log log n) as our other structures.

Fully-Compressed Suﬃx Trees

367

This means that if we start at v and follow suﬃx links successively, i.e. v,
SLink(v), SLink(SLink(v)), . . ., we will ﬁnd a sampled node in at most δ
steps. Note that this property implies that the Root must be sampled, since
SLink(Root) is undeﬁned. We sample the nodes v for which SDep(v) ≡δ/2 0
(cid:3)
and there is a another node v
, v).
(cid:3)[..i], v), for −1 ≤ i ≤ |T
(cid:3)|,
Notice that this guarantees that from the nodes LF (T
only one is sampled. To be precise this guarantees that we sample at most (cid:10)4n/δ(cid:11)
nodes from a suﬃx tree with 2n nodes.

(cid:3)| ≥ δ/2 such that v

(cid:3) and a string |T

(cid:3) = LF (T

In addition to the pointers, that structure the sampled tree, we store in the
sampled nodes their interval representation; their SDep and TDep; the information 
to answer LCA queries in S, LCAS, in constant time [13,14]; and the
information to LAQ queries in S, LAQ
S, in constant time [15,16]; some further
data is introduced later. All this requires O((n/δ) log n) bits of space.

In order to make eﬀective use of the sampled tree, we need a way to map
any node v to its lowest sampled ancestor, LSA(v). Another important op-
(cid:3)), i.e. lowest comeration 
is the lowest common sampled ancestor LCSA(v, v
mon ancestor in the sampled tree S. For example LCSA(3, 4) is the Root,
(cid:3)) =
whereas LCA(3, 4) is [3, 6], i.e. the node labeled b. Note that LCSA(v, v
(cid:3))). Next we show how LSA is sup-
LCAS(LSA(v), LSA(v
ported for leaves in constant time and O((n/δ) log n) extra bits. With that we
also have LCSA in constant time (for leaves; later, we extend this to any node).

(cid:3))) = LSA(LCA(v, v

(cid:2)

4.1 Computing LSA for Leaves
LSA is computed by using an operation Reduce(v), that receives the numeric
representation of leaf v and returns the position, in the parentheses representation 
of the sampled tree, where that leaf should be. Consider for example the
leaf numbered by 5 in Figure 2. This leaf is not sampled, but in the original
tree it appears somewhere between leaf 4 and the end of the tree, more specifically 
between parenthesis ’)’ of 4 and parenthesis ’)’ of the Root. We assume
Reduce returns the ﬁrst parenthesis, i.e. Reduce(5) = 4. In this case since the
parenthesis we obtain is a ’)’ we know that LSA should be the parent of that
node. Hence we compute LSA as follows:

, if there is a ’(’ at Reduce(v)

Reduce(v)
Parent(Reduce(v)) , otherwise

LSA(v) =
To compute Reduce we use a bitmap RedB and an array RedA. The bitmap
RedB is initialized with zeros. For every sampled node v represented as [vl, vr]
we set bits RedB[vl] and RedB[vr + 1] to 1. In our example RedB is 1001110.
This bitmap indicates the leaves for which we must store partial solutions to
Reduce. In our example the leaves are 0, 3, 4, 5. These partial solutions are
(cid:3)
stored in array RedA (in case of a collision vr + 1 = v
l is
stored). In our example these partial results are respectively 0, 1, 3, 4. Therefore
Reduce(v) = RedA[Rank1(RedB, v)− 1], where v is a leaf number and Rank1
counts the number of 1’s in RedB up to and including position v.
First we show that Reduce can be computed in O(1) time with O((n/δ) log n)
bits. The bitmap RedB cannot be stored in uncompressed form because it would

(cid:3)
l, the data for v

368

L.M.S. Russo, G. Navarro, and A.L. Oliveira

require n bits. We store RedB with the representation of Raman et al. [17] that
needs only m log n
m + o(n) bits, where m = O(n/δ) is the number of 1’s in
the bitmap (as every sampled node inserts at most two 1’s in RedB). Hence
RedB needs O((n/δ) log δ) = O((n/δ) log n) bits, and supports Rank1 in O(1)
time. On the other hand, since there are also O(n/δ) integers in RedA, we
can store them explicitly to have constant access time in O((n/δ) log n) bits.
Therefore Reduce can be computed within the assumed bounds. According to
our previous explanation, so can LSA and LCSA, for leaves.

5 The Kernel Operations

We have described the two basic components of our compressed suﬃx tree representation.
 Most of our functionality builds on the LCA operation, which is
hence fundamental to us. In this section we present an entangled mechanism
that supports operations LCA and SLink depending on each other.

(cid:3) such that LCA(v, v

(cid:3)) (cid:4)= Root we have that

5.1 Two Fundamental Observations
We point out that SLink’s and LCA’s commute on suﬃx trees.
Lemma 1. For any nodes v, v
SLink(LCA(v, v

(cid:3)) = X.α and SLink(LCA(v, v

(cid:3))).
(cid:3))) = LCA(SLink(v), SLink(v
(cid:3) are respectively X.α.Y.β and
Proof. Assume that the path-labels of v and v
(cid:3), where Y (cid:4)= Z. According to the deﬁnitions of LCA and SLink, we
X.α.Z.β
(cid:3))) = α. On the other hand
have that LCA(v, v
(cid:3)) are respectively α.Y.β and α.Z.β
(cid:3).
the path-labels of SLink(v) and SLink(v
(cid:3))) is also α. Hence this
Therefore the path-label of LCA(SLink(v), SLink(v
(cid:12)(cid:13)
(cid:3))).
node must be the same as SLink(LCA(v, v
Figure 3 illustrates this lemma; ignore the nodes associated with ψ. The condi-
(cid:3)) (cid:4)= Root is easy to verify, in a suﬃx tree, by comparing the ﬁrst
tion LCA(v, v
letters of the path-label of v and v

(cid:3)[0].
The next Lemma shows a fundamental property for the kernel operations.

(cid:3)) (cid:4)= Root iﬀ v[0] = v

(cid:3), i.e. LCA(v, v

Lemma 2. Let v, v
d = min(δ, r + 1). Then:

SDep(LCA(v, v

(cid:3) be nodes such that SLinkr(LCA(v, v
(cid:3))) = max0≤i<d{i + SDep(LCSA(SLinki(v), SLinki(v

(cid:3))) = Root, and let
(cid:3))))}

(cid:3)

(cid:3)

Proof. The following reasoning holds for any valid i:

SDep(LCA(v, v

)) = i + SDep(SLinki(LCA(v, v

(1)
(2)
(3)
(cid:3)(cid:3)))
Equation (1) holds by iterating the fact that SDep(v
(cid:3)(cid:3)) is deﬁned. Equation (2) results from applyfor 
any node v
ing Lemma 1 repeatedly. Inequality (3) comes from the deﬁnition of LCSA and

(cid:3)
= i + SDep(LCA(SLinki(v), SLinki(v
)))
≥ i + SDep(LCSA(SLinki(v), SLinki(v
(cid:3)

(cid:3)(cid:3)) = 1 + SDep(SLink(v

(cid:3)(cid:3) for which SLink(v

)))

)))

Fully-Compressed Suﬃx Trees

369

X

α

Y Z
v
v

(cid:2)

α

Y Z

2

d − i

ParentS(v

(cid:3))

Parent

δ

ψ

ψ

the

Fig. 3. Schematic representation 
of
relation between 
LCA and SLink, see
Lemma 1. Curved arrows represent 
SLink and straight arrows 
the ψ function.

2

(cid:3)

v

δ

LF

Fig. 4. Schematic representation of the
vi,j nodes of the LAQs operation. The
nodes sampled because of deﬁnition 1 are
in bold and the nodes sampled because of
the condition of TDep are ﬁlled.

(cid:3)(cid:3)(cid:3) is an ancestor of node v

(cid:3)(cid:3)) ≥ SDep(v
(cid:3)(cid:3)(cid:3)).
the fact that if node v
(cid:3))) ≥ max0≤i<d{. . .}. On the other hand, from DeﬁniTherefore 
SDep(LCA(v, v
(cid:3))) is sampled. The
tion 1 we know that for some i < δ the node SLinki(LCA(v, v
(cid:3))) = Root, which
formula goes only up to d, but d < δ only if SLinkd(LCA(v, v
is also sampled. According to the deﬁnition of LCSA inequality (3) becomes an
(cid:12)(cid:13)
equality for that node. Hence SDep(LCA(v, v

(cid:3))) ≤ max0≤i<d{. . .}.

(cid:3)(cid:3) then SDep(v

5.2 Entangled Operations
To apply Lemma 2 we need to support operations LCSA, SDep, and SLink.
Operation LCSA is supported in constant time, but only for leaves (Section 4.1).
Since SDep is applied only to sampled nodes, we have it readily stored in the
sampled tree. Sadakane [5] showed that SLink(v) = LCA(ψ(vl), ψ(vr)), whenever 
v (cid:4)= Root. This is not necessarily equal to the [ψ(vl), ψ(vr)] interval, see
node X.α in Figure 3. In general SLinki(v) = LCA(ψi(vl), ψi(vr)).
Hence all we need is to support LCA. However this depends on Lemma 2.

Lemma 3. LCA(v, v
nodes v, v

(cid:3), where i is given by Lemma 2.

(cid:3)) = LF(v[0..i−1], LCSA(SLinki(v), SLinki(v

(cid:3)))) for any

Proof. This is a direct consequence of Lemma 2. Let i be the index of the max-
(cid:3))) is a sampled node and
imum of the set in Lemma 2, i.e. SLinki(LCA(v, v
(cid:3))). Note that from the defhence 
it is the same as LCSA(SLinki(v), SLinki(v
(cid:3)(cid:3). Applying this
inition of LF mapping we have that LF(v
(cid:12)(cid:13)
iteratively to SLinki(LCA(v, v
To use this lemma we must know which is the correct i. This is easily determined
(cid:3))). Accessing the letters to apply LF is
if we ﬁrst compute SDep(LCA(v, v
not a problem, as we have always to obtain the ﬁrst letter of a path-label,
SLinki(v)[0] = SLinki(v

(cid:3))) we obtain the equality in the lemma.

(cid:3)(cid:3)[0], SLink(v

(cid:3)(cid:3))) = v

(cid:3))[0].

370

L.M.S. Russo, G. Navarro, and A.L. Oliveira

5.3 Breaking the Cycle

To get out of this dependency we need a new idea. We will handle all the compu-
(cid:3)).
tation over leaves, for which we can compute SLink(v) = ψ(v) and LCSA(v, v
}) for any nodes v, v
(cid:3).
Lemma 4. LCA(v, v

(cid:3)) = LCA(min{vl, v

}, max{vr, v

(cid:3)
r

(cid:3)
l

(cid:3)(cid:3)(cid:3) be respectively the nodes on the left and on the right of the
(cid:3)(cid:3) and v
Proof. Let v
(cid:3)(cid:3)(cid:3)
(cid:3)(cid:3)(cid:3)
(cid:3)(cid:3)
r ] respectively.
equality. Assume that they are represented as [v
l , v
l , v
≤ vl, v
≥ vr, v
(cid:3). This
(cid:3)(cid:3) is an ancestor of v and v
(cid:3)(cid:3)
(cid:3)
(cid:3)(cid:3)
(cid:3)
Hence v
l and v
r since v
} ≤ v
} ≤ max{vr, v
≤ min{vl, v
(cid:3)(cid:3) is also an ancestor
(cid:3)(cid:3)
(cid:3)(cid:3)
(cid:3)
(cid:3)
r
l
r , i.e. v
means that v
}. Since v
} and max{vr, v
of min{vl, v
(cid:3)(cid:3)(cid:3) is by deﬁnition the lowest common
(cid:3)
(cid:3)
r
l
l
≤ v
≤ v
≤ v
(cid:3)(cid:3)(cid:3)
(cid:3)(cid:3)(cid:3)
(cid:3)(cid:3)
(cid:3)(cid:3)
r
l
ancestor of these nodes we have that v
r . Using a similar
≤ v
(cid:3)(cid:3)(cid:3). (cid:12)(cid:13)
≤ v
≤ v
(cid:3)(cid:3)
(cid:3)(cid:3)(cid:3)
(cid:3)(cid:3)
r
l
l
reasoning for v
r and hence v
r
l
Observe this property in Figure 3; ignore SLink, ψ and the rest of the tree.

(cid:3)(cid:3)(cid:3) we conclude that v

(cid:3)(cid:3)
r ] and [v

(cid:3)(cid:3) = v

(cid:3)(cid:3)(cid:3)
l

})))}

(cid:3)
r

Using this property and ψ the equation in Lemma 2 reduces to:
}))

(cid:3))) = SDep(LCA(min{vl, v

SDep(LCA(v, v

= max0≤i<d{i+SDep(LCSA(SLinki(min{vl, v
= max0≤i<d{i + SDep(LCSA(ψi(min{vl, v

(cid:3)
r

(cid:3)
l

}, max{vr, v
(cid:3)
l

(cid:3)
l

}), ψi(max{vr, v

}), SLinki(max{vr, v

})))}

(cid:3)
r

(cid:3)) = LF(v[0..i − 1], LCSA(ψi(min{vl, v

Operationally, this corresponds to iteratively taking the ψ function, δ times
or until the Root is reached. At each step we ﬁnd the LCSA of the two current 
leaves and retrieve its stored SDep. The overall process takes O(Ψ δ) time.
Likewise SDep and LCA simplify to:
SDep(v) = SDep(LCA(v, v)) = max0≤i<d{i+SDep(LCSA(ψi(vl), ψi(vr)))}
LCA(v, v
Now it is ﬁnally clear that we do not need SLink to compute LCA. The
time to compute LCA is thus O((Ψ + t)δ). Using LCA we compute SLink in
O((Ψ + t)δ) and SLinki in O(Φ + (Ψ + t)δ) time. Note that the arguments to
LCSA do not correspond necessarily to nodes. Note also that using Lemma 4
we can extend LSA for a general node v as LSA(v) = LSA(LCA(v, v)) =
LSA(LCA(vl, vr)) = LCSA(vl, vr).

}), ψi(max{vr, v

})))

(cid:3)
r

(cid:3)
l

6 Further Operations

We now show how other operations can be computed on top of the kernel ones.
Computing Letter: Since Letter(v, i) = SLinki(v)[0] = ψi(vl)[0], we can
solve it in time O(min(Φ, iΨ)).
Computing Parent: For any node v represented as [vl, vr] we have that
Parent(v) is either LCA(vl − 1, vl) or LCA(vr, vr + 1), whichever is lowest.
This computation is correct because suﬃx trees are compact. Notice that if one
of these nodes is undeﬁned, either because vl = 0 or vr = n, then the parent is
the other node. If both nodes are undeﬁned the node in question is the Root
which has no Parent node.

Fully-Compressed Suﬃx Trees

371

Computing Child: Suppose for a moment that every sampled node stores a
list of its children and the corresponding ﬁrst letters of the edges. In our example 
the Root would store the list {($, [0, 0]), (a, [1, 2]), (b, [3, 6])}, which can
be reduced to {($, 0), (a, 1), (b, 3)}. Hence, for sampled nodes, it would be possible 
to compute Child(v, X) in O(log σ) time by binary searching its child
list. To compute Child on non-sampled nodes we could use a process similar 
to Lemma 3: determine which SLinki(v), with i < δ, is sampled; compute 
Child(SLinki(v), X); and use the LF mapping to obtain the answer,
i.e. Child(v, X) = LF(v[0..i − 1], Child(SLinki(v), X)). This process requires
O(log σ + (Ψ + t)δ) time. Still, it requires too much space since it may need to
store O(σn/δ) integers.
To avoid exceeding our space bounds we mark one leaf out of δ, i.e. mark leaf
v if v ≡δ 0. Do not confuse this concept with sampling, they are orthogonal.
In Figure 1 we mark leaves 0 and 4. For every sampled node, instead of storing
a list with all the children, we consider only the children that contain marked
leaves. In the case of the Root this means excluding the child [1, 2], hence the
resulting list is {($, 0), (b, 3)}. A binary search on this list no longer returns only
one child. Instead, it returns a range of, at most, δ children. Therefore it is
necessary to do a couple of binary searches, inside that range, to delimit the
interval of the correct child. This requires O(Φ log δ) time because now we must
use Letter to drive the binary searches. Overall, we can compute Child(v, X)
in O(log σ + Φ log δ + (Ψ + t)δ) time. Let us now consider space. Ignoring unary
paths in the sampled tree, whose space is dominated by the number of sampled
nodes, the total number of integers stored amortizes to O(n/δ), the number of
marked leaves. Hence this approach requires at most O((n/δ) log n) bits.
Computing TDep: To compute TDep(v) we need to add other O(n/δ) nodes
to the sampled tree S, so as to guarantee that, for any suﬃx tree node v,
Parentj(v) is sampled for some 0 ≤ j < δ. Recall that the TDep(v) values
are stored in S. Notice that TDep(v) = TDep(LSA(v)) + j where LSA(v) =
Parentj(v), hence, computing TDep(v) consists in reading TDep(LSA(v))
and adding the number of nodes between v and LSA(v). The sampling guarantees 
that j < δ. Hence to determine j we iterate Parent until reaching LSA(v).
The total cost is O((Ψ + t)δ2).
Computing LAQt: We extend the ParentS(v) notation to represent LSA(v)
when v is a non-sampled node. Recall that the sampled tree supports constanttime 
level ancestor queries. Hence we have any Parenti
S(v) in constant time
(cid:3)
for any node v and any i. We binary search Parenti
S(v) to ﬁnd the node v
(cid:3))). Notice that this can be computed
with TDep(v
evaluating only the second inequality. Now we iterate the Parent operation,
(cid:3))−d times. We need the additional sampling introduced
from v
(cid:3)) − d < δ. Hence the total time is O(log n +
for TDep to guarantee TDep(v
(Ψ + t)δ2).

(cid:3)) ≥ d > TDep(ParentS(v

(cid:3), exactly TDep(v

Computing LAQs: We start by binary searching Parenti
ﬁnd a node v

(cid:3)) ≥ d − (δ − 1) > SDep(ParentS(v

(cid:3) for which SDep(v

S(SLinkδ−1(v)) to
(cid:3))). Now

372

L.M.S. Russo, G. Navarro, and A.L. Oliveira

S(LSA(LF(v[i..δ − 1], v
(cid:3)))) with
we scan all the sampled nodes vi,j = Parentj
SDep(vi,j) ≥ d − i and i, j < δ. This means that we start at node v
(cid:3), follow
LF, reduce every node found to the sampled tree S and use ParentS until the
SDep of the node drops below d − i. Our aim is to ﬁnd the vi,j that minimizes
SDep(vi,j) − (d − i) ≥ 0, and then apply the LF mapping to it. The answer is
necessarily among the nodes considered.

The time to perform this operation depends on the number of existing vi,j
nodes. For this operation the sampling must satisfy Deﬁnition 1 and the condition 
of TDep. Each condition contributes with at most two sampled nodes for
every δ nodes. Therefore, there are at most 4δ nodes vi,j (see Figure 4). Unfortunately,
 the same trick does not work for TDep and LAQt, because we cannot
know which is the “right” node without bringing all of them back with LF.

(cid:4)= vr,
Computing FChild: To ﬁnd the ﬁrst child of v = [vl, vr], where vl
we simply ask for LAQs(vl, SDep(v) + 1). Likewise if we use vr we obtain the
last child. By TDepS(v) = i we mean that Parenti
S(v) = Root. This is also
deﬁned when v is not sampled. It is possible to skip the binary search step by
choosing v

S(vl), for i = TDepS(vl) − TDepS(LSA(v)) − 1.

(cid:3) = Parenti

is LAQs(vr +
Computing NSib: The next
1, SDep(Parent(v)) + 1) for any v (cid:4)= Root. Likewise we can obtain the previous 
sibling with vl − 1. We must check that the answer has the same parent
as v, to cover the case where there is no previous/next sibling. We can also skip
the binary search.

sibling of v = [vl, vr]

We are ready to state our summarizing theorem.

Theorem 1. Using a compressed suﬃx array (CSA) that supports ψ, ψi,
T [A[v]] and LF in times O(Ψ), O(Φ), O(1), and O(t), respectively, it is possible 
to represent a suﬃx tree with the properties given in Table 1.

7 Conclusions and Future Work

We presented a fully-compressed representation of suﬃx trees, which breaks
the linear-bits space barrier of previous representations at a reasonable (and
in some cases no) time complexity penalty. Our structure eﬃciently supports
common and not-so-common operations, including very powerful ones such as
lowest common ancestor (LCA) and level ancestor (LAQ) queries. In fact our
representation is largely based on the LCA operation. Suﬃx trees have been
used in combination with LCA’s for a long time, but our results show new ways
to explore this partnership.

With respect to practical considerations, we believe that the structure can
be implemented without large space costs associated to the sublinear term
o(n log σ). In fact, by using parentheses representations of the sampled tree and
compressed bitmaps, it seems possible to implement the tree with log n+O(log δ)
bits per sampled node. Our structure has the potential of using much less space

Fully-Compressed Suﬃx Trees

373

than alternative suﬃx tree representations. On the other hand, we can tune the
space/time tradeoﬀ parameter δ to ﬁt the real space needs of the application.
Even though some DNA sequences require 700 Megabytes, that is not always
the case. Hence it is reasonable to use larger representations of the suﬃx tree to
obtain faster operations, as long as the structure ﬁts in main memory.

References

1. Apostolico, A.: Combinatorial Algorithms on Words. In: The myriad virtues of

subword trees. NATO ISI Series, pp. 85–96. Springer, Heidelberg (1985)

2. Gusﬁeld, D.: Algorithms on Strings, Trees and Sequences. Cambridge University

Press, Cambridge (1997)

3. Giegerich, R., Kurtz, S., Stoye, J.: Eﬃcient implementation of lazy suﬃx trees.

Softw., Pract. Exper. 33(11), 1035–1049 (2003)

4. Manber, U., Myers, E.W.: Suﬃx arrays: A new method for on-line string searches.

SIAM J. Comput. 22(5), 935–948 (1993)

5. Sadakane, K.: Compressed Suﬃx Trees with Full Functionality. Theo. Comp. Sys.

6. Navarro, G., M¨akinen, V.: Compressed full-text indexes. ACM Comp. Surv. 39(1)

(2007)

(2007) (article 2)

7. Ferragina, P., Manzini, G., M¨akinen, V., Navarro, G.: Compressed representations

of sequences and full-text indexes. ACM Trans. Algor. 3(2) (2007) (article 20)

8. Manzini, G.: An analysis of the Burrows-Wheeler transform. J. ACM 48(3), 407–

430 (2001)

9. Sadakane, K.: New text indexing functionalities of the compressed suﬃx arrays. J.

of Algorithms 48(2), 294–313 (2003)

10. Foschini, L., Grossi, R., Gupta, A., Vitter, J.: When indexing equals compres-
sion: Experiments with compressing suﬃx arrays and applications. ACM Trans.
Algor. 2(4), 611–639 (2006)

11. Weiner, P.: Linear pattern matching algorithms. In: IEEE Symp. on Switching and

Automata Theory, pp. 1–11 (1973)

12. Lee, S., Park, K.: Dynamic rank-select structures with applications to run-length
encoded texts. In: Ma, B., Zhang, K. (eds.) CPM 2007. LNCS, vol. 4580, pp. 96–
106. Springer, Heidelberg (2007)

13. Bender, M., Farach-Colton, M.: The LCA problem revisited. In: Gonnet, G.H.,
Viola, A. (eds.) LATIN 2000. LNCS, vol. 1776, pp. 88–94. Springer, Heidelberg
(2000)

14. Fischer, J., Heun, V.: A new succinct representation of RMQ-information and
improvements in the enhanced suﬃx array. In: Chen, B., Paterson, M., Zhang, G.
(eds.) ESCAPE 2007. LNCS, vol. 4614, pp. 459–470. Springer, Heidelberg (2007)
15. Bender, M., Farach-Colton, M.: The level ancestor problem simpliﬁed. Theor.

Comp. Sci. 321(1), 5–12 (2004)

16. Geary, R., Raman, R., Raman, V.: Succinct ordinal trees with level-ancestor

queries. In: Munro, J.I. (ed.) SODA, pp. 1–10. SIAM, Philadelphia (2004)

17. Raman, R., Raman, V., Rao, S.S.: Succinct indexable dictionaries with applications

to encoding k-ary trees and multisets. In: SODA, pp. 233–242 (2002)

