Self-indexed Text Compression
Using Straight-Line Programs

Francisco Claude1,(cid:2) and Gonzalo Navarro2,(cid:2)(cid:2)

1 David R. Cheriton School of Computer Science, University of Waterloo

2 Department of Computer Science, University of Chile

fclaude@cs.uwaterloo.ca

gnavarro@dcc.uchile.cl

Abstract. Straight-line programs (SLPs) oﬀer powerful text compression 
by representing a text T [1, u] in terms of a restricted context-free
grammar of n rules, so that T can be recovered in O(u) time. However,
the problem of operating the grammar in compressed form has not been
studied much. We present a grammar representation whose size is of the
same order of that of a plain SLP representation, and can answer other
queries apart from expanding nonterminals. This can be of independent
interest. We then extend it to achieve the ﬁrst grammar representation
able of extracting text substrings, and of searching the text for patterns,
in time o(n). We also give byproducts on representing binary relations.

1 Introduction and Related Work

Grammar-based compression is a well-known technique since at least the seventies,
 and still a very active area of research. From the diﬀerent variants of the
idea, we focus on the case where a given text T [1, u] is replaced by a context-free
grammar (CFG) G that generates just the string T . Then one can store G instead
of T , and this has shown to provide a universal compression method [18]. Some
examples are LZ78 [31], Re-Pair [19] and Sequitur [25], among many others [5].
When a CFG deriving a single string is converted into Chomsky Normal
Form, the result is essentially a Straight-Line Program (SLP), that is, a grammar
where each nonterminal appears once at the left-hand side of a rule, and can
either be converted into a terminal or into the concatenation of two previous
nonterminals. SLPs are thus as powerful as CFGs for our purpose, and the
grammar-based compression methods above can be straightforwardly translated,
with no signiﬁcant penalty, into SLPs. SLPs are in practice competitive with the
best compression methods [11].

There are textual substitution compression methods which are more powerful
than those CFG-based [17]. A well-known one is LZ77 [30], which cannot be
directly expressed using CFGs. Yet, an LZ77 parsing can be converted into an

(cid:2) Funded by NSERC of Canada and Go-Bell Scholarships Program.
(cid:2)(cid:2) Funded in part by Fondecyt Grant 1-080019, and by Millennium Institute for Cell

Dynamics and Biotechnology, Grant ICM P05-001-F, Mideplan, Chile.

R. Kr´aloviˇc and D. Niwi´nski (Eds.): MFCS 2009, LNCS 5734, pp. 235–246, 2009.
c(cid:2) Springer-Verlag Berlin Heidelberg 2009

236

F. Claude and G. Navarro

SLP with an O(log u) penalty factor in the size of the grammar, which might be
preferable as SLPs are much simpler to manipulate [28].

SLPs have received attention because, despite their simplicity, they are able to
capture the redundancy of highly repetitive strings. Indeed, an SLP of n rules can
represent a text exponentially longer than n. They are also attractive because
decompression is easily carried out in linear time. Compression, instead, is more
troublesome. Finding the smallest SLP that represents a given text T [1, u] is
NP-complete [28,5]. Moreover, some popular grammar-based compressors such
as LZ78, Re-Pair and Sequitur, can generate a compressed ﬁle much larger than
the smallest SLP [5]. Yet, a simple method to achieve an O(log u)-approximation
is to parse T using LZ77 and then converting it into an SLP [28], which in
addition is balanced: the height of the derivation tree for T is O(log u). (Also,
any SLP can be balanced by paying an O(log u) space penalty factor.)

Compression is regarded nowadays not just as an aid for cheap archival or
transmission. Since the last decade, the concept of compressed text databases
has gained momentum. The idea is to handle a large text collection in compressed 
form all the time, and decompress just for displaying. Compressed text
databases require at least two basic operations over a text T [1, u]: extract and
ﬁnd. Operation extract returns any desired portion T [l, l + m] of the text. Operation 
ﬁnd returns the positions of T where a given search pattern P [1, m] occurs
in T . We refer as occ to the number of occurrences returned by a ﬁnd operation.
Extract and ﬁnd should be carried out in o(u) time to be practical for large
databases.

There has been some work on random access to grammar-based compressed
text, without decompressing all of it [10]. As for ﬁnding patterns, there has been
much work on sequential compressed pattern matching [1], that is, scanning the
whole grammar. The most attractive result is that of Kida et al. [17], which
can search general SLPs/CFGs in time O(n + m2 + occ). This may be o(u), but
still linear in the size of the compressed text. Large compressed text databases
require indexed searching, where data structures are built on the compressed
text to permit searching in o(n) time (at least for small enough m and occ).

Indeed, there has been much work on implementing compressed text databases
supporting the operations extract and ﬁnd eﬃciently (usually in O(mpolylog(n))
time) [24], but generally based on the Burrows-Wheeler Transform or Compressed 
Suﬃx Arrays, not on grammar compression. The only exceptions are
based on LZ78-like compression [23,8,27]. These are self-indexes, meaning that
the compressed text representation itself can support indexed searches. The fact
that no (or weak) grammar compression is used makes these self-indexes not
suﬃciently powerful to cope with highly repetitive text collections, which arise
in applications such as computational biology, software repositories, transaction
logs, versioned documents, temporal databases, etc. This type of applications require 
self-indexes based on stronger compression methods, such as general SLPs.
As an example, a recent study modeling a genomics application [29] concluded
that none of the existing self-indexes was able to capture the redundancies present
in the collection. Even the LZ78-based ones failed, which is not surprising given

Self-indexed Text Compression Using Straight-Line Programs

237

that LZ78 can output a text exponentially larger than the smallest SLP. The scenario 
[29] considers a set of r genomes of length n, of individuals of the same species,
and can be modeled as r copies of a base sequence, where s edit operations (substitutions,
 to simplify) are randomly placed. The most compact self-indexes [24,13,9]
occupy essentially nrHk bits, where Hk is the k-th order entropy of the base sequence,
 but this is multiplied r times because they are unable of exploiting longrange 
repetitions. The powerful LZ77, instead, is able to achieve nHk + O((r +
s) log n) bits, that is, the compressed base sequence plus O(log n) bits per edit and
per sequence. A properly designed SLP can achieve nHk + O(r log n)+ O(s log2 n)
bits, which is much better than the current techniques. It is not as good as LZ77,
self-indexes based on LZ77 are extremely challenging and do not exist yet.

In this paper we introduce the ﬁrst SLP representation that can support
operations extract and ﬁnd in o(n) time. More precisely, a plain SLP representation 
takes 2n log n bits1, as each new rule expands into two other rules. Our
representation takes O(n log n) + n log u bits. It can carry out extract in time
O((m + h) log n), where h is the height of the derivation tree, and ﬁnd in time
O((m(m + h) + h occ) log n) (see the detailed results in Thm. 3). A part of our
index is a representation for SLPs which takes 2n log n(1 + o(1)) bits and is able
of retrieving any rule in time O(log n), but also of answering other queries on
the grammar within the same time, such as ﬁnding the rules mentioning a given
non-terminal. We also show how to represent a labeled binary relation, which in
addition permits a kind of range query.

Our result constitutes a self-index building on much stronger compression
methods than the existing ones, and as such, it has the potential of being extremely 
useful to implement compressed text databases, in particular the very
repetitive ones, by combining good compression and eﬃcient indexed searching.
Our method is independent on the way the SLP is generated, and as such it can
be coupled with diﬀerent SLP construction algorithms, which might ﬁt diﬀerent
applications.

2 Basic Concepts

2.1 Succinct Data Structures

We make heavy use of succinct data structures for representing sequences with
support for rank/ select and for range queries. Given a sequence S of length n,
drawn from an alphabet Σ of size σ, rankS(a, i) counts the occurrences of symbol
a ∈ Σ in S[1, i], rankS(a, 0) = 0; and selectS(a, i) ﬁnds the i-th occurrence of
symbol a ∈ Σ in S, selectS(a, 0) = 0. We also require that data structures
representing S provide operation accessS(i) = S[i].
For the special case Σ = {0, 1}, the problem has been solved using n + o(n)
bits of space while answering the three queries in constant time [6]. This was
later improved to use O(m log n
m) + o(n) bits, where m is the number of bits set
in the bitmap [26].

1 In this paper log stands for log2 unless stated otherwise.

238

F. Claude and G. Navarro

The general case has been proved to be a little harder. Wavelet trees [13]
achieve n log σ+o(n) log σ bits of space while answering all the queries in O(log σ)
time. Another interesting proposal [12], focused on large alphabets, achieves
n log σ + no(log σ) bits of space and answers rank and access in O(log log σ)
time, while select takes O(1) time. Another tradeoﬀ within the same space [12]
is O(1) time for access, O(log log σ) time for select, and O(log log σ log log log σ)
time for rank.

M¨akinen and Navarro [20] showed how to use a wavelet tree to represent a
permutation π of [1, n] so as to answer range queries. We use a trivial variant
in this paper. Given a general sequence S[1, n] over alphabet [1, σ], we use the
wavelet tree of S to ﬁnd all the symbols of S[i1, i2] (1 ≤ i1 ≤ i2 ≤ n) which are
in the range [j1, j2] (1 ≤ j1 ≤ j2 ≤ σ). The operation takes O(log σ) to count
the number of results, and can report each such occurrence in O(log σ) time by
tracking each result upwards in the wavelet tree to ﬁnd its position in S, and
downwards to ﬁnd its symbol in [1, σ]. The algorithms are almost identical to
those for permutations [20].

2.2 Straight-Line Programs

We now deﬁne a Straight-Line Program (SLP) and highlight some properties.
Deﬁnition 1. [16] A Straight-Line Program (SLP) G = (X = {X1, . . . , Xn}, Σ)
is a grammar that deﬁnes a single ﬁnite sequence T [1, u], drawn from an alphabet
Σ = [1, σ] of terminals. It has n rules, which must be of the following types:
– Xi → α, where α ∈ Σ. It represents string F(Xi) = α.
– Xi → XlXr, where l, r < i. It represents string F(Xi) = F(Xl)F(Xr).
We call F(Xi) the phrase generated by nonterminal Xi, and T = F(Xn).
in the SLP G = (X, Σ)
Deﬁnition 2. [28] The height of a symbol Xi
is deﬁned as height(Xi) = 1 if Xi → α ∈ Σ, and height(Xi) =
1 + max(height (Xl), height (Xr)) if Xi → XlXr. The height of the SLP is
height(G) = height(Xn). We will refer to height(G) as h when the referred grammar 
is clear from the context.

As some of our results will depend on the height of the SLP, it is interesting
to recall that an SLP G of n rules generating T [1, u] can be converted into a
G(cid:3) of O(n log u) rules and height(G(cid:3)) = O(log u), in O(n log u) time [28]. Also,
as several grammar-compression methods are far from optimal [5], it is interesting 
that one can ﬁnd in linear time an O(log u) approximation to the smallest
grammar, which in addition is balanced (height O(log u)) [28].

3 Labeled Binary Relations with Range Queries

In this section we introduce a data structure for labeled binary relations supporting 
range queries. Consider a binary relation R ⊆ A × B, where A =

Self-indexed Text Compression Using Straight-Line Programs

239

{1, 2, . . . , n1}, B = {1, 2, . . . , n2}, a function L : A × B → L ∪ {⊥}, mapping
pairs in R to labels in L = {1, 2, . . . , (cid:6)}, (cid:6) ≥ 1, and the others to ⊥. We support
the following queries:
– L(a, b).
– A(b) = {a, (a, b) ∈ R}.
– B(a) = {b, (a, b) ∈ R}.
– R(a1, a2, b1, b2) = {(a, b) ∈ R, a1 ≤ a ≤ a2, b1 ≤ b ≤ b2}.
– L(l) = {(a, b) ∈ R, L(a, b) = l}.
– The sizes of the sets: |A(b)|,|B(a)|, |R(a1, a2, b1, b2)|, and |L(l)|.
We build on an idea by Barbay et al. [2]. We deﬁne, for a ∈ A, s(a) = b1b2 . . . bk,
where bi < bi+1 for 1 ≤ i < k and B(a) = {b1, b2, . . . , bk}. We build a string
SB = s(1)s(2) . . . s(n1) and write down the cardinality of each B(a) in unary
on a bitmap XB = 0|B(1)|10|B(2)|1 . . . 0|B(n1)|1. Another sequence SL lists the
labels L(a, b) in the same order they appear in SB: SL = l(1)l(2) . . . l(n1),
l(a) = L(a, b1)L(a, b2) . . .L(a, bk). We also store a bitmap XA = 0|A(1)|10|A(2)|1
. . . 0|A(n2)|1.
We represent SB using wavelet trees [13], L with the structure for large alphabets 
[12], and XA and XB in compressed form [26]. Calling r = |R|, SB requires
r log n2+o(r) log n2 bits, L requires r log (cid:6)+r o(log (cid:6)) bits (i.e., zero if (cid:6) = 1), and
XA and XB use O(n1 log r+n1
n2 )+ o(r + n1 + n2) = O(r)+ o(n1 + n2)
bits. We answer queries as follows:
– |A(b)|: This is just selectXA(1, b) − selectXA(1, b − 1) − 1.
– |B(a)|: It is computed in the same way using XB.
– L(a, b): Compute y ← selectXB(1, a − 1) − a + 1. Now, if rankSB (b, y) =
rankSB (b, y+|B(a)|) then a and b are not related and we return ⊥, otherwise
we return SL[selectSB(b, rankSB (b, y + |B(a)|))].
– A(b): We ﬁrst compute |A(b)| and then retrieve the i-th element by doing
yi ← selectSB(b, i) and returning 1 + selectXB(0, yi) − yi.
– B(a): This is SB[selectXB(1, a − 1) − a + 2 . . . selectXB(1, a) − a].
– R(a1, a2, b1, b2): We ﬁrst determine which elements in SB correspond to
←
the range [a1, a2]. We set a(cid:3)
selectXB(1, a2) − a2. Then, using range queries in a wavelet tree [20], we
retrieve the elements from SB[a(cid:3)
– L(l): We retrieve consecutive occurrences of l in SL. For the i-th occurrence
we ﬁnd yi ← selectSL(l, i), then we compute b ← SB[yi] and a ← 1 +
selectXB(0, yi) − yi. Determining |L(l)| is done via rankSL(l, r).

← selectXB(1, a1 − 1) − a1 + 2 and a(cid:3)
, a(cid:3)

n1 + n2 log r+n2

2] which are in the range [b1, b2].

1

1

2

We note that, if we do not support queries R(a1, a2, b1, b2), we can use also the
faster data structure [12] for SB.
Theorem 1. Let R ⊆ A × B be a binary relation, where A = {1, 2, . . . , n1},
B = {1, 2, . . . , n2}, and a function L : A × B → L ∪ {⊥}, which maps every
pair in R to a label in L = {1, 2, . . . , (cid:6)}, (cid:6) ≥ 1, and pairs not in R to ⊥. Then
R can be indexed using (r + o(r))(log n2 + log (cid:6) + o(log (cid:6)) + O(1)) + o(n1 + n2)

240

F. Claude and G. Navarro

bits of space, where r = |R|. Queries can be answered in the times shown below,
where k is the size of the output. One can choose (i) rnk(x) = acc(x) = log log x
and sel(x) = 1, or (ii) rnk(x) = log log x log log log x, acc(x) = 1 and sel(x) =
log log x, independently for x = (cid:6) and for x = n2.

Time (with range)
O(log n2 + acc((cid:6)))
O(1 + k log n2)
O(1 + k log n2)
O(1)
O((k + 1) log n2)

O(rnk((cid:6)))

Operation
L(a, b)
A(b)
B(a)
|A(b)|,|B(a)|
R(a1, a2, b1, b2)
|R(a1, a2, b1, b2)| O(log n2)
L(l)
|L(l)|
We note the asymmetry of the space and time with respect to n1 and n2, whereas
the functionality is symmetric. This makes it always convenient to arrange that
n1 ≥ n2.

Time (without range)
O(rnk(n2) + sel(n2) + acc((cid:6)))
O(1 + k sel(n2))
O(1 + k acc(n2))
O(1)
—
—

O((k + 1)sel((cid:6)) + k log n2) O((k + 1)sel((cid:6)) + k acc(n2))
O(rnk((cid:6)))

4 A Powerful SLP Representation

σ ) = O(n log σ(cid:2)

We provide in this section an SLP representation that permits various queries
on the SLP within essentially the same space of a plain representation.
Let us assume for simplicity that all the symbols in Σ are used in the SLP, and
thus σ ≤ n is the eﬀective alphabet size. If this is not the case and max(Σ) =
σ(cid:3) > n, we can always use a mapping S[1, σ(cid:3)] from Σ to the eﬀective alphabet
range [1, σ], using rank and select in S. By using Raman et al.’s representation
[26], S requires O(σ log σ(cid:2)
n ) bits. Any representation of such an
SLP would need to pay for this space.
A plain representation of an SLP with n rules requires at least 2(n−σ)(cid:10)log n(cid:11)+
σ(cid:10)log σ(cid:11) ≤ 2n(cid:10)log n(cid:11) bits. Based on our labeled binary relation data structure of
Thm. 1, we give now an alternative SLP representation which requires asymptotically 
the same space, 2n log n + o(n log n) bits, and is able to answer a number
of interesting queries on the grammar in O(log n) time. This will be a key part
of our indexed SLP representation.
Deﬁnition 3. A Lexicographic Straight-Line Program (LSLP) G = (X, Σ, s)
is a grammar with nonterminals X = {X1, X2, . . . , Xn}, terminals Σ, and two
types of rules: (i) Xi → α, where α ∈ Σ, (ii) Xi → XlXr, such that:
1. The Xis can be renumbered X(cid:3)
2. F(Xi) (cid:12) F(Xi+1), 1 ≤ i < n, being (cid:12) the lexicographical order.
3. There are no duplicate right hands in the rules.
n, so that G represents the text T = F(Xs).
4. Xs is mapped to X(cid:3)

i in order to obtain an SLP.

Self-indexed Text Compression Using Straight-Line Programs

241

It is clear that every SLP can be transformed into an LSLP, by removing duplicates 
and lexicographically sorting the expanded phrases. We will use LSLPs in
place of SLPs from now on.

Let us regard a binary relation as a table where the rows represent the elements
of set A and the columns the elements of B. In our representation, every row
corresponds to a symbol Xl (set A) and every column a symbol Xr (set B). Pairs
(l, r) are related, with label i, whenever there exists a rule Xi → XlXr. Since
A = B = L = {1, 2, . . . n} and |R| = n, the structure uses 2n log n + o(n log n)
bits. Note that function L is invertible, thus |L(l)| = 1.
To handle the rules of the form Xi → α, we set up a bitmap Y [1, n] so that
Y [i] = 1 if and only if Xi → α for some α ∈ Σ. Thus we know Xi → α in constant
time because Y [i] = 1 and α = rankY (1, i). The total space is n + o(n) = O(n)
bits [6]. This works because the rules are lexicographically sorted and all the
symbols in Σ are used.

This representation lets us answer the following queries.
– Access to rules: Given i, ﬁnd l and r such that Xi → XlXr, or α such that
Xi → α. If Y [i] = 1 we obtain α in constant time as explained. Otherwise,
we obtain L(i) = {(l, r)} from the labeled binary relation, in O(log n) time.
– Reverse access to rules: Given l and r, ﬁnd i such that Xi → XlXr, if
any. This is done in O(log n) time via L(l, r) (if it returns ⊥, there is no
such Xi). We can also ﬁnd, given α, the Xi → α, if any, in O(1) time via
i = selectY (1, α).
– Rules using a left/right symbol: Given i, ﬁnd those j such that Xj → XiXr
(left) or Xj → XlXi (right) for some Xl, Xr. The ﬁrst is answered using
{L(i, r), r ∈ B(j)} and the second using {L(l, i), l ∈ A(j)}, in O(log n) time
per each Xi found.
– Rules using a range of symbols: Given l1 ≤ l2, r1 ≤ r2, ﬁnd those i such
that Xi → XlXr for any l1 ≤ l ≤ l2 and r1 ≤ r ≤ r2. This is answered, in
O(log n) time per symbol retrieved, using {L(a, b), (a, b) ∈ R(l1, l2, r1, r2)}.
Again, if the last operation is not provided, we can choose the faster representation 
[12] (alternative (i) in Thm. 1), to achieve O(log log n) time for all the
other queries.
Theorem 2. An SLP G = (X = {X1, . . . , Xn}, Σ), Σ = [1, σ], σ ≤ n, can be
represented using 2n log n + o(n log n) bits, such that all the queries described
above (access to rules, reverse access to rules, rules using a symbol, and rules
using a range of symbols) can be answered in O(log n) time per delivered datum.
If we do not support the rules using a range of symbols, times drop to O(log log n).
For arbitrary integer Σ one needs additional O(n log max(Σ)

) bits.

n

5 Indexable Grammar Representations

We now provide an LSLP-based text representation that permits indexed search
and random access. We assume our text T [1, u], over alphabet Σ = [1, σ], is
represented with an SLP of n rules.

242

F. Claude and G. Navarro

We will represent an LSLP G using a variant of Thm. 2. The rows will represent 
Xl as before, but these will be sorted by reverse lexicographic order, as if
they represented F(Xl)rev. The columns will represent Xr, ordered lexicographically 
by F(Xr). We will also store a permutation πR, which maps reverse to
direct lexicographic ordering. This must be used to translate row positions to
nonterminal identiﬁers. We use Munro et al.’s representation [22] for πR, with
parameter  = 1
R in
O(log n) time, and the structure needs n log n + O(n) bits of space.
With the LSLP representation and πR, the space required is 3n log n +
o(n log n) bits. We add other n log u bits for storing the lengths |F(Xi)| for
all the nonterminals Xi.

log n, so that πR can be computed in constant time and π−1

5.1 Extraction of Text from an LSLP
To expand a substring F(Xi)[j, j(cid:3)], we ﬁrst ﬁnd position j: We recursively descend 
in the parse tree rooted at Xi until ﬁnding its jth position. Let Xi → XlXr,
then if |F(Xl)| ≥ j we descend to Xl, otherwise to Xr, in this case looking for
position j − |F(Xl)|. This takes O(height (Xi) log n) time. In our way back from
the recursion, if we return from the left child, we fully traverse the right child
left to right, until outputting j(cid:3) − j + 1 terminals.
This takes in total O((height (Xi)+ j(cid:3)− j) log n) time, which is at most O((h+
j(cid:3) − j) log n). This is because, on one hand, we will follow both children of a rule
at most j(cid:3) − j times. On the other, we will follow only one child at most twice
per tree level, as otherwise two of them would share the same parent.

5.2 Searching for a Pattern in an LSLP

Our problem is to ﬁnd all the occurrences of a pattern P = p1p2 . . . pm in the
text T [1, u] deﬁned by an LSLP of n rules. As in previous work [15], except for
the special case m = 1, occurrences can be divided into primary and secondary.
A primary occurrence in F(Xi), Xi → XlXr, is such that it spans a suﬃx of
F(Xl) and a preﬁx of F(Xr), whereas each time Xi is used elsewhere (directly
or transitively in other nonterminals that include it) it produces secondary occurrences.
 In the case P = α, we say that the primary occurrence is at Xi → α
and the other occurrences are secondary.

Our strategy is to ﬁrst locate the primary occurrences, and then track all their
secondary occurrences in a recursive fashion. To ﬁnd primary occurrences of P ,
we test each of the m − 1 possible partitions P = PlPr, Pl = p1p2 . . . pk and
Pr = pk+1 . . . pm, 1 ≤ k < m. For each partition PlPr, we ﬁrst ﬁnd all those Xls
such that Pl is a suﬃx of F(Xl), and all those Xrs such that Pr is a preﬁx of F(Xr).
The latter forms a lexicographic range [r1, r2] in the F(Xr)s, and the former a lexicographic 
range [l1, l2] in the F(Xl)revs. Thus, using our LSLP representation, the
Xis containing the primary occurrences correspond those labels i found within rows
l1 and l2, and between columns r1 and r2, of the binary relation. Hence a query for
rules using a range of symbols will retrieve each such Xi in O(log n) time. If P = α,
our only primary occurrence is obtained in O(1) time using reverse access to rules.

Self-indexed Text Compression Using Straight-Line Programs

243

Now, given each primary occurrence at Xi, we must track all the nonterminals
that use Xi in their right hand sides. As we track the occurrences, we also
maintain the oﬀset of the occurrence within the nonterminal. The oﬀset for the
primary occurrence at Xi → XlXr is |F(Xl)|−k+1 (l is obtained with an access
to rule query for i). Each time we arrive at the initial symbol Xs, the oﬀset gives
the position of a new occurrence.
To track the uses of Xi, we ﬁrst ﬁnd all those Xj → XiXr for some Xr, using
query rules using a left symbol for π−1
R (i). The oﬀset is unaltered within those
new nonterminals. Second, we ﬁnd all those Xj → XlXi for some Xl, using
query rules using a right symbol for i. The oﬀset in these new nonterminals is
that within Xi plus |F(Xl)|, where again πR(l) is obtained from the result using
an access to rule query. We proceed recursively with all the nonterminals Xj
found, reporting the oﬀsets (and ﬁnishing) each time we arrive at Xs.

Note that we are tracking each occurrence individually, so that we can process
several times the same nonterminal Xi, yet with diﬀerent oﬀsets. Each occurrence
may require to traverse all the syntax tree up to the root, and we spend O(log n)
time at each step. Moreover, we carry out m − 1 range queries for the diﬀerent
pattern partitions. Thus the overall time to ﬁnd the occ occurrences is O((m +
h occ) log n).

We remark that we do not need to output all the occurrences of P . If we just
want occ occurrences, our cost is proportional to this occ. Moreover, the existence
problem, that is, determining whether or not P occurs in T , can be answered
just by counting the primary occurrences, and it corresponds to occ = 0. The
remaining problem is how to ﬁnd the range of phrases starting/ending with a
suﬃx/preﬁx of P . This is considered next.

5.3 Preﬁx and Suﬃx Searching

We present diﬀerent time/space tradeoﬀs, to search for Pl and Pr in the respective 
sets.

Binary search based approach. We can perform a binary search over the
F(Xi)s and over the F(Xi)revs to determine the ranges where Pr and P rev
,
respectively, belong. We do the ﬁrst binary search in the nonterminals as they
are ordered in the LSLP. In order to do the string comparisons, we extract the
ﬁrst m terminals of F(Xi), in time O((m + h) log n) (Sec. 5.1). As the binary
search requires O(log n) comparisons, the total cost is O((m + h) log2 n) for the
partition PlPr. The search within the reverse phrases is similar, except that we
extract the m rightmost terminals and must use πR to ﬁnd the rule from the
position in the reverse ordering. This variant needs no extra space.

l

Compact Patricia Trees. Another option is to build Patricia Trees [21] for the
F(Xi)s and for the F(Xi)revs (adding them a terminator so that each phrase
corresponds to a leaf). By using the cardinal tree representation of Benoit et
al. [4] for the tree structure and the edge labels, each such tree can be represented
using 2n log σ + O(n) bits, and traversal (including to a child labeled α) can be

244

F. Claude and G. Navarro

carried out in constant time. The ith leaf of the tree for the F(Xi)s corresponds
to nonterminal Xi (and the ith of the three for the F(Xi)revs, to XπR(i)). Hence,
upon reaching the tree node corresponding to the search string, we obtain the
lexicographic range by counting the number of leaves up to the node subtree and
past it, which can also be done in constant time [4].

The diﬃcult point is how to store the Patricia tree skips, as in principle they
require other 4n log u bits of space. If we do not store the skips at all, we can still
compute them at each node by extracting the corresponding substrings for the
leftmost and rightmost descendant of the node, and checking for how many more
symbols they coincide [6]. This can be obtained in time O(((cid:6) + h) log n), where (cid:6)
is the skip value (Sec. 5.1). The total search time is thus O(m log n+ mh log n) =
O(mh log n).
Instead, we can use k bits for the skips, so that skips in [1, 2k − 1] can be
represented, and a skip zero means ≥ 2k. Now we need to extract leftmost and
rightmost descendants only when the edge length is (cid:6) ≥ 2k, and we will work
O(((cid:6) − 2k + h) log n) time. Although the (cid:6) − 2k terms still can add up to O(m)
(e.g., if all the lengths are (cid:6) = 2k+1), the h terms can be paid only O(1 + m/2k)
times. Hence the total search cost is O((m + h + mh
2k ) log n), at the price of at
most 4nk extra bits of space. We must also do the ﬁnal Patricia tree check due
to skipped characters, but this adds only O((m + h) log n) time. For example,
using k = log h we get O((m + h) log n) time and 4n log h extra bits of space.
As we carry out m− 1 searches for preﬁxes and suﬃxes of P , as well as m− 1

range searches, plus occ extraction of occurrences, we have the ﬁnal result.
Theorem 3. Let T [1, u] be a text over an eﬀective alphabet [1, σ] represented
by an SLP of n rules and height h. Then there exists a representation of T
using n(log u + 3 log n + O(log σ + log h) + o(log n)) bits, such that any substring
T [l, r] can be extracted in time O((r − l + h) log n), and the positions of occ
occurrences of a pattern P [1, m] in T can be found in time O((m(m + h) +
h occ) log n). By removing the O(log h) term in the space, search time raises to
O((m2 + occ)h log n). By further removing the O(log σ) term in the space, search
time raises to O((m(m + h) log n + h occ) log n). The existence problem is solved
within the time corresponding to occ = 0.
Compared with the 2n log n bits of the plain SLP representation, ours requires
at least 4n log n+o(n log n) bits, that is, roughly twice the space. More generally,
as long as u = nO(1), our representation uses O(n log n) bits, of the same order
of the SLP size. Otherwise, our representation is superlinear in the size of the
SLP (almost quadratic in the extreme case n = O(log u)). Yet, if u = nω(1), our
representation takes uo(1) bits, which is still much smaller than the original text.
We have not discussed construction times for our index (given the SLP). Those
are O(n log n) for the binary relation part, and all the lengths |F(Xi)| could be
easily obtained in O(n) time. Sorting the strings lexicographically, as well as
|F(Xi)|, which can
constructing the tries, however, can take as much as
be even ω(u). Yet, as all the phrases are substrings of T [1, u], we can build the
suﬃx array of T in O(u) time [14], record one starting text position of each
F(Xi) (obtained by expanding T from the grammar), and then sorting them in

(cid:2)n

i=1

Self-indexed Text Compression Using Straight-Line Programs

245

O(n log n) time using the inverse suﬃx array permutation (the ordering when
one phrase is a preﬁx of the other is not relevant for our algorithm). To build
the Patricia trees we can build the suﬃx tree in O(u) time [7], mark the n suﬃx
tree leaves corresponding to phrase beginnings, prune the tree to the ancestors
of those leaves (which are O(n) after removing unary paths again), and create
new leaves with the corresponding string depths |F(Xi)|. The point to insert the
new leaves are found by binary searching the string depths |F(Xi)| with level
ancestor queries [3] from the suﬃx tree leaves. The process takes O(u + n log n)
time and O(u log u) bits of space. Reverse phrases are handled identically.

6 Conclusions and Future Work

We have presented the ﬁrst indexed compressed text representation based on
Straight-Line Programs (SLP), which are as powerful as context-free grammars.
It achieves space close to that of the bare SLP representation (in many relevant
cases, of the same order) and, in addition to just uncompressing, it permits extracting 
arbitrary substrings of the text, as well as carrying out pattern searches,
in time usually sublinear on the grammar size. We also give interesting byproducts 
related to powerful SLP and binary relation representations.

We regard this as a foundational result on the extremely important problem of
achieving self-indexes built on compression methods potentially more powerful
than the current ones [24]. As such, there are several possible improvements
we plan to work on, such as (1) reducing the n log u space term; (2) reduce
the O(m2) term in search times; (3) alleviate the O(h) term in search times
by restricting the grammar height while retaining good compression; (4) report
occurrences faster than one-by-one. We also plan to implement the structure to
achieve strong indexes for very repetitive text collections.

References

1. Amir, A., Benson, G.: Eﬃcient two-dimensional compressed matching. In: Proc.

2nd DCC, pp. 279–288 (1992)

2. Barbay, J., Golynski, A., Munro, I., Rao, S.S.: Adaptive searching in succinctly encoded 
binary relations and tree-structured documents. In: Lewenstein, M., Valiente,
G. (eds.) CPM 2006. LNCS, vol. 4009, pp. 24–35. Springer, Heidelberg (2006)

3. Bender, M., Farach-Colton, M.: The level ancestor problem simpliﬁed. Theor.

Comp. Sci. 321(1), 5–12 (2004)

4. Benoit, D., Demaine, E., Munro, I., Raman, R., Raman, V., Rao, S.S.: Representing

trees of higher degree. Algorithmica 43(4), 275–292 (2005)

5. Charikar, M., Lehman, E., Liu, D., Panigrahy, R., Prabhakaran, M., Sahai, A.,

Shelat, A.: The smallest grammar problem. IEEE TIT 51(7), 2554–2576 (2005)

6. Clark, D.: Compact Pat Trees. PhD thesis, University of Waterloo (1996)
7. Farach-Colton, M., Ferragina, P., Muthukrishnan, S.: On the sorting-complexity of

suﬃx tree construction. J. ACM 47(6), 987–1011 (2000)

8. Ferragina, P., Manzini, G.: Indexing compressed texts. J. ACM 52(4), 552–581

(2005)

246

F. Claude and G. Navarro

9. Ferragina, P., Manzini, G., M¨akinen, V., Navarro, G.: Compressed representations

of sequences and full-text indexes. ACM Trans. Alg. 3(2), 20 (2007)

10. Gasieniec, L., Kolpakov, R., Potapov, I., Sant, P.: Real-time traversal in grammarbased 
compressed ﬁles. In: Proc. 15th DCC, p. 458 (2005)

11. Gasieniec, L., Potapov,

I.: Time/space eﬃcient compressed pattern matching.

Fund. Inf. 56(1-2), 137–154 (2003)

12. Golynski, A., Munro, I., Rao, S.: Rank/select operations on large alphabets: a tool

for text indexing. In: Proc. 17th SODA, pp. 368–373 (2006)

13. Grossi, R., Gupta, A., Vitter, J.: High-order entropy-compressed text indexes. In:

Proc. 14th SODA, pp. 841–850 (2003)

14. K¨arkk¨ainen, J., Sanders, P.: Simple linear work suﬃx array construction. In:
Baeten, J.C.M., Lenstra, J.K., Parrow, J., Woeginger, G.J. (eds.) ICALP 2003.
LNCS, vol. 2719, pp. 943–955. Springer, Heidelberg (2003)

15. K¨arkk¨ainen, J., Ukkonen, E.: Lempel-Ziv parsing and sublinear-size index structures 
for string matching. In: Proc. 3rd WSP, pp. 141–155. Carleton University
Press (1996)

16. Karpinski, M., Rytter, W., Shinohara, A.: An eﬃcient pattern-matching algorithm

for strings with short descriptions. Nordic J. Comp. 4(2), 172–186 (1997)

17. Kida, T., Matsumoto, T., Shibata, Y., Takeda, M., Shinohara, A., Arikawa, S.:
Collage system: a unifying framework for compressed pattern matching. Theor.
Comp. Sci. 298(1), 253–272 (2003)

18. Kieﬀer, J., Yang, E.-H.: Grammar-based codes: A new class of universal lossless

source codes. IEEE TIT 46(3), 737–754 (2000)

19. Larsson, J., Moﬀat, A.: Oﬀ-line dictionary-based compression. Proc. IEEE 88(11),

1722–1732 (2000)

20. M¨akinen, V., Navarro, G.: Rank and select revisited and extended. Theor. Comp.

Sci. 387(3), 332–347 (2007)

21. Morrison, D.: PATRICIA – practical algorithm to retrieve information coded in

alphanumeric. J. ACM 15(4), 514–534 (1968)

22. Munro, J., Raman, R., Raman, V., Rao, S.S.: Succinct representations of permutations.
 In: Baeten, J.C.M., Lenstra, J.K., Parrow, J., Woeginger, G.J. (eds.) ICALP
2003. LNCS, vol. 2719, pp. 345–356. Springer, Heidelberg (2003)

23. Navarro, G.: Indexing text using the Ziv-Lempel trie. J. Discr. Alg. 2(1), 87–114

(2004)

24. Navarro, G., M¨akinen, V.: Compressed full-text indexes. ACM Comp. Surv. 39(1),

2 (2007)

25. Nevill-Manning, C., Witten, I., Maulsby, D.: Compression by induction of hierarchical 
grammars. In: Proc. 4th DCC, pp. 244–253 (1994)

26. Raman, R., Raman, V., Rao, S.: Succinct indexable dictionaries with applications
to encoding k-ary trees and multisets. In: Proc. 13th SODA, pp. 233–242 (2002)

27. Russo, L., Oliveira, A.: A compressed self-index using a Ziv-Lempel dictionary. Inf.

Retr. 11(4), 359–388 (2008)

28. Rytter, W.: Application of Lempel-Ziv factorization to the approximation of

grammar-based compression. Theor. Comp. Sci. 302(1-3), 211–222 (2003)

29. Sir´en, J., V¨alim¨aki, N., M¨akinen, V., Navarro, G.: Run-length compressed indexes
are superior for highly repetitive sequence collections. In: Amir, A., Turpin, A., Moffat,
 A. (eds.) SPIRE 2008. LNCS, vol. 5280, pp. 164–175. Springer, Heidelberg (2008)
30. Ziv, J., Lempel, A.: A universal algorithm for sequential data compression. IEEE

TIT 23(3), 337–343 (1977)

31. Ziv, J., Lempel, A.: Compression of individual sequences via variable length coding.

IEEE TIT 24(5), 530–536 (1978)

