Practical Dynamic Entropy-Compressed

Bitvectors with Applications

Joshimar Cordova(B) and Gonzalo Navarro

Department of Computer Science, CeBiB — Center of Biotechnology

and Bioengineering, University of Chile, Santiago, Chile

{jcordova,gnavarro}@dcc.uchile.cl

Abstract. Compressed data structures provide the same functionality
as their classical counterparts while using entropy-bounded space. While
they have succeeded in a wide range of static structures, which do not
undergo updates, they are less mature in the dynamic case, where the
theory-versus-practice gap is wider. We implement compressed dynamic
bitvectors B using |B|H0(B)+o(|B|) or |B|H0(B)(1+o(1)) bits of space,
where H0 is the zero-order empirical entropy, and supporting queries and
updates in O(w) time on a w-bit word machine. This is the ﬁrst implementation 
that provably achieves compressed space and is also practical,
operating within microseconds. Bitvectors are the basis of most compressed 
data structures; we explore applications to sequences and graphs.

1 Introduction

Compact data structures have emerged as an attractive solution to reduce the
signiﬁcant memory footprint of classical data structures, which becomes a more
relevant problem as the amount of available data grows. Such structures aim at
representing the data within almost its entropy space while supporting a rich
set of operations on it. Since their beginnings [12], several compact structures
have been proposed to address a wide spectrum of applications, with important
success stories like ordinal trees with full navigation in less than 2.5 bits [1],
range minimum queries in 2.1 bits per element [7], and full-text indexes using
almost the space of the compressed text [15], among others. Most of the major
practical solutions are implemented in the Succinct Data Structures Library [10],
which oﬀers solid C++ implementations and extensive test datasets.

Most of these implemented structures, however, are static, that is, they do
not support updates to the data once they are built. While dynamic variants
exist for many compact data structures, they are mostly theoretical and their
practicality is yet to be established.

At the core of many compact structures lay simple bitvectors supporting
two important queries: counting the number of bits b up to a given position
(rank) and ﬁnding the position of the i-th occurrence of bit b (select). Such
bitvectors enable well-known compact structures like sequences, two-dimensional

Funded by Basal Funds FB0001 and with Fondecyt Grant 1-140796, Conicyt, Chile.

c(cid:2) Springer International Publishing Switzerland 2016
A.V. Goldberg and A.S. Kulikov (Eds.): SEA 2016, LNCS 9685, pp. 105–117, 2016.
DOI: 10.1007/978-3-319-38851-9 8

106

J. Cordova and G. Navarro

grids, graphs, trees, etc. Supporting insertion and deletion of bits in the bitvectors 
translates into supporting insertion and deletions of symbols, points, edges,
and nodes, in those structures. Very recent work [16] shows that dynamic bitvectors 
are practical and that compression can be achieved for skewed frequencies of
0 s and 1 s, provided that the underlying dynamic memory allocation is handled
carefully. Furthermore, the authors implement the compressed RAM [13] and
show that it is practical by storing in it a directed graph.

In this paper we build on a theoretical proposal [17] to present the ﬁrst practical 
dynamic bitvector representations whose size is provably entropy-bounded.
A ﬁrst variant represents B[1, n] in nH0(B) + o(n) bits, where H0 denotes the
zero-order empirical entropy. For bitvectors with few 1 s, a second variant that
uses nH0(B)(1 + o(1)) bits is preferable. Both representations carry out updates
and rank/select queries in time O(w) on a w-bit machine. In practice, the times
are just a few microseconds and the compression obtained is considerable. Instead
of using our structure to implement a compressed RAM, we use our bitvectors
to implement (a) a practical dynamic wavelet matrix [5] to handle sequences
of symbols and two-dimensional grids, and (b) a compact dynamic graph that
achieves considerable space savings with competitive edge-insertion times.

Along the way we also describe how we handle the dynamic memory allocation 
with the aim of reducing RAM fragmentation, and unveil a few related
practical results that had not been mentioned in the literature.

2 Basic Concepts

(cid:2)

1≤c≤σ

n (lg n

n lg n

n lg n

Given a sequence S[1, n] over the alphabet [1, σ], access(S, i) returns the character 
S[i], rankc(S, i) returns the number of occurrences of character c in
S[1, i] and selectc(S, j) returns the position of the j-th occurrence of c. The
nc
n lg n
(empirical) zero-order entropy of S is deﬁned as H0(S) =
nc ,
where c occurs nc times in S, and is a lower bound on the average code
length for any compressor that assigns ﬁxed (variable-length) codes to symbols.
 When σ = 2 we refer to the sequence as a bitvector B[1, n] and the
entropy becomes H0(B) = m
n−m, where m = n1. The entropy
decreases when m is closer to 0 or n. In the ﬁrst case, another useful formula is
H0(B) = m

m + O(1)).

m + n−m

Dynamism is supported by the operations insert(S, i, c), which inserts the
character c before position i in S and moves characters S[i, n] one position to
the right; delete(S, i), which removes character S[i] and moves the characters
S[i + 1, n] one position to the left; and modify(S, i, c), which sets S[i] = c.
Uncompressed (or plain) bitvector representations use n + o(n) bits, and can
answer queries in O(1) time [3]. Compressed representations reduce the space
to nH0(B) + o(n) bits while retaining the constant query times [24]. Dynamic
bitvectors cannot be that fast, however: queries require Ω(lg n/ lg lg n) time if the
updates are to be handled in O(polylog n) time [8]. Dynamic plain bitvectors
with optimal times O(lg n/ lg lg n) for all the operations exist [23]. M¨akinen
and Navarro [17] presented the ﬁrst dynamic bitvectors using compressed space,

Practical Dynamic Entropy-Compressed Bitvectors with Applications

107

nH0(B) + o(n) bits, and O(lg n) times. It is possible to improve the times to
the optimal O(lg n/ lg lg n) within compressed space [21], but the solutions are
complicated and unlikely to be practical.

A crucial aspect of the dynamic bitvectors is memory management. When
insertions/deletions occur in the bit sequence, the underlying memory area needs
to grow/shrink appropriately. The classical solution, used in most of the theoretical 
results, is the allocator presented by Munro [18]. Extensive experiments
[16] showed that this allocator can have a drastic impact on the actual memory 
footprint of the structure: the standard allocator provided by the operating
system may waste up to 25 % of the memory due to fragmentation.

The ﬁrst implementation of compact dynamic structures we know of is that of
Gerlang [9]. He presents dynamic bitvectors and wavelet trees [11], and uses them
to build a compact dynamic full-text index. However, memory management is
not considered and bitvectors B[1, n] use O(n) bits of space, 3.5n–14n in practice.
A more recent implementation [25] has the same problems and thus is equally
unattractive. Brisaboa et al. [2] also explore plain dynamic bitvectors; they use a
B-tree-like structure where leaves store blocks of bits. While their query/update
times are competitive, the space reported should be read carefully as they do
not consider memory fragmentation. In the context of compact dynamic ordinal
trees, Joannou and Raman [14] present a practical study of dynamic Range MinMax 
trees [21]. Although the space usage is not clear, the times are competitive
and almost as fast as the static implementations [1].

There also exist open-source libraries providing compact dynamic structures.
The ds-vector library [22] provides dynamic bitvectors and wavelet trees, but
their space overhead is large and their wavelet tree is tailored to byte sequences;
memory fragmentation is again disregarded. The compressed data structures
framework Memoria [26] oﬀers dynamic compact bitvectors and ordinal trees,
among other structures. A custom memory allocator is provided to reduce fragmentation,
 but unfortunately the library is not in a stable state yet (as conﬁrmed
by the author of the library).

Klitzke and Nicholson [16] revisit dynamic bitvectors. They present the ﬁrst
practical implementation of the memory allocation strategy of Munro [18] tailored 
to using compact data structures, and show that it considerably reduces
memory fragmentation without incurring in performance penalties. They present
plain dynamic bitvectors B[1, n] using only 1.03n bits. For bitvectors with m (cid:2) n
1 s, they build on general-purpose compressors lz4 and lz4hc to reduce the
space up to 0.06n. However, they lack theoretical guarantees on the compression
achieved. While their work is the best practical result in the literature, the code
and further technical details are unfortunately unavailable due to legal issues (as
conﬁrmed by the ﬁrst author).

3 Dynamic Entropy-Compressed Bitvectors

In this section we present engineered dynamic bitvectors that achieve zero-order
entropy compression. These are based on the ideas of M¨akinen and Navarro [17],

108

J. Cordova and G. Navarro

but are modiﬁed to be more practical. The following general scheme underlies
almost all practical results to date and is used in this work as well. The bitvector 
B[1, n] is partitioned into chunks of contiguous bits and a balanced search
tree (we use AVLs) is built where the leaves store these chunks. The actual
partition strategy and storage used in the leaves vary depending on the desired
compression. Each internal node v of the balanced tree stores two ﬁelds: v.ones
(v.length) is the number of 1 s (total number of bits) present in the left subtree
of v. The ﬁeld v.length is used to ﬁnd a target position i in B: if i ≤ v.length
we descend to the left child, otherwise we descend to the right child and i becomes
i − v.length. This is used to answer access/rank queries and also to ﬁnd the
target leaf where an update will take place (for rank we add up the v.ones ﬁeld
whenever we go right). The ﬁeld v.ones is used to answer select1(B, j) queries:
if j ≤ v.ones the answer is in the left subtree; otherwise we move to the right
child, add v.length to the answer, and j becomes j − v.ones. For select0(B, j)
we proceed analogously, replacing v.ones by v.length− v.ones. The leaves are
sequentially scanned, taking advantage of locality. Section 3.2 assumes the tree
is traversed according to these rules.

3.1 Memory Management

Although Klitzke and Nicholson [16] present and study a practical implementation 
of Munro’s allocator [18], the technical details are brieﬂy mentioned and the
implementation is not available. We then provide an alternative implementation
with its details. In Sect. 5, both implementations are shown to be comparable.
Munro’s allocator is tailored to handle small blocks of bits, in particular
blocks whose size lies in the range [L, 2L] for some L = polylog n. It keeps L + 1
linked lists, one for each possible size, with L + 1 pointers to the heads of the
lists. Each list li consists of ﬁxed-length cells of 2L bits where the blocks of i bits
are stored contiguously. In order to allocate a block of i bits we check if there is
enough space in the head cell of li, otherwise a new cell of 2L bits is allocated
and becomes the head cell. To deallocate a block we ﬁll its space with the last
block stored in the head cell of list li; if the head cell no longer stores any block
it is deallocated and returned to the OS. Finally, given that we move blocks
to ﬁll the gaps left by deallocation, back pointers need to be stored from each
block to the external structure that points to the block, to update the pointers
appropriately. Note that in the original proposal a block may span up to two
cells and a cell may contain pieces of up to three diﬀerent blocks.
Implementation. Blocks are fully stored in a single cell to improve locality. As
in the previous work [16], we only allocate blocks of bytes: L is chosen as a multiple 
of 8 and we only handle blocks of size L, L + 8, L + 16, . . . , 2L, rounding the
requested sizes to the next multiple of 8. The cells occupy T = 2L/8 bytes and are
allocated using the default allocator provided by the system. Doing increments
of 8 bits has two beneﬁts: the total number of allocations is reduced and the
memory pointers returned by our allocator are byte-aligned. The head pointers
and lists li are implemented verbatim. The back pointers are implemented using

Practical Dynamic Entropy-Compressed Bitvectors with Applications

109

a folklore idea: when allocating a block of l bytes we instead allocate l + w/8
bytes and store in the ﬁrst w bits the address of the pointer to the block, so
that when moving blocks to ﬁll gaps the pointer can be modiﬁed. This creates
a strong binding between the external structure and the block, which can be
pointed only from one place. This restriction can be alleviated by storing the
pointer in our structure, in an immutable memory area, and let the external
structures point to the pointer. This requires that the external structures know
that the handle they have for the block is not a pointer to the data but a pointer
to the pointer. In this sense, the memory allocator is not completely transparent.
As a further optimization, given that our dynamic bitvectors are based on
search trees, we will be constantly (de)allocating very small structures representing 
the nodes of the trees (eg. 4 words for a AVL node). We use another folklore
strategy for these structures: given that modern operating systems usually provide 
8 MB of stack memory for a running process, we implement an allocator on
top of that memory, avoiding the use of the heap area for these tiny structures;
(de)allocation simply moves the end of the stack.

3.2 Entropy-Based Compression

(cid:3)

(cid:4)

b
k

Our ﬁrst variant builds on the compression format of Raman et al. [17,24],
modiﬁed to be practical. We partition the bitvector B into chunks of Θ(w2)
bits and these become the leaves of an AVL tree. We store the chunks using the
(class, oﬀset) encoding (c, o) [24]: a chunk is further partitioned into blocks of
b = w/2 bits; the class of a block is the number of 1 s it contains and its oﬀset is
the index of the block among all possible blocks of the same class when sorted
lexicographically. A class component requires lg w bits, while the oﬀset of a block
bits. All class/oﬀset components are concatenated in
of class k requires lg
arrays C/O, which are stored using our custom memory allocator. The overall
space of this encoding is nH0(B)+o(n) bits [24]. The space overhead of the AVL
tree is O(n/w) bits, since there are O(n/w2) nodes, each requiring Θ(w) bits.
Since w = Ω(lg n), this overhead is o(n). It is important to notice that while
leaves represent Θ(w2) logical bits, the actual space used by the (c, o) encoding
may be considerably smaller. In practice we choose a parameter L(cid:4), and all leaves
will store a number of physical bytes in the range [L(cid:4), 2L(cid:4)].

To answer access(B, i)/select(B, j) queries we navigate, using the AVL tree,
to the leaf containing the target position and then decode the blocks sequentially
until the desired position is found. A block is decoded in constant time using
a lookup table that, given a pair (c, o), returns the original b bits of the block.
This table has 2w/2 entries, which is small and can be regarded as program size,
since it does not depend on the data. Note that we only need to decode the
last block; for the previous ones the class component is suﬃcient to determine
how much to advance in array O. For rank1(B, i) we also need to add up the
class components (i.e., number of 1 s) up to the desired block. Again, this only
requires accessing array C, while O is only read to decode the last block. We
spend O(lg n) time to navigate the tree, O(w) time to traverse the blocks in
the target leaf, and O(w) time to process the last block bitwise. Thus queries

110

J. Cordova and G. Navarro

take O(w) time. In practice we set b = 15, hence the class components require 4
bits (and can be read by pairs from each single byte of C), the (uncompressed)
blocks are 16-bit integers, and the decoding table overhead (which is shared by
all the bitvectors) is only 64 KB.

To handle updates we navigate towards the target leaf and proceed to decompress,
 update, and recompress all the blocks to the right of the update position.
If the number of physical bytes stored in a leaf grows beyond 2L we split it in
two leaves and add a new internal node to be tree; if it shrinks beyond L we
move a single bit from the left or right sibling leaf to the current leaf. If this is
not possible (because both siblings store L physical bytes) we merge the current
leaf with one of its siblings; in either case we perform rotations on the internal
nodes of the tree appropriately to restore the AVL invariant.

Recompressing a block is done using an encoding lookup table that, given
a block of b bits, returns the associated (c, o) encoding. This adds other 64 KB
of memory. To avoid overwriting memory when the physical leaf size grows,
recompression is done by reading the leaf data and writing the updated version
in a separate memory area, which is later copied back to the leaf.

3.3 Compression of Very Sparse Bitvectors

m +O(m lg lg n

When the number m of 1 s in B is very low, the o(n) term may be signiﬁcative
compared to nH0(B). In this case we seek a structure whose space depends
mainly on m. We present our second variant (also based on M¨akinen and Navarro
m) bits, while maintaining the O(w)-
[17]) that requires only m lg n
time complexities. This space is nH0(B)(1 + o(1)) bits if m = o(n).
The main building blocks is Elias δ-codes [6]. Given a positive integer x, let
|x| denote the length of its binary representation (eg. |7| = 3). The δ-code for
x is obtained by writing ||x|| − 1 zeros followed by the binary representation of
|x| and followed by the binary representation of x without the leading 1 bit. For
example δ(7) = 01111 and δ(14) = 00100110. It follows easily that the length of
the code δ(x) is |δ(x)| = lg x + 2 lg lg x + O(1) bits.

We partition B into chunks containing Θ(w) 1 s. We build an AVL tree where
leaves store the chunks. A chunk is stored using δ-codes for the distance between
pairs of consecutive 1 s. This time the overhead of the AVL tree is O(m) bits. By
using the Jensen inequality on the lengths of the δ-codes it can be shown [17] that
the overall space of the leaves is m lg n
m) bits and the redundancy
of the AVL tree is absorbed in the second term. In practice we choose a constant
M and leaves store a number of 1 s in the range [M, 2M]. Within this space we
now show how to answer queries and handle updates in O(w) time.

m + O(m lg lg n

To answer access(i) we descend to the target leaf and start decoding the
δ-codes sequentially until the desired position is found. Note that each δ-code
represents a run of 0 s terminated with a 1, so as soon as the current run contains
the position i we return the answer. To answer rank(i) we increase the answer
by 1 per δ-code we traverse. Finally, to answer select1(j), when we reach the
target leaf looking for the j-th local 1-bit we decode the ﬁrst j codes and add

Practical Dynamic Entropy-Compressed Bitvectors with Applications

111

their sum (since they represent the lengths of the runs). Instead, select0(j) is
very similar to the access query.

To handle the insertion of a 0 at position i in a leaf we sequentially search
for the δ-code that contains position i. Let this code be δ(x); we then replace
it by δ(x + 1). To insert a 1, let i(cid:4) ≤ x + 1 be the local oﬀset inside the run
0x−11 (represented by the code δ(x)) where the insertion will take place. We
then replace δ(x) by δ(i(cid:4))δ(x − i(cid:4) + 1) if i(cid:4) ≤ x and by δ(x)δ(1) otherwise. In
either case (inserting a 1 or a 0) we copy the remaining δ-codes to the right of the
insertion point. Deletions are handled analogously; we omit the description. If,
after an update, the number of 1 s of a leaf lies outside the interval [M, 2M] we
move a run from a neighbor leaf or perform a split/merge just as in the previous
solution and then perform tree rotations to restore the AVL invariant.
The times for the queries and updates are O(w) provided that δ-codes are
encoded/decoded in constant time. To decode a δ-code we need to ﬁnd the highest 
1 in a word (as this will give us the necessary information to decode the rest).
Encoding a number x requires eﬃciently computing |x| (the length of its binary
representation), which is also the same problem. Modern CPUs provide special
support for this operation; otherwise we can use small precomputed tables. The
rest of the encoding/decoding process is done with appropriate bitwise operations.
 Furthermore, the local encoding/decoding is done on sequential memory
areas, which is cache-friendly.

4 Applications

4.1 Dynamic Sequences

The wavelet matrix [5] is a compact structure for sequences S[1, n] over a ﬁxed
alphabet [1, σ], providing support for access(i), rankc(i) and selectc(i) queries.
The main idea is to store lg σ bitvectors Bi deﬁned as follows: let S1 = S and
B1[j] = 1 iﬀ the most signiﬁcant bit of S1[j] is set. Then S2 is obtained by moving
to the front all characters S1[j] with B1[j] = 0 and moving the rest to the back
(the internal order of front and back symbols is retained). Then B2[j] = 1 iﬀ the
second most signiﬁcant bit of S2[j] is set, we create S3 by shuﬄing S2 according
to B2, and so on. This process is repeated lg σ times. We also store lg σ numbers
zj = rank0(Bj, n). The access/rank/select queries on this structure reduce to
O(lg σ) analogous queries on the bitvectors Bj, thus the times are O(lg σ) and
the ﬁnal space is n lg σ + o(n lg σ) (see the article [5] for more details).

Our results in Sect. 3 enable a dynamic implementation of wavelet matrices
with little eﬀort. The insertion/deletion of a character at position i is implemented 
by the insertion/deletion of a single bit in each of the bitvectors Bj. For
insertion of c, we insert the highest bit of c in B1[i]. If the bit is a 0, we increase z1
by one and change i to rank0(B1, i); otherwise we change i to z1 + rank1(B1, i).
Then we continue with B2, and so on. Deletion is analogous. Hence all query and
update operations require lg σ O(w)-time operations on our dynamic bitvectors.
By using our uncompressed dynamic bitvectors, we maintain a dynamic string
S[1, n] over a (ﬁxed) alphabet [1, σ] in n lg σ+o(n lg σ) bits, handling queries and

112

J. Cordova and G. Navarro

updates in O(w lg σ) time. An important result [11] states that if the bitvectors
Bj are compressed to their zero-order entropy nH0(Bj), then the overall space
is nH0(S). Hence, by switching to our compressed dynamic bitvectors (in particular,
 our ﬁrst variant) we immediately achieve nH0(S)+ o(n lg σ) bits and the
query/update times remain O(w lg σ).

4.2 Dynamic Graphs and Grids

The wavelet matrix has a wide range of applications [19]. One is directed graphs.
Let us provide dynamism to the compact structure of Claude and Navarro [4].
Given a directed graph G(V, E) with n = |V | vertices and e = |E| edges, consider
the adjacency list G[v] of each node v. We concatenate all the adjacency lists in
a single sequence S[1, e] over the alphabet [1, n] and build the dynamic wavelet
matrix on S. Each outdegree dv of vertex v is written as 10dv and appended to
a bitvector B[1, n + e]. The ﬁnal space is e lg n(1 + o(1)) + O(n) bits.
This representation allows navigating the graph. The outdegree of vertex 
v is computed as select1(B, v + 1) − select1(B, v) − 1. The j-th neighbor 
of vertex v is access(S, select1(B, v) − v + j). The edge (v, u) exists iﬀ
ranku(S, select1(B, v + 1) − v − 1) − ranku(S, select1(B, v) − v) = 1. The main
advantage of this representation is that it also enables backwards navigation of
the graph without doubling the space: the indegree of vertex v is rankv(S, e)
and the j-th reverse neighbor of v is select0(B, selectv(S, j)) − selectv(S, j).
To insert an edge (u, v) we insert a 0 at position select1(B, u)+1 to increment
the indegree of u, and then insert in S the character v at position select1(B, u)−
u + 1. Edge deletion is handled in a similar way. We thus obtain O(w lg n) time
to update the edges. Unfortunately, the wavelet matrix does not allow changing
the alphabet size. Despite this, providing edge dynamism is suﬃcient in several
applications where an upper bound on the number of vertices is known.
This same structure is useful to represent two-dimensional n × n grids with
e points, where we can insert and delete points. It is actually easy to generalize
the grid size to any c × r. Then the space is n lg r(1 + o(1)) + O(n + c) bits. The
static wavelet matrix [5] can count the number of points in a rectangular area in
time O(lg r), and report each such point in time O(lg r) as well. On our dynamic
variant, times become O(w lg r), just like the time to insert/delete points.

5 Experimental Results and Discussion

The experiments were run on a server with 4 Intel Xeon cores (each at 2.4 GHz)
and 96 GB RAM running Linux version 3.2.0-97. All implementations are in C++.
We ﬁrst reproduce the memory fragmentation stress test [16] using our allocator 
of Sect. 3.1. The experiment initially creates n chunks holding C bytes.
Then it performs C steps. In the i-th step n/i chunks are randomly chosen and
their memory area is expanded to C + i bytes. We set C = 211 and use the same
settings [16] for our custom allocator: the cell size T is set to 216 and L is set
to 211. Table 1 shows the results. The memory consumption is measured as the

Practical Dynamic Entropy-Compressed Bitvectors with Applications

113

Resident Set Size (RSS),1 which is the actual amount of physical RAM retained
by a running process. Malloc represents the default allocator provided by the
operating system and custom is our implementation. Note that for all the tested
values of n our allocator holds less RAM memory, and in particular for n = 224
(i.e., nC = 32 GB) it saves up to 12 GB. In all cases the CPU times of our allocator 
are faster than the default malloc. This shows that our implementation is
competitive with the previous one [16], which reports similar space and time.

Table 1. Memory consumption measured as RSS in GBs and CPU time (seconds) for
the RAM fragmentation test.

lg n malloc RSS custom RSS malloc time custom time
18

0.889

0.768

0.668

0.665

19

20

21

22

23

24

1.777

3.552

7.103

14.204

28.407

56.813

1.478

2.893

5.727

11.392

22.725

45.388

1.360

2.719

5.409

10.811

21.870

45.115

1.325

2.635

5.213

10.446

21.163

43.081

Having established that our allocator enables considerable reductions in
RAM fragmentation, we study our practical compressed bitvectors. We generate 
random bitvectors of size n = 50 · 223 (i.e., 50 MB) and set individual bits
to 1 with probability p. We consider skewed frequencies p = 0.1, 0.01, and 0.001.
Preliminary testing showed that, for our variant of Sect. 3.2, setting the range
of physical leaf sizes to [211, 212] bytes provided the best results. Table 2 gives
our results for the compression achieved and the time for queries and updates
(averaging insertions and deletions). We achieve 0.3–0.4 bits of redundancy over
the entropy, which is largely explained by the component c of the pairs (c, o):
these add lg(b + 1)/b = 4/15 = 0.27 bits of redundancy, whereas the O array
adds up to nH0(B). The rest of the redundancy is due to the AVL tree nodes
and the space wasted by our memory allocator. For the very sparse bitvectors
(p = 0.001), the impact of this ﬁxed redundancy is very high.

Operation times are measured by timing 105 operations on random positions
of the bitvectors. The queries on our ﬁrst variant take around 1 µs (and even less
for access), whereas the update operations take 8–15 μs. The operations become
faster as more compression is achieved.

For the very sparse bitvectors (p = 0.001) we also test our variant of Sect. 3.3.
Preliminary testing showed that enforcing leaves to handle a number of 1 s in
the range [128, 256] provided the best results. The last row of Table 2 shows the
compression and timing results for this structure. As promised in theory, the

1 Measured with https://github.com/mpetri/mem monitor.

114

J. Cordova and G. Navarro

Table 2. Memory used (measured as RSS, in MB, in bits per bit, and in redundancy
over H0(B)) and timing results (in microseconds) for our compressed dynamic bitvectors.
 The ﬁrst three rows refer to the variant of Sect. 3.2, and the last to Sect. 3.3.

p

0.1

0.01

0.001
∗0.001 1.50

0.03

MB Bits/n −H0(B) Updates Access Rank Select
1.20
38.57 0.77

15.08

0.80

0.30

1.10

21.27 0.43

19.38 0.39

0.35

0.38

0.02

10.77

8.50

5.26

0.60

0.70

1.38

0.90

0.90

1.47

1.10

1.00

1.35

Table 3. Memory usage (MBs) and times (in seconds) for the online construction and
a breadth-ﬁrst traversal of the DBLP graph to ﬁnd its weakly connected components.
The data for previous work [16] is a rough approximation.

Structure

RSS Ratio Build time Ratio BFS time Ratio

std::vector

22.20 1.00

7.40

Wavelet matrix 4.70

Previous [16]

12.42

0.21

0.30

0.06

9.34

1.00

1.68

9.00

1.00

155.67

30.00

compression achieved by this representation is remarkable, achieving 0.02 bits
of redundancy (its space is much worse on the higher values of p, however). The
query times become slightly over 1 µs and the update times are around 5 μs.

Finally, we present a single application for the dynamic wavelet matrix and
graphs. We ﬁnd the weakly connected components of a sample of the DBLP
social graph stored using the dynamic representation of Sect. 4.2 with plain
dynamic bitvectors, that is, they are stored verbatim. We use range [210, 211]
bytes for the leaf sizes.

The sample dataset consists of 317,080 vertices and 1,049,866 edges
taken from https://snap.stanford.edu/data/com-DBLP.html, with edge directions 
assigned at random. We build the graph by successive insertions of the
edges. Table 3 shows the memory consumption, the construction time (i.e., inserting 
all the edges), and the time to perform a breadth-ﬁrst search of the graph.
Our baseline is a representation of graphs based on adjacency lists implemented
using the std::vector class from the STL library in C++, where each directed
edge (u, v) is also stored as (v, u) to enable backwards navigation. Considerable
space savings are achieved using the dynamic wavelet matrix, 5-fold with respect
to the baseline. The edge insertion times are very competitive, only 70 % slower
than the baseline. The time to perform a full traversal of the graph, however, is
two orders of magnitude slower.

We now brieﬂy make an informal comparison between our results and the
best previous work [16] by extrapolating some numbers.2 For bitvectors with

2 A precise comparison is not possible since their results are not available. We use

their plots as a reference.

Practical Dynamic Entropy-Compressed Bitvectors with Applications

115

density p = 0.1 our ﬁrst variant achieves 77 % compression compared to their
85 %. For p = 0.01 ours achieves 43 % compared to their 35 %, and for p = 0.001
our second variant achieves 3 % compared to their 6 %. In terms of running times
our results handle queries in about 1 µs and updates in 8–15 μs, while their most
practical variant, based on lz4, handles queries and updates in around 10–25 μs.
These results are expected since the encodings we used ((c, o) pairs and δ-codes)
are tailored to answer rank/select queries without the need of full decompression.
 Finally, they also implement a compressed dynamic graph (based on compressed 
RAM and not on compact structures). The rough results (extrapolated
from their own comparison against std::vector) are shown in the last line of
Table 3: they use 50 % more space and 5 times more construction time than our
implementation, but their BFS time is 5 times faster.

6 Conclusions

We have presented the ﬁrst practical entropy-compressed dynamic bitvectors
with good space/time theoretical guarantees. The structures solve queries in
around a microsecond and handle updates in 5–15 µs. An important advantage
compared with previous work [16] is that we do not need to fully decompress the
bit chunks to carry out queries, which makes us an order of magnitude faster.
Another advantage over previous work is the guaranteed zero-order entropy
space, which allows us using bitvectors for representing sequences in zero-order
entropy, and full-text indexes in high-order entropy space [15].

Several improvements are possible. For example, we reported times for querying 
random positions, but many times we access long contiguous areas of a
sequence. Those can be handled much faster by remembering the last accessed
AVL tree node and block. In the (c, o) encoding, we would access a new byte of C
every 30 operations, and decode a new block of O every 15, which would amount
to at least an order-of-magnitude improvement in query times. For δ-encoded
bitvectors, we would decode a new entry every n/m operations on average.

Another improvement is to allow for faster queries when updates are less
frequent, tending to the fast static query times in the limit. We are studying
policies to turn an AVL subtree into static when it receives no updates for some
time. This would reduce, for example, the performance gap for the BFS traversal
in our graph application once it is built, if further updates are infrequent.
Finally, there exist theoretical proposals [20] to represent dynamic sequences
that obtain the optimal time O(lg n/ lg lg n) for all the operations. This is much
better than the O(w lg σ) time we obtain with dynamic wavelet matrices. An
interesting future work path is to try to turn that solution into a practical
implementation. It has the added beneﬁt of allowing us to update the alphabet,
unlike wavelet matrices.

Our implementation of dynamic bitvectors and the memory allocator are

available at https://github.com/jhcmonroy/dynamic-bitvectors.

116

J. Cordova and G. Navarro

References

1. Arroyuelo, D., C´anovas, R., Navarro, G., Sadakane, K.: Succinct trees in practice.

In: Proceedings of the 12th ALENEX, pp. 84–97 (2010)

2. Brisaboa, N., de Bernardo, G., Navarro, G.: Compressed dynamic binary relations.

In: Proceedings of the 22nd DCC, pp. 52–61 (2012)

3. Clark, D.: Compact PAT Trees. Ph.D. thesis, Univ. Waterloo, Canada (1996)
4. Claude, F., Navarro, G.: Extended compact web graph representations. In: Elomaa,
T., Mannila, H., Orponen, P. (eds.) Ukkonen Festschrift 2010. LNCS, vol. 6060,
pp. 77–91. Springer, Heidelberg (2010)

5. Claude, F., Navarro, G., Ord´o˜nez, A.: The wavelet matrix: an eﬃcient wavelet tree

for large alphabets. Inf. Syst. 47, 15–32 (2015)

6. Elias, P.: Universal codeword sets and representations of the integers. IEEE Trans.

Inf. Theor. 21(2), 194–203 (1975)

7. Ferrada, H., Navarro, G.: Improved range minimum queries. In: Proceedings of the

26th DCC, pp. 516–525 (2016)

8. Fredman, M., Saks, M.: The cell probe complexity of dynamic data structures. In:

Proceedings of the 21st STOC, pp. 345–354 (1989)

9. Gerlang, W.: Dynamic FM-Index for a Collection of Texts with Application to
Space-eﬃcient Construction of the Compressed Suﬃx Array. Master’s thesis, Univ.
Bielefeld, Germany (2007)

10. Gog, S., Beller, T., Moﬀat, A., Petri, M.: From theory to practice: plug and play
with succinct data structures. In: Gudmundsson, J., Katajainen, J. (eds.) SEA
2014. LNCS, vol. 8504, pp. 326–337. Springer, Heidelberg (2014)

11. Grossi, R., Gupta, A., Vitter, J.S.: High-order entropy-compressed text indexes.

In: Proceedings of the 14th SODA, pp. 841–850 (2003)

12. Jacobson, G.: Space-eﬃcient static trees and graphs. In: Proceedings of the 30th

FOCS, pp. 549–554 (1989)

13. Jansson, J., Sadakane, K., Sung, W.-K.: CRAM: Compressed Random Access
Memory. In: Czumaj, A., Mehlhorn, K., Pitts, A., Wattenhofer, R. (eds.) ICALP
2012, Part I. LNCS, vol. 7391, pp. 510–521. Springer, Heidelberg (2012)

14. Joannou, S., Raman, R.: Dynamizing succinct tree representations. In: Klasing, R.

(ed.) SEA 2012. LNCS, vol. 7276, pp. 224–235. Springer, Heidelberg (2012)

15. K¨arkk¨ainen, J., Puglisi, S.J.: Fixed block compression boosting in FM-indexes. In:
Grossi, R., Sebastiani, F., Silvestri, F. (eds.) SPIRE 2011. LNCS, vol. 7024, pp.
174–184. Springer, Heidelberg (2011)

16. Klitzke, P., Nicholson, P.K.: A general framework for dynamic succinct and compressed 
data structures. In: Proceedings of the 18th ALENEX, pp. 160–173 (2016)
17. M¨akinen, V., Navarro, G.: Dynamic entropy-compressed sequences and full-text

indexes. ACM Trans. Algorithms 4(3), 32–38 (2008)

18. Munro, J.I.: An implicit data structure supporting insertion, deletion, and search

in o(log2 n) time. J. Comput. Syst. Sci. 33(1), 66–74 (1986)

19. Navarro, G.: Wavelet trees for all. J. Discrete Algorithms 25, 2–20 (2014)
20. Navarro, G., Nekrich, Y.: Optimal dynamic sequence representations. SIAM J.

Comput. 43(5), 1781–1806 (2014)

21. Navarro, G., Sadakane, K.: Fully-Functional static and dynamic succinct trees.

ACM Trans. Algorithms 10(3), 16 (2014)

22. Okanohara, D.: Dynamic succinct vector library. https://code.google.com/archive/

p/ds-vector/. Accessed 30 Jan 2016

Practical Dynamic Entropy-Compressed Bitvectors with Applications

117

23. Raman, R., Raman, V., Rao, S.S.: Succinct dynamic data structures. In: Dehne,
F., Sack, J.-R., Tamassia, R. (eds.) WADS 2001. LNCS, vol. 2125, p. 426. Springer,
Heidelberg (2001)

24. Raman, R., Raman, V., Satti, S.R.: Succinct indexable dictionaries with applications 
to encoding k-ary trees, preﬁx sums and multisets. ACM Trans. Algorithms
3(4), 43 (2007)

25. Salson, M.: Dynamic fm-index library. http://dfmi.sourceforge.net/. Accessed 30

Jan 2016

26. Smirnov, V.: Memoria

library.

https://bitbucket.org/vsmirnov/memoria/.

Accessed 30 Jan 2016

