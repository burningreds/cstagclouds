Engineering with Computers (2006) 22: 111–119
DOI 10.1007/s00366-006-0013-2

O R I GI N A L A R T IC L E

Maria-Cecilia Rivara Æ Carlo Calderon
Andriy Fedorov Æ Nikos Chrisochoides
Parallel decoupled terminal-edge bisection method
for 3D mesh generation

Received: 22 September 2004 / Accepted: 8 November 2005 / Published online: 3 May 2006
Ó Springer-Verlag London Limited 2006

Abstract We present a practical and stable algorithm for
the parallel reﬁnement of
tetrahedral meshes. The
algorithm is based on the reﬁnement of terminal-edges
and associated terminal stars. A terminal-edge is a special 
edge in the mesh which is the longest edge of every
element that shares such an edge, while the elements that
share a terminal-edge form a terminal star. We prove
that the algorithm is inherently decoupled and thus
scalable. Our experimental data show that we have a
stable implementation able to deal with hundreds of
millions of tetrahedra and whose speed is in between one
and two order of magnitude higher from the method and
implementation we presented (Rivara et al., Proceedings
13th international meshing roundtable, 2004).

Keywords Parallel mesh generation Æ 3-dimensional
(3D) Æ Longest-edge Æ Terminal-edge Æ Lepp

1 Introduction

Parallel mesh generation methods should satisfy the
following four practical criteria: (1) stability in order to
guarantee termination and good quality elements for

Maria-Cecilia Rivara and Carlo Calderon’s work was partially
supported by Fondecyt 1040713.

Andriy Fedorov’s work is supported in part by ITR #ACI0085969,
 and NGS #ANI-0203974.

Nikos Chrisochoides’s work is supported in part by NSF Career
Award #CCR-0049086, ITR #ACI-0085969, NGS #ANI-0203974,
and ITR #CNS-0312980.
M.-C. Rivara (&) Æ C. Calderon
Universidad de Chile, Santiago, Chile
E-mail: mcrivara@dcc.uchile.cl
E-mail: carcalde@dcc.uchile.cl

A. Fedorov Æ N. Chrisochoides
College of William and Mary, Williamsburg, VA, USA
E-mail: fedorov@cs.wm.edu
E-mail: nikos@cs.wm.edu

(2)

simple domain
parallel ﬁnite element methods,
decomposition in order to reduce unnecessary pre-processing 
overheads, (3) code re-use in order to beneﬁt
from fully functional, highly optimized, and ﬁne tuned
sequential codes, and (4) scalability. Our parallel mesh
generation algorithm satisﬁes all four requirements.

Parallel mesh generation procedures in general overdecompose 
the original mesh generation problem into N
smaller subproblems which are meshed concurrently
using P (> N) processors [9]. The subproblems can be
formulated to be either tightly [23, 3] or partially coupled 
[2, 34, 18] or even decoupled [19, 10, 4]. The coupling 
of the subproblems determines the intensity of the
communication and the degree of dependency (or syn-
chronization) between the subproblems.

The parallel mesh generation and reﬁnement method
we present in this paper is a decoupled method (i.e.,
requires zero communication and synchronization between 
the subproblems). This is an improved version of
the method we presented in [33] which again required
zero communication between the subproblems, but it
used a central processor for synchronization in order to
maintain the conformity of the distributed mesh. In this
paper we eliminate the synchronization, i.e., there is no
need for a central processor and we prove the correctness
of our algorithm.

complexity of

There are two classes of parallel tetrahedral mesh
generation methods: (1) Delaunay and (2) non-Delaunay.
 There are few parallel implementations [11, 26,
for 3D Delaunay mesh generation due to the
23]
inherent
the Delaunay algorithms.
Moreover, there are (to the best of our knowledge)
only two parallel stable Delaunay mesh generation
algorithms and software [11, 23]. The parallel Delaunay
mesh generator in [11] is for general 3D geometries. It
starts by sequentially meshing the external surfaces of
the geometry and pre-computes domain separators
whose facets are Delaunay-admissible (i.e., the precomputed 
interface faces of the separators will appear
in the ﬁnal Delaunay mesh). The separators decompose
the continuous domain into subdomains which are

112

meshed in parallel using a sequential Delaunay mesh
generation method on each subdomain.

The algorithm in [23] works only for polyhedral
geometries. It maintains the stability of the mesher by
simultaneously partitioning and reﬁning the interface
surfaces and volume of the subdomains [7]—a reﬁnement 
due to a point insertion might extend across subproblem 
(or subdomain) boundaries (or interfaces). The
extension of a cavity beyond subdomain interfaces is a
source of irregular and intensive communication with
variable and unpredictable patterns. Although the
method in [23] can tolerate up to 90% of the commu-
nication—by concurrently reﬁning other regions of the
subdomain while it waits for remote data to arrive—its
scalability is of the order of O (log P), where P is the
number of processors. Unfortunately, the concurrent
reﬁnement can lead to a non-conforming and/or nonDelaunay 
mesh [23]. These problems are solved at the
cost of setbacks which require algorithm/code re-structuring 
[8, 3] or at the cost of mesh re-triangulation [36].
On the other hand, longest-edge bisection algorithms,
introduced by Rivara [27, 28, 31] are much simpler and
easier to implement on both sequential
[20–22] and
parallel [16, 25, 37] platforms. The algorithms are based
on the bisection of triangles/tetrahedra by its longestedge 
as follows: in 2D this is performed by adding an
edge deﬁned by the longest-edge midpoint and its
opposite vertex, while in 3D the tetrahedron is bisected
by adding a triangle deﬁned by the longest-edge midpoint 
and its two opposite vertices.

Jones and Plassman in [16] have proposed a 2D,
four-triangles parallel algorithm for the reﬁnement/
dereﬁnement of triangulations. In order to avoid synchronization,
 the algorithm uses a Monte Carlo rule to
determine a sequence of independent sets of triangles
which are reﬁned in parallel. In order to minimize
communication costs, a mesh partitioning algorithm
based on an imbalanced recursive bisection strategy is
also used. Castan˜ os and Savage in [25] have parallelized
the non-conforming longest edge bisection algorithm
both in 2 and 3D. In this case the reﬁnement propagation
implies the creation of sequences of non-conforming
edges that can cross several submeshes involving several
processors. This also means the creation of non-conforming
interface edges which is particularly complex to deal with
in 3D. To perform this task each processor Pi iterates
between a no-communication phase (where reﬁnement
propagation between processors is delayed) and an
interprocessor communication phase. Diﬀerent processors
can be in diﬀerent phases during the reﬁnement process,
their termination is coordinated by a central processor
P0. Duplicated vertices can be created at the non-conforming
interface edges. A remote cross reference of newly created
interface vertices during the interprocessor communication 
phase along with the concept of nested elements [24]
guarantees the assignment of the same logical name for
these vertices. The load balancing problem is addressed
by using mesh repartitioning based on an incremental
partitioning heuristic.

The method we present here, contrary to the algorithm 
in [25], completely avoids the management of nonconforming 
edges both in the interior of the submeshes
and in the inter-subdomain interface. This property
completely eliminates the communication and synchronization 
between the subdomains for global mesh
reﬁnement, while limited communication is required for
reﬁnement guided by an edge-size density function.

2 Background

In 2D the longest-edge bisection algorithm essentially
guarantees the construction of reﬁned, nested and
unstructured conforming triangulations
(where the
intersection of pairs of neighbor triangles is either a
common vertex, or a common edge) of analogous quality
as the input triangulation. More speciﬁcally the repetitive
use of the algorithms produce triangulations such that: (1)
The smallest angle at of any triangle t obtained
throughout this process, satisﬁes that at ‡ a0/2, where a0 is
the smallest angle of the initial triangulation. (2) A ﬁnite
number of similarly distinct triangles is generated in the
process. (3) For any conforming triangulation the percentage 
of bad quality triangles diminishes as the reﬁnement 
proceeds. Even when analogous properties have not
been fully proved in 3D yet, both empirical evidence [21,
31] and mathematical results on the ﬁnite number of
similar tetrahedra generated over a set of tetrahedra [12]
allow to conjecture that a lower bound on the tetrahedra
quality can be also stated in the 3D setting.

The serial pure longest-edge bisection algorithm [27],
works as follows: for any target triangle t to be reﬁned
both the longest-edge bisection of t and the longest-edge
bisection of some longest-edge neighbors are performed
in order to produce a conforming triangulation. This
task usually involves the management of sequences of
intermediate non-conforming points throughout
the
process.

Alternative longest-edge based algorithms (i.e., the
four-triangles longest-edge algorithm), which use a ﬁxed
number of partition patterns have been also proposed in
[27, 20]. The four-triangles algorithms maintain only one
non-conforming vertex as the reﬁnement propagates
toward larger triangles; however, its generalization to
3D is rather cumbersome.

the use of

More recently,

two new and related
mathematical concepts—the longest-edge propagation
path (Lepp) of a triangle t and its associated terminal
edge, have allowed the development of improved Lepp
based algorithms for the longest edge reﬁnement/dereﬁnement 
of triangulations both in 2 and 3D [29, 30, 32]
which completely avoids the management of non-conforming 
meshes. Moreover, the application of Lepp/
terminal edge concepts to the Delaunay context have
also allowed the development of algorithms for the
quality triangulation of PSLG geometries [30], for the
improvement of obtuse triangulations [13, 14], and for
approximate quality triangulation [35].

113

(a) Lepp (t0) contains every tetrahedron t that shares the
longest edge of t0 with t, and such that longest edge
(t) > longest edge (t0).
(b) For any tetrahedron t¢

in Lepp(t0), the submesh
Lepp(t0) also contains every tetrahedron t not contained 
yet in Lepp (t0), such that t shares the longest
edge of t¢ and where longest edge (t) > longest edge (t¢).

Proposition 1 In 3D, for any tetrahedron t in M, Lepp (t)
has a ﬁnite, variable number of associated terminal-edges.

Proof The proof follows from the fact that every tetrahedron 
t in any Lepp (t0) has a ﬁnite, non-ﬁxed number
of neighbor tetrahedra sharing the longest edge of t. So
in the general case more than one of these tetrahedra has
longest edge greater than the longest edge of t, which
implies that the searching task involved in Deﬁnition 4 is
multidirectional, and stops when a ﬁnite number of
terminal edges (which are local
longest edges in the
mesh), are found in M.

Note that, the Lepp(t0) corresponds to a submesh of
an associated Lepp polyhedron, which captures the local
point distribution around t0 in the direction of the longest 
edge.

Proposition 2 Every terminal-edge l associated to any
Lepp(t0) is the longest edge between all the edges involved
in the chain of tetrahedra traversed to reach l in the Lepp
searching path.

Proof The proof relies in the Lepp deﬁnition which involves 
ﬁnding a set of increasing longest edge tetrahedra
until a terminal edge is found. So, every terminal edge is
a last longest edge in a sequence of increasing longest
edge tetrahedra.

A high level 3D reﬁnement algorithm for the local

reﬁnement of a tetrahedral mesh follows:

In this paper we focus on the parallelization of a
global terminal edge reﬁnement algorithm which makes
implicit use of the Lepp concept. This serial algorithm
performs the repetitive reﬁnement of every terminal edge
greater than a given tolerance on the size of the terminal
edges as follows:

Either for improving or reﬁning a mesh, the Lepp
based algorithms use a terminal-edge point selection criterion 
as follows. For any target element to be improved
or reﬁned, a Lepp searching method is used for ﬁnding the
midpoint of an associated terminal-edge which is selected
for point insertion. Each terminal-edge is a special edge in
the mesh which is the common longest edge of all the
elements (triangles or tetrahedra) that share this edge in
the mesh. Once the point is selected, this is inserted in the
mesh. In the case of the terminal-edge reﬁnement algorithm,
 this is done by longest-edge bisection of all the
elements that share the terminal-edge, which is a very
local operation that simpliﬁes both the algorithm implementation 
and its parallelization. The process is repeated
until the target element is destroyed in the mesh.

In 2D a Lepp based algorithm for the quality
reﬁnement of any triangulation was introduced in [29],
where the reﬁnement of a target triangle t0 essentially
means the repetitive longest-edge bisection of pairs of
terminal triangles sharing the terminal-edge associated
with the current Lepp(t0), until the triangle t0 itself is
reﬁned. Lepp (t0) is deﬁned as the longest edge propagation 
path associated to t0 and corresponds to the
sequence of increasing neighbor longest edge triangles
that ﬁnishes when a terminal-edge is found in the
mesh. For an illustration see Fig. 1, where Lepp(t0) =
{t0,
t3} over the triangulation (a), and the
associated terminal edge is the edge shared by triangles
t2, t3. Triangulations (b) and (c) respectively illustrate
the ﬁrst and second reﬁnement steps, while triangulation 
(d) corresponds to the ﬁnal mesh when the Lepp
Bisection procedure is applied to t0. Note that the new
vertices were enumerated in the order they were created.
 The generalization of this algorithm to 3D is
formulated in the next section.

t1,

t2,

2.1 Serial 3D Lepp and terminal-edge algorithms

As discussed in [29, 32], the 3D algorithm implies a
multi-directional Lepp searching task, involving a set of
terminal-edges. In this case each terminal-edge in the
mesh is the common longest-edge of every tetrahedron
that shares such an edge; and the reﬁnement operation
involves a terminal-star (the set of tetrahedra that share
a terminal-edge) reﬁnement. It is worth noting that the
reﬁnement is conﬁned in the interior of the terminalstar.


Deﬁnition 1 Any edge E in a valid tetrahedral mesh M is
a terminal-edge if E is the longest edge of every tetrahedron 
that shares E. In addition the set of tetrahedra
that share E deﬁne a terminal-star in 3D, while that
every tetrahedron in a terminal star is called a terminal
tetrahedron.

Deﬁnition 2 For any tetrahedron t0 in M, the Lepp (t0) is
a 3D submesh (a set of contiguous tetrahedra) recursively 
deﬁned as follows:

114

Fig. 1 Lepp reﬁnement of target triangle t0 a initial mesh; b ﬁrst
reﬁnement step; c second reﬁnement step; d ﬁnal mesh where
triangle t0 was reﬁned

The following theorem assures that the ﬁnal mesh has

every edge less than b(M):

Theorem 1 The use of the Terminal-edge Reﬁnement
Algorithm with tolerance parameter b(M) produces a
reﬁned mesh MF with every edge in MF less than or equal
to b(M).

Proof The existence of a (non-terminal) edge E in MF
with length(E) > b(M) would imply the existence of at
least one tetrahedron t with longest edge greater than
b(M), which in turn implies the existence of Lepp(t) with
at least one terminal-edge greater than b(M), which
contradicts the deﬁnition of MF.

3 Parallel terminal-edge bisection algorithm

In this section we consider a simple parallel terminaledge 
(PTE) bisection algorithm which globally reﬁnes
any tetrahedral mesh as follows: for each submesh, the
parallel reﬁnement of terminal-edges is performed until
their size is less than or equal to a global user deﬁned
tolerance b(M). A high level description of the method
follows:

Parallel Terminal Edge (PTE) Reﬁnement Algorithm

1. Read Input {a mesh M and edge tolerance b(M)},
2. Partition the mesh M in submeshes Mi, i = 1 ... N,

3. Distribute the submeshes Mi among the processors,
4. Perform submesh PTE reﬁnement over each Mi.
5. Output

Each submesh Mi (Si, Vi) is deﬁned by its interface
surface Si and corresponding set of tetrahedra Vi. The
distribution of the submeshes Mi to processors takes
place by traditional ab initio data mapping methods [6].
Each submesh assigns a unique identiﬁer (ID) to each
new vertex created in the submesh Mi. Based on the
expected size of the mesh, a 32 or 64 bit word is used to
store each ID. The mesh reﬁnement task, based on edge
reﬁnement in each submesh Mi is performed as follows:

It is worth noting that step 2 can be accomplished
without any communication since for each interface
terminal-edge E greater than b(M), every processor that
shares E knows that E must be bisected (in its associated
submesh).

At termination a new ﬁner mesh M¢ (S¢, V¢) is constructed 
by simply computing the union of Si and Vi,
i = 1, ..., N. According to Theorem 1, the resulting
mesh will be a conforming mesh with every edge less
than b(M). The interface vertices, and edges are replicated 
among the subdomains that share them. Consequently 
a simple post-processing step allows to compute
a global vertex ID if this is required, as well as adjacency
information for the ﬁnite element application.

3.1 Theoretical framework

Even when there not exists yet a theoretical bound on
the geometrical quality of the tetrahedra by longest edge
bisection in 3D, two comments are in order: (1) The 3D
algorithm behaves in practice as the 2D algorithm does,
in the sense that, at the ﬁrst global reﬁnement steps, the
mesh quality can show some quality decreasement (by
approximately 1/3) for a small percentage of elements,
while the mesh quality distribution quickly stabilizes
according to a Gaussian distribution. After this point
the reﬁnement algorithm tends to improve the mesh
distribution. (2) In [12]
it has been proved that the
symmetric longest edge bisection of a regular tetrahedron 
produces a ﬁnite number of similarly diﬀerent
tetrahedra, the ﬁst step to state a theoretical bound on
the mesh quality.

By assuming the conjecture that the algorithm do not
signiﬁcantly deteriorate the elements, which holds in
practice, termination results can be stated. The termination 
of the PTE algorithm is based on the Proposition
2 and the Lemmas bellow while. Theorem 2 proves that
the PTE algorithm is decoupled.

Lemma 1 Let E be any interior terminal edge in Mi with
length(E) > b(M) and for which there exists at least one
tetrahedron t in Mi such that E belongs to Lepp(t) and
b(M) < longest edge (t) < length(E). Then the processing 
of E in the step 1 of PTE algorithm implies the
successive processing of a sequence of new terminal edges
in the submesh Mi including the longest edge of t which
also becomes a terminal edge in the mesh. Furthermore,
both the reﬁnement of these terminal edges and their
associated terminal stars is performed in the same step 1.

Proof The existence of t implies that there exists a sequence 
of interior edges in Lepp(t) which need to be
traversed in order to reach the associated terminal edge
E in Mi. Thus in the same step 1 of PTE algorithm, and
in decreasing edge size order, each one of these edges
becomes a terminal edge in Mi greater than b(M) which
is reﬁned in the same step 1.

Lemma 2 Consider a set SB of interface terminal edges to
be processed in step 2 of the algorithm in submesh Mi. If
Mi has interior edges greater than b(M),
then the
reﬁnement of the terminal-edges of SB (in step 2) introduces 
a set SA of interior terminal edges greater than
(b(M) in Mi. Furthermore, the processing of each E in SA
in the next step 1 introduces a sequence of interior terminal 
edges greater than b(M) which are also processed
in the same step 1.

Proof The reﬁnement of each interface terminal edge
Einterface introduces a set Stet of tetrahedra sharing a
bisected edge. For each t in Stet with interior longest
edge E greater than b(M) two cases arise: (1) If E is an
interior terminal edge in SA, then Lemma 3.1 applies and
the processing of E in step 1 can introduce a sequence of
interior terminal edges which are processed in the same
step 1; (2) If E is not a terminal edge then E becomes a
terminal edge by processing another terminal edge in SA
in the same step 1. To prove this consider that submesh
Mi was only modiﬁed by reﬁnement of the terminal edge
Einterface which produced the set Stet and tetrahedron t
with interior longest edge E. By deﬁnition of step 1,
Lepp (t) can only ﬁnish in an interior terminal edge due
to the reﬁnement of Einterface which according to Lemma
1 implies that E will become a reﬁned terminal edge in
the same step 1. Otherwise, Lepp(t) over the current
mesh Mi only has interface terminal edges. So the processing 
of any of this interface terminal edges will reach
E (which will become a reﬁned terminal edge) in the
same step 1 of the PTE algorithm.

Theorem 2 In the PTE algorithm the use of a global edgesize 
tolerance b(M) eliminates interprocessor communication.

Proof Consider any interface edge L > b(M). Then there
are three cases:

Case 1: If L is not a terminal edge in any submesh Mi
that contains L,
then, according to Theorem 1,
throughout the reﬁnement process, L will become a
terminal edge in one of the interface surfaces Si that

115

contains L. Since every existing interface terminal edge
greater than b(M) will be reﬁned (according to Lemmas
3.1 and 3.2), then L will be handled by the Case 3.
Case 2: An interface edge L can be a terminal edge in a
submesh Mi but not a terminal edge in at least one
adjacent submesh Mj. However, since the edge L is
greater than b(M), by performing interior reﬁnement in
Mj, according to Lemmas 3.1 and 3.2, the edge L will
become a terminal edge in Mj, too, which will be handled 
by case 3. The submesh Mj is reﬁned independently
of the submesh Mi and at the end we will have a conforming 
ﬁnal mesh, here all edges are less than or equal
to b(M).
Case 3: An interface edge L is a terminal edge in the
global mesh, i.e., [i=1
N Mi. This implies that L is in turn a
terminal edge in each interface Si of the submesh Mi that
contains it. So after L is independently reﬁned in each
submesh that contain L, we will get a conforming reﬁned
mesh since all the submeshes will bisect L by its midpoint.


Remark Note that for the same b(M) value, both the TE
and PTE algorithms produce the same ﬁnal nested meshes 
whenever that, for every tetrahedra t, its longest
edge is unique. In the case that there exists tetrahedra
which do not have unique longest edge, the longest edge
selection can be made unique by deﬁning the tetrahedra
in a consistent, oriented way. Consequently from a
theoretical point of view, the method is fully stable and
deterministic in the sense that, with adequate tetrahedron 
oriented convention and unique longest edge
selection, for a ﬁxed quality measure and a ﬁxed mesh
reﬁnement criterion (b(M) value), both the sequentially
generated mesh and the parallel one, are identical reﬁned
meshes.

4 Future extensions

The PTE algorithm can be generalized for a variable
edge size (density) function as follows:

In the phase (1) of the VPTE algorithm, each interface 
edge E has a unique b(E) associated value, which
implies that the terminal-edge reﬁnement task (noncommunication 
phase) is a direct generalization of the
PTE algorithm. It is worth noting however that, once

116

ﬁnished phase (1), the reﬁned mesh can have a set S of
non-terminal-edges L greater than b(L). In the case we
need all edges L in the ﬁnal mesh to be greater than b(L),
a communication phase (2) to reﬁne the edges of SL will
be required. This will reﬁne the edges of SL and some
neighbor edges by making explicit use of the Lepp
concept. To perform this task a limited interprocessor
communication, similar to that required in adaptive
reﬁnement but for a small number of edges, is needed.
In the case that an adaptive reﬁnement is needed
based on an error estimator of a ﬁnite element solver, a
limited communication between adjacent submeshes is
required to communicate that an edge L needs to be
reﬁned by all submeshes that share L (no message reply
is required and thus the communication phase is asyn-
chronous). This is subject of an ongoing research in the
University of Chile.

5 Performance evaluation

The experimental study was performed in the Sciclone
cluster in the College of William and Mary which consists 
of several diﬀerent heterogeneous subclusters. We
have used Whirlwind subcluster which consists of 64
single-cpu Sun Fire V120 nodes
(650 MHz, 1 GB
RAM). Also, we have used two geometric models with
diﬀerent needs for reﬁnement: (1) a real human artery
bifurcation model (Fig. 2,
left) and (2) a simpliﬁed
model for a human brain (Fig. 2, right). The initial mesh
of 92K tets for brain human model was decomposed
into 503 subdomains while an initial mesh of 91K tets
for the artery bifurcation model was decomposed into
504 subdomains. We used static cyclic assignment of
subdomains to processors (i.e., processorPid of a subdomainSid 
= subdomainSid % number of processors).

All the timing data reported in this paper use an
optimized C++ implementation of the PTE algorithm,
which in turn uses RemGO3D code which is a C++
serial implementation of the Terminal Edge Reﬁnement
Algorithm developed at the University of Chile, while
the timing data reported in [33] corresponds to a Java
prototype which used a coordinator processor based
implementation. The stopping criterion is a predeﬁned
bound (size of minimum edge) b for the terminal edges.

The stability of the PTE algorithm like any parallel
longest-edge bisection algorithm is proved in [24, 25] and
thus we do not present any data regarding this issue.
Moreover, empirical studies on the quality of longestedge 
subdivision for 3D meshes are addressed in detail in
[20, 21].

Table 1 shows the execution time which is equal to
the time it takes to process all of the subdomains assigned 
to a number of speciﬁcs processors. We report the
maximum processing time (Tmax) of all processors, i.e.,
the time of the processor that dominates the parallel
execution time. The speed of the C++ code varies from
about 10.5K to 10.9K tets per second while the speed of
the same algorithm using Java is about 500–800 tets per
second on the same cluster. An improvement of more
than an order of magnitude.
Table 2 shows the load imbalance measured in terms
of the (Tmax   Tmin), where Tmin is the execution time of
the processor that completes the mesh generation of its
subdomains ﬁrst and waits for the termination of the
rest of the processors. This table shows that work–load
imbalance is a very serious problem. The overdecomposition 
with the cyclic (ab initio) assignment can not
handle such sever imbalances. This suggests that the
dynamic load balancing problem is important for mesh
generation and reﬁnement,
for the PTE algorithm.
Moreover,
in [33] we have seen that the work–load
balancing problem can be exaggerated due to heterogeneity 
of the clusters. The dynamic load balancing of the
PTE method is out of the scope of this paper. Currently
we are working to address this problem; we will use a
parallel runtime system (PREMA [1]) which is developed 
at W&M for this purpose and the new C++
implementation of the PTE algorithm which is developed 
at the University of Chile.

Figure 3 shows the execution time of all processors
for two conﬁgurations (48 and 64 processors). These
data indicate that many processors are out of balance,
i.e., the work load imbalance is more serious than the
fact that the (Tmax   Tmin) is very high. This explain the
lack of scalability in the data of Table 1 despite the fact
that there is no communication and global synchronization 
in PTE method and its new implementation.
However, our data in [5] suggest that after dynamic
load balancing the PTE method will scale well like the

Fig. 2 Surface of the tetrahedra
mesh for an artery bifurcation
model and simpliﬁed model of a
human brain generated from
MRI images [15]

Table 1 Time (s) for
bifurcation example

Mesh size

Processors

1

2

4

8

16

32

3M
6M
15M
30M
72M
242M

284.1
650.4
1,889.0
3,816.8
9,483.4
22,172.0

143.6
330.5
955.2
1,929.1
4,817.9
11,228.1

79.8
184.2
531.6
1,060.5
2,703.4
6,212.6

46.0
107.7
302.8
611.8
1,561.7
3,595.8

28.1
66.0
184.4
369.9
959.7
2,194.9

17.3
40.3
114.4
229.5
588.2
1,352.0

Table 2 Imbalance measured as
the Tmax   Tmin (s)

Mesh size

Processors

3M
6M
15M
30M
72M
242M

1

0.0
0.0
0.0
0.0
0.0
0.0

2

4

8

16

32

3.0
10.6
21.4
41.4
152.4
284.2

16.0
37.9
108.0
191.8
589.0
1,197.1

21.1
50.7
135.0
270.0
727.9
1,633.2

17.0
41.5
109.7
220.3
594.5
1,335.9

13.0
30.4
84.5
169.5
437.7
1,008.9

117

64

12.4
27.6
81.8
156.2
404.7
921.3

64

11.4
25.5
74.4
141.9
369.5
838.9

Fig. 3 Execution time (in s) of
48 (light color bars) and 64
(dark color bars) processors for
generating two tetrahedral
meshes with 72M and 242M
elements for the artery
bifurcation model, respectively

Table 3 Time (s) for the human
brain model

Mesh size

Processors

1

2

4

8

16

32

64

21M
171M

2,650.40
15,198.30

1,425.10
8,169.70

766.70
4,399.34

425.00
2,437.60

254.50
1,460.90

131.80
758.390

87.60
503.00

parallel advancing front which is decoupled method with
zero communication and synchronization.

The data from Tables 3 and 4 conﬁrm our earlier
conclusions on a diﬀerent geometry (the human brain

model). These data indicate that the behavior of PTE
method in terms of load imbalance is geometry independent,
 since we have observed the same behavior in
diﬀerent geometries, too.

Table 4 Imbalance measured as the Tmax   Tmin (s) for the human
brain model

6 Conclusions

Mesh size Processors

1

2

4

8

16

32

64

21M
171M

0.0
0.0

199.8
1,141.1

174.9
1,005.2

148.1
852.4

164.3
944.4

93.1
542.5

76.6
439.9

We have presented a parallel 3D Terminal-Edge mesh
generation and reﬁnement method which is stable with
zero communication and synchronization. The new decoupled 
algorithm and its implementation lead to one to
two orders of magnitude performance improvements
compared to an earlier implementation [33] of the same

118

Fig. 4 Number of tetrahedra of
48 (light color bars) and 64
(dark color bars) processors for
artery bifurcation model for the
input mesh

Fig. 5 Number of tetrahedra of
48 (light color bars) and 64
(dark color bars) processors for
artery bifurcation model for the
ﬁnal mesh (242M tetrahedra)

algorithm. The PTE method relies on 100% code re-use.
We use a single code which can run both on single
processor and many processors due to the decoupling
nature of the PTE method. Usually with code re-use we
take advantage of existing codes that target only single
CPU computers. In this paper we developed a new
method and its implementation that can be used both
for single and multiple CPU computers. This will allow
us to optimize a single code using well understood and
familiar (sequential) programming methods and at the
same time be able to generate faster larger meshes using
multiple processors. The PTE algorithm and its current
implementation are scalable (see Theorem 2). However,
our performance data indicate the contrary due to processor 
work–load imbalances. This is due both to that
PTE is a deterministic method, and to the fact that the
overpartition of the initial mesh produced small submeshes 
with diﬀerent size elements. To illustrate this
imbalance behavior see Figs. 4 and 5 that respectively
show the number of tetrahedra for two conﬁgurations
(48 and 64 processors) for the input and ﬁnal meshes for
the artery bifurcation problem.

Currently we are working in two fronts to address the
scalability and overall performance (i.e., speed) of the
PTE software by: (1) improving the performance of the
Terminal-Edge method by further optimizing the new

C++ implementation of the PTE method and (2)
improving the work–load of processors using dynamic
load balancing methods [1].

In the future we plan to use the new C++ implementation 
of the PTE algorithm within MRTS [17] in
order to implement a parallel out-of-core terminal-edge
mesh generation software capable to generate hundreds
of millions of elements on relative small CoWs.

Acknowledgments The C++ code RemGO3D was developed at
the University of Chile under grant Fondecyt 1040713. The parallel
results were obtained by using computational facilities at the College 
of William and Mary which were enabled by grants from Sun
Microsystems, the National Science Foundation, and Virginia’s
Commonwealth Technology Research Fund. We thank the referees
whose comments helped to improve the paper and Daniel Pizarro
who wrote the previous Java implementation.

References

1. Barker K, Chernikov A (2004) Nikos Chrisochoides, and
Keshav Pingali. A load balancing framework for adaptive and
asynchronous applications. Trans Parallel Distrib Syst 15(2):
183–192

2. Blelloch G, Hardwick J, Miller G, Talmor D (1999) Design and
implementation of a practical parallel delaunay algorithm.
Algorithmica 24:243–269

3. Nave D, Chrisochoides N, Chew LP (2004) Guaranteed-quality
parallel delaunay reﬁnement for restricted polyhedral domains.
Comput Geom: Theor Appl 28(2–3):191–215

4. Chew LP, Chrisochoides N, Sukup F (1997) Parallel constrained
Delaunay meshing. In: ASME/ASCE/SES summer meeting,
special symposium on trends in unstructured mesh generation,
pp 89–96, Northwestern University, Evanston, IL, 1997

5. Chrisochoides N (2006) Numerical solution of partial diﬀerential 
equations on parallel computers, volume in print.
Chapter A survey of parallel mesh generation methods.
Springer, Berlin Heidelberg New York

6. Chrisochoides N, Houstis E, Rice J (1994) Mapping algorithms
and software environment for data parallel PDE iterative
solvers. J Parallel Distrib Comput 21(1):75–95

7. Chrisochoides N, Nave D (2000) Simultaneous mesh generation 
and partitioning. Math Comput Simulation 54(4–5):321–
339

8. Chrisochoides N, Nave D (2003) Parallel Delaunay mesh generation 
kernel. Int J Numer Meth Eng 58:161–176

9. Chrisochoides NP (1996) Multithreaded model for load balancing 
parallel adaptive computations. Appl Numer Math 6:1–
17

10. de Cougny H, Shephard M (1999) Parallel volume meshing
using face removals and hierarchical repartitioning. Comp
Meth Appl Mech Eng

11. Galtier J, George PL (1997) Prepartitioning as a way to mesh
subdomains in parallel. In: Special symposium on trends in
unstructured mesh generation. ASME/ASCE/SES

12. Gutierrez F (2003) The longest edge bisection of regular tetrahedron.
 In: Personal communication

13. Hitschfeld N, Rivara MC (2002) Automatic construction of
non-obtuse boundary and/or interface delaunay triangulations
for control volume methods. Int J Numer Meth Eng 55:803–
816

14. Hitschfeld N, Villablanca L, Krause J, Rivara MC (2003)
Improving the quality of meshes for the simulation of semiconductor 
devices using lepp-based algorithms. Int J Numer
Meth Eng 58:333–347

15. Ito Y (2004) Advance front mesh generator. Unpbulished

Software, March

16. Jones MT, Plassmann PE (1994) Parallel algorithms for the
adaptive reﬁnement and partitioning of unstructured meshes.
In: Proceedings of the scalable high-performance computing
conference

17. Kot A, Chrisochoides N (2004) ‘‘green’’ multi-layered ‘‘smart’’

memory management system. Int Sci J Comput 2(3):91–97

18. Linardakis L, Chrisochoides N (2006) Delaunay decoupling
method for parallel guaranteed quality planar mesh generation.
SISC 27(4): 1394–1423

19. Lohner R, Cebral J (1999) Parallel advancing front grid generation.
 In: International meshing roundtable. Sandia National
Labs

20. Rivara MC (1984) Design and data structure of fully adaptive
multigrid, ﬁnite element software. ACM Trans Math Softw
10:242–264

119

21. Muthukrishnan SN, Shiakolos PS, Nambiar RV, Lawrence KL
(1995) Simple algorithm for adaptative reﬁnement of three-dimensionalﬁnite 
element tetrahedral meshes. AIAA J 33:928–
932

22. Nambiar N, Valera R, Lawrence KL, Morgan RB, Amil D
(1993) An algorithm for adaptive reﬁnement of triangular ﬁnite
element meshes. Int J Numer Meth Eng 36:499–509

23. Nave D, Chrisochoides N, Chew LP (2002) Guaranteed-quality
parallel Delaunay reﬁnement for restricted polyhedral domains.
In: Proceedings of the eighteenth annual symposium on computational 
geometry, pp 135–144

24. Jose´ G. Castan˜ os, John E. Savage (1997) The dynamic adaptation 
of parallel mesh-based computation. In: SIAM 7th
symposium on parallel and scientiﬁc computation 1997

25. Jose´ G. Castan˜ os, John E. Savage (1999) Pared: a framework
for the adaptive solution of pdes. In: 8th IEEE symposium on
high performance distributed computing

26. Okusanya T, Peraire J (1997) 3D parallel unstructured mesh
generation. In: Trends in unstructured mesh generation, pp
109–116

27. Rivara MC (1984) Algorithms for reﬁning triangular grids
suitable for adaptive and multigrid techniques. Int J Numer
Meth Eng 20:745–756

28. Rivara MC (1989) Selective reﬁnement/dereﬁnement algorithms 
for sequences of nested triangulations. Int J Numer
Meth Eng 28:2889–2906

29. Rivara MC (1997) New longest-edge algorithms for the
reﬁnement and/or improvement of unstructured triangulations.
Int J Numer Meth Eng 40:3313–3324

30. Rivara MC, Hitschfeld N, Simpson RB (2001) Terminal edges
Delaunay (small angle based) algorithm for the quality triangulation 
problem. Comput-Aided Des 33:263–277

31. Rivara MC, Levin C (1992) A 3D reﬁnement algorithm for
adaptive and multigrid techniques. Commun Appl Numer
Meth 8:281–290

32. Rivara MC, Palma M (1997) New lepp algorithms for quality
polygon and volume triangulation: implementation issues and
practical behavior. In: Cannan SA, Saigal AMD (eds) Trends
unstructured mesh generationi, Vol. 220, pp 1–8

33. Rivara MC, Pizarro D, Chrisochoides N (2004) Parallel
reﬁnement of tetrahedral meshes using terminal-edge bisection
algorithm. In: Proceedings 13th international meshing roundtable,
 Williamsburg, Virginia, pp 427–436

34. Said R, Weatherill N, Morgan K, Verhoeven N (1999) Distributed 
parallel delaunay mesh generation. Comp Meth Appl
Mech Eng 177:109–125

35. Simpson RB, Hitschfeld N, Rivara MC (2001) Approximate

quality mesh generation. Eng Comput 17:287–298

36. Spielman DA, Teng S-H, U¨ ngo¨ r A (2001) Parallel Delaunay
reﬁnement: algorithms and analyses. In: Proceedings of the
eleventh international meshing roundtable, pp 205–217

37. Williams R (1991) Adaptive parallel meshes with complex
geometry. Numerical grid generation in computational ﬂuid
dynamics and related ﬁelds

