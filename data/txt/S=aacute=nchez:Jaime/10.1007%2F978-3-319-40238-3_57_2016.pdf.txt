Development of an Audio-Haptic Virtual
Interface for Navigation of Large-Scale
Environments for People Who Are Blind

Lotﬁ B. Merabet1(&) and Jaime Sánchez2

1 Laboratory for Visual Neuroplasticity, Massachusetts Eye and Ear Inﬁrmary,

Harvard Medical School, Boston, MA, USA
lotﬁ_merabet@meei.harvard.edu

2 Department of Computer Science, University of Chile,

Blanco Encalada 2120, Santiago, Chile

jsanchez@dcc.uchile.cl

Abstract. We are investigating cognitive spatial mapping skills in people who
are blind through the use of virtual navigation and assessing the transference of
acquired spatial knowledge in large-scale, real-world navigation tasks. Training
is carried out with a user-centered, computer-based, navigation software platform 
called Haptic Audio Game Application (HAGA). This software was
developed to assist in orientation and mobility (O&M) training by introducing
blind users to a spatial layout of a large-scale environment through immersive
and simulation-based virtual navigation. As part of a self-directed, free exploration 
strategy, users interact with HAGA in order to navigate through a simulated 
indoor and outdoor virtual environment that represents an actual physical
space. Navigation is based on the use of iconic and spatialized auditory cues and
vibro-tactile feedback so as to build a cognitive spatial map of the surrounding
environment. The ability to transfer acquired spatial information is then assessed
in a series of physical navigation tasks carried out in the actual target environment 
explored virtually.

Keywords: Multimodal interfaces  Blind  Spatial cognition  Navigation

1 Introduction

The formal instruction of navigation skills in blind users (referred to as orientation and
mobility, or O&M training), is geared at developing strategies to assist with route
planning, updating information regarding one’s position, and reorienting to reestablish
travel as needed [1]. The resultant mental representation of the surrounding external
space is referred to as a cognitive spatial map [2–4]. For a blind individual, hearing and
touch remain the principal modalities for sourcing information regarding the surrounding 
environment. In terms of navigation, information captured through sound and
touch is essential for developing an awareness of relative orientation and distance, as
well as obstacle detection and avoidance [5–7]. The theoretical underpinnings related to
spatial abilities and navigation skills in blind individuals have been the subject of
intense debate (see [8]). For example, as profoundly blind individuals cannot access

© Springer International Publishing Switzerland 2016
M. Antona and C. Stephanidis (Eds.): UAHCI 2016, Part III, LNCS 9739, pp. 595–606, 2016.
DOI: 10.1007/978-3-319-40238-3_57

596

L.B. Merabet and J. Sánchez

spatial information through visual channels, it has been largely assumed that they
(especially children that are born blind) have cognitive difﬁculties in representing
spatial environments [9] and consequently, possess impaired navigation skills. However,
 more recent evidence has shown that blind individuals exhibit equal [10] and in
some cases, even superior [11] navigation performance when compared to sighted
individuals under experimental testing conditions.

Given these contradictory ﬁndings, one has to ask whether difﬁculties in developing
cognitive spatial constructs are due uniquely to the effect of visual deprivation itself
(and related developmental factors such as the timing and profoundness of vision loss)
or, do they also reﬂect an incomplete or inﬂexible acquisition of necessary spatial
information through other sensory channels? From an education and rehabilitation
standpoint, we argue that what is missing is a more efﬁcient way to access, manipulate,
and transfer acquired spatial information for the purposes of navigation; a gap that
could be potentially closed through the use of novel and immersive forms of assistive
technology.

1.1 Virtual Environments and Creating Spatial Mental Maps Through

Sound and Touch

Considerable interest has arisen regarding the educative and training potential of virtual
reality and computer based video games [12, 13]. Speciﬁcally, it has been suggested
that the open structure and self-directed discovery of information inherent in virtual
reality environments improves contextual
learning and the transfer of situational
knowledge and awareness (consider as a prototypical example, the success of ﬂight
simulators for pilot training) [14, 15]. Successfully leveraging these advantages in the
education and rehabilitation arenas would have immense appeal and could potentially
facilitate the learning of demanding tasks, and further promote the transfer of acquired
skills beyond the constraints of the training context itself [13, 16, 17]. Parallel to this
potential application, is the fact that the communications and entertainment industries
have driven many technical advances (both software and hardware) such as extreme
detailed graphics, highly realistic sounds, as well as tactile and motion feedback joystick 
controllers; all of which are aimed at generating a greater sense of environmental
“immersion” for the user. Based on this premise, we propose that virtual environments
can provide a blind individual the possibility to interact with complex contextual
information through the use of non-visual sensory modalities. Extended to the case of
navigation, a well-designed virtual environment could allow access to crucial spatial
information for the purposes of surveying and planning of routes, simulation of travel,
and the playing out of hypothetical navigation scenarios. By interacting with auditory
and tactile sensory cues that describe the spatial layout of an environment (for example,
heading information provided by text to speech (TTS), iconic and spatialized stereo
cues for identifying the spatial location of an object, or pulsatile vibrations felt in the
hands when an obstacle is hit), a blind user can learn the spatial layout of a complex
environment via “structural discovery”. Key to this user-centered approach, is the fact
that spatial information is acquired within context, and in a highly dynamic and
interactive manner that greatly engages the user so as to construct a cognitive spatial

Development of an Audio-Haptic Virtual Interface for Navigation

597

map of that area in an efﬁcient “off-line” manner. Once the spatial information is
acquired, the individual can then translate this knowledge for the purposes of enhanced
navigation skills once they arrive to the actual environment to be explored.

Previous work with blind individuals has shown that spatial information obtained
through novel computer-based approaches including audio [18–21] and tactile/
haptic [22–24] modalities leads to the generation of cognitive spatial maps that, in
turn, may be useful for the purposes of navigation. Furthermore, combined sensory
exploration (i.e. using touch and audio) may lead to an even greater capture of contextual 
information. Indeed, the perceptual advantage of overlapping multisensory
inputs is well established in the psychophysical literature. For example, overlapping
multisensory information can enhance object saliency by improving the likelihood of
detecting and identifying a sensory event and accelerating perceptual reaction times
(see [25] for review). By leveraging these principles, our investigative effort has gone
through an evolution of ideas with the ultimate goal of developing computer based
software for users who are blind and for the purposes of enhancing navigation-related
and spatial cognitive skills. One early approach was the development of “Audio-
Doom”. This was an auditory-based computer game centered on the popular
“ﬁrst-person shooter” game “Doom” (Id Software) and was developed to promote
computer interaction in blind children. Using simple iconic audio cues (for example,
door knocks and footsteps), a blind user can acquire contextual spatial information and
navigate through a predetermined labyrinth of corridors and obstacles. Following game
play, blind users (particularly children) were able to construct (using Lego® blocks) a
physical representation of the route they navigated with great accuracy [26] (see Fig. 1
A). This led to the intriguing possibility that blind users who interact with a virtual
world that represents a real indoor environment (e.g. a building) can not only create a
mental map of its spatial layout, but also transfer this knowledge for the purposes of
navigating in the actual target environment presented in the software. This lead to the
development of Audio-based Environment Simulator (AbES) (see Fig. 1B). We
completed a series of studies and found that blind individuals (both of early and late
onset) were able to use contextual and audio-based spatial cues to explore and accurately 
learn the spatial layout of an indoor environment for which they were previously
unfamiliar. Furthermore, the spatial information learned off-line could be transferred to
navigation tasks carried out within the real physical space represented in the virtual
environment [27–30].

Based on this proof of principle (and as a next step towards creating a truly viable
and robust assistive technology for the purpose of enhancing navigation skills in blind
users), we have created Haptic Audio Game Application (HAGA); a third generation
virtual environment simulator with the following design features: (1) greater immersive
sensory experience with full 3D sound and tactile/haptic feedback (allowing for more
robust acquisition and interaction with spatial contextual information), (2) rapid prototyping 
of any large scale environment desired (including indoor and outdoor), and
(3) wide deployment on a variety of technology platforms and operating systems
(including PC, Mac, and mobile Android, iOS). Finally, HAGA was designed to be
used by blind individuals with profound vision loss and could also be modiﬁed for use
in individuals with low vision (see Fig. 1C).

598

L.B. Merabet and J. Sánchez

Fig. 1.
(A) With AudioDoom, early blind children use simple iconic and spatialized sound cues
to navigate a complex labyrinth of corridors and obstacles (see background ﬁgure). The accuracy
of the cognitive spatial map resulting from exploration and game play is demonstrated by their
ability to construct (using Lego pieces) a physical representation of the target
labyrinth
(foreground ﬁgure). (B) With AbES, this approach was expanded to real-world indoor spaces
(e.g. corridors and rooms of a building). This allows a blind user to learn the spatial layout of an
unfamiliar building (background: building ﬂoor plan, foreground: rendering in AbES, inset: user
navigating in the target building). (C) Using the Unity game engine software platform, virtually
any desired environment (indoor and outdoor) can by quickly rendered (in this case, the
corresponding indoor ﬂoor plan of the building depicted in B was rendered in approximately 3 h;
background ﬁgure). Audio and tactile/haptic sensory information can be accessed using
peripheral interfaces (headphones and Xbox controller with rumble vibration; foreground ﬁgure)
to create a greater immersive experience during exploration. The Unity software platform was
used to create the HAGA software for this study.

1.2

Innovation and Current Gaps in Knowledge

Through the development and assessment of this software, we are attempting to address
gaps, barriers, and questions of fundamental importance related to the rehabilitation and
development of navigation skills in individuals who are blind. First, it is notable that
the ﬁeld of O&M (and blind rehabilitation in general) typically has not beneﬁted from
extensive and rigorous outcomes-based research. This includes systematic evaluations
to compare the effectiveness of various assistive technologies and teaching strategies.
Thus, the establishment of evidence-based best practices are clearly needed. Second,
with current advances in neuroscience and modern day brain imaging methodologies
(such as functional magnetic resonance imaging, or fMRI), we are in a position to
uncover the neurophysiological mechanisms and identiﬁcation of “neural biomarkers”
that are related to behavioral performance. Speciﬁcally, carrying out virtual navigation
studies in the MRI scanner environment can help identify important brain networks
associated with enhanced navigation skills and further, relate this to potentially
important factors such as age, previous visual experience, as well as learning and
training strategies. As part of our ongoing investigation, we completed a brain imaging
study in blind individuals that were asked to navigate in a large scale, indoor virtual

Development of an Audio-Haptic Virtual Interface for Navigation

599

environment using audio-based spatial cues. We found that while performing navigation 
tasks, blind individuals showed brain activation patterns within areas known to
be implicated with spatial processing; very similar to that of individuals with normal
vision. Furthermore, individual navigational abilities appeared to be related to the
utilization of different brain network structures [31]. Thus, by characterizing factors
that inﬂuence brain activity, we may gain important insight with regards to how spatial
navigation skills are learned and ultimately taught in the blind community.

These gaps in knowledge may be due to the fact that the combination of novel assistive
technology development and clinical outcomes-based studies have not been extensively
pursued. We propose to address these gaps through the development of (1) assistive
technology that is more in-line with today’s digital-driven approaches in accessing
information and leveraging the educative potential of virtual reality-based learning,
and (2) employing clinical trial-inspired study design features (e.g. randomization and
masked assessments) to evaluate the efﬁcacy of these assistive technologies.

2 Approach, Design, and Preliminary Results

2.1 Development and Reﬁnement of the Current HAGA Prototype

Our current prototype (HAGA) represents a progressive evolution of a concept;
namely, that non-visual sensory cues (i.e. audio and tactile) can be used as a means to
acquire crucial sensory information and knowledge regarding the spatial layout of an
environment. Interacting with this information in a controlled, safe, self-directed, and a
motivating manner leads to behavioral gains that are not typically seen in more didactic
or rote-based learning strategies. It is important to note that this proposed assistive
technology is not meant (nor can it) replace traditional O&M training but rather, it is
meant to serve as a complement. Compared to other approaches (such as tactile maps or
GPS based systems), it has the advantage of allowing for self-directed exploration and
survey prior to travel (and potentially real-time exploration when deployed on a mobile
platform). The interaction is engaging and dynamic, beyond what is typically provided
by verbal instruction or using a tactile map (which may be limited in terms of its
accessibility). At the same time, it is not limited to outdoor environment use as is the
case of GPS based systems. Finally, our approach encourages learning and promotes
strategy development, rather than supporting device dependency and over-reliance.

Based on our previous ﬁndings, a survey of available assistive technology, and
feedback from potential end users, and teachers of O&M, we have incorporated the
following innovative features in our next generation platform Haptic Audio Game
Application (HAGA): (1) a design to promote dynamic interaction and survey of spatial
information using a self-directed exploratory strategy, (2) greater immersive sensory
experience with full 3D sound and tactile/haptic feedback allowing for more robust
acquisition and interaction with spatial contextual information (including high contrast
visual environments for use with individuals with low vision), (3) mapping of complex,
large scale, indoor and outdoor environments with multiple routes of travel, (4) adapted
functionality to collect data within a neuroimaging scanner, including TTS commands
and automated sequential loading of target routes and paths for block design data

600

L.B. Merabet and J. Sánchez

acquisition (typical for fMRI experiments). Finally, the use of the Unity game engine
development software (http://unity3d.com/) allows for: (1) high stability and rapid
prototyping of virtually any large scale, indoor and outdoor environment desired and
with high ecological validity, (2) wide deployment on a variety of technology platforms
and operating systems (including PC, Mac and mobile Android, iOS), and (3) no cost
for development and support within an engaging open community forum with over 1
million developers (Fig. 2).

Fig. 2.
(A) Haptic Audio Game Interface (HAGA) is a full 3D rendering of the indoor and
outdoor campus (see inset ﬁgures) of the Carroll Center for the Blind. (B) the Unity gaming
engine used to create HAGA allows for any environment to be rapidly rendered and audio sounds
and tactile feedback to added for the purposes of exploring a large-scale environment.

Prior to assessing the effectiveness of the HAGA device, we carried out a usability
evaluation of our current HAGA prototype. This was a combination of expert analysis
and a series of user case studies (i.e. focus group of potential end users), the goal of
which was to reﬁne the usability of the device by identifying the most important
features to be implemented and potential problems to be resolved prior to commencing
the behavioral evaluation study. The expert usability analysis represents a formal
evaluation of the device and typically separates technical factors (relating to system
design and hardware/software considerations) from usability factors (relating to interface 
design and user experience). This serves to guide the development of the prototype
by ensuring that good human-computer interaction principles are followed and that
user-centered design is adopted from the onset. The formal report serves initially as a
feedback loop for iterative software development efforts. Once ﬁnalized, the system
design is then benchmarked against actual data to ensure key usability principles are
retained and has evolved into the most usable and acceptable device possible. The
second stage is a series of user case studies (i.e. focus groups) conducted with the
reﬁned prototype. Feedback on its acceptability, the success of the user interface and
application design choices will be assessed through a three-part post-test questionnaire.
The self-report questionnaire contains both closed-ended statements asking users to
provide answers on a 1 to 7 Likert scale, as well as open-ended questions to elicit
unformatted feedback regarding the system. The questions focus on three areas:

Development of an Audio-Haptic Virtual Interface for Navigation

601

(i) prototype user-interface design (user feedback on the hardware, input/output devices
including the audio and vibro-tactile interface), (ii) scenario design (addressing common 
issues speciﬁc to navigation needs such as devising alternate strategies if the
device fails), and (iii) overall acceptability (to determine the overall user-centered
success of the system). Although the evaluation typically contains only a small number
of participants and is still on-going, the combination of expert usability analysis and
user case series is a well-established approach in isolating the main problems related to
user-centered design. In general, an estimated 3-5 participants are needed to identify
over 75 % of user interface problems in usability [32].

2.2 Navigation with HAGA

The spatial layout of the target environment is learned through self-directed navigation
using HAGA. Speciﬁcally, blind participants use a combination of keyboard strokes
and/or a joystick (an XboxTM controller with “rumble” vibration feature) to move
through the virtual environment (see Fig. 1C). Each step in HAGA is meant to represent 
one typical step in real physical space. Footstep sounds are used to help the user
determine how fast they are moving, and what surface materials they are walking on
(e.g. hard sole sounds on cement, soft sole sounds on carpet, crushing leaves on grass).
Iconic spatialized audio cues are delivered after every step taken to help the user gain a
sense of the spatial layout of objects and rooms surrounding them and their relative
orientation (e.g. a door knocking sound in the left stereo channel is heard as the player
walks past a door on their left, walking by the cafeteria elicits a Doppler-like effect of
chatter noises and clanging dishes). These iconic and spatialized sounds serve as “audio
beacons” or landmarks to help the user capture the overall layout of the area. At any
time, the user can also query “what is front of me?” and “where am I?” via key strokes
to access TTS information identifying obstacles as well as heading cues for various
exits and buildings. Vibratory feedback (rumble feature of the joystick) is used to alert
the user of obstacles encountered and using varying pulsing frequencies identiﬁes their
orientation relative to that obstacle (i.e. high frequency for head-on and lower frequency 
while straﬁng). Orientation information is based on clock hour headings (i.e.
“12” is straight ahead) and “beeps” for every 30° of turning angle, to provide a
consistent egocentric based frame of reference during navigation. Finally, given that the
virtual environment is rendered using a gaming engine that creates a visual ﬁrst person
3D perspective, there is also the possibility to incorporate high contrast, high magniﬁcation 
visual features that can be used by individuals with low vision.

2.3

Subject Recruitment and Testing Environment

Testing will be carried out at the Carroll Center for the Blind (Newton, MA) where
clients with profound vision loss and low vision are continuously enrolled throughout
the year. The campus covers an area over 430,000 sq ft and has multiple sidewalk and
road paths communicating between 4 large buildings (Fig. 3A). For the behavioral
assessment, early and late blind as well as low vision individuals (based on current

602

L.B. Merabet and J. Sánchez

WHO deﬁnitions; males and females aged between 14 and 45 years old) will be invited
to participate in the study. Causes of blindness will vary between subjects however,
retinopathy of prematurity, optic nerve hypoplasia, and macular degeneration will
likely be common causes. Early blind subjects will be deﬁned as individuals with
profound visual deprivation occurring before the age of three (i.e. before language
development). The lower limit for late blind will be deﬁned as 14 years old. These age
limit deﬁnitions have been used in previous investigations as a means to dichotomize
the effect of prior visual experience on development (see [33] for discussion).

Fig. 3. Preliminary results with HAGA used for the purposes of learning the indoor and outdoor
spatial layout of the Carroll Center campus. (A) Aerial view of campus showing the 4 main
buildings and possible routes. (B) Results from a navigation task assessment in an early blind
individual following exploration and route learning with HAGA. Possible outdoor (white) and
indoor (blue) routes are shown. Starting from the meeting point in the main building (star), the
participant was able to successfully navigate to the entrance of the St. Paul building (path “A”,
time: 1 min, 32 s). In a second task, the same individual was able to navigate from the starting
point (star) to the entrance of the technology center (path “B”, time: 1 min, 15 s). In a second set
of navigation tasks, the same participant was asked to ﬁnd an alternate path from the entrance of
the technology center to the St. Paul building (i.e. not crossing through the main building). This
was accomplished successfully despite not being explicitly taught the route (see path “C”, time
2 min: 33).

2.4 Real-World Navigation Task Assessment and Preliminary Results

Eligible participants will be introduced to the HAGA software and familiarized with the
function keys and controls. Each participant will interact with HAGA for 90 min (3,
30 min or 2, 45 min sessions) over a 2 day period. This training time was determined
based on previous and current pilot study work and deemed as sufﬁcient to allow a user
to explore the entire campus. During the training period (i.e. HAGA exploration), early
and late blind participants will be blindfolded to minimize possible confounding due to
residual visual function.

After the training sessions, participants will then be evaluated on their understanding
of the campus layout by carrying out a series navigation tasks (indoor and outdoor) on the
Carroll Center campus. For this behavioral assessment phase, all participants will be

Development of an Audio-Haptic Virtual Interface for Navigation

603

blindfolded to minimize possible confounding related to residual visual function on
navigation task performance. Subjects will be allowed to use a cane as a means of mobility
assistance and remain blindfolded throughout the assessment. Subjects will also be followed 
by an experienced investigator ﬁlming the path taken and to ensure their safety, as
well as provide further task instructions. On a ﬁrst level assessment (task 1), navigation
performance will be assessed by carrying out a series of 10 predetermined origin-target
routes presented in random order. The routes will be of comparable difﬁculty in terms of
distance (comprising of both indoor and outdoor components) and complexity
(i.e. number of turns and potential obstacles) and a maximum travel time of 6 min will be
allowed. For each path, the subject will be brought to a speciﬁc starting point and told to
navigate to a target destination using the shortest route possible. In a second level
assessment (task 2), participants will be escorted (by a sighted guide) from a point of
origin to a target location on campus. They will then be asked to navigate back to the point
of origin using an alternate path (i.e. reminiscent of what is referred to in O&M as a
“drop-off” task). Again, a series of 10 predetermined origin-target paths (presented in a
random order) will be carried out and a maximum travel time of 6 min will allowed for
each route.

This later set “drop off” tasks will serve as a means to assess how well the subject
can mentally manipulate their cognitive map for the purposes of determining alternate
routes (note a counterbalanced design between the two tasks will be carried out in order
to minimize the potential of a learning or “carry over” effects from sequential task
assessments). For these behavioral assessments, the primary outcome of interest will be
mean success in completing the origin-target route pairs (expressed as percent correct).
Secondary outcomes will include mean time to destination (quantiﬁed by absolute
travel time to target as well as preferred walking speed [34, 35]), and route accuracy
(quantiﬁed by the number of extra turns and unintended collisions [36]). This evaluation 
approach has been used successfully to quantify performance in similar navigation 
studies investigating the effectiveness of assistive technology (e.g. [21, 37]).
Navigation performance will be scored by a second investigator following the study
participant. Timing commences once the participant takes their ﬁrst step and stops
when they verbally report that they have arrived at the target destination.

In a pilot study, an early blind participant (previously unfamiliar with the Carroll
Campus) freely interacted with HAGA for 45 min and was asked to navigate a series of
pre-determined origin-target paths and alternate routes. Preliminary results carried out
with this early blind participant suggest that following exploration using HAGA, they
were able to transfer their acquired spatial knowledge by successfully arriving to a series
of target destinations. Furthermore, when asked to ﬁnd an alternative route, they were
able to do so despite never being explicitly taught the existence of the path (Fig. 3B).
This later observation has important practical and safety implications for O&M in
general. Consider the situation where an individual has to ﬁnd an alternative route when
a known path from memory is inaccessible. An individual that has a more robust
cognitive understanding of inter-spatial relationships that characterize their surrounding
environment will be able to devise and access alternative paths when needed.

604

L.B. Merabet and J. Sánchez

3 Discussion and Conclusions

We are investigating cognitive spatial mapping skills in people who are blind through
the use of virtual navigation and assessing the transference of acquired spatial knowledge 
in large-scale, real-world navigation tasks. Towards this goal, we have developed a
user-centered, computer-based, navigation software platform called Haptic Audio Game
Application (HAGA). Using a combination of both iconic and spatialized auditory cues
and vibro-tactile feedback, a user is able to survey and build a cognitive spatial map of
the surrounding environment. Preliminary testing suggests that a blind user can transfer
acquired spatial information as assessed by their performance on a series of physical
navigation tasks carried out in the actual target environment that was explored virtually.
While the investigation is ongoing, we believe the strength of the study is related
to (1) the user-centered and (2) evidence-based clinical trial designs being employed.
By implicating potential end-users of the software early in the design and reﬁnement,
we are more likely to develop assistive technology that will enjoy greater adoption by
the blind community. Second, by implementing evidence-based study design principles
in the evaluation of navigation performance, we are in a position to better evaluate the
true efﬁcacy of the software for its intended purpose and avoid the effect of potential
confounders such prior visual experience and familiarity with technology on observed
performance.

We are also cognizant of potential limitations with this study. For example, we
have found it necessary to place an upper limit on the time allotted for navigation task
assessment. While this has not been an issue in our past studies, participants in this
study will be navigating in a much larger environment (including outdoor spaces) and
thus there is the risk that a greater number of unsuccessful and timed out runs will
occur. In general, this can be avoided by established a rigorous training program and
evaluating participants performance on virtual navigation tasks prior to moving to the
physical navigation assessments. By ensuring that a minimal level of performance and
understanding of the spatial layout are achieved, testing in the actual physical environment 
can be more robust and reliable.

In summary, our interest in developing an interface that combines audio and
tactile/haptic sensory information, and indeed, this is a core design feature of the next
generation assistive device proposed here. From a rehabilitation standpoint, this study
has the potential of developing into a relatively simple, novel, engaging and cost
effective means to supplement classic O&M training for the blind. In line with our goal
of developing this approach as a viable assistive technology tool, we are also looking at
expanding this platform to model public spaces and environments such as airports and
public transit systems.

Acknowledgement. This work was supported by an NIH/NEI RO1 GRANT EY019924 (Lotﬁ
B. Merabet) and also funded by the Chilean National Fund for Scientiﬁc and Technological
Development, Fondecyt HYPERLINK “tel:1150898” 1150898; and the Basal Funds for Centers
of Excellence, FB0003 project, from the Associative Research Program of CONICYT, Chile.
(Jaime Sánchez). The authors would like to thank the research participants, as well as Rabih
Dow, Padma Rajagopal and the staff of the Carroll Center for the Blind (Newton MA, USA) for
their support in carrying out this research.

Development of an Audio-Haptic Virtual Interface for Navigation

605

References

1. Blasch, B.B., Wiener, W.R., Welsh, R.L.: Foundations of Orientation and Mobility, 2nd ed.,

xx, 775 p. AFB Press, New York (1997)

2. Landau, B., Gleitman, H., Spelke, E.: Spatial knowledge and geometric representation in a

child blind from birth. Science 213(4513), 1275–1278 (1981)

3. Strelow, E.R.: What is needed for a theory of mobility: direct perception and cognitive

maps–lessons from the blind. Psychol. Rev. 92(2), 226–248 (1985)

4. Tolman, E.C.: Cognitive maps in rats and men. Psychol. Rev. 55(4), 189–208 (1948)
5. Ashmead, D.H., Hill, E.W., Talor, C.R.: Obstacle perception by congenitally blind children.

Percept. Psychophys. 46(5), 425–433 (1989)

6. Ashmead, D.H., et al.: Spatial hearing in children with visual disabilities. Perception 27(1),

105–122 (1998)

7. Thinus-Blanc, C., Gaunet, F.: Representation of space in blind persons: vision as a spatial

sense? Psychol. Bull. 121(1), 20–42 (1997)

8. Pasqualotto, A., Proulx, M.J.: The role of visual experience for the neural basis of spatial

cognition. Neurosci. Biobehav. Rev. 36(4), 1179–1187 (2012)

9. Axelrod, S.: Effects of early blindness; performance of blind and sighted children on tactile

and auditory tasks, ix, 83 p. American Foundation for the Blind, New York (1959)

10. Loomis, J.M., Klatzky, R.L., Golledge, R.G.: Navigating without vision: basic and applied

research. Optom. Vis. Sci. 78(5), 282–289 (2001)

11. Fortin, M., et al.: Wayﬁnding in the blind: larger hippocampal volume and supranormal

spatial navigation. Brain 131(Pt 11), 2995–3005 (2008)

12. Bavelier, D., Green, C.S., Dye, M.W.: Children, wired: for better and for worse. Neuron 67

(5), 692–701 (2010)

13. Bavelier, D., et al.: Brains on video games. Nat. Rev. Neurosci. 12(12), 763–768 (2011)
14. Dede, C.: Immersive interfaces for engagement and learning. Science 323(5910), 66–69

(2009)

15. Shaffer, D.W., et al.: Video games and the future of learning. Phi Delta Kappan 87, 104–111

(2005)

16. Lange, B., et al.: Designing informed game-based rehabilitation tasks leveraging advances in

virtual reality. Disabil. Rehabil. 34, 1863–1870 (2012)

17. Mayo, M.J.: Video games: a route to large-scale STEM education? Science 323(5910), 79–

82 (2009)

18. Giudice, N.A., Bakdash, J.Z., Legge, G.E.: Wayﬁnding with words: spatial learning and
navigation using dynamically updated verbal descriptions. Psychol. Res. 71(3), 347–358
(2007)

19. Ohuchi, M., et al.: Cognitive-map formation of blind persons in a virtual sound environment.
In: Proceedings of the 12th International Conference on Auditory Display, London, UK
(2006)

20. Riehle, T.H., Lichter, P., Giudice, N.A.: An indoor navigation system to support the visually

impaired. Conf. Proc. IEEE Eng. Med. Biol. Soc. 1, 4435–4438 (2008)

21. Legge, G.E., et al.: Indoor navigation by people with visual impairment using a digital sign

system. PLoS ONE 8(10), e76783 (2013)

22. Johnson, L.A., Higgins, C.M.: A navigation aid for the blind using tactile-visual sensory

substitution. Conf. Proc. IEEE Eng. Med. Biol. Soc. 1, 6289–6292 (2006)

23. Lahav, O.: Using virtual environment to improve spatial perception by people who are blind.

Cyberpsychol. Behav. 9(2), 174–177 (2006)

606

L.B. Merabet and J. Sánchez

24. Pissaloux, E., et al.: Space cognitive map as a tool for navigation for visually impaired.

Conf. Proc. IEEE Eng. Med. Biol. Soc. 1, 4913–4916 (2006)

25. Driver,

J., Noesselt, T.: Multisensory interplay reveals crossmodal

inﬂuences on
‘sensory-speciﬁc’ brain regions, neural responses, and judgments. Neuron 57(1), 11–23
(2008)

26. Sanchez, J., Lumbreras, M.: 3D aural

interactive hyperstories for blind children. Int.

J. Virtual Reality 4(1), 20–28 (1998)

27. Connors, E.C., et al.: Virtual environments for the transfer of navigation skills in the blind: a
comparison of directed instruction vs. video game based learning approaches. Front. Hum.
Neurosci. 8, 223 (2014)

28. Connors, E.C., et al.: Action video game play and transfer of navigation and spatial

cognition skills in adolescents who are blind. Front. Hum. Neurosci. 8, 133 (2014)

29. Connors, E.C., et al.: Development of an audio-based virtual gaming environment to assist

with navigation skills in the blind. J. Visualized Exp. JoVE 73 (2013)

30. Merabet, L.B., et al.: Teaching the blind to ﬁnd their way by playing video games.

PLoS ONE 7(9), e44958 (2012)

31. Halko, M.A., et al.: Real world navigation independence in the early blind correlates with
differential brain activity associated with virtual navigation. Hum. Brain Mapp. 35(6),
2768–2778 (2014)

32. Shneiderman, B., Plaisant, C., Jacobs, S.: Designing the User Interface: Strategies for

Effective Human-Computer Interaction, 5th edn. Addison Wesley, Boston (2009)

33. Merabet, L.B., Pascual-Leone, A.: Neural reorganization following sensory loss:

the

opportunity of change. Nat. Rev. Neurosci. 11(1), 44–52 (2010)

34. Beggs, W.D.: Psychological correlates of walking speed in the visually impaired.

Ergonomics 34(1), 91–102 (1991)

35. Soong, G.P., Lovie-Kitchin, J.E., Brown, B.: Preferred walking speed for assessment of
techniques. Clin.

sighted guide versus non-sighted guide

mobility performance:
Exp. Optom. 83(5), 279–282 (2000)

36. Hartong, D.T., et al.: Improved mobility and independence of night-blind people using

night-vision goggles. Invest. Ophthalmol. Vis. Sci. 45(6), 1725–1731 (2004)

37. Kalia, A.A., et al.: Assessment of indoor route-ﬁnding technology for people with visual

impairment. J. Vis. Impairment Blindness 104(3), 135–147 (2010)

