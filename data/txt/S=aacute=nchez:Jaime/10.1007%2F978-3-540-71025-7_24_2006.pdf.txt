Mobile Messenger for the Blind 

Jaime Sánchez and Fernando Aguayo 

Department of Computer Sciences 

University of Chile, Chile 

{jsanchez, faguayo}@dcc.uchile.cl 

+56 2 673 1280 

Abstract. An increasing number of studies have used technology to help blind 
people to integrate more fully into a global world. We present software to use 
mobile  devices  by  blind  users.  The  software  considers  a  system  of  instant 
messenger to favor interaction of blind users with any other user connected to 
the network. Input/Output implementation modules were emphasized creating a 
9-button virtual keyboard and associated Text-to-Speech technology (TTS). The 
virtual  keyboard  helps  to  write  into  the  pocketPC  without  needing  external 
devices, representing a real challenge  for novice blind users. The TTS engine 
was adapted to blind users by adjusting the engine. Usability evaluation of these 
modules was iteratively applied to end-users. As a result, the integration of the 
designed modules into a communication system helped us to create a messenger 
system specially tailored to people with visual disabilities. 

1   Introduction 

A number of sound-based applications [8, 12, 13, 14, 15, 16] have been developed to 
assist the learning of children with visual disabilities, obtaining significant results in 
the development of tempo-spatial, memory, and logicmathematics 
skills. However, 
these applications are limited to a static context of use such as the school classroom 
and  home.  This  has  lead  authors  to  find  out  how  technology  can  support  mobile 
activities of children with visual disabilities and related issues. 

PocketPC  mobile  devices  emerged  as  a  consequence  of  diverse  efforts  to  create 
more portable PCs. Thus, computer peripherals are replaced depending on associated 
mobile  user  needs  such  as  the  traditional  keyboard  that  is  substituted  by  text 
recognition and on-screen keyboard; and the stereo sound is replaced by mono sound. 
This series of adjustments has modeled user expectations ending up with an efficient 
mobile device.  In many respects the design of these devices did not consider users 
with special needs. Actually, most mobile devices do not model the mental strategies 
of these users.    

Diverse  efforts  have  been  made  in  order  to  adapt  the  PocketPC  to  users  with 
impairments,  achieving  the  design  of  somehow  physically  different  devices  but 
oriented  only  to  specific  users.  An  example  of  this  is  the  creation  of  a  PocketPC 
specially designed for blind users with a screen reader and replacing the tactile screen 
by a series of functional buttons all over the surface [1]. 

C. Stephanidis and M. Pieper (Eds.): ERCIM UI4ALL Ws 2006, LNCS 4397, pp. 369 – 385, 2007. 
© Springer-Verlag Berlin Heidelberg 2007 

370 

J. Sánchez and F. Aguayo 

Some external devices have been created to facilitate the interaction of users with 
special needs that also can help blind users to use the pocketPC. The problem is that 
they  force  users  to  use  an  additional  device  [17]  debilitating  mobility,  the  main 
attribute of mobile devices.     

Our  work  is  focused  on  the  use  of  standard  pocketPC  together  with  software 
specially  designed  for  users  with  visual  impairments,  without  requiring  additional 
hardware. This leads users with visual impairments to use the pocketPC not including 
special requirements, facilitating their integration into a global world. To fully use the 
device blind users only need adequate software and little initial training as it happens 
with sighted users when using new software in their mobile devices.  

The  purpose  of  this  study  was  to  create  software  to  enhance  communication 
between different types of users [6], blind and sighted, by using a standard pocketPC. 
The software used was designed together with blind people in an iterative process to 
facilitate  the  integration  user-device,  and  the  communication  between  the  user  and 
messenger contacts. 

Due to the  fact that this  software  was designed together  with end-users  from the 
very  beginning,  multiple  usability  evaluation  tests  were  applied  to  successively 
designed prototypes before coming up with the final product. This situation pushed us 
to present this work in a nontraditional way, because the implementation is threaded 
with  the  end-user  usability  testing.  It  also  helped  us  to  observe  how  the  initial 
software  product  suffered  changes  to  better  adapt  and  fit  the  needs  of  end-users. 
During usability testing a sample of users from different ages participated to obtain a 
product shaped for different needs.    

One  of  the  most  significant  proposals  of  our  study  is  the  design  of  a  virtual 
keyboard,  specially  designed  for  blind  people.  The  model  used  for  the  keyboard  is 
discussed  in  different  studies  that  use  the  same  model  as  an  alternative  to  the 
traditional keyboard, but it is only applicable to users without disabilities [3, 4, 5]. 

2   Design, Development, and Testing  

In  order  to  improve  the  design,  implementation,  and  re-design  of  the  software,  the 
implementation was divided into 4 stages: 
1. Keyboard 
2. Text-to-Speech (TTS) 
3. Instant Messenger Systems (IMS) 
4. Instant Messenger System with Audio (IMSA) 

These stages, tightly related to the interaction between the user and device,  were 
constantly tested in order to arrive to a real solution for users with visual impairments. 
The comments and suggestions made by users during testing were fully implemented 
by redesigning the prototype and thus improving the software. 

The  tools  used  for  the  development  of  both  pocketPC  and  PC  were  Microsoft 
Visual  Studio  .NET  2003,  programming  language  C#,  and  a  TTS  engine  that  is  an 
external library from ACAPELA. We also used audio files with numbers and letters 
recorded in our research center. 

 

Mobile Messenger for the Blind 

371 

2.1   Sample Participants 

During testing, a group of people with visual impairments participated, ranging from 
users with residual vision to totally blind users. All of them were legally blind. The 
sample consisted in 8 participants, ages 12 to 25 years old. 

2.2   Keyboard 

The most interesting advantage of some mobile devices is the capacity of handwriting 
recognition on the tactile screen. This type of writing has being improved through the 
time due to the lack of keyboard embedded in mobile devices. Event though this input 
mode is very useful for sighted users, it is too complicated to be used by users with 
visual  impairment,  because  they  can  not  see  the  arrangement  of  strokes  neither  the 
areas where these strokes can be drawn.  

Our  research  team  searched  a  solution  to  supply  the  absence  of  a  physical 
keyboard.  We  discarded  voice  recognition  due  to  the  fact  that  it  requires  a  specific 
training for each user besides to an excessive use of pocketPC functioning resources. 
We  then  arrived  to  the  idea  of  using  a  virtual  keyboard  on  the  screen,  specially 
oriented  to  these  users.  This  is  not  the  traditional  keyboard  composed  by  108  keys. 
Rather, we first designed a keyboard with only 12 buttons such as in mobile phones, 
and then we ended up with only 9 buttons. Some studies analyze this type of writing 
in different devices [4, 5]. 

Table 1. Button distribution on the screen 

1 
4 
7 
* 

2 
5 
8 
0 

3 
6 
9 
# 

Our  goal  was  to  create  an  interface  that  could  be  used  by  blind  users  and  at  the 
same  time,  be  efficient  for  writing.  The  first  step  was  to  study  the  use  of  virtual 
keyboards by blind users. In the first approach, a keyboard composed of 12 buttons 
(Fig. 1a) was used, similar to mobile phones with numbers from 1 to 9, including 0, # 
and *, covering the whole screen of the mobile device. During testing this keyboard 
users  hardly  found  the  central  buttons  (4,  5,  6, 7, 8  and  9,  see  Table  1),  but  corner 
buttons from upper and lower rows were very easy to operate due to the fact that for 
them  it  was  very  easy  to  represent  keyboard  corners  that  matched  screen  corners. 
Users also succeeded at localizing buttons 2 and 0 by using the technique of middle 
points. We then conclude that this first prototype was useless because it was complex 
to use.   

After  experimenting  with  the  12-keys  keyboard  we  created  another  prototype  by 

using just 9 buttons (Fig. 1 b). 

 

372 

J. Sánchez and F. Aguayo 

Fig. 1. Evolution of the interface: (a) Interface with 12 buttons, (b) Interface with 9 buttons 

 

By using only numerical buttons from 1 to 9, users could localize buttons 1,3,7,9 
through  the  screen  corners;  buttons  2,4,6,8  through  the  middle  points  of  the  screen 
sides; and button 5 through the projection of middle points (see Table 2). When this 
prototype was tested with blind users, preliminary encouraging results were obtained.  
The next step was the creation of a functional prototype to test the new keyboard 
interface  with  blind  users.  A  TTS  was  embedded  into  this  prototype  to  provide 
feedback to their writing.  

Table 2. New button distribution on the screen 

1 
4 
7 

2 
5 
8 

3 
6 
9 

Different  functionalities  were  added  to  these  9  buttons  by  following  a  known 

scheme as with mobile phones. The functionalities were designed as follow: 
•  Button 1: space and delete 
•  Button 2: keys (a, b, c) 
•  Button 3: keys (d, e, f) 
•  Button 4: keys (g, h, i) 

 

Mobile Messenger for the Blind 

373 

•  Button 5: keys (j, k, l) 
•  Button 6: keys (m, n, o) 
•  Button 7: keys (p, q, r, s) 
•  Button 8: keys (t, u, v) 
•  Button 9: keys (w, x, y, z) 

In order to find the desired letter users have to press multiple times the button that 
contains such letter. To find the desired letter the user has to wait one second and then 
it  is  accepted  as  valid.  This  is  the  same  period  utilized  in  cellular  writing,  and  we 
detected that it is an appropriate time. Also, if after finding the desired letter the user 
presses  another  button,  the  letter  is  accepted  immediately.  Consequently,  a  word  is 
constructed in a similar way as in mobile phones (Fig. 2). 

 

Press Key

Is it

the same  

key? 

NO

Save old letter 

in buffer 

YES

Time
out?

YES

Save current
letter in buffer 

Change letter 

NO

 

Fig. 2. Functioning of the virtual keyboard 

The visual interface is also relevant for users with residual vision, but legally blind, 
that were also our target population. This is the reason why the selection of colors for 
the interface was based on previous studies that use high contrast colors to distinguish 
shapes for users with residual vision [11, 15]. 

When this new prototype was designed, another usability test for buttons use and 
acceptance  was  implemented  to  determine  if  users  could  write  words  efficiently. 
Usability  testing  was  implemented  as  a  case  study  (individually)  by  providing 
pocketPCs  to  users  to  familiarize  them  with  both  the  device  and  the  new  software. 
Then, users were informed about software basic operations and the functions of each 
button.  End-users  had  free  time  to  use  the  software  and  later  they  performed 
predefined tasks such as  writing their complete  name using the software tools. This 
testing was carried out with users of different ages and different degrees of blindness. 
During the test we found out that the time the user spent to choose a letter was very 

374 

J. Sánchez and F. Aguayo 

short for younger users, but it was adequate for older ones. Thus, in order to make the 
testing  easier  for  young  users,  the  time  for  letter  acceptance  was  increased  by 
modifying  the  initial  time  from  1  to  1.5  seconds.  If  the  letter  chosen  was  not  the 
desired, the user had to press the button again until  finding the desired letter. If the 
letter is not set on the pressed button the user had to delete and look for it in another 
button.  

Another  important  detail  was  the  latency  between  the  time  elapsed  from  the 
moment  a  user  pressed  a  button  to  the  TTS  reply  saying  the  corresponding  letter. 
When solving this we found ourselves in a crossroad because the TTS architecture did 
not provide options for an efficient processing. A solution was sought in the field of 
keyboard  prediction  in  order  to  send  the  requirement  to  the  TTS  before  the  user 
pressed a button and thus providing an efficient feedback. This solution did not work 
due to the degrees of freedom of the user when writing. Then we decided to adopt a 
mixed solution when providing feedback to the user depicted in the use of sound [2]. 
The  solution  consisted  of  using  pre-recorded  sounds  and  the  TTS  in  the  following 
way: 
1. Pre-recorded sounds are composed by letters and numbers. 
2. The TTS is used for pronunciation of words and sentences. When a user presses a 
button the corresponding sound to a letter is played, avoiding the TTS processing 
and thus providing an efficient feedback to the user.  
Later, a new testing was implemented in order to validate the changes made and to 
search  for  new  improvements.  When  evaluating  the  use  of  the  virtual  keyboard  we 
observed that: 
1. Users could easily use the keyboard for writing sentences without difficulties. This 

was clearly observed in users with residual vision and adult users. 

that 

they  do  not  have  relieves,  making  harder 

2. Users  who  had  more difficulties in the  use of the  virtual keyboard  were children 
less than 12 years old and those totally blind. This can be explained because they 
had never used a mobile phone and they did not have any experience with the use 
of  multiple function buttons.  In addition to this, buttons are virtual on the screen 
implying 
the  keyboard 
representation.    
The implementation team searched for a mechanism for users to adapt to the new 
input created. A solution was the use of a grid with relieves over the virtual keyboard 
to help users to understand button distribution and functioning. This mechanism did 
not  work  because  the  tested  grids  did  not  provide  the  desired  tactile  feedback.  We 
redesigned again the problem of physical representation of buttons splitting users in 
two groups: 
a. Users that understand the operation of a mobile phone keyboard but did not find the 

position of buttons.  

b. Users  that  had  not  used  a  mobile  phone  keyboard  so  it  was  harder  for  them  to 

understand the use of the virtual keyboard.   
The solution for group “a” was very simple and consisted of placing marks around 
the screen that highlighted the middle points where the buttons of the virtual keyboard 
were placed. In case “b” we arrived to the solution of putting a rubber keyboard from 
 

 

Mobile Messenger for the Blind 

375 

a mobile phone on top of the pocketPC screen (Fig. 3) to help blind users to feel the 
buttons  before  pressing  them.  The  idea  with  this  keyboard  was  to  use  it  in  an 
intermediate  stage  before  using  a  keyboard  with  9  virtual  buttons.  We  obtained 
important  results  after  testing.  After  a  short  time  of  interaction  with  the  rubber 
keyboard, users got along with the virtual keyboard without any problems. The user in 
Fig 3 has the device close to his ear due to the low volume provided by the device, but 
this was later solved by using headphones. 

Fig. 3. Interaction with a rubber keyboard 

 

An  anecdotic  situation  during  testing  was  that  buttons  did  not  have  a  label 
describing different associated letters because it was obviously not necessary for blind 
users  and  users  with  residual  vision  (they  only  discriminate  shapes).  But,  to  testing 
facilitators  labels  were  essential  to  recognize  the  letter  distribution  on  buttons, 
especially when the tasks were performed with novice users using the keyboard. To 
solve this situation we labeled buttons displaying their functionality (Fig. 4) and thus 
supporting the work of the facilitator.  

Once we implemented relevant changes in the interface a new testing was applied 
obtaining  results  that  were  completely  satisfactory.  Users  wrote  quickly  due  to  the 
lack  of  latency  between  the  moment  of  pressing  of  a  virtual  button  and  the 
pronunciation  of  a  letter  by  the  pocketPC.  They  also  started  to  memorize  the  letter 
location in the buttons, decreasing the error rate when searching for a letter.    

The  implementation  of  a  mobile  phone  physical  keyboard  for  the  pocketPC  was 
also relevant because users used this keyboard for writing and after a while, they were 
capable of using the virtual keyboard without the aid of the physical interface. 

The  marks  put  over  the  mobile  device  to  signaling  middle  points  had  also  a 

positive effect on users because the location of central buttons was much easier.  

 

376 

J. Sánchez and F. Aguayo 

Fig. 4. Interface with labels on the buttons 

 

End-users found an interface feedback problem during testing because there was no 
audio  reply  when  they  pressed  out  of  the  buttons,  in  the  space  between  buttons. 
Another important observation was that they were not comfortable with the voice and 
volume  used  by  the  TTS.  The  changes  implemented  included  additional  feedback 
suggested  by  users  during  testing.  This  is  the  reason  why  we  implemented  a  sound 
that  plays  each  time  users  pressed  between  buttons.  Suggested  changes  were  not 
implemented directly to the TTS used, but in a parallel work we developed a module 
to take full advantage of the TTS effectiveness.   

We  finally  implemented  a  new  testing  but  this  time  users  did  not  suggested 

changes to the interface.  

2.3   Text-to-Speech (TTS)  

This  stage  started  in  parallel  with  the  design  of  the  input  interface  and  consisted 
basically  of  finding  an  engine  that  could  translate  words  into  audio  waves,  a  TTS 
(Text-to-Speech). A search was made for available solutions on developed products. 
Compatibility  and  performance  tests  were  implemented  later  focusing  mainly  on 
voice clarity, in Spanish with English support, and without consuming all resources of 
the pocketPC, because the TTS was just an additional tool used for a higher objective. 
Another  essential  requirement  was  the  complexity  of  programming  the  TTS. We 
did not want TTS programming to be a problem but rather an easily adjustable tool. 
The search started with freely distributed products. After diverse testing we arrived to 
the conclusion that products that include the Spanish language had low performance 
because they used excessively the pocketPC processor, decreasing the performance of 
our software.     

 

Mobile Messenger for the Blind 

377 

A  second  search  for  a  TTS  was  extended  to  commercial  software.  We  found 
ACAPELA  which  came  up  with  an  excellent  performance  without  consuming 
excessive pocketPC resources and includes easy-to-use API.  

This  TTS  was  embedded  during  the  first  keyboard  testing,  validating  the 
performance obtained in the preliminary TTS testing. When users used the TTS they 
made comments related to the volume and tone of the voice.    

We  have  to  consider  that  when  we  design  for  sighted  users  we  centered  on  the 
graphical interface by making it pleasant to for the user through the correct selection of 
colors  and  forms.  For  users  with  residual  vision  the  graphical  interface  is  simpler 
because the use of colors is not excessive, only  high contrast colors are  used. Shapes 
have  not  details  because  they  can  distinguish  at  most  general  shapes.  But  sound 
interfaces play a main role providing feedback to users with visual impairment. This is 
the  reason  why  we  confer  so  much  importance  to  their  comments  and  observations 
about the TTS. We have to consider that TTS is very valued in mobile devices without 
considering the type of user because of their potential to empower interfaces [9].   

In order to improve the TTS we decided to search the real possibilities to modify it 
through  manipulation of the  API.  After a  while  we  were  able to  modify three  main 
aspects of the API: Volume, tone and voice speed. The changes to the API allowed 
 

Fig. 5. Testing the TTS 

 

378 

J. Sánchez and F. Aguayo 

manipulating in real time these three parameters. This is the reason why we decided to 
implement exclusively a voice testing of the TTS. Thus we created software that only 
uses the TTS to speak a sentence  multiple times, fixing the volume but  varying the 
tone and voice speed. 

The voice testing consisted of end-users hearing the same sentence multiple times 
but  with variations in tone and voice speed. Users had to mention  which voice  was 
more pleasant and the observer wrote down the corresponding parameters. Then the 
observer played the TTS voice again but this time controlling the parameters in order 
to validate the information gathered initially (Fig. 5).  

Another aspect evaluated during testing  was the feelings of end-users concerning 
the different voices emerged from TTS. We tested if the voice heard could be related 
to  some  person  or  to  a  characteristic  of  a  person.  This  aspect  is  very  important 
because it could easily characterize users with the TTS. 

The testing was carried out with blind users finding the parameters that modified 
the  voice  and  making  it  more  pleasant  to  them.  These  parameters  were  constant  in 
most users with little variation to the rest of the users. 

The characterization of different voices produced when modifying tone and speed 
parameters was successful. End-users were capable of characterizing the voices heard 
such as differentiating a girl, an old woman, a slim man, and a tall man. These results 
leaded  us  to  create  sound-based  emoticons  on  the  messenger  for  blind  users  using 
different  voices.  Once  the  voice  was  defined  we  could  continue  implementing  our 
main software, a messenger system for blind users.  

2.4   Instant Messenger System with Audio Server (IMSAS) 

During the development of this module diverse projections were made concerning the 
solution we should offer and what could provide a pocketPC. If we try projecting an 
instant messenger system in a mobile device we arrive to the conclusion that the main 
features of these systems are completely opposite. On the one hand, online connection 
needs an Instant Messenger System (IMS), and, on the other hand, mobile devices are 
not capable of being permanently connected at a reasonable cost. 

In order to solve this problem we had to modify the features of one of them. One 
option was to get the pocketPC permanently connected to Internet through cell phone 
technology.  This  approach  has  the  disadvantage  of  being  highly  expensive  which 
could lead us to failure. Another approach is using an Instant Messenger System with 
Audio  Server,  IMSAS,  asynchronously.  This  means  to  modify  one  of  the  most 
distributed  and  used  system  protocols,  MSN  Messenger,  which  would  let  us  in  an 
irreconcilable position with the rest of users, making the solution unpractical. 

An  important  goal  of  our  project  was  the  integration  of  visual  impaired  users  to 
massive communication systems such as MSN Messenger, which forced us to use the 
actual  protocol  utilized  by  this  system,  without  making  any  modification.  This 
guaranteed  us  that  our  users  can  communicate  with  other  users  who  use  traditional 
MSN Messenger system capable to be executed in PCs and pocketPCs. 

In  order  to  change  the  synchronous  way  of  working  in  the  IMS,  we  created  an 
intermediate server between IMS users and the Instant Messenger System with Audio 
(IMSA). We designated the IMSA server as IMSAS with the following features: 

 

Mobile Messenger for the Blind 

379 

1. To allow the creation of a synchrony state for IMSA users, since they interact with 
IMSAS, in charge of creating sessions that will keep the synchrony with the rest of 
our contacts. 

2. The  IMSAS  supports  the  fact  that  the  contacts  of  our  users  can  see  them  as 
connected to the messenger system, but absent, in such a way that contacts know 
that their messages will not be answered immediately. 
The IMSAS operation mode is the following: 

1. The user starts a session through the IMSA created in the IMSAS, which keeps a 

permanent connection of the user with the IMS network. 

2. The IMSAS stores sent messages to our users and deliver them when connected to 
the IMSAS through the IMSA. Thus the IMSA is able to communicate the received 
messages.  Users  can  write  messages  while  they  are  disconnected  of  the  IMSAS, 
which are stored in the IMSA and delivered when the connection is reestablished. 
The  IMSAS  is  in  charge  of  providing  queued  messages  to  the  corresponding 
contacts  if  they  are  connected  to  the  network;  otherwise  the  messages  are  stored 
until the contacts connect to the IMS again. 
At  the  moment,  MSN  Messenger  does  not  allow  this  type  of  asynchronous 
communication, however it has been announced that the next version of Messenger 
will  bring  an  asynchronous  communication  implementation  to  leave  messages  to 
offline  users.  When  IMSA  and  IMSAS  are  connected  they  behave  as  a 
conventional  IMS,  receiving  and  delivering  instant  messages.  The  interaction  is 
shown in Fig. 6. 

 

Fig. 6. A communication scheme for the messenger system 

 

380 

J. Sánchez and F. Aguayo 

At the end of the IMSAS implementation we designed an independent evaluation 
of the IMSA by using a basic communication application between messenger users. 
Testing  results  of  IMSAS  were  very  positive  favoring  the  communication  with 
different  type  of  contacts,  being  fully  transparent  that  an  intermediate  server  was 
being  used.  The  next  step  in  the  implementation  was  the  integration  of  the 
implemented solutions (keyboard, TTS, IMSAS) to end with the IMSA development. 

2.5   IMSA 

The  purpose  of  this  module  was  the  integration  of  previous  modules  joined  to  a 
synchronous/asynchronous  communication  system,  which  maintains  the  user’s  state 
and the conversations with contacts, besides to allowing navigation through contacts. 
The  integration  of  the  implemented  solutions  was  very  simple  since  they  were 
considered from the beginning of the development. First, we created a base-program 
to  communicate  to  IMSAS  by  using  a  WI-FI  TCP/IP  connection,  allowing  that 
whenever  users  find an Internet access-point,  they can communicate to IMSAS and 
therefore  being  able  to  put  together  the  required  synchronizations.  The  data  flow 
between IMSA and IMSAS is made by encrypting the data, so they cannot be listened 
in  the  network.  By  having  this  final  platform  we  proceeded  to  integrate  the  virtual 
keyboard with the TTS. Until then we only had integrated solutions separately, but we 
did not have a solution capable to manage resources efficiently: Users, contacts and 
conversation management. 

This  module  was  in  charge  of  capturing  the  user  input  and  executing  the  right 
actions.  For  instance,  if  the  user  navigates  the  list  of  contacts  is  not  necessary  to 
activate  the  virtual  keyboard,  but  it  requires  activating  the  physical  buttons  of  the 
pocketPC.  This  module  helps  to  maintain  different  conversations  with  different 
 

Keyboard

WAV

NO

accept

YES

buffer

Text-to-Speech 

engine 

IMS

Send
button

Send

buffer
Network

INTERNET

Fig. 7. IMSA and IMSAS communication Model 

Interface 

with 

messenger 

IMSA

 

 

Mobile Messenger for the Blind 

381 

contacts and provides feedback to the user when receiving information concerning a 
conversation either made previously with a contact or a new conversation demanded 
by  another  contact.  This  module  also  maintains  the  logs  of  conversations  to 
synchronize them with the IMSAS. 

The  diagram  (Fig.  7)  displays  the  process  of  choosing  letters  to  make  up  words, 
stored in a buffer. When the send button is pressed, the buffer is taken and delivered 
to the network manager. This manager sends the buffer to IMSAS which sends it to 
the  messenger  system.  The  IMSA  is  constantly  listening  to  what  it  gets  from  the 
IMSAS and thus when a message is received the user is immediately notified (Fig. 7). 
After  the  integration  between  communication  modules  we  implemented  end-user 
evaluation with users utilizing the pocketPC to establish a conversation. During this 
evaluation IMSA and IMSAS were used synchronously, facilitating the conversation 
flow. All conversations were established between only two users. The following is an 
extract of a conversation between a user and his instructor in the first session: 

 
Teacher: Hello  
User: Hello   
Teacher: What it’s your name? 
User: Marcos  
Teacher: What do you think about the keyboard? 
User: Difficult and slow   
Teacher: Do you like the writing system? 
User: I have to get used to it 
 
During this stage users showed writing problems, mainly when looking for a letter, 
since  they  did  not  press  the  button  with  the  right  character,  thus  having  to  delete  it 
several  times.  Next,  there  is  an  example  obtained  from  the  virtual  keyboard  logs, 
concerning the writing of a user: 

 
dj[delete]m[delete]ifid[delete]cil[space]t[delete]y[space]la[delete]enj[delete]to. 
The  way  to  interpret  this  log  is  deleting  the  last  character  whenever  the  [delete] 
command  appears  and  adding  a  blank  space  when  a  [space]  command  does  it.  By 
applying  this  rule  the  sentence  “dificil  y  lento”  (“difficult  and  slow”)  appears, 
meaning that the user had to delete it 6 times, causing frustration. As users learned the 
letter  positions  in  the  buttons  they  made  fewer  mistakes  and  frustration  was 
diminished. We could also see that the used sentences were short, due to a fast-answer 
self-pressure  imposed  by  users,  and  as  they  made  plenty  of  errors,  they  needed  to 
answer shortly and precisely.  

After some sessions end-users maintained conversations with a testing guide using 
the  IMSA  and  the  essence  of  the  conversation  was  about  its  usability.  We  provide 
next an outline of a conversation with the same user of the previous example: 

 
User: Hi fernando 
Guide: Hi marcos 
Guide: I need your help 
User: Tell me what you need 
Guide: I need you to tell me what you think about the software that you’re using 

382 

J. Sánchez and F. Aguayo 

User: First, I thought that it was complicated 
User: But now it is easier writing with it 
User: Because I don’t make mistakes when writing 
Guide: What do you think about the usefulness of this software? 
User: Now I can communicate with my friends because I have messenger. 
 
An analysis of this conversation help us to realize that the sentences utilized by the 
user are remarkably longer, this is because the user was able to work efficiently with 
the  virtual  keyboard,  and  making  fewer  errors  in  a  faster  writing.  When  evaluating 
virtual keyboard logs we observed that the mistakes made by the user were minimal 
with less than one average error by sentence. This indicates that the learning curve of 
the keyboard was exponential, helping us to draw future projections on the use of this 
type of interfaces for enhancing the writing in mobile devices by people with visual 
impairments.  The  error  rate  of  users  reduced  remarkably  as  they  got  used  to  the 
virtual keyboard (Fig. 8). 

The  TTS  did  not  receive  any  comments  from  end-users.  This  can  be  seen  as  a 
positive fact since it became a transparent tool that was pleasant to them. Whenever 
the technology is pleasant and functional to users, it becomes invisible. The results of 
 

Fig. 8. Interaction with IMSA 

 

 

Mobile Messenger for the Blind 

383 

the  final  evaluation  of  the  IMSA  were  very  positive  allowing  users  that  had  never 
utilized  an  instant  messenger  system,  to  become  able  to  communicate  without 
difficulties.  From  the  point  of  view  of  users  that  were  familiarized  with  instant 
messenger systems, the results were also positive, since they appreciated the features 
of  the  IMSA  that  were  not  currently  provided  by  the  MSN  Messenger  system, 
combined  with  a  typical  screen  reader  for  blind  people  such  as  the  alert  message 
reception. 

Users  made  favorable  comments  after  usability  evaluation  regarding  the  reasons 

why the liked this tool: 
•  “Because of its novelty. Because as a visual impaired person I can talk to another 

person from another country, from another place”. 

•  “The letter’s order, which it is just like the ones on a cell phone”. 
•  “It’s fast, messages arrive immediately”. 
•  “Communicating with other people is fun”. 

3   Conclusions 

The main result of our study was to enable blind users to use standard pocketPC by 
using  custom  made  software  to  map  their  mental  models.  The  possibility  that  users 
with  visual  disabilities  can  write  on  the  screen  of  a  mobile  device  opens  a  critical 
opportunity  for  these  users  by  closing  the  interaction  gap  between  blind  users  and 
pocketPCs. Even though in the beginning of our study this writing method presented 
some  complexities,  especially  in  novice  users  writing  on  the  screen  of  cell  phones, 
after  a  while  it  became  a  usable  system  that  facilitated  the  interaction  between  the 
device and the user. 

The learning curve of the proposed use of the virtual keyboard was exponential and 
clearly  noticeable  during  the  different  testing  stages.  At  the  beginning,  users  made 
many  errors  because  they  did  not  know  the  exact  location  of  each  letter  on  the 
keyboard  inducing  delays  in  the  writing  time.  We  have  to  consider  that  it  is  very 
different to make an error when typing in a conventional keyboard than when doing it 
with  a  pocketPC,  because  when  the  user  makes  a  error  typing  a  letter  in  a 
conventional keyboard it implies that presses only one key, while in a pocketPC the 
user has to go through all letters of the button and then realize that it is not the correct 
button. Let us consider that for a current keyboard the errors take a t time. If the keys 
have an average of 3 letters on the virtual keyboard, the time consumed by the error 
will be 3*t.  

Another important issue when evaluating the writing time is the process of deleting 
a letter. In the case of a normal keyboard this action is executed by pressing a key; in 
our  virtual  keyboard  it  implies  pressing  a  button  twice.  Therefore,  compared  to  a 
normal keyboard the keyboard introduced in this study strongly penalizes the fact of 
making an error because the t factor in a normal keyboard can be multiplied by 3 in 
our  virtual  keyboard.  This  explains  why  at  the  beginning  of  the  use  of  virtual 
keyboards, the words written by users were short and precise, revealing that they felt 
that it took too much time to write.        

During the testing period users became familiarized with the use of the proposed 
keyboard  achieving  the  memorization  of  the  location  of  letters  which  made  the 

384 

J. Sánchez and F. Aguayo 

writing easier.  After some  sessions of  using the device, our  users found  themselves 
writing at a faster speed due in part to few errors made on writing. The writing speed 
using the virtual keyboard was increased evidencing a better use of the keyboard for 
writing longer sentences than in previous sessions.   

We  believe  that  TTS  technology  has  a  critical  impact  among  these  users.  This 
makes TTS a fundamental tool for system output. For this reason the results obtained 
during testing variations of voice parameters played an important role when creating 
sound-based emoticons. 

Usability  testing  indicated  that  end-users  accepted  well  the  type  of  audio  used 
(TTS) to reproduce the messages, even though they wanted to listen natural instead of 
synthetic voices. Audio comprehension by using the software was also very high and 
meaningful. IMSA allow users that had never utilized an instant messenger system, to 
become  able  to  communicate  without  difficulties.  The  users  that  were  familiarized 
with instant messenger systems, appreciated the features of the IMSA that  were not 
currently provided by MSN Messenger system, combined with a screen reader. 

Future work should include a system for text prediction similar to the systems used 

in some mobile devices [7, 10]. 

Finally, the possibility that blind users can interact with users from different places 
through  these  devices  help  them  to  be  integrated  and  included  in  a  global  world 
without  physical  frontiers.  We  believe  that  mobile  applications  such  as  the  one 
presented in this study can help to diminish those frontiers. 

 

Acknowledgments. This report was funded by the Chilean National Fund of Science 
and Technology, Fondecyt, Project 1060797.  

References 

1.  http://www.geekzone.co.nz/content.asp?contentid=2976, 12 May, 2006 
2.  Gong L.,  Lai J.: Shall We Mix Synthetic Speech and Human Speech? Impact on Users’ 
Performance,  Perception,  and  Attitude.  Proceeding  of  the  ACM  CHI  ´01,  Seattle, 
Washington, USA, March 31 - April 5, 2001, pp 158165 
 

3.  Green  N.,  Kruger  J.,  Faldu  C.,  Amant  R.:  Late  breaking  result  papers:  A  reduced 
QWERTY keyboard for mobile text entry. CHI '04 extended abstracts on Human factors in 
computing systems, Vienna, Austria, April 24-29, 2004, pp 1429-1432  

4.  Hwang S., Lee G.: Qwerty-like 3x4 Keypad Layouts for Mobile Phone. Proceeding of the 

ACM CHI ´05, Portland, Oregon, USA, April 2-7, 2005, pp 1479-1482  

5.  Hwang  S.,  Lee  G.,  Jeon  B.,  Lee  W.,  Cho  I.:  FeelTip:  Tactile  input  Device  for  Small 
Wearable  Information  Appliances.  Proceeding  of  the  ACM  CHI  ´05,  Portland,  Oregon, 
USA, April 2-7, 2005, pp 1475-1478  

6.  Isaacs, E., Walendowski, A., Ranganathan, D.: Hubbub: A wireless instant messenger that 
uses  earcons  for  awareness  and  for  “sound  instant  messenger”.  Proceeding  of  the  ACM 
CHI ´01, Seattle, Washington, USA, March 31 - April 5, 2001, pp 3-4  

7.  Kronlid  F.,  Nilsson  V.:  TreePredict  Improving  text  entry  on  PDA’s.  Proceeding  of  the 

ACM CHI ´01, Seattle, Washington, USA, March 31 - April 5, 2001, pp 441-442  

8.  Lumbreras,  M.,  Sánchez,  J.:  Interactive  3D  sound  hyperstories  for  blind  children. 

Proceedings of the ACM-CHI ’99, Pittsburgh, USA, pp. 318-325 

 

Mobile Messenger for the Blind 

385 

9.  Pakucs  B.:  Butler:  A  Universal  Speech  Interface  for  Mobile  Environments.  Human 
Computer  Interaction  with  Mobile  Devices  and  Services  MobileHCI04,  LNCS  3160, 
University of Strathclyde, Glasgow, Scotland, September 13-16, 2004, pp. 399-403 

10.  Pavlovych A., Stuerzlinger W.: Model for non-expert text entry speed on 12-button phone 
keypads. SIGCHI conference on Human factors in computing systems, Austria, April 2429,
2004, pp 351 – 358 

11.  Rigden    C.:  ‘The  Eye  of  the  Beholder’—Designing  for  Colour-Blind  Users.  Human 

Factors. British Telecommunications Engineering, Vol. 17, Jan. 1999 

12.  Sánchez,  J.,  Lumbreras,  M.,  Cernuzzi,  L.:  Interactive  virtual  acoustic  environments  for 
blind children: Computing, Usability, and Cognition. Proceedings of ACM CHI 2001, pp. 
65-66. Seattle, Washington, April 2-5 

13.  Sánchez,  J.,  Baloian,  N.,  Hassler,  T.,  Hoppe  U.:  AudioBattleship:  Blind  Learners 
Collaboration through Sound. Proceedings of ACM CHI 2003, Fort Lauderdale, Florida, 
April 5-10, pp. 798-799. 

14.  Sánchez,  J.,  Aguayo,  F.:  (2005).  Blind  Learners  Programming  Through  Audio. 

Proceedings of ACM CHI 2005. Portland, Oregon, April 2-7, 2005, pp. 1769-1772 

15.  Sánchez,  J.,  Sáenz,  M.:  3D  Sound  Interactive  Environments  for  Problems  Solving,  The 
Seventh  International  ACM  SIGACCESS  Conference  on  Computers  and  Accessibility  , 
ASSETS, Baltimore, Maryland, USA, 2005, pp 173-179 

16.  Sánchez,  J.,  Sáenz,  M.  (2006).  3D  sound  interactive  environments  for  blind  children 
problem  solving  skills.  Behaviour  &  Information  Technology,  Vol.  25,  No.  4,  July  – 
August 2006, pp. 367 – 378 

17.  Wobbrock  J., Myers  B.,  Aung  H.:  Writing  with  a  joystick:  a  comparison  of  date  stamp, 
selection  keyboard,  and  EdgeWrite.  Proceedings  of  the  2004  conference  on  Graphics 
interface GI '04, Canadian Human-Computer Communications Society, pp.1-8 

   

