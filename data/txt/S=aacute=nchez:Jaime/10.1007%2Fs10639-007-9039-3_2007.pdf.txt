Educ Inf Technol (2007) 12:149–163
DOI 10.1007/s10639-007-9039-3

A model to design interactive learning environments
for children with visual disabilities

Jaime Sánchez

Published online: 23 June 2007
# Springer Science + Business Media, LLC 2007

Abstract Current interactive learning environments cannot be accessed by learners with
disabilities, in particular for students with vision disabilities. Modeling techniques are
necessary to map real-world experiences to virtual worlds by using 3D auditory
representations of objects for blind people. In this paper, a model to design multimedia
software for blind learners is presented. The model was validated with existing educational
software for these users. We describe the modeling of the real world including cognitive
usability testing tasks by considering not only the representation of the real world but also
modeling the learner’s knowledge of the virtual world. The software architecture is also
described by using this model, displaying the components needed to impact learning.
Finally, we analyze critical issues in designing software for learners with visual disabilities
and propose some recommendations and guidelines.

Keywords Modeling methodologies . Children with visual disabilities .
Virtual environments . User adapted interfaces

1 Introduction

A great amount of educational software has been developed for supporting learners with
disabilities. Software for blind people aims to increase their access to current computing
materials based on graphic user interfaces such as games, educational software, and web
navigation systems.

Clearly, the task of developing software for people with visual disabilities has some
complexities. For blind learners, we face the problem of constructing interfaces that do not
rely on graphics. However, we can find some similarities in the process of designing and
constructing computer software for people with different types of disabilities. This is the
case when developing cognitive systems aimed at modeling and implementing the real
world by a computer system.

J. Sánchez (*)
Department of Computer Science, University of Chile, Blanco Encalada 2120, Santiago, Chile
e-mail: jsanchez@dcc.uchile.cl

150

Educ Inf Technol (2007) 12:149–163

Problems in constructing learning software for people with disabilities arise when the
model is presented to the user for interaction purposes. Virtual learning environments for
sighted learners transmit this model to the learners by using graphics (with or without
animation), sounds, and text, taking advantage of a wide spectrum of the computer’s
multimedia capabilities. For people with visual disabilities this spectrum is limited. This
forces the software designers to project the information kept in the model, provided by the
student, through the available auditory channel. Additionally, nontraditional interaction
modes such as haptic devices can be used. The same considerations are valid for the
construction of the learner’s model.

Several virtual reality systems and virtual environments combined with appropriate
human–computer interfaces have been used to enhance sensual capabilities of people with
sensory disabilities. This is the case of presenting graphic information by text-to-speech and
3D auditory navigation environments to construct spatial mental representations and to
assist users in acquiring and developing cognitive skills (Savidis et al. 1996).

A sonic concentration game described by Roth et al. (2000) consists of matching
different pair levels of basic and derived geometric shapes. To represent geometric shapes,
it is necessary to build a two-dimensional sound space. The concept allows the shape to be
rendered by the perception of moving sound in a specific plane. Each dimension
corresponds to a musical instrument, and raster points correspond to pairs of frequencies
on a scale.

AudioDoom (Lumbreras and Sánchez 1999; Sánchez and Lumbreras 1999; Sánchez
2001) allows blind children to explore and interact with virtual worlds by using spatial
sound. The game was based on the traditional Doom game where the player moves through
corridors discovering the environment and solving problems simulated with objects and
entities that inhabit a virtual world.

VirtualAurea (Sánchez 2001) was developed after interactive sound-based virtual
environments proved to trigger the development of cognitive spatial structures in blind
children. VirtualAurea is a set of spatial sound editors that can be used by parents and
teachers to design an ample variety of spatial maps such as the inner structure of the school
and rooms, corridors, and other structures of a house.

The development of mobile applications for users with visual disabilities has been
studied by some researchers. The research project called Blind Interactive Guide System
(Na 2006) proposes an interactive guiding system based on radio frequency identification
technology to provide information needed for navigating inside rooms to the user with
visual disabilities. In another project, Hedgpeth et al. (2005), authors aim to enrich the lives
of people who are blind by developing visually perceptive wearable computers. A PDAsized 
computer receives a video input stream from a wearable camera (which is mounted in
a pair of glasses worn by the user) and communicates what it “sees” through tiny sound
emitters in the earpieces of those glasses. This wearable device can also communicate with
“smart objects” that populate ubiquitous environments.

This paper proposes a model for developing virtual learning environments for learners
with visual disabilities. The model
includes various steps and recommendations by
considering key issues for conceptualization and implementation. Special attention is paid
to the feedback issue considered to be a critical point in existing software. As a result of
using this model in software design and development processes, software products emerge
that contain certain properties that validate a correct impact on visual-impaired student
learning. These properties are classified and outlined in the architecture that
is also
presented. This paper is an extended version of an article presented at the IFIP 19th World
Computer Congress, TC-3 Education stream (Sánchez 2006).

Educ Inf Technol (2007) 12:149–163

151

2 Model

2.1 Development of educational software

We propose a model for creating virtual learning environments for people with visual
disabilities. The modeling process starts with the definition of desired cognitive skills.
Then, we create a virtual environment that includes a navigable world by using adequate
modeling languages, dynamic scene objects, and acting characters. Scenic objects are
characterized by graphic and acoustic attributes; character’s actions are based on
deterministic and nondeterministic plans in the same way as in interactive hyperstories
described by Lumbreras and Sánchez (1999). The learner explores the virtual world by
interacting with appropriate interfaces and obtains interactive feedback. The learner’s
actions, such as sound reproductions, are collected, evaluated, and classified based on
student modeling and diagnostic subsystems. The modeling process follows the steps
illustrated in Fig. 1 (Sánchez et al. 2004).

We define cognitive skills in real-world situations, for example self-motivating activities,
drill and practice applications, problem solving, and entertainment. Objects in fictitious
world scenarios are constructed of geometric primitives. They are characterized by acoustic
attributes and grouped into components with input and output slots. Control elements of the
virtual world are represented by acoustic elements, known as icons and earcons.

We develop an internal computer representation and define a geometric environment
containing a 2D or 3D visual and acoustic model without considering the limitations of
potential users. Modern object-oriented modeling languages are powerful tools for building
virtual worlds using scene graphs, interaction, and animation facilities. We insist on the
necessity of special editors for
teachers and learners to create synthetic models

Fig. 1 A model for designing virtual learning environments for children with visual disabilities

152

Educ Inf Technol (2007) 12:149–163

independently. According to the problem and target users, problem-specific correspondences 
between graphic and acoustic attributes or properties can be used to reduce the
model to its acoustic projection. The resulting model meets the requirements in terms of
certain information channels that should not be used in such a way that cannot be explored
by people with visual disabilities. Another way is to generate the model directly by using a
special editor for learners with visual disabilities.

The acoustic representation of the model for learners with visual disabilities uses spatial
sound. Interaction and navigation are based on acoustic control elements. The learner
explores the object space by navigating, interacting with suitable interfaces, interpreting,
and reproducing the structures. This can be done through navigating without changing
viewpoints or by using internal representations of users by giving them the illusion of being
part of the virtual scene. A blind learner explores neighboring models by grasping them,
tracking objects or listening to typical sounds. Sound-emitting objects help them to build a
mental model of real or fictitious worlds. Additionally, the learner may build an external
reconstruction of the mental model or try to rebuild the acoustic model as it was perceived
and imagined after exploring the model space. We must be certain that conditions during
the reconstruction process are always the same. Therefore, the interfaces involved are
calibrated accordingly.

Depending on the particular parameters and entities of the model, error measures
between internal representations and the model reproduced by the learner are defined. The
learner’s actions are collected, evaluated, and classified. The outcome is transformed to a
user-adapted aural output.

The use of this model in the development of virtual learning environments for users with
visual disabilities ends up with software containing representations and functionalities that
meet
the visually impaired user requirements. To achieve this, we have defined an
architecture to identify which elements of virtual learning environments are essential for a
correct impact on the user’s learning.

To represent the real world, we defined the virtual environment and scenes that users
navigate. We established game rules and how different metaphors (scripts) connect to each
other. The world was designed as a collection of scenes grouped in different zones with
common characteristics (cities, forests, etc.).

The description of the user cognitive impact was made by defining the minimum tasks
set to be accomplished by the student, which resulted in software requirements (navigate
some scenes, use orientation resources, solve certain problems, etc.). According to these
requirements, different content was shown to the user, using a quest or mission-to-fulfill
metaphor, exploring and interacting with the virtual world.

The development process consisted of the computer implementation of the world and the
user’s models. We used an incremental model in such a way that when the first cycle ended,
the user could navigate some zones created in a coherent and simple manner. Thus, in each
cycle, new zones, characters, and functionalities were added. The projections for different
models were designed and new information inputs and outputs flowed from and to the user,
mainly using 3D sounds.

A usability evaluation was implemented to obtain data about the users’ acceptance of the
design model embedded in a prototype and whether or not it represented their mental
model. We also evaluated the accomplishment of the proposed cognitive tasks. Three
instruments were considered for usability evaluation: end-users’ evaluation, heuristic
evaluation, and facilitators’ evaluation. Below, we discuss the main results obtained in the
software usability evaluation. The data obtained were analyzed and studied to figure out
how the metaphors, models, and projections could be improved (Fig. 2).

Educ Inf Technol (2007) 12:149–163

153

Fig. 2 Architecture of the resulting software

2.2 Modeling the resulting architecture

Metaphor of the real world (model) According to the cognitive skills to be developed, real
world metaphors are designed, as well as the activities that learners have to attain the
cognitive goals by considering their interest and motivation.

Editor The editor contains tools to construct an internal model based on 2D/3D graphic
representations or auditory representations. For learners with visual disabilities, models can
be generated by drag and drop actions on icons and images from a gallery. The equivalent
representation of entities is in a text form or phonetic transcription. Children with visual
disabilities choose the internal world objects through sensitive tablets or haptic devices.

Computer representation of the real system The computer representation of the real system
corresponds to the computer representation of the problem or the real world metaphor; it is
knowledge modeling. At this point are functions, parameters, and variables of the state of
the system describing the situation of the represented world and how the transition from one
state to another will be made by considering the interaction of the learner with the software
and reflected on entry variables.

Strategy This component provides the strategies used to model the state of the learner’s
knowledge. They are taken from the field of artificial intelligence applied to intelligent
tutoring. One of them is the overlay model (Kass 1989), which treats the learner’s
knowledge as a subset of an expert knowledge. The differential model (Clancey 1987)
extends the previous model by dividing the learner’s knowledge into two categories:
knowledge that the learner should know and knowledge that is not expected to be known

154

Educ Inf Technol (2007) 12:149–163

by learners. The perturbation model (Kass 1989) supposes that learners should possess
potentially different knowledge in quantity with respect to an expert. The model can
represent the knowledge and beliefs of the learner beyond the ranks of the expert’s model.

Learner model This component represents what the system thinks about the state of the
student’s learning at a certain point. It contains knowledge and skill representations the
learner should construct, the variables of the state of the learner representing the level of
learning at a certain moment, and the rules about how to upgrade this information given the
interaction with the system and reflected in the change from one real world model state to
another. Thus, the learner model is constructed by making inferences from individual
knowledge and by analyzing the performance (Dillenbourg and Self 1992).

Evaluation This component defines the difference between the knowledge model
represented in the software and the knowledge model of the learner generated by the
strategies. Thus, an error measure is produced and projected to the interface as student
feedback.

System projection This is the main component to certify that the software can be fully
assimilated by children with visual disabilities. It
is in charge of projecting most
interactions, state variables, and feedback from and to the software. Below, we present
the attributes to adequately develop a learning virtual environment for children with visual
disabilities.

During our research study, we applied this model to evaluate current software for blind
learners. To accomplish this, we grouped the main components of this model into four
heuristics: metaphor, learning, interaction, and interfaces. When applying these heuristics,
we found strengths and potential errors in software interaction, especially in meeting the
uniqueness of this type of application for blind learners. One example of this evaluation can
be found in Sánchez and Baloian (2005).

Some software previously designed in our research was modeled with this architecture
(Sánchez and Sáenz 2005). For example, the software called Theo and Seth (see Fig. 3a) is
an application to enhance mathematics skills in blind users comprising the following
components: model, strategy, computer representation, model of
learner, evaluation,
learner, and interface. In model, according to the cognitive skills to be developed, diverse
content is presented to the learner by using a metaphor consisting of a farm containing
different elements to help children to solve arithmetic problems. In strategy, Theo and Seth
has embedded a strategy to solve the problems posed that are randomly presented but
deterministic. Computer representation is developed with a functioning methodology and
global variables, allowing coherence to be maintained between results and exercises
presented to the user. Model of learner is the part of the system in charge of saving the
answers of the user and provides the grading results of the problems solved by the learner.
Evaluation evaluates the student’s learning by analyzing the final results obtained by him or
her and comparing them with the correct results. Learner obtains feedback from the system.
Interface is the main component and receives and interprets the parameters entered by the
user through the keyboard by certifying whether the system can be understood by a blind
user. It is in charge of generating the projection of all interactions, state variables, and
feedback from and to the system. The output is represented through audio and images.

Other software called AudioChile and AudioVida (see Fig. 3b, c) are virtual environments 
that can be navigated through 3D sound to enhance spatiality and immersion

Educ Inf Technol (2007) 12:149–163

155

Fig. 3 Screenshots of software developed based on the proposal, a Theo and Seth, b AudioChile, and
c AudioVida

throughout the environment (Sánchez and Sáenz 2006). This software was developed based
on the model described above. Each software has a model, editors, knowledge
representation model, strategy, evaluation, and projection. The model
(metaphor)
represents labyrinths. A person has to navigate through these labyrinths searching for cues
and information to solve a problem posed. Editors consist of the virtual world, including the
definition of its properties, objects, and personages. The knowledge representation model is
based on the mapping of real world behaviors in a virtual world, state variables (physics,
kinetics, and luminous), and actions. The strategy is in charge of following up the actions
performed by the user. AudioChile provides a compass that perceives the user’s orientation
within the virtual world. AudioChile provides different types of food for interaction. The
child must interact with the food and decide whether or not to eat it. The learner model is
such that, as the user goes through the adventure, he/she perceives and processes more
information to solve the problem to reach the end of the story (AudioChile) or to get the
maximum score (AudioVida). The evaluation in AudioChile consists of an evaluation of the
compass to provide information to the user to find diverse objects and personages within
the virtual world. AudioVida evaluates decisions of learners during interaction with the
game. Learners get high scores when taking good decisions and low scores with erroneous
decisions. Projection consists of 3D images and sound virtual worlds. It explores diverse
fine representation qualities to get a high standard of use as well as to solve the problem
posed.

156

2.3 Workflow

Educ Inf Technol (2007) 12:149–163

AudioDoom was used as an exemplary application for blind children. Thus, we validated
our model by describing in more detail the modeling workflow. The whole testing is
described by Baloian et al. (2002). AudioDoom allows blind children to explore and
interact with virtual worlds through spatial sound (Lumbreras and Sánchez 1998, 1999).
AudioDoom has been usability tested with more than 40 Chilean children aged 7–12 years
in school settings for blind children (Sánchez 2001).

the computer-based model

We followed the modeling steps introduced above. First, a computer-based representation 
is derived from the real world scenario by means of abstraction and reduction without
considering the limitations of potential users. Then,
is
reproduced (projected) as an appropriate acoustical internal model explored by people
with sensory disabilities through the use of available communication channels. Appropriate
editors support the modeling process. Important model entities and parameters must be
identified at this stage. By interacting with the model, the learner makes an internal or
external reconstruction, which is later evaluated (see Fig. 4a, b). This can be done by using
appropriate error measures depending on the learner,
the computer-based (internal)
representation, and the reconstructed model. Finally, the degree of similarity is derived
from the error measure and the result is displayed on some learner-adapted output and used
for modeling the learner’s knowledge.

The modeling workflow in AudioDoom starts with the computer representation of the
virtual game world (a simple labyrinth with one main corridor and two secondary corridors
including entities and objects), which is projected to an internal model consisting only of
sounds. At this stage, the role of volume, frequency, melody, and rhythm in representing
different forms, volumes, and distances is analyzed. Learners interact with this model by
“virtually walking” through the labyrinth with keyboard, mouse, and ultrasonic joystick.
Sound-emitting objects help them to build a mental model of the labyrinth. Finally, they
make concrete mental models with Lego blocks and try to rebuild the internal model as it
was perceived and imagined after exploring the spatial structure. Different types of blocks
represent objects of the virtual world in the computer representation.

The concrete reconstruction is checked by a human tutor against the game world or by a
camera and image processing algorithms to look for any spatial correspondence with the
computer representation of the original world model by evaluating the error measure. As we
can see, despite object representation, interfaces, perception modes, and error measures,
there are important tasks that should be undertaken when developing systems for blind

Fig. 4 a Virtual game world embedded AudioDoom, b reconstructed external model by blind learners

Educ Inf Technol (2007) 12:149–163

157

people. A critical task in this modeling process is the reduction of the original model to
only acoustic output.

2.4 Testing

We can describe in more detail two steps of the modeling process. To process an external
model and to evaluate error measures, we assume that a blind child reconstructs
AudioDoom’s maze structure by using Lego blocks. Each block is individually marked
by black bars and dots. A blind user perceives different blocks, selects the appropriate ones,
and rebuilds the mental image by using the blocks one by one.

After each step, a picture is taken with a digital camera placed on a fixed position over
the scene. We highlight a typical state of the reconstruction process and indicate the next
step by adding a new Lego block on the lower wall. Figure 5 shows the situation before and
after the next step. Reducing the colors to black and white makes it possible to apply lowlevel 
image processing routines to detect the new Lego block. After a calibration of the two
pictures, we can localize the new block through an exclusive-or operation on both images
resulting in one or two dashes, or in new dots otherwise. Starting from these new picture
elements, we calculate the position and type of the new Lego block. Finally, the learner’s
model representation is updated.

Thus, by certain low-level operations on succeeding pictures the external model can be
transformed into an internal representation, which is used to feed an error measure function. A
degree of fidelity is derived and displayed by text-to-speech. For a more sophisticated
approach using Lego Robotic Command Explorer robots, see Ressler and Antonishek (2001).
In AudioDoom, the differences concerning parameters such as volume, articulation,
stress, and intonation must be visualized. From these visual patterns, important parameters

Fig. 5 Reconstructed Lego
model

158

Educ Inf Technol (2007) 12:149–163

are detected by means of picture processing and in connection to the stored reference
patterns. An intelligent component of the system interprets these results, displays a
visualization of the error function, and transforms them into instructions for the learner.
Whereas the wave amplitude image provides, in the xy-plane, any feedback concerning
volume and rate, omitted syllables are characterized by a lack of colored areas, intonation,
and articulation by the hue and saturation parameters, as well as the shape of the curves.
AudioDoom is manipulated by using a wireless ultrasonic joystick, called The Owl. By
using this device, the child can interact and move through the environment by clicking in
different voxels of the surrounding space. According to the sound position, the child must
coordinate the haptic/kinesthetic device with the perceived sound position. This scheme of
action–reaction is strongly stimulated in the child because of the strong haptic–acoustic
correlation embedded in the system. To deal with this issue, we designed AudioDoom to be
mainly used with an ultrasonic joystick with three degrees of freedom and spatialized sound
(see Fig. 6).

For designing and developing information-equivalent interfaces for children with visual
disabilities, we followed a similar model workflow as described above in two new systems.
AudioBattleShip (Sánchez et al. 2003) is an interactive version of the board game
Battleship, providing different interfaces for both sighted and blind people to enhance
collaboration and cognition (see Fig. 7). Ebbinghaus’s forgetting function experiments were
implemented in a project concerning the historical replication of key experiments in
psychology (Biella et al. 2003).

AudioMemory (Sánchez and Flores 2004) is software based on the classic memory game
board where the user interacts through keyboard, joystick, and tablets. A few keyboard
keystrokes are used. These devices have been used in different applications previously
developed by the research group after usability testing them with children with visual
disabilities. Microsoft SideWinder, a force feedback joystick, was used. This tool allows
grading the user position in the grid and provides direct feedback with different forces.
Counter forces to the movement are generated per token position change and vibratory
forces indicate that the user is near the grid edge: up, down, left, and right. Force feedback
joysticks allow direct interaction with diverse degrees of freedom. A plastic graphic grid is
posed on the tablet to define the position of each token. A pen is used to point and select
interface elements (see Fig. 8).

Fig. 6 AudioDoom input device
for blind players

Educ Inf Technol (2007) 12:149–163

159

Fig. 7 AudioBattleShip input device for blind players

3 Guidelines and recommendations

Virtual learning environments can be used to simulate features of the real world not physically
available to users due to a wide variety of reasons. They become more realistic through
multimedia displays that include haptic and auditory information. According to Colwell et al.
(1998) and Paciello (1998), there are several domains in which virtual learning environments 
can be used to build educational software for people with visual disabilities:

(a)

In education, a virtual laboratory assists students with physical disabilities in learning
scenarios. Possible applications concern problem solving, strategic games, exploring spaces
or structures, and working with concrete materials. Special virtual learning environment
interfaces, such as head-mounted devices, the space mouse, and gloves, are often included.
(b) Training in virtual learning environments deals with mobility and cognitive skills in

spatial or mental structures.

(c) Rehabilitation is possible in the context of physical therapy—a recovery of manual

skills or learning how to speak and listen to sound can be targeted.

(d) Access to educational systems is facilitated via dual navigation elements such as

earcons, icons, and haptic devices.

Our idea is to support learners with visual disabilities in building conceptual models of
real-world situations as sighted users do. Our approach is comparable to that introduced by
Zajicek et al. (1998). We can identify five important common elements and aspects in
modeling for virtual learning environments for children with visual disabilities:

1. The conceptual model is a consequence of mapping the real or fictitious world situation
into a computer model using digital media by applying adequate modeling languages.

160

Educ Inf Technol (2007) 12:149–163

Fig. 8 AudioMemory input devices 
for blind players

2. The perceptual model is created by developing a perceptual model and a script for
dynamic changes of the model. It can be perceived by the learner using only available
information channels and considering the type of disabilities of end-users. It
is
important to provide surprising elements that call attention and enhance the perception
process. The computer model description should be based on text. Explanation of
graphic objects should be given in caption form. This text can be presented to the blind
by using a text-to-speech plug-in and a Braille display. Intuitive correspondences
between graphic and aural objects must be defined. Attention must be paid to the fact
that only a small number of sounds can be memorized. Also, melodies that help to
identify objects should be used.

3. The implementation design provides icons and earcons in parallel. If there are animated
image sequences or video with sound, subtitles and moving text-banners should be
used. Sound provides a rich set of resources that complement visual access to a virtual
world. The four types of audio examined by Ressler and Wang (1998) are ambient
background music, jingles, spoken descriptions, and speech synthesis. Ambient music
can play and vary as the user moves from one room to another, providing an intuitive
location cue. Jingles or small melodies should characterize special objects and control
elements. Spoken descriptions of objects can play as the viewer moves closer to an
object. Speech synthesizers can read embedded text from the nodes in a scene graph
representation. Recent Web languages provide anchor node descriptions, EnvironmentNodes,
 or WorldInfo nodes. Internet-accessible speech synthesizers supply easy
access to text-to-speech technology. Icons should be simple and big, with clear
messages and fine color contrast with backgrounds. They should be easily interpreted
by users through voice and sound to orient them and help them to understand their
meaning. Buttons should be big and simple with associated voices and sounds for
children with visual disabilities to orient them and map their function.

Educ Inf Technol (2007) 12:149–163

161

4. The implementation tools concerning special editors or languages such as Java,
Java3D, Virtual Reality Modeling Language (VRML), OpenGL, and DirectX should
be used. VRML defines a standard language for representing a directed acyclic graph
containing geometric information nodes that may be communicated over the Internet,
animated, and viewed interactively in real-time. VRML also provides behavior, user
interaction, sensors, events, routes, and interpolators for interactive animation. User
interaction, time-dependent events, or scripts can trigger dynamic changes in some
properties of an object. VRML viewers are available not only as plug-ins to Internet
browsers, but also as interactive objects that may be embedded in standard Office
documents. However, the actual version does not yet support collaborative scenarios.
As shown by Sánchez et al. (2003), a special platform for replicated collaborative
applications such as MatchMaker (TNG) together with Java was successfully applied.
This is a summarizing description of some tools that are currently available. The study
focuses on those that are platform-independent and based on international standards.
5. The interface consists mainly of fonts, images, colors, icons, buttons, and audio. Fonts
should be sans serif, avoiding the use of roman or serif typography, unless uppercase
letters are used. Decorative fonts should be avoided. Size should be 18 points and up
with medium line spacing and background contrasting colors. The best contrasting
colors for people with low vision are yellow/blue and black/white. Images should have
good contrast with backgrounds. They should be simple, clear, and precise, in such a
way that users can easily recognize them with their residual vision. Iconic images
should be preferred instead of photographic representations. Strong and contrasting
blue and yellow spectrum colors should be used. These colors are better perceived by
children with vision loss because they are diametrically opposed in both chrome and
luminescence. Audio is critical in software for children with visual disabilities. It is the
most relevant interface element in this type of software. Good quality audio should be
included and clearly identified as to how and when it should be used for educational
software purposes. Most interaction events should include audio such as to describe
icons, buttons, menus, and contexts. Voices should be used depending on the target
user. Software for children with visual disabilities should use interesting, motivating,
and pleasant voices. Content refers to whenever software is designed for people with
visual disabilities; the instructions should be audible. The interface for children with
low vision should include clearly defined word messages with directions. Verbal
directions should be clear, brief, and precise because the user should remember them
when interacting with the software. Users should have the option to relisten to verbal
directions when needed.

4 Conclusion

This paper presents an integrated model to design virtual learning environments for learners
with disabilities. We provide an overview of the state-of-the-art in the field of existing
educational software for learners with visual disabilities.

We focus on transferring the real world into appropriate computer representations by
introducing an integrated methodology for modeling the real world for these people. The
model starts with the definition of targeting cognitive skills. Then, a virtual environment is
created including a navigable world through the use of a modeling language, dynamic scene
objects, and acting characters. The learner explores the virtual world through appropriate

162

Educ Inf Technol (2007) 12:149–163

interfaces and obtains immediate feedback. Actions such as sound reproductions are
collected, evaluated, and classified based on student’s modeling and diagnostic subsystems.
The usability of the model is illustrated with virtual learning environments for blind
children. We believe that by using models such as the one presented here, the process of
software design can be facilitated and improved.

Some virtual learning environments have already been developed for learners with visual
disabilities. Then, by following our model, we can make generalizations and recommendations 
for designing and improving educational software. The defined architecture for the
proposed model allows identifying software components that need to be considered to
assure a correct impact on the learning of visual impaired users. We have identified each of
these components in the implemented software, which helped us to understand the user’s
interaction with other software elements. Likewise, this allowed us to get an abstraction
about how the user learns when using virtual environments. This is a necessary analysis to
assure correct and effective application of the educational software.

The development of computer systems for learners with visual disabilities should no
longer appear as isolated handcrafted efforts. Rather, efforts should be made to systematize
the construction of these systems. Recent advances in hardware and software design
support this idea and provide hope that the technological and educational foundation for
such systems has already been laid.

In this paper, we present interfaces to provide user interaction and feedback. Mostly they
are either haptic or sound-based interfaces. Current research has highlighted that the use of
spatial sound provides an added value to educational software, allowing users to make a
precise mapping of the virtual environment and thus improving the interaction.

Guidelines and recommendations for software design and implementation for users with
visual disabilities have been included. Their correct use will assure adequate tuning
between the software and methods of user interaction. Guidelines were designed for all
abstraction levels in the process of software design and development, involving problem
modeling (in a real or fictitious situation), I/O interfaces, and implementation (design of
virtual environments). In other words, this model can be used to improve and extend the use
of certain software to assist the learning of learners with visual disabilities.

Finally, we expect to contribute to the field with an explicit and functional model that
can be generalized and replicated to help improve the learning and cognition of users with
visual disabilities.

Acknowledgement This report was funded by the Chilean National Fund of Science and Technology,
Fondecyt, Project 1060797.

References

Baloian, N., Luther, W., & Sánchez, J. (2002). Modeling educational software for people with disabilities:
Theory and practice. In Proceedings of ASSETS 2002 conference (pp.111–118). July 8–11, 2002,
Edinburgh.

Biella, D., Luther, W., Mietzel, G., Musahl, H.-P., & Wormuth, L. (2003). Replication of classical
psychological experiments in virtual environments (Replicave). Proceedings of ECEL03, October 6–7,
2003, Glasgow.

Clancey, W.J. (1987). Methodology for building an intelligent tutoring system. In G. Kearsley (Ed.), Artificial

intelligence and instruction—applications and methods (pp. 193–227). Reading: Addison-Wesley.

Colwell, Ch., Petrie, H., Kornbrot, D., Hardwick, A., & Furner, S. (1998). Haptic virtual reality for blind
computer users. In Proceedings of the third annual acm conference on assistive technologies (pp. 92–99).
April 15–17, 1998, Marina del Rey.

Educ Inf Technol (2007) 12:149–163

163

Dillenbourg, P., & Self, J. (1992). A framework for learner modeling. Interactive Learning Environments, 2

(2), 111–137.

Hedgpeth, T., Rush, M., Black J. A., & Panchanathan, S. (2005). The iCARE iReader Project. CSUN’s 20th
Annual International Conference “Technology and Persons with Disabilities”. March 14–19, 2005, Los
Angeles, CA.

Kass, R.(1989). Student modeling in intelligent tutoring systems—implications for user modeling. In A.

Kobsa & W. Wahlster (Eds.), User models in dialog systems (pp. 386–410). New York: Springer.

Lumbreras, M., & Sánchez, J. (1998). 3D aural interactive hyperstories for blind children. International

Journal of Virtual Reality, 4(1), 20–28.

Lumbreras, M., & Sánchez, J. (1999). Interactive 3D sound hyperstories for blind children. Proceedings of

ACM CHI 1999 (pp. 318–325). Pittsburg.

Na, J. (2006). The blind interactive guide system using RFID-based indoor positioning system. Proceedings

of the ICCHP (pp. 1298–1305). LNCS 4061: Austria.

Paciello, M. (1998). Making the Web accessible for the deaf, hearing and mobility impaired. In Yuri Rubinsky

Insight Foundation. Retrieved from http://xml.coverpages.org/yuriInsightFound.html.

Ressler, S., & Antonishek, B. (2001). Integrating active tangible devices with a synthetic environment for
collaborative engineering. In Proceedings of the 2001 Web3D Symposium (pp. 93–100). Retrieved from
http://www.itl.nist.gov/iaui/ovrt/people/sressler/tangible3.pdf.

Ressler, S., & Wang, Q. (1998). Making VRML accessible for people with disabilities. In Proceedings of
ACM ASSETS 1998, the Third Annual ACM Conference on Assistive Technologies (pp. 144–148).
Retrieved from http://www.itl.nist.gov/iaui/ovrt/people/sressler/vrmlsigcaph.pdf.

Roth, P., Petrucci, L., Assimacopoulos, A., & Pun, Th. (2000). Concentration game, an audio adaptation for

the blind. In CSUN 2000 Conference Proceedings. Los Angeles: California State University.

Sánchez, J. (2001). Interactive virtual acoustic environments for blind children. In Proceedings of ACM CHI

2001 (pp. 23–25). Seattle, April 2–5, 2001.

Sánchez, J. (2006). A model to design multimedia software for learners with visual disabilities. Education for
the 21st century—impact of ICT and digital resources, IFIP 19th world computer congress, TC-3
education (pp. 195–204). August 21–24, 2006, Santiago.

Sánchez, J., & Baloian, N. (2005). Modeling audio-based virtual environments for children with visual
the world conference on educational multimedia, hypermedia &

disabilities. In Proceedings of
telecommunications ED-MEDIA 2005 (pp. 1652–1659). June 27–July 2, 2005, Montreal.

Sánchez, J., Baloian, N., & Flores H. (2004). A methodology for developing audio-based interactive
environments for learners with visual disabilities. In Proceedings of the world conference on educational
multimedia, hypermedia & telecommunications ED-MEDIA 2004 (pp. 540–545). June 21–26, 2004,
Lugano.

Sánchez, J., Baloian, N., Hassler, T., & Hoppe, U. (2003). Audiobattleship: blind learners collaboration

through sound. In Proceedings of ACM CHI 2003 (pp. 798–799). April 5–10, 2003, Ft. Lauderdale.

Sánchez, J., & Flores, H. (2004). Memory enhancement through audio. Published in the proceedings of the sixth
international ACM SIGACCESS conference on computers and accessibility, assets 2004 (pp. 24–31).
October 18–20, 2004, Atlanta.

Sánchez, J., & Lumbreras, M. (1999). Virtual environment interaction through 3D audio by blind children.

Journal of Cyberpsychology and Behavior, CP&B 2(2), 101–111.

Sánchez, J., Sáenz, M. (2005). Developing mathematics skills through audio interfaces. In Proceedings of
11th International Conference on Human–Computer Interaction, HCI 2005. July 22–27, 2005, Las
Vegas.

Sánchez, J., & Sáenz, M. (2006). 3D sound interactive environments for blind children problem solving

skills. Behaviour & Information Technology, 25(4), 367–378.

Savidis, A., Stephanidis, C., Korte, A., Crispie, K., & Fellbaum, K. (1996). A generic direct-manipulation
3D-auditory environment for hierarchical navigation in non-visual interaction. In Proceedings of ACM
ASSETS 96 (pp. 117–123).

Zajicek M., Powell C., & Reeves C. (1998). A web navigation tool for the blind. In Proceedings of ACM

ASSETS’98, the 3rd ACM/SIGCAPH on assistive technologies (pp. 204–206). Los Angeles.

