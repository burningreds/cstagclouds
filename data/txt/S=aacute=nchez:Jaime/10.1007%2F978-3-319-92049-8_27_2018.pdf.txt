Cognitive Impact Evaluation of Multimodal

Interfaces for Blind People: Towards

a Systematic Review

Lana Mesquita1(&), Jaime Sánchez2, and Rossana M. C. Andrade1

1 Department of Computer Science, Universidade Federal do Ceará (UFC),

Fortaleza, CE, Brazil

lanamesquita@great.ufc.br, rossana@ufc.br

2 Department of Computer Science, Universidad de Chile, Santiago, Chile

jsanchez@dcc.uchile.cl

Abstract. Visual disability has a major impact on people’s quality of life.
Although there are many technologies to assist people who are blind, most of
them do not necessarily guarantee the effectiveness of the intended use. Then,
we have conducted a systematic literature review concerning the cognitive
impact evaluation of multimodal interfaces for blind people. We report in this
paper the preliminary results of the systematic literature review with the purpose
of understanding how the cognitive impact is currently evaluated when using
multimodal interfaces for blind people. Among twenty-ﬁve papers retrieved
from the systematic review, we found a high diversity of experiments. Some of
them do not present the data clearly and do not apply a statistical method to
guarantee the results. Besides this, other points related to the experiments are
analyzed. We conclude that there is a need to better plan and present data from
experiments on technologies for cognition of blind people. Moreover, as the
next step in this research, we will investigate these preliminary results with a
qualitative analysis.

Impact evaluation  Cognitive evaluation  Multimodal interfaces

Keywords:
Blind people

1 Introduction

Visual disability has a major impact on the quality of life of people who has it,
including their ability to study, work and to develop personal relationships [1]. In this
aspect, technologies, such as serious games [2], have been designed to assist people
who are blind to support daily life activities. These technologies work as aids to
facilitate their independence, autonomy, and safety. Thus, such technologies improve
the quality of life of people with visual disabilities and could stimulate and develop
several skills, such as cognitive skills [3].

Even though there is technology specialized for blind people (i.e., visually
impaired), they are still using applications that are similar to older applications for the
sighted population. For example, Battleship was one of the earliest games to be produced 
as a computer game with its release in 1979 [4]. AudioBattleShip, a version for

© Springer International Publishing AG, part of Springer Nature 2018
M. Antona and C. Stephanidis (Eds.): UAHCI 2018, LNCS 10907, pp. 365–384, 2018.
https://doi.org/10.1007/978-3-319-92049-8_27

366

L. Mesquita et al.

both blind children and sighted playing together came many years later [5]. In general,
blind people have particular human-computer interaction needs, and the user interface
should be suitable for them.

There are many efforts towards to develop accessible multimodal interfaces for
visually impaired, especially in multimodal games [6, 7]. Despite this effort and in
contrast to the visual interface evolutions of games and applications for sighted people,
interfaces for people who are blind explore other ways to interact with the user. In
general, technologies for people with visual disabilities combine different sources of
perceptual inputs and outputs. The modes (sources of perceptual inputs and outputs)
combined, typically audio and haptics [5, 8], provide multimodal interfaces that enable
multimode channels for the combination of different user senses [2]. Although multimodal 
interfaces could help to improve the learning skills of people with visual disabilities,
 most of these technologies have been not completely validated; mostly, they
remain in the prototype phase without being integrated into the people’s everyday life [9].
In relation to the quality of applications, the No Child Left Behind (NCLB) Act
deﬁned that research in inclusive education must (a) utilize the scientiﬁc method, (b) be
replicated in more than one set by more than one investigator, and, (c) result in ﬁndings
that converge to a clear conclusion [10]. Thus, some studies in this area use
Evidence-Based Practice [5, 11], which meets prescribed criteria related to the research
design, quality, quantity, and effect size of supporting research [12, 13]. Thus, this
method provides the measurement of the effectiveness of using technology.

Considering these multimodal interactions with interfaces for blind people, it is
necessary to verify if the technologies thought for them are effective and how they
impact users in cognitive dimensions [14]. Effective impact evaluation, which has been
used as evidence-based in other domains for users with and without disabilities, should,
therefore, be able to assess precisely the mechanisms by which people with visual
disabilities are developing or enhancing cognitive skills [3]. According to Darin et al.
(2015), there is a gap of studies proposing instruments and methods for evaluating the
cognitive impact in the context of multimodal video games for cognitive enhancement
of people who are blind. In general, the literature studies do not follow guidelines to
support cognitive impact evaluation of multimodal interfaces for blind people.

In pursuing to shed some light concerning this, our work presents

the
state-of-the-art study on cognitive impact evaluation of multimodal interfaces for blind
people with profound inability to distinguish light from dark, or the total inability to see
[15]. This state-of-the-art study consists of a systematic review to analyze how studies
evaluate the cognitive impact in this context. The systematic review is part of an
ongoing work on the design of guidelines for evaluating the cognitive impact development 
and enhancement in multimodal interfaces for blind people. As related literatures,
 some works also propose guidelines for other concepts in the context of
accessibility. As an example, the study [16] deﬁnes games accessibility guidelines
helping developers to design their products with the interface and the parameters
adapted to the needs of users with disabilities.

The remainder of this paper is organized as follows: Sect. 2 includes the Theoretical 
Background; Sect. 3 presents the methodology used in this work; Sect. 4 covers
the results of the systematic review; Sect. 5 discusses the results; and, ﬁnally, Sect. 6
concludes the study.

Cognitive Impact Evaluation of Multimodal Interfaces

367

2 Background

2.1 Cognitive Impact Evaluation

There are technological aspects to these systems which are investigated from the
perspective of how they affect use [17]. Cognitive impact concerns the interaction
between humans and systems. The ﬁeld of human-computer interaction has pioneered
in the formal study of the cognitive relationship between a person’s activities, the
artifact of the computer, and the task [18]. The technologies could enhance human
cognitive capabilities.

Darin et al.’s (2015) literature mapping study shows the cognitive process analyze
applications according to a four-dimensional classiﬁcation (Interface, Interaction,
Cognition, and Evaluation). The evaluation dimension includes two main aspects:
usability and cognitive impact. This last one assures that an application can develop or
enhance any cognitive skills for people with visual disabilities.

Still on this study, the cognition dimension comprises six skills: mental models,
mental maps, spatial structures, Orientation and Mobility (O&M), problem-solving,
and social collaboration. Such approach addresses the main cognitive skills developed
and enhanced for impact evaluation purposes. These dimensions could provide
directions to deﬁne tasks in an experiment to measure the cognitive impact as detection
of some obstacles, a useful data for evaluating O&M [19].

The study also shows that most papers classiﬁed in main cognitive skills are about
Mental Map and O&M. The O&M skill is a broad concept that is also related to
wayﬁnding and navigation. According to Pissaloux and Velázquez (2018, p. 1),
“Human mobility is one of the most important cognitive tasks. Indeed, independent and
secure mobility in real physical space has a direct impact on the quality of life, on
well-being, and on integration in the numeric society.”

Darin et al.

[19] deﬁne mobility in a four-dimensional problem: walking,
wayﬁnding (or orientation), space awareness (or space knowledge) and navigation.
According to this deﬁnition, walking is a low conscious cognitive task and involves
displacement in the near space. It takes in account obstacle detection and localization.
The wayﬁnding is a set of processes to know one’s current position in space to reach
one’s target. The space awareness requires a high consciousness level. It includes
forming mental maps, e.g., know the name of the street on a plan. The navigation, the
highest level cognitive task, is a result of the implementation of all listed above
functions while traveling.

2.2 Experiment in Software Engineering

The impact evaluation of a software is an experiment process and includes several
steps: Scoping; Planning; Operation; Analysis and interpretation; Presentation and
package [20]. This provides a high level of control, using a formal, rigorous and
controlled investigation.

The main concepts involved in the experiment, shown in Table 1, are used to
understand how the cognitive impact is evaluated and to conduct the designing of the
guidelines. Figure 1 shows how these concepts are related to the experimental process.

368

L. Mesquita et al.

Concepts
Measure
Instrumentation

Dependent
variables
Independent
variables
Factors

Table 1. The main concepts of experimental design [20]

Description
A mapping from the attribute of an entity to a measurement value
The instruments for an experiment are of three types, namely objects,
guidelines and measurement instruments
The dependent variables are those we want to see the effect; the
independent variables are those controlled and manipulated
The independent variables are those controlled and manipulated

The independent variables which the experiment changes to verify the
effect. Treatment is one value of a factor

Fig. 1. Variable relationship in the experiment process (Source: [20]).

As an example of experiment process, we consider the measurements, instrumentation,
 and variables from the experiment conducted in [21]. In this study, the authors
evaluate the navigational performance the of virtual environment called Audio-based
Environment Simulator (AbES) that can be explored for the purposes of learning the
layout of an unfamiliar, complex indoor environment. The dependent variable evaluated 
was the navigation performance (Orientation & Mobility, O&M).

Some information about the participants is controlled and works as independent
variables such as etiology of blindness, age, gender, hand preference, and verbal
memory (assessed by using the Wechsler Memory Scale). These variables are controlled 
and ﬁxed to ensure the correct measurement. The factors in the experiment are
the age of blindness onset and the interaction condition with AbES. The factors are
randomly distributed into groups: early blind and late blind; and gamers, directed
navigators, and control group. The measurements variables used are task success,
navigation time, and shortest possible path score.

3 Methodology

The methodology consists in a Systematic Review [22], which aims to review the
existing evidence concerning the impact evaluation of multimodal interfaces and also
seeks to summarize the empirical evidence concerning the strengths and limitations of a

Cognitive Impact Evaluation of Multimodal Interfaces

369

speciﬁc evaluation method [22]. In contrast to an ad hoc literature review, the systematic 
review is a methodologically rigorous analysis and study of research results.
To achieve our goal, the main research question for this ﬁrst part of the proposal
was “How is the cognitive impact evaluated on multimodal interfaces for people who
are blind?” For a better understanding, as a second goal question, we aim to learn the
challenges regarding impact evaluation on this scenario.

The process of a systematic review includes three main phases: planning the
review; conducting the review and reporting the review [22]. During all process of the
systematic review, we used the tool StArt [23] and the software Microsoft Excel1 as a
support to create the protocol, apply the ﬁlters, select the papers and show the results.
We organize all references on software Mendeley2. As the papers retrieved from
PubMed Central are in MEDLINE format, we developed the tool Medline2bibtex3. It
works as a parser to permit the list to be read by both StArt and Mendeley.

The next subsections describe the planning (the study selection criteria, the research
sources selected) and the conducting phase (the search Process, the data extraction form
ﬁelds and the studies quality evaluation). The entire process was stored in an excel
worksheet available online4.

3.1 Planning: Deﬁnition of the Protocol

In the planning phase, we deﬁne a review protocol that speciﬁes the research question
being addressed and the methods that will be used to perform the review [22]. Based on
the goal of systematic review we deﬁne the search string as shown in the Fig. 2.

(  ((evaluat* OR assessment) 

AND (cognitive NEAR (impact or effectiveness))) 

AND (design OR development) 
AND (blind* OR "visually impaired" OR "visual disability") 
AND (multimodal OR haptic OR audio OR auditory OR vibrotactile) 
AND (interface OR "user interact*")   )

Fig. 2. Search string

Study Selection Criteria. We deﬁne the search criteria as studies that present technology 
for people who are blind or visually impaired and has applied a cognitive
impact evaluation. The inclusion and exclusion criteria are according to the goal of the
systematic review. These requirements comply the general objective of this study, but
also aim to have a broader view of the assessment in various technologies in the area.

1 Microsoft Excel - https://products.ofﬁce.com/pt-br/excel
2 Mendeley - https://www.mendeley.com/
3 Medline2Bibtex - https://github.com/lanabia/Medline2Bibtex
4 https://www.dropbox.com/s/4wiiqnwqyd3cquw/Systematic%20Review%20v5.xlsm?dl=0

370

L. Mesquita et al.

Table 2 presents the inclusion (I) and exclusion (E) selection criteria. To be accepted, a
scientiﬁc paper must cover all inclusion criteria and none exclusion criterion.

Table 2.

Inclusion and exclusion criteria

Code
I.01
I.02

I.03
I.04

Code
E.01
E.02
E.03

Inclusion criteria
The study must be published after 1998 and written in English
The study has a technology for people who are blind or visually impaired according
to the search criteria
The study evaluates the technology by using some approach that involves the user
The study evaluates the technology by using a method to evaluate the impact the
cognitive impact
Exclusion Criteria
Title and abstract out the search criteria (I.02, I.03, I.04)
Entire text out the search criteria (I.02, I.03, I.04)
The document is a book, a congress’ abstract, an extended abstract, a poster, an oral
communication, proceedings of a conference, a seminar, a research plenary, a
dictionary or an encyclopedia

The I.01 criterion deﬁnes the scientiﬁc articles must be in English, because it is the
mandatory language for the main events and scientiﬁc journals in the search area. And
they must be published between 1st January 1998 and 2nd August 2017. The year 1998
was a milestone due to the paper Lumbreras et Sánchez (1998) which works with 3D
acoustic interfaces for blind children and is the last study known [24].

The technologies deﬁned in the I.02 criterion include mobile application, computer
software, IoT systems, virtual environments or a video game with multimodal interfaces.
Also, we accept technologies that are not speciﬁcally for people who are blind or visually
impaired with the goal of expanding the results, but the studies present the technology
focused on users with visual disabilities. We exclude from all technologies that uses
Sensory Substitution Devices [19], which substitutes a sense. The device SSD (Sensory
Substitution Devices), out of our scope, includes sensory replacement, haptics as sensory
augmentation, bionic eyes, retinal visual prosthesis, cortical implants and others. This
deﬁnition is important to plan the methodology proposed and to delimit the focus.

We deﬁne the studies type in the E.03. This criterion excludes all studies type
different from primary studies that present technology for people who are blind and its
evaluation. We accept articles, conference papers, short papers, and book chapters. This
criterion includes documents that have the minimum information to understand the
evaluation. We did not cover books because the information is dispersed inside them.

3.2 Conducting

In the conducting phase, ﬁrstly, we identify and select studies. To identify, we did a
manual string research in ﬁve scientiﬁc bases: Scopus, Springer Link, PubMed,
PubMed Central, and Web of Science. We chose the main research bases in the
research area or the bases that index them [25]. Other bases were not included because
they are indexed by the bases considered.

Cognitive Impact Evaluation of Multimodal Interfaces

371

Search Process. The conducting phase starts with the initial search in the scientiﬁc
bases proposed. The string was applied on the metadata of papers, which includes
abstract, index terms, and bibliographic citation data (such as document title, publication 
title, etc.). It was retrieved 2136 papers. Figure 3 resumes the conducting phase
process.

Fig. 3. Filters in the conducting phase

The ﬁrst ﬁlter excluded papers duplicated and document types out the scope due
their format (E.03). The second ﬁlter identiﬁes which paper is in and out the scope by
reading their titles and abstracts (E.01). A lot of papers were excluded in the ﬁrst ﬁlter
because the scientiﬁc base PubMed Central (PMC) brings a lot of medical papers
focused on disease effectiveness and speciﬁc medical statements. Even though the area
of this study is computer science, we decided to insert the PMC in the bases’ list due to
the nature of the subject.

Next, in the third ﬁlter, we evaluated each retrieved paper in its entirety (E.02). If
necessary, besides the entire text, we search more about the technologies and processes
described, as project and institutional websites, videos, newspaper articles and others.
Once we have chosen the select papers, we extract all data required (detailed in the
protocol) to achieve the objective. The organization of the data generates data synthesis,
 which will be shown in the Sect. 5.1. It remains to be done the study quality
assessment of each paper retrieved. Table 3 shows the quantity of papers selected in
each ﬁlter per scientiﬁc bases. The main reason for withdrawing papers in the last ﬁlter
was the evaluation performed is out the search and at most times related to the system
performance, e.g., sensor performance evaluation.

Table 3. Quantity of papers accepted per ﬁlter

Initial search
217 (10,2%)
Scopus
Springer 424 (19,85%)
WoS
VIB
PMC
Manual
Total

1 (0,05%)
7 (0,33%)
1486 (69,57%) 1317
1 (0,05%)
2136

1
1696

9
16
1
2
21
1
50

Filter 1 Filter 2 Filter 3 Acceptance rate
135
235
1
7

5 (20%) 2,3%
9 (36%) 2,1%
100,0%
1 (4%)
14,3%
1 (4%)
8 (32%) 0,5%
1 (4%)
25

100,0%
–

372

L. Mesquita et al.

Data Extraction Form Fields. The data extraction was designed to answer the main
and second questions and to understand the context in which each paper is inserted. We
divide the data collected into three categories: (i) General, (ii) Research and (iii)
Empirical. The general category comprises bibliographic information.

The research category comprises the attributes of the research and the technology
present in the paper. In this category we appraise the scientiﬁc paper into two classiﬁcations.
 The ﬁrst one ﬁt the paper according to the research type [26], which can be
validation research, evaluation research, solution research, philosophical research,
opinion paper or experience papers. The second classiﬁcation ﬁts the technology
presented in the key features of multimodal interfaces for the cognition of people who
are blind [3]. This classiﬁcation is divided into 4-dimension: Interface, Interaction,
Cognition, and Evaluation; and it is applied to video games and virtual environments
(Fig. 4).

Fig. 4. Key features in multimodal interfaces (Image from [3])

For our purpose, we classify only in the interaction,

interface and cognition
dimensions; and we cover, in the classiﬁcation, more than video games and virtual
environments, since we also found these features present in the technologies selected.
These features provide necessary insights for the practical understanding of the issues
involved in their design and evaluation [3]. They are useful in our research for giving a
comprehensive overview of technologies and evaluations regarding the multimodal
interfaces. The research category still shows more information about the research, as
other strategies used to evaluate.

Cognitive Impact Evaluation of Multimodal Interfaces

373

The empirical category provides information speciﬁcally about how the empirical
method evaluates the impact of the cognitive impact. The empirical method classiﬁcation 
[27], which is retrieved in this category, classiﬁes the empirical method in three
types: Experiment, Survey and Case Study.

To conclude the achievements, all data pass by a manual analysis for acquiring
qualitative and quantitative results. The Sect. 4 of this paper brings the compilation of
data extraction.

Studies Quality Evaluation. Study quality assessment. The quality evaluation was
based on [28]. Although the areas are different, we apply adaptation that resulted in the
quality checklist of Table 4. The checklist assesses the studies and measures the weight
of each study found in the ﬁnal results. Each question subtracts or adds points that give
a general score to the empirical method. This score was deﬁned according to the
importance of the requested data and they are all linked to the data extraction form
ﬁelds.

Table 4. Quality assessment form

Q2

Question

Q3
Q4
Q5

ID
Q1 Was it possible to extract all data regarding the data the Key features in multimodal
interfaces (Classiﬁcation category from extraction form? (−0.1 pts per missing input;
min value: −4.1 pts)
Is there a complete description of how the evaluation has been applied? (1.0 pt per
complete input in Empirical category; max value: 8,0 pts)
Are the groups of participants in the experiment randomly assigned? (0.5 pts)
Is the description of the impact evaluation understandable? (0.5 pts)
How many steps does the experiment take (e.g., Pretest, Training Tasks, Main Tasks
and Posttest)? (0.08 pts per step)
Does the article present different evaluation types of the proposal? (0.25 pts x number
of evaluations types)
How many experiments does the paper present? (0.5 pts per experiment, if more than
two experiments)
Is the goal of the evaluation cleared deﬁned? (0.5 pts)
Is the hypothesis (null and alternative) explicitly described in the study? (1 pt)

Q6

Q7

Q8
Q9

4 First Results of the Systematic Review

The systematic review brought 25 papers that have 28 experiments because three
papers have two experiments per paper [19, 29, 30]. Among these 25 papers, some of
them do a cognitive impact evaluation in the same technology. Then the papers present
23 technologies for people who are blind or visually impaired as shown in Table 5. The
next subsections explain all results obtained in each category of data.

374

L. Mesquita et al.

Technology
AbES
[21, 31, 32]
aMFS [29]
ATM machine [33]

Audio Haptic Maze
(AHM) [34]
AudioBattleShip [5]

AudioGene [35]

AudioLink [36]

AudioMath [37]

AudioNature [38]

Audiopolis [8, 39, 40]
AudioStoryTeller [11]

AudioTransantiago
[41]

BlindAid [42]
Building Navigator
[43]
EyeCane [44]

Métro
Métro Mobile (mBN)

Mobile devices
R-MAP

Table 5. Technologies encountered

Technology description
An environment to be explored for the purposes of learning the
layout of an unfamiliar and complex indoor environment
Audible Mobility Feedback System
ATM usage fees are the fees that many banks and interbank
networks charge for the use of their automated teller machines
(ATMs)
AHM allows a school-age blind learner to be able to navigate
through a series of mazes from a ﬁrst-person perspective.
A sound-based interactive and collaborative environment for blind
children. This system is a similar version of the traditional
battleship game for sighted people but including both a graphical
interface for sighted users and an audio-based interface for blind
people
A game that uses mobile and audio-based technology to assist the
interaction between blind and sighted children and to learn genetic
concepts
An interactive audio-based virtual environment for children with
visual disabilities to support their learning of science
An interactive virtual environment based on audio to develop and
use short-term memory, and to assist mathematics learning of
children
An audio-based virtual simulator for science learning implemented
in a mobile device (PocketPC) platform
A video game designed for developing orientation and mobility
A tool for PocketPCto support the development of reading and
writing skills in learners with visual disabilities (LWVD) through
storytelling, providing diverse evaluation tools to measure those
skills
A handheld application that allows users to plan trips and provide
contextual information during the journey through the use of
synthesized voices
The system allows the user to explore a virtual environment
A digital-map software with synthetic speech installed on a cell
phone or PDA
A hand-held device which instantaneously transforms distance
information via sound and vibration
A software solution for blind users that represents a subway system
A software solution for blind users that represents a subway system
in a mobile version
Mobile devices in general
Reconﬁgured Mobile Android Phone is a fully integrated,
stand-alone system, that has an easy-to-use interface to reconﬁgure
an Android mobile phone

(continued)

Cognitive Impact Evaluation of Multimodal Interfaces

375

Table 5.

(continued)

Technology description
An academic prototype of a tangible refreshable multimodal
interface that has an interface similar to a Braille matrix
A video game that allows users who are blind to gradually build up
a mental model based on references between different points on a
Cartesian plane, in a way that is both didactic and entertaining
A 3D virtual environment
A vibrotactile mobility feedback system

Technology
TactiPad

Tower defense

UnrealEd
vMFS

General Data. Among all papers, 14 are conference papers [5, 8, 11, 30, 33, 34, 36,
38–40, 45–48], 1 paper [19] is a book chapter and the others (10 papers) are from
scientiﬁc journals. Figure 5 shows the papers selected regarding the timeline that
started in 2004 and has peaked in 2011 and 2014. The afﬁliation most present is
University of Chile with 15 papers.

Research Data. The papers selected present two types of research [26]: one paper is
validation research [19], and 24 papers are evaluation research. The only validation
research paper is deﬁned in this way due to the technology presented is novel and have
not yet been implemented in practice.

The interfaces, interactions and cognition skills of technologies presented in the
papers are classiﬁed according to their key features of multimodal interfaces [3]. These
characteristics are important to understand which type of interfaces are assessed and
how the impact is evaluated on them. Figure 6 shows that the most notable mode of
interaction is the keyboard and the least used is the mouse. The keyboard does not
generate more complexity and expenses as the Novint Falcon device, used in [34]. The
Novint Falcon is one of the Force Feedback Device that promotes a Tactile and
Kinesthetic Feedback. Mouse and Natural
language are less used. The mouse is
replaced by buttons or other speciﬁc devices for better interaction as shown in [33]. The
most common Feedback is the Sonorous and the main Audio Interface used is the
Iconic Sound, which are sounds associated with each available object and action in the
environment [8].

Quantity per year

5 

4 

1 

1 

1 

1 

2 

2 

2 

2 

1 

3 

0 

0 

2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 

6 

4 

2 

0 

Fig. 5. Timeline papers

376

L. Mesquita et al.

Many papers apply another approach to evaluate other criteria not covered in the
cognitive evaluation. Concerning other strategies used to evaluate the interface, 14
papers [5, 11, 29, 30, 34–39, 41, 42, 47, 48] applied a usability evaluation together with
cognitive evaluation. Other strategies used were: evaluation of a tactile perception [19,
46], system performance [38] that tested the hardware used, evaluation based on HCI
heuristics [33], recognition of pattern [19], obstacle awareness goal [19], homing and
obstacle avoidance [19] and iconic evaluation [39]. Eight papers do not apply other
strategy to evaluate the system or user interaction.

Usability evaluation is the most used assessment besides cognition impact evaluation.
 The usability evaluation is mainly used to obtain information about the user’s
acceptance of the software and the match between his or her mental model and the
representation included in the software [36].

Key features in multimodal interfaces

14

1 

e
s
u
o
M

d
r
a
o
b
y
e
K

3 

e
g
a
u
g
n
a
L
 
l

a
r
u
t
a
N

23

13 11

10 8  6  8 

6  7  8 

5 

14 13

13

18

12

8 

5  3 

2  2  4  6 

l

a
u
s
i
V

e

l
i
t
c
a
T

s
u
o
r
o
n
o
S

c
i
t
e
h
t
s
e
n
K

i

n
e
e
r
c
s
h
c
u
o
T

d
a
p

 
l

a
n
o
i
t
c
e
r
i
D

e
c
i
v
e
d
 
c
i
f
i
c
e
p
S

 

M
&
O

s
p
a
m

 
l

a
t
n
e
M

s
l
e
d
o
M

 
l

a
t
n
e
M

e
r
u
t
c
u
r
t
s
 
l

a
i
t
a
p
S

 

s
e
c
i
v
e
d
k
c
a
b
d
e
e
f
 
e
c
r
o
F

2 

n
o
i
t
a
r
o
b
a

l
l

o
C

g
n
i
v
l
o
s
 

l

m
e
b
o
r
P

D
2

D
3

n
e
k
o
p
S

e
z
i

S

l

r
o
o
C

t
s
a
r
t
n
o
C

t
n
u
o
s
 
o
e
r
e
t
S

d
n
u
o
s
 
c
i
n
o
c
I

s
i
s
e
t
n
y
s
 
h
c
e
e
p
S

d
n
u
o
s
 
d
e
z
i
l

a
i
t
a
p
S

Mode

Feedback

Skills

Graphics

Audio

Adaptation

Interaction

Cognition

Interface

Fig. 6. Results of key features in multimodal interfaces

Empirical Data. Concerning the 28 empirical strategies that evaluate the cognitive
impact, most papers applied experiments and only one applied a case study [38] due to
the small number of participants. As our focus is the experiments, we treat in both types
only the experiment criteria. Due to this fact, we call all empirical methods as
experiments. The data that came from the Empirical category gave a substantial part of
the comprehension of this research. Figure 7 presents all data extracted from each
experiment. It is important to note that not all the papers presented the data sought.
These faults were included in the quality criteria and then considered in the discussions
and conclusions. The empirical data is counted according to the number of experiments
(28 experiments), and not papers.

Cognitive Impact Evaluation of Multimodal Interfaces

377

Fig. 7. Data extracted in Empirical category

Instruments. The instruments are means for data collection in the cognitive impact
evaluation with the objective of identifying some user ability controlled on the
Experiment (as an independent variable), e.g., the mathematics knowledge test in [37]
or is used to guide the evaluation process, e.g., observation guideline to assess O&M
skills in [39]. Among them, there are 15 checklists [5, 8, 11, 32, 34, 37, 39–41, 45, 46]
which include guidelines and speciﬁc tests, 11 questionnaires [11, 29, 32, 35, 36, 38,
42, 43, 46, 47], 7 interviews [8, 29, 33, 39, 41, 42], 6 modeling kits [5, 19, 30, 32, 37,
42] and 9 logs [5, 21, 29–33, 36, 42, 48] which include, in addition to the system log,
the video and audio logs. Many studies produce their instruments (7 experiments),
others work with instruments found in scientiﬁc literature.

Statistical Methods. Figure 8 shows the statistical methods used in the experiment data
analysis. Neither we take account of simple statistical methods as averages and gain
percent, that is the only method used by 11 experiments [19, 30–33, 35, 37, 38, 42].

Statistical methods

Wilcoxon Ranksum test

Welch's t test

Shapiro-Wilk test

Bonferroni-corrected pairwise comparisons

Pearson’s Correlation Coefficients

T-test

1 
1 
1 
1 
1 
1 
1 
1 

4 

2 

8 

0 

1 

2 

3 

4 

5 

6 

7 

8 

9 

Fig. 8. Statistical methods

378

L. Mesquita et al.

Three experiments [36, 39, 45] do not specify the statistical method used, neither in the
references. T-test, which uses statistical concepts to reject or not a null hypothesis is the
most used, followed by ANOVA and Pearson’s Correlation Coefﬁcients. These two
methods are used to analyze the variation between groups. ANOVA procedure was
applied in one, two and three-way.

Resources. Concerning the resources, 7 experiments [11, 30, 36–38, 41] give information 
about the time spent in the evaluation and 2 experiments in the same paper [29]
specify the human resource. The mean of the time among these is 3.4 months and the
longest time spent is 6 months. None of them talk about ﬁnancial resources.

Ethical Concepts. 5 experiments [21, 29, 43, 44] mentioned signing consents. One of
them also applies stop rules to enforcing ethical concept [21] and present the ethics
council that it approved.

Sample. From the number of users to the onset age of blindness, there are many sample
combinations in the selected experiments. The sample choice could be based on level
of experience required to do the task, age or level of blindness. Some characteristic
controlled in the sample are related to the disabilities, as the onset of blindness, the
etiology of a visual impairment or the presence of another disability. The quantity of
users varies as shown in Fig. 9. Most of the experiments (75%) are applied to 3 to 12
users. About the range of age, the most of experiments (9 experiments) are applied in
teenagers (10 to 15 years old). The range and ages are shown in Fig. 10.

The gender distribution in samples is equilibrated in most cases, but we not count
the 11 experiments which do not describe this information. The mean of gender proportion 
is 50% for women and men, with 2% of variance.

12
10
8 
6 
4 
2 
0 

Quantity of users

10

11

2 

1 

3 

1 

3 to 7 users 8 to 12 users

13 to 17

18 to 22

23 to 27

33 to 38

Fig. 9. Quantity of users in the samples

The distribution between the blindness level is varied. There are experiments where
the sample is all formed by people who are blind [49] and there are samples formed
only by people who are blindfolded, as the experiments in [19]. Figure 10 shows the
user age range. Figure 11 shows the blindness level distribution between the samples.

Cognitive Impact Evaluation of Multimodal Interfaces

379

Users Age range

18 19 19 19 19 19 19 19 20 21 25 27 30 

40 41 

8  8  9  10 10 10 10 10 12 12 

Age mean

Fig. 10. Users age range per experiment

Blindness level distribution

80 
70 
60 
50 
40 
30 
20 
10 
0 

100%

0%

Proportion of blind

Proportion of low vision

Proportion of blindfolded

Fig. 11. Blindness level distribution

Tasks. The tasks explored in the experiments are related to the technology assessed.
The tasks to assess the development of O&M skills are based on virtual and real
environments [30, 34]. Some of them use modeling kits to represent the virtual environment 
in the real world and to analyze the space awareness of each participant and
the cognition improvement [30]. Another example of cognitive task is to read of a text
with the guidance of virtual sounds. Some works have used levels of complexity in the
tasks to quantify the cognition impact, for example, the work [45] estimates the task
performance in 5 levels.

Variables. The most current
independent variables, which are controlled in the
experiment, are related to the sample choice, the characteristics deﬁned as etiology of
blindness, age, blindness level and gender [41]. The dependent variables, in which we
want to see the effect, are related to the measures and the impact. The factor, which is a
type of variable controlled is modiﬁed in the experiment to see the cognitive effect. For
example, the experiment proposed in [8] uses as factor different outputs of the multimodal 
interface (audio group, haptic group, and haptic-audio groups).

380

L. Mesquita et al.

Measures. The measure to evaluate the cognitive impact focuses on the performance to
compare before and after the use of the technology assessed, e.g., [37], or between two
groups with and without the technology, e.g. [21]. The measures are strongly related to
the instruments used. For example, the checklists of [34] assess the task using scores
for sensory perception, tempo-spatial development, and O&M skills. In some works,
each measure has a property scale or options, as the Likert scale used in [41]. Eleven
experiments [8, 11, 30, 34, 37–40, 42, 48] apply instruments as pretest and posttest to
measure the impact.

5 Discussion

As shown in the previous section there is consistent research that evaluates the cognition 
impact since 2004, showing the importance of this type of study on technologies
for blind people. Among all papers retrieved from initial search in the systematic
review, we could see in general, that the papers do not evaluate the cognition impact.
The preference of evaluation lies to the system, be it hardware or software. The same
result is seen in papers that are retrieved in the third ﬁlter, one of the most used
strategies to evaluate the technologies is the system evaluation besides cognitive
evaluation. Despite this ﬁnding, the leading role of these technologies is often to help a
user’s cognitive activity. Thus, assessing if the cognitive purpose was achieved is an
important part of constructing software for blind people.

The research data give us an understanding of how the interaction works on
multimodal interfaces of the technologies for blind people. The interaction, cognition
skills, and interface characteristics encountered model the planning of the experiment.
The combination of a keyboard as the mode of interaction, the sonorous feedback and
iconic sound to the interface stands out for the interactions encountered.

In the empirical data, we found a huge diversity of experiments. Although we
understand these differences between the experiments, we come across differences in
the process of planning and presenting the experiment and data acquired. Instruments,
tasks, variables, and measures are strongly related to the technology assessed and the
cognitive purpose. Almost no work presented the variables explicitly according to the
classiﬁcation of dependent variables, independent variables, and factors. To understand
the experiments, we deﬁne these three kinds of variables in each experiment. However,
this adds a cost to fully understand the experiment.

From the number of users to the onset age of blindness, there are many sample
combinations in the selected experiments. One point that stands out in the selection of
the sample is the number of blind users who perform the experiments and how these
data are computed. There is, for example, one work that uses the only blindfolded
sample, which refocus the conclusion of the experiment.

Statistical power is an inherent part of empirical studies that ensure the results
found and to the study conclusions [50]. However, 11 experiments analyzed only the
percentage of the gain or the average. Wohlin et al. [20] mention that one of the
advantages of an experiment
is the ability to perform statistical analysis using
hypothesis testing methods and opportunities for replication.

Cognitive Impact Evaluation of Multimodal Interfaces

381

Almost no experiment dealt with the resources and ethical concepts. Although there
is information about the resources in some papers, in general, this information is not
clear in the text and is often incomplete. The missing information makes difﬁcult to
repeat the experiment. The ethical concepts also are not well covered in the papers; even
it is an essential step to produce an experiment with people who have disabilities [20].

6 Conclusion

The goal of a state-of-the-art is to review the existing evidence concerning the impact
evaluation of multimodal interfaces and also seeks to summarize the empirical evidence
concerning the strengths and limitations of a speciﬁc evaluation method [22]. With this
work, we expected to have created a bibliographic review on the cognitive impact
evaluation based on the steps of the systematic review approach. Our scope was
bounded by multimodal interfaces for people who are blind (in this paper a synonym of
visually impaired).

The technologies for people who are blind have many needs due to the target
audience and special characteristics with multimodal interfaces, moreover, lots of
applications for blind people aims to improve cognitive skills, such as enhancement in
O&M, wayﬁnding, and navigation skills, and thus supporting the user in daily lives. As
so, it is important to point out that the use of evidence-based is essential to measure the
real impact of the technologies.

After compiling the data from the systematic review and analyzing theoretical
foundations, we conclude that there is a need to better plan and present data from
experiments on technologies for blind people. With this, we guarantee the quality of the
experiment itself and the interaction of the technology with respect to the cognitive
objective. Faced with this nego, we propose as future work to better explore the
preliminary results found to improve the data analysis using Grounded Theory and
create a set of guidelines that appropriately guide experiments to evaluate tools for
blind people. These guidelines will provide a way to evaluate the cognitive impact to
development and enhancement in blind people or visually impaired, considering the
main aspects of multimodal interfaces.

Acknowledgments. This research is funded by Chilean FONDECYT #1150898; Basal Funds
for Centers of Excellence, Project FB0003 - CONICYT. We would like to thank FUNCAP for
sponsoring a master scholarship (MDCC/DC/UFC) for the author Lana Mesquita.

References

1. Bamac, B., Aydin, M., Ozbek, A.: Physical ﬁtness levels of blind and visually impaired

goalball team players. Isokinet. Exerc. Sci. 12, 247–252 (2004)

2. Sánchez, J., Darin, T., Andrade, R., Viana, W., Gensel, J.: Multimodal interfaces for
improving the intellect of the blind. In: XX Congresso de Informática Educativa – TISE, vol.
1, pp. 404–413 (2015)

3. Darin, T., Andrade, R., Sánchez, J.: Dimensions to analyze the design of multimodal

videogames for the cognition of people who are blind dimensions (2015)

382

L. Mesquita et al.

4. Hinebaugh, J.P.: A Board Game Education. Rowman & Littleﬁeld Education, Lanham

(2009)

5. Sánchez, J., Baloian, N., Hassler, T.: Blind to sighted children interaction through
collaborative environments. In: de Vreede, G.-J., Guerrero, L.A., Marín Raventós, G. (eds.)
CRIWG 2004. LNCS, vol. 3198, pp. 192–205. Springer, Heidelberg (2004). https://doi.org/
10.1007/978-3-540-30112-7_16

6. Buaud, A., Svensson, H., Archambault, D., Burger, D.: Multimedia games for visually
impaired children. In: Miesenberger, K., Klaus, J., Zagler, W. (eds.) ICCHP 2002. LNCS,
vol. 2398, pp. 173–180. Springer, Heidelberg (2002). https://doi.org/10.1007/3-540-454918_38


7. Grammenos, D., Savidis, A., Georgalis, Y., Stephanidis, C.: Access invaders: developing a
universally accessible action game. In: Miesenberger, K., Klaus, J., Zagler, W.L., Karshmer,
A.I. (eds.) ICCHP 2006. LNCS, vol. 4061, pp. 388–395. Springer, Heidelberg (2006).
https://doi.org/10.1007/11788713_58

8. Sánchez, J., De Borba Campos, M., Espinoza, M., Merabet, L.B.: Audio haptic videogaming

for developing wayﬁnding skills in learners who are blind (2014)

9. Goria, M., Cappaglia, G., Tonellia, A., Baud-Bovyb, G., Finocchietti, S.: Devices for
visually impaired people: high technological devices with low user acceptance and no
adaptability for children. Neurosci. Biobehav. Rev. 69, 79–88 (2016)

10. Alicyn Ferrell, K.A.: Evidence-based practices for students with visual disabilities.

Commun. Disord. Q. 28, 42–48 (2006)

11. Sánchez, J., Galáz, I.: AudioStoryTeller: enforcing blind children reading skills. In:
Stephanidis, C. (ed.) UAHCI 2007. LNCS, vol. 4556, pp. 786–795. Springer, Heidelberg
(2007). https://doi.org/10.1007/978-3-540-73283-9_85

12. Weber, A.: Credentialing in assistive technology. Technol. Disabil. 9, 59–63 (1998)
13. Dalton, E.M.: Assistive technology standards and evidence-based practice: early practice and

current needs, 10 June 2015

14. Dumas, B., Lalanne, D., Oviatt, S.: Multimodal interfaces: a survey of principles, models
and frameworks. In: Lalanne, D., Kohlas, J. (eds.) Human Machine Interaction. LNCS, vol.
5440, pp. 3–26. Springer, Heidelberg (2009). https://doi.org/10.1007/978-3-642-00437-7_1
15. World Health Organization: The prevention of blindness. Report of a WHO Study

Group. Technical report series, no. 518, pp. 1–18 (1973)

16. Ossmann, R., Miesenberger, K.: Guidelines for the development of accessible computer
games. In: Miesenberger, K., Klaus, J., Zagler, W.L., Karshmer, A.I. (eds.) ICCHP 2006.
LNCS, vol. 4061, pp. 403–406. Springer, Heidelberg (2006). https://doi.org/10.1007/
11788713_60

17. Ritter, F.E., Baxter, G.D., Churchill, E.F.: Foundations for Designing User-Centered

Systems. Springer, London (2014). https://doi.org/10.1007/978-1-4471-5134-0

18. Norman, D.A., Donald, A., Carroll, M.: Cognitive artifacts. Des. Interact. Psychol. Hum.-

Comput. Interface 1, 17–38 (1991). Cambridge

19. Pissaloux, E., Velázquez, R.: Model of cognitive mobility for visually impaired and its
experimental validation. In: Pissaloux, E., Velázquez, R. (eds.) Mobility of Visually
Impaired People. LNCS, pp. 311–352. Springer, Cham (2018). https://doi.org/10.1007/978-
3-319-54446-5_11

20. Wohlin, C., Runeson, P., Höst, M., Ohlsson, M.C., Regnell, B., Wesslén, A., Anders, W.:

Experimentation in Software Engineering. Kluwer Academic Publishers, Norwell (2000)

21. Connors, E., Chrastil, E., Sánchez, J., Merabet, L.: Virtual environments for the transfer of
navigation skills in the blind: a comparison of directed instruction vs. video game based
learning approaches. Front. Hum. Neurosci. 8, 223 (2014)

Cognitive Impact Evaluation of Multimodal Interfaces

383

22. Kitchenham, B., Charters, S.: Guidelines for performing systematic literature reviews in

software engineering version 2.3. Engineering 45, 1051 (2007)

23. StArt. http://lapes.dc.ufscar.br/tools/start_tool (2016)
24. Lumbreras, M., Sánchez, J.: 3D aural interactive hyperstories for blind children. Virtual

Real. 119–128 (1998)

25. Harzing, A.-W., Alakangas, S.: Google scholar, scopus and the web of science: a

longitudinal and cross-disciplinary comparison. Scientometrics 106, 787–804 (2016)

26. Petersen, K., Feldt, R., Mujtaba, S., Mattsson, M.: Systematic mapping studies in software
engineering. In: 12th International Conference on Evaluation and Assessment in Software
Engineering, pp. 1–10 (2008)

27. Ampatzoglou, A., Stamelos, I.: Software engineering research for computer games: a

systematic review. Inf. Softw. Technol. 52, 888–901 (2010)

28. de Sousa Santos, I., de Castro Andrade, R.M., Rocha, L.S., Matalonga, S., de Oliveira, K.M.,
Travassos, G.H.: Test case design for context-aware applications: are we there yet? Inf.
Softw. Technol. 88, 1–16 (2017)

29. Adebiyi, A., Sorrentino, P., Bohlool, S., Zhang, C., Arditti, M., Goodrich, G., Weiland, J.D.:
Assessment of feedback modalities for wearable visual aids in blind mobility, 1–17 (2017)
30. Sánchez, J., Maureira, E.: Subway mobility assistance tools for blind users. In: Stephanidis,
C., Pieper, M. (eds.) UI4ALL 2006. LNCS, vol. 4397, pp. 386–404. Springer, Heidelberg
(2007). https://doi.org/10.1007/978-3-540-71025-7_25

31. Connors, E.C., Yazzolino, L.A., Sánchez, J., Merabet, L.B.: Development of an audio-based

virtual gaming environment to assist with navigation skills in the blind, 1–7 (2013)

32. Sánchez, J., Sáenz, M.: Enhancing navigation skills through audio gaming (2014)
33. Shaﬁq, M., Choi, J.-G., Iqbal, M., Faheem, M., Ahmad, M., Ashraf, I., Irshad, A.: Skill
speciﬁc spoken dialogues based personalized ATM design to maximize effective interaction
for visually impaired persona. In: Marcus, A. (ed.) DUXU 2014. LNCS, vol. 8520, pp. 446–
457. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-07638-6_43

34. Sánchez, J., Espinoza, M.M., Sanchez, J., Espinoza, M.M.: Audio haptic videogaming for
navigation skills in learners who are blind. In: ASSETS 11: Proceedings of the 13th
International ACM SIGACCESS Conference on Computers and Accessibility, pp. 227–228.
Association for Computing Machinery, New York (2011)

35. Sánchez, J., Aguayo, F.: AudioGene: mobile learning genetics through audio by blind
learners. In: Kendall, M., Samways, B. (eds.) Learning to Live in the Knowledge Society.
ITIFIP, vol. 281, pp. 79–86. Springer, Boston, MA (2008). https://doi.org/10.1007/978-0-
387-09729-9_10

36. Sánchez, J., Elías, M.: Science learning in blind children through audio-based games. In:
Redondo, M., Bravo, C., Ortega, M. (eds.) Engineering the User Interface. Springer, London
(2008). https://doi.org/10.1007/978-1-84800-136-7_7

37. Sánchez, J., Flores, H., Sánchez, J.: AudioMath: blind children learning mathematics

through audio. Int. J. Disabil. Hum. Dev. 4, 311–316 (2005)

38. Sánchez, J., Flores, H., Sáenz, M.: Mobile science learning for the blind (2008)
39. Sánchez, J., Mascaró, J.: Audiopolis, navigation through a virtual city using audio and haptic
interfaces for people who are blind. In: Stephanidis, C. (ed.) UAHCI 2011. LNCS, vol. 6766,
pp. 362–371. Springer, Heidelberg (2011). https://doi.org/10.1007/978-3-642-21663-3_39
40. Sánchez, J., Espinoza, M., De, B., Merabet, L.: Enhancing orientation and mobility skills in
learners who are blind through video gaming. In: Conference on Creativity & Cognition,
2013, pp. 353–356 (2013)

41. Sánchez, J., Oyarzún, C.: Mobile audio assistance in bus transportation for the blind (2008)
42. Lahav, O., Schloerb, D., Kumar, S., Srinivasan, M.: A virtual environment for people who

are blind – a usability study. J. Assist. Technol. 6, 38–52 (2012)

384

L. Mesquita et al.

43. Kalia, A., Legge, G., Roy, R., Ogale, A.: Assessment of indoor route-ﬁnding technology for

people with visual impairment. J. Vis. Impair. Blind. 104, 135–147 (2010)

44. Buchs, G., Simon, N., Maidenbaum, S., Amedi, A.: Waist-up protection for blind individuals
using the EyeCane as a primary and secondary mobility aid. Restor. Neurol. Neurosci. 35,
225–235 (2017)

45. Villane, J., Sánchez, J.: 3D virtual environments for the rehabilitation of the blind. In:
Stephanidis, C. (ed.) UAHCI 2009. LNCS, vol. 5616, pp. 246–255. Springer, Heidelberg
(2009). https://doi.org/10.1007/978-3-642-02713-0_26

46. Guerreiro, T., Oliveira, J., Benedito, J., Nicolau, H., Jorge, J., Gonçalves, D.: Blind people
and mobile keypads: accounting for individual differences. In: Campos, P., Graham, N.,
Jorge, J., Nunes, N., Palanque, P., Winckler, M. (eds.) INTERACT 2011. LNCS, vol. 6946,
pp. 65–82. Springer, Heidelberg (2011). https://doi.org/10.1007/978-3-642-23774-4_8

47. Hossain, G., Shaik, A.S., Yeasin, M.: Cognitive load and usability analysis of R-MAP for

the people who are blind or visual impaired (2011)

48. Espinoza, M., Sánchez, J., de Borba Campos, M.: Videogaming interaction for mental model
construction in learners who are blind. In: Stephanidis, C., Antona, M. (eds.) UAHCI 2014.
LNCS, vol. 8514, pp. 525–536. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-
07440-5_48

49. Lahav, O., Mioduser, D., Lahav, O.: Blind persons’ acquisition of spatial cognitive mapping
and orientation skills supported by virtual environment. Int. J. Disabil. Hum. Dev. 4, 231–
238 (2005)

50. Dybå, T., Kampenes, V.B., Sjøberg, D.I.K.: A systematic review of statistical power in

software engineering experiments. Inf. Softw. Technol. 48, 745–755 (2006)

