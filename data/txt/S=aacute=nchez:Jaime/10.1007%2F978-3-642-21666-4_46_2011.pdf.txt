mGuides, Design and Usability of a Mobile System to 

Assist Learning in Critical Situations  

Jaime Sánchez and Matías Espinoza 

Department of Computer Science 

Center for Advanced Research in Education (CARE) 

University of Chile, Blanco Encalada 2120, Santiago, Chile 

{jsanchez,maespino}@dcc.uchile.cl 

Abstract. This work presents the usability evaluation of the mGuides system, 
which emerged as a response to the educational needs of students affected by 
the earthquake that hit Chile in the year 2010. With this system, teachers generate 
working guides through an editor, including learning guides and questionnaires 
for their learners. At the same time, students visualize and complete these 
working guides on cellular phones. The objective of this work is to present the 
impact of usability evaluations as part of the process for the development of the 
mGuides system, contributing mainly to the validation of the functionalities and 
the detection of errors. The results show that the mGuides system  was highly 
accepted by both teachers and students, and that it is an intuitive and easy-touse 
tool.   

Keywords: mobile learning, learning guides, usability, web editor. 

1   Introduction 

On  February  27,  2010  in  Chile  there  was  a  very  strong  earthquake  in  the  centralsouthern 
area of the country [5], which had the effect, among other consequences, of 
altering the normal development of school-related activities in the affected area just 
when the school year was supposed to be starting. The students that were in their last 
year of high school were the most affected, due to the fact that by the end of the year 
they had to take the Chilean university entry exam, the deadline for which cannot be 
postponed. In this way, until the situation was to return to normal, the need to make 
up for the lack of classroom infrastructure and lost class time for the students in the 
affected  areas  emerged.  In  this  context,  the  mGuides  (mobile  Guides)  system  was 
created as a response to the previously described needs, involving the participation of 
teachers and students as users of the system. 

Cellular  phones  were  chosen  as  an  objective  platform  because  these  devices  are 
characterized by being lightweight and portable, most teachers and students own one, 
because they are appropriate for working while mobile, and due to the fact that they 
can be used while in uncomfortable places [1]. This means that they can be used both 
inside and outside of the classroom, providing the student with the flexibility needed 
to  work  with  an  educational  application  in  any  place  and  at  any  time.  At  the  same 
time,  the  fact  that  cellular  phones  have  some  significant  limitations  also  had  to  be 

C. Stephanidis (Ed.): Universal Access in HCI, Part III, HCII 2011, LNCS 6767, pp. 415–424, 2011. 
© Springer-Verlag Berlin Heidelberg 2011 

416 

J. Sánchez and M. Espinoza 

taken into consideration [1]. Such limitations include a low processing capacity and 
the reduced screen size, both of which are aspects that limit the kind of content that 
the system can support. This dilemma also invites developers to explore the best possible 
 way  to  present  data,  which  can  be  determined  primarily  through  usability 
evaluations. 

In the literature, there are applications that take advantage of the characteristics of 
mobile devices for emergency, on-site situations, such as MobileMap [7], which supports 
decision making and collaboration among firemen, and another tool that aids in 
collecting  and  manipulating  medical  care  information  during  emergencies  [8].  The 
Southern University at New Orleans proposed a system based on m-learning in order 
to be able to continue  making progress after the destruction produced by Hurricane 
Rita and Hurricane Katrina [9]. 

The development of the mGuides system was influenced by some previous projects 
such as BuinZoo and Museo [2], which are interactive software for learning that can 
be run on a Pocket PC and a Classmate PC, and Mobile tourism [10], which allow for 
the  creation  of  tourist  content  for  mobile  devices.  Another  prior  influential  project 
corresponds  to  the  creation  of  an  educational  RPG  videogame  engine  for  cellular 
phones presented in [3].  

The purpose of this work is to introduce mGuides and presenting the impact of the 
usability evaluations performed as a part of the development process of the mGuides 
system, and as a mechanism for the validation of the system’s functionalities and the 
detection of errors. 

2   mGuides 

Structurally,  each  guide  behaves  as  a  learning  guide  and  a  working  questionnaire. 
Each learning guide is made up of a certain number of pages, in which each page contains 
a title, an image and a paragraph of text. In turn, each questionnaire is made up 
of a certain number of multiple-choice questions, in which each question includes a 
certain statement, an image and five answer choices. 

The mGuides system is made up of two modules: (1) A project editor to be run in a 
personal computer, designed for use by teachers (see figure 1) and for which two different 
prototypes were developed: “Editor 1.0” and “Editor 2.0”, and (2) An engine 
that  executes  the  projects  generated  by  the  editor  to  be  run  in  a  cellular  phone,  designed 
for use by students (see figure 2) and for which one prototype was developed, 
called “Engine”. 

2.1   Editor 1.0 

This version of the editor was a PC application implemented by using C# language in 
Visual  Studio  2008,  in  which  the  teacher  can  create  learning  guides  and  working 
questionnaires. In the case of the learning guides, the editor provides two buttons: one 
that allows the user to add and another that allows the user to erase content pages. In 
the  form  on  each  content  page  the  editor  displays  two  text  fields,  in  which  one  is 
where the title is added and the other is for adding a paragraph of text. In addition, 
there is an area that shows the images that can be added to the page. Under this area 

 

mGuides, Design and Usability of a Mobile System to Assist Learning 

417 

there are also two buttons: one to add an image that is available on the computer and 
another to remove the image that is already linked to the page.  

In the case of the questionnaire, the editor provides two buttons: one that allows 
the  user  to  add  and  another  that  allows  the  user  to  erase  questions  on  the  questionnaire.
 In the form for each question, the editor provides six text fields: one for the text 
of the question, and the other five to add answer choices. There is also a radio button 
in order to mark the correct answer. In addition, there is an area that shows the image 
that can be added to the question. Under this area there are two additional buttons, in 
which  one  can  be  used  to  add  an  image  that  is  available  on  the  computer,  and  the 
other is to remove the image that is already linked to the question.  

The teacher can save his learning guide or questionnaire to a file on the PC, whenever 
it is deemed necessary. 

 Once the learning guides and questionnaires have been generated with the editor 
and saved onto the computer, the teacher can generate a working guide with the “Ex-
port” option on the menu, which allows him to search for one learning guide and one 
questionnaire  on  the  computer,  which  are  combined  through  the  engine  that  is  described 
later in this paper. This generates an application for cellular phones that can 
be stored in the computer and which can be distributed by the teacher through a certain 
tool to his students.  

2.2   Editor 2.0 

In the second version of the editor a web application was developed, which was implemented 
 in  Adobe  Flex  and  PHP,  and  which  maintained  the  end-user  positively 
evaluated  characteristics  of  the  first  editor  and  improved  certain  aspects  that  were 
deemed insufficient. New characteristics were also incorporated, which were necessitated 
by the web-based context of the new system.  

User authentication and an administrator for the learning guides and questionnaires 
were also added (see Fig. 2 (II)), which corresponds to the main screen of the application.
 This administrator shows a list of learning guides and questionnaires created by 
the teacher and which are available to be exported to the cellular phones. In the administrator,
 the teacher can add, edit or eliminate learning guides and questionnaires.  
The option to export a learning guide and questionnaire to an application for cellular 
phones is now available through a button on the upper-level panel of the projects 
administrator. This option, just as in the first version of the editor, allows the teacher 
to search for a learning guide and questionnaire; but in the new edition, two lists are 
generated and grouped into two comboboxes, each one connecting the user’s respective 
learning guides and questionnaires. Through this new feature, the teacher is able 
to choose those guides and questionnaires that most interest him more quickly, facilitating 
the process considerably. In this way, another significant characteristic that was 
incorporated was that the application exported to the cellular phones was made available 
through the web on a space for projects exported by the teacher, thus facilitating 
the distribution of the application.  

2.3   Engine 

The  engine  of  the  application  for  mobile  phones  was  implemented  with  J2ME  language 
on NetBeans. The engine is built in such a way that it is able to combine itself  

 

418 

J. Sánchez and M. Espinoza 

 

Fig.  1.  (I)  Editor  1.0,  content  page  form.  (II)  Editor  2.0,  learning  guides  and  questionnaires 
administrator. 

easily  with  the  structured  learning  guide  and  questionnaire  data  exported  from  the 
editor, allowing the students to work with the working guides created by the teachers. 
This  implies  that  the  students  are  essentially  working  with  the  learning  guide  and 
questionnaire, which are combined in the working guide.  

One important characteristic of the engine is that it adjusts the texts to the size of 
the device’s screen. As such, for interaction with the user the engine utilizes the Up, 
Down,  Left,  Right  and  center  buttons  of  the  telephone’s  joystick,  and  provides  the 
option of using the 8, 2, 4, 6 (directional) and 5 (as the center button) keys to replace 
the joystick in  special situations in  which the cellular phone does  not recognize the 
buttons.  

On a page of the learning guide, or for one question on the questionnaire, the Up 
and Down directional joystick buttons are used to change the focus between the selectable 
elements of the interface. In turn, the center button on the joystick is used to 
confirm the selection of an element and to perform the associated action.  

On a page of the learning guide the student can change the focus between the title, 
the image, the body of the document, the forwards link and the back link. In pressing 
the center button with the focus on the title, a text scroll is displayed in the case that 
the area provided for the title is not sufficient to show the entire text. In pressing the 
center button with the focus on the image, the image is displayed in full screen, allowing 
 the  user  to  explore  the  image  by  using  the  Up,  Down,  Left  and  Right  keys.  In 
pressing the center button with the focus on the body of the document, a text scroll is 
displayed in the case that the area is not large enough to show the entire text. Whenever 
the user enters into the title, the image or the body of the document, he can go 
back to the normal screenshot by pressing the center button. Finally, the forward and 
back links help the user to navigate freely through the learning guide. When the center 
button is pressed with the focus over either of these options, the page advances forwards 
or goes back according to which option was selected.  

For a question on the questionnaire, the student can change the focus between the 
text of the question, the image, the answer choice field, and the answer link. In pressing 
the center button  with the focus on the text of the question, a text scroll is displayed 
in the case that the area is not large enough to show the entire text. In pressing 
the center button with the focus on the image, the image is displayed in full screen, 
allowing the user to explore the image by using the Up, Down, Left and Right keys.  

 

 

mGuides, Design and Usability of a Mobile System to Assist Learning 

419 

Fig. 2. Engine. (I) Contents page. (II) Multiple-choice question. 

 

When the user presses the center button with the focus over the answer choice field, 
he is able to move the focus over the various answer choices using the left and right 
buttons on the joystick, and a text scroll with two visible lines is shown in case the 
area is not large enough to show to entire text of the answer choice. In order to choose 
an answer, the user has only to press the center button on the joystick again. When the 
user  enters  into  the  text  of  the  question  or  the  image,  he  can  return  to  the  normal 
screenshot of the question by pressing the center button. Finally, the continue link is 
used to confirm the answer as the selected answer choice, and move on to the following 
question.  

3   Methodology 

The  usability  of  the  system  modules  was  evaluated  [4]  with  end  users  (teachers  or 
students,  depending  on  the  module  under  evaluation).  In  the  following  section,  the 
methodology utilized is explored in more depth.  

3.1   Sample 

Two independent samples of teachers were selected in different regions of Chile affected 
 by  the  earthquaque,  called  “teacher  sample  n°1”  and  “teacher  sample  n°2”. 
Two  independent  samples  of  students  were  also  selected,  similarly  named  “student 
sample  n°1”  and  “student  sample  n°2”.  Both  the  teachers  and  students  were  given 
permission to participate by their schools, in accordance with their availability, which 
was a factor that determined the quantity and characteristics of the sample users independently.
 Through the instruments administered, the age,  gender, computer  knowledge 
and cellular phone use knowledge of the users was obtained.  

The teacher sample n°1 was made up of 17 users (8 female, 9 male), who teach at 
the  Liceo  Abate  Molina  and  the  Liceo  Marta  Donoso  Espejo  located  in  the  city  of 
Talca, as well as the Colegio Talcahuano, the Liceo Comercial de Talcahuano and the 
Colegio  Inmaculada  Concepcion  located  in  the  city  of  Talcahuano.  Regarding  the 
level of computer knowledge declared by the teachers in the sample, 3 stated having a 
basic level of knowledge, 10 declared having an intermediate level and 4 asserted an 
advanced skill level. The age range varied from 22 to 64 years of age.  

Teacher sample  n°2  was  made up of 11 users (6 female, 5  male) from  the  Liceo 
Diego Portales located in the city of Talca. The level of computer knowledge declared  
 

420 

J. Sánchez and M. Espinoza 

by the teachers in this group came out to 1 declaring basic knowledge, 8 admitting to 
intermediate  level  knowledge,  and  2  asserting  advanced  level  skills.  The  age  range 
varied from 24 to 60 years old.  

The student sample n°1 was made up of 104 users (52 female, 52 male), enrolled in 
Liceo Abate Molina and Liceo Marta Donoso Espejo located in the city of Talca, and 
the  Colegio  Talcahuano,  Liceo  Comercial  de  Talcahuano  and  Colegio  Inmaculada 
Concepcion located in the city of Talcahuano. The level of cellular phone use ability 
declared by these students resulted in 4 declaring a basic level of knowledge, 62 stating 
an intermediate level and 38 declaring an advanced level of knowledge. The age 
range varied between 13 and 18 years of age.  

Student sample n°2 is made up of 5 users (3 females, 2 males), between 17 and 18 
years of age, and enrolled in the Liceo Polivalente Guillermo Labarca Hube A78, located 
in the city of Santiago.  

3.2   Instruments 

Two adaptations of the end-user usability questionnaire [6] were used, in which one 
was designed for use of the editor and the other for the engine. The original questionnaire 
consisted of statements that the user must evaluate on a scale between 1 and 10, 
in which 1 is ‘a little’ and 10 is ‘a lot’, in addition to fields in which the user registers 
his “age”, “gender” and “level of computer knowledge”. The adaptations were made 
by changing the word “software” to “editor” in the statements included in the questionnaire 
 for  evaluating  the  editor,  and  by  increasing  the  number  of  questions  and 
replacing the field “level of computer knowledge” with “level of cellular phone use 
knowledge”  in  the  case  of  the  questionnaire  used  to  evaluate  the  engine.  The  questionnaire 
utilized to evaluate the usability of the editor consisted of 10 statements that 
were evaluated by teacher samples n°1 and n°2. In turn, the questionnaire to evaluate 
the usability of the engine consisted of 24 statements that were evaluated by student 
sample n°1 (the statements used in both questionnaires can be seen in the results sec-
tion). Through the use of these questionnaires, it was sought to evaluate the quality of 
the editor and engine, measuring the respective users’ satisfaction in terms of control, 
use, acceptance and design.  

An  observation  guideline  was  utilized  with  the  participants  from  student  sample 
n°2, with sections designed to measure efficiency, error and the level of learning regarding 
use of the engine. With this guideline, a facilitator recorded the time that the 
students  took  to  perform  the  tasks.  In  addition,  the  guideline  included  a  section  in 
which the evaluator recorded the most common errors that the users made, such as not 
expanding the text scrolls and not expanding the image in the learning guide and the 
questionnaire. In order to record learning of the interface, there are indications in the 
guideline that request the user to execute specific tasks to be performed after having 
completed the global tasks; some of these tasks include expanding the specific texts 
and images and changing pages in the learning guide, and finding, selecting and answering 
a question with a specific answer choice. In addition, this guideline included 
fields for recording the “age” and “gender” of the users.     

 

mGuides, Design and Usability of a Mobile System to Assist Learning 

421 

3.3   Tasks 

Regarding use of the editor by the teachers, three tasks were established: Task 1. Create 
a learning guide, Task 2, Create a questionnaire and Task 3. Export a project with 
the learning guide and questionnaire created. For the first two tasks, digital material 
with texts, questions and images was provided. For the use of the engine by the students,
 two tasks were established: Task 1. Read a learning guide and Task 2. Answer 
the questionnaire. 

3.4   Procedure 

The editor 1.0 was installed in a laptop computer, which was equipped with a mouse 
and an external keyboard. In addition, the digital support material for the construction 
of learning guides and questionnaires was included within the saved files on the computer.
 Facilitators worked individually with each user in the teacher sample n°1, who 
was asked to perform the 3 tasks corresponding to the editor. Once the tasks had been 
completed, each user was provided with a printed copy of the end-user usability questionnaire,
 adapted for use with the editor, and was asked to fill out the questionnaire.  
During a second stage, an application that had been generated with the editor and 
which had three pages of text and a questionnaire with three questions was installed in 
three  different  cellular  phones.  The  content  of  the  questions  was  related  to  science 
learning, which allowed the research team to test the engine’s characteristics. Facilitators 
worked with groups of three users at a time from student sample n°1, who were 
asked to perform the  two tasks corresponding to the  use of the engine individually. 
Once these tasks had been completed, each user was provided with a printed copy of 
the end-user usability questionnaire adapted to use of the engine, and was asked to fill 
out the questionnaire.   

During a third stage, a test server environment was installed in the laptop in order 
to allow the editor 2.0 to be executed, simulating internet connection through the local 
host. In addition, the digital support material for the construction of learning guides 
and questionnaires was included in the saved file system on the computer. Facilitators 
worked with each user from teacher sample n°2 individually, who were asked to perform 
the three tasks corresponding to the editor. Once the tasks had been completed, 
each  user  was  provided  with  a  printed  copy  of  the  end-user  usability  questionnaire 
adapted for use with the editor, and were asked to fill out the questionnaire.  

Finally,  in  the  last  stage  an  application  generated  with  the  editor  that  included  a 
learning guide made up of five pages and a questionnaire with five questions was installed 
 on  a  cellular  phone.  The  questions  contained  mathematics  learning  content, 
which  allowed  researchers  to  measure  the  efficiency,  errors  and  the  users’  learning 
regarding  how  to  use  the  engine.  Facilitators  worked  with  each  user  from  student 
sample n°2 individually, who were asked to perform the two tasks corresponding to 
the engine, which were measured by an evaluator, who filled out the efficiency and 
errors sections of the observation guidelines. Afterwards evaluators proceeded to the 
learning section of the observation guideline, by asking users to perform the specific 
tasks  included  in  the  guideline  one  at  a  time,  recording  whether  or  not  these  tasks 
were successfully completed.  

422 

J. Sánchez and M. Espinoza 

4   Results 

Regarding the evaluation of the editor 1.0 and editor 2.0, results showed that the mean 
scores  attributed  to  each  statement  on  the  guideline  improved  for  the  editor  2.0.  In 
order to prove whether or not this improvement was statistically significant, a Student 
T  test  for  independent  samples  was  utilized,  which  showed  that  for  each  statement, 
the increment was not statistically significant. It is worth pointing out that the results 
for each statement demonstrate the high level of the users’ acceptance of the editor. In 
the following, details regarding the results for each statement are provided: "I like the 
editor" (mean editor 1.0=8.24 (out of 10); mean editor 2.0=8.91), "I would work with 
the editor again" (mean editor 1.0=8.76; mean editor 2.0=9.09), "I would recommend 
the editor to other teachers" (mean editor 1.0=8.76; mean editor 2.0=8.82), "I felt in 
control of the editor" (mean editor 1.0=7.63; mean editor 2.0=7.82), "I knew what to 
do in every section of the editor" (mean editor 1.0=6.82; mean editor 2.0=8.36), "The 
editor  is  interactive"  (mean  editor  1.0=8.47;  mean  editor  2.0=8.82),  "The  editor  is 
easy to use" (mean editor 1.0=8.41; mean editor 2.0=9.18), "The editor is motivating" 
(mean  editor  1.0=8.65;  mean  editor  2.0=9.00),  "The  editor  adjusts  to  my  rhythm" 
(mean editor 1.0=7.88; mean editor 2.0=8.36) and "The images, colors and brightness 
of  the  editor  provide  me  with  information"  (mean  editor  1.0=8.71;  mean  editor 
2.0=8.82). 

is  motivating”  (mean=7.40),  “the  software  adapts 

Regarding the evaluation of the engine, the following results for each of the statements 
were obtained: “I like the software” (mean=7.17 (out of 10)), “the software is 
fun”  (mean=6.68),  “the  software  is  challenging”  (mean=7.50),  “the  software  makes 
me  active”  (mean=6.95),  “I  would  work  with  the  software  again”  (mean=7.85),  “I 
would recommend this software to other students” (mean=8.46), “I learned from using 
 the  software”  (mean=6.39),  “the  software  has  differing  levels  of  difficulty” 
(mean=7.19), “I felt I could control the situations in the software” (mean=6.80), “the 
software is interactive” (mean=7.46), “the software is easy to use” (mean=8.36), “the 
software 
to  my  rhythm” 
(mean=7.28),  “the  software  allowed  me  to  understand  new  things”  (mean=6.36),  “I 
like  the  images  in  the  software”  (mean=5.07),  “The  images  in  the  software  provide 
me  with  information”  (mean=5.98),  “I  understand  the  questions  that  were  asked” 
(mean=7.25), “it is easy to visualize the answer choices” (mean=7.97), “it is easy to 
select an answer” (mean=7.86), “it is easy to use the buttons on the cellular phone” 
(mean=8.62), “I can read the long texts easily” (mean=8.17), “the size of the text is 
adequate”  (mean=7.22),  “the  size  of  the  images  in  adequate”  (mean=4.70)  y  “it  is 
easy to read a large piece of text” (mean=7.40). In general, the results obtained show 
that the students expressed a high level of acceptance of the engine, except for some 
aspects  such  as  manipulating  and  visualizing  the  images  on  the  application,  which 
given the characteristics of cellular phones (small screens), is a difficult problem to 
remedy.  

The results regarding the observation of the use of the engine demonstrated that the 
students were able to complete the tasks in the amount of time that was expected for 
each task. In this way, for task 1 regarding the learning guide, the expected time was 5 
minutes, and the average time it took the users to complete the task was 4 minutes and 
12 seconds. For task 2 regarding the questionnaire, the expected time was 10 minutes, 
while  the  average  time  the  users  took  to  complete  the  task  was  10  minutes  and  12 

 

mGuides, Design and Usability of a Mobile System to Assist Learning 

423 

seconds. As far as the observation of errors, the results show that in general the students 
reduced the number of errors as they progressed through the various questions. 
If  they  originally  did  not  expand  the  texts  or  the  images  in  the  learning  guides  or 
questionnaires, by the end of the session they no longer made such errors. Observation 
 regarding  learning  of  the  interface  showed  that  the  students  failed  only  on  the 
first specific tasks, and that they did well on the later tasks. As a result, the users diminished 
 their  errors  progressively,  until  reaching  zero  errors  when  performing  the 
tasks with the engine.  

5   Conclusions 

The objective of this work was to present the impact of usability evaluations as part of 
the  process  for  the  development  of  the  mGuides  system.  The  usability  evaluations 
were  useful  for  validating  the  elements  that  had  been  designed  and  implemented  in 
each module, and also served to detect design problems that led to a redesign of some 
elements in the following development iteration. As a result, the usability evaluations 
supported  the  development  of  the  mGuides  system  by  validating  the  current  design 
and calling on the developers to improve the system in the following iteration. This 
was achieved through feedback provided by the results obtained in the usability testing,
 and by always using the guiding principle of providing the end users with a simple 
and usable tool in each module of the mGuides system. 

The results of the editor show that in the version 2.0, the editor improved in each of 
the  usability  indicators  utilized,  and  that  although  the  increments  or  gains  were  not 
statistically  significant,  the  results  pertaining  to  both  evaluations  (for  the  editor  1.0 
and editor 2.0) showed that these elements were clearly accepted by the teachers. The 
evaluations  demonstrated  that  the  editor  is  a  usable  and  understandable  tool,  which 
validates the design of its components. The editor allows the teacher to easily create, 
edit and generate a working guide project for his students; however, it is the teacher’s 
job to make sure that the content notes are presented in a logical order, and that the 
questions  in  the  questionnaire  are  related  to  the  contents  presented  in  the  learning 
guides.  

The  results  regarding  the  engine,  shown  through  the  usability  indicators,  demonstrate 
 a  high  level  of  acceptance  in  general  by  the  students.  The  lowest  score  was  
obtained  for  manipulating  and  visualizing  the  images,  which  is  probably  due  to  the 
reduced size of the screen on cellular phones. The most interesting aspects observed 
regarding the students’ use of the engine were the fact that it allowed the users to perform 
tasks within the expected timeframe, that it is very intuitive, and that it is not at 
all difficult to learn to use. The design of this system was very accurate from the point 
of view of the autonomy and freedom of work achieved by the users, given the context 
and the critical  situation  on  which  it is focused. However, for  future  work it is 
proposed to continue improving and extending the system’s characteristics. In addition,
  the  research  team  plans  to  widen  the  possible  uses  of  this  system  to  other  
contexts, and take advantage of the educational potential of the tool, as it can be integrated 
directly into the development of courses in various contents of learning and on 
differing levels. This would necessarily involve new system usability evaluations.  

 

424 

J. Sánchez and M. Espinoza 

Acknowledgments. This report was funded by the Chilean National Fund of Science 
and Technology, Fondecyt #1090352 and Project CIE-05 Program Center Education 
PBCT-Conicyt.  This  work  included  the  assistance  of  Manuel  Ibarra,  student  in  the 
Masters in Computer Sciences program at the Universidad de Chile, specifically regarding 
the usability evaluation, through observation of the students who were using 
the engine.   

References 

1.  Guerrero,  L.,  Ochoa,  S.,  Pino,  J.:  Selecting  Computing  Devices  to  Support  Mobile  Collaboration.
 Group Decision and Negotiation 15(3), 243–271 (2006) 

2.  Sánchez,  J.,  Mendoza,  C.,  Salinas,  A.:  Mobile  serious  games  for  collaborative  problem 
solving.  In:  Wiederhold,  B.K.,  Riva,  G.  (eds.)  The  Annual  Review  of  Cybertherapy  and 
Telemedicine  2009. Studies  in  Health  Technology  and  Informatics  (SHTI),  vol. 144, pp. 
193–197. IOS Press, Amsterdam (2009) 

3.  Sánchez, J., Espinoza, M.: Video Game Design for Mobile Phones. In: World Computer 

Congress 2010, Brisbane, Australia, September 20-23 (2010) (in press) 

4.  Nielsen, J.: Usability engineering. Academic Press Professional, New York (1993) 
5.  CNN en Inglés (2010),  

http://edition.cnn.com/2010/WORLD/americas/02/27/ 
chile.quake/index.html?iref=allsearch  

6.  Sanchez,  J.:  End-user  and  facilitator  questionnaire  for  Software  Usability.  Usability 

evaluation test. University of Chile, Santiago, Chile (2003) 

7.  Monares, A., Ochoa, S., Pino, J., Herskovic, V., Rodriguez-Covili, J., Neyem, A.: Mobile 
computing  in  urban  emergency  situations:  Improving  the  support  to  firefighters  in  the 
field. Expert Syst. Appl. 38(2), 1255–1267 (2011) 

8.  Chittaro, L., Zuliani, F., Carchietti, E.: Mobile devices in emergency medical services: user 
evaluation of a PDA-based interface for ambulance run reporting. In: Löffler, J., Klann, M. 
(eds.) Proceedings of the 1st International Conference on Mobile Information Technology 
for Emergency Response (MobileResponse 2007), pp. 19–28. Springer, Heidelberg (2007) 
9.  Omar,  A.,  Liu,  L.C.,  Koong,  K.S.:  From  disaster  recovery  to  Mobile  Learning:  a  case 

study. Int. J. Mob. Learn. Organ. 2(1), 4–17 (2008) 

10.  Kenteris,  M.,  Gavalas,  D.,  Economou,  D.:  An  innovative  mobile  electronic  tourist  guide 

application. Personal Ubiquitous Comput. 13(2), 103–118 (2009) 

 

