Content-Based Video Copy Detection: PRISMA at TRECVID 2010

PRISMA Research Group, Department of Computer Science, University of Chile

Juan Manuel Barrios and Benjamin Bustos

Blanco Encalada 2120, Santiago, Chile
{jbarrios,bebustos}@dcc.uchile.cl

Abstract

We present PRISMA’s Video Copy Detection system
(P-VCD). The system is based on visual-only global descriptors,
 weighted combinations of distances, a pivotbased 
index structure, and a novel approximated search
and voting algorithm for copy localization.

We submitted four Runs to TRECVID 2010 CCD

a combination of

a combination of

task:
PRISMA.m.balanced.ehdNgryhst:
edge histogram and gray histogram.
PRISMA.m.balanced.ehdNclrhst:
edge histogram and color histogram.
PRISMA.m.nofa.ehdNgryhst: a combination of edge
histogram and gray histogram.
PRISMA.m.nofa.ehdNghT10: a combination of edge histogram 
and gray histogram with a diﬀerent threshold.
P-VCD’s results show that the combination of edge
histogram and gray histogram is slightly better than
edge histogram and color histogram. These results
were positioned above the median, and considering just
video-only Runs, were the bests positioned for Balanced 
and Nofa proﬁle. These results also show that
our pivot-based index enables to discard 99.9% of distance 
evaluations and still have good eﬀectiveness, and
that global descriptors can achieve competitive results
with TRECVID transformations.

1 Introduction

Content-Based Video Copy Detection (CBVCD)
consists in detecting and retrieving videos that are
copies of known original videos. The detection method
only makes use of visual and audio content, avoiding to
embed watermarks into original videos. Joly et al. [8]
propose a deﬁnition of copy based on a subjective notion 
of tolerated transformations. A tolerated transformation 
is a function that creates a new version of a
document where the original document “remains recog-
nizable”. Let T be a set of tolerated transformations,
and u and v be two documents, v will be a copy of u if
∃t ∈ T, t(u) = v.

TRECVID 2010 CCD evaluation considered 56 au-
dio+visual transformations. Reference video collection
was composed of 11,524 video ﬁles with a total extension 
of 425 hours. Query video collection was composed
of 11,256 audio+visual queries (201 base queries × 8 visual 
transformations × 7 audio transformations) with
a total extension of 224 hours.

In this paper we present P-VCD, a CBVCD system
based on visual-only global descriptors, weighted combinations 
of distances, approximated searches with a
pivot-based index structure, and a voting algorithm for
copy localization. The rest of the paper is structured
as follows: Section 2 reviews the current work for CBVCD,
 Section 3 shows our system in details, Section 4
reviews our system’s results in TRECVID 2010, and ﬁnally 
Section 5 gives some conclusions and future work.

2 Related Work

CBVCD systems relies on two diﬀerent tasks. On
the one hand, Content description task consists in calculating 
representative descriptors for a video sequence.
Descriptors can be either global or local. Global descriptors 
represent the content of a whole frame, as in [6] and
[9]. Local descriptors represent the neighborhood of interest 
points in a frame, the most used for CBVCD are
SIFT [13] and SURF [1].

On the other hand, Similarity search task corresponds 
to the algorithm for ﬁnding objects in an indexed 
collection that match the query. Three diﬀerent 
approaches are used for CBVCD: continuous, discrete,
 and probabilistic. First, the continuous approach
corresponds to the traditional search method in metric
spaces. Given a query video and a distance function, it
performs a range or a nearest neighbor search, selecting 
the closest objects to the query. Kim et al. [9] use
a global descriptor and perform a range search linearly
without any indexing structure. Gupta et al. [5] showed
good results for audio-based CBVCD in TRECVID
2009 performing a linear nearest neighbor search implemented 
on GPU. Second, the discrete approach represents 
each descriptor with a value taken from a ﬁxed

3.1 Preprocessing

This task tries to minimize the eﬀect of visual transformations 
by ﬁltering frames and applying inverse
transformations. Each video enters to a transformation 
chain, producing one or more derived videos that
are used by the next tasks.

Figure 2 resumes the transformation chain for query
videos and reference videos. Five transformation were
implemented:
• Skip Irrelevant Frames. Processes the whole video
and marks a frame to be skipped if it is Plain or Outlier.
 A frame is plain when the variance of intensity
pixels or the diﬀerence between minimum and maximum 
intensity pixels is smaller than a threshold. A
frame fi is an outlier if the previous frame fi(cid:0)1 and
next frame fi+1 are similar between them and frame
fi is very diﬀerent to fi(cid:0)1 and fi+1. We used a function 
d(x, y) that scales both frames x and y to 20×15
and sums diﬀerences for every intensity pixel, then fi
is an outlier when both d(fi(cid:0)1, fi) and d(fi, fi+1) are
greater than a threshold and d(fi(cid:0)1, fi+1) is smaller
than a threshold. A skipped frame is replaced with
previous frame in video, or with next non-plain frame
if it is the ﬁrst frame in video.

• Remove Black Borders. Calculates the median
and variance of intensity for each pixel of every frame
in a video, then removes rows and columns from borders 
which medians and variances are smaller than a
threshold.

• Flip. Applies a vertical mirroring to every frame.
• PIP Detection. First, it applies a Laplacian kernel
to every frame and it calculates the median for every 
pixel of processed frames, obtaining a mean-edge
frame. Second, it detects corner candidates by applying 
diﬀerent masks to the mean-edge frame (topleft,
 top-right, bottom-left, and bottom-right corner
masks). Third, it searches for rectangles by joining
one of each four corner types, and it calculates a
score. A picture-in-picture (PIP) is detected when
the rectangle with best score is greater than a threshold.
 Finally, PIP is reverted by creating two new
query videos: foreground video (each frame cropped
to the detected rectangle) and background video (detected 
rectangle ﬁlled with black pixels).

• Camcording Detection. Applies a Laplacian kernel 
to each frame and detects lines with a Hough
Transform. Four types of lines are detected (top, bottom,
 left, and right margins) by restricting position
and slope. Then, calculates the mean and variance of
the four lines for every frame. The camcording is detected 
when the number of lines detected is greater
than a threshold. The movement in camcording is

Figure 1. Tasks in P-VCD System.

set. A system implementing this approach showed good
eﬀectiveness and eﬃciency in TRECVID 2008 [4]. The
Glocal descriptor [12] divides the descriptor space in
blocks and quantizes local descriptors into those blocks
creating a frame global descriptor from its local descriptors,
 then uses an inverted index for searching similar
frames. Third, the probabilistic approach performs a
probability-based approximated search. Joly et al. [8]
partition the space with a Hilbert space ﬁlling curve and
estimate a distribution of descriptors for each block.
Poullot et al. [16] replace the Hilbert’s curves with Zorder 
curves maintaining the probability-based search
method. Law-To et al. [10] deﬁne a voting algorithm
based on a geometric model and track interest points
trajectories detecting copies even for complex transformations 
(like replacement of background or insertion of
characters in a scene).

3 P-VCD System

P-VCD system uses only global visual information
for detecting copies, thus only processes the 1,608
visual-only query videos (201 × 8).
It is divided in
ﬁve tasks: Preprocessing, Frame Sampling, Feature Extraction,
 Similarity Search, and Copy Localization (see
Figure 1).

Feature ExtractionFrame Sampling PreprocessingSimilarity SearchCopy LocalizationQueryVideosReferenceVideosDetectionResultFigure 2. Preprocessing task for query videos and reference videos. Output videos are used by next tasks.

reduced by using a content-relative reference point.
Finally, camcording is reverted by mapping detected
quadrilateral into a 4:3 rectangle.

Our implementation for PIP and camcording detection 
are far from perfect as they have a precision of
about 50% and a false alarm rate of about 50%, thus
original queries are kept with new queries. New queries
are treated as independent queries up to the copy localization 
task, where results are combined with original
queries. As a result of this task, the number of query
videos used by next tasks increased from 1,608 to 5,378.

ciency and for avoiding noisy frames, we extracted three
frames per second. Then, we compared each extracted
frame with the ﬁrst frame of the current chunk. When
the diﬀerence is smaller than a threshold, the frame is
added to the current video chunk, and when it is larger
than the threshold, the current chunk is ﬁnished and a
new video chunk is started with this frame.

The 11,524 reference videos (containing 39,463,431
frames) produced 3,967,815 reference video chunks.
The 5,378 query videos from processing task (containing 
11,526,076 frames) produced 990,246 query video
chunks.

3.2 Frame Sampling Task

3.3 Feature Extraction Task

This task divides each video in groups of similar consecutive 
frames. In the following, we will refer to these
groups as video chunks. Thus, every query video and
reference video is partitioned into video chunks. Note
that a video shot is a serie of interrelated consecutive
frames taken contiguously by a single camera and representing 
a continuous action in time and space [7]. We
do not divide videos into shots, because ﬁrst and last
frames of a shot may be very diﬀerent. A keyframe is
the frame which can represent the salient content of a
shot [18]. We do not extract keyframes, because we use
a whole group of frames for feature extraction instead
of representative frames.

For creating video chunks, we compare two frames by
calculating the maximum diﬀerence between intensity
of pixels after scaling both frames to 20 × 15. For eﬃThis 
task calculates one or more global descriptors
for representing a whole video chunk.
In our experiments,
 three global descriptors were extracted for each
frame:
• Edge Histogram. Based on MPEG-7 descriptor 
[14], captures the spatial distribution of edges in
a frame. Unlike the original deﬁnition, we use 10
orientations and we quantize each histogram bin uniformly 
into a 8-bit value, resulting a vector of 160
dimensions.

• Gray Histogram. Converts a frame to gray scale,
divides it into 3 × 3 blocks, and for each block calculates 
a 20 bins histogram. Each histogram bin is
uniformly quantized into a 8-bit value, resulting a
vector of 180 dimensions.

Skip IrrelevantFramesRemove BlackBordersPIP DetectionCamcordingDetectionSkip IrrelevantFramesRemove BlackBordersReferenceVideoOutputVideoFlipQueryVideoOutputVideoOutputVideoSkip IrrelevantFramesRemove BlackBordersFlipOutputVideoOutputVideoBackgroundForegroundNoDetected• Color Histogram. Divides a frame into 2×2 blocks
and for each block calculates a histogram of 16 bins
for each R, G, and B channel. Each histogram bin
is uniformly quantized into a 8-bit value, resulting a
vector of 192 dimensions.

Finally, we deﬁned the global descriptor for a video
chunk as the average of the global descriptor for each
of its frames [15]. Thus, each video chunk is represented 
by three descriptors:
the Average Edge Histogram 
(AEH), the Average Gray Histogram (AGH),
and the Average Color Histogram (ACH) (160, 180 and
192 bytes, respectively). The following table shows the
space used by reference video chunks and query video
chunks:

AEH
AGH
ACH

reference

query
605 MB 151 MB
681 MB 170 MB
727 MB 181 MB

3.4 Similarity Search Task

Let Q be the set of query video chunks and R be the
set of reference query chunks. The similarity search
tasks takes each query video chunk Q ∈ Q and determines 
the most similar reference video chunks R ∈ R.
For measuring the degree of similarity between two
video chunks a temporal distance function is deﬁned.
The search is an approximated similarity KNN+range
search (i.e., approximated search of the K closest objects 
to a query object inside a range threshold τ ).

3.4.1 Distance Function

Let m be the number of descriptors extracted for a
video chunk, and let dimi and γi be dimensionality and
a similarity function for ith descriptor, i ∈ {1, ..., m}.
In our experiments we deﬁned γi as L1 distance (Man-
hattan) for AEH, AGH and ACH descriptors:

dimi∑

γ1(⃗x, ⃗y) =

j=1

|xj − yj|

Let desci(Q) be the ith descriptor for video chunk
Q, we deﬁned similarity between video chunks Q and
R as a weighted combination between its descriptors:

m∑

δ(Q, R) =

i=1

wi · γi(desci(Q), desci(R))

distances d(o1, o2), and accumulating them into a histogram 
[3].

We set initials weights by ﬁrst calculating distance
histograms for γi functions. Then, we selected each wi
independently to a value that normalizes to 1 the distance 
that covers a fraction α ∈ (0, 1] of pairs on γi

distance histogram. α should be ﬁxed to a near zero
value due that we need smaller distances to be comparable 
(in our experiments we ﬁxed α = 0.0001).

However, depending on visual descriptors that are
being combined, sometimes δ is biased to some γi. The
α-normalization is not accurate when the growth between 
zero and α of γi histograms are too diﬀerent.
Then, we developed a novel technique for correcting
weights. Given a distance histogram, the intrinsic dimensionality 
ρ = (cid:22)2
2(cid:27)2 quantiﬁes how hard is to search
on that metric space [3]. In our experiments, we realized 
that a combination of weights that implies a higher
ρ for δ is less biased that a combination with a smaller
ρ. Then, our correction method is to seek for a local

maximum of ρ given a initial set of weights {w1, ..., wm}

from the α-normalization. For maximization, NewtonRaphson 
method can be used, however we used a simpler 
approach that iteratively replaces one wi with wi±ϵ

if that change increases ρ, and ends when every weight
have been tested and none was updated.

We deﬁned a temporal distance function between
two video chunks as the average similarity between
2W + 1 consecutive chunks:

W∑

1

2W + 1

d=(cid:0)W

DIST (Qs, Rt) =

δ(Qs+d, Rt+d)

(1)

3.4.2 Pivot-based Index

A naive approach for similarity search would requiere
to evaluate DIST function |Q|×|R| times, i.e. 990,246
· 3,967,815 distance evaluations, where each evaluation
requieres at least 160 + 180 operations. This naive approach 
would take several months on a desktop computer.


A function d is a metric when it satisﬁes the properties 
of reﬂexivity (d(x, y) = 0 iﬀ x = y); non-negativity
(d(x, y) > 0 iﬀ x ̸= y); symmetry (d(x, y) = d(y, x));
and triangle inequality (d(x, y) + d(y, z) ≥ d(x, z)). If
every γi complies with metric properties (Manhattan is
a metric), then δ and DIST also complies with metric 
properties. Let P ⊆ R be a set of video chunks
from reference videos, the lower bound function LBP
is deﬁned as:

We selected weights w1, .., wm using a novel technique 
that depends on γi and δ distance histograms.
A distance histogram of a function d is constructed by
sampling many pairs of objects o1, o2 ∈ R, evaluating

LBP (Q, R) = max
P2P

{|DIST (P, Q) − DIST (P, R)|}

(2)

Each object P ∈ P is called a pivot. Note that if
DIST (P, x) is precalculated ∀x ∈ Q∪R, then with just
|P| operations is possible to evaluate LBP . Because
DIST satisﬁes metric properties, then for every pair of
video chunks Q and R:

DIST (Q, R) ≥ LBP (Q, R) ∀P ⊆ R

(3)
The index structure consists of a |P| × |R| table
with distances from each pivot to every reference video
chunk. Additionally, for eﬃcient calculation of DIST ,
each video chunk has a reference to the previous and
next video chunks.

3.4.3 Approximated Search with Pivots
Given a query video chunk Q ∈ Q the ﬁrst step is
to calculate DIST (P, Q)∀P ∈ P, and then perform
a KNN+range search. Algorithm 1 shows a classic algorithm 
for pivot-based KNN+range search.
It uses
Equation 3 to evaluate DIST only when the lower
bound is lesser than both the range τ and the K th candidate 
distance.

Algorithm 1: Classic Pivot KNN+range search.
Input: Q query video chunk, R reference video

chunks, K number of NNs, (cid:28) threshold for
range search, P set of pivots.

Output: List of K nearest neighbors to Q
NNs   new priority queue
foreach R 2 R do
lb   LBP (Q, R)
if lb < (cid:28) and ( size of NNs < K or lb < max
distance in NNs ) then
dist   DIST (Q, R)
if dist < (cid:28) then

// see Equation 3
// see Equation 1

// see Equation 2

add R to NNs with distance dist
if

size of NNs > K then
remove max distance object from NNs

return NNs

However, in our experiments Algorithm 1 was not
fast enough. We tested with diﬀerent values for τ , K,
and P but the results were not satisfying. Then, we
developed a novel technique for approximated searches.
While performing our experiments we realized that
the lower bound for the nearest neighbor was usually
between the lowest lower bounds. With that property
in mind, we developed Algorithm 2 that uses LBP as an
estimator for actual distance: evaluates LBP for every
object, discards objects with LBP greater that threshold 
τ , selects the T lowest LBP values and just for them
evaluates DIST . Finally, between the T evaluated distances,
 selects the K nearest neighbors that are lesser
than τ . This is an approximated search because there

is not guarantee that actual NN’s lower bound will be
between the T lowest lower bounds.

Algorithm 2: Approximated KNN+range search.
Input: Q query video chunk, R reference video

chunks, K number of NNs, (cid:28) threshold for
range search, P set of pivots, T number of
lower bounds.

Q

Output: List of approximated K nearest neighbors to
MinLbs   new priority queue
foreach R 2 R do
lb   LBP (Q, R)
if lb < (cid:28) then

// see Equation 2

add R to MinLbs with distance lb
if

size of MinLbs > T then
remove max distance object from MinLbs

NNs   new priority queue
foreach R 2 MinLbs do
dist   DIST (Q, R)
if dist < (cid:28) then

// see Equation 1

add R to NNs with distance dist
if

size of NNs > K then
remove max distance object from NNs

return NNs

The key diﬀerence between classic search and our
approximated search is that while Algorithm 1 uses a
LBP value as a lower bound for discarding objects with
a high DIST , Algorithm 2 compares LBP values between 
them, assuming that a low/high LBP value implies 
a low/high DIST value, thus LBP is used just as
a cheap DIST estimator.

Algorithm 2 needs that LBP be a good estimator for
DIST . Tightness between LBP and DIST depends
on the size and quality of P. Given two sets of pivots 
P1 ⊂ P2 then LBP1 (a, b) ≤ LBP2 (a, b)∀a, b. Then,

selecting more pivots usually implies better approximations,
 but also implies more operations. With a better
approximation, however, a smaller T is necessary for
selecting actual NNs. Thus, there is a tradeoﬀ between
the |P|, T and approximation. Note that when T tends
to |R| the result of Algorithm 2 tends to Algorithm 1
independently of P.

3.4.4 Pivot selection and evaluation

Pivot selection is critical for Algorithm 2. A naive approach 
for selecting pivots is to select objects randomly
in R. However, a key property for good pivot selection 
is that pivots should be far away between each
other [17]. Sparse Spatial Selection (SSS) [2] uses this
property for selecting pivots incrementally. Algorithm 3
shows our implementation for SSS that ﬁrst randomizes
R and then selects sparse pivots.

Because LBP is a lower bound of DIST , the higher

minimum distance between pivots.

Algorithm 3: Pivot selection, based on SSS [2].
Input: R set of reference video chunks, treshold
Output: P set of pivots
(A0; :::; AjRj)   randomize objects in R
P   fA0g
foreach Ai 2 (A1; :::; AjRj) do

if 8 p 2 P, DIST (p, Ai) (cid:21) treshold then

P   P [ fAig

return P

the value of LBP the tighter to DIST . Then, the evaluation 
algorithm samples many pairs of objects Q and R,
calculates µP that is the average value of LBP (Q, R),
and selects the set of pivots P that has the higher µP .
Because the pivot selection depends on randomization
of R, we selected k sets of pivots, we compared their
respective µP , and we kept the set of pivots with higher
µP .

3.5 Copy Localization

For a query video Q with video chunks {Q1, ..., Qt},
the input to this task is a set {S1, ..., St} where ∀i ∈
{1, ..., t} Si ⊂ R, Si is a list with the nearest neighbors
for query video chunk Qi, and 0 ≤ |Si| ≤ K. The output 
of this task is a list of copy detections candidates,
where each copy detection is composed of the query
video segment (start/end time), the reference video oﬀset,
 and a copy detection score. An oﬀset is the time
than needs to be added to query video segment times
for getting reference video segment times, i.e., query
start + oﬀset = reference start and query end + oﬀset
= reference end.

We designed a voting algorithm for copy localization.
First, it compares every Qi with its neighbors in Si recollecting 
every video reference candidate and minimum
and maximum oﬀsets. Then, for every reference video
and oﬀset interval a copy detection is performed with
Algorithm 4.

For each reference video and oﬀset candidate, Algorithm 
4 returns query video segment (start/end) and
a copy detection score. The score is the sum of votes
of supporting reference video chunks. Each supporting
video chunk and its vote is calculated with Algorithm 5.
M atchV ote is the value for a supporting vote (in our
implementation is 1), that is weighted according to the
relevance of distance and position of the voter chunk.
The relevance of a distance is a value near 0 when distance 
is τ , and is 1 when distance is 0, the relevance
of a position is a value near 0 when voter is the K th,
and is 1 when is the 1st, in-between values should be assigned 
between those limits. M issCost is the cost when
there is not a reference chunk supporting the detection,

Algorithm 4: Copy detection algorithm.
Input: fQ1; :::; Qtg query video, fS1; :::; Stg nearest
neighbors, V reference video candidate, Oﬀset
oﬀset interval candidate.

Output: start, end and score for copy detection in
video V with oﬀset Oﬀset
(start, end, score)   (null, null, 0)
(cstart, cend, cscore)   (null, null, 0)
foreach Si 2 fS1; :::; Stg do
(voter, vote)   CalculateVote(Qi, Si, V, Oﬀset)
cscore   cscore + vote
if cscore < 0 then

(cstart, cend, cscore)   (null, null, 0)

else if vote > 0 then
cstart   voter

if cstart is null then
cend   voter
if cscore > score then

(start, end, score)   (cstart, cend, cscore)

return ( start, end, score)

this value should be a negative value for penalizing detections 
with discontinuities (in our implementation is
-.1).

As a ﬁnal step, this task eliminates eventual overlaps 
between detections (by keeping the detection with
higher score), joins detections for videos that belongs to
the same original query video (created by preprocessing
task), and reports detections with best scores.

We set the decision thresholds by inspecting the density 
of detection scores. For Nofa proﬁle, we tested two
diﬀerent thresholds (nofa.ehdNgryhst a threshold of
7 votes, and nofa.ehdNghT10 a threshold of 10 votes).
However, as shown in next section, both thresholds resulted 
to be too low.

Algorithm 5: CalculateVote function.

Input: Qi query video chunk, Si list of nearest

neighbors for Qi, V reference video candidate,
Oﬀset oﬀset interval candidate.

match.

Output: voter best matching chunk, vote score of
(voter, vote)   (null, M issCost)
foreach (Rk; distance) 2 Si do

if Rk 2 V and oﬀset(Qi,Rk) 2 Oﬀset then

v   M atchV ote

(cid:2) relevance of distance between [0; (cid:28) ]
(cid:2) relevance of k between [1; K]

if v > vote then

(voter, vote)   (Rk, v)

return ( voter, vote)

4 Results Analysis

Visual transformations in TRECVID 2010 are: T1:
simulated camcording; T2: PIP original video in foreground;
 T3:
insertions of pattern; T4: strong reencoding;
 T5: change of gamma; T6: 3 transformations
between blur, change of gamma, frame dropping, contrast,
 compression, ratio, and white noise; T8: 3 transformations 
between crop, shift, contrast, caption, ﬂip,
insertion of pattern, and PIP original video in background;
 T10: random combination of 3 previous transformations.


The evaluation of a submitted Run relies on three

measures:
• NDCR: Measures the eﬀectiveness of the detection.
The closer to zero the better the eﬀectiveness of the
Run. For each proﬁle, a trivial NDCR of 1.0 can be
obtained by submitting an empty Run, thus a good
result should not be greater that 1.0.

• F1: Measures the accuracy in localization after a
copy has been detected correctly. The closer to 1.0
the better the accuracy.

• Mean processing time: Measures the eﬃciency for

processing queries.

These three measures are calculated at a submitted
decision threshold. Additionally, Optimal NDCR and
Optimal F1 are calculated by cutting the Run at the optimal 
decision score. TRECVID calculates these measures 
separately for each transformation, and for easier
comparing purposes we include the average result for
all transformations.

The submitted decision threshold for our four Runs
resulted to be too low. This is because we lacked of
training data to support an accurate selection of a decision 
threshold. Then, the actual NDCR for our four
submissions were too high (much greater than 1.0). Due
to this, our results analysis is based on Optimal NDCR
and Optimal F1 rather than NDCR and F1 at the submitted 
threshold.

The evaluation considered 56 audio+visual transformations 
for the query videos (8 visual transformations
and 7 audio transformations). Two proﬁles were evalu-
ated: Balanced and No False Alarms (Nofa). 22 teams
participated in the evaluation. Each team submitted 4
Runs, which resulted in 37 submissions for Nofa proﬁle 
(with 14 visual-only Runs), and 41 submissions for
Balanced proﬁle (with 15 visual-only Runs). We stated
that a Run is visual-only when its results for NDCR,
F1 and processing time are identical for the 7 audio
transformations in a same visual transformation (thus,
its results are not inﬂuenced by changes in audio).

In our experiments, we tested two descriptor combination 
for δ function in Equation 1:

• AEH and AGH descriptors (balanced.ehdNgryhst,

nofa.ehdNgryhst, and nofa.ehdNghT10 Runs):

δ(Q, R) =

0.069686411 · L1(AEH(Q), AEH(R))
+ 0.090415913 · L1(AGH(Q), AGH(R))
• AEH and ACH descriptors (balanced.ehdNclrhst):

δ(Q, R) = 0.068073519 · L1(AEH(Q), AEH(R))
+ 0.045144545 · L1(ACH(Q), ACH(R))

Weights for these two combinations comes from the

weight selection algorithm presented in section 3.4.1.

Tables 1, 2, and 3 and Figure 3 shows P-VCD sys-
tem’s results for Nofa and Balanced proﬁle. Runs
nofa.ehdNgryhst and nofa.ehdNghT10 only diﬀers on
the submitted threshold, thus both have the same results 
for Optimal NDCR and Optimal F1. For both
Runs, Optimal NDCR is better than the median for
every transformation, its best results are achieved on
T3 and T4 and its worst result on T5 and T6. The
Optimal F1 value tend to be around the median, but it
has its worst localization on T1.

Run balanced.ehdNgryhst had an Average Optimal 
NDCR of .597 (14th global rank and 1st visualonly 
rank) with an Average Optimal F1 of .820 (15th
global rank and 2nd visual-only rank). Its best results
are achieved on T2 and T3 and its worst results on T5.
Run balanced.ehdNclrhst had an Average Optimal 
NDCR of .658 (16th global rank and 3rd visualonly 
rank) with an Average Optimal F1 of .820 (16th
global rank and 3rd visual-only rank). Its best results
are achieved on T2 and T3 and its worst results on T6.
Mean Processing Time is higher than median in all
Runs, in particular for camcording and PIP transformations,
 mainly because the preprocessing task created
more query videos in those cases.
For all Runs, the parameters for Algorithm 2 were
K=6, τ =6, |P| = 9, and T =.001|R|. We ﬁxed these parameters 
by ﬁrst deciding the amount of time that similarity 
search should take (we decided that the search
should take no more than 24 hours total for all queries),
then we tested with diﬀerent values for T and |P| for
complying with that restriction. In Equation 1, we set
W =1 thus DIST function needs more than 1,000 operations 
to be evaluated, but LBP estimated it with just
9 operations, and actual DIST is evaluated just on
0.1% times (3,967 evaluations for each query chunk).
We made our tests on a Intel Q9400 CPU (2.66 GHz ×
4 cores) with 4 GB RAM on a GNU/Linux 2.6.18.

In summary, the results for our submitted Runs were
positioned above the median for Optimal NDCR and

Optimal F1, and considering just visual-only Runs,
they were the bests positioned for Balanced and Nofa
proﬁle. The results for Balanced proﬁle show that
a combination of edge histogram and gray histogram
is slightly better than edge histogram and color histogram.
 The results also show that our pivot-based
approximation enables to discard 99.9% of DIST evaluations 
and still have good eﬀectiveness, and that
global descriptors can achieve competitive results with
TRECVID transformations

5 Conclusions

In general, we are satisﬁed with our results, especially 
considering that: this is our ﬁrst participation in
TRECVID, our video copy detection system uses rather
simple global descriptors, we do not use any audio information,
 and we achieved this results on a standard
desktop computer. It is well known that local descriptors 
should be used for complex transformations [11],
however we have shown that, with a preprocessing task,
an indexing structure, and a good approximation technique,
 global descriptors can achieve competitive results 
with TRECVID transformations and even beat
many systems that use local descriptors and/or combine 
audio and visual information.

Another interesting property is that δ function in
Equation 1 combines descriptors at the similarity search
task, thus enabling a novel way for fusing audio and
visual information. We plan to work on this issue in a
future. Other issues we plan to address are: analyze the
impact of approximated search parameters on detection
result, test other distance functions, and test the fusion
with local descriptors.

References

[1] Herbert Bay, Andreas Ess, Tinne Tuytelaars, and
Luc Van Gool.
features
(SURF). Computer Vision and Image Understanding,
110(3):346–359, 2008.

Speeded-up robust

[2] Benjamin Bustos, Oscar Pedreira, and Nieves Brisaboa.
 A dynamic pivot selection technique for similarity 
search. In Proc. of the intl. workshop on Similarity 
Search and Applications (SISAP), pages 105–112.
IEEE, 2008.

[3] E. Ch´avez, G. Navarro, R. Baeza-Yates, and Jos´e Luis
Marroqu´ın. Searching in metric spaces. ACM Computing 
Surveys, 33(3):273–321, 2001.

[4] Matthijs Douze, Adrien Gaidon, Herve Jegou, Marcin
Inria Lear’s video

Marszalek, and Cordelia Schmid.
copy detection system. In TRECVID, 2008.

[5] Vishwa Gupta, Gilles Boulianne, and Patrick Cardinal.
 CRIM’s content-based audio copy detection system 
for TRECVID 2009. In Proc. of the intl. workshop
on Content-Based Multimedia Indexing (CBMI), 2010.

[6] Arun Hampapur and Ruud Bolle. Comparison of distance 
measures for video copy detection. In Proc. of
the IEEE intl. conf. on Multimedia and Expo (ICME),
pages 737–740. IEEE, 2001.

[7] Alan Hanjalic. Shot-boundary detection: unraveled
and resolved? IEEE Trans. on Circuits and Systems
for Video Tech., 12(2):90–105, 2002.

[8] Alexis Joly, Olivier Buisson, and Carl Fr´elicot.
Content-based copy retrieval using distortion-based
probabilistic similarity search. IEEE Trans. on Multimedia,
 9(2):293–306, 2007.

[9] Changick Kim and Bhaskaran Vasudev. Spatiotemporal 
sequence matching for eﬃcient video copy detection.
 IEEE Trans. on Circuits and Systems for Video
Techn., 15(1):127–132, 2005.

[10] Julien Law-To, Olivier Buisson, Valerie Gouet-Brunet,
and Nozha Boujemaa. Robust voting algorithm based
on labels of behavior for video copy detection. In Proc.
of the intl. conf. on Multimedia (MM), pages 835–844.
ACM, 2006.

[11] Julien Law-To, Li Chen, Alexis Joly, Ivan Laptev,
Olivier Buisson, Valerie Gouet-Brunet, Nozha Boujemaa,
 and Fred Stentiford. Video copy detection: a
comparative study. In Proc. of the intl. conf. on Image 
and Video Retrieval (CIVR), pages 371–378. ACM,
2007.

[12] Duy-Dinh Le, Sebastien Poullot, Michel Crucianu, Xiaomeng 
Wu, Michael Nett, Michael E. Houle, and
Shin’ichi Satoh. National institute of informatics, japan
at TRECVID 2009. In TRECVID, 2009.

[13] David Lowe. Distinctive image features from scaleinvariant 
keypoints. International Journal of Computer
Vision, 60(2):91–110, 2004.

[14] B. S. Manjunath, Jens-Rainer Ohm, Vinod V. Vasudevan,
 and Akio Yamada. Color and texture descriptors.
IEEE Trans. on Circuits and Systems for Video Tech.,
11(6):703–715, 2001.

[15] A. Muﬁt Ferman, S. Krishnamachari, A. Murat Tekalp,
M. Abdel-Mottaleb, and R. Mehrotra. Group-of-
frames/pictures color histogram descriptors for multimedia 
applications. In Proc. of the intl. conf. on Image
Processing (ICIP), pages 65–68. IEEE, 2000.

[16] S´ebastien Poullot, Olivier Buisson, and Michel Crucianu.
 Z-grid-based probabilistic retrieval for scaling up
content-based copy detection. In Proc. of the intl. conf.
on Image and Video Retrieval (CIVR), pages 348–355.
ACM, 2007.

[17] Pavel Zezula, Giuseppe Amato, Vlastislav Dohnal, and
Michal Batko. Similarity Search: The Metric Space
Approach (Advances in Database Systems). Springer,
2005.

[18] Yueting Zhuang, Yong Rui, and Thomas Huang. Video
key frame extraction by unsupervised clustering and
feedback adjustment. Journal of Computer Science and
Technology, 14:283–287, 1999.

Optimal NDCR
Global Rank
Visual-Only Rank
Optimal F1
Global Rank
Visual-Only Rank
Mean processing time [s.]
Global Rank
Visual-Only Rank

T1
.977
12th
1st
.484
21st
5th
269
25th
12th

T2
.631
9th
1st
.946
5th
1st
170
24th
12th

T3
.269
8th
1st
.878
19th
7th
108
20th
8th

T4
.262
8th
2nd
.877
14th
5th
70
14th
5th

T5
.769
15th
5th
.753
24th
8th
84
18th
6th

T6
.708
15th
5th
.838
20th
6th
75
16th
7th

T8
.562
8th
1st
.931
12th
5th
126
23th
11th

.611

10th of 37
1st of 14

T10 Average T1–T10
.708
12th
2nd
.916
15th
5th
123
23th
11th

14th of 37
1st of 14

23th of 37
11th of 14

.828

128

Table 1. Results for nofa.ehdNgryhst and nofa.ehdNghT10 Runs. Global Rank is the position between the
37 submitted Runs for Nofa proﬁle. Visual-Only Rank is the position between the 14 Runs that did not use
audio information.

Optimal NDCR
Global Rank
Visual-Only Rank
Optimal F1
Global Rank
Visual-Only Rank
Mean processing time [s.]
Global Rank
Visual-Only Rank

T1
.977
19th
2st
.484
23st
5th
269
30th
12th

T2
.515
9th
2st
.888
14th
3st
170
29th
13th

T3
.269
9th
1st
.878
17th
7th
108
24th
8th

T4
.262
9th
2nd
.877
14th
6th
70
19th
6th

T5
.777
20th
6th
.747
24th
8th
84
23th
7th

T6
.708
18th
5th
.838
19th
6th
75
21th
8th

T8
.562
14th
2st
.931
11th
6th
126
25th
9th

.597

14th of 41
1st of 15

T10 Average T1–T10
.708
17th
4nd
.916
10th
4th
123
27th
11th

17th of 41
3st of 15

27th of 41
11th of 15

.820

128

Table 2. Results for balanced.ehdNgryhst Run. Global Rank is the position between the 41 submitted Runs
for Balanced proﬁle. Visual-Only Rank is the position between the 15 Runs that did not use audio information.

Optimal NDCR
Global Rank
Visual-Only Rank
Optimal F1
Global Rank
Visual-Only Rank
Mean processing time [s.]
Global Rank
Visual-Only Rank

T1
.962
17th
1st
.583
21st
4th
276
31th
13th

T2
.454
7th
1st
.893
11th
2st
179
30th
14th

T3
.346
11th
2st
.894
14th
5th
112
26th
10th

T4
.608
16th
3nd
.809
22th
8th
70
20th
7th

T5
.638
18th
5th
.849
19th
6th
86
24th
8th

T6
.808
19th
6th
.767
24th
8th
76
22th
9th

T8
.762
17th
4st
.952
6th
3th
130
28th
12th

.658

16th of 41
3st of 15

T10 Average T1–T10
.685
16th
3nd
.813
21th
7th
130
28th
12th

16th of 41
2st of 15

28th of 41
12th of 15

.820

132

Table 3. Results for balanced.ehdNclrhst Run. Global Rank is the position between the 41 submitted Runs
for Balanced proﬁle. Visual-Only Rank is the position between the 15 Runs that did not use audio information.

Figure 3. TRECVID results
four
balanced.ehdNgryhst, and balanced.ehdNclrhst.

our

for

submitted Runs:

nofa.ehdNgryhst,

nofa.ehdNghT10,

204060Transformation number0.0010.0100.1001.00010.000100.0001000.0009999.999minimal NDCRRun score (dot) versus median (---) versus best (box) by transformationTRECVID 2010: copy detection results (no false alarms application profile) Run name:                           PRISMA.m.nofa.ehdNghT10Run type:                           audio+video     204060Transformation number0.00.20.40.60.81.0Optimal mean F1 for TPsRun score (dot) versus median (---) versus best (box) by transformation204060Transformation number0.0010.0100.1001.00010.000100.0001000.0009999.999mean processing time (s)Run score (dot) versus median (---) versus best (box) by transformation204060Transformation number0.0010.0100.1001.00010.000100.0001000.0009999.999minimal NDCRRun score (dot) versus median (---) versus best (box) by transformationTRECVID 2010: copy detection results (no false alarms application profile) Run name:                           PRISMA.m.nofa.ehdNgryhstRun type:                           audio+video     204060Transformation number0.00.20.40.60.81.0Optimal mean F1 for TPsRun score (dot) versus median (---) versus best (box) by transformation204060Transformation number0.0010.0100.1001.00010.000100.0001000.0009999.999mean processing time (s)Run score (dot) versus median (---) versus best (box) by transformation204060Transformation number0.0010.0100.1001.00010.000100.0001000.0009999.999minimal NDCRRun score (dot) versus median (---) versus best (box) by transformationTRECVID 2010: copy detection results (balanced application profile) Run name:                           PRISMA.m.balanced.ehdNclrhstRun type:                           audio+video     204060Transformation number0.00.20.40.60.81.0Optimal mean F1 for TPsRun score (dot) versus median (---) versus best (box) by transformation204060Transformation number0.0010.0100.1001.00010.000100.0001000.0009999.999mean processing time (s)Run score (dot) versus median (---) versus best (box) by transformation204060Transformation number0.0010.0100.1001.00010.000100.0001000.0009999.999minimal NDCRRun score (dot) versus median (---) versus best (box) by transformationTRECVID 2010: copy detection results (balanced application profile) Run name:                           PRISMA.m.balanced.ehdNgryhstRun type:                           audio+video     204060Transformation number0.00.20.40.60.81.0Optimal mean F1 for TPsRun score (dot) versus median (---) versus best (box) by transformation204060Transformation number0.0010.0100.1001.00010.000100.0001000.0009999.999mean processing time (s)Run score (dot) versus median (---) versus best (box) by transformation