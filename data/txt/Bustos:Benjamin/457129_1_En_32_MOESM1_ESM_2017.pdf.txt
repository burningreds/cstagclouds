Eﬃcient Temporal Kernels between Feature Sets

for Time Series Classiﬁcation

Supplementary material

Romain Tavenard1, Simon Malinowski2, Laetitia Chapel3, Adeline Bailly1,

Heider Sanchez4, and Benjamin Bustos4

1 Univ. Rennes 2 / LETG-Rennes COSTEL, IRISA

2 Univ. Rennes 1 / IRISA

3 Univ. Bretagne Sud / IRISA

4 Dept. of Computer Science, Univ. Chile

1 Full results table

The performance in terms of classiﬁcation error rate of the proposed method
(KF S) is given in Table 1, together with competing methods.

Dataset
Adiac *

ArrowHead *

Beef *

BeetleFly

BirdChicken

CBF *
Car *

ChlorineConcentration *

CinCECGtorso

Coﬀee *

Computers
CricketX
CricketY
CricketZ

DiatomSizeReduction *
Distal[...]AgeGroup *

Distal[...]Correct *
DistalPhalanxTW

ECG200 *
ECG5000 *

ECGFiveDays *
Earthquakes *

ElectricDevices *

FaceAll *
FaceFour *
FacesUCR *

BOSS [5] DTDC [3] LS [4] TSBF[2] BoW[1]
0.248
0.194
0.267
0.100
0.200
0.000
0.083
0.442
0.180
0.000
0.328
0.236
0.226
0.203
0.111
0.147
0.178
0.210
0.100
0.051
0.000
0.202
0.361
0.226
0.023
0.054

0.235
0.166
0.200
0.100
0.050
0.002
0.167
0.339
0.113
0.000
0.244
0.264
0.246
0.254
0.069
0.252
0.272
0.324
0.130
0.059
0.000
0.252
0.201
0.218
0.000
0.043

0.299
0.280
0.333
0.350
0.200
0.020
0.217
0.287
0.148
0.000
0.284
0.246
0.226
0.226
0.085
0.338
0.275
0.424
0.160
0.076
0.178
0.295
0.406
0.101
0.182
0.092

0.478
0.154
0.133
0.200
0.200
0.009
0.233
0.408
0.130
0.000
0.416
0.259
0.282
0.259
0.020
0.281
0.221
0.374
0.120
0.068
0.000
0.259
0.413
0.251
0.034
0.061

0.230
0.246
0.433
0.200
0.100
0.012
0.217
0.308
0.288
0.000
0.244
0.295
0.264
0.285
0.101
0.288
0.217
0.324
0.160
0.060
0.123
0.252
0.297
0.256
0.000
0.133

KFS
0.179
0.246
0.200
0.000
0.200
0.000
0.067
0.135
0.013
0.000
0.296
0.233
0.236
0.215
0.046
0.175
0.180
0.215
0.100
0.060
0.055
0.199
0.340
0.198
0.045
0.075

FiftyWords

Fish
FordA
FordB

GunPoint

Ham *

HandOutlines *

Haptics *
Herring *

InlineSkate *

InsectWingbeatSound *

ItalyPowerDemand *

LargeKitchenAppliances

Lightning2 *
Lightning7

Mallat
Meat *

MedicalImages *

Middle[...]AgeGroup *

Middle[...]Correct *
MiddlePhalanxTW *

MoteStrain

NonInvasiveFatalECGThorax1
NonInvasiveFatalECGThorax2

OSULeaf
OliveOil *

PhalangesOutlinesCorrect *

Phoneme *

Plane

Proximal[...]AgeGroup *

Proximal[...]Correct *
ProximalPhalanxTW *

RefrigerationDevices

ScreenType *
ShapeletSim *

ShapesAll *

SmallKitchenAppliances *
SonyAIBORobotSurface1
SonyAIBORobotSurface2

StarlightCurves

Strawberry *
SwedishLeaf *

Symbols *

SyntheticControl
ToeSegmentation1
ToeSegmentation2

Trace

0.295
0.011
0.070
0.289
0.000
0.333
0.097
0.539
0.453
0.484
0.477
0.091
0.235
0.164
0.315
0.062
0.100
0.282
0.455
0.220
0.455
0.121
0.162
0.099
0.045
0.133
0.228
0.735
0.000
0.166
0.151
0.200
0.501
0.536
0.000
0.092
0.275
0.368
0.141
0.022
0.024
0.078
0.033
0.033
0.061
0.038
0.000

0.246
0.074
0.235
0.347
0.013
0.448
0.135
0.601
0.453
0.491
0.527
0.049
0.205
0.131
0.342
0.073
0.067
0.255
0.500
0.258
0.500
0.232
0.159
0.110
0.116
0.133
0.239
0.732
0.000
0.205
0.206
0.229
0.555
0.563
0.400
0.162
0.352
0.290
0.108
0.038
0.043
0.104
0.037
0.003
0.193
0.285
0.010

0.270
0.040
0.043
0.083
0.000
0.333
0.519
0.532
0.375
0.562
0.394
0.040
0.299
0.180
0.205
0.050
0.267
0.336
0.429
0.220
0.494
0.117
0.741
0.230
0.223
0.833
0.235
0.782
0.000
0.166
0.151
0.224
0.485
0.571
0.050
0.232
0.336
0.190
0.125
0.053
0.089
0.093
0.068
0.003
0.066
0.085
0.000

0.242
0.166
0.150
0.401
0.013
0.238
0.146
0.510
0.359
0.615
0.375
0.117
0.472
0.262
0.274
0.040
0.067
0.295
0.422
0.186
0.403
0.097
0.158
0.138
0.240
0.167
0.170
0.724
0.000
0.151
0.127
0.190
0.528
0.491
0.039
0.815
0.328
0.205
0.222
0.023
0.046
0.085
0.054
0.007
0.219
0.200
0.020

0.266
0.023
0.085
0.111
0.020
0.352
0.137
0.490
0.438
0.591
0.426
0.056
0.163
0.164
0.288
0.146
0.117
0.253
0.200
0.312
0.386
0.169
0.076
0.061
0.087
0.167
0.219
0.731
0.000
0.132
0.192
0.210
0.523
0.525
0.011
0.092
0.285
0.092
0.179
0.021
0.047
0.082
0.017
0.003
0.026
0.077
0.000

0.198
0.023
0.070
0.082
0.020
0.276
0.119
0.506
0.391
0.629
0.372
0.050
0.136
0.246
0.260
0.068
0.100
0.246
0.215
0.318
0.358
0.179
0.045
0.042
0.145
0.133
0.179
0.752
0.000
0.180
0.131
0.205
0.485
0.523
0.161
0.142
0.171
0.218
0.153
0.022
0.044
0.061
0.109
0.007
0.044
0.092
0.000

TwoLeadECG
TwoPatterns

UWaveGestureLibraryAll *

UWaveGestureLibraryX
UWaveGestureLibraryY
UWaveGestureLibraryZ

Wafer
Wine *

WordSynonyms *

Worms

WormsTwoClass

Yoga

Average rank

0.019
0.007
0.061
0.238
0.315
0.305
0.005
0.259
0.362
0.442
0.169
0.082
3.165

0.015
0.000
0.062
0.225
0.302
0.321
0.007
0.389
0.270
0.351
0.377
0.144
4.371

0.004
0.007
0.047
0.209
0.297
0.253
0.004
0.500
0.393
0.390
0.273
0.166
3.912

0.134
0.024
0.074
0.169
0.264
0.228
0.005
0.389
0.312
0.312
0.247
0.181
3.753

0.024
0.007
0.173
0.231
0.312
0.274
0.008
0.370
0.365
0.343
0.271
0.131
3.176

0.040
0.000
0.029
0.171
0.246
0.236
0.005
0.352
0.281
0.304
0.265
0.077
2.624

Table 1: Comparison of error rates between our temporal feature set kernel (D =
8192) and local feature based state-of-the-art methods. Recall that BoW baseline
operates on the exact same feature sets as our KFS. Stars indicate datasets for
which cross-validation leads to take temporal information into account (γt > 0).

2 Parameter ranges

Table 2 gives the search range of the diﬀerent parameters used for cross-validation.

Parameter name

C
γK
γf
γt

Kernels

All kernels
All kernels

All variants of KFS

All temporal variants of KFS

Parameter range

100 − 106
10−1 − 105
100 − 106
100 − 106

Table 2: Parameter grids for all kernels used in the paper. For all parameters, 5
values are selected at regular logscale locations in the range.

3 Timing experiments on more datasets

Fig. 1 represents the mean square error approximation of the approximated kernel 
matrix versus the timing for two datasets: Adiac and Ham. These datasets
have been chosen so as to complement conclusions drawn in the paper for
ECG200 dataset. Indeed, Ham corresponds to a long time series case while
Adiac has more training time series than ECG200. This ﬁgure matches Figure
2 of the paper and conﬁrms that a good trade-oﬀ is obtained by the Fourier
approximation.

Fig. 2 shows the error rate versus the feature map dimension for Adiac and
Ham datasets. It conﬁrms the statements made in the paper (Figure 4) about

the beneﬁts of taking temporal information into account and of the kernel normalization.


Fig. 3 depicts the training and testing time of BoW, SQFD-k-means and
SQFD-Fourier as a function of the size of the training set for the same datasets.
This ﬁgure matches Figure 3 of the paper, and identical conclusions can be
drawn. The Fourier approximation leads to lower execution times than k-means
approximation. Compared to BOW, training time is reduced, especially for large
training sets.

(a) Adiac

(b) Ham

Fig. 1: Mean Squared Error (MSE) vs timings of the approximated kernel matrix
(Adiac and Ham).

(a) Adiac

(b) Ham

Fig. 2: Error rates as a function of the feature map dimension (Adiac and Ham).

10−410−310−2Timings(insecondspermatrixelement)10−710−610−510−410−310−2MSEk=23k=28D=23D=212SQFD-k-meansSQFD-Fourier10−410−310−2Timings(insecondspermatrixelement)10−710−610−510−410−310−210−1MSEk=23k=28D=23D=213SQFD-k-meansSQFD-Fourier23252729211FeaturemapdimensionD0.000.050.100.150.200.25ErrorrateSQFD-Fourierwithouttime(γt=0)SQFD-FourierwithtimeSQFD-Fourierwithtime(normalizedkernel)BoW23252729211FeaturemapdimensionD0.00.10.20.30.40.5ErrorrateSQFD-Fourierwithouttime(γt=0)SQFD-FourierwithtimeSQFD-Fourierwithtime(normalizedkernel)BoW(a) Training time (Adiac)

(b) Testing time (Adiac)

(c) Training time (Ham)

(d) Testing time (Ham)

Fig. 3: Training and testing times as a function of the amount of training data
(Adiac and Ham).

4 Pairwise comparisons

Figure 4 compares the performance of the proposed approach with two competing 
approaches : 1-nearest-neighbour (1NN) combined with DTW and BOSSVS 
[6]. Classiﬁcation error rates on the 85 datasets of the UCR-UEA repository
are depicted, together with the Win/Tie/Lose scores and the p-values corresponding 
to one-sided Wilcoxon signed rank tests.

References

1. Bailly, A., Malinowski, S., Tavenard, R., Chapel, L., Guyet, T.: Dense Bag-of-
Temporal-SIFT-Words for Time Series Classiﬁcation. Lecture Notes in Artiﬁcial
Intelligence 9785, 17–30 (2016)

0.20.40.60.81.0Fractionoftrainingset020040060080010001200Trainingtime(inseconds)BoW(k=1024)SQFD-k-means(k=128)SQFD-Fourier(D=1024)0.20.40.60.81.0Fractionoftrainingset0.00.10.20.30.40.50.60.70.8Testingtime(inseconds)BoW(k=1024)SQFD-k-means(k=128)SQFD-Fourier(D=1024)0.20.40.60.8Fractionoftrainingset01000200030004000500060007000Trainingtime(inseconds)BoW(k=1024)SQFD-k-means(k=128)SQFD-Fourier(D=1024)0.20.40.60.8Fractionoftrainingset0.00.20.40.60.81.01.2Testingtime(inseconds)BoW(k=1024)SQFD-k-means(k=128)SQFD-Fourier(D=1024)(a) SQFD vs1 NN-DTW

(b) SQFD vs BOSS-VS

Fig. 4: Pairwise performance comparisons.

2. Baydogan, M.G., Runger, G., Tuv, E.: A Bag-of-Features Framework to Classify
Time Series. IEEE Transactions on Pattern Analysis and Machine Intelligence
35(11), 2796–2802 (2013)

3. G´orecki, T., (cid:32)Luczak, M.: Non-isometric transforms in time series classiﬁcation using

DTW. Knowledge-Based Systems 61, 98–108 (2014)

4. Grabocka, J., Schilling, N., Wistuba, M., Schmidt-Thieme, L.: Learning time-series
shapelets. In: Proceedings of the ACM SIGKDD International Conference on Knowledge 
Discovery and Data Mining. pp. 392–401 (2014)

5. Sch¨afer, P.: The BOSS is concerned with time series classiﬁcation in the presence

of noise. Data Mining and Knowledge Discovery 29(6), 1505–1530 (2014)

6. Sch¨afer, P.: Scalable time series classiﬁcation. Data Mining and Knowledge Discovery 
pp. 1–26 (2015)

0.00.10.20.30.40.50.60.70.81NN-DTW0.00.10.20.30.40.50.60.70.8SQFD-Fourier(D=8192)W/T/L=69/4/12p<10−80.00.10.20.30.40.50.60.70.8BOSS-VS0.00.10.20.30.40.50.60.70.8SQFD-Fourier(D=8192)W/T/L=56/3/24p<10−5