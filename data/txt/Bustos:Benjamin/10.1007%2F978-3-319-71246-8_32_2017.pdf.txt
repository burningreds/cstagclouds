Eﬃcient Temporal Kernels Between Feature

Sets for Time Series Classiﬁcation

Romain Tavenard1(B), Simon Malinowski2, Laetitia Chapel3, Adeline Bailly1,

Heider Sanchez4, and Benjamin Bustos4

1 Univ. Rennes 2 / LETG-Rennes COSTEL, IRISA, Rennes, France

romain.tavenard@univ-rennes2.fr

2 Univ. Rennes 1 / IRISA, Rennes, France

3 Univ. Bretagne Sud / IRISA, Vannes, France

4 Department of Computer Science, University of Chile, Santiago, Chile

Abstract. In the time-series classiﬁcation context, the majority of the
most accurate core methods are based on the Bag-of-Words framework,
in which sets of local features are ﬁrst extracted from time series. A
dictionary of words is then learned and each time series is ﬁnally represented 
by a histogram of word occurrences. This representation induces
a loss of information due to the quantization of features into words as
all the time series are represented using the same ﬁxed dictionary. In
order to overcome this issue, we introduce in this paper a kernel operating 
directly on sets of features. Then, we extend it to a time-compliant
kernel that allows one to take into account the temporal information. We
apply this kernel in the time series classiﬁcation context. Proposed kernel
has a quadratic complexity with the size of input feature sets, which is
problematic when dealing with long time series. However, we show that
kernel approximation techniques can be used to deﬁne a good trade-oﬀ
between accuracy and complexity. We experimentally demonstrate that
the proposed kernel can signiﬁcantly improve the performance of time
series classiﬁcation algorithms based on Bag-of-Words.
Code related to this chapter is available at:
https://github.com/rtavenar/SQFD-TimeSeries
Data related to this chapter are available at:
http://www.timeseriesclassiﬁcation.com

1 Introduction

Time series classiﬁcation has many real-life applications in various domains such
as biology, medicine or speech recognition [17,24,27] and has received a large
interest over the last decades within the data mining and machine learning communities.
 Three main families of methods can be found in the literature in this
context: similarity-based methods, that make use of similarity measures between

Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-319-71246-8 32) contains supplementary material, which is
available to authorized users.

c(cid:2) Springer International Publishing AG 2017
M. Ceci et al. (Eds.): ECML PKDD 2017, Part II, LNAI 10535, pp. 528–543, 2017.
https://doi.org/10.1007/978-3-319-71246-8_32

Eﬃcient Temporal Kernels Between Feature Sets

529

raw time series, shapelet-based methods aiming at extracting small subsequences
that are discriminant of class membership, and feature-based methods, that rely
on a set of feature vectors extracted from time series. The interested reader can
refer to [1] for an extensive presentation of time series classiﬁcation methods.

The most used dissimilarity measures are the euclidean distance (ED) and the
dynamic time warping (DTW). The computational cost of ED is lower than the
one of DTW, but ED is not able to deal with temporal distortions. The combination 
of DTW and k-Nearest-Neighbors (k-NN) is one of the seminal approaches
for time series classiﬁcation thanks to its good performance. Cuturi [9] introduces
the Global Alignment Kernel that takes into account all possible alignments in
order to produce a reliable similarity measure to be used at the core of standard
kernel methods such as Support Vector Machines (SVM).

Ye and Keogh [29] introduce shapelets which are sub-sequences of time series
that have a high discriminating power between the diﬀerent classes. In this
framework, classiﬁcation is done with respect to the presence of absence of such
shapelets in tested time series. Hills et al. [15] use shapelets to transform the
time series into feature vectors representing distances from the time series to the
extracted shapelets. Grabocka et al. [13] propose a new classiﬁcation objective
function (applied to the shapelet transform) to learn the shapelets, that improves
accuracy and reduces the need to search for too many candidates.

Feature-based methods rely on extracting, from each time series, a set of feature
vectors that describe it locally. These feature vectors are quantized into words,
using a learned dictionary. Every time series is ﬁnally represented by a histogram
of word occurrences that then feeds a classiﬁer. Many feature-based approaches
for time series classiﬁcation can be found in the literature [2–4,19,25–27] and they
mostly diﬀer in the features they use.

This Bag-of-Word (BoW) framework has been shown to be very eﬃcient. However,
 it suﬀers from a major drawback: it implies a quantization step that is done
via a ﬁxed partitioning of the feature space. Indeed, for a given dataset, words are
obtained by clustering the whole set of features and this ﬁxed clustering might not
reﬂect very accurately the distribution of features for every individual time series.
This problem has been studied in the computer vision domain for image retrieval
for instance. To overcome this drawback, similarity measures operating directly
on feature sets have been considered [5,6,16]: every instance is represented by its
own raw feature sets, which models more accurately every single distribution of
features. These measures have been shown to improve accuracy in the image classiﬁcation 
context. However, their associated computational cost is quadratic with
the size of the feature sets, which is a strong limitation for their direct use in realworld 
applications. Moreover, they have never been applied to time series classiﬁcation 
purposes to the best of our knowledge.

In this paper, we propose a novel temporal kernel that takes as input a set of
feature vectors extracted from the time series and their timestamps. Unlike standard 
Bag-of-Word approaches, this kernel takes feature localization into account,
which leads to signiﬁcant improvement in accuracy. The distance between feature 
sets is computed using the signature quadratic form distance (SQFD for
short [5]) that is a very powerful tool for feature set comparison but has a high

530

R. Tavenard et al.

computing cost. We hence introduce an eﬃcient variant of our feature set kernel
that relies on kernel approximation techniques.

The rest of this paper is organized as follows. An overview of time series classiﬁcation 
approaches based on BoW is given in Sect. 2, together with alternatives
to BoW mainly used in the image community. The Signature Quadratic Form
Distance [5] is detailed in Sect. 3. In Sect. 4, we introduce a kernel based on this
distance together with a temporal variant of the latter kernel that enables taking
temporal information into account. We also propose an approximation scheme
that allows eﬃcient computation of both kernels. In the experimental section, we
evaluate the proposed kernel using SIFT features adapted to time series [2,7] and
show that it signiﬁcantly outperforms the original algorithm (based on quantized
features) on the UCR datasets [8].

2 Related Work

In this section, we ﬁrst give an overview of state-of-the-art techniques for time
series classiﬁcation that are based on the BoW framework, as the approach proposed 
in this paper aims at going beyond this framework. We focus on core classiﬁers,
 as the one proposed here. Such classiﬁers can be integrated into ensemble
classiﬁers in order to build more accurate overall classiﬁers. More information
about the use of ensemble classiﬁers in this context can be found in [1]. Then, we
give an insight about similarity measures deﬁned on feature sets for object comparison 
purposes. Such measures have been widely used in the image community,
but never for time series classiﬁcation to the best of our knowledge.

2.1 Bag-of-Words Methods for Time Series Classiﬁcation

Inspired by the text mining and computer vision communities, recent works in
time series classiﬁcation have considered the use of Bag-of-Words [2–4,19,25–27].
In a BoW approach, time series are ﬁrst converted into a histogram of word occurrences 
and then a classiﬁer is built upon this representation. In the following, we
focus on explaining how the conversion of time series into BoW is performed in the
literature.

Baydogan et al. [4] propose Time Series Bag-of-Features (TSBF), a BoW approach 
for time series classiﬁcation where local features such as mean, variance
and extrema are extracted on sliding windows. A codebook learned by a class
probability estimate distribution is then used to quantize the features into words.
In [27], discrete wavelet coeﬃcients are computed on sliding windows and then
quantized into words by a k-means algorithm. A similar approach denoted BOSS
using quantized Fourier coeﬃcients is proposed in [25]. The SAX representation
introduced in [18] can also be used to construct words. Histograms of n-grams of
SAX symbols are computed in [19] to form the Bag-of-Patterns (BoP) representation.
 In [26], they propose the SAX-VSM method, which combines SAX with
Vector Space Models. SMTS, a symbolic representation of Multivariate Time
Series (MTS) is designed in [3]. This method works as follows: a feature matrix

Eﬃcient Temporal Kernels Between Feature Sets

531

is built from MTS and the rows of this matrix are feature vectors composed of
a time index, values and gradients of the time series on all dimensions at this
time index. A dictionary of words is computed by giving random samples of this
matrix to diﬀerent decision trees. Xie and Beigi [28] extract keypoints from time
series and describe them by scale-invariant features that characterize the shapes
around the keypoints. Bailly et al. [2] compute multi-scale descriptors based on
the SIFT framework and quantize them using a k-means algorithm. Features
are either computed at regular time locations or at speciﬁc temporal locations
discovered by a saliency detector.

2.2 Alternatives to BoW Quantization for Feature Set Classiﬁcation

Such BoW approaches usually include a quantization step based on a ﬁxed partitioning 
of the whole set of features. This step might prevent from accurately
modeling the distribution of features for every single instance. To overcome this
drawback, several similarity measures deﬁned directly on feature sets have been
designed. Huttenlocher et al. [16] propose the Hausdorﬀ distance that measures
the maximum nearest neighbor distance among features of diﬀerent instances.
The Fisher Kernel [22] relies on a parametric estimation of the feature distribution 
using a Gaussian Mixture Model. Beecks et al. [5] propose the Signature
Quadratic Form distance (SQFD) that is based on the computation of crosssimilarities 
between the features of diﬀerent sets called feature signatures. As we
will see later in this paper, SQFD is closely related to match kernels proposed
in [6]. Beecks et al. [5] show that SQFD is able to reach higher accuracy than
other above classical measures for image retrieval purposes. This makes SQFD a
good candidate for being an alternative to BoW quantization in the time series
classiﬁcation context.

3 Signature Quadratic Form Distance
In the following, we assume that a time series S is represented as a set of n
feature vectors {xi ∈ Rd}. Any algorithm presented in Sect. 2.1 can be used to
extract such a set of feature vectors from the time series and one should note that
considered time series may have diﬀerent number of features extracted. SQFD
is a distance that enables the comparison of instances (time series in our case)
represented by weighted sets of features called feature signatures. In this section,
we review the SQFD distance.

The feature signature of an instance is deﬁned as follows:

F = {(xi, wi)}i=1,...,n, with

n(cid:2)

i=1

wi = 1.

(1)

F can either be composed of the full set of features {xi}, in which case weights
{wi} are all set to 1/n, or by the result of a clustering of the full set of feature
vectors from the instance into n clusters. In the latter case, {xi} are the centroids

532

R. Tavenard et al.

obtained after clustering and {wi} are the weights of the corresponding clusters.
We explain here how the SQFD measure is deﬁned (adopting the time series
point of view).
i )}i=1,...,m be two feature signatures 
associated with two time series S 1 and S 2. Let k : Rd × Rd → R be a
similarity function deﬁned between feature vectors. The SQFD between F 1 and
F 2 is deﬁned as:

i )}i=1,...,n and F 2 = {(x2

Let F 1 = {(x1

i , w1

i , w2

(cid:3)

w1−2 A wT

1−2,

(2)

where w1−2 = (w1
m) is the concatenation of the weights
of F 1 and the opposite of the weight of F 2, and A is a square matrix of dimension
(n + m):

1, . . . , w1

SQFD(F 1,F 2) =
1, . . . ,−w2
n,−w2

⎛
⎜⎜⎝

A1 A1,2

A2,1 A2

⎞
⎟⎟⎠ ,

A =

(3)

,

−γf (cid:3)xi−xj(cid:3)2

where A1 (resp. A2) is the similarity matrix (computed using k) between features
from F 1 (resp. F 2), A1,2 is the cross-similarity matrix between features from F 1
and those from F 2, and A2,1 is the transpose of A1,2. It is shown in [5] that the
RBF kernel

kRBF(xi, xj) = e

(4)
where γf is called the kernel bandwidth, is a good choice for computing local
similarity between two features.
SQFD is hence the square root of a weighted sum of local similarities between
features from sets F 1 and F 2. When no clustering is used, pairwise local similarities 
are all taken into account, resulting in a very ﬁne grain estimation of the
similarity between series that we will refer to as exact SQFD in the following.
However, its calculation has a high cost, as it requires the computation of a
number of local similarities that is quadratic in the size of the sets. A reasonable
alternative presented in [5] consists in ﬁrst quantizing each set using a diﬀerent 
k-means and then computing SQFD between feature signatures representing
the quantized version of the sets. In the following, we will refer to this latter
alternative as the k-means approximation of SQFD (SQFD-k-means for short).

4 Eﬃcient Temporal Kernel Between Feature Sets

In this section, we ﬁrst derive a kernel from the SQFD distance, considering an
RBF kernel as the local similarity, and extend it to a time-sensitive kernel. We
then propose a way to alleviate its computational burden.

4.1 Feature Set Kernel

Let us consider the equal weight case in the SQFD formulation, which is the one
considered when no pre-clustering is performed on the feature sets. By expanding

Eﬃcient Temporal Kernels Between Feature Sets

533

Eq. (2) in this speciﬁc case, we get:
n(cid:2)

n(cid:2)

SQFD(F 1,F 2)2 =

i=1

1
n2
− 2
n · m

j=1
n(cid:2)

m(cid:2)

i=1

j=1

kRBF(x1

i , x1

j) +

1
m2

m(cid:2)

m(cid:2)

i=1

j=1

kRBF(x2

j)
i , x2

kRBF(x1

i , x2

j).

(5)

Note that SQFD then corresponds to a biased estimator of the squared diﬀerence 
between the mean of the samples F 1 and F 2 which is classically used to test
the diﬀerence between two distributions [14]. One can also recognize in Eq. (5)
the match kernel [6] (also known as set kernel [11]). By denoting K the match
kernel associated with kRBF (in what follows, we will always denote with capital
letters kernels that operate on sets whereas kernels operating in the feature space
will be named with lowercase k), we have:

SQFD(F 1,F 2)2 = K(F 1,F 1) + K(F 2,F 2) − 2K(F 1,F 2).

(6)

In other words, SQFD is the distance between feature sets embedded in the
Reproducing Kernel Hilbert Space (RKHS) associated with K. Finally, we build
a feature set kernel, denoted KFS by embedding SQFD into an RBF kernel:

KFS(F 1,F 2) = e

−γK SQFD(F 1

,F 2

)

2

,

(7)

where γK is the bandwith of KFS. This kernel can then be used at the core of
standard kernel methods such as Support Vector Machines (SVM).

4.2 Time-Sensitive Feature Set Kernel

Kernel KFS as deﬁned in Eq. (7) ignores the temporal location of the features in
the time series, only taking into account cross-similarities between the features.
In order to integrate temporal information into kRBF, let us augment the features
with the time index at which they are extracted: for all 1 ≤ i ≤ n, we denote
i = (xi, ti). The time-sensitive kernel between features associated with their
xt
time of occurrence, denoted ktRBF, is deﬁned as:

ktRBF(xt

i, xt

j) = e

−γt (tj−ti)

2 · kRBF(xi, xj).

(8)

In practice, ti and tj are relative timestamps ranging from 0 (beginning of the
considered time series) to 1 (end of the time series) so that features extracted
from time series of diﬀerent lengths can easily be compared. As the product of
two positive semi-deﬁnite kernels, ktRBF is itself a positive semi-deﬁnite kernel.
It can be seen as a temporal adaptation of the convolutional kernel for images
introduced in [20]. Figure 1 illustrates the impact of the parameter γt of ktRBF on
the resulting similarity matrices A for SQFD: kRBF (γt = 0) takes all matches
into account without considering their temporal locations, whereas our timesensitive 
kernel favors diagonal matches, γt controlling the rigidity of this process.

534

R. Tavenard et al.

Denoting xi = (xi1, xi2, . . . , xid), Eq. (8) can be re-written as:

ktRBF(xt

i, xt

j) = e

−γf

(cid:2)(cid:3)d

l=1(xjl−xil)

(cid:4)

,

2

+

γt
γf

(tj−ti)

2

(cid:10)

where γf is the scale parameter. By deﬁning g(xi, ti) =
we get:

xi1, . . . , xid,

−γf (cid:3)g(xi,ti)−g(xj ,tj )(cid:3)2

i, xt

j) = e

ktRBF(xt

(10)
In other words, if we build time-sensitive features g(x, t) by concatenating
rescaled temporal information and the raw features, the time-sensitive kernel in
Eq. (10) can be seen as a standard RBF kernel of scale parameter γf operating on
these augmented features. Finally, replacing kRBF with ktRBF in Eq. (7) deﬁnes
a time-sensitive variant for our feature set kernel.

.

(cid:3)

(9)
(cid:11)

γt
γf ti

,

(a) γt = 0

(b) Medium γt value

(c) Large γt value

Fig. 1. Impact of the ktRBF kernel on similarity matrices A1,2. From left to right,
growing γt values are used from γt = 0 (i.e. the kRBF kernel case) to a large γt value that
ignores almost all non-diagonal matches. Blue colors indicate low similarity whereas
red colors represent high similarity (Matrices A1,2 are shown but similar observations
hold for matrices A1, A2 and A2,1). Best viewed in color.

4.3 Temporal Kernel Normalization

As can be seen in Fig. 1, when γt grows, the number of zero entries in A increases,
hence the norm of A decreases. It is valuable to normalize the resulting match
kernel K so that, if all local kernel responses are equal to 1, the resulting match
kernel evaluation is also equal to 1. The corresponding normalization factor is:

⎛
⎝ n(cid:2)

s2 =

m(cid:2)

exp

−γt( i

n− j

m)2

−1

⎞
⎠

.

(11)

i=1

j=1

When γt = 0, this is equivalent to the

n·m normalization term in the match
kernels. When γt > 0, this can be computed either through a double for-loop or
approximated by its limit when n and m tend towards inﬁnity:

1

Eﬃcient Temporal Kernels Between Feature Sets
(cid:12)

(cid:11)−1

1

1

(cid:10)(cid:12)

ˆs2 =

0

(cid:10)(cid:13)

0

(cid:14)

π
γt

=

2

dt1dt2

−γt(t1−t2)
e
(cid:15)(cid:16)

(cid:17)

2F

2γt

(cid:11)−1

(cid:18)
− 1

+ e

−γt − 1
γt

535

(12)

(13)

where F is the cumulative distribution function of a centered-reduced Gaussian.
In practice, we observe that even for very small feature sets (n = m = 5),
the relative approximation error done when using ˆs normalization instead of s
normalization is less than 2%, which is suﬃcient to eﬃciently rescale kernels.

4.4 Eﬃcient Computation of Feature Set Kernels

As mentioned earlier, exact SQFD computation is demanding as it requires ﬁlling 
the A matrix (Eq. (3)), leading to the evaluation of (n + m)2 local kernels
kRBF(xi, xj). To lower the computational cost of kernel KFS (Eq. (7)), two standard 
approaches can be considered. The ﬁrst one, presented in Sect. 3, relies on a
k-means quantization of each feature set, leading to a time complexity of O(k2)
where k is the chosen number of centroids extracted per set. Another approach
is to build an explicit ﬁnite-dimensional approximation of the RKHS associated
with the match kernel K and approximate SQFD as the Euclidean Distance in
this space, as explained below.

Kernel functions compute the inner product between two feature vectors

embedded on a feature space thanks to a feature map Φ:

k(xi, xj) = (cid:5)Φ(xi), Φ(xj)(cid:6) .

(14)

In the case of an RBF kernel, the associated feature map ΦRBF is inﬁnite dimensional.
 Let us now assume that one can embed feature vectors in a space of
dimension D such that the dot product in this space is a good approximation
of the RBF kernel on features. In other words, let us assume that there exists a
ﬁnite mapping φRBF such that:

kRBF(xi, xj) ≈ (cid:5)φRBF(xi), φRBF(xj)(cid:6) .

Then, the match kernel K becomes:

K(F 1,F 2) =

n(cid:2)

i=1
n(cid:2)

i=1

1
n · m
≈ 1
n · m
(cid:21)
n(cid:2)

m(cid:2)

j=1
m(cid:2)

j=1

≈

kRBF(x1

j)
i , x2

(cid:19)
φRBF(x1

(cid:20)
j)
i ), φRBF(x2

1
n
(cid:22)

i=1

i )
φRBF(x1
,
(cid:23)(cid:24)
(cid:25)
φ(F 1)

(cid:26)

1
m
(cid:22)

m(cid:2)

j=1

φRBF(x2
j)
(cid:23)(cid:24)
(cid:25)
φ(F 2)

(15)

(16)

(17)

(18)

536

R. Tavenard et al.

Hence, approximating kRBF using φRBF is suﬃcient to approximate the
match kernel itself and the explicit feature map φ for K is the barycenter of
explicit feature maps φRBF for features in the set.

Using Eq. (6), we can derive:
SQFD(F 1,F 2)2 ≈ (cid:19)

(cid:20)
φ(F 1), φ(F 1)

(cid:19)
+
≈ (cid:8)φ(F 1) − φ(F 2)(cid:8)2

φ(F 2), φ(F 2)

(cid:20) − 2

(cid:19)

(cid:20)
φ(F 1), φ(F 2)
(19)
(20)

In other words, once feature sets are projected in this ﬁnite-dimensional
space, approximate SQFD computation is performed through a Euclidean distance 
computation in O(D) time, where D is the dimension of the feature map.
In this piece of work, we use the Random Fourier Features [23] to build a ﬁnite
mapping that approximates the local kernel kRBF. Other kernel approximation
techniques [10] could be used, but our experience showed that Random Fourier
Features reached very good performance for our problem, as showed in the next
Section. Random Fourier Features represent each datapoint as its projection on
a Fourier basis. The inverse Fourier transform of the RBF kernel is a Gaussian
distribution p(u) = N (0, σ
−2I) and the Random Fourier Features are obtained
by projecting each original feature into a set of sampling Fourier components
p(u), before passing through a sinusoid:

(cid:13)

(cid:27)
cos(uT

2
D

(cid:28)T

φRBF(xi) =

1 xi + b1), . . . , cos(uT

Dxi + bD)

(21)

where coeﬃcients bi are drawn from a uniform distribution in [−π, π].

Overall, building on a kernelized version of SQFD, we have presented a way
to incorporate time in the representation as well as a scheme for eﬃcient computation 
of the kernel. In the following Section, we will show experimentally
that these improvements help reaching very competitive performance for a wide
range of time-series classiﬁcation problems.

5 Experimental Results with Temporal SIFT Features

Our feature set kernel (with and without temporal information) can be implemented 
in any algorithm in lieu of a BoW approach. To evaluate the performances 
of this kernel, we use it in conjunction with time series-based ScaleInvariant 
Feature Transform (SIFT) features as it has been demonstrated in [2]
that they signiﬁcantly outperform most state-of-the-art local-feature-based time
series classiﬁcation algorithms applied on UCR datasets. In this section, we ﬁrst
recall the temporal SIFT features. We then evaluate the impact of kernel approximation 
in terms of both eﬃciency and accuracy. We also analyze the impact of
integrating temporal information in the feature set kernel. Finally, we compare
our proposed approach with state-of-the-art time series classiﬁers.

Eﬃcient Temporal Kernels Between Feature Sets

537

5.1 Dense Extraction of Temporal SIFT Features

In [2], time series-derived SIFT features are used in a BoW approach for time
series classiﬁcation. We review here how these features are computed (the interested 
reader can refer to [2] for more details). A time series S is described by
keypoints extracted every τstep time instants. Each keypoint is composed of a
set of features that gives a description of the time series at diﬀerent scales. More
formally, let L(S, σ) = S ∗ G(t, σ) be the convolution of a time series with a
Gaussian ﬁlter G(t, σ) = 1√
2 of width σ. A keypoint at time instant
t is described at scale σ as follows. nb blocks of size a are selected around the
keypoint. At each point of these blocks, the gradient magnitudes of L(S, σ) are
computed and weighted so that points that are farther in time from the keypoint 
have less inﬂuence. Then, each block is described by two values: the sum
of the positive gradients and the sum of negative gradients in the block. The
feature vectors of all the keypoints computed at all scales compose the feature
set describing the time series S.

−t
e

2π σ

2

/2σ

5.2 Experimental Setting

For the sake of reproducibility, all presented experiments are conducted on public
datasets from the UCR archive [8] and the Python source code used to produce
the results (which heavily relies on sklearn [21] package) is made available for
download1. All experiments are run on dense temporal SIFT features extracted
using the publicly available software presented in [2]. For SIFT feature extraction,
 we choose to use ﬁxed parameters for all datasets. Features are extracted
at every time instant (τstep = 1), at all scales, with the block size a = 4 and the
number of blocks per feature nb = 12, resulting in 24-dimensional feature vectors.
By using such a parameter set that achieves robust performance across datasets,
we restrict the numbers of parameters to be tuned during cross-validation, without 
severely degrading performance. Finally, all experiments presented here are
repeated 5 times and medians over all runs are reported.

5.3 Impact of Kernel Approximation

We analyze here the impact of the kernel approximation (in terms of trade-oﬀ
between complexity and accuracy) on the proposed kernels. Timings are reported
for execution on a laptop with 2.9 GHz dual core CPU and 8 GB RAM.

Eﬀectiveness. Figure 2 presents the trade-oﬀ between accuracy and execution
time obtained for the ECG200 dataset of the UCR archive. Two methods are
considered: SQFD-k-means is the approximation scheme that was proposed in [5]
and SQFD-Fourier is the one used in this paper. First, this ﬁgure conﬁrms that
the assumption made in Eq. (15) is safe: one can obtain good approximation

1 https://github.com/rtavenar/SQFD-TimeSeries: contains code and supplementary

material.

538

R. Tavenard et al.

Fig. 2. Mean Squared Error (MSE) vs timings of the approximated kernel matrix
(ECG200 ). Timings are reported in seconds per matrix element. As a reference, exact
computation of feature set kernel takes 0.082 s per matrix element.
of an RBF kernel using ﬁnite dimension mapping. Then, for our approach, we
observe that the use of larger dimensions leads to better kernel matrix estimation
at the cost of larger execution time. The same applies for SQFD-k-means when
varying the k parameter. In order to compare approximation methods, Fig. 2 can
be read as follows: for a given MSE level (on the y-axis), the lower the timing, the
better. This comparison leads to the conclusion that our proposed approximation
scheme reaches better trade-oﬀs for a wide range of MSE values. Note that this
behaviour is observed on most of the datasets we have experimented on.

Sensitivity to the Amount of Training Data. In this section, we study
the evolution of both training and testing times as a function of the amount
of training time series. To do so, we compare both eﬃcient approximations of
KFS listed above with a standard RBF kernel operating on BoW representations
of the feature sets. In Fig. 3, all considered methods exhibit linear dependency
between the training set size and the computation time for training. For BoW,
this dependency comes from the k-means computation that has O(nkd) time
complexity, where n is the number of features used for quantization, k is the

Fig. 3. Training and testing times as a function of the amount of training data
(ECG200 ). Training timings correspond to full training of the method for a given
parameter set whereas test timings are reported per test time series.

Eﬃcient Temporal Kernels Between Feature Sets

539

number of clusters and d is the feature dimension. The same argument holds for
SQFD-k-means, and this explains the observed diﬀerence in slope, as lower values 
of k are typically used in this context. SQFD-Fourier present an even lower
slope, which correspond to the computation of projected features (one per time
series). Concerning testing times, BoW as well as SQFD-Fourier have almost
constant computation needs, whereas SQFD-k-means computation time is linearly 
dependent in the number of training time series. Note that this comparison
is done on ECG200 dataset for which the number of training time series is small.
In this context, computation of the feature set representation (k-means quantization 
or feature map) dominates the processing time for both training and
testing. In other settings where the number of training time series is large, processing 
time will be dominated by the computation of pairwise similarities which
is, as stated above, linear in k for BoW, quadratic in k for SQFD-k-means, and
linear in D for SQFD-Fourier. Once again, our proposed approximation scheme
tends to better approximate the exact kernel matrix with lower timings (both
in the training and the testing phase) than its competitor.

5.4 Impact of the Temporal Information

Let us now turn our focus to the impact of temporal information on the classiﬁcation 
performance. To do so, we use KFS in an SVM classiﬁer. In order to
have a fair comparison of the performances, all parameters (except the dimension 
D of the feature map) are set through cross-validation on the training set.
The same applies for experiments presented in the following subsections and the
range of tested parameter values are provided in the Supplementary Material.
As a reference, BoW performance with RBF kernel (using the same temporal
SIFT features) is also reported. To compute this baseline, the number k of codewords 
is also cross-validated. Figure 4 shows the error rates for dataset ECG200
as a function of the dimension D of the feature map, considering (i) our feature
set kernel without temporal information, (ii) its equivalent with temporal information 
and ﬁnally (iii) the normalized temporal kernel. One should ﬁrst notice
that in all cases, a higher dimension D tends to lead to better performance.
This ﬁgure also illustrates the importance of the temporal kernel normalization:

Fig. 4. Error rates as a function of the feature map dimension (ECG200 ).

540

R. Tavenard et al.

the normalized temporal kernel reaches better performance than the feature set
kernel with no time information, whereas the performance of the non-normalized
one is worse. Indeed, when γt increases, the non-normalized version suﬀers from
a bad scaling of kernel responses that impairs the learning process of the SVM.2
In the following, we use the same parameter ranges as above, and we crossvalidate 
the parameter related to the time (γt ∈ {0}∪(cid:29)
). By doing so,
we oﬀer the possibility for our method to learn (during training) whether time
information is of interest or not for a given dataset. We present experiments run
on the 85 datasets from the UCR Time Series Classiﬁcation archive and observe
that in more than 3/4 of our experiments, the temporal variant of our kernel
is selected by cross-validation (i.e. γt > 0), which conﬁrms the superiority of
temporal kernels for such applications. Corresponding datasets are marked with
a star in the full result table provided as Supplementary material.

100 − 106

(cid:30)

5.5 Pairwise Comparisons on UCR Datasets

Pairwise comparisons of methods are presented in Fig. 5, in which green dots
(data points lying below the diagonal) correspond to cases where the x-axis
method has higher classiﬁcation error rates, black ones represent cases where
both methods share the same performance and red dots stand for cases for which
the y-axis method has higher error rates. In these plots, Win/Tie/Lose scores are

Fig. 5. Pairwise performance comparisons. Reported values are error rates. (Color
ﬁgure online)

2 See Supplementary material for experiments on more datasets.

Eﬃcient Temporal Kernels Between Feature Sets

541

also reported where “Win” indicates the number of times the y-axis method outperforms 
the x-axis one. Finally, p-values corresponding to one-sided Wilcoxon
signed rank tests are provided to assess statistical signiﬁcance of observed differences 
and our signiﬁcance level is set to 5%.

First, Fig. 5a conﬁrms our observation made for ECG200 dataset: incorporating 
temporal information allows, for many datasets, to improve the classiﬁcation 
performances. We then compare the eﬃcient version of our temporal kernel
on feature sets with standard BoW approach running on the same feature sets
(Fig. 5b). This ﬁgure shows improvement for a wide range of datasets: when testing 
the statistical diﬀerences between both methods, one can observe that our
feature set kernel signiﬁcantly outperforms the BoW approach. Finally, when
considering other state-of-the-art competitors3, our feature set kernel for time
series show state-of-the-art performance, signiﬁcantly outperforming BOSS [25],
DTDC [12], LearningShapelets (LS) [13] and TSBF [4]. This high accuracy is
achieved with reasonable classiﬁcation time (e.g. 300 ms per test time series on
NonInvasiveFetalECG1 dataset, one of the largest UCR datasets).

6 Conclusion

Many local features have been designed for time-series representations and used
in a BoW framework for classiﬁcation purposes. To improve these approaches,
we introduce in this paper a new temporal kernel between feature sets that gets
rid of quantized representations. More precisely, we propose to kernelize SQFD
for time-series classiﬁcation purposes. We also derive a temporal feature set kernel,
 allowing one to take into account the time instant at which the features are
extracted. In order to alleviate the high computational burden of this kernel and
make it tractable for large datasets, we propose an approximation technique
that allows fast computation of the kernel. Extensive experiments show that
the temporal information helps improving classiﬁcation accuracy. The temporal
information is taken into account thanks to a simple RBF kernel, and we believe
that the performance could be further improved by exploring other ways to incorporate 
time information into the local kernels. This is the main direction for our
future work. Finally, our temporal feature set kernel signiﬁcantly outperforms the
initial BoW-based method and leads to competitive results w.r.t state-of-the-art
time series classiﬁcation algorithms. This kernel is likely to improve performance
of any time series classiﬁcation approach based on BoW.

Acknowledgments. Supported by the Millennium Nucleus Center for Semantic Web
Research under Grant NC120004, the ANR through the ASTERIX project (ANR-13-
JS02-0005-01), ´Ecole des docteurs de l’UBL as well as by the Brittany Region.

3 For the sake of brevity, we focus on standalone classiﬁers that are shown in [1] to

outperform competitors in their categories.

542

R. Tavenard et al.

References

1. Bagnall, A., Lines, J., Bostrom, A., Large, J., Keogh, E.: The great time
series classiﬁcation bake oﬀ: a review and experimental evaluation of recent
algorithmic advances. Data Min. Knowl. Discov. 31(3), 606–660 (2017).
https://link.springer.com/article/10.1007/s10618-016-0483-9

2. Bailly, A., Malinowski, S., Tavenard, R., Chapel, L., Guyet, T.: Dense bag-of-
temporal-SIFT-words for time series classiﬁcation. In: Douzal-Chouakria, A., Vilar,
J.A., Marteau, P.-F. (eds.) AALTD 2015. LNCS (LNAI), vol. 9785, pp. 17–30.
Springer, Cham (2016). https://doi.org/10.1007/978-3-319-44412-3 2

3. Baydogan, M.G., Runger, G.: Learning a symbolic representation for multivariate

time series classiﬁcation. Data Min. Knowl. Discov. 29(2), 400–422 (2015)

4. Baydogan, M.G., Runger, G., Tuv, E.: A bag-of-features framework to classify time

series. IEEE Trans. Pattern Anal. Mach. Intell. 35(11), 2796–2802 (2013)

5. Beecks, C., Uysal, M.S., Seidl, T.: Signature quadratic form distance. In: Proceedings 
of ACM International Conference on Image and Video Retrieval, pp. 438–445
(2010)

6. Bo, L., Sminchisescu, C.: Eﬃcient match kernel between sets of features for visual

recognition. Adv. Neural Inf. Process. Syst. 22, 135–143 (2009)

7. Candan, K.S., Rossini, R., Sapino, M.L.: sDTW: computing DTW distances using
locally relevant constraints based on salient feature alignments. In: Proceedings of
International Conference on Very Large DataBases, vol. 5, pp. 1519–1530 (2012)
8. Chen, Y., Keogh, E., Hu, B., Begum, N., Bagnall, A., Mueen, A., Batista, G.:
The UCR time series classiﬁcation archive (2015). www.cs.ucr.edu/∼eamonn/time
series data/

9. Cuturi, M.: Fast global alignment kernels. In: Proceedings of International Conference 
on Machine Learning, pp. 929–936 (2011)

10. Drineas, P., Mahoney, M.W.: On the nystr¨om method for approximating a gram
matrix for improved kernel-based learning. J. Mach. Learn. Res. 6, 2153–2175
(2005)

11. G¨artner, T., Flach, P.A., Kowalczyk, A., Smola, A.J.: Multi-instance kernels. In:

Proceedings of International Conference on Machine Learning (2002)

12. G´orecki, T., (cid:4)Luczak, M.: Non-isometric transforms in time series classiﬁcation using

DTW. Knowl.-Based Syst. 61, 98–108 (2014)

13. Grabocka, J., Schilling, N., Wistuba, M., Schmidt-Thieme, L.: Learning time-series
shapelets. In: Proceedings of ACM SIGKDD International Conference on Knowledge 
Discovery and Data Mining, pp. 392–401 (2014)

14. Gretton, A., Borgwardt, K.M., Rasch, M., Sch¨olkopf, B., Smola, A.J.: A kernel
method for the two-sample-problem. In: Advances in Neural Information Processing 
Systems, pp. 513–520 (2006)

15. Hills, J., Lines, J., Baranauskas, E., Mapp, J., Bagnall, A.: Classiﬁcation of time
series by shapelet transformation. Data Min. Knowl. Discov. 28(4), 851–881 (2014)
16. Huttenlocher, D.P., Klanderman, G.A., Rucklidge, W.J.: Comparing images using
the Hausdorﬀ distance. IEEE Trans. Pattern Anal. Mach. Intell. 15(9), 850–863
(1993)

17. Le Cun, Y., Bengio, Y.: Convolutional networks for images, speech, and time series.
In: The Handbook of Brain Theory and Neural Networks, vol. 3361, pp. 255–258
(1995)

Eﬃcient Temporal Kernels Between Feature Sets

543

18. Lin, J., Keogh, E., Lonardi, S., Chiu, B.: A symbolic representation of time series,
with implications for streaming algorithms. In: Proceedings of ACM SIGMOD
Workshop on Research Issues in Data Mining and Knowledge Discovery, pp. 2–11
(2003)

19. Lin, J., Khade, R., Li, Y.: Rotation-invariant similarity in time series using bag-

of-patterns representation. Int. J. Inf. Syst. 39, 287–315 (2012)

20. Mairal, J., Koniusz, P., Harchaoui, Z., Schmid, C.: Convolutional kernel networks.

In: Advances in Neural Information Processing Systems, pp. 2627–2635 (2014)

21. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O.,
Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A.,
Cournapeau, D., Brucher, M., Perrot, M., Duchesnay, E.: Scikit-learn: machine
learning in Python. J. Mach. Learn. Res. 12, 2825–2830 (2011)

22. Perronnin, F., Dance, C.: Fisher kernels on visual vocabularies for image categorization.
 In: Proceedings of IEEE Conference on Computer Vision and Pattern
Recognition, pp. 1–8 (2007)

23. Rahimi, A., Recht, B.: Random features for large-scale kernel machines. In:

Advances in Neural Information Processing Systems, pp. 1177–1184 (2007)

24. Sakoe, H., Chiba, S.: Dynamic programming algorithm optimization for spoken
word recognition. IEEE Trans. Acoust. Speech Sig. Process. 26(1), 43–49 (1978)
25. Sch¨afer, P.: The BOSS is concerned with time series classiﬁcation in the presence

of noise. Data Min. Knowl. Discov. 29(6), 1505–1530 (2014)

26. Senin, P., Malinchik, S.: SAX-VSM: interpretable time series classiﬁcation using
SAX and vector space model. In: Proceedings of IEEE International Conference
on Data Mining, pp. 1175–1180 (2013)

27. Wang, J., Liu, P., She, M.F.H., Nahavandi, S., Kouzani, A.: Bag-of-words representation 
for biomedical time series classiﬁcation. Biomed. Sig. Process. Control
8(6), 634–644 (2013)

28. Xie, J., Beigi, M.: A scale-invariant local descriptor for event recognition in 1D
sensor signals. In: Proceedings of IEEE International Conference on Multimedia
and Expo, pp. 1226–1229 (2009)

29. Ye, L., Keogh, E.: Time series shapelets: a new primitive for data mining. In:
Proceedings of ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining, pp. 947–956 (2009)

