IMGpedia: A Linked Dataset with

Content-Based Analysis of Wikimedia Images

Sebasti´an Ferrada(B), Benjamin Bustos, and Aidan Hogan

Department of Computer Science, Center for Semantic Web Research,

Universidad de Chile, Santiago, Chile

{sferrada,bebustos,ahogan}@dcc.uchile.cl

Abstract. IMGpedia is a large-scale linked dataset that incorporates
visual information of the images from the Wikimedia Commons dataset:
it brings together descriptors of the visual content of 15 million images,
450 million visual-similarity relations between those images, links to
image metadata from DBpedia Commons, and links to the DBpedia
resources associated with individual images. In this paper we describe the
creation of the IMGpedia dataset, provide an overview of its schema and
statistics of its contents, oﬀer example queries that combine semantic and
visual information of images, and discuss other envisaged use-cases for
the dataset.

Resource type: Dataset
Permanent URL: https://dx.doi.org/10.6084/m9.ﬁgshare.4991099.v2

1 Introduction

Many datasets have been published on the Web following Semantic Web standards 
and Linked Data principles. At the core of the resulting “Web of Data”,
we can ﬁnd linked datasets such as DBpedia [6], which contains structured data
automatically extracted from Wikipedia; and Wikidata [10], where users can
directly add and curate data in a structured format. We can also ﬁnd various
datasets relating to multimedia, such as LinkedMDB describing movies, BBC
Music describing music bands and genres, and so forth. More recently, DBpedia 
Commons [9] was released, publishing metadata extracted from Wikimedia
Commons1: a rich source of multimedia containing 38 million freely usable media
ﬁles (image, audio and video).

Related Work. Amongst the available datasets describing multimedia, the
emphasis has been on capturing the high-level metadata of the multimedia ﬁles
(e.g., author, date created, ﬁle size, width, duration) rather than audio or visual
features of the multimedia content itself. However, as mentioned in previous

1 http://commons.wikimedia.org.
c(cid:2) Springer International Publishing AG 2017
C. d’Amato et al. (Eds.): ISWC 2017, Part II, LNCS 10588, pp. 84–93, 2017.
DOI: 10.1007/978-3-319-68204-4 8

IMGpedia

85

works (e.g., [1,4,8]), merging structured metadata with multimedia contentbased 
descriptors could lead to a variety of applications, such as semanticallyenhanced 
multimedia publishing, retrieval, preservation, etc. While such works
have proposed methods to describe the audio or visual content of multimedia ﬁles
in Semantic Web formats, we are not aware of any public linked dataset incorporating 
content-based descriptors of multimedia ﬁles. For example, DBpedia
Commons [9] does not extract any audio/visual features directly from the multimedia 
ﬁles of Wikimedia Commons, but rather only captures metadata from
the documents describing the ﬁles.

Contribution. Along these lines, we have created IMGpedia: a linked dataset
incorporating visual descriptors and visual similarity relations for the images
of Wikimedia Commons, linked with both the DBpedia Commons dataset
(which provides metadata for the images, such as author, license, etc.) and the
DBpedia dataset (which provides metadata about resources associated with the
image). The initial use-case we are exploring for IMGpedia is to perform visuosemantic 
queries over the images, where, for example, using SPARQL federation
over IMGpedia and DBpedia, we could request: given a picture of the Cusco
Cathedral, retrieve the top-k most similar cathedrals in Europe. More generally,
as discussed later, we foresee a number of potential use-cases for the dataset as
a test-bed for research in the potentially fruitful intersection of the Multimedia
and Semantic Web areas.

Outline. In this paper, we describe the IMGpedia dataset2. We ﬁrst introduce
the image analysis used to extract visual descriptors and similarity relations
from the images of Wikimedia Commons. Next we give an overview of the
lightweight ontology used to represent the resulting visual information as RDF.
We then provide some high-level statistics of the resulting dataset and the bestpractices 
used in its publication. Thereafter, we provide some example visuosemantic 
queries and their results. Finally we conclude with discussion of other
use-cases we envisage as well as our future plans to improve upon and extend
the IMGpedia dataset.

2 Image Analysis

Wikimedia Commons is a dataset of 38 million freely-usable media ﬁles contributed 
and maintained collaboratively by users. Around 16 million of these
media ﬁles are images, which are hosted on a mirror server accessible via
rsync3. We downloaded the images, with a total size of 21 TB, in order to be
able to process them oﬄine. The download took 40 days with a bandwidth of

2 In a previous short paper, we proposed the idea of the project and gave details of
initial progress [3]; this paper describes the dataset resulting from that initial work.

3 rsync://ftpmirror.your.org/wikimedia-images/.

86

S.Ferrada et al.

500 GB/day. In order to facilitate later image processing tasks, we only consider
images with (commonly supported) JPG or PNG encodings, equivalent to 92% of
the images.

After the acquisition of the images, we proceeded to compute diﬀerent visual
descriptors, which are high-dimensional vectors that capture diﬀerent elements
of the content of the images (such as color distribution or shape/texture infor-
mation); later we will use these descriptors to compute visual similarity between
images, where we say that two images are visually similar if the distance between
their descriptors is low. The descriptors computed are the following:

– Gray Histogram Descriptor: We transform the image from color to
grayscale and divide it into a ﬁxed number of blocks. A histogram of 8-bit
gray intensities is then calculated for each block. The concatenation of all
histograms is used to generate a description vector with 256 dimensions.

– Histogram of Oriented Gradients Descriptor: We extract edges of the
grayscale image by computing its gradient (using Sobel kernels), applying
a threshold, and computing the orientation of the gradient. Finally, a histogram 
of the orientations is made and used as a description vector with 288
dimensions.

– Color Layout Descriptor: We divide the image into blocks and for each
block we compute the mean (YCbCr) color. Afterwards the Discrete Cosine
Transform is computed for each color channel. Finally the concatenation of
the transforms is used as the descriptor vector, with 192 dimensions.

Computing the descriptors was performed on a machine with Debian 4.1.1,
a 2.2 GHz 24-core Intel® Xeon® processor, and 120 GB of RAM. With multithreading,
 computing GHD took 43 h, HOG took 107 h, while CLD took 127 h.
We have made implementations to compute these visual descriptors available in
multiple programming languages under a GNU GPL license [3]4.

The next task is to use these descriptors to compute the visual similarity
between pairs of images. Given the scale of the dataset, in order to keep a manageable 
upper-bound on the resulting data (we selected ∼4 billion triples as a
reasonable limit), we decided to compute the 10 nearest neighbors for each image
brute-force comparisons, we
according to each visual descriptor. To avoid
use approximate search methods where we selected the Fast Library for Approximated 
Nearest Neighbors (FLANN) since it has been proven to scale for large
datasets [7]5. In order to facilitate multi-threading, we divide the images into 16
buckets, where for each image, we initialize 16 threads to search for the 10 nearest 
neighbors in each bucket. At the end of the execution we have 160 candidates

(cid:3)
(cid:2)
n
2

4 https://github.com/scferrada/imgpedia.
5 We conﬁgured FLANN with a goal precision of 90% and tested it on a brute-forced
gold standard of 20,000 images. FLANN achieved an actual precision of 79% on this
dataset. However, while the gold standard took 3.5 days to compute with 16 threads,
FLANN ﬁnished in 13 min with 1 thread. We concluded that FLANN oﬀers a good
precision/eﬃciency trade-oﬀ for a large-scale collection of images such as ours.

IMGpedia

87

to be the global 10 nearest neighbors so we choose the 10 with the minimum distances 
among them to obtain the ﬁnal result. This process took about 13 h with
the machine previously described. In Fig. 1 we show an example of the results of
the similarity search based on the HOG descriptor, which captures information
about edges in the image.

Source Image

1-NN

2-NN

3-NN

4-NN

5-NN

6-NN

7-NN

8-NN

9-NN

10-NN

Fig. 1. 10 nearest neighbors of an image of Hopsten Marktplatz using HOG

3 Ontology and Data

The visual descriptors and similarity relations of the images form the core of
the IMGpedia dataset. To represent this information as RDF, we create a
custom lightweight IMGpedia ontology. All IMGpedia resources are identiﬁed 
under the http://imgpedia.dcc.uchile.cl/resource/ namespace. The vocabulary 
is described in RDFS/OWL at http://imgpedia.dcc.uchile.cl/ontology; this
vocabulary (authoritatively) extends related terms from the DBpedia Ontology,
schema.org and the Open Graph Protocol where appropriate, and has been submitted 
to the Linked Open Vocabularies (LOV) service. In Fig. 2, we show the
classes, datatypeand 
object-properties available for representing images, their
visual descriptors and the similarity links between them.

An imo:Image is an abstract resource representing an image of the Wikimedia 
Commons dataset, describing the dimensions of the image (height and
width), the image URL in Wikimedia Commons, and an owl:sameAs link to
the complementary resource in DBpedia Commons. In Listing 1 we see an
example of the RDF for the imo:Image representation of Hopsten Marktplatz.

88

S.Ferrada et al.

imo:appearsIn

foaf:depiction

imo:fileURL

owl:sameAs

owl:Thing

wc:File

dbo:Image

imo:similar

imo:Image

imo:height (xsd:float )
imo:width (xsd:float )

imo:sourceImg
imo:targetImg

imo:ImageRelation

imo:distance (xsd:float )

imo:describes

imo:Descriptor

imo:value (xsd:string )

rdfs:subClassOf

imo:CLD

imo:GHD

imo:HOG

imo:usesDescriptorType

Fig. 2. IMGpedia ontology overview: classes are shown in boxes; solid edges denote
relations between instances of both classes, dotted lines are between the classes themselves,
 while dashed lines are from instances to classes; external terms are italicized;
datatype properties are listed inside the class boxes for conciseness.

Listing 1. RDF example of a visual entity

@prefix imo : < http :// i m g p e d i a . dcc . u c h i l e . cl / o n t o l o g y # >
@prefix im : < http :// i m g p e d i a . dcc . u c h i l e . cl / r e s o u r c e / >
@prefix dbcr : < http :// commons . d b p e d i a . org / r e s o u r c e / File : >
im : H o p s t e n _ M a r k t p l a t z _ 3 . jpg a imo : Image ;

owl : sameAs dbcr : H o p s t e n _ M a r k t p l a t z _ 3 . jpg ;
imo : width 400 ; imo : height 300 ;
imo : f i l e U R L < http :// commons . w i k i m e d i a . org / wiki / File : H o p s t e n _ M a r k t p l a t z _ 3 . jpg >.

An imo:Descriptor respresents a visual descriptor of an image and is linked
to it through the imo:describes relation. An imo:Descriptor can be of type
imo:GHD, imo:HOG, or imo:CLD corresponding to the three types of descriptors
previously discussed. In Listing 2 we show an example of a visual descriptor in
RDF. To keep the number of output triples manageable, we store the vector of
the descriptor as a string; storing individual dimensions as (192–288) individual
objects would inﬂate the output triples to an unmanageable volume; in addition,
we do not currently anticipate SPARQL queries over individual values of the
descriptor.

Listing 2. RDF example of a descriptor

im : H o p s t e n _ M a r k t p l a t z _ 3 . jpg . HOG a imo : HOG ;

imo : d e s c r i b e s im : H o p s t e n _ M a r k t p l a t z _ 3 . jpg ;
imo : value "[0.34418711 , 0.10582313 , 0.05867421 , ...]".

An imo:ImageRelation is a resource that contains the similarity links
between two images; it also contains the type of descriptor that was used and the
Manhattan distance between the descriptors of both images. Although Manhattan 
distance is symmetric, these relations are materialized based on a k-nearestneighbors 
(k-nn) search, where image a being in the k-nn of b does not imply
the inverse relation; hence the image relation captures a source and target image
where the target is in the k-nn of the source. We also add a imo:similar relation
from the source image to the target k-nn image. Listing 3 shows an example of
a k-nn relation in RDF.

IMGpedia

89

Listing 3. RDF example of a visual similarity relation

im : 1 7 6 1 4 7 a c 9 5 6 6 0 a 4 7 d 5 d 5 8 c 5 7 d 5 2 6 0 5 7 2 c d c e 1 1 f 9 8 a d 4 . HOG a imo : I m a g e R e l a t i o n ;

imo : s o u r c e I m a g e im : H o p s t e n _ M a r k t p l a t z _ 3 . jpg ;
imo : t a r g e t I m a g e im : Boze_Cialo - glowny . JPG ;
imo : d i s t a n c e 1 . 2 1 9 6 6 0 e +01 ; imo : u s e s D e s c r i p t o r imo : HOG .

im : H o p s t e n _ M a r k t p l a t z _ 3 . jpg imo : s i m i l a r im : Boze_Cialo - glowny . JPG .

Finally, aside from the links to DBpedia Commons, we also provide links
to DBpedia, which provides a context for the images. To create these links,
we use an SQL dump of English Wikipedia and perform a join between
the table of all images and the table of all articles, so we can have pairs
(image name,article name) if the image appears in the article. In Listing 4 we
give some example links for DBpedia. Such links are not provided by DBpedia
Commons.

Listing 4. RDF example of DBpedia links

im : C h a m o m i l e _ o r i g i n a l _ s i z e . jpg imo : a p p e a r s I n dbr : N e p h e l i u m _ h y p o l e u c u m .
im : R o s e _ A m b e r _ F l u s h _ 2 0 0 7 0 6 0 1 . jpg imo : a p p e a r s I n dbr : N e p h e l i u m _ h y p o l e u c u m .
im : R o s e _ A m b e r _ F l u s h _ 2 0 0 7 0 6 0 1 . jpg imo : a p p e a r s I n dbr : A c e r _ s h i r a s a w a n u m .
im : HondaS2000 -004. png imo : a p p e a r s I n dbr : A l f a _ R o m e o _ S c i g h e r a .

4 Dataset

The dataset of IMGpedia contains information about 14.7 million images of
Wikimedia Commons, the description of their content, links to their most
similar images and to the DBpedia resources that form part of their context. A
general overview of the size and data of IMGpedia can be seen in Table 2. There
we can see that for each visual entity we computed three diﬀerent descriptors
and for each descriptor we computed 10 similarity links using the 10 nearest
neighbors, deﬁning a similarity graph with 14.7 million vertices and 442 million
edges.

Accessibility and Best Practices. IMGpedia is available as a Linked Dataset
(with dereferenceable IRIs), as a SPARQL endpoint (using Virtuoso), and as a
dump. Locations are provided in Table 1. As aforementioned, we provide a lightweight 
RDFS/OWL Ontology that extends well-known vocabularies as appropriate.
 We also provide a VoID description of the dataset, which includes metadata
from DC-terms as well as brief provenance statement using the PROV ontology
and licensing information. With respect to the license, the most restrictive licensing 
clauses allowed for images on Wikipedia Commons are attribution and
share-alike6; non-derivative or non-commercial clauses are not permitted. Hence
we release IMGpedia under an Open Database License (ODC-ODbL) license7,
which is an attribution/share-alike license speciﬁcally intended for databases.
According to the 5-star model for Linked Open Data [2], IMGpedia is a 5-star

6 https://commons.wikimedia.org/wiki/Commons:Licensing.
7 http://www.opendatacommons.org/licenses/odbl/.

90

S.Ferrada et al.

Table 1. Locations of IMGpedia resources

Resource

Location

LD IRI (example) http://imgpedia.dcc.uchile.cl/resource/

Rose Amber Flush 20070601.jpg

SPARQL endpoint http://imgpedia.dcc.uchile.cl/sparql

Dump

VoID

http://imgpedia.dcc.uchile.cl/dumps/20170506/

http://imgpedia.dcc.uchile.cl/dumps/20170506/void.nt

Ontology

http://imgpedia.dcc.uchile.cl/ontology#

Issue tracker

http://github.com/scferrada/imgpedia/issues

Datahub

http://datahub.io/dataset/imgpedia

Table 2. High-level statistics for IMGpedia

Name

Count Description

Visual entities
Links to DBpedia Commons

14,765,300 Entities about the images

14,765,300 Links to additional image

metadata

Descriptors

44,295,900 Visual descriptors of the

images

Similarity links

442,959,000 Nearest neighbor relations

between images

IRIs
Links to DBpedia

502,020,200 Unique resource names

12,683,423 Links to the resource

about the article of the
image

Triples

3,119,207,705 Number of triples present

in the graph

dataset since it is an RDF graph that uses IRIs to identify its resources and
provides links to other data sources (DBpedia and DBpedia Commons) to
provide context. IMGpedia also has an issue tracker on GitHub, so users and
collaborators can request features for future versions and report any problems
they may ﬁnd. The dataset is also registered at DataHub so researchers and
other public can easily ﬁnd and use it.

With respect to sustainability, given the large sizes of the dumps, we have yet
to ﬁnd a mirror host to replicate the data. However, internally, data are replicated
on NAS storage and the source code is provided to replicate the dataset from the
source Wikimedia Commons images. The ﬁrst author has also secured funding
to pursue a PhD on the topic, which will start this year; hence the dataset will be
in active maintenance and development. With respect to updating the dataset,
while building the original dataset was costly, we are planning to implement an
incremental update where rsync is used to fetch new images; the descriptors

IMGpedia

91

for these images can then be computed, while only the k-nn similarity relations
involving new images (potentially pruning old relations) need to be computed.

5 Use-Cases

We ﬁrst provide some examples of queries that IMGpedia can answer.

First, we can query the visual similarity relations to ﬁnd images that are
similar by color, edges and/or intensity according to the nearest neighbor computation.
 In Listing 5 we show such a query, requesting the nearest neighbors
of the image of Hopsten Marktplatz using the HOG descriptor (capturing visual
similarity of edges). The results of this query are the images shown previously
in Fig. 1.

Listing 5. SPARQL Query for similar images to Hopsten Marktplatz

SELECT DISTINCT ? Target ? Distance WHERE {

? rel imo : s o u r c e I m a g e im : H o p s t e n _ M a r k t p l a t z _ 3 . jpg ;
imo : u s e s D e s c r i p t o r T y p e imo : HOG ;
imo : t a r g e t I m a g e ? Target ;
imo : d i s t a n c e ? D i s t a n c e . }

ORDER BY ? Distance

Second, we can use federated SPARQL queries to perform visuo-semantic
retrieval of images, combining visual similarity of images with semantic metadata 
through links to DBpedia. In Listing 6, we show an example federated 
SPARQL query using the DBpedia SPARQL endpoint that takes the
images from articles categorized as “Roman Catholic cathedrals in Europe” and
looks for similar images from articles categorized as “Museum”. In Fig. 3, we
show the retrieved images. To obtain more accurate results, SPARQL property 
paths can be used in order to include hierarchical categorizations, e.g.
dcterms:subject/skos:broader* can be used in the ﬁrst SERVICE clause to
obtain all cathedrals that are labeled as a subcategory of European cathedral,
such as French cathedral.

Listing 6. Query for images of museums similar to European Catholic cathedrals

S E L E C T D I S T I N C T ? urls ? urlt WHERE {

SERVICE < http :// d b p e d i a . org / sparql >{

? sres dcterms : s u b j e c t dbc : R o m a n _ C a t h o l i c _ c a t h e d r a l s _ i n _ E u r o p e . }

? s o u r c e imo : a p p e a r s I n ? sres ;
imo : similar ? target ;
imo : fileURL ? urls .

? t a r g e t imo : a p p e a r s I n ? tres ;

imo : fileURL ? urlt .

SERVICE < http :// d b p e d i a . org / sparql >{

? tres dcterms : s u b j e c t ? sub
FILTER ( CONTAINS ( STR (? sub ) , " Museum "))}}

With regards to usage, we released IMGpedia to the public on May 6th, 2017
and we keep a log of the SPARQL queries asked through the query endpoint,
which at the time of writing (11 weeks later) contains 588 queries. However, we
emphasize that IMGpedia was recently published. Our current plan is to further
explore the potential of semantically-enhanced image retrieval that IMGpedia

92

S.Ferrada et al.

Cathedral of St. Mary and Museum of Fine Arts

Basilica of St. John L. and Nat. Hist. Museum of Helsinki Cathedral of St. Mary and Dumbarton House Museum

Fig. 3. Results of Listing 6 query

oﬀers. The dataset also opens up a number of other use-cases. For example,
one could consider combining the semantic information from DBpedia and the
visual similarity information of IMGpedia to create a labeled dataset along the
lines of ImageNet8, but with variable levels of granularity (e.g., Catholic cathedral,
 cathedral, religious building, etc.). Another use-case would be to develop
a clustering technique for images based both on visual similarity and semantic
context. We also believe that IMGpedia can compliment existing research works
in the intersection of the Semantic Web and Multimedia, where it could provide
a test-bed for works on media fragments [4,8], or on combining SPARQL with
multimedia retrieval [5], etc.

6 Conclusions and Future Work

In this paper we have presented IMGpedia: a linked dataset that oﬀers visual
descriptors and similarity relations for the images of Wikimedia Commons;
this dataset is also linked with DBpedia and DBpedia Commons to provide
semantic context and further metadata. We described the construction of the
dataset, the structure and provenance of the data, statistics of the dataset, and
the supporting resources made available. Finally, we showed some examples of
visuo-semantic queries enabled by the dataset and discussed potential use-cases.
There are many things that can be improved and added to IMGpedia. We
will develop a web application to make IMGpedia more user-friendly, where
users can ask queries intuitively (without needing SPARQL) and browse through
results where images are displayed. We also plan to explore more modern visual
descriptors that can help us to improve the current similarity relations between
images, as well as deﬁning similarity relations that combine descriptors.

Acknowledgments. This work was supported by the Millennium Nucleus Center for
Semantic Web Research, Grant № NC120004 and Fondecyt, Grant № 11140900. We
would also like to thank Camila Fa´undez for her assistance.

References

1. Addis, M., Allasia, W., Bailer, W., Boch, L., Gallo, F., Wright, R.: 100 million
hours of audiovisual content: digital preservation and access in the PrestoPRIME
project. In: INTLDPIF.
 ACM (2010)

8 http://www.image-net.org/.

IMGpedia

93

2. Berners-Lee, T.: Linked Data. In: W3C Design Issues, July 2006 (2010)
3. Ferrada, S., Bustos, B., Hogan, A.: IMGpedia: enriching the web of data with

image content analysis. In: AMW. CEUR (2016)

4. Kurz, T., Kosch, H.: Lifting media fragment uris to the next level. In: Linked Media

Workshop (LIME-SemDev) at ESWC (2016)

5. Kurz, T., Schlegel, K., Kosch, H.: Enabling access to linked media with SPARQLMM.
 In: World Wide Web (WWW), pp. 721–726 (2015)

6. Lehmann, J., Isele, R., Jakob, M., Jentzsch, A., Kontokostas, D., Mendes, P.N.,
Hellmann, S., Morsey, M., van Kleef, P., Auer, S., Bizer, C.: DBpedia - a largescale,
 multilingual knowledge base extracted from wikipedia. Semant. Web J. 6,
167–195 (2014)

7. Muja, M., Lowe, D.G.: Fast approximate nearest neighbors with automatic algorithm 
conﬁguration. In: VISSApp, pp. 331–340. INSTICC Press (2009)

8. Troncy, R., Mannens, E., Pfeiﬀer, S., Deursen, D.V.: Media fragments URI 1.0. In:

W3C Recommendation (2012)

9. Vaidya, G., Kontokostas, D., Knuth, M., Lehmann, J., Hellmann, S.: DBpedia
commons: structured multimedia metadata from the wikimedia commons. In: Arenas,
 M., et al. (eds.) ISWC 2015. LNCS, vol. 9367, pp. 281–289. Springer, Cham
(2015). doi:10.1007/978-3-319-25010-6 17

10. Vrandeˇci´c, D., Kr¨otzsch, M.: Wikidata: a free collaborative knowledgebase. Comm.

ACM 57, 78–85 (2014)

