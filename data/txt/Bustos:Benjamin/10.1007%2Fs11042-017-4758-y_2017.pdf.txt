Multimed Tools Appl (2017) 76:22019–22042
DOI 10.1007/s11042-017-4758-y

Combining pixel domain and compressed domain index
for sketch based image retrieval
Carlos Alberto Fraga Pimentel Filho1,3 · Benjamin Bustos2 ·
Arnaldo de Albuquerque Ara´ujo3 · Silvio Jamil Ferzoli Guimar˜aes1

Received: 30 November 2016 / Revised: 21 March 2017 / Accepted: 25 April 2017 /
Published online: 25 May 2017
© Springer Science+Business Media New York 2017

Abstract Sketch-based image retrieval (SBIR) lets one express a precise visual query with
simple and widespread means. In the SBIR approaches, the challenge consists in representing 
the image dataset features in a structure that allows one to efficiently and effectively
retrieve images in a scalable system. We put forward a sketch-based image retrieval solution 
where sketches and natural image contours are represented and compared, in both, the
compressed-domain of wavelet and in the pixel domain. The query is efficiently performed
in the wavelet domain, while effectiveness refinements are achieved using the pixel domain
to verify the spatial consistency between the sketch strokes and the natural image contours.
Also, we present an efficient scheme of inverted lists for sketch-based image retrieval using
the compressed-domain of wavelets. Our proposal of indexing presents two main advantages,
 the amount of the data to compute the query is smaller than the traditional method
while it presents a better effectiveness.

Keywords Sketch-based image retrieval · Multimedia indexing · Scalability

(cid:2) Silvio Jamil Ferzoli Guimar˜aes

sjamil@pucminas.br

Arnaldo de Albuquerque Ara´ujo
arnaldo@dcc.ufmg.br

1 Department of Computer Science/VIPLAB, Pontifical Catholic University of Minas Gerais (PUC

Minas), Belo Horizonte, Brazil

2 Department of Computer Science, University of Chile, Santiago, Chile

3 Department of Computer Science/NPDI, Federal University of Minas Gerais (UFMG), Belo

Horizonte, Brazil

22020

1 Introduction

Multimed Tools Appl (2017) 76:22019–22042

Content-based image retrieval (CBIR) aims to deal not only with the absence or insufficiency of
annotations for most of the images, but also to support alternative retrieval approaches, relying 
on visual perception, which is more appropriate in many scenarios. With the popularity
of the touch screen devices, drawing a digital sketch is easier and faster than on traditional
computer devices. Thus, query an image by sketch can be a useful tool on mobile devices
to search the desired images.

Within CBIR, sketch-based image retrieval (SBIR) aims to return images that are similar
to a sketch made by the user (typically a simple set of strokes). Further, SBIR is particularly
adapted to situations where a user has a mental image of what he/she is searching. In this
scenario, a sketch image is useful specially when the image dataset is not annotated or the
user has no similar example image to use as query input. SBIR is also adequate when the
user imagines a configuration of lines that cannot be described in words. As discussed by
Smeulders et al. [33], “Pictures have to be seen and searched as pictures: by objects, by
style, by purpose”.

There are two important challenges in SBIR: (i) finding a relevant visual content representation 
associated with a similarity measure that allows effective comparison with a query
that is not a picture, but rather a drawing made by a user, and (ii) making retrieval scalable 
to large image datasets. In order to overcome these challenges, it is important to build
an appropriate index structure able to better exploit the content representation by the similarity 
measure. Thus, we consider that both challenges should be jointly addressed to find
efficient solutions for the SBIR problem.

In this paper, we propose a method where sketches and natural contours extracted from
images are represented and compared in both, the compressed-domain of wavelets and the
pixel domain. The relevant information regarding to image content has, thus, a compact
representation in the wavelet domain that can be readily employed by an efficient index for
retrieval by similarity. Our goal is to retrieve from large datasets all images that are visually
similar to a query sketch. Figure 1 presents an outline of the SBIR approach. The process
is divided in two main steps: (i) in the offline process, the features are extracted from each
image of the dataset and indexed in a database; and (ii) in the online process, the query
sketch has its features extracted in order to compare their similarity to the ones stored in the
database. Finally, the most similar images are sorted and displayed.

Among several advantages of querying images by sketch we can detach the possibility
to depict the image as the user desire in such a way that is not possible to describe in words.
In this sense, the user can describe the desired objects as well as the position, rotation and
scale of the objects. For example, in some cases, the user wants to visually describe not only
the object, but position and rotation aspects like in the sketches depicted in Fig. 6. Thus, the
present approach presents this characteristic as an advantage.

This paper is an extension of our previous work presented in Filho et al. [11], where
we proposed the use of the compressed-domain index using wavelet coefficients. This
paper presents several new aspects in comparison with the previous one. These aspects are
highlighted in the following:
– We combine the efficiency of the compressed-domain of wavelets index [11] and the
effectiveness of the pixel domain comparison with the Oriented Chamfer Matching
(OCM) [4] in a single similarity measure. The inclusion of OCM verifies the consistency 
of edges position contributing for the improvement in effectiveness of the
approach presented in [11], and further overcoming effectiveness with more efficiency
than [4].

Multimed Tools Appl (2017) 76:22019–22042

22021

Fig. 1 Outline of the sketch-based image retrieval approach. Upper part: offline indexing procedure, including 
image preprocessing, contour detection and thresholding; edge orientation estimation; oriented edgel
neighborhood estimation inside a radius r (given in pixels); standard Haar wavelet decomposition; set of
most significant coefficients A. Lower part: the online retrieval, sketch input Q; edge orientation estimation;
oriented edgel neighborhood estimation; wavelet decomposition; similarity measure; sorting the similarity
results; and display of the classified images

– We use a variable number of wavelet coefficients rather than a fixed number to represent 
the image contours in the compressed-domain as in out previous work [11].
This change improves the efficiency in relation to [11] and simplifies the comparison 
method. Also, it allows us to use classical information retrieval measures to better
evaluate this approach.

– We present a sketch collection with more than 100 sketches drawn by several volunteers

for the VGG Paris dataset.1

Section 2 presents some works which are directly related to our method. Section 3
provides a detailed description of the proposed approach concerning to the content representation 
and the similarity measure for SBIR. Sections 3.3 and 3.4 focus on the index
structure that supports scalable and efficient retrieval. Section 4 presents the experimental
evaluation. A smaller image dataset is used for selecting appropriate parameters and evaluating 
the effectiveness of our approach while a second dataset with more than 535K images
issued from ImageNet is then employed for the efficiency evaluation comparison with those
of Cao et al. [4] and Filho et al. [11]. Further, in this section, we compare our approach with
the methods presented in [13]. Finally, Section 5 concludes this paper and presents some
directions for future work.

1Visual Geometry Group – http://www.robots.ox.ac.uk/∼vgg/data/parisbuildings/index.html

22022

2 Related work

Multimed Tools Appl (2017) 76:22019–22042

In SBIR we have two main approaches lines. The first one, without learning techniques,
mostly based on contour comparison, and an other based on learning techniques, either
using BoVW, either using other techniques.

2.1 No learning approaches

The work presented by Kurita et al. [16] is one of the first approaches using the idea of
sketch as input to search image collections. In this paper, the authors addressed the problem 
of SBIR with an approach named Query-by-Visual Example (QVE). For this work, the
authors used a dataset of art gallery paintings. The query and the images of the dataset are
transformed into an abstract representation of the image edges in 64 × 64 pixels. Aiming
to compare the images, the similarity measure estimates a local correlation of the sketch
edges with the corresponding neighbourhood of the target images. However, the low resolution 
images lead to low effectiveness, and the scope of this technique was restricted to art
galleries images only.

Probably, one of the most known works in CBIR and SBIR using low level features is the
IBM’s Query by Image Content (QBIC) system described in Myron et al. [12]. Developed
in the middle of the nineties, the QBIC is a tool that allows users to query visual content not
only in images and but also in videos. Regarding to visual features on which the users can
search images, it is possible to explore: color, texture and shape. In QBIC system, the query
can be performed by example or by sketch.

An elastic shape similarity for sketch-based image retrieval was shown in Del Bimbo
and Pala [7]. The proposal is based on elastic deformation of the user sketch regarding
to match objects in the images of the dataset. This approach is an attempt to approximate 
how human perception works in shape similarity perception of objects. Spatial
relationships among objects in multi-object queries play an important role on this work. Furthermore,
 the elastic matching is integrated with arrangements to provide scale and partial
rotation invariant, however, the main drawback of this work, is the expensive computational
cost.

Jacobs et al. [14] used the compressed-domain of wavelets to represent the images from
the dataset. The wavelet decomposition allows one to have a good image approximation with
just a few amount of data. This same property is successfully used for lossy image compression 
[8]. Typically, in this context, just a few wavelet coefficients with the largest magnitude
are used to represent an approximation of the original image, allowing the construction of a
very small index for the dataset. The mentioned approach uses query-by-painting [38] and
the query may be interactive, i.e., while the user draws the query image, a preview of the
results is automatically shown.

Significant advances in efficiency were made recently (e.g., Cao et al. [4]. In such work,
named Mind-Finder, the authors presented a contour-based matching algorithm to assess the
similarity of sketches and natural image contours. In this approach, the authors converted
each image of the dataset into a set of oriented edgel descriptors. The edgels are contour
image pixels or edges, plus their line orientation. Following, this visual word is indexed
using inverted lists. In the query step, the edgels of the query sketch are matched to the
dataset index using a variation of the Chamfer Matching (CM) [2] algorithm. In this work,
the variation of the CM is named Oriented Chamfer Matching (OCM) [35], because of
the addition of the orientation information. Two million image composes the dataset of
this approach, indexed in main memory with 6.5 GB. However, indexing large datasets is

Multimed Tools Appl (2017) 76:22019–22042

22023

limited to the available memory space, and the amount of data to process a query makes the
approach very expensive on very large datasets.

A significative advance on scalability was proposed by Filho et al. [11]. In order to reduce
the index size and computational cost on the query time, the image edge maps, quantized
in six orientations, are transformed to the wavelet domain, where only the most significant
coefficients can represent the edge map. Thus, the index can be reduced to only 3–5% of
the index size of [4], and by consequence, the query process is much faster. This approach
has enabled practical applications for SBIR. Although the approach from Filho et al. [11]
presents improvements in efficiency, and the same efficacy using natural image contorus as
query input, effectiveness was not its main focus.

A similar approach to Filho et al. [11], where the objective is to save memory space and
computational cost is presented in Tseng et al. [37]. This SBIR application is also based
on the approach of Cao et al. [4], and the authors focused on the idea of SBIR for mobile
devices. This approach also splits the image edges on six quantized orientations, and then,
the authors apply the Distance Transform (DT) on each oriented channel. Following, high
dimensional DT features are projected in a compact hash bits. Retrieval performance is
competitive to Cao et al. [4], requiring only 3% of the memory storage, according to the
authors. However, this work presented just a few results and did not use a common dataset
and sketches to compare their approach with others. Also based on memory saving and
large image datased indexing, the work presented in [39] requires only 3% of memory
transforming the image contours in shape words.

Sun et al. [36] also used the idea of hash bits as presented in Tseng et al. [37]. The
objective of this work is to perform a big data sketch-based image retrieval, indexing more
than 1.5 billion images. The basis of this work is the same of Cao et al. [4] and the attacked
problem is to build a compact index that represents the oriented image contours as in Tseng
et al. [37] and Filho et al. [11]. The main problem of this work is the contour detector which
is fast but not accurate.

2.2 Supervised learning approaches

According to the kind of approach, some techniques use some learning learning process.
Most recent approaches make use of descriptors and “Bag-of-Visual-Words” (BoVW).
Among the set of visual words within the image database, a subset of visual words is
selected to represent the approximation of the hole set of visual words. One advantage of
this kind of technique is that visual words do not have spatial position encoded, making the
retrieval more robust to spatial variation. On the other hand, different sketches may have
similar set of visual words.

Hu and Collomosse [13] presented another approach adopting “Bag-of-Visual-Words”.
Histogram of Oriented Gradient (HOG) descriptor is adapted to Gradient Field HOG (GF-
HOG). Authors tested eight common distance measures frequently used in text (“Bag-of-
Words”) retrieval. The approach was evaluated using 33 shape categories search patterns.
Further, the authors incorporated semantic keywords aiming to enable the use of annotated
sketches for image search. As shown in the paper, the keywords inserted in the similarity 
measure increased the results in semantic terms. This work designed complex topology
model and presents competitive effectiveness, however difficult to scale up. Still in this context,
 the work presented by Bui and Collomosse [3] extended the GF-HOG context to enable
color-shape retrieval and comprehensively evaluate several early and late-fusion approaches
for integrating the modality of color. A second contribution of the work is the use of inverse
index to allow queries in large datasets.

22024

Multimed Tools Appl (2017) 76:22019–22042

Saavedra and Bustos [26] described a method based on Histogram of Edge Local Orientations 
(HELO). Local orientations are computed based on directional fields of fingerprints,
in the context of biometric processing. The proposed description is invariant to scale and
translation transforms. Aiming to achieve rotation invariance, the authors applied two different 
normalization processes, one using Principal Component Analysis (PCA), and other
using polar coordinates. More recently, Saavedra presented an extension of HELO in [27]
with a Soft HELO (SHELO) and an improved SHELO named RST-SHELO in [28].

Saavedra and Barrios [29] also proposed an approach based on mid-level patterns of
sketches presented as Learned KeyShape (LKS). In this approach, the authors figure out
keyShapes on a huge amount of sketch patches. A number of K keyShapes centers is defined
to build a histogram of size K where the histogram is built in three steps (i) Voting, (ii)
Spatial Division and (iii) Normalization.

Concerning to collection of sketches, Eitz et al. [9] presented a study with more than
20,000 sketches distributed over 250 categories. Authors described a sketch representation
approach in a form of “Bag-of-Visual-Words” for training a multi-class Support Vector
Machine (SVM) for classifying sketches. Human vs. machine sketch classification is confronted 
and the results showed that humans can correctly identify the object category 73%
of the time, while computers presented an accuracy of 56%. Furthermore, in this work,
the authors affirmed that automatic recognition of the sketch category can be potentially
employed to improve the SBIR task.

Germane to the evaluation performance on large scale SBIR approaches, the authors
of [10] designed a benchmark and built a dataset of more than 30,000 ratings, according
to human criteria of how much a pair sketch/image is similar. The evaluation study was
performed in a controlled environment where the users were instructed on how to perform
the task. This analysis demonstrated that humans match sketch and image pairs in a similar
way. Also, the approach presented a new local descriptor based on shape and Scale-Invariant
Feature Transform (SIFT) descriptors [19]. These descriptors are then adapted to a “bag-of-
features” [32] approach for SBIR domain.

More recently, Yonggang et al. [22] the authors propose a SBIR approach based on Convolutional 
Neural Networks (CNN). Like in object classification, where the convolutional
neural network learns the convolutional features to describe the objects, in the proposed
approach, the neural network learns the feature vectors closer for input sketch-image pairs
that are labeled as similar, and then push away irrelevant features. The authors implement
this approach by turning two neural networks linked by one loss function.

In [23], the authors proposed a sketch-based image retrieval approach with re-ranking
and relevance feedback schemes, that taked into account the use of the semantics in query
sketches and the top ranked images of the initial results. Recently, in [21], the authors proposed 
a system based on shape and structure similarity computed from compact sketch
descriptors, moreover, a trained support vector machine classier provides semantic ltering,
which is then combined with median ltering to return the ranked results.

In [20], the authors proposed a representation scheme which takes sketch strokes into
account with local features, thereby facilitating efficient retrieval with codebooks. According 
to the authors, stroke features are detected via densely sampled points on stroke
lines from which local gradients are further enhanced and described by a quantized histogram 
of gradients. Furthermore, a codebook is organized in a hierarchical vocabulary tree,
which maintains structural information of visual words and enables efficient retrieval 
in
sub-linear time.

Multimed Tools Appl (2017) 76:22019–22042

22025

3 Sketch-finder OCM description

The objective of our approach for SBIR is to retrieve the most similar images to the user
sketch input, in which the similarity between a sketch and one image uses two main criteria:
(i) shape sensitive, which means that the contour shape of the target image objects must
be as close as possible to the sketch, and (ii) position sensitive, which means that image
contours should be as close as possible to the sketch strokes in terms of position and scale.
The present approach which is outlined in Fig. 1 aims to improve the effectiveness when
compared to Cao et al. [4] and Filho et al. [11], while preserving efficiency. For achieving 
these goals, we use the compressed-domain index based on the wavelet decomposition
and the pixel domain to verify the spatial consistency of the edgels. An edgel (x, y, θ ), as
defined by Cao et al. [4], is an edge pixel spatial position (x, y) plus its orientation (θ ).

3.1 Indexing process

For the similarity measure, we use two steps of image comparison, one based on the
compressed-domain of the wavelet [11] and another directly in the pixel domain, the OCM.
The index is built during an offline process over all images of the dataset, as illustrated in
the offline box of Fig. 1. A detailed description of each processing step is presented on the
following.

–

Image resize. Aiming to have all images with the same size, important for matching
the spatial position (x, y) of the wedgels, all images are resized to the same pattern,
256 × 256 pixels.

– Contour detection. Because our approach uses black and white line-based sketches,
we need to detect the contours of the dataset images. As such, a good algorithm of
contour detection is fundamental for the success of this approach. To obtain the image
contours, we use the hierarchical Ultrametric Contour Map (UCM) approach, with the
default parameters, described in [1].

– Threshold. To obtain the most important object contours, the result given by the UCM
algorithm is then thresholded. The threshold choice and other parameters were obtained
on an optimization process with a genetic algorithm, as described in Section 4.2.
– Orientation estimation. The natural contours of each image I and the query sketch Q
are quantified in N oriented  channels, where  is a set of quantified orientations:
 = {θ1, θ2, ..., θN
}. In this work, the edge orientation is estimated and quantified
in six intervals, i.e., N = 6: θ1 ∈ (−15
),... and, θ6 ∈
). Each edge pixel (x, y) plus its orientation information θ, named edgel,
(135
is defined as pi = (x, y, θ ) [4].

), θ2 ∈ (15

◦ ∼ 165

◦ ∼ 15

◦

◦

◦ ∼ 45

◦

– Oriented edgel neighborhood. For each orientation θ, the pixel neighborhood of each
edge element in a radius r, given in pixels, is integrated to the set of edgels, making this
neighborhood part of the set of edgels Lθ = {p1, p2, ..., p|Lθ|}, where |Lθ| represents
the number of edgels in the set Lθ . In this paper, the neighborhood of edgels is simply
named “edgel neighborhood map” GI
– Wavelet transform. Only largest magnitude coefficients are used to encode each transθ 
, composing the set of wedgels Aθ = {w1, w2, ..., w|Aθ|}, where |Aθ|
formed GI
represents the number of element in the set Aθ and a wedgel is a quadruple wi =
(x, y, s, θ ), with its coefficient at spatial position represented by (x, y), its quantized

θ , of an Image I and orientation θ.

22026

Multimed Tools Appl (2017) 76:22019–22042

sign s, and its edgel neighborhood map orientation represented by θ. Instead of using
a fixed number of coefficients as in [11], we select the largest magnitude coefficients
greater than a given threshold ω.

(cid:2)

Figure 2b represents the edge orientation estimation obtained from Fig. 2a. In this figure,
we show just three orientations to simplify the idea and illustration, although we actually use
six orientations. Figure 2c presents an illustration of three
edgel neighbourhood,
 with radius r = 2 pixels, obtained from Fig. 2b. More details on defining r are
given in Section4.2. Figure 2d represents the positive wedgels in white dots, and the negative 
ones in black. These coefficients were obtained from the Haar wavelet transform of GI
represented in Fig. 2c. Selected coefficients or wedgels are quantized to +1 if the coefficient 
is positive, or −1 otherwise; in our approach, the real value of the selected coefficients
is not important. Further details on the wavelet transform algorithm used in this approach
are presented in Jacobs et al. [14].

,GI

GI

,GI

θ 2

θ 1

(cid:3)

θ 3

θ

3.2 Similarity measure

As described in Filho et al. [11], the use of the compressed-domain of wavelet is an efficient
strategy for SBIR. Thus, in this work we first use the wavelet similarity to perform an
efficient query that is in forward refined in terms of effectiveness using the OCM.

Fig. 2 Visual feature extraction: Preprocessing: (a) image contour map; (b) estimation of edge orientation
and segmentation of their contour maps; (c) neighborhood edgel estimation; (d) wavelet domain representation
of the ten most positive coefficients (white dots) and the ten most negative coefficients (black dots)

Multimed Tools Appl (2017) 76:22019–22042

22027

The similarity measure SimQ,T between the query sketch Q and some target image
T of the dataset is given by the wavelet coefficients comparison W fQ,T times the edgel
similarity PT →Q performed by the Oriented Chamfer Matching (OCM). The similarity
measure SimQ,T between Q and T is presented in Eq. 1 and the estimation of W fQ,T and
PT →Q are presented in Eqs. 2 and 11, respectively.

SimQ,T = W fQ,T × PT →Q

(1)

in which f means the similarity measure to be used for comparing the coefficients as
defined in the next sections.

3.2.1 Wavelet comparison
The comparison similarity of wavelet coefficients W fQ,T can be performed in two ways: (i)

based on the number of similar coefficients; or (ii) based on classical information retrieval
methods.

3.2.2 Wavelet comparison by similarity measure metric

Consider the problem of computing the similarity, on the wavelet domain, between a query
sketch Q and a potential target T . The comparison is taken in several parts, one for each
combination of the set K = {Gr , θ, s}, to compute the intersection of wavelet coefficients
between QK and TK, where QK represents the set of coefficients of Q in the configuration
K and TK the set of coefficients of Q in the configuration K. Each coefficient map in Fig. 2d
represents two sets of K, in white dots the positive set of coefficients and in black the negative 
set. For example, in the first map of coefficients in Fig. 2d we have K1 = {G2, 0
,+1}
and K2 = {G2, 0
,−1}. The intersection of QK ∩ TK is the set of equal coefficients where
coefficients have the same position [x, y], edgel neighborhood Gr , orientation θ and coefficient 
signal s. However, in the wavelet similarity measure, we simply need the number of
coefficients in the set that is given by |QK ∩ TK|.

◦

◦

W GENQ,T = 1/

(cid:4)

1 + NK(cid:5)
i=1

(abs(|QKi

| − |TKi

|) + (|QKi

| − |QKi

∩ TKi

|) + (|TKi

| − |QKi

∩ TKi

|))

(cid:6)

(2)

| and |TKi

| respectively denote the number of coefficients in |Q| and |T | for
where |QKi
the map Ki and N denotes the number of maps.
According to Eq. 2, the comparison W GENQ,T in each map K of the compressed-domain
considers the distance between the sketch Q and the target image T in three parts: first, the
absolute difference between |QKi
|, this will compare if query and target image
are consistent in terms of information amount; second, the difference between the number
∩ TKi
|
of sketch coefficients |QKi
informs if all sketch QKi coefficients are found; and third, the difference between the
| and the number of coefficients in the internumber 
of the target image coefficients |TKi
section |QKi
| informs if all target TKi coefficients are matches to QKi. Note that
|QKi
|−|QKi
∩TKi
| ≥ 0. This distance measure in three parts
|−|QKi
| ≥0 and |TKi
is important because similar sketch and image must have a similar number of coefficients
and all coefficients must match one each other.

| and the number of coefficients in the intersection |QKi

| and |TKi

∩ TKi
∩TKi

22028

Multimed Tools Appl (2017) 76:22019–22042

For simplification, Eq. 2 can be rewritten as in Eq. 3.
|) + |QKi

W GENQ,T = 1/

(abs(|QKi

| − |TKi

⎛
⎝1 + NK(cid:5)
i=1

| + |TKi

| −2| QKi

∩ TKi

⎞
⎠ (3)
|)

3.2.3 Wavelet comparison by information retrieval metrics

Classical information retrieval methods have been used successfully for text retrieval. In this
section, we explore these methods adapted to visual retrieval approach where we exchange

W TQ,T in Eq. 1 by two classical models used for general information retrieval: the “term frequency 
inverse document frequency” (tf-idf ) W T IQ,T [24, 30, 32] orOkapi BM25 W BM25X

Q,T

[25].

Term frequency - inverse term frequency is the product of two statistical measures.
Inside tf-idf, the “term frequency” (tf ) part captures the relevance of the word inside a
document d. In our case, the word is the wedgel while the document is obviously an image.
Concerning to the “inverse document frequency” idf, this part captures the informativeness 
of visual words over the entire indexed data. According to the mathematical theory
of communication [31], the quantity of information of a random variable is inversely
proportional to its probability of occurrence. Thus, visual words that appear in many different 
images are less informative than these that appear rarely. Given a set of words,
A = {w1, w2, ..., w|A|}, the tf and idf are respectively computed by Eqs. 4 and 5.

tf (wi ) = nid

(4)
where nid is the number of occurrences of a word wi in the document d, andn d is the total
number of words in the document d. For Boolean frequencies, this is our case, tf (wi ) = 1
if the word exists in a document d, and 0 otherwise. The idf is presented in the following.

nd

idf (wi ) = log

N

ni

(5)

where ni is the number of occurrences of the word wi in the whole dataset and N is the
number of documents in the whole dataset, or the number of images.

Finally, tf-idf is the product of the two terms, tf and idf, shown in Eq. 6.

W t iQ,T = M(cid:5)

i=1

tf × idf (wi ) = nid

nd

log

N

ni

(6)

Okapi BM25 In information retrieval, Okapi BM25 is a ranking function used by search
engines to rank text documents according to their relevance. Part of the BM25 weight is
based on the previous presented idf. Okapi BM25 is not a single function, but actually a
family of scoring functions, with slightly different components and parameters. We present,
in Eq. 7, the BM25 function which is considered to be one of the most prominent.

BM25(wi ) = idf (wi ) ×

(7)
where nid is the number of occurrences of word wi in a document d, | d | is the number of
words in the document d, avgdl is the average number of words per documents, and k1 and
b are free parameters, by default, k1 ∈ [1.2, 2.0] and b = 0.75.

1 − b + b × |d|

nid + k1 ×

avgdl

nid × (k1 + 1)
(cid:2)

(cid:3)

Multimed Tools Appl (2017) 76:22019–22042

22029

Because the number of occurrences word wi in a document d (nid) is always binary,
i.e., it has in maximum one word i, we discard this information because every match of
wavelet coefficient is also an occurrence of a word, what is redundant. Further, once that
the experiments with variable b and k1 = 1 show that the best value for both is 1.0, we can
simplify Eq. 7 to the one presented in Eq. 8:

BM25X(wi ) = avgdl
| d |

(8)

It is important to note that this simplification takes in consideration changing the parameter 
b = 0.75 in Eq. 7 to b = 1.0 in Eq. 8, what makes both equations different as shown by
the results presented in Table 4. According to the results presented in Table 4, the function
of Eq. 8 can be used as an alternative to the one that we propose in Eq. 3 once that Eq. 8 is
more simple and computationally faster than Eq. 3.

W BM25X

Q,T

= M(cid:5)
i=1

BM25X(wiQ,T )

(9)

where M represents the number of wavelet coefficients in the set |Q∩T |, and wiQ,T is the

it h coefficient of the set.

3.2.4 Oriented chamfer matching

The proposed compressed-domain index of Filho et al. [11] presents a very compact index
resulting in fast query. However, the effectiveness is just equivalent for natural contours
input, but not superior to the one presented by Cao et al. [4] for real sketches. By analyzing
the results of the wavelet comparison, we observed that this method is robust to some spatial
variation between the position of the sketch strokes and the image contours. Nevertheless,
the consistency on the quantity of strokes at similar position is not very well matched in the
wavelet domain due to the loss of information. To overcome this problem, a verification in
terms of how much the number of edgels are similar between the query sketch and the image
contour, at similar spatial position, becomes necessary. One strategy that can be applied to
solve this problem is the Chamfer Matching [2] or, more specifically, the Oriented Chamfer
Matching [4, 17, 18].

Although the use of OCM decreases the efficiency of the method, this second comparison
does not present high computational cost for two reasons: first, it does not process two
similarity measures of OCM as described by Cao et al. [4]. Let Q → T be the similarity
from the sketch Q to the target image T and T → Q the similarity from T to Q. The
present approach just performs the OCM of T → Q. Second, all oriented edgel maps of
the dataset are indexed already with their neighbours in a radius r given in pixels, saving
computational cost to compute the edgels inside a radius r during the query time, as defined
in Hit function PT →Q, presented in Eq.11. Another possibility, is to process the OCM
only for the z best ranked images on the wavelet comparison, saving computational cost and
index size.
Formally, consider the set of edgels LQ, representing the oriented sketch contours, whose
position of the edgels p ∈ LQ is denoted by Xp = (xp, yp), and its gradient orientation
of a target image contour Q
is denoted by θp. In this pixel consistency, the Hit map MT
θ , θ ∈ , where
with N oriented quantized channels, and each channel is a binary map MT
r is the tolerance radius, given in pixels, |LQ| is the number of edgels of Q, θQ and θT

22030

Multimed Tools Appl (2017) 76:22019–22042

represents respectively the orientation θ of Q and T . The hit of edgel between Q and T is
defined in Eq. 10.

(cid:11)

H itQ(p) =

1 ∃p ∈ LQ((cid:9) XQ − XT (cid:9)2 (cid:2) r & θQ = θT )
0 otherwise

(10)

(cid:5)

PT →Q = 1|LQ|

(11)
Pixel consistency idea between the query sketch Q and some target image T is shown in
Fig. 3. Although, in such example, the pixel consistency is presented without considering
the edge orientation for simplification of the scheme, the comparison takes orientation into
account.

p∈LH itQ(p)

3.3 Query process

The query process has some common steps to the indexing outline described in Section 3.1:
image resize; orientation estimation; neighborhood edgel map; and wavelet transform. Contour 
detection is not necessary on the query time because the sketch is already a contour
entity, and the threshold is not applied because the input is already binary. Therefore, we
describe on the following just the processes not in common to these explained in Section 3.1
for indexing the dataset. Figure 1 presents the query outline of the proposed approach in the
online box.
Similarity measure. In this step, the set of wedgels AQ obtained from the sketch is
used to load the correspondent inverted file list of each wedgel wi ∈ AQ to the main
memory. The set of edgels LQ is also retrieved in inverted files for the OCM refinement.
 After, we measure the similarity with Eq. 1. More details about inverted file list
can be found in Section 3.4.
Sorting the similarity. After computing the similarity of the sketch to the image contours 
of the dataset, we sort the similarity of each image using the quicksort algorithm
and present the z best ranked images to the user.

–

–

The proposed compressed-domain index of Filho et al. [11] presents a very compact
index and fast query, however, the effectiveness is just equivalent, but not superior to the
one presented in Cao et al. [4]. Aiming to improve the effectiveness, we propose a spatial
pixel consistency comparison step using the Oriented Chamfer Matching.

Although our approach also uses the pixel domain to improve the effectiveness, this
second comparison does not present very much computational cost for two reasons, first,

Fig. 3 Pixel consistency comparison between the sketch and some target image. In this figure, from left to
right; the sketch strokes Q; the target image contours T ; and the representation of sketch stroke pixels inside
a neighborhood radius r (given in pixels) of T illustrating the OCM comparison T → Q

Multimed Tools Appl (2017) 76:22019–22042

22031

it does not process two similarity measures of OCM as in Cao et al. [4], i.e., the similarity
between the sketch Q and the target image T (Q → T ) and the similarity (T → Q). The
present approach just performs the OCM of (T → Q). Second, all oriented edgel maps of
the dataset are already indexed with their neighbours in a radius r, saving computational
cost to compute the edgels in a radius r during the query time, as defined in Hit function of
Eq. 11.

3.4 Index structure

In the present work, the structure can be visualized as two indexes, both are based on the
inverted list strategy. The first index is used for the compressed-domain of the wavelet
coefficients (wedgels) and the second is used for the pixel domain of edgels. The compact
index of the present approach was designed to support efficient and effective query. Our
approach presents an efficient query mainly because the compressed-domain has only few
number of wavelet coefficients to process. The effectiveness of our approach is improved
by spatial pixel consistency verification using the OCM as described in Section 3.3. In the
following, we present the compressed and the pixel domain index.

3.4.1 Wedgel index

Aiming to improve performance, the “matching” of wedgels is performed by retrieving each
inverted file list of image (IDs), associated with each wedgels (x, y, s, θ ) of the sketch. This
strategy acts like a shortcut to go directly to the important information to process, without
having to read entire index. Each image ID in the inverted lists represents one hit or match
in the similarity measure. After processing the corresponding lists of each sketch wedgel,
the similarity of the images of the indexed dataset, is then, sorted using quick sort algorithm.
Figure 4 presents the index structure for the compressed-domain of wavelet coefficients.
The wavelet domain with its most significant coefficients, already quantized, is shown in
Fig. 4a. Negative coefficients are represented by black dots, while the positive ones are represented 
in white. To simplify the idea, here, we illustrate just three orientations, although,
actually we have six. Figure 4b presents the wedgel dictionary (x, y, s, θ ), and Fig. 4c
presents the list of image IDs associated with each wedgel of the dictionary.

3.4.2 Neighborhood edgel map index

In order to save computational cost, instead of computing the neighbourhood edgel map in
the query time, we can compute the neighbourhood in a radius r for all images of the dataset
in the indexing time, thus, saving this pre computed data in a form of inverted file lists to
compose an index of edgels. These inverted lists are similar to these representing wedgels,
however, in such case, these lists of image IDs represent, therefore, the neighbourhood of
edgels used in the Hit function (Eq. 11).

The neighborhood edgels index is responsible for the pixel consistency verification using
the OCM. For each neighborhood edgel (x, y, θ ) of the dictionary, we store the list of image
IDs where there is an edgel in the neighborhood of each image. Instead of comparing the
radius r of all dataset edgels to the strokes of the sketch, we just have to retrieve the set of
sketch edgel LQ lists, giving a computational cost of O(LQ) lists to process. In a real time
application, we can compute the neighborhood of the edgel maps in the query time only
for the z best ranked images of the compressed-domain measure, i.e., the wavelet similarity
(WQ,T ), thus, exchanging memory space cost by a small computational cost.

22032

Multimed Tools Appl (2017) 76:22019–22042

Fig. 4 Compressed-domain of wavelet coefficients index using inverted lists: (a) wavelet domain representation 
of wedgels; (b) wedgel dictionary; and (c) inverted list of image IDs for each wedgel

Figure 5 presents the neighborhood edgel map index structure. Figure 5a illustrates
the three neighborhood edgel maps, Fig. 5b represents the neighborhood edgel dictionary
(x, y, θ ), and Fig. 5c represents the list of image IDs associated with each edgel of the
dictionary. In this figure, we illustrate just three orientations, aiming simplification of the
scheme, although, actually we have six orientations.

Although the computational cost is saved in query time, the only drawback of this

strategy is storage, once that it exchanges computational cost by memory space.

4 Experimental evaluation

For the experimental evaluation, we first used a small image dataset (Paris Dataset) to apply
a genetic algorithm in order to find the best configuration set of parameters, as described
in Section 4.2. The Paris Dataset is a homogeneous collection of 6,412 images collected by
the Visual Geometry Group (VGG2) from Flickr.

To collect the sketches for the Paris dataset queries, we asked some voluntaries to draw
one sketch for each one of the 11 categories of Paris landmarks present in the dataset (La
Defense, Tour Eiffel, Hotel des Invalides, Mus´ee du Louvre, Moulin Rouge, Mus´ee d’Orsay,
Notre Dame, Panth´eon, Pompidou, Sacr´e Cœur and Arc de Triomphe). We selected 10 users

2Visual Geometry Group – http://www.robots.ox.ac.uk/∼vgg/data/parisbuildings/index.html

Multimed Tools Appl (2017) 76:22019–22042

22033

Fig. 5 Neighborhood edgel indexing using inverted lists: (a) neighborhood edgel map GI
dictionary; and (c) inverted list of image sIDs

θ ; (b) edgel

composing a set of 110 sketches. The sketches and the ground-truth are available.3 Figure 6
presents some sketches collected for the Paris dataset query evaluation. Also, the Paris
dataset was used to compare the effectiveness of Sketch-Finder OCM (GEN) with SketchFinder 
[11] and Mind-Finder [4]. The efficiency was evaluated considering the precision of
z best rank position, and in this paper we used the 20 best positions as in Sun et al. [36].

To evaluate the efficiency among the works of Filho et al. [11], Cao et al. [4] and SketchFinder 
OCM (GEN) presented here, we used the CPU (Central Processing Unit) time and
I/O (Input/Output) in a large dataset with more than 535,000 images. The measure of I/O
computes the disk reading in bytes, while CPU is measured as time in seconds taken to
process a query. The images were issued from ImageNet2011 fall release4 in two main
categories; Building Edifice (node n02913152) and Vehicle (node n04524313) with all
subcategories of each category. The tests on this dataset were performed using 75 queries.
A third dataset, Flickr15K presented by Hu et al. [13] was used to compare Sketch-Finder
OCM (GEN) with [13] and the other approaches also presented by Hu and Collomosse
[13]. The Flickr15K is a dataset with 14,660 images issued from Flickr. Also, Hu and
Collomosse [13] presented a set of 330 sketches partitioned into 33 categories that we used
in our queries. The categories of the dataset Flickr15K contain shapes, e.g., hart, landmarks,
e.g., Eiffel Tower, Big Ben and objects, e.g., bike and airplane.

3Paris sketches and ground-truth – https://sites.google.com/site/sketchretrieval/
4ImageNet – http://www.imagenet.org/

22034

Multimed Tools Appl (2017) 76:22019–22042

Fig. 6 Examples of the Paris sketch dataset

4.1 Parameter relevance evaluation

Aiming to evaluate the relevance of the parameters of Sketch-Finder OCM (GEN), we
applied the 2k Factorial Design model presented in [15]. We have six parameters or factors
and, as in the 2k Factorial Design each factor of k has two alternative levels, the higher and
lower values of the factor, therefore, our analysis has 26, or 64 possibilities configuration of
parameter combinations.

(a) neighborhood edgel radius for the first wavelet transform: this parameter correradius 
size used before the wavelet

The parameters and their effects are described in the following:
sponds to the first neighborhood edgel map Gr1
transform;
corresponds to the second Gr2
sponds to the third Gr3

θ

θ

radius size used before the wavelet transform;

θ

(b) neighborhood edgel radius for the second wavelet transform: this parameter

(c) neighborhood edgel radius for the third wavelet transform: this parameter correradius 
size used before the wavelet transform;

(d) neighborhood edgel radius for pixel matching (OCM): this parameter corresponds
to the radius rOCM used in the OCM. In Sketch-Finder OCM (GEN), 0 means to not
use the pixel matching, i.e., only measure the wavelet similarity (WQ,T );

(e) wavelet coefficients threshold: this parameter corresponds to the wavelet coefficient
threshold ω that determines what coefficients are used to represent each neighborhood
edgel map in the compressed-domain of wavelet and which coefficients are discarded.
The coefficient is selected if its absolute value is greater than ω;
threshold of the image contours: this parameter is used for thresholding the UCM
image for indexing.

(f)

Table 1 presents the relevance (R) of each factor in percentage, as well as, the lowest (L)

and the highest (H) values used to test each factor.

The two most important parameters in our set are the threshold of the image contours and
the radius size of the OCM similarity comparison, both summing almost 75% of impact in
the precision. The variation of second neighborhood edge map radius presents almost 18%
of impact, while the variation of the second and the third radius maps, and the variation of
the wavelet threshold are almost insignificant.

Multimed Tools Appl (2017) 76:22019–22042

Table 1 2k Factorial Design
Model

Factor

In this table, Factor is the
parameter of Sketch-Finder OCM
(GEN) that we use to measure the
impact, L represents the lowest
value of the parameter, H the
highest value and R its respective
relevance in percentage

a) Edgel map 1
b) Edgel map 2
c) Edgel map 3
d) Edgel map (OCM)
e) Coefficient Threshold
f) Contour Threshold

22035

R

1.8%
18%
1.5%
47.5%
3.8%
27.4%

L

1
10
25
0
3.0
0.15

H

10
25
45
45
8.0
0.30

The main difference between Sketch-Finder OCM (GEN) and the one presented in
Filho et al. [11] is the addition of the Oriented Chamfer Matching for comparing sketch and
image contours. The effect of this parameter, presented by the 2k Factorial Design, indicates
improvement in effectiveness of our proposal over Filho et al. [11].

4.2 Parameter tuning with genetic algorithm

The parameters described in Section 4.1 need to be well set for obtaining the best
performance of Sketch-Finder OCM (GEN). Genetic algorithms are robust search and optimization 
techniques for finding the global optimum in a multimodal landscape. Therefore,
we present in this section, how the parameters of Sketch-Finder OCM (GEN) were chosen
using a genetic evolution approach [34]. To set the parameters, due to the large number of
experiments, we used a small dataset, the Paris dataset with a ground-truth for the sketches
that we collected.

The parameters and their intervals were chosen as described in Section 4.1. We started
the genetic algorithm with a population of one hundred random different configurations,
where each parameter had a random value between the lowest and highest limits (L and
H) presented in Table 1. On each “evolution” iteration, the best fitness solutions were preserved 
for the next iteration or “generation” creating new solutions through crossover and
mutation between them, while under averaged solutions were disrupted. Random selected
best solutions were used in pairs to generate two new solutions with crossover and mutation 
of parameters. For each pair of solution, with a probability Pc of 90%, we applied a
crossover into three of the six parameters randomly selected, and with a probability Pm of
10%, we applied a mutation of random value between the limits L and H of Table 1 into two
parameters, also randomly selected.
To represent and evaluate each individual parameter configuration in a single fitness
value, we used the Mean Average Precision (MAP) obtained from the precision×recall curve
[6]. We conducted extensive experiments for achieving the best rank of the proposed approach.
More than 50 genetic generation iterations, each one with 100 individual set of parameters
were performed, what gave more than 5,000 experiments. Each experiment built one index
solution and performed on it, 110 queries by sketch, i.e., a total of 550,000 queries.

4.3 Evaluation of sketch retrieval effectiveness and efficiency

The experiments on sketch retrieval used the best parameters obtained by the genetic algorithm 
in average of 110 queries. For Sketch-Finder OCM, this configuration is respectively
9, 15, 28, 6.0, 44 and 0.16 for the parameters a, b, c, d, e and f, described in Section 4.1.

22036

Multimed Tools Appl (2017) 76:22019–22042

Fig. 7 First 50 ranked images for the methods Sketch-Finder OCM (GEN), Sketch-Finder and Mind-Finder
on Paris dataset

For Sketch-Finder, we used the same parameters in common with Sketch-Finder OCM.
This configuration is respectively 9, 15, 28, 44 and 0.16 for the parameters: a, b, c, e and f
presented in Section 4.1.

Regarding to the evaluation of the Mind-Finder [4], we applied the same parameters to
the steps in common with Sketch-Finder and Sketch-Finder OCM (GEN), in other words,
image resolution of 256 × 256, same contour detection (UCM) and threshold = 0.16.

Fig. 8 Query results for the Paris dataset

Multimed Tools Appl (2017) 76:22019–22042

22037

Table 2 Average CPU query
time estimated in seconds for 75
queries using the ImageNet
dataset
This table presents the average
time (CPU AVG), in seconds, and
the standard deviation (CPU SD)

Approach

CPU AVG

CPU SD

Sketch-Finder
Sketch-Finder OCM (GEN)
Mind-Finder

6.56
31.66
394.43

1.13
11.73
401.57

For the radius r we experimented several configurations, between 25 and 65 pixels, finding 
that r = 45 brings the best fitness on 110 queries of the Paris dataset, using the same
criteria for the fitness as in Sketch-Finder OCM (GEN). The selected parameters were used
as default for the Mind-Finder in the comparisons with Sketch-Finder and Sketch-Finder
OCM (GEN). The experiments were realized in a machine with CPU Intel Xeon X5670
with 2.93 GHz and 72 Gb of RAM memory. For the effectiveness evaluation, we considered 
the 50 first ranked images of the methods Sketch-Finder OCM (GEN), Sketch-Finder
and Mind-Finder on Paris dataset. According to the results, Sketch-Finder OCM (GEN)
overcame Sketch-Finder and Mind-Finder (MF) in terms of effectiveness as shown by the
precision curve of the 50 first ranked images presented in Fig. 7.

Figure 8 shows some queries performed with Sketch-Finder OCM (GEN) using the Paris
dataset. The first image on each line is the query image and the following images are the
four best ranked images.

To compare Sketch-Finder OCM (GEN) to Mind-Finder, we also evaluated the CPU
query time in seconds and I/O reading in bytes. To evaluate the CPU time and I/O bytes
of the queries, we used the subset of the ImageNet with 75 queries. Regarding to the CPU
cost, we did not consider the time of I/O used to retrieve the inverted file list of IDs. With
this strategy we can simulate the CPU time without considering I/O variations and main
memory limitations. The benchmark of the CPU cost is presented in seconds in Table 2,
with its average time (AVG) and standard deviation (SD) of the 75 queries.

Regarding to I/O cost, we measured it in bytes considering the size of all inverted lists of
IDs used on each one of the 75 queries. Table 3 presents the average I/O and the standard
deviation.

All the programs used to compute the edges, the queries, the similarity measures and the
indexing (and their source code) is available online at http://www.icei.pucminas.br/projetos/
viplab/sketchfinder.html.

4.3.1 Comparison of ranking functions

We compared our weighting function presented in Eq. 3 with “term frequency inverse document 
frequency” (tf-idf ) and Okapi BM25A weighting functions. For this comparison, we
followed the same parameter set of Sketch-Finder OCM, changing only the weighting function.
 We used the Paris dataset with 110 sketch queries, as in the experiments presented in

Table 3 Average I/O estimated
in bytes for 75 queries using the
ImageNet dataset
This table presents the average
(I/O AVG) data reading, in bytes,
and the standard deviation (I/O
SD)

Approach

Sketch-Finder
Sketch-Finder OCM (GEN)
Mind-Finder

I/O AVG
2.89 × 108
1.52 × 109
2.96 × 109

I/O SD
1.58 × 107
4.08 × 108
2.79 × 108

22038

Multimed Tools Appl (2017) 76:22019–22042

Table 4 Weighting function
comparison for Sketch-Finder
OCM using Paris dataset

Ranking

Sketch-Finder OCM (GEN)
Sketch-Finder OCM (TFIDF)
Sketch-Finder OCM (BM25)
Sketch-Finder OCM (BM25X)

MAP

0.103
0.080
0.097
0.104

W BM25Q,T

Section 4.2. Sketch-Finder OCM was configured with values 9, 15, 28, 6.0, 44 and 0.16,
which are respectively the parameters a, b, c, d, e and f, described in Section 4.1.
In these experiments, we changed the wavelet function W GENQ,T by W T IQ,T for (tf-idf ) and
For the experiments with Sketch-Finder OCM (BM25) we used k1 = 1.0 andb = 0.75.
Table 4 presents the comparison results of the three weighting functions. Sketch-Finder
OCM. The results are presented in Mean Average Precision (MAP).

for BM25 in Eq. 1.

4.4 Comparison with other SBIR approaches

Like in [4, 11, 16, 36, 37] our approach do not use any learning technique, thus we compare 
our approach with Mind-Finder [4], which is in the same category. Table 5 presents
the comparison of our approach and Mind-Finder on Flickr15K database. As presented in
Table 5, our approach overcomes Mind-Finder on Flickr15K database as well as in Paris
database, as presented in Fig. 7.

We also compare our approach with SBIR approaches using some learning technique.
Table 5 presents the comparison of our approach with GF-HOG [13], HOG [5], SIFT [19],
RST-SHELO [28], Siamese CNN [22] and LKS [29].

It is important to detach that our Sketch-Finder OCM (GEN) is sensitive to affine transforms,
 which is a desired characteristic when the user has chosen to query an image by
sketch, exactly because he/she wants not only the desired object that has in mind, but also, at
similar position and scale. Notwithstanding Sketch-Finder OCM (GEN) presented a lower
MAP than the SSIM, HOG and GF-HOG, the ground-truth used is designed for approaches
that are invariant to affine transform, what brought a negative impact to our method and
Mind-Finder, that were not designed for SBIR with affine transforms.

Table 5 Comparison of our
approach and SBIR approaches
with and without learning
techniques (Flickr15K database)

Kind

Approach

No learning
No learning
Learning
Learning
Learning
Learning
Learning
Learning

Sketch-Finder OCM (GEN)
MindFinder
GF-HOG
HOG
SIFT
RST-SHELO
Siamese CNN
LKS

MAP

0.087
0.085
0.122
0.109
0.091
0.200
0.195
0.245

Multimed Tools Appl (2017) 76:22019–22042

22039

5 Conclusion

This work presented an approach for SBIR using both, the compressed-domain and the
pixel domain indexes. The compressed-domain index allows the comparison between the
sketch and the image dataset contours in a few set of data while the pixel domain is used to
improve the precision by applying a spatial pixel consistency verification using the Oriented
Chamfer Matching.

The main difference between Sketch-Finder OCM (GEN) and the one presented in Filho
et al. [11] is the spatial pixel consistency verification. The impact of this similarity presented
by the 2k Factorial Design model (Section 4.1) shows the improvement of effectiveness of
our proposal, that overcomes Mind-Finder [4] and Sketch-Finder [11].

The query of Sketch-Finder OCM (GEN) is faster than Cao et al. [4] because we store
the edgel maps with precomputed radius. As drawback, Sketch-Finder OCM (GEN) has
a bigger index comparing to the other experimented approaches, however, in a real time
application we can avoid this strategy and compute the radius of the edgel maps only for the
best ranked images of the wavelet similarity, thus, presenting almost the same efficiency of
the Sketch-Finder while preserving the effectiveness.

Another advantage of Sketch-Finder OCM (GEN) is that it can simulate the SketchFinder 
just without the use of the OCM comparison. This can be an advantage to avoid
the second comparison when the server is performing several requests at the same time,
applying the complete test when the server load is low or when the query effectiveness is
priority. Although the computation of the OCM increases the similarity measure cost of
the Sketch-Finder, even with this second step, Sketch-Finder OCM (GEN) is still faster
and more efficient than [4]. Finally, after the efficiency gain with the use of compresseddomain,
 the combination of several features in one inverted list instead of one list per feature
improved even more the efficiency of our method.

Regarding the further works, one interesting issue to study is how to extract the “sketch”

perception of the natural image taking into account the edges of the objects.

Acknowledgments The authors are grateful to CAPES/COFECUB, FAPEMIG (PPM-006-16), CNPq
(307062/2016-3) and PUC Minas for the financial support to this work.

References

1. Arbelaez P, Maire M, Fowlkes C, Malik J (2011) Contour detection and hierarchical image segmentation.

IEEE Trans Pattern Anal Mach Intell 33(5):898–916. doi:10.1109/TPAMI.2010.161

2. Brogefors G (1988) Hierarchical chamfer matching: a parametric edge matching algorithm. IEEE Trans

Pattern Anal Mach Intell 10(6):849–865. doi:10.1109/34.9107

3. Bui T, Collomosse J (2015) Scalable sketch-based image retrieval using color gradient features. In: The

IEEE international conference on computer vision (ICCV) workshops

4. Cao Y, Wang C, Zhang L, Zhang L (2011) Edgel index for large-scale sketch-based image search. In:

CVPR, pp 761–768

5. Dalal N, Triggs B (2005) Histograms of oriented gradients for human detection. In: Proceedings 
of the 2005 IEEE computer society conference on computer vision and pattern recognition
(CVPR’05) - Volume 1 - Volume 01, CVPR ’05. IEEE Computer Society, Washington, pp 886–893.
doi:10.1109/CVPR.2005.177

6. Davis J, Goadrich M (2006) The relationship between precision-recall and roc curves. ACM, New York
7. Del Bimbo A, Pala P (1997) Visual image retrieval by elastic matching of user sketches. IEEE Trans

Pattern Anal Mach Intell 19(2):121–132. doi:10.1109/34.574790

8. DeVore RA, Jawerth BD, Lucier BJ (1992) Image compression through wavelet transform coding. IEEE

Trans Inf Theory 38(2):719–746

22040

Multimed Tools Appl (2017) 76:22019–22042

9. Eitz M, Hays J, Alexa M (2012) How do humans sketch objects? ACM Trans Graph 31(4):44:1–44:10.

doi:10.1145/2185520.2185540

10. Eitz M, Hildebrand K, Boubekeur T, Alexa M (2011) Sketch-based image retrieval: benchmark and

bag-of-features descriptors. IEEE Trans Vis Comput Graph 17(11):1624–1636

11. Filho CAF, Ara´ujo AA, Crucianu M, Gouet-Brunet V (2013) Sketch-finder: efficient and effective
sketch-based retrieval for large image collections. In: Proceedings of the 2013 XXVI conference on
graphics, patterns and Images, SIBGRAPI ’13. IEEE Computer Society, Washington, pp 234–241.
doi:10.1109/SIBGRAPI.2013.40

12. Flickner M, Sawhney H, Niblack W, Ashley J, Huang Q, Dom B, Gorkani M, Hafner J, Lee D, Petkovic
D, Steele D, Yanker P (1995) Query by image and video content: the qbic system. Computer 28(9):23–
32. doi:10.1109/2.410146

13. Hu R, Collomosse J (2013) A performance evaluation of gradient field hog descriptor for sketch based

image retrieval. Comput Vis Image Underst 117(7):790–806. doi:10.1016/j.cviu.2013.02.005

14. Jacobs CE, Finkelstein A, Salesin DH (1995) Fast multiresolution image querying. In: Proceedings of

SIGGRAPH 95, pp 277–286

15. Jain R (2008) The art of computer systems performance analysis:. Wiley India pvt limited
16. Kurita T, Otsu N, Hirata K (1992) A sketch retrieval method for full color image database-query by
visual example. In: 1992 IEEE Computer society conference on computer vision and applications (IAPR
1992). IEEE, pp 530–533

17. Lee YJ, Grauman K (2009) Shape discovery from unlabeled image collections. 2013 IEEE Conference

on Computer Vision and Pattern Recognition 0:2254–2261. doi:10.1109/CVPRW.2009.5206698

18. Liu MY, Tuzel O, Veeraraghavan A, Chellappa R (2010) Fast directional chamfer matching. In: CVPR.

IEEE Computer society

19. Lowe DG (2004) Distinctive image features from scale-invariant keypoints. Int J Comput Vision

60(2):91–110. doi:10.1023/B:VISI.0000029664.99615.94

20. Ma C, Yang X, Zhang C, Ruan X, Yang MH (2016) Sketch retrieval via local dense stroke features.
Image Vis Comput 46:64–73. doi:10.1016/j.imavis.2015.11.007 . http://www.sciencedirect.com/science/
article/pii/S0262885615001389

21. Polsley S, Ray J, Hammond T (2017) Sketchseeker: Finding similar sketches. IEEE Transactions on

Human-Machine Systems 47(2):194–205. doi:10.1109/THMS.2017.2649684

22. Qi Y, Song Y, Zhang H, Liu J (2016) Sketch-based image retrieval via siamese convolutional neural
network. In: 2016 IEEE international conference on image processing, ICIP 2016, Phoenix, AZ, USA,
September 25-28, 2016, pp 2460–2464. doi:10.1109/ICIP.2016.7532801

23. Qian X, Tan X, Zhang Y, Hong R, Wang M (2016) Enhancing sketch-based image retrieval by re-ranking

and relevance feedback. IEEE Trans Image Process 25(1):195–208. doi:10.1109/TIP.2015.2497145

24. Robertson S (2004) Understanding inverse document frequency: on theoretical arguments for idf. J Doc

60:2004

25. Robertson S, Zaragoza H, Taylor M (2004) Simple bm25 extension to multiple weighted fields. In:
Proceedings of the thirteenth ACM international conference on information and knowledge management,
CIKM ’04. ACM, New York, pp 42–49, doi:10.1145/1031171.1031181

26. Saavedra J, Bustos B (2010) An improved histogram of edge local orientations for sketch-based image
retrieval. In: Goesele M, Roth S, Kuijper A, Schiele B, Schindler K (eds) Pattern recognition, lecture
notes in computer science, vol 6376. Springer, Berlin Heidelberg, pp 432–441

27. Saavedra JM (2014) Sketch based image retrieval using a soft computation of the histogram of edge local
orientations (s-HELO). In: 2014 IEEE International conference on image processing, ICIP 2014, Paris,
France, October 27-30, 2014, pp 2998–3002. doi:10.1109/ICIP.2014.7025606

28. Saavedra JM (2015) Rst-shelo: sketch-based image retrieval using sketch tokens and square root

normalization. Multimedia Tools Appl 1–21

29. Saavedra JM, Barrios JM (2015) Sketch based image retrieval using learned keyshapes (LKS). In: Proceedings 
of the british machine vision conference 2015, BMVC, 2015, Swansea, UK, September 7-10,
2015, pp 164.1–164.11. doi:10.5244/C.29.164

30. Salton G, McGill MJ (1986) Introduction to modern information retrieval. McGraw-Hill, Inc., New York
31. Shannon C (1948) A mathematical theory of communication. Bell Syst Tech J 27(379-423):623–656
32. Sivic J, Zisserman A (2003) Video google: a text retrieval approach to object matching in videos. In:
Proceedings of the 9th IEEE international conference on computer vision - vol. 2, ICCV ’03. IEEE
Computer Society, Washington, p 1470

33. Smeulders AWM, Worring M, Santini S, Gupta A, Jain R (2000) Content-based image retrieval at the
end of the early years. IEEE Trans Pattern Anal Mach Intell 22(12):1349–1380. doi:10.1109/34.895972
34. Srinivas M, Patnaik LM (1994) Adaptive probabilities of crossover and mutation in genetic algorithms.

IEEE Trans Syst Man Cybern 24(4):656–667

Multimed Tools Appl (2017) 76:22019–22042

22041

35. Stenger BDR (2004) Model-based hand tracking using a hierarchical bayesian filter
36. Sun X, Wang C, Xu C, Zhang L (2013) Indexing billions of images for sketch-based retrieval. In:
Proceedings of the 21st ACM international conference on multimedia, MM ’13. ACM, New York,
pp 233-242. doi:10.1145/2502081.2502281

37. Tseng KY, Lin YL, Chen YH, Hsu WH (2012) Sketch-based image retrieval on mobile devices using
compact hash bits. In: Proceedings of the 20th ACM international conference on multimedia, MM ’12.
ACM, New York, pp 913–916

38. Venters CC, Hartley RJ, Hewitt WT (2005) Content-based image retrieval query paradigms. In:

Encyclopedia of information science and technology (i), pp 556–563

39. Xiao C, Wang C, Zhang L, Zhang L (2015) Sketch-based image retrieval via shape words. In: Proceedings 
of the 5th ACM on international conference on multimedia retrieval, Shanghai, China, June 23-26,
2015, pp 571–574. doi:10.1145/2671188.2749360

Carlos Alberto Fraga Pimentel Filho has received PhD title from Federal University of Minas Gerais
(2014), master degree in Information and Systems from Salvador University (2008) and bachelor degree
in computer science from Tiradentes University (2005). He worked at Atalaia Institute of Research and
Development as researcher on speech recognition for Brazilian Portuguese. He has experience on computer
vision, speech recognition and image segmentation. Now, he is post doctorate researcher PUC Minas, Belo
Horizonte working on mathematical morphology and image segmentation.

Benjamin Bustos is an Associate Professor at the Department of Computer Science, University of Chile. He
is head of the PRISMA Research Group, and he is also a researcher at the Millennium Nucleus Center for
Semantic Web Research. He leads research projects in the domain of content-based multimedia information
retrieval. His research interests include similarity search, 3D object retrieval, multimedia mining, semantic
Web, metric/nonmetric indexing, and pattern recognition. He obtained a doctoral degree in natural sciences
from the University of Konstanz, Germany, in 2006.

22042

Multimed Tools Appl (2017) 76:22019–22042

Arnaldo de Albuquerque Ara´ujo received his D.Sc. degree in Electrical Engineering, from the Universidade 
Federal da Paraiba (UFPB), Campina Grande, Brazil, in 1987, did graduate studies (Image Processing)
at the RWTH Aachen, Germany, 1981-1985, and made sabbatical years at ESIEE Paris, 1994-1995, ENSEA
Cergy-Pontoise, 2005, and at the UPMC Paris 6, 2008-2009, all three in France. He was an associate professor 
at the Electrical Engineering Department (DEE), UFPB, 1985-1989. Arnaldo is a full professor at
the Computer Science Department, Universidade Federal de Minas Gerais (UFMG), Belo Horizonte, MG,
Brazil (since 1990). His research interests include digital image processing and computer vision applications
to medicine, fine arts, and satellite imagery, and content based information retrieval. He has advised 15 PhD
thesis and 45 MSc dissertations. Arnaldo has been ad hoc consultant for several Brazilian foundations for
research and development. He is member of SBC, IEEE (senior member), ACM and EURASIP.

Silvio Jamil Ferzoli Guimar˜aes received his B.Sc. M.Sc. and D.Sc. degrees in Computer Science, from
Universidade Federal de Vic¸osa (UFV), Universidade Estadual de Campinas (UNICAMP) and Universidade
Federal de Minas Gerais (UFMG), in 1997, 1999 and 2003, respectively. Thanks to co-supervision program
between UFMG and Universit´e Marne-la-Vall´ee, he received another D.Sc. degree in Informatique. Silvio
is a member of the Computer Science Department (DCC) staff, Pontif´ıcia Universidade Cat´olica de Minas
Gerais (PUC Minas), Belo Horizonte-MG, Brazil (since 2002). He is the founder and header of the AudioVisual 
Information Processing Laboratory. His research interests include digital image and video processing
and computer vision applications, hierarchical information analysis, multimedia information systems, and
content based information (image and video) retrieval.

