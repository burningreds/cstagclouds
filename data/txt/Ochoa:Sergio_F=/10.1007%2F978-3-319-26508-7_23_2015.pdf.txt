Improving Social Communication Disorders

Through Human-Avatar Interaction

Esperanza Johnson1, Ramón Hervás1(&), Tania Mondéjar2,

José Bravo1, and Sergio F. Ochoa3

1 MAmI Research Lab, University of Castilla-la Mancha,

Paseo de la Universidad 4, Ciudad Real, Spain
MEsperanza.Johnson@alu.uclm.es,

{ramon.hlucas,jose.bravo}@uclm.es

http://mami.uclm.es

2 eSmile, Psychology for Children and Adolescents,

Calle Toledo 79 1ºE, Ciudad Real, Spain

Tania.mondejar@esmile.es

3 Computer Science Department, Universidad de Chile,
Av. Blanco Encalada 2120, 3rd Floor, Santiago, Chile

sochoa@dcc.uchile.cl

Abstract. Current assistive technologies can improve the quality of life of
people who have been diagnosed with different forms of Social Communications 
Disorders (SCD). In this paper we describe the way in which we have
approached an assistive system for improving SCD, based on human-avatar
interaction. Since the ﬁnal development is in progress, this paper contributes
with a general taxonomy that classiﬁes the different kinds of interaction between
humans and avatars, including the relationship among them and their communication 
skills. These skills are typically training to treat SCD.

Keywords: Affective computing  Social communication disorder  Cognitive
disabilities  Android  Human-avatar interaction

1 Introduction

The proposed project comes from a growing interest from the scientiﬁc community to
help people with cognitive developmental problems (e.g., autism spectrum disorder,
and attention deﬁcit disorder). These disorders typically encompass problems with
social interaction, social understanding and pragmatics (i.e., using language in proper
context). Assistive systems come in the shape of those affected living more independent 
lives, with a lesser degree of difﬁculties and issues. In short, improve their quality
of life in whatever way we can. In this particular case, we aim to improve it in terms of
communication (social interaction) with other individuals, as well as a better understanding 
of themselves (introspection).

This paper proposes an application for tablets-pc that will have an interactive
avatar, by which the user’s action will generate a reaction from the avatar, and such a
reaction will generate another reaction from the user, occurring a Human-Avatar
communication. The avatar will draw the social interaction and communication with a

© Springer International Publishing Switzerland 2015
J. Bravo et al. (Eds.): AmIHEALTH 2015, LNCS 9456, pp. 237–243, 2015.
DOI: 10.1007/978-3-319-26508-7_23

238

E. Johnson et al.

narrative role-playing game, in which the user is immersed in the different situations.
With this, the system helps the usersto know how to behave in certain situations and
what emotions will occur.

2 Related Work

Concerning this research topic, we have found information mainly related to applications 
that act as assistive systems, as well as various focused on the design of avatars.
The literature reports many assistive systems, such as smart homes that could help older
adults with the ADL (Activities of Daily Living), following the performance on each
activity [1, 2]; augmented reality-based systems to give guidance to people with
dementia based on well-known points of interest [3]; an eye tracker system for people
with severe motor disability to interact with an avatar [4]; and training systems that
assist older people with mental and physical damages through the use of an avatar in
their televisions [5].

In terms of avatar based interaction, there have been several interesting proposals
that we have looked into, including those centered on human-avatar interaction in a
simulation game, and quantifying how engaged the played is [6] and user interfaces
based on 3D avatars for interactive television, and how the user interacts and reacts to it
[7]. There is also a proposal of AI Framework for the supporting behavioral animation
of avatars, to make them behave more realistically [8], as well as research works that
discuss children’s avatar preferences [9], which has been key for the idea behind the
avatar creation of this paper.

3 Human-Avatar Interaction

This section will cover the aspect of human-avatar interaction. We discuss the different
kinds of interaction that a person can have with an avatar, classifying them in different
categories. Each kind of interaction can be useful to treat different communicative and
cognitive skills. Since our primary users are children with SCD, the human-avatar
communication helps to improve both verbal and non-verbal social communication
skills. Particularly, these skills include responding to others, using gestures (waving or
pointing), talking about emotions and feelings, taking turns when talking, staying on a
topic, adjust speech to ﬁt different contexts, etc. These skills depend on particular
cognitive processes related to communication.

In this paper we have taken into account seven cognitive processes to work with
those communication skills. Firstly, joint attention (J) is the shared focus of two
individuals on a same point or look to the other in the eyes; focused attention (F) is the
cognitive process of selectively concentrating on a discrete aspect of the communication;
 emotional states (E) are the feeling that we are seeming during the communication,
 intonation (I), i.e., variations of spoken pitch that indicates the attitudes of the
speaker; self-control (S), the ability to control one behavior and desires in communication 
demands; proprioception (P), the sense of body position (e.g., hands, arms, face)
related to body-language; and, understanding (U), as the general comprehension of
matters in the conversation.

Improving Social Communication Disorders

239

This taxonomy has been subdivided into implicit and explicit interactions (Table 1).
This comes from careful thinking and a realization that some of the interactions that
have been planned are involuntary, meaning that the gathered information and data
from the user does not come explicitly in some cases, which will be exempliﬁed
shortly. From there, we have identiﬁed in the most basic human information pro-
cessing: the human senses.

In terms of explicit interactions, we also refer to them as voluntary interactions, as
the interactions are done by the user with full intention and knowledge to do so.
Because of this way to understand explicit interactions, we have classiﬁed the following 
interactions as explicit: Ocular, Gestural (Facial and Body-language), Verbal,
Tactile, Object and Audio Interaction.

The previous interactions show the different senses involved in such a process. In
particular, ocular interaction refers to eye interaction from the user, and a clear example
would be eye-tracking applications in which the user is asked to follow or ﬁx their gaze
on something on a screen. Facial Interaction would be the user moving their facial
muscles to form expressions, Verbal Interaction is the action of speaking, Tactile
Interaction refers to the user interaction with their hand and anything they can touch,
whether it be a tactile screen, Body Interaction refers to the user moving their bodies in
whichever way it would be needed, Object Interaction refers to the interaction between
or with an object, and ﬁnally, Audio Interaction that is used frequently in studies that
involve any reaction to audio stimulation.

The interactions we have categorized as implicit are: Ocular, Gestural (Facial and
Body), Smell, and Biofeedback Interaction. The reason we have included Ocular,
Facial and Body Interaction in this section, as well as the previous one, is because there
are a variety of articles that study the involuntary aspects of those actions, such as the
unintentional wander of the eyes to detect the degree of attention [10], as well as facial
and body tics that are not a voluntary action from the user.

Then the Smell Interaction is what would be one-sided reception senses that play an

important part in the interaction, but it is rarely available to output any data.

is possible to gather different vital signal

Biofeedback Interaction requires the use of peripherals and more advanced technology.
 It
to improve human-avatar
inter-actions, such as blood pressure, heart rate, and brain activity. We deepen on the
last one in this paper because neurological reactions to certain interactions are commonly 
used to observe the nature of said reaction on a deeper level. This is an obvious
the previous
aspect

that humans cannot control. Table 1 presents visually all

Table 1. Taxonomy of the different types of interaction.

Interaction

Involved technology

Explicit interaction

(Bidirectional)

Ocular
Gesture

Facial

Camera
Camera, Motion Capture

Cognitive
processes
J, F, S
E, P, S

Sensors

Body

Camera, Movement

E, P, S

Sensors

(Continued)

240

E. Johnson et al.

Implicit interaction (Human

to Avatar)

Table 1.

(Continued)

Interaction

Involved technology

Verbal
Tactile

Object
Audio
Ocular
Gesture

Facial

Microphone
Tactile Sensors,
Accelerometer

Sensors, Mobile devices
Speakers, Headphones
Camera
Camera, Motion Capture

Sensors

Body

Camera, Movement

Biofeedback
Interaction

Sensors

Headset, Vital Sign

devices

Cognitive
processes
E, I, U
J, S, U

J, S, U
E, S
F
E, S

E, S

E, S

information and includes the communicative and cognitive skills that can be treat with
each kind of interaction.

4 Proposal for Cognitive Stimulation Through Avatars

The current project proposes an interactive avatar that will create the previously
mentioned human-avatar interaction using available technology. The primary technology 
that will be used is a tablet-pc (Android), as our initial target audiences were
children, and they are very much at ease with this technology. This is because it is a
fairly extended and used device nowadays; it can be acquired without investing too
much money, and has many of the elements that were mentioned in Table 1 that would
be needed to create the interaction.

First design decision, based on the importance of the empathy of children that it is
related to the avatar gender [9], was the creation of an androgynous avatar, so that the
children themselves would decide the gender of the avatar that they are interacting
with, as to draw a more positive response with such an avatar. The initial draft of the
avatar is shown in Fig. 1.

Figure 1 also shows the interactions that the avatar supports, indicated by arrows
that point both ways, representing bilateral communication between the human and the
avatar. Particularly, voice-based communication, video and audio-based interaction
refer to the Audio, Verbal, Ocular and Facial interactions that were explained in Sect. 3.
Gamiﬁcation and serious games refer to the narrative role playing games that the avatar
will play with the human, though in this case it is unilateral as it is something that the
avatar sends to the human, and any communication during the game falls within the
bilateral interaction that was previously mentioned. Natural multi-interaction comes
from the human, and it is speciﬁed as natural because the human reactions are not
programmed, such as the avatar’s interactions.

Improving Social Communication Disorders

241

Fig. 1. Avatar-based interactions included in the system and draft of the avatar.

The interactions is implemented in a tablet-pc, and therefore the technology used
will be mostly with the sensors that are available, which are a camera for Ocular and
Facial Interaction, Microphone for the Verbal Interaction and Headphones/Speakers for
the Audio Interaction. The Tactile Interaction is both with the accelerometer that most
tablets have in place, as well as the tactile screen.

In order to maximize the Human-Avatar Interaction, we are including an aspect of
emotion recognition, as to garner a more precise reaction from the avatar to the facial
expressions of the user. This will be accomplished using Active Shape Models and
Support Vector Machines that has been proven to yield good results [11], and recognizes 
the six basic emotions (happiness, sadness, anger, fear, disgust and surprise, as
well as a neutral) that will be present during the interaction.

5 Future Development

the story (narrative aspect of the game). The user would be put

Next step is the development of Avatar-based videogames with the mentioned interactions.
 The games would be narrative role playing games, by which the user would be
put in a certain situation (role playing aspect of the game) and there it would advance
throughout
in
day-to-day situations, so that when the time comes, they will be more used to the idea
of those situations, as well as have a better set of skills to face said situation, both in
general communication terms, as well as social. The interactions in these games will be
mostly explicit interactions with the avatar, in terms of Ocular, Facial, Verbal, Tactile
and Object, and we will also have implicit interactions with Biofeedback, using the
Emotiv Epoc neuroheadset, which will be used in controlled environments to verify that
we are gathering the results that we are expecting of this project. The user will interact
with the avatar, simulating these situations, and s/he may also be asked to interact with
certain objects in a room, for some of those situations.

The intention set for the future of this project is to make it as interactive as possible.
It would be desirable to interact with other objects that the user would interact with
(objects with NFC tags or other sensors) to maximize the options to use in the narrative
role playing game mentioned in the introduction.

242

E. Johnson et al.

6 Conclusions

There is a variety of taxonomies for classifying the interactions between a human and a
virtual avatar. As we can observe, there are enough to make the experience as realistic
as possible, and the different studies show us the characteristics an avatar should have
to draw a more positive response. We hope that the taxonomy presented in this paper
will aid more people when delving into interactions and SCD, both with a better
understanding of the interactions and how they affect different cognitive processes.

The importance of realism in the interaction is also why we chose the emotion
recognition that was previously stated, as it gave the best results compared to others
that we have studied, and such good results would improve not only the realism of the
interaction between human and avatar, but also the possibilities of an improved social
skill learning with a more accurate emotional reaction from the avatar for certain situations,
 and a better representation of said emotional state.

Acknowledgments. This work was conducted in the context of UBIHEALTH project under
International Research Staff Exchange Schema (MC-IRSES 316337).

References

1. Fontecha, J., Hervás, R., Mondéjar, T., González, I., Bravo, J. Towards context-aware and

user-centered analysis in assistive environments. J. Med. Syst. 39(120) (2015)

2. Serna, A., Pigot, H., Rialle, V.: Modeling the progression of Alzheimer’s disease for
cognitive assistance in smart homes. User Model. User-Adap. Inter. 17(4), 415–438 (2007)
3. Hervas, R., Fontecha, J., Bravo, J.: An assistive navigation system based on augmented
reality and context awareness for people with mild cognitive impairments. IEEE J. Biomed.
Health Inform. 18(1), 368–374 (2013)

4. Adjouadi, M., Sesin, A., Ayala, M., Cabrerizo, M.: Remote eye gaze tracking system as a
computer interface for persons with severe motor disability. In: Miesenberger, K., Klaus, J.,
Zagler, W.L., Burger, D. (eds.) ICCHP 2004. LNCS, vol. 3118, pp. 761–769. Springer,
Heidelberg (2004)

5. Plischke, H., Kohls, N.: Keep it simple! Assisting older people with mental and physical
training. In: Stephanidis, C. (ed.) Part I, HCII. LNCS, vol. 5614, pp. 278–287. Springer,
Heidelberg, Berlin (2009)

6. Norris, A.E., Weger, H., Bullinger, C., Bowers, A.: Quantifying engagement: measuring
player involvement in human–avatar interactions. Comput. Hum. Behav. 34, 1–11 (2014)
7. Ugarte, A., García, I., Ortiz, A., Oyarzun, D.: User interfaces based on 3D avatars for
interactive television. In: Cesar, P., Chorianopoulos, K., Jensen, J.F. (eds.) EuroITV 2007.
LNCS, vol. 4471, pp. 107–115. Springer, Heidelberg (2007)

8. Iglesias, A., Luengo, F., Ortiz, A., Oyarzun, D.: AI framework for decision modeling in
behavioral animation of virtual avatars. In: Shi, Y., van Albada, G.D., Dongarra, J., Sloot, P.
M.A. (eds.) ICCS 2007. LNCS, vol. 4488, pp. 89–96. Springer, Heidelberg, Berlin (2007)
9. Inal, Y., Sancar, H., Cagiltay, K.: Children’s avatar preferences and their personalities. In:
Society for Information Technology and Teacher Education International Conference,
pp. 4259–4266. Florida, USA (2006)

Improving Social Communication Disorders

243

10. Galgani, F., Sun, Y., Lanzi, P.L., Leigh, J.: Automatic analysis of eye tracking data for
medical diagnosis. In: IEEE Symposium on Computational Intelligence and Data Mining,
CIDM 2009, pp. 195–202. IEEE, March 2009

11. Lozano-Monasor, E., López, M.T., Fernández-Caballero, A., Vigo-Bustos, F.: Facial
expression recognition from webcam based on active shape models and support vector
machines. In: Pecchia, L., Chen, L.L., Nugent, C., Bravo, J. (eds.) IWAAL 2014. LNCS,
vol. 8868, pp. 147–154. Springer, Heidelberg (2014)

