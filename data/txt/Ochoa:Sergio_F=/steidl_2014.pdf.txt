Vocal Emotion Recognition
State-of-the-Art in Classiﬁcation of Real-Life Emotions

October 26, 2010

Stefan Steidl

International Computer Science Institute (ICSI)
at Berkeley, CA

Overview

2 / 49

1 Different Perspectives on Emotion Recognition

2 FAU Aibo Emotion Corpus

3 Own Results on Emotion Classiﬁcation

4 INTERSPEECH 2009 Emotion Challenge

S. Steidl: Vocal Emotion Recognition

Overview

1 Different Perspectives on Emotion Recognition

Psychology of Emotion
Computer Science

2 FAU Aibo Emotion Corpus

3 Own Results on Emotion Classiﬁcation

4 INTERSPEECH 2009 Emotion Challenge

S. Steidl: Vocal Emotion Recognition

Facial Expressions of Emotion

3 / 49

4 / 49

S. Steidl: Vocal Emotion Recognition

Universal Basic Emotions

5 / 49

Paul Ekman

postulates the existence of
6 basic emotions:
anger, fear, disgust,
surprise, joy, sadness
other emotions are mixed or blended
emotions
universal facial expressions

S. Steidl: Vocal Emotion Recognition

Terminology

Different affective states [1]:

type of affective state

intensity


duration


syn-
chronization





emotion
mood
interpersonal stances
attitudes
personality traits
◦: low, : medium, : high, : very high, -: indicates a range

 - 
 - 
 - 
◦ - 
◦ - 


 - 
 - 


◦
◦





◦
◦

6 / 49

appraisal
elicitation



rapidity 
of
change


behavioral

impact








◦





◦ - 
◦









event
focus







[1] K. R. Scherer: Vocal communication of emotion: A review of research

paradigms, Speech Communication, Vol. 40, pp. 227-256, 2003

S. Steidl: Vocal Emotion Recognition

7 / 49

8 / 49

Terminology (cont.)

Deﬁnition of Emotion

Emotion (Scherer)
episodes of coordinated changes in several components including at least:

neurophysiological activation,
motor expression, and
subjective feeling but possibly also
action tendencies and cognitive processes

in response to external or internal events of major signiﬁcance to the
organism

S. Steidl: Vocal Emotion Recognition

Vocal Expression of Emotion

Results from studies in Psychology of Emotion

Intensity
F0 ﬂoor/mean
F0 variability
F0 range
Sentence contour
High frequency energy
Speech and articulation rate

anger/
rage

fear/
panic

sadness

joy/

elation

boredom stress

À
À
À
À
Ä
À
À

À
À

À(Ä)1

À
À

Ä
Ä
Ä
Ä
Ä
Ä
Ä

À
À
À
À

(À)2
(À)2

Ä
Ä

Ä

À
À

1 Banse and Scherer found a decrease in F0 range
2 inconclusive evidence

Goal

Classiﬁcation of the subject’s actual emotional state
(some sort of lie detector for emotions)

S. Steidl: Vocal Emotion Recognition

9 / 49

10 / 49

Human-Computer Interaction (HCI)

Emotion-Related User States

naturally occurring states of users in human-machine
communication
emotions in a broader sense
coordinated changes in several components NOT required
classiﬁcation of the perceived emotional state,
not necessarily the actual emotion of the speaker

S. Steidl: Vocal Emotion Recognition

Pattern Recognition

Pattern Recognition Point of View

classiﬁcation task: choose 1 of n given classes
discrimination of classes rather than classiﬁcation
deﬁnition of “good” features
machine classiﬁcation

Actually not needed

deﬁnition of term emotion
information on how speciﬁc features change

S. Steidl: Vocal Emotion Recognition

Emotional Speech Corpora

Acted data

based on Basic Emotions theory
suited for studying prototypical emotions
corpora easy to create (inexpensive, no labeling process)
high audio quality
balanced classes
neutral linguistic content (focus on acoustics only)
high recognition results

S. Steidl: Vocal Emotion Recognition

Emotional Speech Corpora (cont.)

Popular corpora

11 / 49

12 / 49

Emotional Prosody Speech and Transcript corpus (LDC): 15
classes
Berlin Emotional Speech Database (EmoDB): 7 classes
89.9 % accuracy (speaker independent LOSO evaluation, speaker
adaptation, feature selection) [2]
Danish Emotional Speech Corpus: 5 classes
74.5 % accuracy (10-fold SCV, feature selection) [3]

[2] B. Vlasenko et al.: Combining Frame and Turn-Level Information for Robust

Recognition of Emotions within Speech, INTERSPEECH 2007

[3] Schuller et al.: Emotion Recognition in the Noise Applying Large Acoustic

Feature Sets, Speech Prosody 2006

S. Steidl: Vocal Emotion Recognition

Emotional Speech Corpora (cont.)

Naturally occurring emotions

13 / 49

states that actually appear in HCI (real applications)
difﬁcult to create (appropriate scenario needed, ethical concerns,
need to label data)
low emotional intensity
in general ≥ 80 % neutral
low audio quality (reverberation, noise, far-distance microphones)
needed for machine classiﬁcation (because conditions between
training and test must not differ too much)
research on both acoustic and linguistic features possible
new research questions: optimal emotion unit
almost no corpora large enough for machine classiﬁcation
available (do not exist or are not available for research)

S. Steidl: Vocal Emotion Recognition

Overview

14 / 49

1 Different Perspectives on Emotion Recognition

2 FAU Aibo Emotion Corpus

Scenario
Labeling of User States
Data-driven Dimensions of Emotion
Units of Analysis
Sparse Data Problem

3 Own Results on Emotion Classiﬁcation

4 INTERSPEECH 2009 Emotion Challenge

S. Steidl: Vocal Emotion Recognition

15 / 49

16 / 49

The FAU Aibo Emotion Corpus

51 children (30 f, 21 m) at the age of 10 to 13
8.9 hours of spontaneous speech (mainly short commands)
48,401 words in 13,642 audio ﬁles

S. Steidl: Vocal Emotion Recognition

FAU Aibo Emotion Corpus (cont.)

data base for CEICES and INTERSPEECH 2009 Emotion
Challenge

available for scientiﬁc, non-commercial use
http://www5.cs.fau.de/FAUAiboEmotionCorpus

[4] S. Steidl: Automatic Classiﬁcation of Emotion-Related User States

in Spontaneous Children’s Speech, Logos Verlag, Berlin
available online:
http://www5.cs.fau.de/en/our-team/steidl-stefan/dissertation/

S. Steidl: Vocal Emotion Recognition

Emotion-Related User States

11 categories: prior inspection of the data before labeling

17 / 49

joyful
surprised
motherese
neutral
bored
emphatic
helpless
touchy/irritated
reprimanding
angry
other

S. Steidl: Vocal Emotion Recognition

motherese
the way mothers/parents address their babies –
either because Aibo is well-behaving or because
the child wants Aibo to obey; positive equivalent to
reprimanding

emphatic
pronounced, accentuated, sometimes
hyper-articulated way but without showing any
emotion

reprimanding
the child is reproachful, reprimanding, ‘wags the
ﬁnger’

18 / 49

Labeling of User States

Labeling:

5 students of linguistics
holistic labeling on the word level
majority vote

emotion category
angry (A)
touchy (T)
reprimanding (R)
emphatic (E)
neutral (N)
motherese (M)
joyful (J)
...
all

words

134
419
463
2,807
39,975
1,311
109

0.3 %
0.9 %
1.0 %
5.8 %
82.6 %
2.7 %
0.2 %

48,401

100.0 %

S. Steidl: Vocal Emotion Recognition

19 / 49

20 / 49

Labeling of User States (cont.)

Confusion matrix

emotion category

angry (A)
touchy (T)
reprimanding (R)
emphatic (E)
neutral (N)
motherese (M)
joyful (J)

e

t

o
v

y
t
i
r
o
a
m

j

A

43.3
4.5
3.8
1.3
0.4
0.0
0.1

T

13.0
42.9
15.7
5.8
2.2
0.8
0.6

R
12.9
11.7
45.8
6.7
1.5
1.4
1.1

E

12.1
13.7
14.0
53.6
13.9
4.9
7.3

N
18.1
23.5
18.2
29.9
77.8
30.4
32.4

M
0.1
1.0
1.3
1.2
2.7
61.1
2.0

J
0.0
0.1
0.1
0.5
0.5
0.9
54.2

S. Steidl: Vocal Emotion Recognition

Data-driven Dimensions of Emotions
Non-metric dimensional scaling:

arranging the emotion categories in the 2-dimensional space
states that are often confused are close to each other

+interaction

n
o

i
t
c
a
r
e

t

n

i

−interaction

negative

motherese

reprimanding

touchy

neutral
emphatic

angry

joyful

valence

positive

S. Steidl: Vocal Emotion Recognition

21 / 49

22 / 49

Units of Analysis

Units of analysis

v1

v2

p3

s3

stopp

Aibo

g’radeaus

fein

machst

du

das

stopp sitz

word level

chunk level

turn level

Ohm_18_342

Ohm_18_343

Advantages/disadvantages of larger units

+ more information
− less emotional homogeneity

S. Steidl: Vocal Emotion Recognition

Sparse Data Problem
Super classes:

Anger: angry, touchy/irritated, reprimanding
Emphatic
Neutral
Motherese

1

0.5

0

-0.5

-1
-1.5

i

g
n
d
n
a
m

y
h
c
u
o
t

i
r
p
e
r

y
r
g
n
a

c
i
t
a
h
p
m
e

l

a
r
t
u
e
n

e
s
e
r
e
h
t
o
m

l

u
f
y
o

j

1

0.5

0

-0.5

-1

Motherese

Neutral

Anger

Emphatic

-1

-0.5

0

0.5

1

1.5

-1.5

-1

-0.5

0

0.5

1

1.5

S = 0.32 RSQ = 0.73

S = 0.19 RSQ = 0.90

S. Steidl: Vocal Emotion Recognition

Sparse Data Problem (cont.)

Data subsets

Aibo corpus

Aibo turn set

Aibo chunk set

Aibo word set

taken from

# chunks
18,216
4,543
4,543
6,413

# turns
13,642
3,996
3,996
3,996

data set

Aibo corpus
Aibo word set
Aibo chunk set
Aibo turn set

number of

words
48,401
6,070
13,217
17,618

S. Steidl: Vocal Emotion Recognition

Overview

23 / 49

24 / 49

1 Different Perspectives on Emotion Recognition

2 FAU Aibo Emotion Corpus

3 Own Results on Emotion Classiﬁcation
Results for different Units of Analysis
Machine vs. Human
Feature Types and their Relevance

4 INTERSPEECH 2009 Emotion Challenge

S. Steidl: Vocal Emotion Recognition

25 / 49

26 / 49

Most Appropriate Unit of Analysis

Classiﬁcation

complete set of features
classiﬁcation with Linear Discriminant Analysis (LDA)
51-fold speaker-independent cross-validation

unit of
analysis
word level
chunk level
turn level

number of
features
265
700
700

number of
samples
6,070 words
4,543 chunks
3,996 turns

average
recall
67.2 %
68.9 %
63.2 %

Chunks: best compromise between

length of the segment
homogeneity of the emotional state within the segment

S. Steidl: Vocal Emotion Recognition

Machine Classiﬁer vs. Human Labeler
Entropy based measure:

labeler

class

1
2
3
4

A
E
A
A

decoder:

M

2→

1

A
0.75

E

0.25

+

→

1
2

A
0.0

E
0.0

A

E

0.375 0.125

N
0.0

N
0.0

N
0.0

M
0.0

M
1.0

M
0.5

Hdec = 1.41

implicit weighting of classiﬁcation ‘errors’
depending on the word that is classiﬁed

S. Steidl: Vocal Emotion Recognition

27 / 49

28 / 49

Machine Classiﬁer vs. Human Labeler (cont.)
Classiﬁcation: Aibo word set

]

%

[

y
c
n
e
u
q
e
r
f

.
l
e
r

0.25

0.2

0.15

0.1

0.05

0

avg. human labeler
machine classiﬁer

0.2

0.4

0.6

0.8
1
entropy

1.2

1.4

1.6

[5] S. Steidl, M. Levit, A. Batliner, E. Nöth, H. Niemann:

“Of All Things the Measure is Man” – Classiﬁcation of Emotions and
Inter-Labeler Consistency, ICASSP 2005

S. Steidl: Vocal Emotion Recognition

Evaluation of Different Types of Features

Types of features

acoustic features

prosodic features
spectral features
voice quality features

linguistic features

Evaluation

Artiﬁcial Neural Networks (ANN)
51-fold speaker-independent cross-validation
combination by early or late fusion

S. Steidl: Vocal Emotion Recognition

Acoustic Features: Prosody
Prosody

suprasegmental characteristics such as

pitch contour
energy contour
temporal shortening/lengthening of words
duration of pauses between words

S. Steidl: Vocal Emotion Recognition

Acoustic Features: Prosody (cont.)
Classiﬁcation results: Aibo chunk set

29 / 49

30 / 49

]

%

[

l
l

a
c
e
r

e
g
a
r
e
v
a

80

70

60

50

40

30

20

10

0

50.6

54.4

58.5

59.0

42.0

p

a

u

s

e

s

(1

6)

F

0(2

9)

all

d

uratio

e

n

erg

y

n

(3

7)

(2

5)

S. Steidl: Vocal Emotion Recognition

Acoustic Features: Spectral Characteristics (cont.)
Classiﬁcation results: Aibo chunk set

]

%

[

l
l

a
c
e
r

e
g
a
r
e
v
a

80

70

60

50

40

30

20

10

0

59.0

58.9

48.2

pro

s

o

d

y

M

F

C

C

(2

4)

(1
0
7)

for

m

jitter/s

a

nts

hi

m

(2)

H

N

R

T

E

O

b

e
st

c

o

(6

4)

(1

6)

m

er

(4)

m

bin

atio

n

S. Steidl: Vocal Emotion Recognition

Acoustic Features: Voice Quality
Classiﬁcation results: Aibo chunk set

31 / 49

32 / 49

]

%

[

l
l

a
c
e
r

e
g
a
r
e
v
a

80

70

60

50

40

30

20

10

0

59.0

58.9

48.2

47.0

52.3

32.5

H

N

R

pro

s

o

d

y

M

F

C

C

(2

4)

(1
0
7)

for

m

jitter/s

a

nts

hi

m

(2)

T

E

O

b

e
st

(6

4)

c

o

(1

6)

m

er

(4)

m

bin

atio

n

S. Steidl: Vocal Emotion Recognition

33 / 49

34 / 49

Acoustic Features: Combination
Classiﬁcation results: Aibo chunk set

59.0

58.9

48.2

47.0

65.4

52.3

]

%

[

l
l

a
c
e
r

e
g
a
r
e
v
a

80

70

60

50

40

30

20

10

0

32.5

H

N

R

pro

s

o

d

y

M

F

C

C

(2

4)

(1
0
7)

for

m

jitter/s

a

nts

hi

m

(2)

T

E

O

b

e
st

(6

4)

c

o

(1

6)

m

er

(4)

m

bin

atio

n

S. Steidl: Vocal Emotion Recognition

Linguistic Features

Types of linguistic features

word characteristics

average word length (number of letters, phonemes, syllables)
proportion of word fragments
average number of repetitions

part-of-speech features
unigram models
bag-of-words

S. Steidl: Vocal Emotion Recognition

Linguistic Features (cont.)
Part-of-Speech (POS) Features

only 6 coarse POS categories
can be annotated without considering context

l

a
t
o
t

f
o
%

A

n

g
er

E

m

N

M

e

utral

p

h

atic

J

o

yful

-

O

th

er

oth

ere

s

e

S. Steidl: Vocal Emotion Recognition

Linguistic Features (cont.)

Unigram Models

35 / 49

nouns, proper names

inﬂected adjectives
not inﬂected adjectives
present/past participles
(other) verbs, inﬁnitives

auxiliaries

articles, pronouns,
particles, interjections

36 / 49

u(w, e) = log10

P(e|w)
P(e)

Anger
böser (bad)
stehenbleiben (stop)
nein (no)
aufstehen (get up)
Aibo (Aibo)

Neutral
okay (okay)
und (and)
Stück (bit)
in (in)
noch (still)

P(A|w)
29.2 %
18.9 %
17.0 %
12.3 %
10.1 %
P(N|w)
98.6 %
98.5 %
98.5 %
98.2 %
96.2 %

Emphatic
stopp (stop)
halt (halt)
links (left)
rechts (right)
nein (no)

Motherese
fein (ﬁne)
ganz (very)
braver (good)
sehr (very)
brav (good)

P(E|w)
30.5 %
29.3 %
20.5 %
18.9 %
17.6 %
P(M|w)
57.5 %
41.9 %
36.0 %
23.5 %
21.7 %

S. Steidl: Vocal Emotion Recognition

Linguistic Features (cont.)

Bag-of-Words

utterance: Aibo, geh nach links! (Aibo, move to the left!)

. . .

1
4

0

0

. . .

1
4

. . .

1
4

. . .

1
4

. . .

Aibo

allen

geh

nach

links

Aibolein

representation of the linguistic content
word order getting lost
various dimensionality reduction techniques

S. Steidl: Vocal Emotion Recognition

Linguistic Features (cont.)
Classiﬁcation results: Aibo chunk set

37 / 49

38 / 49

61.9

61.9

62.2

54.3

56.1

]

%

[

l
l

a
c
e
r

e
g
a
r
e
v
a

80

70

60

50

40

30

20

10

0

w

ord

P

O

S

(6)

u

nigra

m

m

B

O

W

(2
5

4

b

e
st

c

o

m

statistic

s

(6)

S. Steidl: Vocal Emotion Recognition

o

d

els

(1

6)

→

5
0)

bin

atio

n

Combination of Acoustic and Linguistic Features
Classiﬁcation results: Aibo chunk set

39 / 49

65.4

62.2

67.1

68.9

]

%

[

l
l

a
c
e
r
e
g
a
r
e
v
a

80

70

60

50

40

30

20

10

0

b

a

c

(late
o
fu

u

e
st
stic
n,

sio

lin
(late
g
fu

b

e
st
uistic
sio
n,

A

atio

s

n

c

m

o
fe
bin
ature

N

A

N
)

c

m

o
fe
bin
ature

N

N
)

atio

s

n

sio

atio

n,

n

A

N

N
)

o

c

(late
fu

m

bin

c

(e
arly

o

m

fu

bin
sio

atio
n,

n

L

D

A
)

40 / 49

S. Steidl: Vocal Emotion Recognition

Similar Results within CEICES

CEICES: Combining Efforts for Improving Automatic Classiﬁcation of Emotional
User States

collaboration of various research groups within the European
Network of Excellence HUMAINE (2004-2007)
state-of-the-art feature set with ≥ 4,000 features

SVM (linear kernel), 3-fold speaker-independent cross-validation
selection of 150 features (SFFS): surviving feature types?
only chunk based features, no information outside Aibo chunk set

[6] A. Batliner, S. Steidl, B. Schuller, D. Seppi, T. Vogt, J. Wagner, L. Devillers, L.

Vidrascu, V. Aharonson, L. Kessous, N. Amir:
Whodunnit – Searching for the Most Important Feature Types Signalling
Emotion-Related User States in Speech,
Computer, Speech, and Language, Vol. 25, Issue 1 (January 2011), pp. 4-28

S. Steidl: Vocal Emotion Recognition

Similar Results within CEICES(cont.)

41 / 49

n
o

i
t

a
r
u
d

y
g
r
e
n
e

0
F

m
u
r
t
c
e
p
s

m
u
r
t
s
p
e
c

y
t
i
l

a
u
q

e
c
o
v

i

s
t

l

e
e
v
a
w

c

i
t
s
u
o
c
a

l
l

a

W
O
B

S
O
P

s
c
i
t

n
a
m
e
s

r
e
h
g
h

i

391
10

265
32

333
16

656 1699
15
16
49.6 56.3 46.8 46.2
6.7 21.3 10.7 10.0
2.3
2.6 12.1

216
5
46.4 38.7 35.3
10.7
3.4
2.3
1.0

153
7

4.7
4.6

4.8

3713
101

476
25

31
7
– 37.4 48.1
4.7

12
17
56.0
11.3
5.3 22.6 141.7

67.3 16.7
2.7

a
i
r
a
v

12
0
–
0.0
0.0

i

c
i
t
s
u
g
n

i
l

l
l

a

l
l

a

531
49
–

4244
150
65.5
32.7 100.0
3.5
9.6

23

33

28

17
54.9 56.9 46.7 49.9
18.7 22.0 15.3 11.3
7.2 12.5
2.6

6.9

11

23

15
50.4 41.5 44.9
15.3
1.4

150
27
63.4 53.2 54.9
7.3 10.0 100.0 62.7 18.0
7.2

6.9

94

4.0 19.7 87.1 225.0 16.7

27
57.9
18.0

2
–

150
62.6
0.1 100.0
28.2

# total
#
F MEASURE
SHARE
PORTION

#
F MEASURE
SHARE
PORTION

S
F
F
S

S
F
F
S

S. Steidl: Vocal Emotion Recognition

Overview

42 / 49

1 Different Perspectives on Emotion Recognition

2 FAU Aibo Emotion Corpus

3 Own Results on Emotion Classiﬁcation

4 INTERSPEECH 2009 Emotion Challenge

S. Steidl: Vocal Emotion Recognition

43 / 49

44 / 49

INTERSPEECH 2009 Emotion Challenge

New goals:

challenge with standardized test conditions

open microphone: using the complete corpus

highly unbalanced classes
including all observed emotional categories
including chunks with low inter-labeler agreement

S. Steidl: Vocal Emotion Recognition

INTERSPEECH 2009 Emotion Challenge (cont.)

Speaker independent training and test sets

2-class problem: NEGative vs. IDLe

#
train
test

(cid:80)

NEG
3 358
2 465
5 823

IDL
6 601
5 792
12 393

(cid:80)

9 959
8 257
18 216

5-class problem: Anger, Emphatic, Neutral, Positive, Rest

#
train
test

(cid:80)

A
881
611
1 492

E

2 093
1 508
3 601

N
5 590
5 377
10 967

P
674
215
889

R
721
546
1 267

S. Steidl: Vocal Emotion Recognition

(cid:80)

9 959
8 257
18 216

INTERSPEECH 2009 Emotion Challenge (cont.)

Sub-Challenges

1 Feature Sub-Challenge

optimisation of feature extraction/selection;
classiﬁer settings ﬁxed

2 Classiﬁer Sub-Challenge

optimisation of classiﬁcation techniques;
feature set given

3 Open Performance Sub-Challenge

optimisation of feature extraction/selection and
classiﬁcation techniques

S. Steidl: Vocal Emotion Recognition

45 / 49

46 / 49

INTERSPEECH 2009 Emotion Challenge (cont.)

Participants

Open Performance

Sub-Challenge

Classiﬁer

Sub-Challenge

Feature

Sub-Challenge

2 classes 5 classes

2 classes 5 classes

2 classes 5 classes

number of
participants



–
–
–
–


–
–
–
–
–

–
–

–
–
–

–
–



–

–
–
–
–



–
–
–
–



7
2
2
1
1
1

[7] B. Schuller, A. Batliner, S. Steidl, D. Seppi:

Recognising Realistic Emotions and Affect in Speech: State of the Art and
Lessons Learnt from the First Challenge, Speech Communication, Special Issue
Sensing Emotion and Affect - Facing Realism in Speech Processing, to appear

S. Steidl: Vocal Emotion Recognition

47 / 49

48 / 49

INTERSPEECH 2009 Emotion Challenge (cont.)
2-class problem: NEGative vs. IDLe

71.2

70.3

unweighted avg. recall
weighted avg. recall

67.7

67.1

67.2

66.4

67.9

67.6

69.2

68.3

]

%

[

l
l

a
c
e
r

e
g
a
r
e
v
a

74

72

70

68

66

64

62

60

B

V

B

a

s

o
gt

elin

e

arraet

al.

C

L

u

P

B

K

e

n

olz

o

o

z

k

c

k

Vla
s

g

o

e
hl

hic

et

et

et

ote

al.

al.

urt
al.

D

M

u

m

ajority

c

o

u

e

n

m

a

n

k

o

n

et

et
al.

al.

h
el

et

v

otin
al.

g

et

al.

S. Steidl: Vocal Emotion Recognition

INTERSPEECH 2009 Emotion Challenge (cont.)

5-class problem: Anger, Emphatic, Neutral, Positive, Rest

]

%

[

l
l

a
c
e
r

e
g
a
r
e
v
a

55

50

45

40

35

44.0

41.2 41.4 41.4 41.6 41.6 41.7

39.4 39.4

38.2

38.2

unweighted average recall
weighted average recall

B

B

V

D

a

s

o
gt

arraelin


C

e

L

B

K

M

e

e

o

z

u

m

Pla

L

u

Vla
s

e

n

o

u

c

n
et
h
el

el
hic

al.
ote

g

o

e

n

et

k

o

et

et

al.

et

al.

k
al.

urt

o

c

k

m

a

ajority
al.

et

n

n

et

al.

et

al.

v

otin
al.

g

et

al.

S. Steidl: Vocal Emotion Recognition

State-of-the-Art: Summary

49 / 49

Berlin Emotion Speech Database

7-class problem: hot anger, disgust, fear/panic, happiness,
sadness/sorrow, boredom, neutral
balanced classes
 90 % accuracy

FAU Aibo Emotion Corpus

4-class problem: Anger, Emphatic, Neutral, Motherese
subset with roughly balanced classes (Aibo chunk set)
 69 % unweighted average recall
5-class problem: Anger, Emphatic, Neutral, Positive, Rest
highly unbalanced classes, complete corpus
 44 % unweighted average recall
2-class problem: NEGative vs. IDLe
highly unbalanced classes, complete corpus
 71 % unweighted average recall

S. Steidl: Vocal Emotion Recognition

