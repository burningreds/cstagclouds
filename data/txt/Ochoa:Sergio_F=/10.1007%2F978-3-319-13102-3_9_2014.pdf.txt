Towards a Non-intrusive Self-management

System for Asthma Control Using Smartphones

Iv´an Gonz´alez1, Cristian Carret´on1, Sergio F. Ochoa2, and Jos´e Bravo1

1 MAmI Research Lab, University of Castilla-La Mancha Paseo de la Universidad,

Ciudad Real, Spain

ivan.gzdiaz@gmail.com, cristian.carreton@alu.uclm.es, Jose.Bravo@uclm.es

2 Computer Science Department, University of Chile, Santiago de Chile, Chile

sochoa@dcc.uchile.cl

Abstract. A noise-robust algorithm for segmentation of breath events
during continuous speech is presented. The built-in microphone of a
smartphone is used to capture the speech signal (voiced and breath
frames) under conditions of a relatively noisy background. A template
matching approach, using mel-cepstrograms, is adopted for constructing
several similarity measurements to distinguish between breath and nonbreath 
frames. Breath events will be used for lung function regression.

Keywords: asthma control, breath detection, MFCC, mel-cepstrogram,
lung function.

1

Introduction

Asthma control refers to a set of procedures to describe the extent to which external 
symptoms of respiratory chronic condition (asthma severity) are minimized
through preventive medical intervention and personal care of asthma.

In the last two decades there have been numerous advances that have allowed
a better understanding of asthma by improving the methods of diagnosis and
the eﬀectiveness of the medication to prevent exacerbations. However, studies
have been less focused in asthma control. In this sense, they have materialized in
the development of management guidelines e.g. [1]. These asthma management
guidelines emphasize the nature of the disease, which is subject to numerous predisposition 
factors, triggers and heterogeneous symptomatology. Therefore, the
evaluation of the extent of asthma severity (and asthma control consequently) is
considered a complex and multidimensional task. In order to give some instrumental 
support for self-management asthma control, portable spirometers have
become the most widely used tool, providing daily and objective measurements
of lung function like: FEV1 (Forced Expiratory Volume in 1 sec), FVC (Forced
Vital Capacity), ... ; and also holding and history of recent past records for the
patient which is so useful to analyse asthma severity state and progression.

However, portable spirometers have some shortcomings that impede its popular 
implantation as a support tool
for self-management asthma control.
For example, they require manual steps for setting and some sort of training

R. Herv´as et al. (Eds.): UCAmI 2014, LNCS 8867, pp. 44–47, 2014.
c(cid:2) Springer International Publishing Switzerland 2014

Non-intrusive Self-management System for Asthma Control

45

in order to get accurate measurements and these are serious inconvenients for
some patients (e.g. elderly or children). In addition, forced spirometry requires
explicit interaction in an invasive way, since a long exhalation is needed. In order
to deal with the disadvantages of forced spirometry and its dedicated devices,
some active areas of research like m-Health [2] have plenty of opportunities. The
main goal of this area is health monitoring without intrusion. That is, receive
feedback about vital signs and process it, trying to achieve minimal interaction
and reducing embedded sensors everything possible.

In this paper we present an ongoing study in the context of m-Health which
is part of a bigger framework for asthma control. Here we propose a noise-robust
algorithm for segmentation of breath events (inhalations and exhalations) during
continuous speech through the built-in microphone, while the smartphone is in
conversation position (speaker near the ear and microphone near the mouth).
Once breath sounds are exactly delimited, next step will be extract some features
from them in order to build a regression model to estimate some lung functions.

2 Previous Work

Before adopting the noise-robust segmentation of breath events as our development 
line, we tried to use the microphone to capture the acoustic signal of
a forced expiration, with the intention of modelling the patient’s vocal tract
and the reverberation of sound around his head and ﬁnally estimate some lung
functions using regression. In other words, make a transformation of audio samples 
of a forced expiration, captured through the microphone like uncalibrated
signals of pressure, into measures of airﬂow which are useful to construct a regression 
model of some lung functions. This procedure was founded on SpiroSmart
app [3]. The ﬁrst phase of this previous approach was to calculate the spectrogram 
from the acoustic signal of the forced expiration (Fig. 1a). After it, two
diﬀerent ﬁlters were applied in order to discard low energy frequencies in each
FFT frame (ﬁlter over columns, Fig. 1b) and to preserve only relative large resonances 
(ﬁlter over rows, Fig. 1c). After spectrogram ﬁltration, ﬁnally the average
resonance magnitude in each FFT frame was calculated to be used for feature
extraction of ﬂow rate over time. Each of these features would become a input
for the lung function regression model. Although SpiroSmart’s authors achieved
good results with this approach, we gave up that research line as part of our

(a)

(b)

(c)

Fig. 1. (a) Original spectrogram of a forced expiration; (b) After discarding low energy
frequencies (black&green points); (c) After removing short noise resonances (< 300ms)

46

I. Gonz´alez et al.

framework for asthma control. The main reason was that the spectrogram was
strongly inﬂuenced for air blowing distance, saturating the microphone (if the
blow is too close); and losing important energy in some frequencies (if the blow
is far away). Also, it was to sensitive to background noise.

3 Case of Use: Noise-Robust Segmentation Algorithm

Here we show a general view of the algorithm developed for the exactly demarcation 
of breath sounds during continuous speech in a relatively noisy environment.
The outcome of this algorithm (in terms of delimited acoustic frames which are
part of inhalations or exhalations time events) will feed a feature extraction
algorithm as previous step for lung function regression.

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)
(cid:7)(cid:8)(cid:4)(cid:9)(cid:10)(cid:11)(cid:3)
(cid:12)(cid:3)(cid:5)

(cid:13)(cid:3)(cid:14)(cid:15)(cid:5)(cid:6)(cid:16)(cid:17)(cid:18)(cid:19)(cid:20)(cid:12)(cid:5)(cid:3)(cid:9)(cid:3)(cid:14)(cid:5)

(cid:21)(cid:22)(cid:16)(cid:23)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:16)(cid:22)(cid:2)(cid:4)(cid:9)(cid:3)(cid:12)

(cid:24)(cid:25)(cid:26)(cid:27)(cid:27)(cid:9)(cid:12)(cid:28)(cid:16)

(cid:29)(cid:2)(cid:3)(cid:30)(cid:3)(cid:9)(cid:10)(cid:6)(cid:4)(cid:12)(cid:31)(cid:12)(cid:16)(cid:22)(cid:31)(cid:11)(cid:5)(cid:3)(cid:2)

(cid:34)(cid:31)(cid:14)(cid:18)(cid:21)(cid:35)(cid:3)(cid:18)(cid:16)(cid:36)(cid:16)(cid:22)(cid:2)(cid:4)(cid:9)(cid:9)(cid:31)(cid:14)(cid:15)

(cid:21)(cid:22)(cid:16)(cid:6)(cid:31)(cid:15)(cid:6)(cid:3)(cid:2)(cid:16)(cid:22)(cid:2)(cid:3)(cid:32)(cid:20)(cid:3)(cid:14)(cid:33)(cid:31)(cid:3)(cid:12)

(cid:37)(cid:14)(cid:5)(cid:21)(cid:16)(cid:12)(cid:20)(cid:23)(cid:22)(cid:2)(cid:4)(cid:9)(cid:3)(cid:12)

(cid:38)(cid:3)(cid:11)(cid:30)(cid:33)(cid:3)(cid:10)(cid:12)(cid:5)(cid:2)(cid:21)(cid:15)(cid:2)(cid:4)(cid:9)

(cid:33)(cid:4)(cid:11)(cid:33)(cid:20)(cid:11)(cid:4)(cid:5)(cid:31)(cid:21)(cid:14)(cid:16)(cid:22)(cid:21)(cid:2)(cid:16)(cid:3)(cid:4)(cid:33)(cid:6)

(cid:23)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:16)(cid:22)(cid:2)(cid:4)(cid:9)(cid:3)

(cid:39)(cid:40)(cid:16)(cid:33)(cid:21)(cid:9)(cid:10)(cid:21)(cid:14)(cid:3)(cid:14)(cid:5)

(cid:41)(cid:3)(cid:9)(cid:21)(cid:42)(cid:4)(cid:11)(cid:16)(cid:24)(cid:27)(cid:5)(cid:6)(cid:16)(cid:38)(cid:43)(cid:40)(cid:40)(cid:28)

(cid:17)(cid:42)(cid:3)(cid:2)(cid:4)(cid:15)(cid:3)(cid:16)(cid:21)(cid:22)(cid:16)(cid:4)(cid:11)(cid:11)(cid:16)(cid:23)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)

(cid:38)(cid:3)(cid:11)(cid:30)(cid:33)(cid:3)(cid:10)(cid:12)(cid:5)(cid:2)(cid:21)(cid:15)(cid:2)(cid:4)(cid:9)(cid:12)

(cid:44)(cid:4)(cid:2)(cid:31)(cid:4)(cid:14)(cid:33)(cid:3)(cid:16)(cid:21)(cid:22)(cid:16)(cid:4)(cid:11)(cid:11)(cid:16)(cid:23)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)

(cid:38)(cid:3)(cid:11)(cid:30)(cid:33)(cid:3)(cid:10)(cid:12)(cid:5)(cid:2)(cid:21)(cid:15)(cid:2)(cid:4)(cid:9)(cid:12)

(cid:1)(cid:4)(cid:33)(cid:45)(cid:15)(cid:2)(cid:21)(cid:20)(cid:14)(cid:18)(cid:16)(cid:14)(cid:21)(cid:31)(cid:12)(cid:3)(cid:16)(cid:31)(cid:14)(cid:22)(cid:11)(cid:20)(cid:3)(cid:14)(cid:33)(cid:3)(cid:16)(cid:24)(cid:4)(cid:11)(cid:10)(cid:6)(cid:4)(cid:16)(cid:25)(cid:26)(cid:28)(cid:46)

(cid:4)(cid:11)(cid:10)(cid:6)(cid:4)(cid:47)(cid:24)(cid:17)(cid:42)(cid:3)(cid:2)(cid:4)(cid:15)(cid:3)(cid:48)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:12)(cid:48)(cid:38)(cid:3)(cid:11)(cid:33)(cid:3)(cid:10)(cid:12)(cid:5)(cid:2)(cid:21)(cid:15)(cid:2)(cid:4)(cid:9)(cid:12)(cid:28)(cid:49)(cid:24)(cid:26)(cid:16)(cid:50)(cid:16)(cid:4)(cid:11)(cid:10)(cid:6)(cid:4)(cid:28)(cid:47)(cid:51)(cid:21)(cid:31)(cid:12)(cid:3)(cid:48)(cid:52)(cid:3)(cid:9)(cid:10)(cid:11)(cid:4)(cid:5)(cid:3)(cid:48)(cid:38)(cid:4)(cid:5)(cid:2)(cid:31)(cid:8)(cid:28)

(cid:1)(cid:4)(cid:33)(cid:45)(cid:15)(cid:2)(cid:21)(cid:20)(cid:14)(cid:18)
(cid:51)(cid:21)(cid:31)(cid:12)(cid:3)
(cid:3)(cid:8)(cid:4)(cid:9)(cid:10)(cid:3)(cid:16)(cid:12)(cid:3)(cid:5)
(cid:24)(cid:22)(cid:31)(cid:2)(cid:12)(cid:5)(cid:16)
(cid:25)(cid:26)(cid:27)(cid:27)(cid:9)(cid:12)
(cid:2)(cid:3)(cid:33)(cid:21)(cid:2)(cid:18)(cid:31)(cid:14)(cid:15)(cid:28)

(cid:29)(cid:2)(cid:3)(cid:30)(cid:3)(cid:9)(cid:10)(cid:6)(cid:4)(cid:12)(cid:31)(cid:12)(cid:16)(cid:22)(cid:31)(cid:11)(cid:5)(cid:3)(cid:2)

(cid:21)(cid:22)(cid:16)(cid:6)(cid:31)(cid:15)(cid:6)(cid:3)(cid:2)(cid:16)(cid:22)(cid:2)(cid:3)(cid:32)(cid:20)(cid:3)(cid:14)(cid:33)(cid:31)(cid:3)(cid:12)

(cid:34)(cid:31)(cid:14)(cid:18)(cid:21)(cid:35)(cid:3)(cid:18)(cid:16)(cid:36)(cid:16)(cid:22)(cid:2)(cid:4)(cid:9)(cid:9)(cid:31)(cid:14)(cid:15)

(cid:37)(cid:14)(cid:5)(cid:21)(cid:16)(cid:12)(cid:20)(cid:23)(cid:22)(cid:2)(cid:4)(cid:9)(cid:3)(cid:12)

(cid:38)(cid:3)(cid:11)(cid:30)(cid:33)(cid:3)(cid:10)(cid:12)(cid:5)(cid:2)(cid:21)(cid:15)(cid:2)(cid:4)(cid:9)
(cid:33)(cid:4)(cid:11)(cid:33)(cid:20)(cid:11)(cid:4)(cid:5)(cid:31)(cid:21)(cid:14)(cid:16)(cid:22)(cid:21)(cid:2)(cid:16)(cid:5)(cid:6)(cid:3)

(cid:23)(cid:4)(cid:33)(cid:45)(cid:15)(cid:2)(cid:21)(cid:20)(cid:14)(cid:18)(cid:16)(cid:14)(cid:21)(cid:31)(cid:12)(cid:3)(cid:16)(cid:3)(cid:8)(cid:4)(cid:9)(cid:10)(cid:11)(cid:3)(cid:16)(cid:12)(cid:3)(cid:5)

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:16)(cid:52)(cid:3)(cid:9)(cid:10)(cid:11)(cid:4)(cid:5)(cid:3)(cid:16)(cid:38)(cid:4)(cid:5)(cid:2)(cid:31)(cid:8)

(cid:39)(cid:40)(cid:16)(cid:33)(cid:21)(cid:9)(cid:10)(cid:21)(cid:14)(cid:3)(cid:14)(cid:5)

(cid:41)(cid:3)(cid:9)(cid:21)(cid:42)(cid:4)(cid:11)(cid:16)(cid:24)(cid:27)(cid:5)(cid:6)(cid:16)(cid:38)(cid:43)(cid:40)(cid:40)(cid:28)

(cid:1)(cid:4)(cid:33)(cid:45)(cid:15)(cid:2)(cid:21)(cid:20)(cid:14)(cid:18)(cid:16)(cid:14)(cid:21)(cid:31)(cid:12)(cid:3)(cid:16)(cid:20)(cid:10)(cid:18)(cid:4)(cid:5)(cid:3)(cid:16)(cid:24)(cid:4)(cid:11)(cid:10)(cid:6)(cid:4)(cid:16)(cid:25)(cid:26)(cid:28)(cid:46)

(cid:4)(cid:11)(cid:10)(cid:6)(cid:4)(cid:16)(cid:47)(cid:16)(cid:24)(cid:1)(cid:4)(cid:33)(cid:45)(cid:15)(cid:2)(cid:21)(cid:20)(cid:14)(cid:18)(cid:48)(cid:51)(cid:21)(cid:31)(cid:12)(cid:3)(cid:48)(cid:38)(cid:3)(cid:11)(cid:33)(cid:3)(cid:10)(cid:12)(cid:5)(cid:2)(cid:21)(cid:15)(cid:2)(cid:4)(cid:9)(cid:28)(cid:16)(cid:49)(cid:16)(cid:24)(cid:26)(cid:16)(cid:50)(cid:16)(cid:4)(cid:11)(cid:10)(cid:6)(cid:4)(cid:28)(cid:16)(cid:47)(cid:16)(cid:24)(cid:17)(cid:33)(cid:5)(cid:20)(cid:4)(cid:11)(cid:48)(cid:43)(cid:2)(cid:4)(cid:9)(cid:3)(cid:48)(cid:38)(cid:3)(cid:11)(cid:48)(cid:33)(cid:3)(cid:10)(cid:12)(cid:5)(cid:2)(cid:21)(cid:15)(cid:2)(cid:4)(cid:9)(cid:48)(cid:31)(cid:28)

(cid:51)(cid:21)(cid:31)(cid:12)(cid:3)(cid:16)(cid:52)(cid:3)(cid:9)(cid:10)(cid:11)(cid:4)(cid:5)(cid:3)(cid:16)(cid:38)(cid:4)(cid:5)(cid:2)(cid:31)(cid:8)

Fig. 2. Template matrices for: breath example set (top) & noise example (bottom)

In order to be able to diﬀerentiate between breath and non-breath frames
a template matching approach, focused on Mel-cepstrogram calculation like
the proposed in [4], has been used. In addition, we have extended this template 
matching procedure to distinguish between weak breath frames and only
background noise frames, due to their similarity. To address this last task, the
template matching procedure is also used as similarity measurement and again
MFCC extraction is needed to build a Mel-cepstrogram of the ﬁrst 100ms of
the recording (which are representative of background noise). This method has
some steps in common with the MFCC Voice Activity Detection (VAD) algorithm 
in [5]. Fig. 2 show a schematic block view describing the construction of
the template matrices which will be used like similarity measurements for the
breath example set (top) and for the background noise example (bottom) respectively.
 Both template matrices share several calculation steps. After selection of

Non-intrusive Self-management System for Asthma Control

47

some representative isolated frames for breath set and ﬁrst 100ms of the recording 
for the background noise example, pre-emphasis ﬁlter is used, then Hamming
windowing and framing into subframes (10ms) are obtained as previous step for
Mel-cepstrogram calculation of each frame and DC removal. Finally, each template 
matrix is updated under background noise inﬂuence using a formula and
alpha factor as in [5]. An actual frame can be classiﬁed as breath/non-breath using 
a similarity coeﬃcient Sc formed by its Mel-cepstrogram M (i), the variance
matrix of breath set V and the breath template matrix T :

Sc =

(cid:2)

(cid:2)

rows

1

columns | M(i)−T

V

; Lower Sc =⇒ less similarity

(1)

|2

Other time domain features like: short-time energy and ZCR can be added to
improve the breath/non-breath classiﬁcation. For frames already classiﬁed as
breath, a second similarity coeﬃcient (like the ﬁrst one but using the noise
template matrix instead) must be used to discard some false positives considered
as breath frames, but in fact they are background noise frames.

4 Conclusion

The algorithm proposed is giving us good results for breath segmentation in
relatively noisy environment with very few false negatives. However, the false
positive rate (in silence/noisy environment) must be reduced a little more, for
example, taking into account what is happening around the detected breath
events and not considering them like independent frames. Once this is done, we
will work in the feature extraction from breath events.

Acknowledgment. This work is conducted in the context of the FRASE MINECO
project (TIN2013-47152-C3-1-R). Also, we appreciate the support of UBIHEALTH
project under International Research Staﬀ Exchange Schema (MC-IRSES 316337).

References

1. The global initiative for asthma (GINA), http://www.ginasthma.org/
2. Bravo, J.: m-Health: Mobile Computing and Health Monitoring. J. of Computation

in Biosciences and Engineering

3. Larson, E.C., Goel, M., Boriello, G., Helthe, S., Rosenfeld, M., Patel, S.N.: SpiroS-
mart: Using a microphone to measure lung function on a mobile phone. In: UbiComp,
pp. 280–289 (2012)

4. Ruinskiy, D., Lavner, Y.: An Eﬀective Algorithm for Automatic Detection and Exact
Demarcation of Breath Sounds in Speech and Song Signals. IEEE Transactions on
Audio, Speech, and Language Processing 15(3), 838–850 (2007)

5. Hongzhi, W., Yuchao, X., Meijing, L.: Study on the MFCC similarity-based voice

activity detection algorithm. In: AIMSEC, pp. 4391–4394 (2011)

