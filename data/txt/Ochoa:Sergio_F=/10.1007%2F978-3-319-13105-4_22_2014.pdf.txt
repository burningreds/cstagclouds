 

Monitoring Moods in Elderly People  

through Voice Processing 

Víctor Rojas1, Sergio F. Ochoa1, and Ramón Hervás2 

1 Computer Science Department, Universidad de Chile 
Av. Blanco Encalada 2120, 3rd Floor, Santiago, Chile 

{vrojas,sochoa}@dcc.uchile.cl 

2 Castilla-La Mancha University 

Ciudad Real, Spain 

ramon.hlucas@uclm.es 

Abstract. Depression is a mental illness that is difficult to diagnose and treat. This 
mental  disorder  affects  many  older  adults  due  several  reasons,  for  instance  because 
of their physical limitations and the natural reduction of their social circle. 
This  article  presents  a  system  for  monitoring  the  mood  of  the  elderly  through 
voice processing. The system is particularly focused on detecting sadness, which 
allows caregivers of family members to react on-time in supporting the person in 
need. The sadness recognition is done by classifying emotions in groups, according 
to the Circumflex Model of Affect. After evaluating the system using several 
emotion databases, the obtained results indicate that this solution is able to recognize 
94% of the cases in men and 79% in women. This solution can be embedded 
in ubiquitous systems that monitor the mood of people in several scenarios. 

Keywords:  Emotion  recognition,  social 
processing, emotion monitoring, gender recognition. 

isolation,  older  adults,  voice 

1 

Introduction 

When people become older, physical and emotional problems affect them more and 
more. One of those problems is the depression, which can range from temporary episodes 
 of  sadness,  to  severe  persistent  depression.  There  are  many  factors  that  can 
cause  a  depressive  status  in  elderly  people,  for  instance  the  physical  and  cognitive 
limitations, emotional fragility and social isolation. Social isolation is affecting many 
older  adults.  The  results  of  a  recent  study  performed  on  3858  random  elders  (>  75 
years-old) indicate that 77% of them suffer social isolation, social inactivity or loneliness 
[1]. These psychological conditions make elders highly prone to suffer emotional 
disruptions or diseases, like the depression [2].  

Frequently the relatives of elderly people suffering depression do not react because 
they  are  not  aware  of  the  real  condition  of  an  older  adult.  Detecting  symptoms  of 
depression and making family members aware of it must be a priority, as a first step 
to  treat  this  mental  illness.  There  is  strong  evidence  indicating  that  the  emotional 
condition  of  older  adults  suffering  from  depression  improves,  when  they  perceive 

L. Pecchia et al. (Eds.): IWAAL 2014, LNCS 8868, pp. 139–146, 2014. 
© Springer International Publishing Switzerland 2014 

140 

V. Rojas, S.F. Ochoa, and R. Hervás 

support  from  their  social  networks;  e.g.  from  their  relatives  [3].  Therefore,  making 
family members (or other supporting people) aware of these situations could contribute 
to address this problem. 

The voice is an important instrument to get information about the emotional condition 
of a person [4]. Taking  advantage of this  situation,  we developed a monitoring 
system able to detect negative emotions in elders through voice processing. The system 
runs, as a background process in a social application named SocialConnector [5], 
that  elders  have  installed  in  their  houses  for  interacting  with  other  family  members 
using (synchronous and asynchronous) voice messages. The SocialConnector runs on 
a slate that is fixed to a wall of a room where elders usually stay; for instance at the 
living room. The system that recognizes the negative emotions transforms the slate in 
a sensor able to identify when these people need external support. In these cases, the 
system delivers messages to caregivers or family members making them aware of this 
situation. The elderly people are not conscious of the sensing process or the actions 
taken by the system. This solution can be embedded in many other ubiquitous applications 
with similar purposes. 

Next section reports the related work. Section 3 presents the strategy proposed to 
recognize negative emotions (particularly sadness) in elderly people. Section 4 shows 
the architecture of the monitoring system, which implements the strategy described in 
section 3. Section 5 presents the conclusions and further work. 

2 

Related Work 

Dickerson et al. [6] created a real-time depression monitoring system for homes. The 
system involves several wireless sensors and devices that are used by the monitored 
people.  In  order  to  do  a  tracking  of  depression  symptoms,  the  authors  analyze  the 
sleep,  weight, speech and other factors. The results indicate that in speech analysis, 
the fundamental frequency and speech pause times are variables, which help predict 
affect in the voice. 

Trying to detect mood in speech, Alghowinem et al. [7] recorded voice from several 
subjects  with depression. The collected data was classified using Hidden Markov 
Models  in  order  to  evaluate  different  audio  features  in  the  mood  detection  process. 
The analysis of results indicated that the Mel-frequency cepstral coefficients, energy 
and intensity features allow high mood recognition rates when male and female audio 
samples are analyzed together. 

The  use  of  classification  methods  for  detecting  depression  or  mood  using  audio 
features is quite common. Several frameworks and tools are available to extract these 
audio features, for instance openSMILE [8] and Yaafe [9]. The first one is a toolbox 
developed specially for extracting audio features from voice and then processing this 
information in batch. This includes the use of audio descriptors (i.e. audio features) 
that allow extracting features from the voice, such as emotions, age range or sleepiness.
 The results can be returned in different formats; even in formats that eases the 
use of the support vector machine (SVM) classification technique [10]. 

Concerning Yaafe [9], it is focused only in the audio features extraction, and it can 
use any audio sample as input, including music and voice. The results are represented 
in CSV format, making easier the post processing of such information.  

 

 

3 

Monitoring Moods in Elderly People through Voice Processing 

141 

Emotions Recognition Strategy 

Following the guidance of previous works in the area [4, 11], we used the SVM method 
to classify the emotions inferred from audio records. We utilized openSMILE for 
retrieving  the  audio  features,  and  the  LibSVM  library  [12]  for  the  SVM  modelling. 
The  audio  samples  for  evaluating  the  emotion  detection  proposal  were  taken  from 
three emotion databases: SAVEE, Emo-DB and RekEmozio. SAVEE (Surrey AudioVisual 
Expressed Emotion) [13] includes emotions in audio and video samples. The 
people in the samples use English language and the database includes seven emotions: 
anger, disgust, fear, happiness, neutral, sadness and surprise. 

Concerning Emo-DB (Berlin Database of Emotional Speech) [14], it contains only 
 audio  samples  in  German  language.  The  emotions  included  in  the  samples  are: 
anger, boredom, disgust, fear, happiness, neutral and sadness. In case of RekEmozio 
[15], this database was built with samples in Basque and Spanish language. It includes 
audio and video samples, and the emotions considered in this database are the same as 
in the SAVEE database. All of the audio samples contained in these databases were 
recorded by professional actors, and all of them are adult people. 

3.1 

Strategies for Emotion Classification 

In  this  section  we  describe  the  strategies  used  for  classifying  emotions  and  the  obtained 
results. 

3.1.1.   Individual classification of emotions  
The first step  was to classify  the  following seven emotions from  the previously  mentioned 
databases: anger, disgust, fear, happiness, neutral, sadness and surprise. For each 
emotion and database the samples  were divided in two  groups:  training and  test. The 
number of samples in both groups was similar. The tests were applied for every database 
in separated way. Two kinds of kernels were used in these tests, linear and polynomial,
 with different values for Gamma (G) and C parameters. For both kernels emobase2010 
 was  chosen  as  the  audio  descriptor.  This  descriptor  comes  in  openSMILE 
toolbox [16], and following the guidance of such an initiative we used degree 2 and 3 
for a polynomial kernel, and different values for G and C: (G=1, C=1), (G=1, C=1000), 
(G=1000, C=1), (G=0.0001, C=1000), (G=1000, C=0.0001). After training data, generating 
 the  model  and  evaluating  the  strategy,  the  obtained  results  were  the  following 
quite poor. Using linear kernel the recognition rates for every emotion were below 30% 
in most cases. The results were even worst when using polynomial kernel, for degree 2 
and 3. The differences in values of G and C did not make a difference. Moreover, using 
this classification strategy, sadness and neutral emotions tends to be in the same group. 

3.1.2.   Classification Based on Two Groups  
After bad results obtained with the first classification strategy, and taking advantage 
from  the  similarity  found  between  some  emotions,  a  new  classification  based  in 
groups was evaluated. These groups were: anger-fear-happiness-surprise and disgust-
neutral-sadness. 

 

142 

V. Rojas, S.F. Ochoa, and R. Hervás 

As  expected,  the  results  of  classifying  emotions  in  these  two  groups  were  better 
than in the previous case, since these two subsets are clearly differentiable. However, 
the emotion disgust was not expressed in the same way in the different databases; it 
was confused with anger in Emo-DB, and with neutral in SAVEE. Provided that disgust 
 is  not  a  frequent  emotion  in  people,  and  given  the  problem  for  classify  it,  we 
decided to take it away from the second group. The new results were a bit better than 
the  previous  ones,  but  not  good  enough  to  be  used  in  real  scenarios.  Therefore  we 
evaluated a strategy that uses three groups of emotions. 

3.1.3.   Classification Based on Three Groups  

The fear was put into a particular group, because sometimes it is confused with neutral 
 and  sadness.  This  leaves  us  three  groups  for  classifying  emotions:  anger-
happiness-surprise, neutral-sadness, and fear. Once again we used linear kernel with 
the emobase2010 descriptor for representing SVM model. 

The  classification  using  these  groups  improved  considerably.  Table  1  shows  the 
classification rates (in terms of True Positives- TP) obtained for each group, using the 
average over all testing samples. However, the groups are still large and they do not 
allow isolate each emotion (particularly sadness); therefore they need extra processing 
to reach such a goal. 

Table 1. Results in three groups’ classification 

Group 

Average TP (%) 

Anger-Happiness-Surprise 

Neutral-Sadness 

Fear 

87.1 
88.9 
67.2 

3.2  Detecting Sadness 

Having emotions rightly classified in groups, the next step was to recognize each of 
them. From these six emotions, sadness is the one that is more related to depression. 
Therefore, next task was to recognize this emotion. Only neutral and sadness emotion 
samples were used in this test, because they are together in the same group.  

We used a training-all-together strategy to improve classification rate. In this strategy 
all the training samples from all databases are joined in order to make the trained 
model more robust. This helps perform a better definition of the features that characterize 
each group. 

The linear kernel and the emobase2010 and emo_large [16] descriptors were used 
for training the model. The emo_large descriptor was included because it helps obtain 
better results when using simple classes (e.g. a category with a single emotion). 

The  tests  were  done  using  both  descriptors  and  the  results  showed  that  the 
emo_large descriptor was not useful for this task, having really low recognition rates. 
However, emobase2010 obtained a rate of TP close to 71% for sadness emotion. This 
result is good enough as to consider the use of this strategy in real scenarios. 

 

 

Monitoring Moods in Elderly People through Voice Processing 

143 

3.3  Using Arousal and Valence 

In 1980 the psychologist Russell designed a circular taxonomy of emotions known as 
the  circumflex  model  of  affect  (Fig.  1),  where  an  emotion  represents  an  entity  with 
two poles: arousal and valence [17]. Arousal means the intensity expressed through 
an emotion, and valence represents the pleasure that the people feel while expressing 
an emotion. Using these dimensions, we tried to improve the average TP recognition 
rate for sadness obtained with the previous strategies.  

 

Fig. 1. Circumflex model of affect (from [17]) 

 

Four groups were created corresponding to every quadrant in Fig. 1. We used the 
emobase2010  descriptor  with  linear  and  polynomial  kernel  (degree=2,  G=1,  C=1). 
Although  these  kernels  have  a  similar  performance,  the  linear  obtains  more  TP  and 
less FP (False Positive). Using this classification strategy we reached an 81% of average 
of TP for sadness recognition. This represents a 10% higher than the last classification 
(see section 3.2). 

The next step was to automate this classification process, by performing a first classification 
 in  three  groups  (see  section  3.1.3),  and  then  process  the  results  using  this 
strategy. The results of using this double filter process do not improve the rate of TP, 
but it allows us assuring that one sample belongs to the groups where it is classified.  

3.4  Considering Gender in the Emotion Detection 

Gender recognition  was included in the double filter recognition process, because it 
provides additional information to identify the person that is talking. For instance, let 
us suppose that an elder female, who is being monitored, is visited by his neighbor. 
The system processing the voices captured from the physical environment will detect 
two  people  talking,  and  it  can  determine  the  mood  of  each  of  them.  However  it  is 
required to determine the identify of these persons (when possible) to record properly 
such  information  and  deliver  notifications  to  relatives  of  a  person  when  the  system 
detects that such a person is in need of external support. 

The use of contextual information about the gender of the people that is talking allows 
us performing a simple and accurate first filter to determine the people identity. 
In many cases, the use of this filter is enough to determine people identify; therefore 
we included it in the proposed strategy for emotion detection.  

 

144 

V. Rojas, S.F. Ochoa, and R. Hervás 

For gender recognition we used audio records of two minutes long for training the 
classifier.  The  audio  records  did  not  belong  to  the  previously  mentioned  databases, 
and they included the five voice of woman and five of men.  

After exploring several alternatives, we found a simple strategy that has high performance 
for gender recognition. It consists on classifying the training samples according 
to two criteria. First, we classify them according to the respective emotions (as shown in 
the previous section), and then we separated them considering the people gender. This 
gave us a training set that classifies the samples according to the emotion and gender.  

Then, we used the linear kernel and the IS_10paraling descriptor [7]. This latter allows 
extracting the paralinguistic features of the voice, like age and gender. Using this 
solution we classified the samples stored in the three databases (i.e., SAVEE, EmoDB 
and RekEmozio) according to the gender of the person that is talking. The results 
showed a 94% of TP in men and a 79% for women. This process was implemented in 
the mood monitoring system to classify and recognize sadness and gender. Next section 
describes the architecture of this system. 

4 

Architecture of System 

The system uses two steps for processing the voice (Fig. 2): (1) recording the audio 
through the microphone of the slate that the older adults have at home and (2) recognizing 
emotion and gender using classification method described in section 3.4. 
 

 

 

 

 

Fig. 2. Voice processing strategy 

Fig. 3. Mood tracking process  

 

These  results  are  used  as  input  for  the  mood  tracking  process.  This  process  performs 
two activities (Fig. 3): (1) records the information about the mood of a person 
in a server, and (2) in case of detecting sadness, the system delivers a notification to a 
set  of  relatives  whom  provides  support  to  the  monitored  person.  The  information 
stored  in  the  server  can  be  accessed  on  demand  by  these  relatives  (or  caregiver  or 
doctors). Several visual representations of this information are available to help them 
understand the whole situation. 

 

 

Monitoring Moods in Elderly People through Voice Processing 

145 

The information stored in the server also indicates neutral emotions and happiness 
of  the  monitored  people.  These  emotions  were  included  in  the  records  not  only  for 
their  usefulness,  but  also  because  the  recognition  strategy  showed  high  accuracy  in 
the detection of these emotions (over 80% of TP). In a next step we will include as 
much emotions as we can; however it is important to keep in mind that the accuracy 
of the recognition process is (at least) as important as the detected emotion. 

In order to tests the implemented system we replied the tests described in sections 
3.3 and 3.4. The obtained results were the similar to those previously reported. This 
means  that  the  implemented  system  can  be  embedded  in  an  ubiquitous  computing 
application,  for  monitoring  of  the  people  mood  with  a  quite  high  accuracy  (at  least 
sadness, neutral emotion and happiness). 

5 

Conclusions and Future Work 

In this work we focused on the detection of the sadness using voice processing. We 
have  chosen  this  emotion  because  it  is  related  to  depressive  states,  particularly  in 
elderly people. The detection of sadness was achieved using two group classifications 
strategies.  During  a  first  step  the  emotions  were  grouped  according  to  similarities 
between them. In a second step these groups of emotions were processed considering 
arousal  and  valence  (i.e.  affective  dimensions).  This  double  filter  process  was  evaluated 
 using three emotion databases, and the obtained results  showed  high rates of 
true positive for sadness recognition (94% in men and 79% in women). This process 
also recognizes neutral emotions and happiness of the monitored people with a high 
accuracy (over 80% of true positive). Although the emotion databases used in these 
tests  had  audio  records  in  different  languages,  such  an  aspect  does  not  seem  to  be 
relevant  for  the  emotion  detection.  Something  similar  happened  with  the  emotions 
detected in adults and elderly people. Therefore, the emotion detection performed in 
by the proposed solution seems to be transversal to various social contexts.  

In a second stage we added gender recognition to such a solution as a way to ease 
the  identification  of  people.  This  process  was  implemented  as  a  mood  monitoring 
service and embedded in the SocialConnector system [5]. The service keeps the previously 
mentioned performance for sadness, neutral emotions and happiness.  

After  recording  the  voice,  the  system  processes  the  audio  record  and  stores  in  a 
server  the  information  about  the  detected  emotion.  If  that  emotion  is  sadness,  the 
system delivers a  notification to relatives (according  to a  certain  policy) in order to 
make them aware of such a situation.  

The information stored in the server (about the mood of the monitored person) can 
be accessed on demand by the relatives, caregivers or doctors. Using visual representations 
the system shows them the history of emotions detected in such a person. Such 
information can be used not only to monitor people suffering from depression, but also 
to perform early detection and diagnose of people vulnerable to these mental illness.  

The next steps in this initiative considers to increase the number of emotions accurately 
detected using the monitoring system, and also to evaluate the proposal in a real 
scenario. 

 

146 

V. Rojas, S.F. Ochoa, and R. Hervás 

Acknowledgments.  This  work  has  been  supported  by  Fondecyt  (Chile),  Grant  Nº 
1120207, and by the UBIHEALTH project, FP7-PEOPLE-2012-IRSES, European Commission,
 Grant: 316337.  

References 

1.  Tilvis, R.S., Routasalo, P., Karppinen, H., Strandberg, T.E., Kautiainen, H., Pitkala, K.H.: 
Social isolation, social activity and loneliness as survival indicators in old age: a nationwide 
survey with a 7-year follow-up. European Geriatric Medicine 3(1), 18–22 (2012) 

2.  Cornwell,  E.Y.,  Wwaite,  L.J.:  Social  Disconnectedness,  Perceived  Isolation,  and  Health 

among Older Adults. Journal of Health and Social Behavior 50(1), 31–48 (2009) 

3.  Cohen,  S.,  Mermelstein,  R.,  Kamarck,  T.,  Hoberman,  H.:  Measuring  the  functional  
components  of  social  support.  In:  Sarason,  I.G.,  Sarason,  B.R.  (eds.)  Social  support: 
theory, research and application, Martinus Nijhoff Publishers, Dordrecht (1985) 

4.  Schuller, B.: The Computational Paralinguistics Challenge. IEEE Signal Processing Magazine 
29(4), 97–101 (2012) 

5.  Muñoz, D., Gutierrez, F., Ochoa, S.F., Baloian, N.: Enhancing Social Interaction between 
Older Adults and Their Families. In: Nugent, C., Coronato, A., Bravo, J. (eds.) IWAAL 
2013. LNCS, vol. 8277, pp. 47–54. Springer, Heidelberg (2013) 

6.  Dickerson,  R.,  Gorlin,  E.,  Stankovic,  J.:  Empath:  a  continuous  remote  emotional  health 
monitoring system for depressive illness. In: Proc. of the Wireless Health 2011, San Diego, 
USA, pp. 5–14 (2011) 

7.  Alghowinem, S., Goecke, R., Wagner, M.: From joyous to clinically depressed: Mood detection 
 using  spontaneous  speech.  In:  Proc.  of  the  Int.  Florida  Artificial  Intelligence  Research 
Society Conference (FLAIRS 2012), pp. 141–146. AAAI Press, Marco Island (2012) 
8.  Eyben, F., Wöllmer, M., Schuller, B.: OpenSmile - The Munich Versatile and Fast OpenSource 
 Audio  Feature  Extractor.  In:  Proc.  of  ACM  Multimedia  (MM),  pp.  1459–1462. 
ACM Press, Florence (2010) 

9.  Yaafe, http://yaafe.sourceforge.net/ (last visit: June 12, 2014) 
10.  Cortes, C., Vapnik, V.: Support-vector networks. Machine Learning 20(3), 273–297 (1995) 
11.  Steidl,  S.:  Vocal  Emotion  Recognition:  State-of-the-Art  in  Classification  of  Real-Life 
Emotions.  International  Computer  Science  Institute  (ICSI).  Berkeley,  USA  (2010), 
http://www.stanford.edu/class/linguist287/materials/steidl.p
df (last visit: June 17, 2014) 

12.  LibSVM,  http://www.csie.ntu.edu.tw/~cjlin/libsvm/  (last  visit:  June  12, 

2014) 

13.  Haq, S., Jackson, P.J., Edge, J.D.: Audio-Visual Feature Selection and Reduction for Emotion 
Classification. In: Proceedings of International Conference on Auditory-Visual Speech 
Processing, Tangalooma, Australia, pp. 185–190 (2008) 

14.  Burkhardt, F., Paeschke, A., Rolfes, M., Sendlmeier, W., Weiss, B.: A Database of German 

Emotional Speech. In: Proc. of Interspeech 2005, Lisbon, Portugal, pp. 1517–1520 (2005) 

15.  López,  J.M.,  Cearreta,  I.,  Garay,  N.,  de  López  Ipiña,  K.,  Beristain,  A.:  Creación  de  
una base de datos emocional bilingüe y multimodal. In: Proc. of the 7th Spanish Human 
Computer Interaction Conference, Interaccion 2006, Puertollano, Spain, pp. 55–66 (2006) 
16.  Eyben, F., Wöllmer, M., Schuller, B.: OpenSMILE: the Munich open Speech and Music 
(2010), 

Interpretation  by  Large  Space  Extraction 
http://openSMILE book 2.0-rc1 (last visit: June 14, 2014) 

toolkit.  OpenSMILE  Book 

17.  Russell, J.A.:  A circumflex  model of affect. Journal of Personality  and Social Psychology 
39(6), 1161–1178 (1980) 

18. 

 

 

