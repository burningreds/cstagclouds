Landmark-Based Histograms of Oriented
Gradients for Facial Emotion Recognition

Pablo Guerrero(B), Mat´ıas Pavez, Diego Ch´avez, and Sergio F. Ochoa

Computer Science Department, Universidad de Chile, Santiago, Chile

pguerrer@ing.uchile.cl

Abstract. The automatic recognition of human emotions is used to support 
several computing paradigms, like aﬀective, positive and pervasive
computing. Histograms of oriented gradients (HOG) have been successfully 
used with such a purpose, by processing facial images. However, the
results of using HOG vary depending on the position of the facial components 
in the image used as input. This paper presents an extension to the
HOG method, which was named Landmark-based Histograms of Oriented
Gradients (LaHOG), that not only calculates HOG blocks in the whole
face, but also in speciﬁc positions around selected facial landmarks. In
this sense, the new method is more robust than its predecessor. In order
to evaluate the capabilities and limitations of this proposal, we used it
to recognize emotions in face images from the FACES database. In such
a process we used two classiﬁcation strategies: support vector machines
and logistic regression. The results show that the extended method signiﬁcantly 
surpasses the performance of HOG in the tested database.

Keywords: Emotion recognition · Facial recognition · Pervasive computing 
· Emotional computing · Histograms of oriented gradients · Social
robotics · Logistic regression · Support vector machines

1 Introduction

Over recent decades, machines have increased their capability of performing
complex tasks, being integrated into our world and interacting with humans in
a natural fashion. Several new computing paradigms, such as positive, aﬀective 
and pervasive computing, take advantage of these capabilities. Two regular
services used by these systems are the user identiﬁcation and the automatic emotion 
recognition, typically using voice or face recognition. The research eﬀorts
in this direction are gaining momentum, given they can provide a transversal 
improvement to systems and services used in several application scenarios,
such as monitoring patients, determining driver stress, performing psychological
diagnosis and training, diagnosing psychological disorders and stimulating social
interactions [14,25]. In this paper we address only one aspect of this challenge;
particularly, the emotion recognition using facial gestures.

Some of the most successful strategies for emotion recognition rely on the
extraction of histograms of oriented gradients (HOG) from face images
c(cid:2) Springer International Publishing Switzerland 2015
I. Cleland et al. (Eds.): IWAAL 2015, LNCS 9455, pp. 288–299, 2015.
DOI: 10.1007/978-3-319-26410-3 27

Landmark-Based Histograms of Oriented Gradients

289

[3,7,8,10,16,18]. Particularly, HOG features are retrieved from a window that
is centered in the face of the people held in the image. Although the HOG approach 
has shown to be useful, there is an intrinsic assumption that facial gestures
will produce gradient changes in determined regions of the face. This assumption 
will often hold since facial gestures are strongly related to movements of
discriminatory facial components, like the mouth, nose, eyes and eyebrows.

Typically, the face detection process gives an estimated position for the
bounding box of a person’s face. Considering these bounding boxes, we can
assume that the position of the facial components inside the image are quite stable.
 However, diﬀerences in the relative positions of those components reduce the
validity of the former assumption. These diﬀerences may be due to: (i) discrepancies 
among the borders of the detected face, (ii) variations in the morphology
among faces of diﬀerent people, and (iii) rotations of the face in the picture.

In this work, we propose an extension to the HOG model to deal with such
spatial variations for the emotion recognition problem. The proposed extension
uses an existing method for ﬁnding landmarks in face images. Then, it uses
the positions of these facial landmarks to extract a set of reﬁned HOG features.
These features are ﬁnally added to the HOG features obtained for the whole face,
thus easing the emotions recognition. The proposed method, named Landmarkbased 
Histograms of Oriented Gradients (LaHOG), was evaluated using face
images from the FACES database [11]. The obtained results indicate that the
additional information retrieved by LaHOG helps increase the accuracy of the
emotion recognition process; therefore, the performance of the proposed method
is higher than its predecessor. Additionally, two classiﬁcation strategies were
tested and compared in order to have more detailed perspective on the model
performance. Researchers of positive, aﬀective or pervasive computing can use
this proposal not only to perform emotion detection in facial images, but also to
improve their own image-based emotion detection algorithms.

The next section presents and discusses the related work. Sections 3 and 4
introduce the HOG and LaHOG methods respectively. Section 5 shows and discusses 
the evaluation process and the obtained results. Finally, Sect. 6 presents
the conclusions and future work.

2 Related Work

The recognition of human emotions from facial images has been addressed in
many studies. The literature in this ﬁeld is too vast, and reviewing it comprehensively 
is out of the scope of this paper. The reader is referred to [5,13,21] for
a more complete view of the previous works.

Typically, the recognition of emotions from facial images can be divided in
three stages: face detection and tracking, feature extraction and classiﬁcation.
Concerning detection and tracking, some works (e.g., [19]) focus on the identiﬁcation 
of the so-called action units (AUs) [12], which are speciﬁc facial muscle
movements. A comparison of several classiﬁcation techniques used to recognizing
AUs is available in [4].

290

P. Guerrero et al.

There are also methods for face detection and tracking that follow other
approaches. For instance, in [22] a ratio template is presented to detect faces
based on three-dimensional forms. A modiﬁed version of this ratio template is
used in [2] to track faces in cluttered scenes. Following the same purpose, the
PersonSpotter system [23] was proposed. This system builds an elastic graph out
of a stereo image of frontal faces to perform the recognition. However, one of
the most used face detectors for this type of input data is the Viola-Jones face
detector [26], which performs quick recognitions with high success rates. Concerning 
the recognition using 3D models, Candide [1] is a widely used strategy,
as well as the piecewise B´ezier volume deformation model [24] that also proposes
a tracking algorithm.

Concerning the feature extraction methods, Yun et al. [28] show an experimental 
comparison of some well-known methods, considering their performance.
The comparison includes HOG, Local Binary Paterns, Gabor Filters and combinations 
of them. The use of HOG for feature extraction is an strategy followed
by several works; for instance, Orrite et al. [18] focus on the selection of features
from a HOG vector obtained from the whole image. Similarly, in [8] the authors
use a dynamic-size grid with the same purpose, whose dimensions are calculated
as a function of the distance between detected eyes. Other works apply a Pyramid 
of HOG (PHOG) on a speciﬁc window to perform the feature extraction
[3,10,16]. For example, in [3], a PHOG is used in conjunction with Gabor ﬁlters
over the mouth region in order to detect smiles. Dhall et al. [10] propose the use
PHOG and Local Phase Quantization features over the whole face in selected
frames inside a sequence. Finally, in [16] PHOG is combined with a bag of words
based on Scale-Invariant Feature Transform.

There are also other methods that use of facial landmarks for feature extraction,
 and thus to recognize emotion. For instance, in [20] the positions of selected
facial landmarks are used to recognize AUs in proﬁle images. In [7], HOG features
are extracted from two windows whose coordinates are obtained from detected
eyes; one for the eyes and eyebrows and the other for the nose and mouth. In [29],
the positions of manually located facial landmarks are used to extract Gabor ﬁlters 
and form a feature vector. In [15], the displacements of the nodes of the
Candide grid are used to recognizing emotions and detect AUs.

Regarding the classiﬁcation stage, there is also a vast literature whose review
is out of the scope of this paper. The reader is refered to [17] for a thorough
revision of the most used methods. The most commonly used approach is the
use of Support Vector Machines (SVM). SVM in its original formulation allows
the discrimination between two classes that are linearly sepparable. Later, the
use of non-linear kernels has made SVM able to perform non-linear separations.
Another fairly used approach, which usually gets comparable results to those
of SVM, is Logistic Regression. Both approaches were originally designed to
perform binary classiﬁcation. However, later enhancements have made them able
to perform multi-class classiﬁcation.

All these proposals have shown to be useful to address one or more stages of
the emotion recognition process using facial images. However, the results of these

Landmark-Based Histograms of Oriented Gradients

291

methods vary depending on the position of the facial components in the image. In
this paper we present a method that helps deal with such a challenge using HOG
as a baseline for emotion detection. Particularly, the main contribution of this
proposal is the use of a deformable model of the face, in order to position partspeciﬁc 
HOG windows. To the best of our knowledge, this is the ﬁrst attempt of
recognizing emotions using landmark-centered HOG windows. Additionally, this
paper presents an experimental comparison of the performance of several sets of
parameters for both emotion recognition methods; i.e., HOG and LaHOG.

3 The Histograms of Oriented Gradients Method

Histograms of Oriented Gradients are a feature extraction method, originally
proposed by Dalal and Triggs [9]. The HOG feature vector is extracted from a
window and it basically measures the relative importance of the diﬀerent gradient
directions in diﬀerent regions of the window. With that purpose, the image
gradient is densely calculated across the window, and then the window is tiled
into cells. For each cell, a number of direction bins are considered in order to
count the gradients following that direction. In fact, for neglecting noise and
mostly taking into account relevant gradients, for each pixel, the modulus of its
gradient is summed to the bin that contains its direction. Neighbor cells are then
grouped into blocks and their values are normalized with respect to the block.
The blocks are overlapped, and thus each cell typically belongs to several blocks.
Cells and blocks are typically designed to be square.

The ﬁnal feature vector is built by aggregating the normalized bin values
present in every cell of every block. Therefore, information from a single cell
may appear several times within the feature vector, but associated with diﬀerent
blocks and thus having diﬀerent normalization values.
This method has several parameters, such as: (i) the size of the cell, cs × cs,
for which typical values are 6 × 6 and 8 × 8 pixels, (ii) the size of the block,
bs × bs, usually 2 × 2 or 3 × 3 cells, (iii) the range of directions considered,
whether 0◦ − 180◦ (unsigned direction) or 0◦ − 360◦ (signed direction), and (iv)
the number of direction bins, nb, usually 9. Then, the descriptor size, ds, can
be computed as shown in Eq. (1), where w is the width in pixels of the square
image in use.

ds = nb · b2
s ·

2

(1)

(cid:2)

w
cs

(cid:3)
− bs + 1

HOG features can be used for emotion recognition in a very straightforward
fashion. A ﬁrst convenient step is to get the HOG window positioned in the
face. This is achieved by using a face detection algorithm [26] that gets a square
window (scaled to a 144 × 144 pixel window) that ﬁts the face. HOG features
are extracted from this rescaled window. Then, the HOG feature vector is used
as an input for a classiﬁer that gives the recognized emotion as an output. This
approach is considered as a baseline strategy for our method.

292

P. Guerrero et al.

4 The Landmark-Based Histograms of Oriented

Gradients Method

The proposed approach intends to overcome the problems that arise from the
variations in positions of the relevant face parts in diﬀerent pictures. To make
the feature vector more robust to those variations, the HOG blocks are not
only calculated from the whole face window, but also from smaller windows
centered in selected face landmarks. Next subsection explains the way to obtain
the feature vector and the classiﬁers used to perform the emotion recognition.

4.1 The Feature Vector

The feature vector is composed of two parts. The ﬁrst part corresponds to the
regular HOG feature vector, extracted from a window containing the whole face
as described in Subsect. 3. The second part corresponds to several HOG feature
vectors, extracted from smaller windows positioned in selected parts of the face.
The method for extracting these HOG vectors involves two sequential steps.
During the ﬁrst step, a number of facial landmarks are extracted from the face
picture. For that purpose, we used the method proposed by Yu et al. [27], which
is based on a two-stage cascaded deformable shape model and it is highly reliable.
Figure 1 shows examples of landmarks extracted in face pictures.

Fig. 1. Landmark extraction examples based on the work of Yu et al. [27].

Then, all the detected landmarks, excepting those of the face border, are
selected as centers of relevant face parts. A window containing a single HOG
block is considered around the position of each selected landmark. The size of
this window is a linear function of some measurement of the face size (more
details in Subsect. 5.2). Figure 2 shows the extracted windows for the same faces
as before.

Landmark-Based Histograms of Oriented Gradients

293

Fig. 2. Examples of selected landmarks and related windows for the images shown in
Fig. 1.

4.2 The Classiﬁers

Two kinds of classiﬁers were considered to carry out the emotion recognition
process. The ﬁrst one is a Logistic Regression Classiﬁer [17], which is able to deal
with multiple classes. The second one is a ν−Support Vector Machine (ν−SVM)
classiﬁer, from the LIBSVM library [6], which handles multi-class problems using
a one-against-one approach. Furthermore, only linear kernels were used for the
SVM classiﬁer.

5 Evaluation of LaHOG

The proposed method was compared to the plain use of HOG, in order to determine 
if the use of landmarks helps improve the emotion recognition process.
Next subsections presents the dataset and the parameters used in this evaluation 
process, as well as the obtained results in this comparison.

5.1 Dataset

The FACES Database [11] was selected as a test bed for the proposed method.
It consists of 2,052 face images corresponding to 171 people belonging to three
groups: young, middle-age and older. Each picture in the database contains a
face expressing one of the six diﬀerent emotions available: anger, fear, disgust,
happiness, sadness and neutrality. For each person and for each emotion, two
pictures are available.

294

P. Guerrero et al.

5.2 Parameter Selection

There are some parameters of both methods (i.e., HOG and LaHOG) that need
to be selected for conducting the comparison process; particularly the cell size
cs, block size bs, number of orientation bins nb, and signed/unsigned gradients.
We have decided to use nb = 9 and unsigned orientations (0◦ − 180◦) for both
methods, since these values have allowed to get good results for similar problems
reported in the literature [9].

In the case of HOG, we also had to determine the size of the resized cropped
face, and in the case of LaHOG, we speciﬁed the width of each landmark centered 
window. This width is calculated as a fraction of the distance between two
speciﬁc landmarks: one in the chin and one in the right eye-brow. Then, the
window width, wLaHOG, is calculated as follows:

wLaHOG = 2λ||pchin − pr−eyebrow||

(2)
where λ is the scale factor parameter and pchin and pr−eyebrow are the positions 
of the detected chin and right eyebrow center, respectively. Consequently,
all landmark-centered windows are rescaled to (csbs) × (csbs), and then, HOG
descriptors are calculated over each of them. Figure 3 shows the landmarks used
when computing wLaHOG and the eﬀect of λ on the landmark window size. The
red line represents the distance between both landmarks and the green windows
are the result of applying diﬀerent λ values for a single landmark.

Fig. 3. Examples of window widths as a function of λ. The width of the window is
calculated as a fraction of the distance between two landmarks (red line between green
squares). The three green squares centered in the landmark (red ex) between the eyes
show the window sizes corresponding to λ = {1/8, 1/12, 1/20} (Color ﬁgure online).

In order to select appropriate parameters for each method, we performed
prior experiments on the FACES database. The people facial images in the

Landmark-Based Histograms of Oriented Gradients

295

dataset were randomly split into a training (1,200 images) and a test (852 images)
subsets. As a result, 1,200 (200 per emotion) images were selected for training
and 852 (142 per emotion) for testing. A Logistic Regression classiﬁer was then
used to obtain the recognition rates for the selected subsets. In order to get more
statistically signiﬁcant results, this process was repeated 100 times, with a different 
random dataset permutation each time. Table 1 shows the values assumed
by the parameters considered in the evaluation of the baseline method, and their
average emotion recognition rates after 100 cycles. The conﬁguration that shown
the highest recognition rate for HOG is the following: κ = 144, cs = 8, bs = 3.

Table 1. Set of parameters and their respective average emotion recognition rates for
the baseline method

κ

72

bs

HOG[%]

cs
6 × 6 2 × 2 90.95 ± 1.00
3 × 3 91.58 ± 0.95
8 × 8 2 × 2 89.98 ± 1.04
3 × 3 90.40 ± 1.04
144 6 × 6 2 × 2 91.68 ± 0.96
3 × 3 92.34 ± 0.92
8 × 8 2 × 2 91.72 ± 0.95
3 × 3 92.49 ± 0.95
288 6 × 6 2 × 2 90.83 ± 1.09
3 × 3 91.32 ± 1.06
8 × 8 2 × 2 91.31 ± 1.11
3 × 3 91.94 ± 1.02

Table 2 shows the parameters and the respective results for the LaHOG
method using, for the whole face feature extraction, the best parameter set
found for the baseline method. Therefore, the modiﬁed parameters used in this
evaluation only correspond to those required to compute the landmark descriptors.
 The results show that the selected sets of parameters behave similarly, and
there are several conﬁguration with an average recognition rate very close to the
highest. Nevertheless, we selected the set of parameters that achieved the highest
average value: λ = 1/8, cs = 8 and bs = 3. Additionally, LaHOG gets a higher
recognition rate with lower deviation from mean in every tested conﬁguration.

5.3 Analysis of the Evaluation Results

With the purpose of comparing the performance (in terms of emotion recog-
nition) of the two methods, we will now focus on the results obtained when
considering the best parameters for each method.

296

P. Guerrero et al.

Table 2. Set of parameters and their respective average emotion recognition rates for
LaHOG

bs

λ
1/8

cs
LaHOG[%]
6 × 6 2 × 2 93.32 ± 0.88
3 × 3 93.84 ± 0.90
8 × 8 2 × 2 93.34 ± 0.84
3 × 3 93.89 ± 0.89
1/12 6 × 6 2 × 2 93.21 ± 0.89
3 × 3 93.68 ± 0.87
8 × 8 2 × 2 93.27 ± 0.92
3 × 3 93.78 ± 0.91
1/20 6 × 6 2 × 2 93.06 ± 0.94
3 × 3 93.46 ± 0.91
8 × 8 2 × 2 93.11 ± 0.93
3 × 3 93.57 ± 0.92

The ν−SVM classiﬁer yields an accuracy of 92.68 ± 0.97 % and 93.70 ±
0.94 % for the HOG and the LaHOG methods respectively. Similarly, the Logistic 
Regression approach yields an accuracy of 92.49 ± 0.95 % and 93.89 ± 0.89 %
for these methods, which shows that both classiﬁers behave similarly. However,
due to the high time consumption and large memory usage of the ν − SV M
classiﬁer, we will use the Logistic Regression classiﬁer for the remaining of the
paper.

Tables 3 and 4 show the confusion matrices for HOG and LaHOG respectively.
 Note that LaHOG achieves a higher average recognition rate for each of
the tested emotions. If we look in detail the behavior of these methods for each
emotion separately, we can see that happiness and fear are the easiest emotions
to be recognized. In contrast, sadness and anger get signiﬁcantly lower recognition 
rates. Sadness usually gets confused with anger and neutrality, whereas
anger gets mostly confused with disgust and sadness. Finally, both disgust and
neutrality get confused with anger and sadness most of the time.

Table 3. Confusion Matrix for the HOG Method

Anger Disgust Fear Happiness Neutrality Sadness
88.65
3.41
0.06
Happiness 0.45
Neutrality 3.06
5.38

5.65
94.58
0.08
0.13
0.31
1.70

0.18
0.02
95.99
0.23
2.39
1.49

0.15
0.42
0.01
99.15
0.30
1.25

1.87
0.08
2.37
0.04
91.32
4.94

Anger
Disgust

Fear

Sadness

3.49
1.49
1.48
0.01
2.62
85.23

Landmark-Based Histograms of Oriented Gradients

297

Table 4. Confusion Matrix for the LaHOG Method

Anger Disgust Fear Happiness Neutrality Sadness
90.04
2.88
0.01
Happiness 0.39
Neutrality 1.87
4.68

0.20
0.31
0.01
99.45
0.25
1.29

1.80
0.01
1.46
0.04
94.17
4.75

2.62
1.32
0.80
0.01
2.69
86.52

0

0.11
0.03
1.59

0.18
0.02
97.72

0

0.99
1.17

5.15
95.45

Anger
Disgust

Fear

Sadness

These results make the proposed method potentially useful for addressing
the challenge of monitoring people in several scenarios. Therefore, it could be
embedded in ambient assisted living systems, home care solutions and also in
health monitoring systems. Considering that the modern medicine addresses the
chronic diseases using an integral approach, where the patients’ and caregivers’
mood are important elements in the caring process, implementations of the proposed 
emotion recognition method could contribute to support treatments in
this area.

6 Conclusions and Future Work

This paper has presented the Landmark-based Histograms of Oriented Gradients 
(LaHOG) method for emotion recognition using frontal facial images. This
method is based on HOG and facial landmark extraction. Several combinations
of parameters were evaluated for both HOG and LaHOG. The experimental
results obtained using the FACES database show that the performance of the
presented method is higher than the baseline method. Although these results are
still preliminary, the obtained recognition rates make LaHOG very promising.
Considering the classiﬁers used in the evaluation process, Logistic Regression
showed a performance similar to SVM; however, the former used much less computational 
resources than the latter.

Real-time processing is not achievable by the current implementation of the
LaHOG method. Therefore, a very important issue to be addressed in the future
is the optimization of the landmark extraction process, in order to extend the
range of problems that can be addressed using this solution. One possible strategy 
to reach this goal is to track the deformable model from one frame to another,
for instance, using particle ﬁlters.

Another important issue to analyze in this improvement process is the high
dimensionality of the feature vector. The use of automatic dimensionality reduction 
techniques could oﬀer further advantages, in terms of performance and
eﬃciency, to the recognition process.

Aknowledgments. This work has been partially supported by Fondecyt (Chile),
grant Nro. 1150252.

298

P. Guerrero et al.

References

1. Ahlberg, J.: Candide-3 - an updated parameterized face. Technical report, LiTH-
ISY-R-2326, Department of Electrical Engineering, Linkping University, Sweden
(2001)

2. Anderson, K., McOwan, P.W.: A real-time automated system for the recognition
of human facial expressions. IEEE Trans. Syst. Man Cybern. B Cybern. 36(1),
96–105 (2006)

3. Bai, Y., Guo, L., Jin, L., Huang, Q.: A novel feature extraction method using pyramid 
histogram of orientation gradients for smile recognition. In: 2009 16th IEEE
International Conference on Image Processing (ICIP), pp. 3305–3308, November
2009

4. Bartlett, M., Littlewort, G., Frank, M., Lainscsek, C., Fasel, I., Movellan, J.: Recognizing 
facial expression: machine learning and application to spontaneous behavior.
In: IEEE Computer Society Conference on Computer Vision and Pattern Recognition,
 CVPR 2005, vol. 2, pp. 568–573, June 2005

5. Bettadapura, V.: Face expression recognition and analysis: the state of the art.

Technical report, College of Computing, Georgia Institute of Technology (2012)

6. Chang, C.C., Lin, C.J.: LIBSVM: a library for support vector machines. ACM
Trans. Intell. Syst. Technol. 2, 1–27 (2011). http://www.csie.ntu.edu.tw/∼cjlin/
libsvm

7. Chen, J., Chen, Z., Chi, Z., Fu, H.: Facial expression recognition based on facial
components detection and hog features. In: Scientiﬁc Cooperations International
Workshops on Electrical and Computer Engineering Subﬁelds, Istanbul, Turkey,
August 2014

8. Dahmane, M., Meunier, J.: Emotion recognition using dynamic grid-based hog

features. In: FG, pp. 884–888. IEEE (2011)

9. Dalal, N., Triggs, B.: Histograms of oriented gradients for human detection. In:
International Conference on Computer Vision & Pattern Recognition, vol. 2, pp.
886–893, June 2005

10. Dhall, A., Asthana, A., Goecke, R., Gedeon, T.: Emotion recognition using phog

and lpq features. In: FG, pp. 878–883. IEEE (2011)

11. Ebner, N.C., Riediger, M., Lindenberger, U.: Faces. a database of facial expressions
in young, middle-aged, and older women and men: development and validation.
Behav. Res. Meth. 42(1), 351–362 (2010)

12. Ekman, P., Friesen, W. (eds.): The Facial Action Coding System. Consulting Psychologists 
Press, Palo Alto (1978)

13. Fasel, B., Luettin, J.: Automatic facial expression analysis: a survey. Pattern 
Recogn. 36(1), 259–275 (2003). http://www.sciencedirect.com/science/
article/pii/S0031320302000523

14. Ko(cid:3)lakowska, A., Landowska, A., Szwoch, M., Szwoch, W., Wr´obel, M.R.: Emotion
recognition and its applications. In: Hippe, Z.S., Kulikowski, J.L., Mroczek, T.,
Wtorek, J. (eds.) Human-Computer Systems Interaction: Backgrounds and Applications 
3. AISC, vol. 300, pp. 51–62. Springer, Heidelberg (2014)

15. Kotsia, I., Pitas, I.: Facial expression recognition in image sequences using geometric 
deformation features and support vector machines. IEEE Trans. Image Process.
16(1), 172–187 (2007)

16. Li, Z., Ichi Imai, J., Kaneko, M.: Facial-component-based bag of words and phog
descriptor for facial expression recognition. In: IEEE International Conference on
Systems, Man and Cybernetics, SMC 2009, pp. 1353–1358, October 2009

Landmark-Based Histograms of Oriented Gradients

299

17. Murphy, K.P.: Machine Learning: A Probabilistic Perspective. The MIT Press,

Cambridge (2012)

18. Orrite, C., Ga˜n´an, A., Rogez, G.: HOG-based decision tree for facial expression
classiﬁcation. In: Araujo, H., Mendon¸ca, A.M., Pinho, A.J., Torres, M.I. (eds.)
IbPRIA 2009. LNCS, vol. 5524, pp. 176–183. Springer, Heidelberg (2009)

19. Pantic, M., Patras, I.: Detecting facial actions and their temporal segments in
nearly frontal-view face image sequences. In: 2005 IEEE International Conference
on Systems, Man and Cybernetics, vol. 4, pp. 3358–3363, October 2005

20. Pantic, M., Patras, I.: Dynamics of facial expression: recognition of facial actions
and their temporal segments from face proﬁle image sequences. IEEE Trans. Syst.
Man Cybern. Part B Cybern. 36(2), 433–449 (2006)

21. Pantic, M., Rothkrantz, L.J.M.: Automatic analysis of facial expressions: the state
of the art. IEEE Trans. Pattern Anal. Mach. Intell. 22(12), 1424–1445 (2000).
http://dx.doi.org/10.1109/34.895976

22. Sinha, P.: Perceiving and recognizing three-dimensional forms. Ph.D. Thesis,

Massachusetts Institute of Technology (1995)

23. Steﬀens, J., Elagin, E., Neven, H.: Personspotter-fast and robust system for human
detection, tracking and recognition. In: Proceedings of the Third IEEE International 
Conference on Automatic Face and Gesture Recognition, pp. 516–521, April
1998

24. Tao, H., Huang, T.S.: A piecewise Bezier volume deformation model and its applications 
in facial motion capture. In: Advances in Image Processing and Under-
standing: A Festschrift for Thomas S. Huang (2002)

25. Tivatansakul, S., Ohkura, M., Puangpontip, S., Achalakul, T.: Emotional healthcare 
system: emotion detection by facial expressions using japanese database. In:
2014 6th Computer Science and Electronic Engineering Conference (CEEC), pp.
41–46, September 2014

26. Viola, P., Jones, M.J.: Robust real-time face detection. Int. J. Comput. Vis. 57(2),

137–154 (2004)

27. Yu, X., Huang, J., Zhang, S., Yan, W., Metaxas, D.: Pose-free facial landmark
ﬁtting via optimized part mixtures and cascaded deformable shape model. In:
2013 IEEE International Conference on Computer Vision (ICCV), pp. 1944–1951,
December 2013

28. Yun, W., Kim, D., Park, C., Kim, J.: Hybrid facial representations for emotion

recognition. ETRI J. 35(6), 1021–1028 (2013)

29. Zheng, W., Zhou, X., Zou, C., Zhao, L.: Facial expression recognition using kernel
canonical correlation analysis (KCCA). IEEE Trans. Neural Netw. 17(1), 233–238
(2006)

