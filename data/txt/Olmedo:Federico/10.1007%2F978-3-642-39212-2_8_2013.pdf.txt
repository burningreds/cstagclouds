Beyond Differential Privacy: Composition Theorems

and Relational Logic for f-divergences

between Probabilistic Programs

Gilles Barthe and Federico Olmedo

IMDEA Software Institute, Madrid, Spain

{Gilles.Barthe,Federico.Olmedo}@imdea.org

Abstract. f-divergences form a class of measures of distance between probability 
distributions; they are widely used in areas such as information theory and
signal processing. In this paper, we unveil a new connection between f-divergences 
and differential privacy, a conﬁdentiality policy that provides strong privacy 
guarantees for private data-mining; speciﬁcally, we observe that the notion
of α-distance used to characterize approximate differential privacy is an instance
of the family of f-divergences. Building on this observation, we generalize to arbitrary 
f-divergences the sequential composition theorem of differential privacy.
Then, we propose a relational program logic to prove upper bounds for the fdivergence 
between two probabilistic programs. Our results allow us to revisit
the foundations of differential privacy under a new light, and to pave the way for
applications that use different instances of f-divergences.

1 Introduction

Differential privacy [12] is a policy that provides strong privacy guarantees in private
data analysis: informally, a randomized computation over a database D is differentially
private if the private data of individuals contributing to D is protected against arbitrary
adversaries with query access to D. Formally, let  ≥ 0 and 0 ≤ δ ≤ 1: a randomized
algorithm c is (, δ)-differentially private if its output distributions for any two neighbouring 
inputs x and y are (e, δ)-close, i.e. for every event E:

Pr c(x)E ≤ e Pr c(y)E + δ

where Pr c(x)E denotes the probability of event E in the distribution obtained by running 
c on input x. One key property of differential privacy is the existence of sequential
and parallel composition theorems, which allows building differentially private computations 
from smaller blocks. In this paper, we focus on the ﬁrst theorem, which states
that the sequential composition of an (1, δ1)-differentially private algorithm with an
(2, δ2)-differentially private one yields an (1 + 2, δ1 + δ2)-differentially private algorithm.


f -divergences [2,10] are convex functions that can be used to measure the distance between 
two distributions. The class of f -divergences includes many well-known notions
of distance, such as statistical distance, Kullback-Leibler divergence (relative entropy),

F.V. Fomin et al. (Eds.): ICALP 2013, Part II, LNCS 7966, pp. 49–60, 2013.
c(cid:2) Springer-Verlag Berlin Heidelberg 2013

50

G. Barthe and F. Olmedo

or Hellinger distance. Over the years, f -divergences have found multiple applications
in information theory, signal processing, pattern recognition, machine learning, and security.
 The practical motivation for this work is a recent application of f -divergences
to cryptography: in [24], Steinberger uses Hellinger distance to improve the security
analysis of key-alternating ciphers, a family of encryption schemes that encompasses
the Advanced Encryption Standard AES.

Deductive Veriﬁcation of Differentially Private Computations. In [6], we develop an
approximate probabilistic Hoare logic, called apRHL, for reasoning about differential
privacy of randomized computations. The logic manipulates judgments of the form:

c1 ∼α,δ c2 : Ψ ⇒ Φ

where c1 and c2 are probabilistic imperative programs, α ≥ 1, 0 ≤ δ ≤ 1 and Ψ
and Φ are relations over states. As for its predecessor pRHL [5], the notion of valid
judgment rests on a lifting operator that turns a relation R over states into a relation
∼α,δ
R over distributions of states: formally, the judgment above is valid iff for every pair
of memories m1 and m2, m1 Ψ m2 implies ((cid:2)c1(cid:3) m1) ∼α,δ
((cid:2)c2(cid:3) m2). The deﬁnition
of the lifting operator originates from probabilistic process algebra [15], and has close
connections with ﬂow networks and the Kantorovich metric [11].
apRHL judgments characterize differential privacy, in the sense that c is (, δ)-differentially 
private iff the apRHL judgment c ∼e,δ Ψ : c ⇒ ≡ is valid, where Ψ is
a logical characterization of adjacency—for instance, two lists of the same length are
adjacent if they differ in a single element.

Φ

Problem Statement and Contributions. The goal of this paper is to lay the theoretical 
foundations for tool-supported reasoning about f -divergences between probabilistic
computations. To achieve this goal, we start from [6] and take the following steps:

1. as a preliminary observation, we prove that the notion of α-distance used to characterize 
differential privacy is in fact an f -divergence;

2. we deﬁne a notion of composability of f -divergences and generalize the sequential

composition theorem of differential privacy to composable divergences;

3. we generalize the notion of lifting used in apRHL to composable f -divergences;
4. we deﬁne f pRHL, a probabilistic relational Hoare logic for f -divergences, and

prove its soundness.

Related Work. The problem of computing the distance between two probabilistic computations 
has been addressed in different areas of computer science, including machine
learning, stochastic systems, and security. We brieﬂy point to some recent developments.


Methods for computing the distance between probabilistic automata have been studied 
by Cortes and co-authors [8,9]; their work, which is motivated by machine-learning
applications, considers the Kullback-Leibler divergence as well as the Lp distance.

Approximate bisimulation for probabilistic automata has been studied, among others,
 by Segala and Turrini [23] and by Tracol, Desharnais and Zhioua [25]. The survey 
[1] provides a more extensive account of the ﬁeld.

Composition Theorems and Relational Logic for f-divergences

51

In the ﬁeld of security, approximate probabilistic bisimulation is closely connected
to quantitative information ﬂow of probabilistic computations, which has been studied 
e.g. by Di Pierro, Hankin and Wiklicky [20]. More recently, the connections between 
quantitative information ﬂow and differential privacy have been explored e.g.
by Barthe and K¨opf [4], and by Alvim, Andr´es, Chatzikokolakis and Palamidessi [3].
Moreover, several language-based methods have been developed for guaranteeing differential 
privacy; these methods are based on runtime veriﬁcation, such as PINQ [17]
or Airavat [22], type systems [21,14], or deductive veriﬁcation [7]. We refer to [19] for
a survey of programming languages methods for differential privacy.

2 Mathematical Preliminaries

In this section we review the representation of distributions used in our development
and recall the deﬁnition of f -divergences.

2.1 Probability Distributions

(cid:2)

a∈A μ(a) = 1 (resp.

Throughout the presentation we consider distributions and sub-distributions over discrete 
sets only. A probability distribution (resp. sub-distribution) over a set A is an
a∈A μ(a) ≤ 1). We let D(A)
object μ : A → [0, 1] such that
(resp. D≤1(A)) be the set of distributions (resp. sub-distributions) over A.
Distributions are closed under convex combinations: given distributions (μi)i∈N in
i∈N wi = 1 and wi ≥ 0 for all i ∈ N, the
D(A) and weights (wi)i∈N such that
i∈N wi μi is also a distribution over A. Thus, given μ ∈ D(A)
convex combination
and M : A → D(B), we deﬁne the distribution bind μ M over B as
a∈A μ(a) M (a).
Likewise, sub-distributions are closed under convex combinations.

(cid:2)

(cid:2)

(cid:2)

(cid:2)

2.2 f-divergences
Let F be the set of non-negative convex functions f : R
+
0 such that f is continuous 
at 0 and f (1) = 0. Then each function in F induces a notion of distance between
probability distributions as follows:
Deﬁnition 1 (f-divergence). Given f ∈ F, the f -divergence Δf (μ1, μ2) between two
distributions μ1 and μ2 in D(A) is deﬁned as:
(cid:3)

0 → R

(cid:4)

(cid:5)

+

Δf (μ1, μ2) def=

μ2(a)f

μ1(a)
μ2(a)

a∈A

The deﬁnition adopts the following conventions, which are used consistently throughout
the paper:

0f (0/0) = 0

xf (1/x)
Moreover, if Δf (μ1, μ2) ≤ δ we say that μ1 and μ2 are (f, δ)-close.

0f (t/0) = t lim
x→0+

and

if t > 0

52

G. Barthe and F. Olmedo

f-divergence

f

Statistical distance
Kullback-Leibler1

Hellinger distance

|t − 1|

SD(t) = 1
2
KL(t) = t ln(t) − t + 1
HD(t) = 1
2

t − 1)2

√

(

Simpliﬁed Form
(cid:2)

|μ1(a) − μ2(a)|
(cid:4)

(cid:3)

μ1(a)
μ2(a)

μ1(a) − (cid:5)

(cid:4)2

μ2(a)

1
2

a∈A
a∈A μ1(a) ln

(cid:3)(cid:5)

(cid:2)

(cid:2)

a∈A

1
2

Fig. 1. Examples of f-divergences

When deﬁning f -divergences one usually allows f to take positive as well as negative
values in R. For technical reasons, however, we consider only non-negative functions.
We now show that we can adopt this restriction without loss of generality.
Proposition 1. Let F(cid:5)
to take negative
values. Then for every f ∈ F(cid:5)
(cid:5)−(1)(t − 1),
there exists g ∈ F given by g(t) = f (t) − f
(cid:5)
such that Δf = Δg. (Here f
− denotes the left derivative of f , whose existence can be
guaranteed from the convexity of f .).

be deﬁned as F, except that we allow f ∈ F(cid:5)

The class of f -divergences includes several popular instances; these include statistical
distance, relative entropy (also known as Kullback-Leibler divergence), and Hellinger
distance. In Figure 1 we summarize the convex function used to deﬁne each of them
and we also include a simpliﬁed form, useful to compute the divergence. (In case of
negative functions, we previously apply the transformation mentioned in Proposition 1,
so that we are consistent with our deﬁnition of f -divergences.)

In general, Δf does not deﬁne a metric. The symmetry axiom might be violated and
the triangle inequality holds only if f equals a non-negative multiple of the statistical
distance. The identity of indiscernibles does not hold in general, either.

3 A Sequential Composition Theorem for f-divergences

In this section we show that the notion of α-distance used to capture differential privacy
is an f -divergence. Then we deﬁne the composition of f -divergences and show that the
sequential composition theorem of differential privacy generalizes to this setting.

3.1 An f-divergence for Approximate Differential Privacy

In [6] we introduced the concept of α-distance to succinctly capture the notion of differentially 
private computations. Given α ≥ 1, the α-distance between distributions μ1
and μ2 in D(A) is deﬁned as

Δα(μ1, μ2) def= max
E⊆A

dα(μ1(E), μ2(E))

1 Rigorously speaking, the function used for deﬁning the Kullback-Leibler divergence should
be given by f (t) = t ln(t) + t − 1 if t > 0 and f (t) = 1 if t = 0 to guarantee its continuity
at 0.

Composition Theorems and Relational Logic for f-divergences

53

where dα(a, b) def= max{a − αb, 0}. (This deﬁnition slightly departs from that of [6],
in the sense that we consider an asymmetric version of the α-distance. The original
version, symmetric, corresponds to taking dα(a, b) def= max{a− αb, b − αa, 0}). Now
we can recast the deﬁnition of differential privacy in terms of the α-distance and say
that a randomized computation c is (, δ)-differentially private iff Δe (c(x), c(y)) ≤ δ
for any two adjacent inputs x and y.

Our composition result of f -divergences builds on the observation that α-distance is
an instance of the class of f -divergences.
Proposition 2. For every α ≥ 1, the α-distance Δα(μ1, μ2) coincides with the
f -divergence ΔADα

(μ1, μ2) associated to function ADα(t) def= max{t − α, 0}.

3.2 Composition

One key property of f -divergences is a monotonicity result referred to as the data processing 
inequality [18]. In our setting, it is captured by the following proposition:
Proposition 3. Let μ1,μ2 ∈ D(A), M : A → D(B) and f ∈ F. Then

Δf (bind μ1 M, bind μ2 M ) ≤ Δf (μ1, μ2)

In comparison, the sequential composition theorem for differential privacy [16] is captured 
by the following theorem.
Theorem 1. Let μ1,μ2 ∈ D(A), M1, M2 : A → D(B) and α, α

(cid:5) ≥ 1. Then

Δαα(cid:2) (bind μ1 M1, bind μ2 M2) ≤ Δα(μ1, μ2) + max

a

Δα(cid:2) (M1(a), M2(a))

Note that the data processing inequality for α-distance corresponds to the composition
theorem for the degenerate case where M1 and M2 are equal. The goal of this paragraph
is to generalize the sequential composition theorem to f -divergences. To this end, we
ﬁrst deﬁne a notion of composability between f -divergences.
Deﬁnition 2 (f-divergence composability). Let f1, f2, f3 ∈ F. We say that (f1, f2)
is f3-composable iff for all μ1, μ2 ∈ D(A) and M1, M2 : A → D(B), there exists
μ3 ∈ D(A) such that

Δf3 (bind μ1 M1, bind μ2 M2) ≤ Δf1 (μ1, μ2) +

μ3(a)Δf2 (M1(a), M2(a))

(cid:3)

a∈A

Our notion of composability is connected to the notion of additive information measures 
from [13, Ch. 5]. To justify the connection, we ﬁrst present an adaptation of their
deﬁnition to our setting.
Deﬁnition 3 (f-divergence additivity). Let f1, f2, f3 ∈ F. We say that (f1, f2) is
f3-additive iff for all distributions μ1, μ2 ∈ D(A) and μ

(cid:5)
1, μ

2 ∈ D(B),
(cid:5)
2) ≤ Δf1 (μ1, μ2) + Δf2 (μ
(cid:5)
(cid:5)
(cid:5)
2)
1, μ
, i.e. (μ×μ
(cid:5))(a, b) def= μ(a)μ

(cid:5)

(cid:5)(b).

Δf3 (μ1 × μ

1, μ2 × μ
(cid:5)

Here, μ×μ

(cid:5)

denotes the product distribution of μ and μ

It is easily seen that composability entails additivity.

G. Barthe and F. Olmedo

54
Proposition 4. Let f1, f2, f3 ∈ F such that (f1, f2) is f3-composable. Then (f1, f2) is
f3-additive.

The f -divergences from Figure 1 present good behaviour under composition. The statistical 
distance, Hellinger distance and the Kullback-Leibler divergence are composable
w.r.t. themselves. Moreover, α-divergences are composable.

Proposition 5
• (SD, SD) is SD-composable;
• (KL, KL) is KL-composable;
• (HD, HD) is HD-composable;
• (ADα1, ADα2) is ADα1α2-composable for every α1, α2 ≥ 1.
The sequential composition theorem of differential privacy extends naturally to the class
of composable divergences.
Theorem 2. Let f1, f2, f3 ∈ F. If (f1, f2) is f3-composable, then for all μ1, μ2 ∈ D(A)
and all M1, M2 : A → D(B),

Δf3 (bind μ1 M1, bind μ2 M2) ≤ Δf1 (μ1, μ2) + max

a

Δf2 (M1(a), M2(a))

Theorem 2 will be the cornerstone for deriving the sequential composition rule of
f pRHL. (As an intermediate step, we ﬁrst show that the composition result extends
to relation liftings.)

4 Lifting

The deﬁnition of valid apRHL judgment rests on the notion of lifting. As a last step
before deﬁning our relational logic, we extend the notion of lifting to f -divergences.
One key difference between our deﬁnition and that of [6] is that the former uses two
witnesses, rather than one. In the remainder, we let supp (μ) denote the set of elements
a ∈ A such that μ(a) > 0. Moreover, given μ ∈ D(A× B), we deﬁne π1(μ) and π2(μ)
by the clauses π1(μ)(a) =
Deﬁnition 4 (Lifting). Let f ∈ F and δ ∈ R
R of a relation
R ⊆ A × B is deﬁned as follows: given μ1 ∈ D(A) and μ2 ∈ D(B), μ1 ∼f,δ
R μ2
iff there exist μL, μR ∈ D(A × B) such that: i) supp (μL) ⊆ R; ii) supp (μR) ⊆ R;
iii) π1(μL) = μ1; iv) π2(μR) = μ2 and v) Δf (μL, μR) ≤ δ. The distributions μL and
μR are called the left and right witnesses for the lifting, respectively.

a∈A μ(a, b).
0 . Then (f, δ)-lifting ∼f,δ

(cid:2)

(cid:2)

b∈B μ(a, b) and π2(μ)(b) =

+

A pleasing consequence of our deﬁnition is that the witnesses for relating two distributions 
are themselves distributions, rather than sub-distributions; this is in contrast with
our earlier deﬁnition from [6], where witnesses for the equality relation are necessarily
sub-distributions. Moreover, our deﬁnition is logically equivalent to the original one
from [15], provided δ = 0, and f satisﬁes the identity of indiscernibles. In the case
of statistical distance and α-distance, our deﬁnition also has a precise mathematical
relationship with (an asymmetric variant of) the lifting used in [6].

Composition Theorems and Relational Logic for f-divergences

55

Proposition 6. Let α ≥ 1, μ1 ∈ D(A) and μ2 ∈ D(B). If μ1 ∼ADα,δ
μ2 then there
exists a sub-distribution μ ∈ D(A × B) such that: i) supp (μ) ⊆ R; ii) π1(μ) ≤ μ1;
iii) π2(μ) ≤ μ2 and iv) Δα(μ1, π1μ) ≤ δ, where ≤ denotes the natural pointwise order
on the space of sub-distributions, i.e. μ ≤ μ
We brieﬂy review some key properties of liftings. The ﬁrst result characterizes liftings
over equivalence relations, and will be used to show that f -divergences can be characterized 
by our logic.

iff μ(a) ≤ μ

(cid:5)(a) for all a.

R

(cid:5)

Proposition 7 (Lifting of equivalence relations). Let R be an equivalence relation
over A and let μ1, μ2 ∈ D(A). Then,

μ1 ∼f,δ

R μ2 ⇐⇒ Δf (μ1/R, μ2/R) ≤ δ,

where μ/R is a distribution over the quotient set A/R, deﬁned as (μ/R)([a]) def= μ([a]).
In particular, if R is the equality relation ≡, we have

μ1 ∼f,δ≡ μ2 ⇐⇒ Δf (μ1, μ2) ≤ δ

Our next result allows deriving probability claims from lifting judgments. Given
R ⊆ A × B we say that the subsets A0 ⊆ A and B0 ⊆ B are R-equivalent, and
write A0 =R B0, iff for every a ∈ A and b ∈ B, a R b implies a ∈ A0 ⇐⇒ b ∈ B0.
Proposition 8 (Fundamental property of lifting). Let μ1 ∈ D(A), μ2 ∈ D(B), and
R ⊆ A × B. Then, for any two events A0 ⊆ A and B0 ⊆ B,

μ1 ∼f,δ

R μ2 ∧ A0 =R B0 =⇒ μ2(B0) f

(cid:4)

μ1(A0)
μ2(B0)

(cid:5)

≤ δ

Our ﬁnal result generalizes the sequential composition theorem from the previous section 
to arbitrary liftings.
Proposition 9 (Lifting composition). Let f1, f2, f3 ∈ F such that (f1, f2) is f3composable.
 Moreover let μ1 ∈ D(A), μ2 ∈ D(B), M1 : A → D(A
(cid:5)) and
M2 : B → D(B
R2 M2(b) for all a and b such
that a R b, then

R1 μ2 and M1(a) ∼f2,δ2

(cid:5)). If μ1 ∼f1,δ1

(bind μ1 M1) ∼f3,δ1+δ2

R2

(bind μ2 M2)

5 A Relational Logic for f-divergences

Building on the results of the previous section, we deﬁne a relational logic, called
f pRHL, for proving upper bounds for the f -divergence between probabilistic computations 
written in a simple imperative language.

5.1 Programming Language

We consider programs written in a probabilistic imperative language pWHILE. The
syntax of the programming language is deﬁned inductively as follows:

56

G. Barthe and F. Olmedo

C ::= skip

| V ← E
| V $← DE
|
| while E do C
| C; C

if E then C else C

nop
deterministic assignment
random assignment
conditional
while loop
sequence

Here V is a set of variables, E is a set of deterministic expressions, and DE is a set
of expressions that denote distributions from which values are sampled in random assignments.
 Program states or memories are mappings from variables to values. More
precisely, memories map a variable v of type T to a value in its interpretation (cid:2)T (cid:3). We
use M to denote the set of memories. Programs are interpreted as functions from initial
memories to sub-distributions over memories. The semantics, which is given in Figure 
2, is based on two evaluation functions (cid:2)·(cid:3)E and (cid:2)·(cid:3)DE for expressions and distribution 
expressions; these functions respectively map memories to values and memories to
sub-distributions of values. Moreover, the deﬁnition uses the operator unit, which maps
every a ∈ A to the unique distribution over A that assigns probability 1 to a and probability 
0 to every other element of A, and the null distribution μ0, that assigns probability
0 to all elements of A. Note that the semantics of programs is a map from memories to
sub-distributions over memories. Sub-distributions, rather than distributions, are used to
model probabilistic non-termination. However, for the sake of simplicity, in the current
development of the logic, we only consider programs that terminate with probability 1
on all inputs and leave the general case for future work.

(cid:2)skip(cid:3) m
(cid:2)c; c(cid:3)(cid:3) m
(cid:2)x ← e(cid:3) m
(cid:2)x $← μ(cid:3) m
(cid:2)if e then c1 else c2(cid:3) m = if ((cid:2)e(cid:3)E m = true) then ((cid:2)c1(cid:3) m) else ((cid:2)c2(cid:3) m)
(cid:2)while e do c(cid:3) m

= unit m
= bind ((cid:2)c(cid:3) m) (cid:2)c(cid:3)(cid:3)
= unit (m {(cid:2)e(cid:3)E m/x})
= bind ((cid:2)μ(cid:3)DE m) (λv. unit (m{v/x}))

((cid:2)[while e do c]n(cid:3) m f )

= λf. supn∈N

where

[while e do c]0 = if ((cid:2)e(cid:3)E m = true) then (unit m) else μ0
[while e do c]n+1 = if e then c; [while e do c]n

Fig. 2. Semantics of programs

5.2 Judgments
f pRHL judgments are of the form c1 ∼f,δ c2 : Ψ ⇒ Φ, where c1 and c2 are programs,
Ψ and Φ are relational assertions, f ∈ F and δ ∈ R
+
0 . Relational assertions are ﬁrstorder 
formulae over generalized expressions, i.e. expressions in which variables are
tagged with a (cid:13)1(cid:14) or (cid:13)2(cid:14). Relational expressions are interpreted as formulae over pairs
of memories, and the tag on a variable is used to indicate whether its interpretation

Composition Theorems and Relational Logic for f-divergences

57

should be taken in the ﬁrst or second memory. For instance, the relational assertion
x(cid:13)1(cid:14) = x(cid:13)2(cid:14) states that the values of x coincide in the ﬁrst and second memories. More
generally, we use ≡ to denote the relational assertion that states that the values of all
variables coincide in the ﬁrst and second memories.

An f pRHL judgment is valid iff for every pair of memories related by the precondition 
Ψ, the corresponding pair of output distributions is related by the (f, δ)-lifting
of the post-condition Φ.
Deﬁnition 5 (Validity in f pRHL). A judgment c1 ∼f,δ c2 : Ψ ⇒ Φ is valid, written
|= c1 ∼f,δ c2 : Ψ ⇒ Φ, iff

∀m1, m2 • m1 Ψ m2 =⇒ ((cid:2)c1(cid:3) m1) ∼f,δ

Φ

((cid:2)c2(cid:3) m2)

f pRHL judgments provide a characterization of f -divergence. Concretely, judgments
with the identity relation as post-condition can be used to derive (f, δ)-closeness results.

Proposition 10. If |= c1 ∼f,δ c2 : Ψ ⇒ ≡, then for all memories m1, m2,

m1 Ψ m2 =⇒ Δf ((cid:2)c1(cid:3) m1, (cid:2)c2(cid:3) m2) ≤ δ

Moreover, f pRHL characterizes continuity properties of probabilistic programs. We
assume a continuity model in which programs are executed on random inputs, i.e. distributions 
of initial memories, and we use f -divergences as metrics to compare program
inputs and outputs.
Proposition 11. Let f1, f2, f3 ∈ F such that (f1, f2) is f3-composable. If we have
|= c1 ∼f2,δ2 c2 : ≡ ⇒ ≡, then for any two distributions of initial memories μ1 and μ2,

Δf1 (μ1, μ2) ≤ δ1 =⇒ Δf3 (bind μ1 (cid:2)c1(cid:3), bind μ2 (cid:2)c2(cid:3)) ≤ δ1 + δ2

Finally, we can use judgments with arbitrary post-condictions to relate the probabilities 
of single events in two programs. This is used, e.g. in the context of game-based
cryptographic proofs.
Proposition 12. If |= c1 ∼f,δ c2 : Ψ ⇒ Φ, then for all memories m1, m2 and events
E1, E2,

m1 Ψ m2 ∧ E1 =Φ E2 =⇒ ((cid:2)c2(cid:3) m2)(E2) f

(cid:4)

((cid:2)c1(cid:3) m1)(E1)
((cid:2)c2(cid:3) m2)(E2)

(cid:5)

≤ δ

5.3 Proof System

Figure 3 presents a set of core rules for reasoning about the validity of an f pRHL
judgment. All the rules are transpositions of rules from apRHL [6]. However, f pRHL
rules do no directly generalize their counterparts from apRHL. This is because both
logics admit symmetric and asymmetric versions, but apRHL and f pRHL are opposite
variants: f pRHL is asymmetric and apRHL is symmetric. Refer to Section 5.4 for a
discussion about the symmetric version of f pRHL.

58

G. Barthe and F. Olmedo

∀m1, m2 • m1 Ψ m2 =⇒ (m1 {(cid:2)e1(cid:3) m1/x1}) Φ (m2 {(cid:2)e2(cid:3) m2/x2})

(cid:6) x1 ← e1 ∼f,0 x2 ← e2 : Ψ ⇒ Φ

[assn]

∀m1, m2 • m1 Ψ m2 =⇒ Δf ((cid:2)μ1(cid:3)DE m1, (cid:2)μ2(cid:3)DE m2) ≤ δ

(cid:6) x1 $← μ1 ∼f,δ x2 $← μ2 : Ψ ⇒ x1(cid:9)1(cid:10) = x2(cid:9)2(cid:10)

[rand]

Ψ =⇒ b(cid:9)1(cid:10) ≡ b(cid:3)(cid:9)2(cid:10)

(cid:6) c1 ∼f,δ c(cid:3)

1 : Ψ ∧ b(cid:9)1(cid:10) ⇒ Φ

(cid:6) c2 ∼f,δ c(cid:3)

2 : Ψ ∧ ¬b(cid:9)1(cid:10) ⇒ Φ

(cid:6) if b then c1 else c2 ∼f,δ if b(cid:3) then c(cid:3)

1 else c(cid:3)

2 : Ψ ⇒ Φ

[cond]

Θ def= b(cid:9)1(cid:10) ≡ b(cid:3)(cid:9)2(cid:10)

(f1, . . . , fn) composable and monotonic

Ψ ∧ e(cid:9)1(cid:10) ≤ 0 =⇒ ¬b(cid:9)1(cid:10)

(cid:6) c ∼f1,δ c(cid:3) : Ψ ∧ b(cid:9)1(cid:10) ∧ b(cid:3)(cid:9)2(cid:10) ∧ e(cid:9)1(cid:10) = k ⇒ Ψ ∧ Θ ∧ e(cid:9)1(cid:10) < k

(cid:6) while b do c ∼fn,nδ while b(cid:3) do c(cid:3) : Ψ ∧ Θ ∧ e(cid:9)1(cid:10) ≤ n ⇒ Ψ ∧ ¬b(cid:9)1(cid:10) ∧ ¬b(cid:3)(cid:9)2(cid:10) [while]

(cid:6) skip ∼f,0 skip : Ψ ⇒ Ψ

[skip]

(cid:6) c1 ∼f,δ c2 : Ψ ∧ Θ ⇒ Φ
(cid:6) c1 ∼f,δ c2 : Ψ ∧ ¬Θ ⇒ Φ

(cid:6) c1 ∼f,δ c2 : Ψ ⇒ Φ

[case]

[weak]

(cid:6) c1; c(cid:3)

1 ∼f2,δ2 c(cid:3)
2 : Ψ ⇒ Φ

(f1, f2) is f3-composable
(cid:6) c1 ∼f1,δ1 c2 : Ψ ⇒ Φ(cid:3) (cid:6) c(cid:3)
1 ∼f3,δ1+δ2 c2; c(cid:3)
(cid:6) c1 ∼f(cid:2),δ(cid:2) c2 : Ψ(cid:3) ⇒ Φ(cid:3)
Ψ ⇒ Ψ(cid:3) Φ(cid:3) ⇒ Φ f ≤ f(cid:3)
(cid:6) c1 ∼f,δ c2 : Ψ ⇒ Φ

δ(cid:3) ≤ δ

2 : Φ(cid:3)⇒ Φ

[seq]

Fig. 3. Core proof rules

We brieﬂy describe some main rules, and refer the reader to [6] for a longer description 
about each of them. Rule [seq] relates two sequential compositions and is a direct
consequence from the lifting composition (see Proposition 9). Rule [while] relates two
loops that terminate in lockstep. The bound depends on the maximal number of iterations 
of the loops, and we assume given a loop variant e that decreases at each iteration,
and is initially upper bounded by some constant n. We brieﬂy explain the side condi-
tions: (f1, . . . , fn) is composable iff (fi, f1) is fi+1-composable for every 1 ≤ i < n.
Moreover, (f1, . . . , fn) is monotonic iff fi ≤ fi+1 for 1 ≤ i < n. Note that the rule is
given for n ≥ 2; specialized rules exist for n = 0 and n = 1. This rule readily specializes 
to reason about (, δ)-differential privacy by taking fi = ADαi, where α = e.

If an f pRHL judgment is derivable using the rules of Figure 3, then it is valid. Formally,

Proposition 13 (Soundness). If (cid:16) c1 ∼f,δ c2 : Ψ ⇒ Φ then |= c1 ∼f,δ c2 : Ψ ⇒ Φ.

5.4 Symmetric Logic

One can also deﬁne a symmetric version of the logic by adding as an additional clause
in the deﬁnition of the lift relation that Δf (μR, μL) ≤ δ. An instance of this logic is the
symmetric apRHL logic from [6]. All rules remain unchanged, except for the random

Composition Theorems and Relational Logic for f-divergences

59

sampling rule that now requires the additional inequality to be checked in the premise of
the rule.

6 Conclusion

This paper makes two contributions: ﬁrst, it unveils a connection between differential 
privacy and f -divergences. Second, it lays the foundations for reasoning about
f -divergences between randomized computations. As future work, we intend to implement 
support for f pRHL in EasyCrypt [4], and formalize the results from [24].
We also intend to investigate the connection between our notion of lifting and ﬂow
networks.

Acknowledgments. This work was partially funded by the European Projects FP7256980 
NESSoS and FP7-229599 AMAROUT, Spanish project TIN2009-14599
DESAFIOS 10 and Madrid Regional project S2009TIC-1465 PROMETIDOS.

References

1. Abate, A.: Approximation metrics based on probabilistic bisimulations for general statespace 
markov processes: a survey. Electronic Notes in Theoretical Computer Sciences (2012)
(in print)

2. Ali, S.M., Silvey, S.D.: A general class of coefﬁcients of divergence of one distribution from
another. Journal of the Royal Statistical Society. Series B (Methodological) 28(1), 131–142
(1966)

3. Alvim, M.S., Andr´es, M.E., Chatzikokolakis, K., Palamidessi, C.: On the relation between
differential privacy and Quantitative Information Flow. In: Aceto, L., Henzinger, M., Sgall,
J. (eds.) ICALP 2011, Part II. LNCS, vol. 6756, pp. 60–76. Springer, Heidelberg (2011)

4. Barthe, G., Gr´egoire, B., Heraud, S., B´eguelin, S.Z.: Computer-aided security proofs for the
working cryptographer. In: Rogaway, P. (ed.) CRYPTO 2011. LNCS, vol. 6841, pp. 71–90.
Springer, Heidelberg (2011)

5. Barthe, G., Gr´egoire, B., Zanella-B´eguelin, S.: Formal certiﬁcation of code-based cryptographic 
proofs. In: 36th ACM SIGPLAN-SIGACT Symposium on Principles of Programming 
Languages, POPL 2009, pp. 90–101. ACM, New York (2009)

6. Barthe, G., K¨opf, B., Olmedo, F., Zanella-B´eguelin, S.: Probabilistic relational reasoning for
differential privacy. In: 39th ACM SIGPLAN-SIGACT Symposium on Principles of Programming 
Languages, POPL 2012, pp. 97–110. ACM, New York (2012)

7. Chaudhuri, S., Gulwani, S., Lublinerman, R., Navidpour, S.: Proving programs robust. In:
19th ACM SIGSOFT Symposium on the Foundations of Software Engineering and 13rd
European Software Engineering Conference, ESEC/FSE 2011, pp. 102–112. ACM, New
York (2011)

8. Cortes, C., Mohri, M., Rastogi, A.: Lp distance and equivalence of probabilistic automata.

Int. J. Found. Comput. Sci. 18(4), 761–779 (2007)

9. Cortes, C., Mohri, M., Rastogi, A., Riley, M.: On the computation of the relative entropy of

probabilistic automata. Int. J. Found. Comput. Sci. 19(1), 219–242 (2008)

10. Csisz´ar, I.: Eine informationstheoretische ungleichung und ihre anwendung auf den beweis
der ergodizitat von markoffschen ketten. Publications of the Mathematical Institute of the
Hungarian Academy of Science 8, 85–108 (1963)

60

G. Barthe and F. Olmedo

11. Deng, Y., Du, W.: Logical, metric, and algorithmic characterisations of probabilistic bisimulation.
 Tech. Rep. CMU-CS-11-110, Carnegie Mellon University (March 2011)

12. Dwork, C.: Differential privacy. In: Bugliesi, M., Preneel, B., Sassone, V., Wegener, I. (eds.)

ICALP 2006. LNCS, vol. 4052, pp. 1–12. Springer, Heidelberg (2006)

13. Ebanks, B., Sahoo, P., Sander, W.: Characterizations of Information Measures. World Scientiﬁc 
(1998)

14. Gaboardi, M., Haeberlen, A., Hsu, J., Narayan, A., Pierce, B.C.: Linear dependent types for
differential privacy. In: 40th ACM SIGPLAN–SIGACT Symposium on Principles of Programming 
Languages, POPL 2013, pp. 357–370. ACM, New York (2013)

15. Jonsson, B., Yi, W., Larsen, K.G.: Probabilistic extensions of process algebras. In: Bergstra,
J., Ponse, A., Smolka, S. (eds.) Handbook of Process Algebra, pp. 685–710. Elsevier, Amsterdam 
(2001)

16. McSherry, F.: Privacy integrated queries: an extensible platform for privacy-preserving data

analysis. Commun. ACM 53(9), 89–97 (2010)

17. McSherry, F.D.: Privacy integrated queries: an extensible platform for privacy-preserving
data analysis. In: 35th SIGMOD International Conference on Management of Data, SIGMOD 
2009, pp. 19–30. ACM, New York (2009)

18. Pardo, M., Vajda, I.: About distances of discrete distributions satisfying the data processing
theorem of information theory. IEEE Transactions on Information Theory 43(4), 1288–1293
(1997)

19. Pierce, B.C.: Differential privacy in the programming languages community. Invited Tutorial
at DIMACS Workshop on Recent Work on Differential Privacy Across Computer Science
(2012)

20. Di Pierro, A., Hankin, C., Wiklicky, H.: Measuring the conﬁnement of probabilistic systems.

Theor. Comput. Sci. 340(1), 3–56 (2005)

21. Reed, J., Pierce, B.C.: Distance makes the types grow stronger: a calculus for differential privacy.
 In: 15th ACM SIGPLAN International Conference on Functional programming, ICFP
2010, pp. 157–168. ACM, New York (2010)

22. Roy, I., Setty, S.T.V., Kilzer, A., Shmatikov, V., Witchel, E.: Airavat: security and privacy for
MapReduce. In: 7th USENIX Conference on Networked Systems Design and Implementation,
 NSDI 2010, pp. 297–312. USENIX Association, Berkeley (2010)

23. Segala, R., Turrini, A.: Approximated computationally bounded simulation relations for
probabilistic automata. In: 20th IEEE Computer Security Foundations Symposium, CSF
2007, pp. 140–156. IEEE Computer Society (2007)

24. Steinberger, J.: Improved security bounds for key-alternating ciphers via hellinger distance.

Cryptology ePrint Archive, Report 2012/481 (2012), http://eprint.iacr.org/

25. Tracol, M., Desharnais, J., Zhioua, A.: Computing distances between probabilistic automata.

In: Proceedings of QAPL. EPTCS, vol. 57, pp. 148–162 (2011)

