Reasoning about Recursive Probabilistic Programs ∗

Federico Olmedo

Benjamin Lucien Kaminski

Joost-Pieter Katoen

Christoph Matheja

{federico.olmedo, benjamin.kaminski, katoen, matheja}@cs.rwth-aachen.de

RWTH Aachen University, Germany

6
1
0
2

 
r
a

M
9

 

 
 
]

O
L
.
s
c
[
 
 

1
v
2
2
9
2
0

.

3
0
6
1
:
v
i
X
r
a

Abstract
This paper presents a wp–style calculus for obtaining expectations
on the outcomes of (mutually) recursive probabilistic programs. We
provide several proof rules to derive one– and two–sided bounds
for such expectations, and show the soundness of our wp–calculus
with respect to a probabilistic pushdown automaton semantics. We
also give a wp–style calculus for obtaining bounds on the expected
runtime of recursive programs that can be used to determine the
(possibly inﬁnite) time until termination of such programs.

Categories and Subject Descriptors F.3.1 [Logics and Meaning
of Programs]: Specifying and Verifying and Reasoning about Programs.


Keywords
ﬁcation · weakest pre–condition calculus · expected runtime.

recursion · probabilisitic programming · program veriIntroduction


1.
Uncertainty is nowadays more and more pervasive in computer science.
 Applications have to process inexact data from, e.g., unreliable 
sources such as wireless sensors, machine learning methods,
or noisy biochemical reactors. Approximate computing saves resources 
such as e.g. energy by sacriﬁcing “strict” correctness for
applications like image processing that can tolerate some defects in
the output by running them on unreliable hardware, circuits that every 
now and then (deliberately) produce incorrect results [4]. Probabilistic 
programming [28] is a key technique for dealing with uncertainty.
 Put in a nutshell, a probabilistic program takes a (prior)
probability distribution as input and obtains a (posterior) distribution.
 Probabilistic programs are not new at all; they have been investigated 
by Kozen [20] and others in the early eighties. In the last
years, the interest in these programs has rapidly grown. In particular,
 the incentive by the AI community to use probabilistic programs 
for describing complex Bayesian networks has boosted the
ﬁeld of probabilistic programming [10]. Probabilistic programs are
used in, amongst others, machine learning, systems biology, security,
 planning and control, quantum computing, and software–
deﬁned networks. Indeed almost all programming languages, ei-

∗ This work was supported by the Excellence Initiative of the German
federal and state government.

[Copyright notice will appear here once ’preprint’ option is removed.]

ther being functional, object–oriented, logical, or imperative, in the
meanwhile have a probabilistic variant.

This paper focuses on recursive probabilistic programs. Recursion 
in Bayesian networks where a variable associated with a particular 
domain entity can depend probabilistically on the same variable 
associated to a different entity, is “common and natural” [29].
Recursive probability models occur in gene regulatory networks
that describe (possibly recursive) rule–based dependencies between
genes. Finally, programs describing randomized algorithms are often 
recursive by nature. “Sherwood” algorithms exploit randomization 
to increase efﬁciency by avoiding or reducing the probability
of worst–case behavior. Varying quicksort by selecting the pivot
randomly (rather than doing this deterministically) avoids very uneven 
splits of the input array. Its worst–case runtime is the same as
the average–case runtime of Hoare’s deterministic quicksort since
the likelihood of obtaining a quadratic worst–case is signiﬁcantly
lowered [24, Sec. 2.5]. A “Sherwood” variant of binary search
splits the input array at a random position, and yields a similar
effect—expected runtimes of worst–, average– and best–case are
aligned [21, Sec. 11.4.4]. “Sherwood” techniques are also useful in
selection, median ﬁnding, and hashing (such as Bloom ﬁlters).

The purpose of this paper is to provide a framework for enabling
formal reasoning about recursive probabilistic programs. This rigorous 
reasoning is important to prove the correctness of such programs.
 This includes statements about the expected outcomes of
recursive probabilistic programs, as well as assertions about their
termination probability. These are challenging problems. For instance,
 consider the (at ﬁrst sight simple) recursive program:

Prec3

⊲ {skip} [1/2] {call Prec3; call Prec3; call Prec3 }

which terminates immediately with probability 1/2 or invokes itself
three times otherwise. It turns out that this program terminates with
(irrational) probability √5−1
2 —the reciprocal of the golden ratio.
Correctness proofs of the “Sherwood” versions of quicksort and
binary search do exist but typically rely on mathematical ad–hoc
reasoning about expected values. The aim of this paper is to enable
such proofs by means of formal veriﬁcation of the algorithm itself.
Besides correctness, our interest is in analyzing the expected
runtime of recursive probabilistic programs in a rigorous manner.
This enables obtaining insight in their efﬁciency and moreover provides 
a method to show whether the expected time until termination 
is ﬁnite or inﬁnite—a crucial difference for probabilistic programs 
[9, 16]. Again, analyses of expected runtimes of recursive
randomized algorithms do exist using standard mathematics [24,
Sec. 2.5], probabilistic recurrence relations [19], or dedicated techniques 
for divide–and–conquer algorithms [6], usually taking for
granted—far from trivial—relationships between the underlying
random variables. Here the aim is to do this from ﬁrst principles
by formal veriﬁcation techniques, directly on the algorithm.

To accomplish these goals, this paper presents two weakest pre–
condition–style calculi for reasoning about recursive probabilistic
programs. The ﬁrst calculus is an extension of McIver and MorReasoning 
about Recursive Probabilistic Programs

1

2016/3/10

gan’s calculus [23] for non–recursive programs and enables obtaining 
expectations on the outcomes of (mutually) recursive probabilistic 
programs. Compared to an existing extension with recursion 
[22], our approach provides a clear separation between syntax
and semantics. We prove the soundness of our wp–calculus with
respect to a probabilistic pushdown automaton semantics. This is
complemented by a set of proof rules to derive one– and two–sided
bounds for expected outcomes of recursive programs. We illustrate
the usage of these proof rules by analyzing the termination probability 
of the example program above. Subsequently, we provide a
variant of our wp–style calculus for obtaining bounds on the expected 
runtime of probabilistic programs. This extends our recent
approach [17] towards treating recursive programs. The application 
of this calculus includes proving positive almost–sure termination,
 i.e., does a program terminate with probability one in ﬁnite
expected time? Our framework enables (in a very succinct way) establishing 
a (well–known) relationship between the expected runtime 
of a probabilistic program with its termination behavior: If
an (abort–free) program has ﬁnite expected runtime, then it terminates 
almost–surely. We provide a set of proof rules for expected
runtimes and show the applicability of our approach by proving
several correctness properties as well as the expected runtime of
the ‘Sherwood’ variant of binary search.

Organization of the paper. Section 2 presents our probabilistic
programming language with recursion. Section 3 presents the wp–
style semantics for reasoning about program correctness. Section 4
introduces several proof rules for reasoning about the correctness of
recursive programs. Section 5 presents the expected runtime transformer 
together with proof rules for recursive programs. Section 6
describes an operational probabilistic pushdown automata semantics 
and relates it to the wp–style semantics. Section 7 discusses
some extensions of the results presented in the previous sections.
Section 8 presents a detailed analysis of the ‘Sherwood’ variant
of binary search. Finally, Section 9 discusses related work and
Section 10 concludes. Detailed proofs are provided in the appendix,
which is added for the convenience of the reviewer, and will not be
part of the ﬁnal version (if accepted).

2. Programming Model
To model our probabilistic recursive programs we consider a simple
imperative language `a la Dijkstra’s Guarded Command Language
(GCL) [7] with two additional features: First, a (binary) probabilisitic 
choice operator to endow our programs with a probabilistic
behavior. For instance, the program

{x := x+1} [1/3] {x := x−1}

either increases x with probability 1/3 or decreases it with probability 
2/3 = 1 − 1/3. Second, we allow for procedure calls. For
simplicity, our development assumes the presence of only a single
procedure, say P. We defer the treatment of multiple (possibly mutually 
recursive) procedures to Section 7.

Formally, a command of our language, coined pRGCL, is deﬁned 
by the following grammar:

C ::= skip

if (E ) {C} else {C}

| V := E
| abort
|
| {C} [p] {C}
|
| C; C

call P

no–op
assignment
abortion
conditional branching
probabilistic choice
procedure call
sequential composition

S be the set of program states. Finally, we also assume an interpretation 
function JE K for expressions that maps program states to
values.

No–op, assignments, conditionals and sequential composition
are standard. {c1} [p] {c2} represents a probabilistic choice: it
behaves as c1 with probability p and as c2 with probability 1−p.
Finally call P makes a (possibly recursive) call to procedure P.

For our development we assume that procedure P manipulates
the global program state and we thus dispense with parameters and
return statements for passing information across procedure calls.
The declaration of P consists then of its body and we use P ⊲ c
to denote that c ∈ C is the body of P. We say that a command is
closed if it contains no procedure calls.

A pRGCL program is then given by a pair hc, Di, where c ∈ C is
the “main” command and D : {P } → C is the declaration of P.1 In
order not to clutter the notation, when c is closed we simply write
c for program hc, Di, for any declaration D.
Example 1. To illustrate the use of our language consider the following 
declaration of a (faulty) recursive procedure for computing
the factorial of a natural number stored in x:

Pfact ⊲ if (x ≤ 0) {y := 1} else

(cid:8) {x := x−1; call Pfact; x := x+1} [5/6]
{x := x−2; call Pfact; x := x+2}; y := y · x(cid:9)

In each recursive call x is decreased either by one or two, with
probability 5/6 and 1/6, respectively. Therefore some factors might
be missing in the computation of the factorial of x.
△
As a ﬁnal remark, observe that the language does not support
guarded loops in a native way because they can be simulated. Concretely,
 the usual guarded loop while (E) do {c} is simulated by the
recursive procedure Pwhile ⊲ if (E) {c; call Pwhile} else {skip}.

3. Weakest Pre–Expectation Semantics
Inspired by Kozen [20], McIver and Morgan [22] generalized Dijk-
stra’s weakest pre–condition semantics to (a variant of) pRGCL.
In particular, they deﬁned the semantics of recursive programs
using ﬁxed point techniques. In this section we present a different
approach where the behavior of a recursive program is deﬁned as
the limit of its ﬁnite approximations (or truncations) and prove it
equivalent to their deﬁnition based on ﬁxed points.

3.1 Deﬁnition
The wp-semantics over pRGCL generalizes Dijkstra’s weakest precondition 
semantics over GCL twofold: First, instead of being predicates 
over program states, pre– and post–conditions are now (non–
negative) real–valued functions over program states. Secondly, instead 
of merely evaluating a (boolean–valued) post–condition in the
ﬁnal state(s) of a program, we now measure the expected value of
a (real–valued) post–condition w.r.t. the distribution of ﬁnal states.
Formally, if f : S → R≥0 we let

wp[c, D](f ) , λs • EJc,DK(s) (f ) ,

where Jc, DK(s) denotes the distribution of ﬁnal states from executing 
hc, Di in initial state s and EJc,DK(s) (f ) denotes the expected

value of f w.r.t. the distribution of ﬁnal states Jc, DK(s). Consider
for instance program

ccoins : {x := 0} [1/2] {x := 1}; {y := 0} [1/3] {y := 1}

that ﬂips a pair of fair and biased coins. We have

wp[ccoins](f ) = λs •

1

6 f (s[x,y/0,0]) + 1

3 f (s[x, y/0, 1])

We assume a set V of program variables and a set E of expressions
over program variables. As usual, we assume that program states
are variable valuations, i.e. mappings from variables to values; let

1 We chose the declaration of P to be a mapping from a singleton and not
the mere body of P because this minimizes the changes to accommodate
the subsequent treatment to multiple procedures.

Reasoning about Recursive Probabilistic Programs

2

2016/3/10

+ 1

6 f (s[x,y/1,0]) + 1

3 f (s[x, y/1, 1]) ,

where s[x1, . . . , xn/v1, . . . , vn] represents the state obtained by
updating in s the value of variables x1, . . . , xn to v1, . . . , vn,
respectively. As above, when c is closed, we usually write wp[c]
instead of wp[c, D], as a declaration D plays no role.

Observe that, in particular, if [A] denotes the indicator function 
of a predicate A over program states, wp[c, D]([A])(s) gives
the probability of (terminating and) establishing A after executing
hc, Di from state s. For instance we can determine the probability
that the above program ccoins establishes x = y from state s through

6 · 1 + 1

3 · 0 + 1

6 · 0 + 1

wp[ccoins]([x=y])(s) = 1
Moreover, for a deterministic program c that from state s terminates 
in state s′, Jc, DK(s) is the Dirac distribution that concentrates

3 · 1 = 1
2 .

all its mass in s′ and wp[c, D](cid:0)[A](cid:1)(s) reduces to 1 · [A] (s′), which

gives 1 if s′ |= A and 0 otherwise. This yields the classical weakest
pre–condition semantics of ordinary sequential programs.

To reason about partial program correctness, pRGCL also admits 
a liberal version of the transformer wp[ · ], namely wlp[ · ]. In
the same vein as for ordinary sequential programs, wp[c, D]([A])(s)
gives the probability that program hc, Di terminates and establishes
event A from state s, while wlp[c, D]([A])(s) gives the probability
that hc, Di terminates and establishes A, or diverges.

Formally, the transformer wp operates on unbounded, so–called
expectations in E , {f | f : S → [0, ∞]}, while the transformer
wlp operates on bounded expectations in E≤1 , {f | f : S →
[0, 1]}. Our expectation transformers have thus type wp[ · ] : E →
E and wlp[ · ] : E≤1 → E≤1.2 In the probabilistic setting pre– and
post–conditions are thus referred to as pre– and post–expectations.
Notation. We use boldface for constant expectations, e.g. 1 denotes 
the constant expectation λs • 1. Given an arithmetical expression 
E over program variables we write E for the expectation that
in states s returns JEK(s). Given a Boolean expression G over program 
variables let [G] denote the {0, 1}–valued expectation that on
state s returns 1 if JGK(s) = true and 0 if JGK(s) = false. Finally,
given variable x, expression E and expectation f we use f [x/E]
to denote the expectation that on state s returns f (s[x/JEK(s)]).
Moreover, “(cid:22)” denotes the pointwise order between expectations,
i.e. f1 (cid:22) f2 iff f1(s) ≤ f2(s) for all states s ∈ S.

3.2 Inductive Characterization
McIver and Morgan [22] showed that the expectation transformers
wp and wlp can be deﬁned by induction on the program’s structure.
We now recall their result, taking an alternative approach to handle
recursion: While McIver and Morgan use ﬁxed point techniques,
we follow e.g. Hehner [12] and deﬁne the semantics of a recursive
procedure as the limit of an approximation sequence. We believe
that this approach is sometimes more intuitive and closer to the
operational view of programs.

In the same way as the semantics of loops is deﬁned as the
limit of their ﬁnite unrollings, we deﬁne the semantics of recursive
procedures as the limit of their ﬁnite inlinings. Formally, the nth 
inlining callDn P of procedure P w.r.t. declaration D is deﬁned
inductively by

callD0 P = abort

callDn+1 P = D(P )(cid:2)call P/callDn P(cid:3) ,

where c[call P /c′] denotes the syntactic replacement of every occurrence 
of call P in c by c′.3 The family of commands callDn P

2 The transformer wlp is well–typed because wlp[c, D](f )(s) ≤
sups′ f (s′) for every state s.
3 The formal deﬁnition of this syntactic replacement proceeds by a routine
induction on the structure of c; see Figure 7 in Section A.4 for details.

c

wp[c, D](f )

skip
x := E
abort
if (G) {c1} else {c2}
{c1} [p] {c2}
call P

c1; c2

c

abort
call P

f
f[x/E]
0
[G] · wp[c1, D](f ) + [¬G] · wp[c2, D](f )
p · wp[c1, D](f ) + (1−p) · wp[c2, D](f )
supn wp[callDn P ](f )

wp[c1, D](cid:0)wp[c2, D](f )(cid:1)

wlp[c, D](f )

1
inf n wlp[callDn P ](f )

Figure 1. Expectation transformer semantics of pRGCL programs.
The wlp[ · ] transformer follows the same rules as wp[ · ], expect for
abort and procedure calls. Sum, product, supremum and inﬁmum
over expectations are all deﬁned pointwise.

deﬁne a sequence of approximations to call P where callD0 P is the
“poorest” approximation, while the larger the n, the more precise
the approximation becomes. Observe that, in general, callDn+1 P
mimics the exact behavior of call P for all executions that ﬁnish
after at most n recursive calls.

The expectation transformer semantics over pRGCL is provided
in Figure 1. The action of transformers on procedure calls is deﬁned
as the limit of their action over the n-th inlining of the procedures.
For the rest of the language constructs, we follow McIver and Morgan 
[22]. Let us brieﬂy explain each of the rules. wp[skip, D] behaves 
as the identity since skip has no effect. The pre–expectation
of an assignment is obtained by updating the program state and
then applying the post–expectation, i.e. wp[x := E, D] takes post–
expectation f to pre–expectation f [x/E] = λs • f (s[x/JEK(s)]).
wp[abort, D] maps any post–expectation to the constant pre–
expectation 0. Observe that expectation 0 is the probabilistic
counterpart of predicate false. wp[if (G) {c1} else {c2}, D] behaves
either as wp[c1, D] or wp[c2, D] according to the evaluation of
G. wp[{c1} [p] {c2}, D] is obtained as a convex combination of
wp[c1, D] and wp[c2, D], weighted according to p. wp[call P , D]
behaves as the limit of wp on the sequence of ﬁnite truncations
(or inlinings) of P. We take the supremum because the sequence
is increasing. Observe that we advertently include no declaration
in wp[callDn P ](f ) because callDn P is a closed command for every
n. Finally, wp[c1; c2, D] is obtained as the functional composition 
of wp[c1, D] and wp[c2, D]. The wlp transformer follows the
same rules as wp, except for the abort statement and procedure
calls. wlp[abort, D] takes any post–expectation to pre–expectation
1. (Expectation 1 is the probabilistic counterpart of predicate true.)
wlp[call P , D] also behaves as the limit of wlp on the sequence of
ﬁnite truncations of P. This time we take the inﬁmum because the
sequence is decreasing.
Example 2. Reconsider ccoins = c1; c2 from Section 3.1 with

c1 : {x := 0} [1/2] {x := 1} and c2 : {y := 0} [1/3] {y := 1} .

We use our weakest pre–expectation calculus to formally determine
the probability that the outcome of the two coins coincide:

wp[ccoins]([x=y])

= wp[c1](cid:0)wp[c2]([x=y])(cid:1)
= wp[c1](cid:0) 1
= wp[c1](cid:0) 1
2 · wp[x := 0](cid:0) 1

= 1

3 · wp[y := 0]([x=y]) + 2
3 · [x=0] + 2

3 · [x=1](cid:1)

3 · [x=0] + 2

3 · wp[y := 1]([x=y])(cid:1)

3 · [x=1](cid:1)

Reasoning about Recursive Probabilistic Programs

3

2016/3/10

3 · [x=0] + 2

+ 1

2 · wp[x := 1](cid:0) 1
2 ·(cid:0) 1

3 · [0=0] + 2
3 + 1
3 = 1

2 · 1

= 1
= 1
△
The transformers wp and wlp enjoy several appealing algebraic

3 · [0=1](cid:1) + 1

3 · [1=1](cid:1)

3 · [1=0] + 2

2 · 2

3 · [x=1](cid:1)
2 ·(cid:0) 1

2

properties, which we summarize below.
Lemma 3.1 (Basic properties of w(l)p). For every program hc, Di,
every f1, f2, and increasing ω–chain f0 (cid:22) f1 (cid:22) · · · in E, g1, g2,
and every decreasing ω–chain g0 (cid:23) g1 (cid:23) · · · in E≤1, and scalars
α1, α2 ∈ R≥0 it holds:
Continuity:

supn wp[c, D](fn) = wp[c, D](supn fn)
inf n wlp[c, D](gn) = wlp[c, D](inf n gn)

Monotonicity:

Linearity:

f1 (cid:22) f2 =⇒ wp[c, D](f1) (cid:22) wp[c, D](f2)
g1 (cid:22) g2 =⇒ wlp[c, D](g1) (cid:22) wlp[c, D](g2)

wp[c, D](α1 · f1 + α2 · f2)
= α1 · wp[c, D](f1) + α2 · wp[c, D](f2)

Preserv. of 0,1: wp[c, D](0) = 0 and wlp[c, D](1) = 1

Proof. See Appendix A.1.

Program termination. Since the termination behavior of a program 
is given by the probability that it establishes true, we can
readily use the transformer wp to reason about program termination.
 It sufﬁces to consider the weakest pre–expectation of
the program w.r.t. post–expectation [true] = 1. Said otherwise,
wp[c, D](1)(s) gives the termination probability of program hc, Di
from state s. In particular, if the program terminates with probability 
1, we say that it terminates almost–surely.

3.3 Characterization based on Fixed Points
Next we use a continuity argument on the transformer w(l)p to
prove that its action on recursive procedures can also be deﬁned
using ﬁxed point techniques. This alternative characterization rests
on a subsidiary transformer w(l)p[ · ]♯
θ, which is a slight variant
of w(l)p[ · ]. The main difference between these transformers is
the mechanism that they use to give semantics to procedure calls:
w(l)p[ · ] relies on a declaration D, while w(l)p[ · ]♯
θ relies on a so–
called (liberal) semantic environment θ : E → E (θ : E≤1 → E≤1)
which is meant to directly encode the semantics of procedure calls.
Then w(l)p[call P ]♯
θ(f ) gives θ(f ), while for all other program
constructs c, w(l)p[c]♯
θ(f ) agrees with w(l)p[c](f ); see Figure 8
in Section A.2 for details. For technical reasons, in the remainder
of our development we will consider only continuous semantic
environments in SEnv , {f | f : E → E is upper continuous}
and LSEnv , {f | f : E≤1 → E≤1 is lower continuous}.4 This
is a natural assumption since we are interested only in semantic
environments that are obtained as the w(l)p–semantics of a pRGCL
program, which are continuous by Lemma 3.1.

The semantics of recursive procedures can now be readily given

as the ﬁxed point of a semantic environment transformer.
Theorem 3.1 (Fixed point characterization for procedure calls).
Given a declaration D : {P } → C for procedure P,

wp[call P , D] = lfp⊑(cid:16)λθ : SEnv • wp[D(P )]♯
θ(cid:17)
wlp[call P , D] = gfp⊑(cid:16)λθ : LSEnv • wlp[D(P )]♯
θ(cid:17) .

Proof. See Appendix A.2.

4 A (liberal) semantic environment θ is upper (lower) continuous iff for
every increasing ω-chain f0 (cid:22) f1 (cid:22) · · · (decreasing ω-chain f0 (cid:23) f1 (cid:23)
· · · ), supn θ(fn) = θ(supn fn) (inf n θ(fn) = θ(inf n fn)).

Reasoning about Recursive Probabilistic Programs

4

The ﬁxed points above are taken w.r.t. the pointwise order “⊑”
over semantic environments: given θ1, θ2 ∈ SEnv (resp. θ1, θ2 ∈
LSEnv), θ1 ⊑ θ2 iff θ1(f ) (cid:22) θ2(f ) for all f ∈ E (resp. f ∈ E≤1).
Theorem 3.1 reveals an inherent difference between the complexities 
of reasoning about loops and general recursion: The semantics 
of loops can be given as the ﬁxed point of an expectation
transformer (see e.g. [25]), while the semantics of recursion requires 
the ﬁxed point of a (higher order) environment transformer.
This fact was already noticed by Dijkstra [7, p. xvii] and later on
conﬁrmed by Nelson [26, p. 517] for non–probabilisitic programs.

4. Correctness of Recursive Programs
In this section we introduce some proof rules for effectively reasoning 
about the behavior of recursive programs. For that we require
the notion of constructive derivability. Given logical formulae A
and B, we use A (cid:13) B to denote that B can be derived assuming
A. In particular, we will consider claims of the form

w(l)p[call P ](f1) ⊲⊳ g1 (cid:13) w(l)p[c](f2) ⊲⊳ g2 ,

where ⊲⊳∈ {(cid:22), (cid:23)}, f1, g1 give the speciﬁcation of call P and f2, g2
the speciﬁcation of c. Notice that in such a claim we omit any
procedure declaration as the derivation is independent of P ’s body.
Our ﬁrst two rules are extensions of well–known rules for ordinary 
recursive programs (see e.g. [14]) to a probabilistic setting:

wp[call P ](f ) (cid:22) g (cid:13) wp[D(P )](f ) (cid:22) g

wp[call P , D](f ) (cid:22) g

g (cid:22) wlp[call P ](f ) (cid:13) g (cid:22) wlp[D(P )](f )

g (cid:22) wlp[call P , D](f )

[wp-rec]

[wlp-rec]

So for proving that a procedure call satisﬁes a speciﬁcation (given
by f, g), it sufﬁces to show that the procedure’s body satisﬁes the
speciﬁcation, assuming that the recursive calls in the body do, too.
Example 3. Reconsider the procedure Prec3 with declaration

D(Prec3) : {skip} [1/2] {call Prec3 ; call Prec3; call Prec3}

presented in the introduction. We prove that it terminates with
probability at most ϕ =
from any initial state. Formally,
this is captured by wp[call P , D](1) (cid:22) ϕ. To prove this, we apply
rule [wp-rec]. We must then establish the derivability claim

√5−1

2

wp[call P ](1) (cid:22) ϕ (cid:13) wp[D(Prec3)](1) (cid:22) ϕ .

The derivation goes as follows:

=

=

(cid:22)

=

(cid:22)

=

(cid:22)

=

wp[D(Prec3 )](1)
{def. of wp}

1

2 · wp[skip](1) + 1

2 · wp[call Prec3 ; call Prec3; call Prec3](1)

{def. of wp}

1

2 + 1

1

2 + 1

2 · wp[call Prec3; call Prec3](cid:0)wp[call Prec3](1)(cid:1)

{assumption, monot. of wp}
2 · wp[call Prec3; call Prec3 ](ϕ)
{def. of wp, scalab. of wp twice}

2 ϕ · wp[call Prec3](cid:0)wp[call Prec3](1)(cid:1)

1

2 + 1

1

2 + 1

{assumption, monot. of wp}
2 ϕ · wp[call Prec3](ϕ)
{scalab. of wp}
2 ϕ2 · wp[call Prec3](1)
{assumption, monot. of wp}
2 ϕ3
{algebra}

1

2 + 1

1

2 + 1

ϕ

△

2016/3/10

An appealing feature of our approximation semantics is that to
prove the following soundness result we do not need to resort to a
continuity argument on the expectation transformers.
Theorem 4.1 (Soundness of rules [w(l)p-rec]). Rules [wp-rec] and
[wlp-rec] are sound w.r.t. the w(l)p semantics in Figure 1.

c

skip
x := E
abort

ert[c, D](t)

1 + t
1 + t[x/E]
0

Proof. See Appendix A.3.

Rules [w(l)p-rec] allow deriving only one–sided bounds for the
weakest (liberal) pre–expectation of a procedure call. It is also
possible to derive two–sided bounds by means of the following
rules:

l0 = 0,

u0 = 0,

ln (cid:22) wp[call P ](f ) (cid:22) un (cid:13) ln+1 (cid:22) wp[D(P )](f ) (cid:22) un+1

supn ln (cid:22) wp[call P , D](f ) (cid:22) supn un

l0 = 1,

u0 = 1,

[wp-recω ]

ln (cid:22) wlp[call P ](f ) (cid:22) un (cid:13) ln+1 (cid:22) wlp[D(P )](f ) (cid:22) un+1

inf n ln (cid:22) wlp[call P , D](f ) (cid:22) inf n un

[wlp-recω]

In constrast to rules [w(l)p-rec], these rules require exhibiting
two sequences of expectations hlni and huni rather than a single
expectation g to bound the weakest (liberal) pre–expectation of
a procedure call. Intuitively ln (un) represents a lower (upper)
bound for the weakest pre–expectation of the n-inlining of the
procedure, i.e. from the premises of the rules we will have ln (cid:22)
w(l)p[callDn P ](f ) (cid:22) un for all n ∈ N.

Observe that both rules can be specialized to reason about one–
sided bounds. For instance, by setting un+1 = ∞ in [wp-recω]
we can reason about lower bounds of wp[call P , D](f ), which is
not supported by rule [wp-rec]. Similarly, by taking ln = 0 in rule
[wlp-recω] we can reason about upper bounds of wlp[call P , D](f ).
Example 4. Reconsider the procedure Prec3 from Example 3. Now
we prove that the procedure terminates with probability at least
from any initial state. To this end, we rely on the
ϕ =
fact that ϕ can be characterized by the asymptotic behavior of the
sequence hϕni, where ϕ0 = 0 and ϕn+1 = 1
n. In symbols,
ϕ = supn ϕn. We wish then to prove that

√5−1

2 + 1

2 ϕ3

2

if (G) {c1} else {c2}
{c1} [p] {c2}

1 + [G] · ert [c1, D](t) + [¬G] · ert [c2, D](t)
p · ert [c1, D](t) + (1−p) · ert [c2, D](t)

call P

lfp⊑ (cid:16)λη : RtEnv • 1 ⊕ ert [D(P )]♯
ert [c1, D](cid:0)ert [c2, D](t)(cid:1)

c1; c2
Figure 2. Rules for the expected runtime transformer ert. lfp⊑ (F )
denotes the least ﬁxed point of transformer F : RtEnv → RtEnv
w.r.t. the pointwise order “⊑” between runtime environments.

η(cid:17)(t)

5. The Expected Runtime of Programs
To further our study of recursive probabilistic programs we now
develop a calculus for reasoning about the expected or average runtime 
of pRGCL programs. This calculus builds upon our previous
work in [17] and is able to handle recursive procedures.

5.1 The Expected Runtime Transformer ert
We assume a runtime model where executing a skip statement, an
assignment, evaluating the guard in a conditional branching and invoking 
a procedure5 consumes one unit of time. On the other hand,
combining two programs by means of a sequential composition or
a probabilistic choice consumes no additional time other than that
consumed by the original programs. Likewise, halting a program
execution with an abort statement consumes no unit of time.

Since the runtime of a program varies according to the initial
state from which it is executed, our aim is to associate to each
program hc, Di a mapping that takes each state s to the expected
time until hc, Di terminates on s. Such mappings will range over

the set of runtimes T ,(cid:8)t(cid:12)(cid:12) t : S → [0, ∞](cid:9).6

passing style formalized by the transformer

To associate each program to its runtime we use a continuation

ert [ · ] : T → T .

supn ϕn (cid:22) wp[call Prec3 , D](1) .

To establish this formula we apply the one side variant of rule [wprecω 
] to reason about lower bounds of wp[call Prec3, D](1), that is,
we implicitly take un+1 = ∞. We must then establish

ϕn (cid:22) wp[call Prec3 ](1) (cid:13) ϕn+1 (cid:22) wp[D(Prec3 )](1) .

If t ∈ T represents the runtime of the computation that follows
program hc, Di, then ert [c, D](t) represents the overall runtime of
hc, Di, plus the computation following hc, Di. Runtime t is usually
referred to as the continuation of hc, Di. In particular, by setting
the continuation of a program to zero we recover the runtime of the
plain program. That is, for every initial state s,

The derivation follows the same steps as those taken in Example 3
to give upper bounds on wp[call Prec3, D](1). Combining the result
proved with that in Example 3, we conclude that ϕ =
is the
exact termination probability of hcall Prec3 , Di.
△

√5−1

2

Lastly, we can establish the correctness our rules.

Theorem 4.2 (Soundness of rules [w(l)p-recω]). Rules [w(l)p-recω ]
are sound w.r.t. the w(l)p semantics in Figure 1.

Proof. See Appendix A.3.

To conclude the section we would like to point out that the
rule [wp-recω] is related to previous work on proof rules. It can
be viewed as a generalization of Jones’s loop rule [15] to the case
of recursion (even though Jones originally presented a one–sided
version) and as an adaptation of Audebaud and Paulin-Mohring’s
rule [1] to our weakest pre–expectation semantics. The counterpart
of the rule for partial correctness, on the other hand, is, to the best
of our knowledge, novel.

ert [c, D](0)(s)

gives the expected runtime of program hc, Di from state s.

The transformer ert [c, D] is deﬁned by induction on the structure 
of c, following the rules in Figure 2. The rules are deﬁned so
as to correspond to the aforementioned runtime model. That is,
ert [c, D](0) captures the expected number of assignments, guard
evaluations, procedure calls and skip statements in the execution
of hc, Di. Most rules are self–explanatory. ert [skip, D] adds one
unit of time to the continuation since skip does not modify the program 
state and its execution takes one unit of time. ert [x := E, D]
also adds one unit of time, but to the continuation evaluated in the
state resulting from the assignment. ert [abort, D] yields always the

5 Loosely speaking, the overall runtime of a procedure call is then one plus
the runtime of executing the procedure’s body.
6 Strictly speaking, the set of runtimes T coincides with the set of unbounded 
expectations E but we prefer to distinguish the two sets since they
are to represent different objects. We will, however, keep the same notations
for runtimes as for expectations, for example t[x/E], t1 (cid:22) t2, etc.

Reasoning about Recursive Probabilistic Programs

5

2016/3/10

constant runtime 0 since abort aborts any subsequent program execution 
(making their runtime irrelevant) and consumes no time.
ert [if (G) {c1} else {c2}, D] adds one unit of time to the runtime
of either of its branches, depending on the value of the guard.
ert [{c1} [p] {c2}, D] gives the weighted average between the runtime 
of its branches, each of them weighted according to its probability.
 ert [c1; c2, D] ﬁrst applies ert [c2, D] to the continuation and
then ert [c1, D] to the resulting runtime of this application. Finally,
ert [call P, D] is deﬁned using ﬁxed point techniques.

To understand the intuition behind the deﬁnition of ert [call P, D]
recall that call P consumes one unit of time more than the body of
P . To capture this fact we make use of the auxiliary runtime transη 
: T → T (cf. expectation transformer wp[ · ]♯
former ert [ · ]♯
θ).
This transformer behaves as ert except that for deﬁning its action
on a procedure call, it relies on a so–called runtime environment η
in RtEnv , {η | η : T → T is upper continuous} instead of on a
procedure declaration. Concretely, ert [call P, D]♯
η takes continuation 
t to η(t) and for all other program constructs, ert [ · ]♯
η follows
the same rule as ert [ · ]. Using this transformer we can (implicitly)
deﬁne ert [call P , D] by the equation

ert [call P, D] = 1 ⊕ ert [D]♯

ert[call P ,D] ,

where 1 = λt : T • 1 represents the constantly 1 runtime transformer 
and “⊕” the point–wise sum between runtime transformers,
i.e. for γ1, γ2 : T → T, we let (γ1 ⊕ γ2)(t) , γ1(t) + γ2(t).
The above equation leads to the ﬁxed point characterization of
ert [call P, D] in Figure 2.

We remark that, as opposed to w(l)p, it is not posible to deﬁne
the action ert [call P , D] of ert on a procedure call in terms of its
action ert [callDn P ] on the ﬁnite inlinings. This is because when
computing ert [callDn P](t), to be correct the transformer should add
one unit of time each time a procedure call was inlined, and this is
not recoverable from callDn P.7

This concludes our deﬁnition of the transformer ert. We devote
the remainder of the section to study several of its properties. We
begin with Theorem 5.1 summarizing some algebraic properties.
Theorem 5.1 (Basic properties of ert). For any program hc, Di,
any constant runtime k = λs • k for k ∈ R≥0, any t, u ∈ T, and
any increasing ω–chain t0 (cid:22) t1 (cid:22) · · · of runtimes, it holds:
Continuity:
Monotonicity:
Propagation
of constants:
Preservation
of inﬁnity:

supn ert [c, D](tn) = ert [c, D](supn tn);
t (cid:22) u =⇒ ert [c, D](t) (cid:22) ert [c, D](u);
ert [c, D](k + t) = k + ert [c, D](t)
provided hc, Di is abort–free;
ert [c, D](∞) = ∞
provided hc, Di is abort–free.

Proof. Monotonicity follows from continuity. Other properties are
prooven by induction on c; see Appendix A.6.

The next result establishes a connection between ert and wp.

Theorem 5.2. For every program hc, Di and runtime t,

ert [c, D](t) = ert [c, D](0) + wp[c, D](t) .

Proof. By induction on the program structure, considering the
stronger version of the statement

ert [c, D](t1 + t2) = ert [c, D](t1) + wp[c, D](t2) .

See Appendix A.7 for details.

7 If we adopt a model where the runtime of a procedure call coincides
with the runtime of its body, we could just take ert [call P , D](t) =
supn ert [callDn P ](t).

Theorem 5.2 allows giving a very short proof of a well–known
result relating expected runtimes and termination probabilities: If a
program has ﬁnite expected runtime, it terminates almost surely.
Theorem 5.3. For every abort–free program hc, Di and initial
state s of the program,

ert [c, D](0)(s) < ∞ =⇒ wp[c, D](1)(s) = 1 .

Proof. By instantiating Theorem 5.2 with t = 1 and using the
propagation of constants property of ert (Theorem 5.1) to decompose 
ert [c, D](1) as 1 + ert [c, D](0).

Observe that in Theorem 5.3 we cannot drop the abort–free
requirement on the program. To see this, consider the program c =
{skip}[1/2]{abort}. The program has a ﬁnite runtime (ert [c](0) =
1/2 < ∞) and terminates, however, with probability less than one
(wp[c](1) = 1/2 < 1). Moreover, observe that Theorem 5.3 is only
valid on the stated direction: A probabilistic program can terminate
almost–surely and require, still, an expected inﬁnite time to reach
termination. This phenomenon is illustrated, for instance, by the
one dimensional random walk; see e.g. [17, §7].

Even though Theorem 5.3 constitutes a well–known and natural
result on probabilistic programs, our contribution here is to give the
ﬁrst fully formal proof of such a result.

5.2 Proof Rules for Recursive Programs
The runtime of procedure calls, which includes, in particular, recursive 
programs, is deﬁned using ﬁxed points. To avoid reasoning
about ﬁxed points we propose some proof rules based on invariants.
We show that an adaptation of the proof rules for procedure calls
from our wp–calculus is sound for the ert–calculus. The rules are:

ert [call P ](t) (cid:22) 1+u (cid:13) ert [D(P )](t) (cid:22) u

ert [call P , D](t) (cid:22) 1+u

[eet-rec]

l0 = 0,

u0 = 0,

1+ln (cid:22) ert [call P ](t) (cid:22) 1+un

(cid:13) ln+1 (cid:22) ert [D(P )](t) (cid:22) un+1

1+ supn ln (cid:22) ert [call P , D](t) (cid:22) 1+ supn un

[eet-recω ]

Compared to the proof rules from the wp–calculus, these proof
rules require incrementing by one unit some of the bounds. Loosely
speaking, this is because the runtime of a procedure call is one plus
the runtime of its body, whereas the semantics of a procedure call
fully agrees with the semantics of its body.
Example 5. To illustrate the use of the rules, consider the faulty
factorial procedure with declaration

if (x ≤ 0) {{y := 1} else {{c1} [5/6] {c2}; y := y·x} ,
D(Pfact) :
where c1 = x := x−1; call Pfact; x := x+1 and c2 = x := x−2;
call Pfact; x := x+2. We prove that on input x = k ≥ 0, the
expected runtime of the procedure is 2 + αk, where

αk =

1

49(cid:16)121 + 210k + 432(cid:0)− 1

6(cid:1)k+1(cid:17) .

Since the term 432(−1/6)k+1 is negligible, we can approximate the
procedure’s runtime by 4.5 + 4.3k. We can formally capture our
exact runtime assertion by

ert [call Pfact, D](0) = 1 + supn tn ,

where tn = 1+[x < 0]·1+[0 ≤ x ≤ n]·αx +[x > n]·αn+1. To
see this, observe that the sequence hαki is increasing and therefore,
supn tn = 1 + [x < 0] · 1 + [0 ≤ x] · αx. We prove the runtime
assertion using rule [eet-recω] with instantiations t = 0 and ln =
un = tn for n ≥ 1. We have to discharge the premise

ert [call Pfact](0) = 1 + tn (cid:13) ert [D(Pfact)](0) = tn+1 .

Reasoning about Recursive Probabilistic Programs

6

2016/3/10

Since some simple calculations yield

ert [D(Pfact)](0) = 1 + [x ≤ 0] · 1

+ [x > 0] ·(cid:0) 5

6 · ert [c1](1) + 1

6 · ert [c2](1)(cid:1) ,

our next step is to compute ert [c1](1) (the calculations are identical
for ert [c2](1)). To do so, we rely on assumption ert [call P ](0) =
1 + tn and the propagation of constants property of ert.

ert [c1](1) = ert [x := x−1; call Pfact](cid:0)ert [x := x+1](1)(cid:1)

= 2 + ert [x := x−1; call Pfact](0)
= 2 + ert [x := x−1](1 + tn)
= 4 + tn[x/x + 1]

The derivation then concludes by showing that

tn+1 = 1 + [x ≤ 0] · 1

+ [x > 0] ·(cid:16) 5

6(cid:0)4 + tn[x/x+1](cid:1) + 1

6(cid:0)4 + tn[x/x+2](cid:1)(cid:17) ,

which after some term reordering reduces to proving that α0 = 1,
α1 = 7 and αk+2 = 5 + 5
△
We conclude the section establishing the soundness of the rules.
Theorem 5.4 (Soundness of rules [eet-rec], [eet-recω ]). Rules [eet-
rec] and [eet-recω ] are sound w.r.t. the ert–calculus in Figure 2.

6 αk+1 + 1

6 αk.

Proof. See Appendix A.8.

6. Operational Semantics
We provide an operational semantics for pRGCL programs in terms
of pushdown Markov chains with rewards (PRMC) [3] and prove
the transformer wp to be sound with respect to this semantics. Due
to space limitations, this section contains an informal introduction
only. Corresponding formal deﬁnitions are found in Appendix A.9.
For simplicity, we assume a canonical labeling for each command 
c ∈ C together with auxiliary functions init, succ1, succ2 and
stmt determining the initial location, the ﬁrst and second successor
of a location and the program statement corresponding to a label.
As an example, the labels attached to each statement of program c
from Example 3 are as follows:

c : {skip1} [1/2]2 {call P 3; call P 4; call P 5 } .

The deﬁnition of the auxiliary functions is straightforward. For
instance, we have init(c) = 2, succ1(1) = ↓, succ2(2) = 3, and
stmt (2) = c, where ↓ is a special symbol indicating termination
of a procedure. Moreover, label Term stands for termination of the
whole program.

Our operational semantics of pRGCL programs is given as an

execution relation, where each step is of the form

hℓ, si

γ, p, γ′

−−−−→(cid:10)ℓ′, s′(cid:11) .

Here, ℓ, ℓ′ are program labels, s, s′ ∈ S are program states, γ is
a program label being popped from and γ′ a ﬁnite sequence of
labels being pushed on the stack, respectively. p ∈ [0, 1] denotes
the probability of executing this step.

This execution relation corresponds to the transition relation of
a PRMC, where each pair hℓ, si is a state and the stack alphabet is
given by the set of all labels of a given pRGCL program. Moreover,
given f ∈ E, a reward of f (s) is assigned to each state of the form
hTerm, si. Otherwise, the reward of a state is 0. Figure 3 shows the
rules deﬁning the operational semantics of pRGCL programs. The
rules in Figure 3 are self–explanatory. In case of a procedure call,
the calls successor label is pushed on the stack and execution continues 
with the called procedure. Whenever a procedure terminates,

γ, 1, γ · 5

γ, 1, γ · 4

↓, 1, ε

γ, 1/2, γ

3

2

γ, 1/2, γ

1

γ, 1, γ

↓

4

4, 1, ε

γ0, 1, γ0

5, 1, ε

5

Term

γ, 1, γ · ↓

Figure 4. PRMC of program c from Example 3. Since c affects no
variables, the second component of states is omitted.

i.e. reaches a state h↓, si, and the stack is non–empty, a return address 
is popped from and execution continues at this address.

Figure 4 shows the PRMC of example program c. The initial
state is 2 (the probabilistic choice). Say the right branch is chosen;
we move to 3. The statement at 3 is a call, and the address after the
call is 4; so 4 is pushed and the procedure body is reentered. Say
now the left branch is chosen; we move to 1 (the skip) and then
terminate, i.e. we move to ↓. Recall that return address 4 is on top
of the stack; 4 is popped, we move to 4 to continue execution.

The expected reward that PRMC P associated to program hc, Di
reaches a set of target states T from initial state hℓ, si is deﬁned as

ExpRewPf

sJc,DK (T ) = Xπ∈Π(hℓ, si,T )

ProbP (π) · rew (π) ,

where π is a path from hℓ, si to some target state, ProbP (π) is the
probability of π and rew (π) is the reward collected along π.

We are now in a position to state the relationship between the

operational model and the denotational semantics:
Theorem 6.1 (Correspondence Theorem). Let c ∈ C, f ∈ E, and
T = {hTerm, si | s ∈ S} 8. Then for each s ∈ S, we have

ExpRewPf

sJc,DK (T ) = wp[c, D](f )(s) .

Proof. See Appendix A.10.

In the spirit of [11] a similar result can be obtained for wlp.
For that one needs a liberal expected reward being deﬁned as the
expected reward plus the probability of not reaching the target
states at all. One can then show a similar correspondence to wlp.

7. Extensions
Mutual recursion. Both our wp– and ert–calculus can be extended 
to handle multiple procedures. Say we want to handle m
(possibly mutually recursive) procedures P1, . . . , Pm with declaration 
D ∈ Cm. The deﬁnition of wp[call Pi, D] remains the same,
we only need to adapt the deﬁnition of the n-inlining callDn Pi of
procedure Pi as to inline the calls of all procedures:
callDn+1 Pi = D(P )[call P1/callDn P1, . . . , call Pm/callDn Pm] .
As for the ert–calculus, a runtime environment is now a tuple
η = (η1, . . . , ηm), where ηi is meant to provide the behavior of
procedure Pi in ert [·]♯
η = ηi. The action of ert
on procedure calls is then deﬁned simultaneously as9

η, i.e. ert [call Pi]♯

8 T denotes the set of states representing successful termination of the
pushdown automaton.
9 For determining the least ﬁxed point, environments are compared
component-wise, i.e. (η1, . . . , ηm) ⊑ (ν1, . . . , νm) iff ηi ⊑ νi for all
i = 1 . . . m.

Reasoning about Recursive Probabilistic Programs

7

2016/3/10

stmt (ℓ) = if (G) {c1} else {c2} s |= G succ1 (ℓ) = ℓ′

stmt (ℓ) = if (G) {c1} else {c2} s 6|= G succ2 (ℓ) = ℓ′

stmt (ℓ) = x := E succ1 (ℓ) = ℓ′
−−−−→ (cid:10)ℓ′, s(cid:2)x 7→ s(E)(cid:3)(cid:11)

hℓ, si

γ, 1, γ

[if1]

[assign]

stmt (ℓ) = abort

hℓ, si

γ, 1, γ
−−−−→ hℓ, si

[abort]

hℓ, si

γ, 1, γ
−−−−→ hℓ′, si

[if2]

stmt (ℓ) = {c1} [p] {c2} succ2 (ℓ) = ℓ′

hℓ, si

γ, 1−p, γ
−−−−−−→ hℓ′, si

[prob2]

h↓, si

ℓ′, 1, ε
−−−−→ hℓ′, si

[return]

h↓, si

γ0, 1, γ0
−−−−−−→ hTerm, si

[terminate]

stmt (ℓ) = skip succ1 (ℓ) = ℓ′

hℓ, si

γ, 1, γ
−−−−→ hℓ′, si

[skip]

hℓ, si

γ, 1, γ
−−−−→ hℓ′, si

stmt (ℓ) = {c1} [p] {c2} succ1 (ℓ) = ℓ′

hℓ, si

γ, p, γ
−−−−→ hℓ′, si

[prob1]

stmt (ℓ) = call P succ1 (ℓ) = ℓ′
γ, 1, γ·ℓ′
−−−−−−→ (cid:10)init(cid:0)D(P )(cid:1), s(cid:11)

hℓ, si

[call]

Figure 3. Rules for deﬁning an operational semantics for pRGCL programs. For sequential composition there is no dedicated rule as the
control ﬂow is encoded via the succ1 and the succ2 functions.

(cid:0)ert [call P1, D] , . . . , ert [call Pm, D](cid:1) =
lfp(cid:16)λη • (cid:16)1 ⊕ ert [D(P1)]♯

η , . . . , 1 ⊕ ert [D(Pm)]♯

The proof rules for reasoning about procedure calls in both calculi
are easily adapted. We show only the case of [wp-rec]; the others
admit a similar adaptation.

η(cid:17)(cid:17) .

wp[call P1](f1) (cid:22) g1, ... , wp[call Pm](fm) (cid:22) gm (cid:13) wp[D(P1)](f1) (cid:22) g1

...

wp[call P1](f1) (cid:22) g1, ... , wp[call Pm](fm) (cid:22) gm (cid:13) wp[D(Pm)](fm) (cid:22) gm

wp[call Pi, D](fi) (cid:22) gi

for all i = 1 . . . m

The rule reasons about all the procedures simultaneously. Roughly
speaking, the rule premise requires deriving the speciﬁcation gi
for the body of each procedure Pi, assuming the corresponding
speciﬁcation for each procedure call in it. The rule conclusion
establishes the speciﬁcation of the set of procedures altogether.

Random samplings. All our results remain valid if the pRGCL
language allows for random samplings (from distributions with discrete 
support). In a random sampling x := µ, µ represents a probability 
distribution which is sampled and its outcome is assigned to
program variable x. In Section 8 we exploit this extension to model
a probabilistic variant of the binary search.

Alternative runtime models. The ert–calculus can be easily
adapted to capture alternative runtime models. For instance we
can capture the model where we are interested in counting only
the number of procedure calls and also more ﬁne–grained models
such as that where the time consumed by an assignment (or guard
evaluation) depends on some notion of size of the expression being
assigned (guard being evaluated). Likewise, the ert–calculus can
be easily adapted so as to take into account the costs of ﬂipping the
(possibly biased) coin from probabilistic choices.

Soundness of the ert–calculus. We can also establish the soundness 
of the ert–calculus w.r.t. the operational semantics based on
PRMC. This only requires changes in the reward function.

8. Case Study
In this section we show the applicability of our approach analyzing
a probabilistic, so–called Sherwood [21], variant of the binary
search. The main difference w.r.t. the classical version is that in
each recursive call the pivot element is picked uniformly at random
from the remaining array, aligning this way worst–, best– and
average–case of the algorithm runtime.

The algorithm we analyze searches for value val

in array
a[left .. right ]. It is encoded by procedure B with declaration
D presented in Figure 5. We use random assignment mid :=

uniform(left, right ) to model the random election of the pivot. For
simplicity, we assume that the random assignment is performed in
constant time 1 if left ≤ right and that it diverges if left > right.

Partial correctness. We verify the following partial correctness
property: When B is invoked in a state where left ≤ right,
a[left .. right ] is sorted, and val occurs in a[left .. right ], then the
invocation of B stores in mid the index where val lies. Formally,

g (cid:22) wlp[call B, D](f ), with

g = [left ≤ right ] ·(cid:2)sorted(left, right )(cid:3)
·(cid:2)∃x ∈ [left, right ] : a[x] = val(cid:3)
f = (cid:2)a[mid ] = val(cid:3) ,

where (cid:2)sorted(y, z)(cid:3) is the indicator function of a[y .. z] being

sorted. In order to prove g (cid:22) wlp[call B](f ) we apply rule [wlp-
rec]. We are then left to prove

g (cid:22) wlp[call B](f ) (cid:13) g (cid:22) wlp[D(B)](f ) .

The way in which we propagate post–expectation f from the exit
point of the procedure till its entry point, obtaining pre–expectation
g, is fully detailed in Figure 5. To do so we use assumption g (cid:22)
wlp[call B](f ) and monotonicity of wlp.

Dually, we can verify that when val is not in the array, the value
of a[mid ] after termination of B is different from val . A detailed
derivation of this property is provided in Appendix A.11, Figure 9.

Expected runtime. We perform a runtime analysis of the algorithm 
for those inputs where val does not occur in the array. Under
this assumption we can distinguish two cases: either val is smaller
than every element in the array or larger than all of them.

For the ﬁrst case we show that the expected runtime of the

algorithm is upper bounded by 1 + u, with

u = [left > right ] · ∞ + 3

+ [left < right ] · (5 · Hright−left +1 − 5/2) ,

and Hk bing the k-th harmonic number. Formally, we show that

ert [call B](0) (cid:22) 1 + u

applying rule [eet-rec]. We must then establish

ert [call B](0) (cid:22) 1 + u (cid:13) ert [D](0) (cid:22) u .

The details of this derivation are provided in Figure 6.

Similarly, when val is greater than every element in the array,

the expected runtime is upper bounded by 1 + u, with

u = [left > right ] · ∞ + 3

+ [left < right ] · (6 · Hright−left +1 − 3) .

Reasoning about Recursive Probabilistic Programs

8

2016/3/10

g(cid:22)

1 : mid := uniform(left, right);

right

[left<right]
right−left+1

Xi=left



(cid:2)a[i]<val(cid:3)·g[left/min(i + 1, right)]
+(cid:2)a[i]>val(cid:3)·g[right/max(i − 1, left)]


+(cid:2)a[i]=val(cid:3)
+ [left = right ] ·(cid:2)a[left] = val(cid:3)
[left < right] ·(cid:16)(cid:2)a[mid] < val(cid:3) · g[left/ · · · ]
+(cid:2)a[mid] > val(cid:3) · g[right/ · · · ]
+(cid:2)a[mid] = val(cid:3)(cid:17) + [left ≥ right ] · f
(cid:2)a[mid]<val(cid:3)·g[left/ · · · ] + (cid:2)a[mid ]>val(cid:3)·g[right/ · · · ]

if (left < right){

+(cid:2)a[mid]=val(cid:3) · f

if (a[mid] < val ){

g[left/min(mid + 1, right)]
left := min(mid + 1, right);
g
call B
f

} else {

(cid:2)a[mid ] > val(cid:3) · g[right/ · · · ] +(cid:2)a[mid] ≤ val(cid:3) · f

if (a[mid] > val ){

g[right/max(mid − 1, left)]
right := max(mid − 1, left);
g
call B
f

2 :

3 :

4 :

5 :

6 :

7 :

8 :

9 :

} else { f skip f } f

10 :
11 :
12 : } else { f skip f } f

} f

Figure 5. Declaration D (boldface) of the probabilistic binary
search procedure B together with the proof (lightface) that call B
ﬁnds the index of val when started in a sorted array a[left .. right ]
which contains value val. We write j C h for j (cid:22) wp[C] (h).

The veriﬁcation for this case is analogous therefore omitted.

Combining the two cases we conclude that when the sought–
after value does not occur in the array, the algorithm terminates in

expected time in Θ(cid:0) log n(cid:1), where n = right − left + 1 is the size

of the array, since Hk ∈ Θ(log k).

9. Related Work
wp–style reasoning for recursive programs. Recursion has been
treated for non–probabilistic programs. Hesselink [14] provided
several proof rules for recursive procedures, both for total and partial 
correctness. Our ﬁrst two proof rules are extensions of his rules
to the probabilistic setting. Predicate transformer semantics for recursive 
non–deterministic procedures has been provided by Bonsangue 
and Kok [2] and Hesselink [13]. Nipkow [27] provides an
operational semantics and a Hoare logic for recursive (parameter-
less) non–deterministic procedures. Zhang et al. [33] establishes
the equivalence between an operational semantics and a weakest
pre–condition semantics for recursive programs in Coq. To some
extent our transfer theorem between probabilistic pushdown automata 
and the wp–semantics can be considered as a probabilistic
extension of this work.
Deductive reasoning for recursive probabilistic programs.
Jones
provided several proof rules for recursive probabilistic programs
in her Ph.D. dissertation [15]. One of our proof rules is a generalisation 
of Jones’ proof rule to general recursion. McIver and
Morgan [22] also provide a wp–semantics of probabilistic recursive 
programs. While [22] use ﬁxed point techniques, we fol-

u= [left > right ] · ∞ + 3 + [left < right]

·

right

5 +




Xi=left

right − left + 1

[min(i + 1, right) < right]



2 + [left < right] ·(cid:16)2 + (cid:2)a[mid ] < val(cid:3) · (3

+ [min(mid + 1, right) < right]

· (cid:0)5 · Hright−min(i+1, right )<right +1

− 5/2)

1 : mid := uniform(left, right);







if (left < right){

·(cid:0)5 · Hright−min(mid+1, right )+1 − 5/2(cid:1)

+ (cid:2)a[mid ] > val(cid:3) · (· · · )(cid:17)
3 +(cid:2)a[mid] < val(cid:3) · u[left/min(mid + 1, right)]
+ (cid:2)a[mid] > val(cid:3) · (· · · )

if (a[mid] < val ){

2 + u[left/min(mid + 1, right)]
left := min(mid + 1, right);
1 + u
call B
0

} else {

2 + (cid:2)a[mid] > val(cid:3) · (· · · )

if (a[mid] > val ){

2 + u[right/max(mid − 1, left)]
right := max(mid − 1, left);
1 + u
call B
0

2 :

3 :

4 :

5 :

6 :

7 :

8 :

9 :

} else { 1 skip 0 } 0

10 :
11 :
12 : } else { 1 skip 0 } 0

} 0

Figure 6. Runtime analysis of the probabilistic binary search procedure 
for the case that every value occurring in a[left .. right ] is
smaller than val . We write j C h for j (cid:23) ert [C] (h).

low e.g. Hehner [12] and deﬁne the semantics of a recursive procedure 
as the limit of an approximation sequence. In contrast to our
approach based on procedures, [22] introduced recursion through
the language constructor rec B, where B is a program–semantics
transformer. (Intuitively B encodes how the recursive procedure
deﬁned (and invoked) by rec B transforms the outcome of its recursive 
calls). Our approach provides a strict separation between
program syntax and semantics. Moreover our approach based on
procedure calls can model mutual recursion in a natural way (see
Section 7), while the approach in [22] approach does not accommodate 
so naturally to such cases. Audebaud and Paulin-Mohring [1]
present a mechanized method for proving properties of randomized
algorithms in the Coq proof assistant. Their approach is based on
higher–order logic, in particular using a monadic interpretation of
programs as probabilistic distributions. Our proof rule for obtaining
two–sided bounds on recursive programs is directly adapted from
their work. They however do neither relate their work to an operational 
model and nor support the analysis of expected runtimes.
Semantics of recursive probabilistic programs. Gupta et al. consider 
the interplay between constraints, probabilistic choice, and recursion 
in the context of a (concurrent) constraint–based probabilistic 
programming language. They provide an operational semantics
using labeled transition systems and (weak) bisimulation as well
as a denotational semantics. Recursion is treated operationally by
considering the limit of syntactic ﬁnite approximations. In the denotational 
semantics, the mixture of probabilities and constraints

Reasoning about Recursive Probabilistic Programs

9

2016/3/10

violates basic monotonicity properties for a standard treatment of
recursion. Their main result is that the transition system semantics 
modulo weak bisimulation is fully abstract with respect to the
input–output relation of processes. They do neither consider non–
determinism nor reasoning about recursive probabilistic programs.
Pfeffer and Koller [29] provide a measure–theoretic semantics of
recursive Bayesian networks and show that every recursive probabilistic 
relational database has a probability measure as model. This
is complemented by an inference algorithm that obtains approximations 
by basically unfolding the recursive Bayesian network. Recently,
 Toronto et al. [30] provided a measure–theoretic semantics
for a probabilistic programming language with recursion. Their interpretation 
of recursive programs is however restricted to (almost
surely) terminating programs.

Probabilistic pushdown automata. The analysis of probabilistic 
pushdown automata, which correspond to the model of recursive 
Markov chains, has been well–investigated. Key computational
problems for analyzing classes of these models can be reduced to
computing the least ﬁxed point solution of corresponding classes
of monotone polynomial systems of non–linear equations. For subclasses 
of these models termination probabilities, ω–regular properties,
 and expected runtimes can be algorithmically obtained. Recent 
surveys are provided by Etessami [8] and Brazdil et al. [3].
Our transfer theorem indicates that (some of) these results are transferable 
to obtaining weakest pre–expectations for recursive probabilistic 
programs having a ﬁnite–control probabilistic push–down
automata. A detailed study is outside the scope of this paper and
left for future work.

10. Conclusion
We have presented two wp-calculi: one for reasoning about correctness,
 and one for analysing expected rum-times of recursive probabilistic 
programs. The wp-calculi have been related, equipped with
proof rules, and exempliﬁed by analysing a Sherwood version of binary 
search. A relation with a straightforward operational interpretation 
using pushdown Markov chains has been established. We believe 
that this work provides a good basis for the automation of the
analysis of recursive probabilistic programs. Future work consists
of applying our calculi to other recursive randomized algorithms
(such as quick sort with random pivot selection). Other future work
includes investigating a generalisation of Colussi’s technique [5] to
transform a recursive program and its correctness proof into a nonrecursive 
program with its accompanying correctness proof. This
would allow to transfer—typically simpler—correctness proofs of
the recursive probabilistic programs to non-recursive ones.

References
[1] P. Audebaud and C. Paulin-Mohring. Proofs of randomized algorithms

in Coq. Science of Comp. Progr., 74(8):568 – 589, 2009.

[2] M. Bonsangue and J. Kok. The weakest precondition calculus: Recursion 
and duality. Formal Aspects of Computing, 6(1):788–800, 1994.
[3] T. Br´azdil, J. Esparza, S. Kiefer, and A. Kucera. Analyzing probabilistic 
pushdown automata. Formal Methods in System Design, 43
(2):124–163, 2013.

[4] M. Carbin, S. Misailovic, and M. C. Rinard. Verifying quantitative
reliability for programs that execute on unreliable hardware. In Proc.
of OOPSLA, pages 33–52. ACM, 2013.

[5] L. Colussi. Recursion as an effective step in program development.

ACM Trans. Program. Lang. Syst., 6(1):55–67, Jan. 1984.

[6] B. C. Dean. A simple expected running time analysis for randomized
“divide and conquer” algorithms. Discrete Appl. Math., 154(1):1–5,
2006.

[7] E. W. Dijkstra. A Discipline of Programming. Prentice Hall, 1976.

[8] K. Etessami. Analysis of probabilistic processes and automata theory.

In Handbook of Automata Theory. 2016. (to appear).

[9] L. M. F. Fioriti and H. Hermanns. Probabilistic termination: SoundIn 
Proc. of POPL, pages

ness, completeness, and compositionality.
489–501. ACM, 2015.

[10] A. D. Gordon, T. A. Henzinger, A. V. Nori, and S. K. Rajamani. ProbIn 
Future of Software Engineering (FOSE),

abilistic programming.
pages 167–181. ACM, 2014.

[11] F. Gretz, J.-P. Katoen, and A. McIver. Operational versus weakest
pre-expectation semantics for the probabilistic guarded command language.
 Perform. Eval., 73:110–132, 2014.

[12] E. Hehner. do considered od: A contribution to the programming

calculus. Acta Informatica, 11(4):287–304, 1979. .

[13] W. H. Hesselink. Predicate-transformer semantics of general recursion.
 Acta Informatica, 26(4):309–332, 1989.

[14] W. H. Hesselink. Proof rules for recursive procedures. Formal Aspects

of Computing, 5(6):554–570, 1993. .

[15] C. Jones. Probabilistic Non-determinism. PhD thesis, University of

Edinburgh, 1989.

[16] B. L. Kaminski and J. Katoen. On the hardness of almost-sure terIn 
Prof. of MFCS, Part I, volume 9234 of LNCS, pages

mination.
307–318. Springer, 2015.

[17] B. L. Kaminski, J.-P. Katoen, C. Matheja, and F. Olmedo. Weakest
precondition reasoning for expected run–times of probabilistic programs.
 In Proc. of ESOP, LNCS, 2016. To appear.

[18] B. L. Kaminski, J.-P. Katoen, C. Matheja, and F. Olmedo. Weakest
precondition reasoning for expected run–times of probabilistic programs.
 ArXiv e-prints, 2016.

[19] R. M. Karp. Probabilistic recurrence relations. J. ACM, 41(6):1136–

1150, 1994.

[20] D. Kozen. Semantics of Probabilistic Programs. J. Comput. Syst. Sci.,

22(3):328–350, 1981.

[21] J. McConnell. Analysis of Algorithms – An Active Learning Approach.

Jones and Bartlett Publishers, Inc., 2008.

[22] A. McIver and C. Morgan. Partial correctness for probabilistic demonic 
programs. Theor. Comp. Sc., 266(12):513 – 541, 2001.

[23] A. McIver and C. Morgan. Abstraction, Reﬁnement And Proof For

Probabilistic Systems. Springer, 2004.

[24] M. Mitzenmacher and E. Upfal. Probability and Computing: Randomized 
Algorithms and Probabilistic Analysis. Cambridge University 
Press, 2005.

[25] C. Morgan. Proof rules for probabilistic loops. In Proceedings of the

BCS-FACS 7th Reﬁnement Workshop. Springer, 1996.

[26] G. Nelson. A generalization of Dijkstra’s calculus. ACM Trans.

Program. Lang. Syst., 11(4):517–561, Oct. 1989.

[27] T. Nipkow. Hoare logics for recursive procedures and unbounded
nondeterminism. In Proc. of CSL, volume 2471 of LNCS, pages 103–
119. Springer, 2002.

[28] A. Pfeffer. Practical Probabilistic Programming. Manning Publications,
 2016.

[29] A. Pfeffer and D. Koller.

probability models.
/ The MIT Press, 2000.

Semantics and inference for recursive
In Proc. of AAAI, pages 538–544. AAAI Press

[30] N. Toronto, J. McCarthy, and D. V. Horn. Running probabilistic
programs backwards. In Proc. of ESOP, volume 9032 of LNCS, pages
53–79. Springer, 2015.

[31] W. Wechler. Universal Algebra for Computer Scientists, volume 25 of

EATCS Monographs on Theor. Comp. Science. Springer, 1992.

[32] G. Winskel. The Formal Semantics of Programming Languages: An

Introduction. MIT Press, 1993.

[33] X. Zhang, M. Munro, M. Harman, and L. Hu. Weakest precondition
for general recursive programs formalized in Coq. In Proc. of TPHOL,
volume 2410 of LNCS, pages 332–348. Springer, 2002.

Reasoning about Recursive Probabilistic Programs

10

2016/3/10

A. Appendix
For our proofs about transformer wp, we observe that “(cid:22)” endows
the set of unbounded expectations E with the structure of an upper
ω–cpo10, where the supremum of an increasing ω–chain f0 (cid:22) f1 (cid:22)
· · · is given pointwise, i.e. (supn fn)(s) , supn fn(s). Likewise,
“(cid:22)” endows the set of bounded expectations E≤1 with the structure
of a lower ω–cpo, where the inﬁmum of a decreasing ω–chain
f0 (cid:23) f1 (cid:23) · · · is given pointwise, i.e. (inf n fn)(s) , inf n fn(s).
Upper ω–cpo (E, (cid:22)) has as botom element the constant expectation
0, while lower ω–cpo (E≤1, (cid:22)) has as top element the constant
expectation 1.
In what follows, we usually refer to the set of upper continuous
expectation transformers11 over (E, (cid:22)) and the set of lower contin-
upp-cont
uous expectation transformers over (E≤1, (cid:22)). We use E
→ E
and E≤1
A.1 Basic Properties of the w(l)p–Transformer
Proof of Continuity. We prove continuity by induction on the program 
structure. Let f0 (cid:22) f1 (cid:22) f2 (cid:22) · · · and g0 (cid:23) g1 (cid:23) g2 (cid:23) · · ·
For the base cases we have:

→ E≤1 to denote such sets.

low-cont

skip:

wp[skip, D](cid:18)sup

n

fn(cid:19) = sup

n

fn = sup

wp[skip, D] (fn)

n

and

For the induction hypothesis we assume that for any two programs
c1 and c2 continuity holds. Then we can perform the induction step:

if (G) {c1} else {c2}:

wp[if (G) {c1} else {c2}, D](cid:18)sup
= [G] · wp[c1, D](cid:18)sup

n

n

fn(cid:19)

fn(cid:19) + [¬G] · wp[c2, D](cid:18)sup

n

fn(cid:19)

= [G] · sup

wp[c1, D] (fn) + [¬G] · sup

wp[c2, D] (fn)

n

n

= sup

[G] · wp[c1, D] (fn) + [¬G] · wp[c2, D] (fn)

n

= sup

wp[if (G) {c1} else {c2}, D] (fn)

n

and

wlp[if (G) {c1} else {c2}, D](cid:16)inf
= [G] · wlp[c1, D](cid:16)inf

gn(cid:17) + [¬G] · wp[c2, D](cid:16)inf

gn(cid:17)

n

n

n

wlp[c1, D] (gn) + [¬G] · inf
n

= [G] · inf
n

wlp[c2, D] (gn)

gn(cid:17)

= inf
n
= inf
n

[G] · wlp[c1, D] (gn) + [¬G] · wlp[c2, D] (gn)

wlp[if (G) {c1} else {c2}, D] (gn)

{c1} [p] {c2}:

x := E:

gn(cid:17) = inf

n

n

wlp[skip, D](cid:16)inf
wp[x := E, D](cid:18)sup

n

and

wlp[x := E, D](cid:16)inf

n

abort:

and

wp[abort, D](cid:18)sup

n

wlp[abort, D](cid:16)inf

n

gn = inf
n

wlp[skip, D] (gn)

fn(cid:19) = (cid:18)sup

fn(cid:19)[x/E]

n
= sup
fn[x/E]

n

= sup

wp[x := E, D] (fn)

n

gn(cid:17) = (cid:16)inf

gn(cid:17)[x/E]

n
gn[x/E]
= inf
n
= inf
n

wlp[x := E, D] (gn)

fn(cid:19) = 0 = sup

n

0

= sup

wp[abort, D] (fn)

n

gn(cid:17) = 1 = inf

n

= inf
n

wp[abort, D] (gn)

wp[{c1} [p] {c2}, D](cid:18)sup
= p · wp[c1, D](cid:18)sup

n

n

fn(cid:19)

fn(cid:19) + (1 − p) · wp[c2, D](cid:18)sup

n

fn(cid:19)

= p · sup

wp[c1, D] (fn) + (1 − p) · sup

wp[c2, D] (fn)

n
p · wp[c1, D] (fn) + (1 − p) · wp[c2, D] (fn)

n

= sup

n

= sup

wp[{c1} [p] {c2}, D] (fn)

n

and

wlp[{c1} [p] {c2}, D](cid:16)inf
= p · wlp[c1, D](cid:16)inf

n

n

gn(cid:17)

gn(cid:17) + (1 − p) · wp[c2, D](cid:16)inf

n

gn(cid:17)

wlp[c1, D] (gn) + (1 − p) · inf
n

= p · inf
n
p · wlp[c1, D] (gn) + (1 − p) · wlp[c2, D] (gn)

wlp[c2, D] (gn)

wlp[{c1} [p] {c2}, D] (gn)

= inf
n
= inf
n
c1; c2:

n

fn(cid:19)(cid:19)
wp[c2, D] (fn)(cid:19)

= wp[c1, D](cid:18)sup

n

= sup

wp[c1, D] (wp[c2, D] (fn))

n

= sup

wp[c1; c2, D] (fn)

n

gn(cid:17) = wlp[c1, D](cid:16)wlp[c2, D](cid:16)inf

n

gn(cid:17)(cid:17)
wlp[c2, D] (gn)(cid:17)

= wlp[c1, D](cid:16)inf

n

1

wp[c1; c2, D](cid:18)sup

n

fn(cid:19) = wp[c1, D](cid:18)wp[c2, D](cid:18)sup

10 Given a binary relation ≤ over a set A, we say that (A, ≤) is an upper
(resp. lower) ω-cpo if ≤ is reﬂexive, transitive and antisymmetric, and
every increasing ω-chain a0 ≤ a1 ≤ · · · (resp. decreasing ω-chain
a0 ≥ a1 ≥ · · · ) in A has a supremum supn an (resp. an inﬁmum
inf n an) in A.
11 A function f : A → B between two upper (resp. lower) ω-cpos (A, ≤A)
and (B, ≤B) is upper (resp. lower) continuous iff for every increasing ωchain 
a0 ≤A a1 ≤A · · · (resp. decreasing ω-chain a0 ≥A a1 ≥A · · · ),
supn f (an) = f (supn an) (resp. infn f (an) = f (inf n an)).

and

wlp[c1; c2, D](cid:16)inf

n

Reasoning about Recursive Probabilistic Programs

11

2016/3/10

wlp[c1, D] (wp[c2, D] (gn))

abort:

wp[abort, D] (α1 · f1 + α2 · f2)

Since callDk P is call–free for every n and we have already proven
continuity for all call–free programs, we have

= inf
n
= inf
n

wlp[c1; c2, D] (gn)

fn(cid:19) = sup

k

gn(cid:17) = inf

k

n

fn(cid:19)

wp(cid:2)callDk P(cid:3)(cid:18)sup
gn(cid:17)
wlp(cid:2)callDk P(cid:3)(cid:16)inf

n

fn(cid:19) = sup

n

n

gn(cid:17) = inf
fn(cid:19) = sup

k

= sup

n

= sup

n

wp(cid:2)callDk P(cid:3) (fn)
wlp(cid:2)callDk P(cid:3) (gn)
wp(cid:2)callDk P(cid:3) (fn)
wp(cid:2)callDk P(cid:3) (fn)

k

wp[call P ] D(fn)

sup

n

sup

call P :

wp[call P, D](cid:18)sup

n

and

n

wlp[call P, D](cid:16)inf
wp(cid:2)callDk P(cid:3)(cid:18)sup
wlp(cid:2)callDk P(cid:3)(cid:16)inf
wp[call P, D](cid:18)sup

n

n

n

for every n and hence

and

and

wlp[call P, D](cid:16)inf

n

gn(cid:17) = inf

k
= inf
n
= inf
n

inf
n
inf
k
wlp[call P ] D(gn) .

wlp(cid:2)callDk P(cid:3) (gn)
wlp(cid:2)callDk P(cid:3) (gn)

Proof of Monotonicity. Assume f1 (cid:22) f2. Then

wp[c, D] (f2) = wp[c, D] (sup{f1, f2})

= sup{wp[c, D] (f1), wp[c, D] (f2)}

(continuity of wp)

which implies wp[c, D] (f1) (cid:22) wp[c, D] (f2), and

wlp[c, D] (f1) = wp[c, D] (inf{f1, f2})

= inf{wlp[c, D] (f1), wlp[c, D] (f2)} ,

(continuity of wlp)

which implies wlp[c, D] (f1) (cid:22) wlp[c, D] (f2).

Proof of Linearity. We prove linearity by induction on the program
structure. For the base cases we have:

skip:

wp[skip, D] (α1 · f1 + α2 · f2)

= α1 · f1 + α2 · f2
= α1 · wp[skip, D] (f1) + α2 · wp[skip, D] (f2)

x := E:

wp[x := E, D] (α1 · f1 + α2 · f2)

= (α1 · f1 + α2 · f2)[x/E]
= α1 · f1[x/E] + α2 · f2[x/E]
= α1 · wp[x := E, D] (f1) + α2 · wp[x := E, D] (f2)

= 0
= α1 · 0 + α2 · 0
= α1 · wp[abort, D] (f1) + α2 · wp[abort, D] (f2)

For the induction hypothesis we assume that for any two programs
c1 and c2 linearity holds. Then we can perform the induction step:
if (G) {c1} else {c2}:

wp[if (G) {c1} else {c2}, D] (α1 · f1 + α2 · f2)

= [G] · wp[c1, D] (α1 · f1 + α2 · f2)

+ [¬G] · wp[c2, D] (α1 · f1 + α2 · f2)

= [G] · (α1 · wp[c1, D] (f1) + α2 · wp[c1, D] (f2))

+ [¬G] · (α1 · wp[c2, D] (f1) + α2 · wp[c2, D] (f2))

= α1 · ([G] · wp[c1, D] (f1) + [¬G] · wp[c2, D] (f1))

+ α2 · ([G] · wp[c1, D] (f2) + [¬G] · wp[c2, D] (f2))

= α1 · wp[if (G) {c1} else {c2}, D] (f1)

+ α2 · wp[if (G) {c1} else {c2}, D] (f2)

{c1} [p] {c2}:

wp[{c1} [p] {c2}, D] (α1 · f1 + α2 · f2)

= p · wp[c1, D] (α1 · f1 + α2 · f2)

+ (1 − p) · wp[c2, D] (α1 · f1 + α2 · f2)

= p · (α1 · wp[c1, D] (f1) + α2 · wp[c1, D] (f2))

+ (1 − p) · (α1 · wp[c2, D] (f1) + α2 · wp[c2, D] (f2))

= α1 · (p · wp[c1, D] (f1) + (1 − p) · wp[c2, D] (f1))

+ α2 · (p · wp[c1, D] (f2) + (1 − p) · wp[c2, D] (f2))

= α1 · wp[{c1} [p] {c2}, D] (f1)

+ α2 · wp[{c1} [p] {c2}, D] (f2)

c1; c2:

wp[c1; c2, D] (α1 · f1 + α2 · f2)

= wp[c1, D] (wp[c2, D] (α1 · f1 + α2 · f2))
= wp[c1, D] (α1 · wp[c2, D] (f1) + α2 · wp[c2, D] (f2))
= α1 · wp[c1, D] (wp[c2, D] (f1))

+ α2 · wp[c1, D] (wp[c2, D] (f2))

= α1 · wp[c1; c2, D] (f1) + α2 · wp[c1; c2, D] (f2)

call P :

wp[call P, D] (α1 · f1 + α2 · f2)

= sup

n

wp(cid:2)callDn P(cid:3) (α1 · f1 + α2 · f2)

Since callDn P is call–free for every n and we have already proven
linearity for all call–free programs, we have

for every n and hence

sup

wp(cid:2)callDn P(cid:3) (α1 · f1 + α2 · f2)
= α1 · wp(cid:2)callDn P(cid:3) (f1) + α2 · wp(cid:2)callDn P(cid:3) (f2)
wp(cid:2)callDn P(cid:3) (α1 · f1 + α2 · f2)
α1 · wp(cid:2)callDn P(cid:3) (f1) + α2 · wp(cid:2)callDn P(cid:3) (f2)
wp(cid:2)callDn P(cid:3) (f1) + α2 · sup
wp(cid:2)callDn P(cid:3) (f2)

n

n

n

n

= α1 · wp[call P, D] (f1) + α2 · wp[call P, D] (f2)

= α1 · sup

= sup

Proof of Preservation of 0 and 1. We prove preservation of 0 and
1 by induction on the program structure. For the base cases we
have:

Reasoning about Recursive Probabilistic Programs

12

2016/3/10

wp[skip, D] (0) = 0

wlp[skip, D] (1) = 1

wp[x := E, D] (0) = 0[x/E] = 0

call P :

and

wp[call P, D] (0) = sup

n

wlp[call P, D] (1) = inf
n

wp(cid:2)callDn P(cid:3) (0)
wlp(cid:2)callDn P(cid:3) (1)

Since callDn P is call–free for every n and we have already proven
preservation of 0 and 1 for all call–free programs, we have

wlp[x := E, D] (1) = 1[x/E] = 1

and

skip:

and

x := E:

and

abort:

and

wp[abort, D] (0) = 0

wlp[abort, D] (1) = 1

For the induction hypothesis we assume that for any two programs
c1 and c2 preservation of 0 and 1 holds. Then we can perform the
induction step:

if (G) {c1} else {c2}:

wp[if (G) {c1} else {c2}, D] (0)

= [G] · wp[c1, D] (0) + [¬G] · wp[c2, D] (0)
= [G] · 0 + [¬G] · 0
= 0

and

wlp[if (G) {c1} else {c2}, D] (1)

= [G] · wlp[c1, D] (1) + [¬G] · wlp[c2, D] (1)
= [G] · 1 + [¬G] · 1
= 1

{c1} [p] {c2}:

wp[{c1} [p] {c2}, D] (0)

= p · wp[c1, D] (0) + (1 − p) · wp[c2, D] (0)
= p · 0 + (1 − p) · 0
= 0

and

c1; c2:

and

wlp[if (G) {c1} else {c2}, D] (1)

= p · wlp[c1, D] (1) + (1 − p) · wlp[c2, D] (1)
= p · 1 + (1 − p) · 1
= 1

wp[c1; c2, D] (0) = wp[c1, D] (wp[c2, D] (0))

= wp[c1, D] (0)
= 0

wlp[c1; c2, D] (1) = wlp[c1, D] (wlp[c2, D] (1))

= wlp[c1, D] (1)
= 1

wp(cid:2)callDn P(cid:3) (0) = 0
wlp(cid:2)callDn P(cid:3) (1) = 1

for every n and hence

wp[call P, D] (0) = sup

and

n

wlp[call P, D] (1) = inf
n

wp(cid:2)callDn P(cid:3) (0) = 0
wlp(cid:2)callDn P(cid:3) (1) = 1 .

A.2 Fixed Point Characterization of Recursive Procedures
Establishing the results from Theorem 3.1 requires a subsidiary
result connecting w(l)p[·] with w(l)p[·]♯ in the presence of non–
recursive procedure calls.
Lemma A.1. For every command c and closed command c′,

wp[c]♯

wp[c′] = wp(cid:2)c, P ⊲ c′(cid:3) .

Proof. By induction on the structure of c. Except for procedure
calls, the proof for all other program constructs follows immediately 
from de deﬁnition of wp[·], wp[·]♯
(·) and the inductive hypotheses 
in the case of compound instructions. For the case of procedure 
calls, the proof relies on the fact that as c′ is a closed command,
 callP ⊲ c′
P = c′ for all n ≥ 1. Concretely, we reason as
follows:

n

wp[c]♯

wp[c′](f )
{def. wp[·]♯

wp[c′](f )

(·)}

{sup. of a constant sequence}

supn wp[c′](f )

{observation above}

supn wphcallP ⊲ c′
{wphcallP ⊲ c′
supn wphcallP ⊲ c′

n+1 Pi(f )
Pi(f ) = 0}
Pi(f )

{def. wp[·]}

n

0

=

=

=

=

=

wp[call P , P ⊲ c′]

Now we are in a position to prove Theorem 3.1. Consider ﬁrst

the case of ﬁxed point characterization

(cid:17) .
wp[call P , D] = lfp⊑(cid:16)λθ : SEnv • wp[D(P )]♯
}

{z

|

F

θ

Its proof comprises two major steps:

Reasoning about Recursive Probabilistic Programs

13

2016/3/10

1. Use the continuity of F : (SEnv, ⊑) → (SEnv, ⊑) established

by Lemma A.6 to conclude that

lfp⊑ (F ) = supn F n(⊥SEnv) ,

where F n denotes the composition of F with itself n times
(i.e. F 0 = id and F n+1 = F ◦ F n) and ⊥SEnv = λf : E • 0 is
the constantly 0 environment.

2. Show that

∀f : E • F n(⊥SEnv)(f ) = wp(cid:2)callDn P(cid:3)(f )

for all n ≥ 0.

Then the proof follows immediately since by deﬁnition of wp,

we have

wp[call P , D](f ) = supn wp(cid:2)callDn P(cid:3)(f )

= supn F n(⊥SEnv)(f ) = lfp⊑ (F ) (f ) .

We now consider each of these two steps in details. Step 1
follows immediately from an application of Kleene’s Fixed Point
Theorem. Step 2 proceeds by induction on n. The base case is
straightforward:

F 0(⊥SEnv)(f ) = ⊥SEnv(f ) = 0

For the inductive case we have

= wp[abort] (f ) = wp(cid:2)callD0 P(cid:3)(f ) .

=

=

=

=

=

=

F n+1(⊥SEnv)(f )

{def. of F n+1}

F(cid:0)F n(⊥SEnv)(cid:1)(f )
F n(⊥SEnv)(f )

{def. of F }

wp[D(P )]♯
{I.H.}
wp[D(P )]♯

(f )

wp[callD

n P]
{Lemma A.1}

wp[D(P ), P ⊲ callDn P ](f )

{Lemma A.4}

wp[D(P )[call P /callDn P ]](f )

{def. n-inl.}

wp[callDn+1 P ](f )

Now we turn to the ﬁxed point characterization

wlp[call P , D] = gfp⊑(cid:16)λθ : LSEnv • wlp[D(P )]♯
(cid:17) .
}

{z
gfp⊑ (G) = inf n Gn(⊤LSEnv) ,

The proof follows a dual argument. We ﬁrst apply Kleene’s Fixed
Point Theorem to show that

where ⊤LSEnv = λf : E≤1 • 1 is the constantly 1 environment. Next
we show by induction on n that

|

G

θ

∀f : E≤1 • Gn(⊤LSEnv)(f ) = wlp(cid:2)callDn P(cid:3)(f )

The proof concludes combining these two results since

wlp[call P , D](f ) = inf n wlp(cid:2)callDn P(cid:3)(f )

= inf n Gn(⊤LSEnv)(f ) = gfp⊑ (G) (f ) .

Lemma A.2. [32, p. 127] Suppose an,m are elements of upper ωcpo 
(A, ≤) with the property that an,m ≤ an′,m′ whenever n ≤ n′
and m ≤ m′. Then,

supn (supm an,m) = supm (supn an,m) = supi ai,i .

Lemma A.3 (Monotone Sequence Theorem). If hani is a monotonic 
increasing sequence in a closed interval [L, U ] ⊆ [−∞,
+∞], then the supremum supn an coincides with limn→∞ an. Dually,
 if hani is a monotonic decreasing sequence in a closed interval 
[L, U ] ⊆ [−∞, +∞], the inﬁmum inf n an coincides with
limn→∞ an.
A.3 Soundness of w(l)p Rules
Fact A.1. To carry on the proofs we use the fact that from
w(l)p[call P ](f1) ⊲⊳ g1 (cid:13) w(l)p[c](f2) ⊲⊳ g2 ,

it follows that for all environment D⋆,

w(l)p[call P , D
We provide detailed proofs for rules [wp-rec] and [wp-recω ]; the

⋆](f1) ⊲⊳ g1 =⇒ w(l)p[c, D

⋆](f2) ⊲⊳ g2 .

proof of rules [wlp-rec] and [wlp-recω] follows a dual argument.

Soundness of rule [wp-rec]. Since by deﬁnition, wp[call P , D](f )
= supn wp[callDn P , D](f ), to establish the conclusion of the rule
it sufﬁces to show that

∀n • wp(cid:2)callDn P(cid:3)(f ) (cid:22) g ,

which we do by induction on n. The base case is immediate since
callD0 P = abort and wp[abort] (f ) = 0. For the inductive case,
we reason as follows:

wp[callDn+1 P ](f ) (cid:22) g

⇔ wp[D(P )[call P/callDn P ]](f ) (cid:22) g
⇔ wp[D(P ), P ⊲ callDn P ](f ) (cid:22) g
⇐ wp[call P , P ⊲ callDn P ](f ) (cid:22) g
⇔ wp[call P [call P /callDn P]](f ) (cid:22) g
⇔ wp[callDn P ](f ) (cid:22) g

{def. n-inl.}
{Lemma A.4 }
{rule prem, Fact A.1}
{Lemma A.4}
{def. subst.}
{I.H.}

Soundness of rule [wp-recω ]. We prove that the rule’s premises
entail ln (cid:22) wp[callDn P ] (f ) (cid:22) un for all n ∈ N. The conclusion
of the rule then follows immediately by taking the supremum over
n on the three sides of the equation. We proceed by induction on
n. The base case is trivial since by deﬁnition, wp[callD0 P ] (f ) =
wp[abort] (f ) = 0 and by the rule’s premise, l0 = u0 = 0. For
the inductive case we reason as follows:

⇔

⇔

⇔

⇐

⇔

⇔

ln+1 (cid:22) wp[callDn+1 P ](f ) (cid:22) un+1

{def. n-inl.}

ln+1 (cid:22) wp[D(P )[call P/callDn P ]](f ) (cid:22) un+1

{Lemma A.4}

ln+1 (cid:22) wp[D(P ), P ⊲ callDn P ](f ) (cid:22) un+1

{rule prem, Fact A.1}

ln (cid:22) wp[call P , P ⊲ callDn P ](f ) (cid:22) un

{Lemma A.4}

ln (cid:22) wp[call P [call P /callDn P]](f ) (cid:22) un

{def. subst.}

ln (cid:22) wp[callDn P ](f ) (cid:22) un

{I.H.}

true

A.4 Substitution of Procedure Calls
Lemma A.4. For every command c and closed command c′,

wp(cid:2)c(cid:2)call P/c′(cid:3)(cid:3) = wp(cid:2)c, P ⊲ c′(cid:3) .

Proof. By induction on the structure of c. Except for procedure
calls, the proof for all other program constructs follows from de
deﬁnition of wp and some simple calculations (and the inductive

Reasoning about Recursive Probabilistic Programs

14

2016/3/10

c

c[call P /c′]

skip
x := E
abort
call P
if (G) {c1} else {c2}
{c1} [p] {c2}
c1; c2

skip
x := E
abort
c′
if (G) {c1[call P /c′]} else {c2[call P /c′]}
{c1[call P/c′]} [p] {c2[call P/c′]}
c1[call P /c′] ; c2[call P /c′]

Figure 7. Syntactic replacement of procedure calls.

hypotheses in the case of compound instructions). For the case of
procedure calls, the proof relies on the fact that as c′ is a closed
command, callP ⊲ c′
P = c′ for all n ≥ 1. Concretely, we reason as
follows:

n

wp[call P [call P/c′]](f )

{def. subst.}

wp[c′](f )

{sup. of a constant sequence}

supn wp[c′](f )

{observation above}

supn wphcallP ⊲ c′
{wphcallP ⊲ c′
supn wphcallP ⊲ c′

n+1 Pi(f )
Pi(f ) = 0}
Pi(f )

{def. wp[·]}

n

0

=

=

=

=

=

wp[call P , P ⊲ c′]

A.5 Continuity of Transformer w(l)p[·]♯
θ

c

wp[c]♯

θ (f )

skip
x := E
abort
if (G) {c1} else {c2}

{c1} [p] {c2}
call P

c1; c2

c

abort

f
f[x/E]
0
[G] · wp[c1]♯
p · wp[c1]♯
θ(f )
wp[c1]♯

wlp[c]♯

θ (f )

1

θ(cid:0)wp[c2]♯

θ (f )(cid:1)

θ (f ) + [¬G] · wp[c2]♯
θ (f ) + (1−p) · wp[c2]♯

θ (f )
θ (f )

Figure 8. Expectation transformer w(l)p[·]♯
differs from wp[·]♯
θ only in abort instructions.

θ. Transformer wlp[·]♯

θ

As a preliminary step to discuss the continuity of w(l)p[·]♯

(·) we
observe that order relation “⊑” (see paragraph below Theorem 3.1)
endows the set of environments SEnv with the structure of an
upper ω–cpo with botom element ⊥SEnv = λf : E • 0, where the
supremum of an increasing ω–chain θ0 ⊑ θ1 ⊑ · · ·
is given
pointwise, i.e. (supn θi)(f ) = supn θi(f ). Likewise, “⊑’ endows
the set of liberal environments LSEnv with the structure of a lower
ω–cpo with top element ⊤LSEnv = λf : E≤1 • 1, where the inﬁmum
of a decreasing ω–chain θ0 ⊒ θ1 ⊒ · · · is given pointwise, i.e.
(inf n θi)(f ) = inf n θi(f ).

We will discuss two kind of continuity results for w(l)p[·]♯
(·).
First, we show that for every environment θ, expectation transformer 
w(l)p[·]♯

θ is continuous, or equivalently, that
wp[c]♯
(·) : (SEnv, ⊑) → (SEnv, ⊑)
wlp[c]♯
(·) : (LSEnv, ⊑) → (LSEnv, ⊑)

This result will be established in Lemma A.5. Second, we show that
the above environment transformers are themselves continuous, i.e.
that

wp[c]♯

wlp[c]♯

(·) : (SEnv, ⊑)
(·) : (LSEnv, ⊑)

upp-cont
→ (SEnv, ⊑)

low-cont
→ (LSEnv, ⊑)

This result will be established in Lemma A.6.
Lemma A.5. Let θ ∈ SEnv and f0 (cid:22) f1 (cid:22) · · · be an ascending
ω–chain of expectations in E. Then for every command c,

wp[c]♯

θ (supn fn) = supn wp[c]♯

θ(fn) .

is a descending ω–chain of

Analogously, if f0 (cid:23) f1 (cid:23) · · ·
expectations in E≤1,
wlp[c]♯

θ (inf n fn) = inf n wlp[c]♯

θ(fn) .

Proof. By induction on the structure of c. Except for procedure
calls, all program constructs use the same proof argument as for
the continuity of plain transformer w(l)p[·], which has already been
dealt with in e.g. [11]. For procedure calls we reason as follows.

=

=

=

wp[call P ]♯

θ (supn fn)

{def. wp[·]♯

θ}

θ (supn fn)

{θ is continuous by hypothesis}

supn θ(fn)

{def. wp[·]♯

θ}
supn wp[call P ]♯

θ(fn) .

The reasoning to show that

wlp[call P ]♯

θ (inf n fn) = inf n wlp[call P ]♯

θ (fn)

is analogous.

Lemma A.6. Let θ0 ⊑ θ1 ⊑ · · · be an ascending ω–chain in
SEnv. Then for every command c,

Analogously, if θ0 ⊒ θ1 ⊒ · · · is a descending ω–chain in LSEnv,

wp[c]♯

supn θn

= supn wp[c]♯
θn

.

wlp[c]♯

infn θn

= inf n wlp[c]♯
θn

.

θ; the case of wlp[c]♯

Proof. By induction on the structure of c. We consider only the
case of wp[c]♯
θ is analogous. For the three basic
instructions c = skip, c = x := E and c = abort the proof
is straightforward since the action of transformer wp[·]♯
(·) on these
instructions is independent of the semantic environment at stake
(i.e. constant functions are always continuous). For the remaining
program constructs we reason as follows:

Procedure Call:

=

=

=

wp[call P ]♯

{def. wp[·]♯

supn θn
θ}

(f )

(supn θn)(f )

{def. supn θn}

supn θn(f )

{def. wp[·]♯
θ}
supn wp[call P ]♯
θn

(f ) .

Reasoning about Recursive Probabilistic Programs

15

2016/3/10

Sequential Composition:

=

=

=

=

⋆=

=

wp[c1; c2]♯

(f )

{def. wp[·]♯

supn θn
θ}

{I.H. on c2}

wp[c1]♯

wp[c1]♯

{Lemma A.5}

θn

supn θn

supm θm(cid:0)wp[c2]♯
supm θm(cid:0)supn wp[c2]♯
supm θm(cid:0)wp[c2]♯
θm(cid:0)wp[c2]♯
(f )(cid:1)

θi(cid:0)wp[c2]♯

θi

θn

(f )(cid:1)
(f )(cid:1)
(f )(cid:1)
(f )(cid:1)

θn

supn wp[c1]♯

{I.H. on c1}

supn supm wp[c1]♯
{Lemma A.2}

supi wp[c1]♯

{def. wp[·]♯
θ}
supi wp[c1; c2]♯
θi

(f )

For applying Lemma A.2 in step (*) we have to show that

θn

θn

wp[c1]♯

wp[c1]♯
wp[c1]♯

whenever n ≤ n′ and m ≤ m′. To this end, we use a transitivity
argument and show that

θm(cid:0)wp[c2]♯
θm(cid:0)wp[c2]♯
θm(cid:0)wp[c2]♯

(f )(cid:1) (cid:22) wp[c1]♯
(f )(cid:1) (cid:22) wp[c1]♯
θn′ (f )(cid:1) (cid:22) wp[c1]♯

θm′(cid:0)wp[c2]♯
θm(cid:0)wp[c2]♯
θm′(cid:0)wp[c2]♯

θn′ (f )(cid:1)
θn′ (f )(cid:1)
θn′ (f )(cid:1)

(2)
To prove Equation (1) we ﬁrst apply the I.H. on c2. Since continuity
entails monotonicity, we obtain wp[c2]♯
θn′ , which
θn
itself gives wp[c2]♯
θn′ (f ). We are left to show
θn
that wp[c1]♯
(·) is monotonic, which follows by its continuity
guaranteed by Lemma A.5. To prove Equation (2), we apply the
I.H. on c1. Again, since the continuity of wp[c1]♯
(·) implies its
monotonicity, we obtain wp[c1]♯
θm′ , which establishes
Equation (2).

(f ) (cid:22) wp[c2]♯

⊑ wp[c2]♯

⊑ wp[c1]♯

(1)

θm

θm

A.6 Basic Properties of Transformer ert
We begin by presenting some preliminary results that will be necessary 
for establishing the main results about the ert transformer.
Fact A.2 ((RtEnv, ⊑) is an ω–cpo). Let “⊑” denotes the pointwise
order between runtime environments, i.e. for η1, η2 ∈ RtEnv,
η1 ⊑ η2 iff η1(t) (cid:22) η2(t) for every t ∈ T. Relation “⊑” endows
the set of runtime environments RtEnv with the structure of an
upper ω–cpo with botom element ⊥RtEnv = λt : T • 0, where the
supremum of an increasing ω–chain η0 ⊑ η1 ⊑ · · ·
is given
pointwise, i.e. (supn ηi)(t) = supn ηi(t).
Lemma A.7 (Continuity of ert [·]♯
an ascending ω–chain in RtEnv. Then for every command c,

η w.r.t. η). Let η0 ⊑ η1 ⊑ · · · be

ert [c]♯

supn ηn

= supn ert [c]♯
ηn

.

Proof. The proof follows the same argument as that for establishing
the continuity of transformer wp (see Lemma A.6).

Lemma A.8 (ert [c]♯
(·) preserves continuity). For every command
c and every (upper continuous) runtime environment η ∈ RtEnv,
ert [c]♯

η is a continuous runtime transformer in T

upp-cont
→ T.

Proof. By induction on the program structure. For every program
constructs different from a procedure call, the reasoning is similar
to that used in Lemma A.5 to prove the same property for transformer 
wp[·]♯. For a procedure call the statement follows immediately 
since η is continuous by hypothesis.

Lemma A.9 (Alternative characterization of ert [call P, D]). Let
F (η) = 1 ⊕ ert [D(P )]♯

η. Then

ert [call P , D] = supn F n(⊥RtEnv) ,

where ⊥RtEnv = λt : T • 0 and F n(⊥RtEnv) denotes the repeated
application of F from ⊥RtEnv n times (i.e. F 0(⊥RtEnv) = id and
F n+1(⊥RtEnv) = F (F n(⊥RtEnv))).

Conditional Branching:

wp[call if (G) {c1} else {c2}]♯

supn θn

(f )

Proof. Using Lemma A.7 one can show that F is an (upper) continuous 
runtime transformer. The result then follows from a direct
application of Kleene’s Fixed Point Theorem and Fact A.2.

To present the following lemma we use the notion of expanding
runtime environments. Given η0, η1 ∈ RtEnv, θ ∈ SEnv and
k, ∆ ∈ R≥0 we say that hη1, η0, θi are hk, ∆i–expanding iff

implies

t1 − t0 (cid:23) k · (1−f ) + ∆

η1(t1) − η0(t0) (cid:23) k ·(cid:0)1 − θ(f )(cid:1) + ∆

for all t0, t1 ∈ T and f ∈ E≤1.
Lemma A.10. Let hη1, η0, θi be hk, ∆i–expanding environments12
and c be an abort–free command. Then

implies

t1 − t0 (cid:23) k · (1−f ) + ∆

ert [c]♯
η1

(t1) − ert [c]♯
η0

for all t0, t1 ∈ T and f ∈ E≤1.

(t0) (cid:23) k ·(cid:0)1 − wp[c]♯

θ(f )(cid:1) + ∆

=

=

(∗)
=

=

(∗∗)
=

=

{def. wp[·]♯

θ}

[G] · wp[c1]♯

supn θn
{I.H. on c1,c2}
[G] · supn wp[c1]♯
θn
{Lemma A.3}
wp[c1]♯
θn

[G] ·

lim
n→∞
{algebra of limits}

lim

{Lemma A.3}

n→∞(cid:0)[G] · wp[c1]♯
supn(cid:0)[G] · wp[c1]♯

{def. wp[·]♯

θ}

θn

θn

(f ) + [¬G] · wp[c2]♯

supn θn

(f )

(f ) + [¬G] · supn wp[c2]♯
θn

(f )

(f ) + [¬G] ·

wp[c2]♯
θn

(f )

lim
n→∞

(f ) + [¬G] · wp[c2]♯
θn

(f ) + [¬G] · wp[c2]♯
θn

(f )(cid:1)
(f )(cid:1)

supn wp[call if (G) {c1} else {c2}]♯
θn

To apply Lemma A.3 in steps (*) and (**) we have to show that
sequences hwp[c1]♯
(f )i are increasing. This
θn
follows by I.H. on c1 and c2 since continuity entails monotonicity.

(f )i and hwp[c2]♯
θn

Probabilistic Choice: follows the same argument as conditional
branching.

Proof. By induction on the structure of c.

12 See paragraph above.

Reasoning about Recursive Probabilistic Programs

16

2016/3/10

No–op:

Sequential Composition:

ert [skip]♯
η1

(t1) − ert [skip]♯
η0

(cid:23) k ·(cid:0)1 − wp[skip]♯

{def. of ert [·]♯

θ(f )(cid:1) + ∆

η, wp[·]♯
θ}

(t0)

(1 + t1) − (1 + t0) (cid:23) k · (1−f ) + ∆

{hypothesis}

true

⇔

⇐

Assignment:

⇔

⇐

⇐

⇐

(t0)

ert [c1; c2]♯
η1

(t1) − ert [c1; c2]♯
η0

ert [c1]♯

η, wp[·]♯
θ}

{def. of ert [·]♯

(cid:23) k ·(cid:0)1 − wp[c1; c2]♯
η1(cid:0)ert [c2]♯
(cid:23) k ·(cid:0)1 − wp[c1]♯

θ(f )(cid:1) + ∆
θ(cid:0)wp[c2]♯

{IH on c1}

(t1) − ert [c1]♯

η1

ert [c2]♯
η1

(t1) − ert [c2]♯
η0

{IH on c2}

t1 − t0 (cid:23) k · (1−f ) + ∆

(t0)

η0

η1(cid:0)ert [c2]♯

θ(f )(cid:1)(cid:1) + ∆
(t0) (cid:23) k ·(cid:0)1 − wp[c2]♯

θ(f )(cid:1) + ∆

{hypothesis}

true

(t0)

F i+1(⊥)(t1) − F i(⊥)(t0) (cid:23) 1 − wp[call P , D](1) ,

and then conclude using a telescopic sum argument as follows:

Reasoning about Recursive Probabilistic Programs

17

2016/3/10

ert [x := E]♯
η1

(t1) − ert [x := E]♯
η0

(t0)

η, wp[·]♯
θ}

{def. of ert [·]♯

θ(f )(cid:1) + ∆

(cid:23) k ·(cid:0)1 − wp[x := E]♯
(1 + t1)[x/E] − (1 + t0)[x/E] (cid:23) k ·(cid:0)1 − f [x/E](cid:1) + ∆
(t1 − t0)[x/E] (cid:23)(cid:0)k · (1−f ) + ∆(cid:1)[x/E]

{hypothesis}

{algebra}

true

⇔

⇔

⇐

Procedure Call:

(t0)

ert [call P]♯
η1

(t1) − ert [call P ]♯
η0

{def. of ert [·]♯

(cid:23) k ·(cid:0)1 − wp[call P ]♯
η1(t1) − η0(t0) (cid:23) k ·(cid:0)1 − θ(f )(cid:1) + ∆

θ(f )(cid:1) + ∆

{hη1, η0, θi is hk, ∆i–expanding}

η, wp[·]♯
θ}

t1 − t0 (cid:23) k · (1−f ) + ∆

{hypothesis}

true

⇔

⇐

⇐

Probabilistic Choice:

ert [{c1} [p] {c2}]♯
η1

(t1) − ert [{c1} [p] {c2}]♯
η0

θ(f )(cid:1) + ∆
(t0)(cid:1)
(t0)(cid:1)

(t1) − ert [c2]♯
η0
θ(f ) + (1−p) · wp[c2]♯

η1

η1

η, wp[·]♯
θ}

{def. of ert [·]♯

(t1) − ert [c1]♯
η0

(cid:23) k ·(cid:0)1 − wp[{c1} [p] {c2}]♯
p ·(cid:0)ert [c1]♯
+ (1−p) ·(cid:0)ert [c2]♯
(cid:23) k ·(cid:0)1 −(cid:0)p · wp[c1]♯
p ·(cid:0)k ·(cid:0)1 − wp[c1]♯
θ(f )(cid:1) + ∆(cid:1)
+ (1−p) ·(cid:0)k ·(cid:0)1 − wp[c2]♯
(cid:23) k ·(cid:0)1 −(cid:0)p · wp[c1]♯

{algebra (equality holds)}

{IH on c1, c2}

true

θ(f )(cid:1) + ∆(cid:1)

θ(f ) + (1−p) · wp[c1]♯

⇔

⇐

⇐

θ(f )(cid:1)(cid:1) + ∆

θ(f )(cid:1)(cid:1) + ∆

Conditional Branching: analogous to the case of probabilistic
choice.

Lemma A.11. Let P be an abort–free procedure with declaration
D. Then for every runtime t,

ert [call P ](t) (cid:23) supn n ·(cid:0)1 − wp[call P , D](1)(cid:1) .

Proof. Let F (η) = 1 ⊕ ert [D(P )]♯
η. Since by Lemma A.9,
ert [call P, D] = supn F n(⊥), the result follows from showing
that for all n ≥ 0,

F n+1(⊥)(t) (cid:23) (n + 1) ·(cid:0)1 − wp[call P , D](1)(cid:1) .

To establish this, we ﬁrst prove by induction on i that whenever
t1 − t0 (cid:23) 0,

F i+1(⊥)(t) − F i(⊥)(t)

F n+1(⊥)(t) = F 0(⊥)(t) +Xn

i=0

i=0

F i+1(⊥)(t) − F i(⊥)(t)

(cid:23) Xn
(cid:23) (n + 1) ·(cid:0)1 − wp[call P , D](1)(cid:1) .

For the inductive proof we reason as follows. For the base case we
have

F 1(⊥)(t1) − F 0(⊥)(t0) (cid:23) 1 − wp[call P , D](1)

{def. of F n, ⊥}

(t1) − ⊥(t0) (cid:23) 1 − wp[call P , D](1)

(t1) (cid:23) 1 − wp[call P , D](1)

⇔

⇔

⇐

1 + ert [D(P )]♯
⊥
{def. of ⊥}
1 + ert [D(P )]♯
⊥

true

{wp[call P , D](1) (cid:23) 0}

while for the inductive case we have,

No–op:

F i+2(⊥)(t1) − F i+1(⊥)(t0) (cid:23) 1 − wp[call P , D](1)

⇔

{def. of F n}

F i(⊥)(t0)(cid:1)

(cid:0)1 + ert [D(P )]♯

(cid:23) 1 − wp[call P , D](1)

{algebra}

F i+1(⊥)(t1)(cid:1) −(cid:0)1 + ert [D(P )]♯
F i(⊥)(t0)

wp[call P ,D]

by Theorem 3.1}

ert [D(P )]♯

ert [D(P )]♯

F i(⊥)(t0)

{wp[call P , D] = wp[D(P )]♯

F i+1(⊥)(t1) − ert [D(P )]♯
(cid:23) 1 ·(cid:0)1 − wp[call P , D](1)(cid:1) + 0
F i+1(⊥)(t1) − ert [D(P )]♯
(cid:23) 1 ·(cid:0)1 − wp[D(P )]♯
wp[call P ,D](1)(cid:1) + 0
(cid:10)F i+1(⊥), F i(⊥), wp[call P , D](cid:11) are h1, 0i–expanding
(cid:10)F i+1(⊥), F i(⊥), wp[call P , D](cid:11) are h1, 0i–expanding

t1 − t0 (cid:23) 1 · (1 − 1) + 0 and

{Lemma A.10}

{hypothesis}

{IH}

⇔

⇔

⇐

⇐

⇐

true

Lemma A.12. For every constant k ∈ R≥0 and abort–free program 
hc, Di,

ert [c, D](k) (cid:23) k .

Proof. By induction on the structure of c. Except for the case of
procedure calls, all other program constructs pose no difﬁculty.
For the case of a procedure call, we make a case distinction on
the termination behaviour of the procedure. If from state s the
procedure terminates almost surely, i.e. wp[call P , D] (1)(s) = 1,
the result follows from Theorem 5.2 and the linearity of wp[·] (see
Lemma 3.1) since

ert [call P, D](k)(s)

= ert [call P, D](0)(s) + wp[call P , D](k)(s)
≥ wp[call P , D](k)(s)
= k · wp[call P , D](1)(s) = k

If, on the contrary,
the procedure terminates with probability
strictly less than 1 from state s, we conclude applying Lemma A.11
since

ert [call P , D](k)(s)

(cid:1)
≥ supn n ·(cid:0)1 − wp[call P , D](1)(s)
}

= ∞ ≥ k .

{z

|

>0

For stating the following lemma we use the notion of “constant
separable” runtime environment. We say that η ∈ RtEnv is constant 
separable into υ ∈ RtEnv iff for all k ∈ R≥0 and t ∈ T,
η(k + t) = k + υ(t).

Lemma A.13. Let η be a runtime environment constant separable13 
into υ. Then for all command c,

ert [c]♯

η(k + t) = k + ert [c]♯

υ(t) .

Proof. By induction on the structure of c.

13 See paragraph above.

=

=

=

=

=

ert [skip]♯

η(k + t)
{def. of ert [·]♯

η}

1 + k + t

{def. of ert [·]♯
υ(t)

k + ert [skip]♯

υ}

ert [x := E]♯

η(k + t)
η}

{def. of ert [·]♯

(k + t)[x/E]

{k[x/E] = k}

k + t[x/E]

{def. of ert [·]♯
k + ert [x := E]♯

υ}
υ(t)

ert [call P ]♯
η1

{def. of ert [·]♯

(k + t)
η}

η(k + t)

{η constant separable into υ}

k + υ(t)

{def. of ert [·]♯
υ}
υ(t)

k + ert [call P]♯

Assignment:

Procedure Call:

=

=

=

Probabilistic Choice:

=

=

=

=

ert [{c1} [p] {c2}]♯
η(k + t)
{def. of ert [·]♯
η}

p · ert [c1]♯

η(k + t) + (1−p) · ert [c2]♯

η(k + t)

{I.H. on c1, c2}

p ·(cid:0)k + ert [c1]♯

{algebra}

k + p · ert [c1]♯

υ(t)(cid:1) + (1−p) ·(cid:0)k + ert [c2]♯

υ(t)(cid:1)

υ(t) + (1−p) · ert [c2]♯

υ(t)

{def. of ert [·]♯

υ}

k + ert [{c1} [p] {c2}]♯

υ(t)

Conditional Branching: analogous to the case of probabilistic
choice.

Sequential Composition:

=

=

=

=

ert [c1; c2]♯
η1

(k + t)
η}

{def. of ert [·]♯

ert [c1]♯

ert [c1]♯

{I.H. on c2}

η(cid:0)ert [c2]♯
η(cid:0)k + ert [c2]♯
υ(cid:0)ert [c2]♯

η(k + t)(cid:1)
υ(t)(cid:1)
υ(t)(cid:1)

{I.H. on c1}

{def. of ert [·]♯
υ}
υ(t)

k + ert [c1; c2]♯

k + ert [c1]♯

Lemma A.14. Let (D1, ≤1), (D2, ≤2) and (D, ≤) be upper ω–
cpos with botom elements ⊥1, ⊥2 and ⊥, respectively. Moreover
let F1 : D1 → D1, F2 : D2 → D2, f1 : D1 → D, f2 : D2 → D be
upper continuous and h1, h2 : D → D. If
1. ∀d1 • f1(F1(d1)) ≤ h1(f1(d1)) and ∀d2 • f2(F2(d2)) ≤

h2(f2(d2)),

Reasoning about Recursive Probabilistic Programs

18

2016/3/10

2. f1(⊥1) ≤ f2(lfp (F2)) and f2(⊥2) ≤ f1(lfp (F1)), and
3. h1(f2(lfp (F2))) ≤ f2(lfp (F2)) and h2(f1(lfp (F1))) ≤

f1(lfp (F1)),

fact that F preserves continuity, i.e. η continuous implies F (η)
continuous (see Lemma A.8).

then

Proof.

⇔

⇔

f1(lfp (F1)) = f2(lfp (F2)) .

f1(lfp (F1)) = f2(lfp (F2))

{”≤” is a partial order over D}

f1(lfp (F1)) ≤ f2(lfp (F2)) ∧ f2(lfp (F2)) ≤ f1(lfp (F1))

{Kleene’s Fixed Point Theorem, F1, F2 continuous}

We prove the above pair of inequalities by induction on n. We exhibit 
the details only for the ﬁrst one; the second one follows a

⇔

⇐

{f1, f2 continuous}

{∀n • an ≤ S =⇒ supn an ≤ S}

f1(cid:0)supn F n
∧ f2(cid:0)supn F n
supn f1(cid:0)F n
∧ supn f2(cid:0)F n
∀n • f1(cid:0)F n
∧ ∀n • f2(cid:0)F n

1 (⊥1)(cid:1) ≤ f2(lfp (F2))
2 (⊥2)(cid:1) ≤ f1(lfp (F1))
1 (⊥1)(cid:1) ≤ f2(lfp (F2))
2 (⊥2)(cid:1) ≤ f1(lfp (F1))
1 (⊥1)(cid:1) ≤ f2(lfp (F2))
2 (⊥2)(cid:1) ≤ f1(lfp (F1))
similar argument. The base case f1(cid:0)F 0
lows from hypothesis 2. For the inductive case f1(cid:0)F n+1
(⊥1)(cid:1)
1 (⊥1)(cid:1)(cid:1)
1 (⊥1)(cid:1)(cid:1)

f1(cid:0)F n+1
f1(cid:0)F(cid:0)F n
h1(cid:0)f1(cid:0)F n

f2(lfp (F2)) we reason as follows:

1
{def. of F n+1}

{IH, monot. of h1}

{hyp. 1}

=

≤

≤

h1(f2(lfp (F2)))

1 (⊥1)(cid:1) ≤ f2(lfp (F2)) fol-
(⊥1)(cid:1) ≤

1

≤

{hyp. 3}
f2(lfp (F2))

Proof of Theorem 5.1. The proof of all properties proceeds by
induction on the program structure. Except for the case of probabilistic 
choice and procedure call, all other programs constructs
have already been dealt with in [17, 18]. For probabilistic choice
we follow the same reasoning as for conditional branches. We are
left to analyze then only the case of procedure calls. For each of the
properties we reason as follows:

Continuity. Let F (η) = 1 ⊕ ert [D(P )]♯
η.

=

=

=

=

ert [call P, D](supn tn)

{Lemma A.9}

supm F m(⊥RtEnv)(supn tn)

{F m(⊥RtEnv) continuous; see below}

supm supn F m(⊥RtEnv)(tn)

{Lemma A.2}

supn supm F m(⊥RtEnv)(tn)

{Lemma A.9}

supn ert [call P, D](tn)

We are only left to prove that F m(⊥RtEnv) is continuous for all
m ∈ N. We prove this by induction on m. The base case is
immediate since F 0(⊥RtEnv) = ⊥RtEnv and ⊥RtEnv is continuous.
For the inductive case we have F m+1(⊥RtEnv) = F (F m(⊥RtEnv)).
The continuity of F m+1(⊥RtEnv) follows from the I.H. and the

Propagation of constants. By letting F (η) = 1 ⊕ ert [D(P )]♯
η we
can recast the property as lfp (F ) (k + t) = k + lfp (F ) (t), or
equivalently, as

• λt⋆

(cid:0)λη⋆

To prove this equation, we apply Lemma A.14 with instantiations

• η⋆(k+t⋆)(cid:1)(lfp (F )) =(cid:0)λη⋆

• λt⋆

• k+η⋆(t⋆)(cid:1)(lfp (F )) .

F1 = F2 = F
f1 = λη⋆
• λt⋆
f2 = λη⋆
• λt⋆
h1 = λη⋆
• λt⋆
h2 = λη⋆
• λt⋆

• η⋆(k + t⋆)
• k + η⋆(t⋆)
• 1 + ert [D(P )]♯
• k + 1 + ert [D(P )]♯

λt′

• η⋆(t′−k) (k + t⋆)
• η⋆(t′)−k (t⋆)

λt′

and underlying ω-cpos (D1, ≤1) = (D2, ≤2) = (D, ≤) =
(RtEnv, ⊑) and botom elements ⊥1 = ⊥2 = ⊥ = ⊥RtEnv. The
application of Lemma A.14 requires the continuity of F which follows 
from Lemma A.7, the continuity of f1 and f 2, which holds
because runtime environments are continuous by deﬁnition, and ﬁnally 
the monotonicity of h1 and h2. This latter fact, together with
the fact that h1 and h2 are effectively well–deﬁned (i.e. have type
RtEnv → RtEnv) can be proved with an inductive argument (on
the structure of D(P )).

We are left to discharge hypotheses 1–3 of Lemma A.14. A
simple unfolding of the involved functions yields f1(F (η)) ⊑
h1(f1(η)) and f2(F (η)) ⊑ h2(f2(η)) for all η ∈ RtEnv;
this establishes hypothesis 1. As for hypothesis 2, f1(⊥RtEnv) ⊑
f2(lfp (F )) holds because f1(⊥RtEnv) = ⊥RtEnv and f2(⊥RtEnv) ⊑
f1(lfp (F )) reduces to k (cid:22) ert [call P , D](k + t), which holds
in view of the monotonicity of transformer ert and Lemma A.12.
Finally, to discharge hypothesis 3 we reason as follows:

⇔

⇔

⇔

⇔

⇔

⇐

h1(f2(lfp (F )))(t) (cid:22) f2(lfp (F ))(t)

{def. of h1, f2, F ; let η(t′ ) = k + ert [call P , D](t′−k)}

1 + ert [D(P )]♯

η (k + t) (cid:22) k + ert [call P , D](t)

{η is constant separable into ert [call P , D]; Lemma A.13 }
ert[call P ,D] (t) (cid:22) k + ert [call P , D](t)

1 + k + ert [D(P )]♯

{def. of F }

k + F (ert [call P, D])(t) (cid:22) k + ert [call P, D](t)

{def. of ert}

k + F (lfp (F ))(t) (cid:22) k + lfp (F ) (t)

{def. of lfp}

k + lfp (F ) (t) (cid:22) k + lfp (F ) (t)

{”(cid:22)” is a partial order}

true

Reasoning about Recursive Probabilistic Programs

19

2016/3/10

⇔

⇔

⇔

⇔

⇔

⇐

h2(f1(lfp (F )))(t) (cid:22) f1(lfp (F ))(t)

{def. of h2, f1, F ; let υ(t′) = ert [call P , D](t′+k) − k}

k + 1 + ert [D(P )]♯

υ (t) (cid:22) ert [call P , D](k + t)

{ert [call P , D] is constant separable into υ; Lemma A.13 }

k + 1 +(cid:0)ert [D(P )]♯

(cid:22) ert [call P , D](k + t)
{algebra; def. of F }

ert[call P ,D] (k+t) − k(cid:1)

F (ert [call P, D])(k + t) (cid:22) ert [call P , D](k + t)

{def. of ert}

F (lfp (F ))(k + t) (cid:22) lfp (F ) (k + t)

{def. of lfp}

lfp (F ) (k + t) (cid:22) lfp (F ) (k + t)

{”(cid:22)” is a partial order}

true

Preservation of inﬁnity. By the monotonicity of ert [c, D] and
Lemma A.12, we have

which itself entails ert [c, D](∞) = ∞.

ert [c, D](∞) (cid:23) k ∀k ∈ R≥0 ,

A.7 Relation between Transformers ert and wp
To establish Theorem 5.2 we make use of a subsidiary result. This
result relies on the notion of separable runtime environment. We
say that a runtime environment η is separable into runtimes environments 
η1 and η2 iff we have η(t1 + t2) = η1(t1) + η2(t2) for
every any two runtimes t1 and t2.

Lemma A.15. For every command c and runtime environment η
separable into η1 and η2,

ert [c]♯

η(t1 + t2) = ert [c]♯
η1

(t1) + wp[c]♯
η2

(t2) .

Sequential Composition:

η (t1 + t2)(cid:1)

(t1) + wp[c2]♯
η2

ert [c1; c2]♯

η(t1 + t2)
η}

{def. of ert [·]♯

ert [c1]♯

ert [c1]♯

{I.H. on c2}

η(cid:0)ert [c2]♯
η(cid:0)ert [c2]♯
η1(cid:0)ert [c2]♯

η1

{I.H. on c1}

=

=

=

=

ert [c1]♯

η1
{def. of ert [·]♯
η , wp[·]♯
η}

(t1)(cid:1) + wp[c1]♯

ert [c1; c2]♯

η(t1) + wp[c1; c2]♯

η(t2)

(t2)(cid:1)
η2(cid:0)wp[c2]♯

η2

(t2)(cid:1)

Procedure Call:

=

=

=

ert [call P]♯

η(t1 + t2)
η}

{def. of ert [·]♯

η(t1 + t2)

{η sep. into η1, η2}

η1(t1) + η2(t2)

{def. of ert [·]♯

η , wp[·]♯
η}

ert [call P]♯
η1

(t1) + wp[call P ]♯
η2

(t2)

Proof of Theorem 5.2. The proof proceeds by induction on the
program structure, but for the inductive reasoning to work we need
to consider a stronger statement, namely

ert [c, D](t1 + t2) = ert [c, D](t1) + wp[c, D](t2) .

(3)
(We recover the original statement by taking t1 = 0). For all
program constructs c different from a procedure call, establishing 
Equation 3 follows exactly the same argument as that used in
Lemma A.15 for establishing

ert [c]♯

η(t1 + t2) = ert [c]♯
η1

(t1) + wp[c]♯
η2

(t2)

η and ert [·] obey the same deﬁnition rule for such

since ert [·]♯
program constructs.

For the case of a procedure call we have to prove that

Proof. For the basic instructions (skip, abort and assignment), the
statement follows immediately from the deﬁnitions of ert and wp.
For the remaining program constructs we reason as follows:

ert [call P, D] (t1 + t2) = ert [call P, D] (t1) + wp[call P , D] (t2) .
Since

Conditional Branching:

ert [if (G) {c1} else {c2}]♯

η(t1 + t2)

=

=

=

=

η}
η(t1 + t2) + [¬G] · ert [c2]♯

η(t1 + t2)

{def. of ert [·]♯
1 + [G] · ert [c1]♯
{I.H. on c1,c1}

1 + [G] ·(cid:0)ert [c1]♯
+ [¬G] ·(cid:0)ert [c2]♯

{algebra}

η1

(t1) + wp[c1]♯
η2
(t1) + wp[c2]♯
η2

η1

(t2)(cid:1)
(t2)(cid:1)

1 + [G] · ert [c1]♯
(t1) + [¬G] · ert [c2]♯
η1
η1
+ [G] · wp[c1]♯
(t2) + [¬G] · wp[c2]♯
η2
η2
{def. of ert [·]♯
η , wp[·]♯
η}
ert [if (G) {c1} else {c2}]♯
η1

(t1)

+wp[if (G) {c1} else {c2}]♯
η2

(t2)

(t1)
(t2)

Probabilistic Choice: analogous to the conditional branching case.

ert [call P , D] = lfp (F ) where F (η) = 1 ⊕ ert [D(P )]♯
η
wp[call P , D] = lfp (G) where G(θ) = wp[D(P )]♯

θ ,

and both F and G are continuous (see Lemma A.6 and Lemma A.7),
by Kleene’s Fixed Point Theorem our statement can be recast as

supn F n(⊥RtEnv)(t1 + t2) =

supn F n(⊥RtEnv)(t1) + supn Gn(⊥SEnv)(t2) ,

where ⊥SEnv = λf : E • 0, ⊥RtEnv = λt : T • 0, F n(⊥RtEnv) =
F (. . . F (F (⊥RtEnv)) . . .) denotes the repeated application of F
from ⊥RtEnv n times and likewise for Gn(⊥SEnv). Since a standard 
property of complete partial orders ensures that F n(⊥RtEnv)
and Gn(⊥SEnv) are monotonic w.r.t. n, we can use the Monotone
Sequence Theorem (Lemma A.3) to replace supn with limn→∞ in
the above equation and this way “merge” the two limits in the RHS
into a single limit. The above equation is then entailed by formula

∀n • F n(⊥RtEnv)(t1+t2) = F n(⊥RtEnv)(t1) + Gn(⊥SEnv)(t2) ,
which we prove by induction on n. The base case is immediate
since for every runtime t, F 0(⊥RtEnv)(t) = G0(⊥SEnv)(t) = 0.
For the inductive case we reason as follows:

Reasoning about Recursive Probabilistic Programs

20

2016/3/10

F n+1(⊥RtEnv)(t1 + t2) =

F n+1(⊥RtEnv)(t1) + Gn+1(⊥SEnv)(t2)

⇔

{def. F n+1, Gn+1}

1 + ert [D(P )]♯

F n(⊥RtEnv) (t1 + t2) =

1 + ert [D(P )]♯

F n(⊥RtEnv) (t1) + wp[D(P )]♯

Gn(⊥SEnv) (t2)

⇔

{algebra}

ert [D(P )]♯

F n(⊥RtEnv) (t1 + t2) =

ert [D(P )]♯

F n(⊥RtEnv) (t1) + wp[D(P )]♯

Gn(⊥SEnv) (t2)

⇐

{Lemma A.15, I.H.}

true

A.8 Soundness of Proof Rules for ert
To establish the soundness of rules [eet-rec] and [eet-recω ] we make
use of the following result.

Fact A.3. The derivability assertion

ert [call P ](t1) (cid:22) u1 (cid:13) ert [c](t2) (cid:22) u2

implies that for every runtime environment η,

η(t2) (cid:22) u2 .
The result remain valid if we reverse all inequalities.

η(t1) (cid:22) u1 =⇒ ert [c]♯

We have already used a similar result for establishing the soundness 
of rules [wp-rec] and [wp-recω ] (even though in that case the
conclusion was stated using wp[·] instead of wp[·]♯

θ).

Soundness of rule [eet-rec]. Let runtime environment η⋆ map t to
u and all other runtimes to (the constant runtime) ∞. The validity
of the rule follows from the following reasoning:

⇔

⇔

⇐

⇔

⇔

⇐

⇔

η(cid:1)(t) (cid:22) 1 + u
η(cid:1) ⊑ 1 ⊕ η⋆

{Park’s Lemma14, Fact A.2, Lemma A.7}

ert [call P, D](t) (cid:22) 1 + u

{def. ert (Figure 2)}

{def. η⋆,⊑}

lfp⊑(cid:0)λη : RtEnv • 1 ⊕ ert [D(P )]♯
lfp⊑(cid:0)λη : RtEnv • 1 ⊕ ert [D(P )]♯
1⊕η⋆ ⊑ 1 ⊕ η⋆

1 ⊕ ert [D(P )]♯
{def. η⋆,⊑}
1 + ert [D(P )]♯

1⊕η⋆(t) (cid:22) 1 + u

{algebra}

ert [D(P )]♯

1⊕η⋆(t) (cid:22) u

{Fact A.3, rule premise}

(1 ⊕ η⋆)(t) (cid:22) 1 + u

{def. η⋆}

true

Soundness of rule [eet-recω ]. For simplicity, we consider the one–
side version of the rule for obtaining lower bound only:

1 + ln (cid:22) ert [call P](t) (cid:13) ln+1 (cid:22) ert [D(P )](t)

l0 = 0

1+ supn ln (cid:22) ert [call P, D](t)

14 If H : D → D is an upper continuous function over an upper ω–cpo
(D, ⊑) with bottom element, then H(d) ⊑ d implies lfp⊑ (H) ⊑ d for
every d ∈ D [31].

The reasoning for the orignal—two–side rule—is analogous. The
validity of the above rule follows from the following reasoning:

⇔

⇔

1 + supn ln (cid:22) ert [call P , D](t)

{def. ert (Figure 2), F (η) = 1 ⊕ ert [D(P )]♯

η}

1 + supn ln (cid:22) lfp⊑ (F ) (t)

{Kleene’s Fixed Point Thm, Lemma A.7}

1 + supn ln (cid:22) supn F n(⊥RtEnv)(t)

Since F n(⊥RtEnv) is monotonic w.r.t. n, supn F n(⊥RtEnv) =
supn F n+1(⊥RtEnv) and the reasoning continues as follows:

⇔

⇔

⇐

1 + supn ln (cid:22) supn F n+1(⊥RtEnv)(t)

{k + supn an = supn k + an}

supn 1 + ln (cid:22) supn F n+1(⊥RtEnv)(t)

∀n • 1 + ln (cid:22) F n+1(⊥RtEnv)(t)

We prove the above statement by induction on n. For the base case
we have

1 + l0 (cid:22) F 1(⊥RtEnv)(t)

{rule premise, def F 1(⊥RtEnv)}

1 (cid:22) 1 + ert [D(P )]♯

{ert [D(P )]♯

⊥RtEnv

(t)

⊥RtEnv

(t) (cid:23) 0}

⇔

⇐

For the inductive case we have

true

⇔

⇔

⇐

⇔

1 + ln+1 (cid:22) F n+2(⊥RtEnv)(t)

{def F n+2(⊥RtEnv)}

1 + ln+1 (cid:22) 1 + ert [D(P )]♯

{algebra}

F n+1(⊥RtEnv)(t)

ln+1 (cid:22) ert [D(P )]♯

F n+1(⊥RtEnv)(t)

{Fact A.3, rule premise}
1 + ln (cid:22) F n+1(⊥RtEnv)(t)

{I.H.}

true

A.9 Operational Model of pGCL
Deﬁnition A.1 (Pushdown Markov Chains with Rewards). A
pushdown Markov chain with rewards (PRMC) is a tuple P =
(Q, qinit , Γ, γ0, ∆, rew), where

• Q is a countable set of control states,
• qinit ∈ Q is the initial control state,
• Γ is a ﬁnite stack alphabet,
• γ0 ∈ Γ is a special bottom–of–stack symbol,

• ∆ : Q × Γ 99K D(Q) ×(cid:0)Γ \ {γ0}(cid:1)∗ (where D(Q) denotes

the set of probability distributions over Q) is a probabilistic
transition relation,

• rew : Q → R≥0 is a reward function.
ak−→
A path of P is a ﬁnite sequence ρ = (q0, β0)
(qk, βk), where q0 = qinit , β0 = γ0, and for all 1 ≤ i ≤ k

a1−→ · · ·

holds βi ∈ γ0 · (cid:0)Γ \ {γ0}(cid:1)∗ and ∃ µ ∈ D(Q) and ∃ γ1 ∈ Γ

and ∃ γ2 ∈ Γ \ {γ0} ∪ {ε}, such that ∆(qi−1, γ1) = (µ, γ2)
and βi−1 = w · γ1 and βi = w · γ2 and µ(qi) = ai > 0.
The set of paths in P is denoted by PathsP. In the following
ak−→ (qk, βk). The probability of ρ
let ρ = (q0, β0)
i=1 ai be a path. The reward of a
i=0 rew (qi). The
expected reward for reaching a set of target states T ⊆ Q is

is given by ProbP (ρ) = Qk
path ρ is given by rew (ρ) = ProbP (ρ) · Pk
given by ExpRewP (T ) = Pρ′∈P rew (ρ′) where P = {ρ′ ∈

a1−→ · · ·

Reasoning about Recursive Probabilistic Programs

21

2016/3/10

rew (ρ′) = 0.

aj−→ (qj , βj ), qj ∈ T, ∀ 0 ≤ ℓ <
PathsP | ρ′ = (q0, β0)
j : qℓ 6∈ T }. We stick to the convention that an empty sum yields

a1−→ · · ·

value zero, i.e. in particularPρ′∈∅

We assume a given labeling for each program c ∈ C that
speciﬁes the control ﬂow of c as illustrated in Section A.9. Let
Lab∗ denote the ﬁnite set of labels used in a given program C. We
assume a special symbol ↓ to denote successful termination of a
program. Furthermore, we make use of the following operations
between statements and labels.
• init : C → Lab∗ gives the label corresponding to the beginning

of a given program.

• stmt : Lab∗ → (C ∪ {↓}) gives the statement associated to a

label used in a program,

• succ1, succ2 : Lab∗ →(cid:0)Lab∗ ∪ {↓}(cid:1) give the ﬁrst and second

successor label of a given program label. In case ℓ ∈ Lab∗ has
no such successor, we deﬁne succ1 (ℓ) = ↓ and succ2 (ℓ) = ↓,
respectively.

Deﬁnition A.2 (Operational PRMCs). Let σ0 ∈ S and f ∈ E.
The operational PRMC of program hc, Di starting in initial state
σ0 with respect to post–expectation f is given by Pf
σ0 Jc, DK =
(Q, qinit , Γ, γ0, ∆, rew) where

• Q = (cid:8)(ℓ, σ) | ℓ ∈ Lab∗ ∪ {↓, Term}, σ ∈ S(cid:9),

• qinit = hinit(c), σ0i,
• Γ = Lab∗ ∪ {γ0},
• ∆ is given by the least partial function satisfying the rules
provided in Figure 3,

• rew (hTerm, σi) = f (σ) for each σ ∈ S and rew (q) = 0, if q

is not of the form hTerm, σi.

σJcKi (T ) ,
σ JcK in the sense

A.10 Soundness of Transformer wp
Proof of Theorem 6.1. For simplicity in the remainder we will
assume the program declaration D ﬁxed and therefore, omit it.

Consider ﬁrst an automaton n(cid:10)Pf

same as Pf
σ JcK, but counts the number of symbols that currently
lie on top of γ0 on the stack and which self–loops if that number is
exactly n and Pf
σ JcK would perform another push onto the stack.
It is evident that
ExpRewPf

σ JcK(cid:11) that behaves exactly the

ExpRew

nhPf

σJcK (T ) = sup
n∈N

σ JcK(cid:11) exhibits a partial behavior of Pf

σ JcK. In the other direction, every path π of Pf

also a path of Pf
σ JcK
that reaches T can be implemented with ﬁnite stack size. Therefore,
there exists an n0 ∈ N such that for all n ≥ n0 the path π is also a

σ JcK(cid:11) that reaches T is (up to renaming)

since n(cid:10)Pf
that every path of n(cid:10)Pf
path of n(cid:10)Pf

σ JcK(cid:11).

Consider now that by Theorem 3.1 and its proof we can conclude 
that

wp[c]♯

wp[callD

n P]

sup
n∈N

(f ) = wp[c, D] (f ) .

It is therefore only left to show that the missing link

λσ • ExpRew

nhPf

σJcKi (T ) = wp[c]♯

wp[callD

n P]

(f )

holds for all n ∈ N. The proof of this equality proceeds by
induction on n:

The base case n = 0: We have to show that
σJcKi (T ) = wp[c]♯

λσ • ExpRew

0hPf

holds. Whenever the automaton Pf

wp[callD

0 P]

(f )

σ JcK would perform the push

action associated with a procedure call, the automaton 0(cid:10)Pf

σ JcK(cid:11)

immediately self–loops as no push to the stack whatsoever is allowed 
in this restricted automaton. Therefore, we can syntactically
replace every call in c by an abort and still obtain the same behavior 
for the corresponding restricted automaton. Formally,

ExpRew

0hPf

Now, since syntactically callD0 P = abort we have

σJcKi (T ) = ExpRew
wp[c]♯

wp[callD

0 P]

(f ) = wp[c]♯

abort (f )

0hPf

σJc[call P/abort]Ki (T ) .

and therefore, it is left to show that

0hPf

λσ • ExpRew

holds. The proof of this equality proceeds by structural induction
on c: For the base cases we have:

σJc[call P/abort]Ki (T ) = wp[c]♯

abort (f )

The effectless program skip: On the denotational side, we have

wp[skip]♯

abort (f )(σ) = f (σ) .

On the operational side we have skip[call P /abort] = skip. Let
init(skip) = ℓ, stmt (ℓ) = skip, and succ1 (ℓ) = ↓. The only path

σ JskipK(cid:11) reaching T is

of 0(cid:10)Pf
ρ =(cid:0) hℓ, σi , γ0(cid:1) 1−→(cid:0) h↓, σi , γ0(cid:1) 1−→(cid:0) hTerm, σi , γ0(cid:1)

and its reward is

As ρ is the only path reaching T , we have

1 · 1 ·(cid:0)0 + 0 + f (σ)(cid:1) = f (σ) .
σJskipKi (T ) = f (σ) = wp[skip]♯

ExpRew

0hPf

abort (f )(σ) .

The diverging program abort: On the denotational side, we have

wp[abort]♯

abort (f )(σ) = 0(σ) = 0 .

On the operational side we have abort[call P/abort] = abort.
Let init(abort) = ℓ, stmt (ℓ) = abort, and succ1 (ℓ) = ↓. The

σ JabortK(cid:11) are all of the form

paths of 0(cid:10)Pf
(cid:0) hℓ, σi , γ0(cid:1) 1−→(cid:0) hℓ, σi , γ0(cid:1) 1−→(cid:0) hℓ, σi , γ0(cid:1) 1−→ · · ·

and none of them ever reaches T . Thus the expected reward is an
empty sum and we therefore have

ExpRew

0hPf

σJabortKi (T ) = 0 = wp[abort]♯

abort (f )(σ) .

The assignment x := E: On the denotational side, we have

wp[x := E]♯

abort (f )(σ) = f [x/E] (σ)

On the operational side we have x := E[call P/abort] = x :=
E. Let init(x := E) = ℓ, stmt (ℓ) = x := E, and succ1 (ℓ) = ↓.

= f(cid:0)σ(cid:2)x 7→ σ(E)(cid:3)(cid:1) .

The only path of 0(cid:10)Pf

ρ = (cid:0) hℓ, σi , γ0(cid:1) 1−→(cid:0)(cid:10)↓, σ(cid:2)x 7→ σ(E)(cid:3)(cid:11) , γ0(cid:1)

σ Jx := EK(cid:11) reaching T is
1−→(cid:0)(cid:10)Term, σ(cid:2)x 7→ σ(E)(cid:3)(cid:11) , γ0(cid:1)

and its reward is

1 · 1 ·(cid:16)0 + 0 + f(cid:0)σ(cid:2)x 7→ σ(E)(cid:3)(cid:1)(cid:17) = f(cid:0)σ(cid:2)x 7→ σ(E)(cid:3)(cid:1) .

As ρ is the only path reaching T , we have

ExpRew

0hPf

σJx:=EKi (T ) = f(cid:0)σ(cid:2)x 7→ σ(E)(cid:3)(cid:1)

= wp[x := E]♯

abort (f )(σ) .

The call call P: On the denotational side, we have

wp[call P ]♯

abort (f )(σ) = wp[abort]♯

abort (f )(σ)

On the operational side we have call P [call P /abort] = abort.

Therefore, we can fall back to the base case abort.

Reasoning about Recursive Probabilistic Programs

22

2016/3/10

The inductive hypothesis on c1 and c2: We now assume that for
arbitrary but ﬁxed programs ci, with i ∈ {1, 2}, holds
σJci[call P/abort]Ki (T ) = wp[ci]♯

We can then proceed with the inductive steps:

λσ • ExpRew

abort (f ) .

0hPf

The sequential composition c1; c2: On the denotational side, we
have

wp[c1; c2]♯
Operationally, we have

abort (f )(σ) = wp[c1]♯

abort(cid:16)wp[c2]♯

abort (f )(cid:17) (σ) .

(c1; c2)[call P/abort] = c1[call P /abort] ; c2[call P /abort] .

We furthermore observe that any path of the automaton

reaching T is of the form

0DPf

σ Jc1[call P /abort] ; c2[call P/abort]KE
ρ = (cid:0) hinit(c1[call P /abort]), σi , γ0(cid:1) a1−→ · · ·

and any such a path’s reward is given by

is a path of

and it’s reward is given by

and reward

k

Yi=1 ai! ·(cid:0)0+ · · · +0 + wp[c2[call P /abort]]♯

= wp[c2[call P /abort]]♯

abort (f )(σ′) .

abort (f )(σ′)(cid:1)

Keeping that in mind and applying the inductive hypothesis to c1
now yields the desired statement:

ExpRew

0hPf

= ExpRewP

= wp[c1]♯

wp[c2[call P/abort]](f )
σ

σJc1[call P /abort];c2[call P/abort]Ki (T )
abort(cid:16)wp[c2]♯

abort (f )(cid:17) (σ)

Jc1[call P /abort]K (T )

= wp[c1; c2]♯

abort (f )(σ)

(I.H. on c1)

The conditional choice if (G) {c1} else {c2}: We distinguish two
cases:

In Case 1 we have σ |= G. Then on the denotational side, we

have

wp[if (G) {c1} else {c2}]♯

abort (f )(σ)

([G] (σ) = 1 and [¬G] (σ) = 0)

abort (f )(cid:1)(σ)

abort (f ) + [¬G] · wp[c2]♯

= wp[c1]♯

abort (f )(σ)

On the operational side we have

= (cid:0) [G] · wp[c1]♯
(cid:0)if (G) {c1} else {c2}(cid:1)[call P /abort]

= if (G) {c1[call P/abort]} else {c2[call P /abort]} .

Regarding the control ﬂow, let the following hold:
init(if (G) {c1[call P /abort]} else {c2[call P/abort]}) = ℓ,
stmt (ℓ) = if (G) {c1[call P /abort]} else {c2[call P /abort]},
succ1 (ℓ) = init(c1[call P /abort]), and ﬁnally
succ2 (ℓ) = init(c2[call P /abort]). We observe that any path of

0(cid:10)Pf

reaching T is of the form

σ Jif (G) {c1[call P /abort]} else {c2[call P /abort]}K(cid:11) ﬁnally
ρ = (cid:0) hℓ, σi , γ0(cid:1)

k

1−→(cid:0) hinit(c1[call P /abort]), σi , γ0(cid:1) a2−→ · · ·
ak−→(cid:0)(cid:10)↓, σ′(cid:11) , γ0(cid:1) 1−→(cid:0)(cid:10)Term, σ′(cid:11) , γ0(cid:1)
Yi=2 ai! ·(cid:0)0 + 0 + · · · + 0 + f (σ′)(cid:1)
Yi=2 ai! · f (σ′) .

=

k

1 ·

Next, observe that removing from any such path ρ the initial segward


ment, i.e. removing(cid:0) hℓ, σi , γ0(cid:1) 1−→ , gives a path of the form

(cid:0) hinit(c1[call P/abort]), σi , γ0(cid:1) a2−→ · · ·
ak−→(cid:0)(cid:10)↓, σ′(cid:11) , γ0(cid:1) 1−→(cid:0)(cid:10)Term, σ′(cid:11) , γ0(cid:1) ,

which is a path of 0(cid:10)Pf
Yi=2 ai! ·(cid:0)0 + · · · + 0 + f (σ′)(cid:1) =
σJc1[call P/abort]Ki. Thus

σ Jc1[call P /abort]K(cid:11) reaching T with re-
Yi=2 ai! · f (σ′) .
σJif (G) {c1[call P/abort]} else {c2[call P/abort]}Ki we obtain exσ 
Jif (G) {c1[call P /abort]} else {c2[call P/abort]}KE

Notice that if we remove the initial segments from every path in
Paths
actly the set Paths

0DPf

0hPf

0hPf

k

k

Next, we observe that for any such path ρ a sufﬁx of it, namely

k

ak′

ak−→(cid:0)(cid:10)↓, σ′(cid:11) , γ0(cid:1)
1−→(cid:0)(cid:10)init(c2[call P /abort]), σ′(cid:11) , γ0(cid:1) ak+2−−−→ · · ·
−→(cid:0)(cid:10)↓, σ′′(cid:11) , γ0(cid:1)
1−→(cid:0)(cid:10)Term, σ′′(cid:11) , γ0(cid:1)
Yi=1 ai! ·
0 + · · · + 0
 ·(cid:0)0 + · · · + 0 + f (σ′′)(cid:1)
ai

Yi=k+2

ai

Yi=1 ai! ·
Yi=k+2
 · f (σ′′)

+

=

k′

k′

k

k′

ak′

(cid:0)(cid:10)init(c2[call P /abort]), σ′(cid:11) , γ0(cid:1) ak+2−→ · · ·
−→(cid:0)(cid:10)↓, σ′′(cid:11) , γ0(cid:1) 1−→(cid:0)(cid:10)Term, σ′′(cid:11) , γ0(cid:1) ,
σ′ Jc2[call P /abort]KE reaching T with reward
0DPf
ai

Yi=k+2
 ·(cid:0)0 + · · · + 0 + f (σ′′)(cid:1)
ai

Yi=k+2
 · f (σ′′) .
σ′ Jc2[call P/abort]KE
0DPf

=

k′

Moreover, we can think of the expected reward of

as an expectation

f

λσ′ • ExpRewP

σ′Jc2[call P/abort]K (T ) ,
which by the inductive hypothesis on c2 is equal to
abort (f ) .
abort(f )

wp[c2]♯
wp[c2[call P/abort]]♯
σ

Jc1[call P /abort]K(cid:29) and
σ Jc1[call P/abort] ; c2[call P/abort]K(cid:11) have the same expected 
reward, as in the former all paths reaching T have the form

0(cid:10)Pf

0(cid:28)P

Therefore,

(cid:0) hinit(c1[call P/abort]), σi , γ0(cid:1) a1−→ · · ·
ak−→(cid:0)(cid:10)↓, σ′(cid:11) , γ0(cid:1) 1−→(cid:0)(cid:10)Term, σ′(cid:11) , γ0(cid:1)

Reasoning about Recursive Probabilistic Programs

23

2016/3/10

ExpRew

ward. This immediately yields the desired statement:

σ Jc1[call P /abort]K(cid:11) have the same expected reas 
well as 0(cid:10)Pf
σs(cid:0)if (G) {c1} else {c2}(cid:1)[call P /abort]{(cid:29) (T )
0(cid:28)Pf
0hPf
σJif (G) {c1[call P/abort]} else {c2[call P/abort]}Ki (T )
σJc1[call P/abort]Ki (T )

= ExpRew

0hPf

= ExpRew
= wp[c1]♯
= wp[if (G) {c1} else {c2}]♯

abort (f )(σ)

abort (f )(σ)

The reasoning for Case 2, i.e. σ 6|= G, is completely analogous
using the inductive hypothesis on c2

(I.H. on c1)

The probabilistic choice {c1} [p] {c2}: On the denotational side,
we have

wp[{c1} [p] {c2}]♯

abort (f )(σ)

abort (f ) + (1 − p) · wp[c2]♯
abort (f )(σ) + (1 − p) · wp[c2]♯

On the operational side we have

= p · wp[c1]♯

= (cid:0)p · wp[c1]♯
(cid:0){c1} [p] {c2}(cid:1)[call P/abort]

= {c1[call P/abort]} [p] {c2[call P /abort]}

abort (f )(cid:1)(σ)

abort (f )(σ)

Let init({c1[call P/abort]}[p]{c2[call P/abort]}) = ℓ, stmt (ℓ) =
{c1[call P /abort]} [p] {c2[call P /abort]}, let
succ1 (ℓ) = init(c1[call P /abort]), and let
succ2 (ℓ) = init(c2[call P /abort]). We observe that any path of

0(cid:10)Pf

either of the form

σ J{c1[call P /abort]} [p] {c2[call P /abort]}K(cid:11) reaching T is
ρ1 = (cid:0) hℓ, σi , γ0(cid:1)

p

k

−→(cid:0) hinit(c1[call P/abort]), σi , γ0(cid:1) a2−→ · · ·
ak−→(cid:0)(cid:10)↓, σ′(cid:11) , γ0(cid:1) 1−→(cid:0)(cid:10)Term, σ′(cid:11) , γ0(cid:1)
Yi=2 ai! ·(cid:0)0 + · · · + 0 + f (σ′)(cid:1)!
p · 0 +
Yi=2 ai! · f (σ′) ,

= p ·

k

and it’s reward is given by

or it is of the form

and it’s reward is given by

ρ2 =(cid:0) hℓ, σi , γ0(cid:1) 1−p

−−→

k′

k′

a′

2−→ · · ·

(cid:0) hinit(c2[call P /abort]), σi , γ0(cid:1) a′
−→(cid:0)(cid:10)↓, σ′′(cid:11) , γ0(cid:1) 1−→(cid:0)(cid:10)Term, σ′′(cid:11) , γ0(cid:1)
(1 − p) ·
a′i
 ·(cid:0)0 + · · · + 0 + f (σ′′)(cid:1)

Yi=2
0 +

a′i

Yi=2
 · f (σ′′) .
σJ{c1[call P/abort]} [p] {c2[call P/abort]}Ki

= (1 − p) ·

Paths

0hPf

k′

Notice that there is a possibility to partition the set

into two sets Pp containing those paths starting with

p

−→ (cid:0) hinit(c1[call P /abort]), σi , γ0(cid:1), and a set

(cid:0) hℓ, σi , γ0(cid:1)
P1−p containing those paths starting with (cid:0) hℓ, σi , γ0(cid:1)
(cid:0) hinit(c2[call P /abort]), σi , γ0(cid:1).

1−p
−→

Next, observe that removing from any path in Pp the initial
−→ , gives exactly the set

p

segment, i.e. removing (cid:0) hℓ, σi , γ0(cid:1)

Paths
reaching T are of the form

0hPf

σ Jc1[call P/abort]K(cid:11)

σJc1[call P/abort]Ki. The paths of 0(cid:10)Pf
(cid:0) hinit(c1[call P/abort]), σi , γ0(cid:1) a2−→ · · ·
ak−→(cid:0)(cid:10)↓, σ′(cid:11) , γ0(cid:1) 1−→(cid:0)(cid:10)Term, σ′(cid:11) , γ0(cid:1) ,

and have reward

k

Yi=2 ai! ·(cid:0)0 + · · · + 0 + f (σ′)(cid:1) =
removing(cid:0) hℓ, σi , γ0(cid:1) 1−p

0hPf

Dually, removing from any path in P1−p the initial segment, i.e.

−−→ , gives exactly the set

k

Yi=2 ai! · f (σ′).

The paths of Pf

k′

a′

Paths

σ Jc2[call P/abort]K reaching T are of the form

σJc2[call P /abort]Ki .
(cid:0) hinit(c2[call P /abort]), σi , γ0(cid:1) a′
−→(cid:0)(cid:10)↓, σ′′(cid:11) , γ0(cid:1) 1−→(cid:0)(cid:10)Term, σ′′(cid:11) , γ0(cid:1) ,
a′i
a′i


Yi=2
Yi=2
 ·(cid:0)0 + · · · + 0 + f (σ′)(cid:1) =
 · f (σ′′).

Since Pp and P1−p was a partition of the path set

2−→ · · ·

k′

k′

and have reward

Paths
we can conclude:
0hPf

ExpRew

0hPf

σJ{c1[call P /abort]} [p] {c2[call P /abort]}Ki ,
σJ{c1[call P/abort]} [p] {c2[call P/abort]}Ki (T )

= p · ExpRew

0hPf

σJc1[call P/abort]Ki (T )

+ (1 − p) · ExpRew

= p · wp[c1]♯

abort (f )(σ) + (1 − p) · wp[c1]♯

0hPf

σJc2[call P/abort]Ki (T )
abort (f )(σ)

(I.H. on c1 and c2)

= wp[{c1} [p] {c2}]♯

abort (f )(σ)

This ends the proof for the base case of the induction on n and

we can now state the inductive hypothesis:

Inductive hypothesis on n: We assume that for an arbitrary but
ﬁxed n ∈ N holds

λσ • ExpRew

nhPf

σJcKi (T ) = wp[c]♯

wp[callD

n P]

(f )

for all programs c. We can then proceed with the inductive step:

Inductive step n → n + 1: We now have to show that

λσ • ExpRew

n+1hPf

σJcKi (T ) = wp[c]♯

wp[callD

n+1 P]

(f )

holds assuming the inductive hypothesis on n. The proof of this
equality proceeds quite analogously, again by structural induction
on c:

The base cases skip, abort, x := E: The proofs for these base
cases are completely analogous to the proofs conducted in the base
case n = 0.

The procedure call call P: The procedure call is technically a
base case in the structural induction on c as it is an atomic statement.
 It does, however, require using the inductive hypothesis on
n. The proof goes as follows: By an argument on the transition relation 
∆ of n+1(cid:10)Pf

n+1hPf

ExpRew

σ Jcall P K(cid:11) we see that
σJcall PKi (T ) = ExpRew

nhPf

σJD(P )Ki (T ) .

Reasoning about Recursive Probabilistic Programs

24

2016/3/10

To the right hand side, we can apply the inductive hypothesis on n
and then obtain the desired result:

σJcall PKi (T )
nhPf
σJD(P )Ki (T )

λσ • ExpRew

n+1hPf

= λσ • ExpRew
= wp[D(P )]♯

callD

= wp[call P ]♯

n P (f )
n+1 P (f )

callD

(I.H. on n)

Inductive hypothesis and all inductive steps: The inductive hypothesis 
and the proofs for the inductive steps are completely analogous 
to the inductive hypothesis and the proofs conducted in the
base case n = 0. Exemplarily, we shall sketch the proof for the
sequential composition: By a lengthy argument and application of
the inductive hypothesis on c2 (completely analog to the base case
for n = 0) one arrives at
n+1hPf

ExpRew

σJc1;c2Ki (T )
n+1*P

wp[c2]

σ

♯
wp[callD
P

n+1]

Jc1K+ (T ) .

= ExpRew

Applying the inductive hypothesis on c1 then yields the desired
result:

n+1]

σ

♯
wp[c2 ]
wp[callD
P

n+1*P
P n+1](cid:18)wp[c2]♯

wp[callD

wp[callD

P n+1]

(f )

Jc1K+ (T )
(f )(cid:19)

P n+1]

wp[callD

λσ • ExpRew

= wp[c1]♯

= wp[c1; c2]♯

A.11 Case Study
The omitted details for proving the second partial correctness property 
are provided in Figure 9.

[left<right]
right−left+1

right

Xi=left

+(cid:2)a[i]>val(cid:3)·g[right/max(i − 1, left)](cid:19)
(cid:18)(cid:2)a[i]<val(cid:3)·g[left/min(i + 1, right)]

1 : mid := uniform(left, right);

+ [left = right] ·(cid:2)a[left] 6= val(cid:3)
[left < right] ·(cid:16)(cid:2)a[mid] < val(cid:3) · g[left/ · · · ]

+(cid:2)a[mid] > val(cid:3) · g[right/ · · · ]

+ [left ≥ right] · f
if (left < right){

2 :

3 :

4 :

5 :

6 :

7 :

8 :

9 :

(cid:2)a[mid]<val(cid:3)·g[left/ · · · ] + (cid:2)a[mid]>val(cid:3)·g[right/ · · · ]

if (a[mid] < val ){

g[left/min(mid + 1, right)]
left := min(mid + 1, right);
g
call B
f

} else {

(cid:2)a[mid] > val(cid:3) · g[right/ · · · ] +(cid:2)a[mid ] < val(cid:3)

if (a[mid] > val ){

g[right/max(mid − 1, left)]
right := max(mid − 1, left);
g
call B
f

} else { f skip f } f

10 :
11 :
12 : } else { f skip f } f

} f

Figure 9. Proof that call B ﬁnds an index at which the value
at this position is unequal to val when started in a sorted array
a[left .. right ] in which the value val does not exist. We write
j C h for i (cid:22) wp[C] (h). Recall that g = [left ≤ right ] ·

(cid:2)sorted(left, right )(cid:3) · (cid:2)∀x ∈ [left, right ] : a[x]
6= val(cid:3) and
f =(cid:2)a[mid] 6= val(cid:3), and that we assume g (cid:22) wlp[call B] (f ).

Reasoning about Recursive Probabilistic Programs

25

2016/3/10

