Weakest Precondition Reasoning for Expected

Run–Times of Probabilistic Programs

Benjamin Lucien Kaminski(B), Joost-Pieter Katoen(B), Christoph Matheja(B),

and Federico Olmedo(B)

Software Modeling and Veriﬁcation Group, RWTH Aachen University,

Ahornstraße 55, 52074 Aachen, Germany
{benjamin.kaminski,katoen,matheja,
federico.olmedo}@cs.rwth-aachen.de

Abstract. This paper presents a wp–style calculus for obtaining bounds
on the expected run–time of probabilistic programs. Its application
includes determining the (possibly inﬁnite) expected termination time of
a probabilistic program and proving positive almost–sure termination—
does a program terminate with probability one in ﬁnite expected time?
We provide several proof rules for bounding the run–time of loops, and
prove the soundness of the approach with respect to a simple operational 
model. We show that our approach is a conservative extension
of Nielson’s approach for reasoning about the run–time of deterministic
programs. We analyze the expected run–time of some example programs
including a one–dimensional random walk and the coupon collector
problem.

Keywords: Probabilistic programs · Expected run–time · Positive
almost–sure termination · Weakest precondition · Program veriﬁcation

1 Introduction

Since the early days of computing, randomization has been an important tool
for the construction of algorithms. It is typically used to convert a deterministic
program with bad worst–case behavior into an eﬃcient randomized algorithm
that yields a correct output with high probability. The Rabin–Miller primality 
test, Freivalds’ matrix multiplication, and the random pivot selection in
Hoare’s quicksort algorithm are prime examples. Randomized algorithms are
conveniently described by probabilistic programs. On top of the usual language
constructs, probabilistic programming languages oﬀer the possibility of sampling
values from a probability distribution. Sampling can be used in assignments as
well as in Boolean guards.

The interest in probabilistic programs has recently been rapidly growing.
This is mainly due to their wide applicability [10]. Probabilistic programs are

This work was supported by the Excellence Initiative of the German federal and
state government.

c(cid:2) Springer-Verlag Berlin Heidelberg 2016
P. Thiemann (Ed.): ESOP 2016, LNCS 9632, pp. 364–389, 2016.
DOI: 10.1007/978-3-662-49498-1 15

wp–Reasoning for Expected Run–Times of Probabilistic Programs

365

for instance used in security to describe cryptographic constructions and security
experiments. In machine learning they are used to describe distribution functions
that are analyzed using Bayesian inference. The sample program
Cgeo : b := 1; while (b = 1) {b :≈ 1/2 ·(cid:3)0(cid:4) + 1/2 ·(cid:3)1(cid:4)}

for instance ﬂips a fair coin until observing the ﬁrst heads (i.e. 0). It describes a
geometric distribution with parameter 1/2.

The run–time of probabilistic programs is aﬀected by the outcome of their
coin tosses. Technically speaking, the run–time is a random variable, i.e.
it is
t1 with probability p1, t2 with probability p2 and so on. An important measure
that we consider over probabilistic programs is then their average or expected
run–time (over all inputs). Reasoning about the expected run–time of probabilistic 
programs is surprisingly subtle and full of nuances. In classical sequential
programs, a single diverging program run yields the program to have an inﬁnite
run–time. This is not true for probabilistic programs. They may admit arbitrarily 
long runs while having a ﬁnite expected run–time. The program Cgeo, for
instance, does admit arbitrarily long runs as for any n, the probability of not
seeing a heads in the ﬁrst n trials is always positive. The expected run–time of
Cgeo is, however, ﬁnite.

In the classical setting, programs with ﬁnite run–times can be sequentially
composed yielding a new program again with ﬁnite run–time. For probabilistic
programs this does not hold in general. Consider the pair of programs
C1 : x := 1; b := 1; while (b = 1) {b :≈ 1/2 ·(cid:3)0(cid:4) + 1/2 ·(cid:3)1(cid:4); x := 2x}
C2 : while (x > 0) {x := x − 1}.

and

The loop in C1 terminates on average in two iterations; it thus has a ﬁnite
expected run–time. From any initial state in which x is non–negative, C2 makes
x iterations, and thus its expected run–time is ﬁnite, too. However, the program 
C1; C2 has an inﬁnite expected run–time—even though it almost–surely
terminates, i.e. it terminates with probability one. Other subtleties can occur as
program run–times are very sensitive to variations in the probabilities occurring
in the program.

Bounds on the expected run–time of randomized algorithms are typically
obtained using a detailed analysis exploiting classical probability theory (on
expectations or martingales) [9,22]. This paper presents an alternative approach,
based on formal program development and veriﬁcation techniques. We propose a
wp–style calculus `a la Dijkstra for obtaining bounds on the expected run–time of
probabilistic programs. The core of our calculus is the transformer ert, a quantitative 
variant of Dijkstra’s wp–transformer. For a program C, ert [C] (f) (σ)
gives the expected run–time of C started in initial state σ under the assumption 
that f captures the run–time of the computation following C. In particular,
ert [C] (0) (σ) gives the expected run–time of program C on input σ (where 0 is
the constantly zero run–time). Transformer ert is deﬁned inductively on the program 
structure. We prove that our transformer conservatively extends Nielson’s

366

B.L. Kaminski et al.

approach [23] for reasoning about the run–time of deterministic programs. In
addition we show that ert [C] (f) (σ) corresponds to the expected run–time in a
simple operational model for our probabilistic programs based on Markov Decision 
Processes (MDPs). The main contribution is a set of proof rules for obtaining
(upper and lower) bounds on the expected run–time of loops. We apply our approach 
for analyzing the expected run–time of some example programs including
a one–dimensional random walk and the coupon collector problem [20].

We ﬁnally point out that our technique enables determining the (possibly
inﬁnite) expected time until termination of a probabilistic program and proving
(universal) positive almost–sure termination—does a program terminate with
probability one in ﬁnite expected time (on all inputs)? It has been recently
3 –
shown [16] that the universal positive almost–sure termination problem is Π 0
complete, and thus strictly harder to solve than the universal halting problem
for deterministic programs. To the best of our knowledge, the formal veriﬁcation
framework in this paper is the ﬁrst one that is proved sound and can handle
both positive almost–sure termination and inﬁnite expected run–times.

Related work. Several works apply wp–style– or Floyd–Hoare–style reasoning to
study quantitative aspects of classical programs. Nielson [23,24] provides a Hoare
logic for determining upper bounds on the run–time of deterministic programs.
Our approach applied to such programs yields the tightest upper bound on the
run–time that can be derived using Nielson’s approach. Arthan et al. [1] provide
a general framework for sound and complete Hoare–style logics, and show that
an instance of their theory can be used to obtain upper bounds on the run–time
of while programs. Hickey and Cohen [13] automate the average–case analysis 
of deterministic programs by generating a system of recurrence equations
derived from a program whose eﬃciency is to be analyzed. They build on top
of Kozen’s seminal work [18] on semantics of probabilistic programs. Berghammer 
and M¨uller–Olm [3] show how Hoare–style reasoning can be extended to
obtain bounds on the closeness of results obtained using approximate algorithms
to the optimal solution. Deriving space and time consumption of deterministic
programs has also been considered by Hehner [11]. Formal reasoning about probabilistic 
programs goes back to Kozen [18], and has been developed further by
Hehner [12] and McIver and Morgan [19]. The work by Celiku and McIver [5]
is perhaps the closest to our paper. They provide a wp–calculus for obtaining
performance properties of probabilistic programs, including upper bounds on
expected run–times. Their focus is on reﬁnement. They do neither provide a
soundness result of their approach nor consider lower bounds. We believe that
our transformer is simpler to work with in practice, too. Monniaux [21] exploits
abstract interpretation to automatically prove the probabilistic termination of
programs using exponential bounds on the tail of the distribution. His analysis
can be used to prove the soundness of experimental statistical methods to determine 
the average run–time of probabilistic programs. Brazdil et al. [4] study
the run–time of probabilistic programs with unbounded recursion by considering 
probabilistic pushdown automata (pPDAs). They show (using martingale
theory) that for every pPDA the probability of performing a long run decreases

wp–Reasoning for Expected Run–Times of Probabilistic Programs

367

exponentially (polynomially) in the length of the run, iﬀ the pPDA has a ﬁnite
(inﬁnite) expected runtime. As opposed to our program veriﬁcation technique,
[4] considers reasoning at the operational level. Fioriti and Hermanns [8] recently
proposed a typing scheme for deciding almost-sure termination. They showed,
amongst others, that if a program is well-typed, then it almost surely terminates.
This result does not cover positive almost-sure-termination.

Organization of the paper. Section 2 deﬁnes our probabilistic programming language.
 Section 3 presents the transformer ert and studies its elementary properties 
such as continuity. Section 4 shows that the ert transformer coincides with the
expected run–time in an MDP that acts as operational model of our programs.
Section 5 presents two sets of proof rules for obtaining upper and lower bounds
on the expected run–time of loops. In Sect. 6, we show that the ert transformer
is a conservative extension of Nielson’s approach for obtaining upper bounds on
deterministic programs. Section 7 discusses two case studies in detail. Section 8
concludes the paper.

The proofs of the main facts are included in the body of the paper. The
remaining proofs and the calculations omitted in Sect. 7 are included in an
extended version of the paper [17].

2 A Probabilistic Programming Language

In this section we present the probabilistic programming language used throughout 
this paper, together with its run–time model. To model probabilistic
programs we employ a standard imperative language `a la Dijkstra’s Guarded
Command Language [7] with two distinguished features: we allow distribution
expressions in assignments and guards to be probabilistic. For instance, we allow
for probabilistic assignments like

y :≈ Unif[1 . . . x]

which endows variable y with a uniform distribution in the interval [1 . . . x]. We
allow also for a program like
(cid:2)

p ·(cid:3)true(cid:4) + (1−p) ·(cid:3)false(cid:4)(cid:3) {x := x + 1}

x := 0; while

which uses a probabilistic loop guard to simulate a geometric distribution with
success probability p, i.e. the loop guard evaluates to true with probability p and
to false with the remaining probability 1−p.

Formally, the set of probabilistic programs pProgs is given by the grammar

C ::=

empty
| skip
| halt
| x :≈ μ
| C; C
| {C} (cid:2) {C}
| if (ξ) {C} else {C}
| while (ξ) {C}

empty program
eﬀectless operation
immediate termination
probabilistic assignment
sequential composition
non–deterministic choice
probabilistic conditional
probabilistic while loop

368

B.L. Kaminski et al.

Here x represents a program variable in Var, μ a distribution expression in DExp,
and ξ a distribution expression over the truth values, i.e. a probabilistic guard, in
DExp. We assume distribution expressions in DExp to represent discrete probability 
distributions with a (possibly inﬁnite) support of total probability mass 1.
We use p1·(cid:3)a1(cid:4)+···+ pn·(cid:3)an(cid:4) to denote the distribution expression that assigns
probability pi to ai. For instance, the distribution expression 1/2·(cid:3)true(cid:4)+1/2·(cid:3)false(cid:4)
represents the toss of a fair coin. Deterministic expressions over program variables 
such as x−y or x−y > 8 are special instances of distribution expressions—
they are understood as Dirac probability distributions1.

To describe the diﬀerent language constructs we ﬁrst present some preliminaries.
 A program state σ is a mapping from program variables to values in Val.
Let Σ (cid:3) {σ | σ : Var → Val} be the set of program states. We assume an interpretation 
function (cid:2) · (cid:3) : DExp → (Σ → D(Val)) for distribution expressions, D(Val)
being the set of discrete probability distributions over Val. For μ ∈ DExp, (cid:2)μ(cid:3)
maps each program state to a probability distribution of values. We use (cid:2)μ : v(cid:3)
as a shorthand for the function mapping each program state σ to the probability
that distribution (cid:2)μ(cid:3)(σ) assigns to value v, i.e. (cid:2)μ : v(cid:3)(σ) (cid:3) Pr(cid:2)μ(cid:3)(σ)(v), where
Pr denotes the probability operator on distributions over values.

We now present the eﬀects of pProgs programs and the run–time model that
we adopt for them. empty has no eﬀect and its execution consumes no time.
skip has also no eﬀect but consumes, in contrast to empty, one unit of time.
halt aborts any further program execution and consumes no time. x :≈ μ
is a probabilistic assignment that samples a value from (cid:2)μ(cid:3) and assigns it to
variable x; the sampling and assignment consume (altogether) one unit of time.
C1; C2 is the sequential composition of programs C1 and C2. {C1} (cid:2) {C2} is
a non–deterministic choice between programs C1 and C2; we take a demonic
view where we assume that out of C1 and C2 we execute the program with the
greatest run–time. if(ξ){C1}else{C2} is a probabilistic conditional branching:
with probability (cid:2)ξ : true(cid:3) program C1 is executed, whereas with probability
(cid:2)ξ : false(cid:3) = 1−(cid:2)ξ : true(cid:3) program C2 is executed; evaluating (or more rigorously,
sampling a value from) the probabilistic guard requires an additional unit of time.
while (ξ) {C} is a probabilistic while loop: with probability (cid:2)ξ : true(cid:3) the loop
body C is executed followed by a recursive execution of the loop, whereas with
probability (cid:2)ξ : false(cid:3) the loop terminates; as for conditionals, each evaluation of
the guard consumes one unit of time.
Example 1 (Race between tortoise and hare). The probabilistic program

(cid:2)

1/2 ·(cid:3)true(cid:4) + 1/2 ·(cid:3)false(cid:4)(cid:3) {h :≈ h + Unif[0 . . . 10]}

h :≈ 0; t :≈ 30;
while (h ≤ t) {

if
else {empty};
t :≈ t + 1

},

1 A Dirac distribution assigns the total probability mass, i.e. 1, to a single point.

wp–Reasoning for Expected Run–Times of Probabilistic Programs

369

adopted from [6], illustrates the use of the programming language. It models a
race between a hare and a tortoise (variables h and t represent their respective
positions). The tortoise starts with a lead of 30 and in each step advances one
step forward. The hare with probability 1/2 advances a random number of steps
between 0 and 10 (governed by a uniform distribution) and with the remaining
probability remains still. The race ends when the hare passes the tortoise. (cid:8)
We conclude this section by ﬁxing some notational conventions. To keep our
program notation consistent with standard usage, we use the standard symbol
:= instead of :≈ for assignments whenever μ represents a Dirac distribution
given by a deterministic expressions over program variables. For instance, in
the program in Example 1 we write t := t + 1 instead of t :≈ t + 1. Likewise,
when ξ is a probabilistic guard given as a deterministic Boolean expression over
program variables, we use (cid:2)ξ(cid:3) to denote (cid:2)ξ : true(cid:3) and (cid:2)¬ξ(cid:3) to denote (cid:2)ξ : false(cid:3).
For instance, we write (cid:2)b = 0(cid:3) instead of (cid:2)b = 0: true(cid:3).

3 A Calculus of Expected Run–Times

Our goal is to associate to any program C a function that maps each state σ
to the average or expected run–time of C started in initial state σ. We use the
functional space of run–times

(cid:5)
(cid:5) f : Σ → R
∞
≥0

T (cid:3) (cid:4)
∞
to model such functions. Here, R
≥0 represents the set of non–negative real values
extended with ∞. We consider run–times as a mapping from program states to
real numbers (or ∞) as the expected run–time of a program may depend on the
initial program state.

f

(cid:6)

We express the run–time of programs using a continuation–passing style by

means of the transformer

ert[ · ]: pProgs → (T → T).

Concretely, ert [C] (f) (σ) gives the expected run–time of program C from state
σ assuming that f captures the run–time of the computation that follows C.
Function f is usually referred to as continuation and can be thought of as being
evaluated in the ﬁnal states that are reached upon termination of C. Observe
that, in particular, if we set f to the constantly zero run–time, ert [C] (0) (σ)
gives the expected run–time of program C on input σ.

The transformer ert is deﬁned by induction on the structure of C following
the rules in Table 1. The rules are deﬁned so as to correspond to the run–time
model introduced in Sect. 2. That is, ert [C] (0) captures the expected number of
assignments, guard evaluations and skip statements. Most rules in Table 1 are
self–explanatory. ert[empty] behaves as the identity since empty does not modify 
the program state and its execution consumes no time. On the other hand,
ert[skip] adds one unit of time since this is the time required by the execution 
of skip. ert[halt] yields always the constant run–time 0 since halt aborts

370

B.L. Kaminski et al.

Table 1. Rules for deﬁning the expected run–time transformer ert. 1 is the constant 
run–time λσ.1. Eη (h) (cid:3) (cid:2)
v Prη(v) · h(v) represents the expected value of (random 
variable) h w.r.t. distribution η. For σ ∈ Σ, f [x/v] (σ) (cid:3) f (σ [x/v]), where
σ [x/v] is the state obtained by updating in σ the value of x to v. max{f1, f2} (cid:3)
λσ. max{f1(σ), f2(σ)} represents the point–wise lifting of the max operator over R
∞
≥0
to the function space of run–times. lfp X• F (X) represents the least ﬁxed point of the
transformer F : T → T.

any subsequent program execution (making their run–time irrelevant) and consumes 
no time. The deﬁnition of ert on random assignments is more involved:
(cid:7)
v Pr(cid:2)μ(cid:3)(σ)(v) · f(σ [x/v]) is obtained by adding one
ert [x :≈ μ] (f) (σ) = 1 +
unit of time (due to the distribution sampling and assignment of the value sam-
pled) to the sum of the run–time of each possible subsequent execution, weighted
according to their probabilities. ert[C1; C2] applies ert[C1] to the expected run–
time obtained from the application of ert[C2]. ert[{C1} (cid:2) {C2}] returns the
maximum between the run–time of the two branches. ert[if (ξ) {C1} else {C2}]
adds one unit of time (on account of the guard evaluation) to the weighted sum
of the run–time of the two branches. Lastly, the ert of loops is given as the least
ﬁxed point of a run–time transformer deﬁned in terms of the run–time of the
loop body.

Remark. We stress that the above run–time model is a design decision for the
sake of concreteness. All our developments can easily be adapted to capture
alternative models. These include, for instance, the model where only the number
of assignments in a program run or the model where only the number of loop
iterations are of relevance. We can also capture more ﬁne–grained models, where
for instance the run–time of an assignment depends on the size of the distribution
expression being sampled.
Example 2 (Truncated geometric distribution). To illustrate the eﬀects of the
ert transformer consider the program in Fig. 1. It can be viewed as modeling a
truncated geometric distribution: we repeatedly ﬂip a fair coin until observing

wp–Reasoning for Expected Run–Times of Probabilistic Programs

371

Ctrunc : if 1/2

true + 1/2

false

true + 1/2
if 1/2
else {succ := false}

false

}

{succ := true} else {

{succ := true}

Fig. 1. Program modeling a truncated geometric distribution

the ﬁrst heads or completing the second unsuccessful trial. The calculation of
the expected run–time ert [Ctrunc] (0) of program Ctrunc goes as follows:
ert [Ctrunc] (0)

= 1 + 1
2
+ 1
2
= 1 + 1
2

= 1 + 1
2

· ert [succ := true] (0)
· ert [if (. . .) {succ := true} else {succ := false}] (0)
· 1 + 1
· 1 + 1

· ert [succ := true] (0) + 1
· 1 + 1

· (cid:8)
· (cid:8)

(cid:9)
· 1

1 + 1
2

1 + 1
2

= 5
2

2

2

2

· ert [succ := false] (0)

2

(cid:9)

(cid:8)
Therefore, the execution of Ctrunc takes, on average, 2.5 units of time.
Note that the calculation of the expected run–time in the above example is
straightforward as the program at hand is loop–free. Computing the run–time of
loops requires the calculation of least ﬁxed points, which is generally not feasible
in practice. In Sect. 5, we present invariant–based proof rules for reasoning about
the run–time of loops.
The ert transformer enjoys several algebraic properties. To formally state
these properties we make use of the point–wise order relation “(cid:10)” between run–
times: given f, g ∈ T, f (cid:10) g iﬀ f(σ) ≤ g(σ) for all states σ ∈ Σ.
Theorem 1 (Basic properties of the ert transformer). For any program
C ∈ pProgs, any constant run–time k = λσ.k for k ∈ R≥0, any constant r ∈ R≥0,
and any two run–times f, g ∈ T the following properties hold:

M onotonicity :

P ropagation of
constants:
P reservation of ∞ :
Sub−additivity :

Scaling :

f (cid:10) g =⇒ ert [C] (f) (cid:10) ert [C] (g) ;
ert [C] (k + f) = k + ert [C] (f)
provided C is halt−f ree;
ert [C] (∞) = ∞
provided C is halt−f ree;
ert [C] (f + g) (cid:10) ert [C] (f) + ert [C] (g) ;
provided C is f ully probabilistic; 2
ert [C] (r · f) (cid:12) min{1, r} · ert [C] (f) ;
ert [C] (r · f) (cid:10) max{1, r} · ert [C] (f) .

2 A program is called fully probabilistic if it contains no non–deterministic choices.

372

B.L. Kaminski et al.

Proof. Monotonicity follows from continuity (see Lemma 1 below). The remaining
(cid:13)(cid:14)
proofs proceed by induction on the program structure; see [17] for details.

We conclude this section with a technical remark regarding the well–deﬁnedness
of the ert transformer. To guarantee that ert is well–deﬁned, we must show the
existence of the least ﬁxed points used to deﬁne the run–time of loops. To this
end, we use a standard denotational semantics argument (see e.g. [26, Chap. 5]):
First we endow the set of run–times T with the structure of an ω–complete
partial order (ω–cpo) with bottom element. Then we use a continuity argument
to conclude the existence of such ﬁxed points.
Recall that (cid:10) denotes the point–wise comparison between run–times. It easily
follows that (T,(cid:10)) deﬁnes an ω–cpo with bottom element 0 = λσ.0 where the
supremum of an ω–chain f1 (cid:10) f2 (cid:10) ···
in T is also given point–wise, i.e. as
supn fn (cid:3) λσ. supn fn(σ). Now we are in a position to establish the continuity
of the ert transformer:

Lemma 1 (Continuity of the ert transformer). For every program C and
every ω–chain of run–times f1 (cid:10) f2 (cid:10) ··· ,

ert [C] (supn fn) = supn ert [C] (fn) .

(cid:13)(cid:14)
Proof. By induction on the program structure; see [17] for details.
Lemma 1 implies that for each program C ∈ pProgs, guard ξ ∈ DExp, and run–
time f ∈ T, function Ff (X) = 1 + (cid:2)ξ : false(cid:3) · f + (cid:2)ξ : true(cid:3) · ert [C] (X) is also
continuous. The Kleene Fixed Point Theorem then ensures that the least ﬁxed
point ert [while (ξ) {C}] (f) = lfp Ff exists and the expected run–time of loops
is thus well-deﬁned.

Finally, as the aforementioned function Ff is frequently used in the remainder

of the paper, we deﬁne:
Deﬁnition 1 (Characteristic functional of a loop). Given program C ∈
pProgs, probabilistic guard ξ ∈ DExp, and run–time f ∈ T, we call

F

(cid:5)ξ,C(cid:6)
f

: T → T, X (cid:15)→ 1 + (cid:2)ξ : false(cid:3) · f + (cid:2)ξ : true(cid:3) · ert [C] (X)
the characteristic functional of loop while (ξ) {C} with respect to f.
When C and ξ are understood from the context, we usually omit them and
simply write Ff for the characteristic functional associated to while (ξ) {C}
with respect to run–time f. Observe that under this deﬁnition, the ert of loops
can be recast as

ert [while (ξ) {C}] (f) = lfp F

(cid:5)ξ,C(cid:6)
f

.

This concludes our presentation of the ert transformer. In the next section we
validate the transformer’s deﬁnition by showing a soundness result with respect
to an operational model of programs.

wp–Reasoning for Expected Run–Times of Probabilistic Programs

373

4 An Operational Model for Expected Run–Times

We prove the soundness of the expected run–time transformer with respect to
a simple operational model for our probabilistic programs. This model will be
given in terms of a Markov Decision Process (MDP, for short) whose collected
reward corresponds to the run–time. We ﬁrst brieﬂy recall all necessary notions.
A more detailed treatment can be found in [2, Chap. 10]. A Markov Decision
Process is a tuple M = (S, Act, P, s0, rew) where S is a countable set of states,
Act is a (ﬁnite) set of actions, P: S×Act×S → [0, 1] is the transition probability
function such that for all states s ∈ S and actions α ∈ Act,

(cid:8)
P(s, α, s

) ∈ {0, 1} ,

(cid:10)

s(cid:2)∈S

s0 ∈ S is the initial state, and rew : S → R≥0 is a reward function. Instead of
(cid:8) (cid:16) p. An MDP M is a Markov chain if
(cid:8)) = p, we usually write s α−→ s
P(s, α, s
(cid:8) ∈ S there
no non–deterministic choice is possible, i.e. for each pair of states s, s
(cid:8)) (cid:17)= 0.
exists exactly one α ∈ Act with P(s, α, s
A scheduler for M is a mapping S: S + → Act, where S + denotes the set of
non–empty ﬁnite sequences of states. Intuitively, a scheduler resolves the non–
determinism of an MDP by selecting an action for each possible sequence of
states that has been visited so far. Hence, a scheduler S induces a Markov chain
which is denoted by MS. In order to deﬁne the expected reward of an MDP,
we ﬁrst consider the reward collected along a path. Let PathsMS (PathsM
ﬁn)
denote the set of all (ﬁnite) paths π (ˆπ) in MS. Analogously, let PathsMS(s)
and PathsMS
ﬁn (s) denote the set of all inﬁnite and ﬁnite paths in MS starting in
state s ∈ S, respectively. For a ﬁnite path ˆπ = s0 . . . sn, the cumulative reward
of ˆπ is deﬁned as

rew(ˆπ) (cid:3) n−1(cid:10)

rew(sk).

k=0

For an inﬁnite path π, the cumulative reward of reaching a non–empty set of
target states T ⊆ S, is deﬁned as rew(π, ♦T ) (cid:3) rew(π(0) . . . π(n)) if there exists
an n such that π(n) ∈ T and π(i) /∈ T for 0 ≤ i < n and rew(π, ♦T ) (cid:3) ∞
otherwise. Moreover, we write Π(s, T ) to denote the set of all ﬁnite paths ˆπ ∈
ﬁn (s), s ∈ S, with ˆπ(n) ∈ T for some n ∈ N and ˆπ(i) /∈ T for 0 ≤ i < n.
PathsMS
The probability of a ﬁnite path ˆπ is
|ˆπ|−1(cid:11)

PrMS{ˆπ} (cid:3)

k=0

P(sk, S(s1, . . . , sk), sk+1).

The expected reward that an MDP M eventually reaches a non–empty set of
states T ⊆ S from a state s ∈ S is deﬁned as follows. If

PrMS{s |= ♦T} = inf

S

inf
S

(cid:10)

ˆπ∈Π(s,T )

PrMS{ˆπ} < 1

374

B.L. Kaminski et al.

Fig. 2. Rules for the transition probability function of operational MDPs.

then ExpRewM (s |= ♦T ) (cid:3) ∞. Otherwise,
ExpRewM (s |= ♦T ) (cid:3) sup

(cid:10)

S

ˆπ∈Π(s,T )

PrMS{ˆπ} · rew(ˆπ).

We are now in a position to deﬁne an operational model for our probabilistic 
programming language. Let ↓ denote a special symbol indicating successful
termination of a program.
Deﬁnition 2 (The operational MDP of a program). Given program C ∈
pProgs, initial program state σ0 ∈ Σ, and continuation f ∈ T, the operational
(cid:2)C(cid:3) = (S, Act, P, s0, rew), where
MDP of C is given by Mf
σ0
– S (cid:3) ((pProgs ∪ {↓} ∪ {↓; C | C ∈ pProgs}) × Σ) ∪ {(cid:3) sink(cid:4)},
– Act (cid:3) {L, τ, R},
– the transition probability function P is given by the rules in Fig. 2,
– s0 (cid:3) (cid:3)C, σ0(cid:4), and
– rew : S → R≥0 is the reward function deﬁned according to Table 2.
Since the initial state of the MDP Mf
σ0
Mf
is uniquely given, instead of ExpRew

(cid:2)C(cid:3) of a program C with initial state σ0
(cid:2)C(cid:3) ((cid:3)C, σ0(cid:4) |= ♦T ) we simply write
σ(cid:2)C(cid:3) (T ) .

ExpRewMf

σ0

wp–Reasoning for Expected Run–Times of Probabilistic Programs

375

Table 2. Deﬁnition of the reward function rew : S → R≥0 of operational MDPs.

The rules in Fig. 2 deﬁning the transition probability function of a program’s
MDP are self–explanatory. Since only guard evaluations, assignments and skip
statements are assumed to consume time, i.e. have a positive reward, we assign
a reward of 0 to all other program statements. Moreover, note that all states of
the form (cid:3)empty, σ(cid:4), (cid:3)↓, σ(cid:4) and (cid:3) sink(cid:4) are needed, because an operational MDP
is deﬁned with respect to a given continuation f ∈ T. In case of (cid:3)empty, σ(cid:4), a
reward of 0 is collected and after that the program successfully terminates, i.e.
enters state (cid:3)↓, σ(cid:4) where the continuation f is collected as reward. In contrast,
since no state other than (cid:3) sink(cid:4) is reachable from the unique sink state (cid:3) sink(cid:4),
the continuation f is not taken into account if (cid:3) sink(cid:4) is reached without reaching
a state (cid:3)↓, σ(cid:4) ﬁrst. Hence the operational MDP directly enters (cid:3) sink(cid:4) from a state
of the form (cid:3)halt, σ(cid:4).
Example 3 (MDP of Ctrunc). Recall the probabilistic program Ctrunc from
(cid:2)Ctrunc(cid:3) for an arbitrary ﬁxed state
Example 2. Figure 3 depicts the MDP Mf
σ ∈ Σ and an arbitrary continuation f ∈ T. Here labeled edges denote the value
σ
of the transition probability function for the respective states, while the reward
of each state is provided in gray next to the state. To improve readability, edge
labels are omitted if the probability of a transition is 1. Moreover, Mf
(cid:2)Ctrunc(cid:3)
σ
is a Markov chain, because Ctrunc contains no non-deterministic choice.
(cid:2)Ctrunc(cid:3) contains three ﬁnite
paths ˆπtrue, ˆπfalse true, ˆπfalse false that eventually reach state (cid:3) sink(cid:4) starting from
the initial state (cid:3)Ctrunc, σ(cid:4). These paths correspond to the results of the two
(cid:2)C(cid:3) to eventually
probabilistic guards in C. Hence the expected reward of Mf
reach T = {(cid:3) sink(cid:4)} is given by
σ
ExpRewMf

A brief inspection of Fig. 3 reveals that Mf
σ

σ(cid:2)Ctrunc(cid:3) (T )

(cid:7)
ˆπ∈Π(s,T ) PrM{ˆπ} · rew(ˆπ)

(cid:7)
ˆπ∈Π(s,T ) PrMS{ˆπ} · rew(ˆπ)
(Mf
σ

= supS
=
= PrM{ˆπtrue} · rew(ˆπtrue) + PrM{ˆπfalse true} · rew(ˆπfalse true)
+ PrM{ˆπfalse false} · rew(ˆπfalse false)

(cid:2)Ctrunc(cid:3) = M is a Markov chain)

376

B.L. Kaminski et al.

1

C, σ

1/2

1/2

1/2

1

succ := true, σ

C , σ

1

1/2

f (σ [succ/true])

, σ [succ/true]

succ := false, σ 1

0

sink

, σ [succ/false]

f (σ [succ/false])

Fig. 3. The operational MDP Mf
C(cid:4)

σ(cid:2)Ctrunc(cid:3) corresponding to the program in Example 3.
denotes the subprogram if(1/2·(cid:4)true(cid:5)+1/2·(cid:4)false(cid:5)){succ := true}else{succ := false}.

=

(cid:2)

1
2
+
+

2

(cid:3) · (1 + 1 + f(σ [succ/true]))
· 1 · 1
(cid:2)
· 1
· 1 · 1
(cid:2)
· 1 · 1
· 1
· f(σ [succ/true]) + 1
· f(σ [succ/true]) + 1

(cid:3) · (1 + 1 + 1 + f(σ [succ/true]))
(cid:3) · (1 + 1 + 1 + f(σ [succ/false]))

2

1
2
1
2
= 1 + 1
2
= 5
2 + 3

4

4

4

· (6 + f(σ [succ/true]) + f(σ [succ/false]))
· f(σ [succ/false]).
Observe that for f = 0, the expected reward ExpRewMf
σ(cid:2)Ctrunc(cid:3) (T ) and the
expected run–time ert [C] (f) (σ) (cf. Example 2) coincide, both yielding 5/2. (cid:8)
The main result of this section is that ert precisely captures the expected reward
of the MDPs associated to our probabilistic programs.
Theorem 2 (Soundness of the ert transformer). Let ξ ∈ DExp, C ∈
pProgs, and f ∈ T. Then, for each σ ∈ Σ, we have

ExpRewMf

σ(cid:2)C(cid:3) ((cid:3) sink(cid:4)) = ert [C] (f) (σ) .

Proof. By induction on the program structure; see [17] for details.

(cid:13)(cid:14)

5 Expected Run–Time of Loops

Reasoning about the run–time of loop–free programs consists mostly of syntactic
reasoning. The run–time of a loop, however, is given in terms of a least ﬁxed

wp–Reasoning for Expected Run–Times of Probabilistic Programs

377

point. It is thus obtained by ﬁxed point iteration but need not be reached within
a ﬁnite number of iterations. To overcome this problem we next study invariant–
based proof rules for approximating the run–time of loops.

We present two families of proof rules which diﬀer in the kind of the invariants
they build on. In Sect. 5.1 we present a proof rule that rests on the presence of
an invariant approximating the entire run–time of a loop in a global manner,
while in Sect. 5.2 we present two proof rules that rely on a parametrized invariant
that approximates the run–time of a loop in an incremental fashion. Finally in
Sect. 5.3 we discuss how to improve the run–time bounds yielded by these proof
rules.

5.1 Proof Rule Based on Global Invariants

The ﬁrst proof rule that we study allows upper–bounding the expected run–time
of loops and rests on the notion of upper invariants.
Deﬁnition 3 (Upper invariants). Let f ∈ T, C ∈ pProgs and ξ ∈ DExp. We
say that I ∈ T is an upper invariant of loop while (ξ) {C} with respect to f iﬀ

1 + (cid:2)ξ : false(cid:3) · f + (cid:2)ξ : true(cid:3) · ert [C] (I) (cid:10) I

or, equivalently, iﬀ F

(cid:5)ξ,C(cid:6)
f

(I) (cid:10) I, where F

(cid:5)ξ,C(cid:6)
f

is the characteristic functional.

The presence of an upper invariant of a loop readily establishes an upper bound
of the loop’s run–time.
Theorem 3 (Upper bounds from upper invariants). Let f ∈ T, C ∈
pProgs and ξ ∈ DExp. If I ∈ T is an upper invariant of while (ξ) {C} with
respect to f then

ert [while (ξ) {C}] (f) (cid:10) I.

Proof. The crux of the proof is an application of Park’s Theorem3 [25] which,
given that F

(cid:5)ξ,C(cid:6)
f

is continuous (see Lemma 1), states that
(cid:10) I.

(I) (cid:10) I =⇒ lfp F

(cid:5)ξ,C(cid:6)
f

(cid:5)ξ,C(cid:6)
f

F

The left–hand side of the implication stands for I being an upper invariant, while
(cid:13)(cid:14)
the right–hand side stands for ert [while (ξ) {C}] (f) (cid:10) I.
Notice that if the loop body C is itself loop–free, it is usually fairly easy to verify
that some I ∈ T is an upper invariant, whereas inferring the invariant is—as in
standard program veriﬁcation—one of the most involved part of the veriﬁcation
eﬀort.

3 If H : D → D is a continuous function over an ω–cpo (D,(cid:6)) with bottom element,
then H(d) (cid:6) d implies lfp H (cid:6) d for every d ∈ D.

378

B.L. Kaminski et al.

Example 4 (Geometric distribution). Consider loop

Cgeo : while (c = 1) {c :≈ 1/2 · (cid:3)0(cid:4) + 1/2 · (cid:3)1(cid:4)} .

From the calculations below we conclude that I = 1 + (cid:2)c = 1(cid:3) · 4 is an upper
invariant with respect to 0:

1 + (cid:2)c (cid:17)= 1(cid:3) · 0 + (cid:2)c = 1(cid:3) · ert [c :≈ 1/2 · (cid:3)0(cid:4) + 1/2 · (cid:3)1(cid:4)] (I)

= 1 + (cid:2)c = 1(cid:3) · (cid:2)
= 1 + (cid:2)c = 1(cid:3) · (cid:2)
= 1 + (cid:2)c = 1(cid:3) · 4 = I (cid:10) I
Then applying Theorem 3 we obtain

1 + 1
2
1 + 1
2

(cid:3)

· I [c/0] + 1
· I [c/1]
· (1 + (cid:2)0 = 1(cid:3) · 4
) + 1
(cid:12)
(cid:15)
(cid:13)(cid:14)
2
= 1

2

(cid:17)

(cid:16)
Cgeo

ert

(0) (cid:10) 1 + (cid:2)c = 1(cid:3) · 4.

· (1 + (cid:2)1 = 1(cid:3) · 4
(cid:12)
(cid:15)

(cid:13)(cid:14)
= 5

(cid:3)
)

In words, the expected run–time of Cgeo is at most 5 from any initial state where
(cid:8)
c = 1 and at most 1 from the remaining states.
The invariant–based technique to reason about the run–time of loops presented
in Theorem 3 is complete in the sense that there always exists an upper invariant
that establishes the exact run–time of the loop at hand.
Theorem 4. Let f ∈ T, C ∈ pProgs, ξ ∈ DExp. Then there exists
an upper
to f such that
ert [while (ξ) {C}] (f) = I.
Proof. The result follows from showing that ert [while (ξ) {C}] (f) is itself an
upper invariant. Since ert [while (ξ) {C}] (f) = lfp F
this amounts to showing 
that
(cid:3) (cid:10) lfp F

invariant I of while (ξ) {C} with respect

(cid:13)(cid:14)
which holds by deﬁnition of lfp .
Intuitively, the proof of this theorem shows that ert [while (ξ) {C}] (f) itself is
the tightest upper invariant that the loop admits.

(cid:5)ξ,C(cid:6)
f
(cid:5)ξ,C(cid:6)
f

(cid:5)ξ,C(cid:6)
f

(cid:5)ξ,C(cid:6)
f

lfp F

F

(cid:2)

,

5.2 Proof Rules Based on Incremental Invariants
We now study a second family of proof rules which builds on the notion of
ω–invariants to establish both upper and lower bounds for the run–time of loops.
Deﬁnition 4 (ω–invariants). Let f ∈ T, C ∈ pProgs and ξ ∈ DExp. Moreover
let In ∈ T be a run–time parametrized by n ∈ N. We say that In is a lower ω–
invariant of loop while (ξ) {C} with respect to f iﬀ

(cid:5)ξ,C(cid:6)
f

(0) (cid:12) I0

F

and

(cid:5)ξ,C(cid:6)
f

F

(In) (cid:12) In+1

for all n ≥ 0.

Dually, we say that In is an upper ω–invariant iﬀ

(cid:5)ξ,C(cid:6)
f

(0) (cid:10) I0

F

and

F

(cid:5)ξ,C(cid:6)
f

(In) (cid:10) In+1

for all n ≥ 0.

wp–Reasoning for Expected Run–Times of Probabilistic Programs

379

Intuitively, a lower (resp. upper) ω–invariant In represents a lower (resp. upper)
bound for the expected run–time of those program runs that ﬁnish within n + 1
iterations, weighted according to their probabilities. Therefore we can use the
asymptotic behavior of In to approximate from below (resp. above) the expected
run–time of the entire loop.
Theorem 5 (Bounds from) ω–invariants). Let f ∈ T, C ∈ pProgs,
ξ ∈ DExp.
1. If In is a lower ω–invariant of while (ξ) {C} with respect to f and lim

exists4, then

2. If In is an upper ω–invariant of while (ξ) {C} with respect to f and lim

ert [while (ξ) {C}] (f) (cid:12) lim

n→∞ In.

n→∞ In

n→∞ In

exists, then

ert [while (ξ) {C}] (f) (cid:10) lim

n→∞ In.

f = 0 and F n+1

Proof. We prove only the case of lower ω–invariants since the other case follows
by a dual argument. Let Ff be the characteristic functional of the loop with
f ). By the Kleene Fixed Point
respect to f. Let F 0
(cid:10) . . . forms an
Theorem, ert [while (ξ) {C}] (f) = supn F n
f and since F 0
f
ω–chain, by the Monotone Sequence Theorem5, supn F n
f . Then
(cid:12) In. We prove this by induction
the proof follows from showing that F n+1
(cid:12) I0 holds because In is a lower ω–invariant. For the
on n. The base case F 1
f
inductive case we reason as follows:

f = limn→∞ F n

= Ff (F n

(cid:10) F 1

f

f

f

F n+2

f

= Ff

F n+1

f

(cid:2)

(cid:3) (cid:12) Ff (In) (cid:12) In+1.

Here the ﬁrst inequality follows by I.H. and the monotonicity of Ff (recall that
ert[C] is monotonic by Theorem 1), while the second inequality holds because In
(cid:13)(cid:14)
is a lower ω–invariant.
Example 5 (Lower bounds for Cgeo). Reconsider loop Cgeo from Example 4. Now
we use Theorem 5.1. to show that 1 + (cid:2)c = 1(cid:3) · 4 is also a lower bound of its
run–time. To this end we ﬁrst show that In = 1 + (cid:2)c = 1(cid:3) · (4 − 3/2n) is a lower
ω–invariant of the loop with respect to 0:

F0(0) = 1 + (cid:2)c (cid:17)= 1(cid:3) · 0 + (cid:2)c = 1(cid:3) · ert [c :≈ 1/2(cid:3)0(cid:4) + 1/2(cid:3)1(cid:4)] (0)

= 1 + (cid:2)c = 1(cid:3) · (cid:2)
= 1 + (cid:2)c = 1(cid:3) · 1 = 1 + (cid:2)c = 1(cid:3) · (4 − 3/20) = I0 (cid:12) I0
F0(In) = 1 + (cid:2)c (cid:17)= 1(cid:3) · 0 + (cid:2)c = 1(cid:3) · ert [c :≈ 1/2(cid:3)0(cid:4) + 1/2(cid:3)1(cid:4)] (In)

· 0 [c/0] + 1

· 0 [c/1]

1 + 1
2

(cid:3)

2

= 1 + (cid:2)c = 1(cid:3) · (cid:2)

· In [c/1]
∞
4 Limit limn→∞ In is to be understood pointwise, on R
≥0,
λσ. limn→∞ In(σ) and limn→∞ In(σ) = ∞ is considered a valid value.
5 If (cid:4)an(cid:5)n∈N is an increasing sequence in R
mum supn an.

· In [c/0] + 1

1 + 1
2

i.e.

(cid:3)

2

∞
≥0, then limn→∞ an coincides with supre-

limn→∞ In =

380

B.L. Kaminski et al.

= 1 + (cid:2)c = 1(cid:3) · (cid:8)
= 1 + (cid:2)c = 1(cid:3) · (cid:2)

1 + 1
2
4 − 3

2n+1

· (1 + 0) + 1

· (cid:2)

(cid:2)
= In+1 (cid:12) In+1

1 +

(cid:3)

2

4 − 3

2n

(cid:3)(cid:3)(cid:9)

Then from Theorem 5.1 we obtain

(cid:8)

(cid:3)(cid:9)

(cid:17)

ert

Cgeo

(cid:16)
Cgeo

1 + (cid:2)c = 1(cid:3) · (cid:2)

4 − 3
2n
(cid:16)

(0) (cid:12) lim
n→∞

= 1 + (cid:2)c = 1(cid:3) · 4.
(cid:17)
(0) (cid:10) 1 + (cid:2)c = 1(cid:3) · 4
Combining this result with the upper bound ert
established in Example 4 we conclude that 1 + (cid:2)c = 1(cid:3) · 4 is the exact run–time
of Cgeo. Observe, however, that the above calculations show that In is both a
lower and an upper ω–invariant (exact equalities F0(0) = I0 and F0(In) = In+1
hold). Then we can apply Theorem 5.1 and 5.2 simultaneously to derive the exact
run–time without having to resort to the result from Example 4.
Invariant Synthesis. In order to synthesize invariant In = 1+(cid:2)c = 1(cid:3)·(4 − 3/2n),
we proposed template In = 1+(cid:2)c = 1(cid:3)·an and observed that under this template
the deﬁnition of lower ω–invariant reduces to a0 ≤ 1, an+1 ≤ 2 + 1
2 an, which is
(cid:8)
satisﬁed by an = 4 − 3/2n.
Now we apply Theorem 5.1 to a program with inﬁnite expected run–time.
Example 6 (Almost–sure termination at inﬁnite expected run–time). Recall the
program from the introduction:

C : 1: x := 1; b := 1;

2: while (b = 1) {b :≈ 1/2(cid:3)0(cid:4) + 1/2(cid:3)1(cid:4); x := 2x};
3: while (x > 0) {x := x − 1}

Let Ci denote the i-th line of C. We show that ert [C] (0) (cid:12) ∞.6 Since

ert [C] (0) = ert [C1] (ert [C2] (ert [C3] (0)))

we start by showing that

ert [C3] (0) (cid:12) 1 + (cid:2)x > 0(cid:3) · 2x

using lower ω–invariant Jn = 1 + (cid:2)n > x > 0(cid:3) · 2x + (cid:2)x ≥ n(cid:3) · (2n − 1). We omit
here the details of verifying that Jn is a lower ω–invariant. Next we show that

ert [C2] (1 + (cid:2)x > 0(cid:3) · 2x) (cid:12) 1 + (cid:2)b (cid:17)= 1(cid:3) · (cid:2)
+ (cid:2)b = 1(cid:3) · (cid:2)

(cid:3)
1 + (cid:2)x > 0(cid:3) · 2x
7 + (cid:2)x > 0(cid:3) · ∞(cid:3)

by means of the lower ω–invariant
In = 1 + (cid:2)b (cid:17)= 1(cid:3) · (cid:2)
6 Note that while this program terminates with probability one, the expected run–time

(cid:9)
2n + n · (cid:2)x > 0(cid:3) · 2x

(cid:3)
1 + (cid:2)x > 0(cid:3) · 2x

+ (cid:2)b = 1(cid:3) · (cid:8)

7 − 5

.

to achieve termination is inﬁnite.

wp–Reasoning for Expected Run–Times of Probabilistic Programs

381

(cid:9)
· (1 + 0 [x, b/2x, 1])
2
1 + 1
2

· (1 + 0 [x, b/2x, 0]) + 1
+ (cid:2)b = 1(cid:3) · (cid:2)
· 1 + 1
+ (cid:2)b = 1(cid:3) · 2 = I0 (cid:12) I0

Let F be the characteristic functional of loop C2 with respect to 1 + (cid:2)x > 0(cid:3)· 2x.
The calculations to establish that In is a lower ω–invariant now go as follows:
F (0) = 1 + (cid:2)b (cid:17)= 1(cid:3) · (cid:2)
+ (cid:2)b = 1(cid:3) · (cid:8)
= 1 + (cid:2)b (cid:17)= 1(cid:3) · (cid:2)
= 1 + (cid:2)b (cid:17)= 1(cid:3) · (cid:2)
F (In) = 1 + (cid:2)b (cid:17)= 1(cid:3) · (cid:2)
+ (cid:2)b = 1(cid:3)·(cid:8)
1 + 1
2
= 1 + (cid:2)b (cid:17)= 1(cid:3) · (cid:2)
+ (cid:2)b = 1(cid:3)·(cid:8)
1 + 1
2
= 1 + (cid:2)b (cid:17)= 1(cid:3) · (cid:2)
+ (cid:2)b = 1(cid:3)·(cid:8)
= In+1 (cid:12) In+1

(cid:3)
1 + (cid:2)x > 0(cid:3) · 2x
1 + 1
2
(cid:3)
1 + (cid:2)x > 0(cid:3) · 2x
(cid:3)
1 + (cid:2)x > 0(cid:3) · 2x
(cid:3)
1 + (cid:2)x > 0(cid:3) · 2x
· (1 + In [x, b/2x, 0]) + 1
(cid:3)
1 + (cid:2)x > 0(cid:3) · 2x
· (3 + (cid:2)2x > 0(cid:3) · 4x) + 1
(cid:3)
1 + (cid:2)x > 0(cid:3) · 2x
(cid:9)
2n+1 + (n+1) · (cid:2)x > 0(cid:3) · 2x

(cid:9)
· (1 + In [x, b/2x, 1])

7 − 5

9 − 5

2n + n · (cid:2)2x > 0(cid:3) · 4x

(cid:3)
· 1

(cid:9)(cid:9)

2

2

·(cid:8)

2

Now we can complete the run–time analysis of program C:

ert [C] (0)

(cid:2)

= ert [C1] (ert [C2] (ert [C3] (0)))
(cid:3)
1 + (cid:2)b (cid:17)= 1(cid:3) · (cid:2)
(cid:12) ert [C1]
1 + (cid:2)x > 0(cid:3) · 2x
(cid:8)
(cid:8)
1 + (cid:2)b (cid:17)= 1(cid:3) · (cid:2)
ert[b := 1]
= ert[x := 1]
+ (cid:2)b = 1(cid:3) · (cid:2)

+ (cid:2)b = 1(cid:3) · (cid:2)
7 + (cid:2)x > 0(cid:3) · ∞(cid:3)(cid:3)
(cid:3)
1 + (cid:2)x > 0(cid:3) · 2x
7 + (cid:2)x > 0(cid:3) · ∞(cid:3)(cid:9)(cid:9)

= ert [x := 1] (8 + (cid:2)x > 0(cid:3) · ∞) = 8 + ∞ = ∞

Overall, we obtain that the expected run–time of the program C is inﬁnite
even though it terminates with probability one. Notice furthermore that sub–
programs while (b = 1) {b :≈ 1/2(cid:3)0(cid:4) + 1/2(cid:3)1(cid:4); x := 2x} and while (x > 0) {x :=
x − 1} have expected run–time 1 + (cid:2)b(cid:3) · 4 and 1 + (cid:2)x > 0(cid:3) · 2x, respectively, i.e.
both have a ﬁnite expected run–time.

(cid:3)
1 + (cid:2)x > 0(cid:3) · 2x

Invariant synthesis. In order to synthesize the ω–invariant In of loop C2 we
propose the template In = 1 + (cid:2)b (cid:17)= 1(cid:3) · (cid:2)
an +
(cid:3)
bn · (cid:2)x > 0(cid:3) · 2x
and from the deﬁnition of lower ω–invariants we obtain a0 ≤ 2,
an+1 ≤ 7/2+ 1/2· an and b0 ≤ 0, bn+1 ≤ 1+ bn. These recurrences admit solutions
an = 7 − 5/2n and bn = n.
(cid:8)
As the proof rule based on upper invariants, the proof rules based on ω-invariants
are also complete: Given loop while (ξ) {C} and run–time f, it is enough to
consider the ω-invariant In = F n+1
f is deﬁned as in the proof of

+ (cid:2)b = 1(cid:3) · (cid:2)

, where F n

f

382

B.L. Kaminski et al.

Theorem 5 to yield the exact run–time ert [while (ξ) {C}] (f) from an application
of Theorem 5. We formally capture this result by means of the following theorem:
Theorem 6. Let f ∈ T, C ∈ pProgs and ξ ∈ DExp. Then there exists a (both
lower and upper) ω–invariant In of while (ξ) {C} with respect to f such that
ert [while (ξ) {C}] (f) = limn→∞ In.
Theorem 6 together with Theorem 4 shows that the set of invariant–based proof
rules presented in this section are complete. Next we study how to reﬁne invariants 
to make the bounds that these proof rules yield more precise.

5.3 Reﬁnement of Bounds

(cid:5)ξ,C(cid:6)
f

(I) (cid:12) I), then F

An important property of both upper and lower bounds of the run–time of loops
is that they can be easily reﬁned by repeated application of the characteristic
functional.
Theorem 7 (Reﬁnement of bounds). Let f ∈ T, C ∈ pProgs and ξ ∈ DExp.
If I is an upper (resp. lower) bound of ert [while (ξ) {C}] (f) and F
(I) (cid:10) I
(resp. F
(I) is also an upper (resp. lower) bound, at
least as precise as I.
Proof. If I is an upper bound of ert [while (ξ) {C}] (f) we have lfp F
Then from the monotonicity of F
(I) (cid:10) I we obtain
and from F
ert [while (ξ) {C}] (f) = lfp F

(cid:10) I.
(recall that ert is monotonic by Theorem 1)

(I) (cid:10) I,

(cid:5)ξ,C(cid:6)
f

(cid:5)ξ,C(cid:6)
f

(cid:5)ξ,C(cid:6)
f

) (cid:10) F

(cid:5)ξ,C(cid:6)
f

(cid:5)ξ,C(cid:6)
f

(cid:5)ξ,C(cid:6)
f

(cid:5)ξ,C(cid:6)
f

(cid:5)ξ,C(cid:6)
f

= F

(cid:5)ξ,C(cid:6)
f

(lfp F

(cid:5)ξ,C(cid:6)
f

which means that F
(I) is also an upper bound, possibly tighter than I. The
(cid:13)(cid:14)
case for lower bounds is completely analogous.
Notice that if I is an upper invariant of while(ξ){C} then I fulﬁlls all necessary
conditions of Theorem 7. In practice, Theorem 7 provides a means of iteratively
improving the precision of bounds yielded by Theorems 3 and 5, as for instance
for upper bounds we have

ert [while (ξ) {C}] (f) (cid:10) ··· (cid:10) F

(cid:5)ξ,C(cid:6)
f

(cid:5)ξ,C(cid:6)
f

F

(I)

(cid:8)

(cid:9) (cid:10) F

(cid:5)ξ,C(cid:6)
f

(I) (cid:10) I.

(cid:5)ξ,C(cid:6)
f

(L) (cid:10) L (resp. F

If In is an upper (resp. lower) ω-invariant, applying Theorem 7 requires checking
(L) (cid:12) L), where L = limn→∞ In. This proof
that F
obligation can be discharged by showing that In forms an ω-chain, i.e. that
In (cid:10) In+1 for all n ∈ N.

(cid:5)ξ,C(cid:6)
f

wp–Reasoning for Expected Run–Times of Probabilistic Programs

383

6 Run–Time of Deterministic Programs

The notion of expected run–times as deﬁned by ert is clearly applicable to
deterministic programs, i.e. programs containing neither probabilistic guards
nor probabilistic assignments nor non–deterministic choice operators. We show
that the ert of deterministic programs coincides with the tightest upper bound
on the run–time that can be derived in an extension of Hoare logic [14] due to
Nielson [23,24].

In order to compare our notion of ert to the aforementioned calculus we
restrict our programming language to the language dProgs of deterministic programs 
considered in [24] which is given by the following grammar:
C ::= skip | x := E | C; C | if (ξ) {C} else {C} | while (ξ) {C} ,
where E is a deterministic expression and ξ is a deterministic guard, i.e. (cid:2)E(cid:3)(σ)
and (cid:2)ξ(cid:3)(σ) are Dirac distributions for each σ ∈ Σ. For simplicity, we slightly
abuse notation and write (cid:2)E(cid:3)(σ) to denote the unique value v ∈ Val such that
(cid:2)E : v(cid:3)(σ) = 1.
(cid:2)C(cid:3) of a program C ∈ dProgs and
a program state σ ∈ Σ is a labeled transition system. In particular, if a terminal
state of the form (cid:3)↓, σ
(cid:2)C(cid:3), it is unique.
Hence we may capture the eﬀect of a deterministic program by a partial function
C(cid:2) · (cid:3)( · ) : dProgs × Σ (cid:11) Σ mapping each C ∈ dProgs and σ ∈ Σ to a program
(cid:8)(cid:4) that is reachable in the
state σ
MDP M0
σ

(cid:2)C(cid:3) from the initial state (cid:3)C, σ(cid:4). Otherwise, C(cid:2)C(cid:3)(σ) is undeﬁned.

(cid:8) ∈ Σ if and only if there exists a state (cid:3)↓, σ

For deterministic programs, the MDP M0
σ

(cid:8)(cid:4) is reachable from the initial state of M0

σ

Nielson [23,24] developed an extension of the classical Hoare calculus for
total correctness of programs in order to establish additionally upper bounds on
the run–time of programs. Formally, a correctness property is of the form

{ P } C { E ⇓ Q } ,

where C ∈ dProgs, E is a deterministic expression over the program variables,
and P, Q are (ﬁrst–order) assertions. Intuitively, { P } C { E ⇓ Q } is valid,
written |=E { P } C { E ⇓ Q }, if and only if there exists a natural number
k such that for each state σ satisfying the precondition P , the program C terminates 
after at most k · (cid:2)E(cid:3)(σ) steps in a state satisfying postcondition Q. In
particular, it should be noted that E is evaluated in the initial state σ.

Figure 4 is taken verbatim from [24] except for minor changes to match our
notation. Most of the inference rules are self–explanatory extensions of the standard 
Hoare calculus for total correctness of deterministic programs [14] which is
obtained by omitting the gray parts.

The run–time of skip and x := E is one time unit. Since guard evaluations
are assumed to consume no time in this calculus, any upper bound on the run–
time of both branches of a conditional is also an upper bound on the run–time
of the conditional itself (cf. rule [if]). The rule of consequence allows to increase
an already proven upper bound on the run–time by an arbitrary constant factor.
Furthermore, the run–time of two sequentially composed programs C1 and C2

384

B.L. Kaminski et al.

{ P } skip { 1 ⇓ P } [skip]
{ P ∧ E2 = u } C1 { E1 ⇓ Q ∧ E2 ≤ u } { Q } C2 { E2 ⇓ R }

{ Q [x/ E ] } x := E { 1 ⇓ Q } [assgn]

{ P } C1; C2 { E1 + E2 ⇓ R }

[seq]

where u is a fresh logical variable
{ P ∧ ξ } C1 { E ⇓ Q } { P ∧ ¬ξ } C2 { E ⇓ Q }

{ P } if (ξ) {C1} else {C2} { E ⇓ Q }

[if]

{ P (z + 1) ∧ E = u } C { E1 ⇓ P (z) ∧ E ≤ u }

{ ∃z• P (z) } while (ξ) {C} { E ⇓ P (0) }

[while]

where z ∈ N, P (z + 1) ⇒ ξ ∧ E ≥ E1 + E , P (0) ⇒ ¬ξ ∧ E ≥ 1
and u is a fresh logical variable
{ P } C { E ⇓ Q }
{ P } C { E ⇓ Q } [cons]
where P ⇒ P ∧ E ≤ k · E for some k ∈ N and Q ⇒ Q

Fig. 4. Inference system for order of magnitude of run–time of deterministic programs
according to Nielson [23].

is, intuitively, the sum of their run–times E1 and E2. However, run–times are
expressions which are evaluated in the initial state. Thus, the run–time of C2
has to be expressed in the initial state of C1; C2. Technically, this is achieved
by adding a fresh (and hence universally quantiﬁed) variable u that is an upper
(cid:8)
2 in the
bound on E2 and at the same time is equal to a new expression E
(cid:8)
precondition of C1; C2. Then, the run–time of C1; C2 is given by the sum E1+E
2.
The same principle is applied to each loop iteration. Here, the run–time of
the loop body is given by E1 and the run–time of the remaining z loop iterations,
(cid:8), is expressed in the initial state by adding a fresh variable u. Then, any upper
E
bound of E ≥ E1 + E
(cid:8) is an upper bound on the run–time of z loop iterations.
We denote provability of a correctness property { P } C { E ⇓ Q } and a
total correctness property { P } C { ⇓ Q } in the standard Hoare calculus by
(cid:16)E { P } C { E ⇓ Q } and (cid:16) { P } C { ⇓ Q }, respectively.
Theorem 8 (Soundness of ert for deterministic programs). For all C ∈
dProgs and assertions P, Q, we have

(cid:16) { P } C { ⇓ Q } implies (cid:16)E { P } C { ert [C] (0) ⇓ Q }.

Proof. By induction on the program structure; see [17] for details.

Intuitively, this theorem means that for every terminating deterministic program,
the ert is an upper bound on the run–time, i.e. ert is sound with respect to the
inference system shown in Fig. 4. The next theorem states that no tighter bound

wp–Reasoning for Expected Run–Times of Probabilistic Programs

385

can be derived in this calculus. We cannot get a more precise relationship, since
we assume guard evaluations to consume time.
Theorem 9 (Completeness of ert w.r.t. Nielson). For all C ∈ dProgs,
assertions P, Q and deterministic expressions E, (cid:16)E { P } C { E ⇓ Q }
implies that there exists a natural number k such that for all σ ∈ Σ satisfying
P , we have

ert [C] (0) (σ) ≤ k · ((cid:2)E(cid:3)(σ)) .

Proof. By induction on the program structure; see [17] for details.

(cid:13)(cid:14)

Theorem 8 together with Theorem 9 shows that our notion of ert is a conservative
extension of Nielson’s approach for reasoning about the run–time of deterministic
programs. In particular, given a correctness proof of a deterministic program C
in Hoare logic, it suﬃces to compute ert [C] (0) in order to obtain a corresponding
proof in Nielson’s proof system.

7 Case Studies

In this section we use our ert–calculus to analyze the run–time of two well–known
randomized algorithms: the One–Dimensional (Symmetric) Random Walk and
the Coupon Collector Problem.

7.1 One–Dimensional Random Walk

Consider program

Prw : x := 10; while (x > 0){x :≈ 1/2 · (cid:3)x−1(cid:4) + 1/2 · (cid:3)x+1(cid:4)} ,

which models a one–dimensional walk of a particle which starts at position
x = 10 and moves with equal probability to the left or to the right in each
turn. The random walk stops if the particle reaches position x = 0. It can be
shown that the program terminates with probability one [15] but requires, on
average, an inﬁnite time to do so. We now apply our ert–calculus to formally
derive this run–time assertion.

The expected run–time of Prw is given by

ert [Prw] (0) = ert [x := 10] (ert [while (x > 0) {C}] (0)) ,

where C stands for the probabilistic assignment in the loop body. Thus, we need
to ﬁrst determine run–time ert [while (x > 0) {C}] (0). To do so we propose

In = 1 + (cid:2)0 < x ≤ n(cid:3) · ∞

as a lower ω–invariant of loop while (x > 0) {C} with respect to 0; detailed
calculations for verifying that In is indeed a lower ω–invariant can be found in
the extended version of the paper [17]. Theorem 5 then states that
n→∞ 1 + (cid:2)0 < x ≤ n(cid:3) · ∞ = 1 + (cid:2)0 < x(cid:3) · ∞.

ert [while (x > 0) {C}] (0) (cid:12) lim

386

B.L. Kaminski et al.

Altogether we have

ert [Prw] (0) = ert [x := 10] (ert [while (x > 0) {C}] (0))

(cid:12) ert [x := 10] (1 + (cid:2)0 < x(cid:3) · ∞)
= 1 + (1 + (cid:2)0 < x(cid:3) · ∞) [x/10]
= 1 + (1 + 1 · ∞) = ∞,

which says that ert [Prw] (0) (cid:12) ∞. Since the reverse inequality holds trivially,
we conclude that ert [Prw] (0) = ∞.

7.2 The Coupon Collector Problem

Now we apply our ert–calculus to solve the Coupon Collector Problem. This
problem arises from the following scenario7: Suppose each box of cereal contains
one of N diﬀerent coupons and once a consumer has collected a coupon of each
type, he can trade them for a prize. The aim of the problem is determining the
average number of cereal boxes the consumer should buy to collect all coupon
types, assuming that each coupon type occurs with the same probability in the
cereal boxes.

The problem can be modeled by program Ccp below:

cp := [0, . . . , 0]; i := 1; x := N
while (x > 0) {

while (cp[i] (cid:17)= 0) {

i :≈ Unif[1 . . . N]

};
cp[i] := 1; x := x − 1

}

Array cp is initialized to 0 and whenever we obtain the ﬁrst coupon of type i,
we set cp[i] to 1. The outer loop is iterated N times and in each iteration we
collect a new—unseen—coupon type. The collection of the new coupon type is
performed by the inner loop.

We start the run–time analysis of Ccp introducing some notation. Let Cin
and Cout, respectively, denote the inner and the outer loop of Ccp. Furthermore,
let #col (cid:3) (cid:7)
i=1[cp[i] (cid:17)= 0] denote the number of coupons that have already
N
been collected.

Analysis of the inner loop. For analyzing the run–time of the outer loop we need
to refer to the run–time of its body, with respect to an arbitrary continuation
g ∈ T. Therefore, we ﬁrst analyze the run–time of the inner loop Cin. We propose

7 The problem formulation presented here is taken from [20].

wp–Reasoning for Expected Run–Times of Probabilistic Programs

387

the following lower and upper ω–invariant for the inner loop Cin:

n = 1 + [cp[i] = 0] · g
J g
+ [cp[i] (cid:17)= 0] · n(cid:10)

(cid:18)

k=0

(cid:19)

k

(cid:18)

#col
N

2 +

1
N

N(cid:10)

j=1

(cid:19)
(cid:2)cp[j] = 0(cid:3) · g[i/j]

.

Moreover, we write J g for the same invariant where n is replaced by ∞.
n is indeed a lower and upper ω–invariant is proA 
detailed veriﬁcation that J g
vided in the extended version of the paper [17]. Theorem 5 now yields

J g = lim

n→∞ J g
Since the run–time of a deterministic assignment x := E is

n→∞ J g

(cid:10) ert [Cin] (g) (cid:10) lim

n

n = J g.

ert [x := E] (f) = 1 + f [x/E] ,

the expected run–time of the body of the outer loop reduces to

ert [Cin; cp[i] := 1; x := x − 1] (g)

= 2 + ert [Cin] (g[x/x − 1, cp[i]/1])
= 2 + J g[x/x−1, cp[i]/1]
= 2 + J g[x/x − 1, cp[i]/1].

((cid:12))

((cid:5))

(†)
(by (cid:5))
(by (cid:12))

Analysis of the outer loop. Since program Ccp terminates right after the execution 
of the outer loop Cout, we analyze the run–time of the outer loop Cout with
respect to continuation 0, i.e. ert [Cout] (0). To this end we propose
(cid:19)
k

(cid:18)

(cid:19)

(cid:18)

∞(cid:10)

n(cid:10)

#col + (cid:13)

N

In = 1 +

[x > (cid:13)] ·

3 + [n (cid:17)= 0] + 2 ·

(cid:7)=0

− 2 · [cp[i] = 0] · [x > 0] ·

(cid:18)

∞(cid:10)

k=0

k

k=0
(cid:19)

#col
N

as both an upper and lower ω–invariant of Cout with respect to 0. A detailed
veriﬁcation that In is an ω-invariant is found in the extended version of the
paper [17]. Now Theorem 5 yields

n→∞ In = I,
where I denotes the same invariant as In with n replaced by ∞.

n→∞ In (cid:10) ert [Cout] (0) (cid:10) lim

I = lim

(‡)

Analysis of the overall program. To obtain the overall expected run–time of
program Ccp we have to account for the initialization instructions before the
outer loop. The calculations go as follows:

388

B.L. Kaminski et al.

ert [Ccp] (0)

= ert [cp := [0, . . . , 0]; i := 1; x := N ; Cout] (0)
= 3 + ert [Cout] (0) [x/N, i/1, cp[1]/0, . . . , cp[N]/0]
= 3 + I[x/N, i/1, cp[1]/0, . . . , cp[N]/0]
= 4 + [N > 0] · (cid:8)
(cid:8)(cid:7)∞
N−1
(cid:7)=1
= 4 + [N > 0] · (cid:8)
N−1
(cid:7)=1
= 4 + [N > 0] · 2N · (2 + HN−1),

4N + 2

4N + 2

(cid:9)(cid:9)

(cid:3)k

(cid:7)

(cid:7)

(cid:2)

(cid:7)
N

k=0

(cid:9)

N
(cid:7)

(by (cid:5))
(by ‡)

(cid:8)

(cid:9)

geom. series and
sum reordering

where HN−1 (cid:3) 0 + 1/1 + 1/2 + 1/3 + ··· + 1/N−1 denotes the (N−1)-th harmonic
number. Since the harmonic numbers approach asymptotically to the natural
logarithm, we conclude that the coupon collector algorithm Ccp runs in expected
time Θ(N · log(N)).

8 Conclusion

We have studied a wp–style calculus for reasoning about the expected run–
time and positive almost–sure termination of probabilistic programs. Our main
contribution consists of several sound and complete proof rules for obtaining
upper as well as lower bounds on the expected run–time of loops. We applied
these rules to analyze the expected run–time of a variety of example programs
including the well-known coupon collector problem. While ﬁnding invariants
is, in general, a challenging task, we were able to guess correct invariants by
considering a few loop unrollings most of the time. Hence, we believe that our
proof rules are natural and widely applicable.

Moreover, we proved that our approach is a conservative extension of Niel-
son’s approach for reasoning about the run–time of deterministic programs and
that our calculus is sound with respect to a simple operational model.

Acknowledgement. We thank Gilles Barthe for bringing to our attention the coupon
collector problem as a particularly intricate case study for formal veriﬁcation of expected
run–times and Thomas Noll for bringing to our attention Nielson’s Hoare logic.

References

1. Arthan, R., Martin, U., Mathiesen, E.A., Oliva, P.: A general framework for sound

and complete Floyd-Hoare logics. ACM Trans. Comput. Log. 11(1), 7 (2009)

2. Baier, C., Katoen, J.: Principles of Model Checking. MIT Press, Cambridge (2008)
3. Berghammer, R., M¨uller-Olm, M.: Formal development and veriﬁcation of approximation 
algorithms using auxiliary variables. In: Bruynooghe, M. (ed.) LOPSTR
2004. LNCS, vol. 3018, pp. 59–74. Springer, Heidelberg (2004)

4. Br´azdil, T., Kiefer, S., Kucera, A., Varekov´a, I.H.: Runtime analysis of probabilistic
programs with unbounded recursion. J. Comput. Syst. Sci. 81(1), 288–310 (2015)

wp–Reasoning for Expected Run–Times of Probabilistic Programs

389

5. Celiku, O., McIver, A.K.: Compositional speciﬁcation and analysis of cost-based
properties in probabilistic programs. In: Fitzgerald, J.S., Hayes, I.J., Tarlecki, A.
(eds.) FM 2005. LNCS, vol. 3582, pp. 107–122. Springer, Heidelberg (2005)

6. Chakarov, A., Sankaranarayanan, S.: Probabilistic program analysis with martingales.
 In: Sharygina, N., Veith, H. (eds.) CAV 2013. LNCS, vol. 8044, pp. 511–526.
Springer, Heidelberg (2013)

7. Dijkstra, E.W.: A Discipline of Programming. Prentice Hall, Upper Saddle River

(1976)

8. Fioriti, L.M.F., Hermanns, H.: Probabilistic termination: soundness, completeness,
and compositionality. In: Principles of Programming Languages (POPL), pp. 489–
501. ACM (2015)

9. Frandsen, G.S.: Randomised Algorithms. Lecture Notes. University of Aarhus,

Denmark (1998)

10. Gordon, A.D., Henzinger, T.A., Nori, A.V., Rajamani, S.K.: Probabilistic programming.
 In: Future of Software Engineering (FOSE), pp. 167–181. ACM (2014)
11. Hehner, E.C.R.: Formalization of time and space. Formal Aspects Comput. 10(3),

290–306 (1998)

12. Hehner, E.C.R.: A probability perspective. Formal Aspects Comput. 23(4), 391–

419 (2011)

13. Hickey, T., Cohen, J.: Automating program analysis. J. ACM 35(1), 185–220

(1988)

14. Hoare, C.A.R.: An axiomatic basis for computer programming. Commun. ACM

12(10), 576–580 (1969)

15. Hurd, J.: A formal approach to probabilistic termination. In: Carre˜no, V.A.,
Mu˜noz, C.A., Tahar, S. (eds.) TPHOLs 2002. LNCS, vol. 2410, pp. 230–245.
Springer, Heidelberg (2002)

16. Kaminski, B.L., Katoen, J.-P.: On the hardness of almost–Sure termination. In:
Italiano, G.F., Pighizzini, G., Sannella, D.T. (eds.) MFCS 2015. LNCS, vol. 9234,
pp. 307–318. Springer, Heidelberg (2015)

17. Kaminski, B.L., Katoen, J.P., Matheja, C., Olmedo, F.: Weakest precondition reasoning 
for expected run-times of probabilistic programs. ArXiv e-prints (2016).
http://arxiv.org/abs/1601.01001

18. Kozen, D.: Semantics of probabilistic programs. J. Comput. Syst. Sci. 22(3), 328–

350 (1981)

19. McIver, A., Morgan, C.: Abstraction Reﬁnement and Proof for Probabilistic Systems.
 Monographs in Computer Science. Springer, New York (2004)

20. Mitzenmacher, M., Upfal, E.: Probability and Computing: Randomized Algorithms

and Probabilistic Analysis. Cambridge University Press, Cambridge (2005)

21. Monniaux, D.: An abstract analysis of the probabilistic termination of programs.
In: Cousot, P. (ed.) SAS 2001. LNCS, vol. 2126, pp. 111–126. Springer, Heidelberg
(2001)

22. Motwani, R., Raghavan, P.: Randomized Algorithms. Cambridge University Press,

Cambridge (1995)

23. Nielson, H.R.: A Hoare-like proof system for analysing the computation time of

programs. Sci. Comput. Program. 9(2), 107–136 (1987)

24. Nielson, H.R., Nielson, F.: Semantics with Applications: An Appetizer. Undergraduate 
Topics in Computer Science. Springer, London (2007)

25. Wechler, W.: Universal Algebra for Computer Scientists. EATCS Monographs on

Theoretical Computer Science, vol. 25. Springer, Heidelberg (1992)

26. Winskel, G.: The Formal Semantics of Programming Languages: An Introduction.

MIT Press, Cambridge (1993)

