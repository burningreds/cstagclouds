Query-Sets++: A Scalable Approach for Modeling Web

Sites

Barbara Poblete1,2, Myra Spiliopoulou3, and Marcelo Mendoza4

1 Department of Computer Science (DCC), University of Chile, Chile

2 Yahoo! Research Latin-America, Chile

3 Otto-von-Guericke-University Magdeburg, Germany

4 Universidad T´ecnica Federico Santa Mar´ıa, Chile

Abstract. We explore an effective approach for modeling and classifying Web
sites in the World Wide Web. The aim of this work is to classify Web sites
using features which are independent of size, structure and vocabulary. We establish 
Web site similarity based on search engine query hits, which convey document 
relevance and utility in direct relation to users’ needs and interests. To
achieve this, we use a generic Web site representation scheme over different feature 
spaces, built upon query trafﬁc to the site’s documents. For this task we
extend, in a non-trivial way, our prior work using query-sets for single document
representation. We discuss why this previous methodology is not scalable for a
large set of heterogeneous Web sites. We show that our models achieve very compact 
Web site representations. Furthermore, our experiments on site classiﬁcation
show excellent performance and quality/dimensionality trade-off. In particular,
we sustain a reduction in the feature space to 5% of the size of the bag-of-words
representation, while achieving 99% precision in our classiﬁcation experiments
on DMOZ.

Keywords: Web Sites, Query Mining, Classiﬁcation.

1 Introduction

The fast expansion of the Web has made it increasingly important to ﬁnd ways to extend
the paradigm of Web Information Retrieval (IR) towards richer functionalities. In this
work, we shift the focus of Web IR from the traditional retrieval of documents that
satisfy a certain query, towards the retrieval of complete Web sites. Speciﬁcally, Web
site retrieval can enhance search in several ways, for example: (1) retrieving sites that
are relevant to a speciﬁc query, (2) retrieving sites that are similar to another given
site, (3) grouping similar sites, and (4) building groups of sites, where each group is
representative of a speciﬁc topic.

In particular, in this work we focus on organizing Web sites, which corresponds to
applications (3) and (4). Speciﬁcally, we analyze Web site models which allow us to automatically 
classify sites into predetermined categories. Currently, this type of problem
is solved through human-intensive initiatives such as the DMOZ1 and Yahoo!2 directories,
 which, for a given taxonomy, manually ﬁnd and assign Web sites that ﬁt best each

1 http://dmoz.org
2 http://dir.yahoo.com

R. Grossi et al. (Eds.): SPIRE 2011, LNCS 7024, pp. 129–134, 2011.
c(cid:2) Springer-Verlag Berlin Heidelberg 2011

130

B. Poblete, M. Spiliopoulou, and M. Mendoza

topic. Obviously, this approach does not scale well in a rapidly growing and heterogeneous 
environment like the Web; since it requires knowledge of all possible topics and
regular inspection of all of the top Web sites for each topic.

We believe that by obtaining a suitable Web site model we will also have the basis for
addressing applications (1) and (2). Our solution is based on the wisdom of the crowds
paradigm. We consider two Web sites as being similar if users access both of them to
satisfy similar information needs. This type of approach allows us to identify Web sites
that are similar according to their perceived information content, independent of their
structure, size, vocabulary and speciﬁc data.

Our contribution is threefold: First, we address the problem of classifying providing
a solution that reﬂects how each Web site is perceived by users. Second, we show that
our prior work [8], for creating compact document models using queries cannot be
extended to large collections of Web sites in a trivial way. Third, we propose several
feature spaces which are appropriate for representing Web sites on the Web.

2 Related Work

We make the distinction between similar and duplicate or near-duplicate document
detection. Although there is extensive work in the latter area, it pursues a different goal
than similarity research: the goal is to detect almost identical documents. In our work
we focus on similarity research with the goal of ﬁnding Web sites that satisfy similar
information needs from users, but are not necessarily alike in contents, extension or
vocabulary.

There have been several efforts towards the automatic classiﬁcation of Web sites.
Ester et al. [3] propose to model Web sites as feature vectors. They consider each topic
as a feature in a topic-based space, representing each Web site as a topic-based feature 
vector. Later, Kriegel and Schubert [6] applied the previous method for automatic
maintenance of Web directories. They modiﬁed the method representing each topic by
the centroid of the Web sites which belong to the topic. Lindemann and Littig [7] studied 
structural properties of Web sites, using them as features for Web site description.
On the other hand, Rajalakshmi and Aravindan [9] present a soft computing approach
for Web site classiﬁcation. Their approach is based on features extracted from URLs,
without fetching Web site contents.

3 General Concepts

Web site: A formal and unambiguous deﬁnition of Web site is an open problem [2].
Deciding how to automatically classify documents into Web sites is not the purpose
of our work. Instead, we are interested in the task of organizing Web sites as wholes.
Therefore, we adhere to a simple heuristic deﬁnition for Web site which considers as
part of a Web site all of the documents that appear under the same host name [3].

Query-set mining: Query-sets are discovered by analyzing all of the search queries
from which a document was clicked, to obtain groups of terms that are used together to
reach the document. Formally, let L be a search engine query log and let Q be the set

Query-Sets++: A Scalable Approach for Modeling Web Sites

131
of distinct queries registered in L. Therefore, each query q ∈ Q can be repeated one or
more times in L. Then, for a given document d, we denote as Q(d) the set of distinct
queries in Q that resulted in a request for d, and as L(d) the portion of L that contains
clicks to d. Further, we denote as QT (d) the set of query-terms used in Q(d).

4 Generic Web Site Vectorization

We introduce a simple scheme for modeling Web sites as vectors over an arbitrary
feature space. We use an extension of the traditional vector-space model for documents,
with the variation that a Web site vector is composed by the sum of the vectors of its
documents. We use this for the creation of several query-based feature spaces.
Let D = {d1, d2, . . . , dn} be a collection of documents and let F = {f1, f2, . . . , fm}
be the set of features that characterize those documents. Further, let wi,j be the weight
associated to the pair (di, fj). Then, the generic document vector for di is deﬁned as
−→
di =< wi,1, wi,2, . . . , wi,j, . . . , wi,m >. This vector is a generalization of the vector
space document model, which incorporates an arbitrary m-dimensional feature space
F. In the traditional vector space representation, F corresponds to the set of terms in D,
and wi,j to the weight of the jth-term in the ith-document - given by the term frequency
in di. Next, we create a representation for Web sites consisting of the aggregation of
each site’s document vectors. Let SITES = {s1, s2, . . . , sN} be a set of Web sites and
let D be the collection of all documents in SITES . Where sk ⊆ D for k = 1, . . . , N.
Then, the vector representation of a Web site sk built over the generic feature space F
−→
sk =< ck,1, ck,2, . . . , ck,j , . . . , ck,m >, where each ck,j corresponds to a weight as-
is:
sociated to the pair (sk, fj) for fj ∈ F. The value of ck,j is the normalized counterpart
(cid:2)
k,j, according to the tf-idf scaling scheme proposed in [1,5]:
of w

(cid:6)
(cid:2)
Where w
k,j is the sum of the weights of the documents in sk for a given feature fj:
(cid:2)
(cid:2)
k,l) is the feature with the largest weight in sk
di∈sk wi,j. and maxfl∈F(w
k,j =
w
and nj is the number of sites where fj appears. In particular, this vectorization of a Web
site sk ∈ SITES requires two parameters to be speciﬁed: 1) the feature space F over
the documents constituting all the sites in SITES , and 2) the weighting scheme for the
features over the documents.

5 Query-Based Feature Spaces for Web Sites

We deﬁne several variations of the feature space selection for Web sites. We model each
site s in a given set of Web sites SITES , as a vector over a feature space consisting of
elements that are either queries, query-terms or query-sets. In detail:
• “QUERYTERMS Model”: The feature space F consists of all individual queryterms 
that constitute the queries leading to documents in the SITES :
F = ∪s∈SITES (∪d∈sQT (d))

(cid:2)

ck,j =

0.5 +

(cid:3)

(cid:4)

0.5 · w
(cid:2)
k,j
(cid:2)
maxfl∈F(w
k,l)

×

−log2

nj
N

(cid:5)

,

B. Poblete, M. Spiliopoulou, and M. Mendoza

132
• “FULLQUERIES Model”: The feature spaceF consists of complete queries, namely
the queries used to access the documents in SITES : F = ∪s∈SITES (∪d∈sQ(d))
• “FREQPATTERNS Model”: The feature space F consists of the frequent query-sets
for each document in SITES , subject to threshold τ: F =∪s∈SITES (∪d∈sQS(d, τ))
• “FULLPATTERNS Model”: The feature space F consists of all query-set elements
for all documents, i.e. the support threshold τ is zero:F =∪s∈SITES (∪d∈sQS(d, 0))
• “MAXPATTERNS Model”: The feature space F consists of all maximal querysets 
for the documents in SITES , i.e. the frequency threshold to zero as above:
F = ∪s∈SITES

(cid:5)
(cid:7)QS(d, 0)

∪d∈s

• “FULLQUERIESPLUS Model”: The feature space F contains for each document
d those query-sets for which there is a query in Q (not necessarily in Q(d)), i.e.
independently of whether this query resulted in a request for d:
F = ∪s∈SITES (∪d∈s(QS(d, 0) ∩ Q))

(cid:4)

For the vectorization of a Web site using these feature spaces, we also need the weights
of the features for the individual documents. Let fj be a feature, i.e. a query-term, a
query-set or a complete query, depending on the feature space we use. The weight of
fj for a document d ∈ D is either: a) the number of queries in L(d) that contain fj, in
the case that fj is a query-terms or query-set, or b) the number of queries in L(d) that
match exactly fj, in the case that fj is a query. In other words, the weight of each fj
for a document d is clicks(fj, d) as deﬁned earlier. Then, the unnormalized weight of
feature fj for the site sk ∈ SITES is the sum: w

d∈sk clicks(fj, d).

(cid:2)
k,j =

(cid:6)

Note that QUERYTERMS and FREQPATTERNS models, correspond to the models

presented in [8], the other remaining feature spaces are novel.

6 Model Evaluation and Results

We present our methodology for modeling and classifying Web sites using the previously 
deﬁned Web site representations. We present a comparison of their clustering and
classiﬁcation performance, and consider as a baseline bag-of-words model (denoted as
“TEXT model” hereafter). As in related work, we use the DMOZ directory as a “gold
standard” for this task.

Dataset. As a data source we use a sample of the Yahoo! UK query log, which contains
2,109,198 distinct queries and 3,991,719 query instances. The vocabulary of the query
instances contains 239,274 distinct query terms. Our log sample contained 977 Web
sites in DMOZ, which included 5, 070 URLs with hits from queries. These Web sites
classiﬁed into 216 DMOZ categories.

Table 1 shows the number of features obtained for each Web site model in our dataset.
Models based on query sets reduce signiﬁcantly the dimensionality of the original feature 
space (obtained using the vector model). FULLPATTERNS reduces the dimensionality 
by approximately 1/3 of the original feature space, increasing the number of not
null entries with respect QUERYTERMS by approximately 400%.

Difﬁculties with the FREQPATTERNS model. This model represents the aggregated
version for the best single-document representation proposed in [8]. This method

Query-Sets++: A Scalable Approach for Modeling Web Sites

133

involves a minimum frequency threshold τ for each query-set size (number of elements
in a set). Empirically, we observe that the selection of this parameter is not possible
when faced with documents from a large heterogeneous group of Web sites. In general,
 frequent itemset mining allows to identify many itemsets with little support and
a few itemsets that have high support values. Interestingly, we observe that the aggregated 
distribution of pattern sizes for documents from multiple Web sites does not ﬁt
the previous assumption. Our collection shows many itemsets with low support and also
many itemsets with high support. Therefore, the selection of τ would require manual
inspection of each Web site’s distribution, which is not desirable.

Experimental Validation. In this study, we consider DMOZ categories to be the real
categories of the Web sites. Therefore, we evaluate the classiﬁcation solutions against
this external quality indicator, which we consider as a ground truth. In particular, categories 
in DMOZ follow a tree hierarchy, this allows to evaluate at different levels of
granularity, ranging from general to very speciﬁc. By truncating the tree at height 3
(denoted @3) we consider 45 only categories, at 4 (@4), 75 categories, 5 (@5), 104
categories, and also using the complete tree (Full Dir.) with 216 categories. When truncating 
the hierarchy tree, all of the categories that extend beyond the cutting point are
collapsed into their parent node.

We create a classiﬁer in which each training instance is composed of a collection
of features, deﬁned using a Web site model, and a label that indicates which category
the instance belongs to. Each testing instance is categorized using the classiﬁer. The
nominal label and the predicted label are compared to evaluate the performance of the
model. We use a method based on logistic regression. Additionally, since this is a multiclass 
problem, we extend the logistic regression model using the one versus rest method
(OVR) [4]. The OVR method has shown comparable precision performance to real
multi-class methods reducing training time.

To evaluate the performance of the models we compare for each testing instance the
nominal class and the predicted class. From the four possible cases presented when
comparing both labels, true positives (tp), false positives (fp), false negatives (fn) and
true negatives (tn), we calculate the accuracy measure for the tuning and training pro-
tp+f p ). Then the
cess (
overall score is calculated by measuring the average.

tp+tn+f p+f n ) and the precision measure for the testing step (

tp+tn

tp

The dataset is partitioned to perform 3-fold cross validation. Each fold preserves
the original proportions existing among the categories. Table 2 shows the performance
of the classiﬁers, each entry represents a precision value. As Table 2 shows, FULLPATTERNS 
outperforms the other models in all evaluations, with exceptional testing
precision (99.69 @Full Dir.). In particular, FULLPATTERNS outperform TEXT by approximately 
10% when we consider the full directory. Also, FULLPATTERNS reduces
by approximately 50% the dimensionality of the TEXT feature space, increasing the
precision performance. Regarding FULLQUERIES, FULLQUERIESPLUS or MAXPATTERNS,
 the reduction of the feature space dimensionality is more extreme (app.
the 5% of the TEXT feature space) without losing precision. Regarding the use of frequent 
patterns in queries for Web site modeling, it can be observed that the models
based on query-sets outperform QUERYTERMS. This result shows the usefulness of
the Web site models based on patterns.

134

B. Poblete, M. Spiliopoulou, and M. Mendoza

Table 1. Number of features of each model

Table 2. Testing Precision

Model No. of Features Not Null Entries

FULLPATTERNS
FULLQUERIES
FULLQUERIESPLUS
MAXPATTERNS
QUERYTERMS
TEXT

56,929
9,151
8,957
10,518
6,763
178,449

72,981
9,875
12,269
11,098
16,096
591,004

7 Conclusions

No. of Clusters

75

46
@3 @4 @5 Full Dir.

104

216

FULLPATTERNS 98.97 99.38 98.15
FULLQUERIES 90.48 83.31 89.96
FULLQUERIESPLUS 88.43 86.28 87.51
MAXPATTERNS 93.75 93.14 93.55
QUERYTERMS 87.64 87.23 87.64
TEXT 94.06 93.34 91.60

99.69
97.03
96.72
96.92
88.97
89.25

In this work we focus on generating simple and scalable Web site representations for
classiﬁcation. Our approach is centered on providing models which convey users’ information 
needs when visiting relevant Web documents from search engines. Additionally
we aim at ﬁnding similarities between Web sites in a way which is independent of
content, structure and size.

The usefulness of our Web site models is measured by applying categorization to
Web site vectors, with the objective of organizing similar sites. Our experimental evaluation 
shows that our models approach use signiﬁcantly less features than the full text
approach obtaining better results. In addition, the categorization process shows that
FULLPATTERNS is the best discriminative model, achieving a precision performance
of 99.69% (almost perfect), 10% over the baseline. This is an important result, especially 
considering that FULLPATTERNS reduces the dimensionality of the original
feature space by approximately 50%.

References

1. Baeza-Yates, R., Ribeiro-Neto, B.A.: Modern Information Retrieval. ACM Press / AddisonWesley 
(1999)

2. Bharat, K., Chang, B.W., Henzinger, M.R., Ruhl, M.: Who links to whom: Mining linkage

between web sites. In: ICDM 2001 (2001)

3. Ester, M., Kriegel, H.P., Schubert, M.: Web site mining: a new way to spot competitors, customers 
and suppliers in the world wide web. In: KDD 2002 (2002)

4. Fan, R., Chang, K., Hsieh, C., Wang, X., Lin, C.: Liblinear: A library for large linear classiﬁcation.
 JMLR 9, 1871–1874 (2008)

5. Karypis, G.: CLUTO a clustering toolkit, http://www.cs.umn.edu/˜cluto
6. Kriegel, H.P., Schubert, M.: Classiﬁcation of websites as sets of feature vectors. In: IASTED

2004 (2004)

7. Lindemann, C., Littig, L.: Coarse-grained classiﬁcation of web sites by their structural properties.
 In: WIDM 2006 (2006)

8. Poblete, B., Baeza-Yates, R.: Query-sets: using implicit feedback and query patterns to organize 
web documents. In: WWW 2008 (2008)

9. Rajalakshmi, R., Aravindan, C.: Naive bayes approach for website classiﬁcation. In: Das,
V.V., Thomas, G., Lumban Gaol, F. (eds.) AIM 2011. CCIS, vol. 147, pp. 323–326. Springer,
Heidelberg (2011)

