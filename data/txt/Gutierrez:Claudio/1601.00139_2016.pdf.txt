6
1
0
2

 

n
a
J
 

2

 
 
]
I
S
.
s
c
[
 
 

1
v
9
3
1
0
0

.

1
0
6
1
:
v
i
X
r
a

Group Centrality for Semantic Networks:
a SWOT analysis featuring Random Walks

Camilo Garrido, Ricardo Mora, and Claudio Gutierrez

Department of Computer Science, Universidad de Chile, Chile

{cgarrido,rmora,cgutierr}@dcc.uchile.cl

Abstract. Group centrality is an extension of the classical notion of
centrality for individuals, to make it applicable to sets of them. We perform 
a SWOT (strengths, weaknesses, opportunities and threats) analysis 
of the use of group centrality in semantic networks, for diﬀerent
centrality notions: degree, closeness, betweenness, giving prominence to
random walks. Among our main results stand out the relevance and NPhardness 
of the problem of ﬁnding the most central set in a semantic
network for an speciﬁc centrality measure.

1

Introduction

The notion of centrality in graphs has been thoroughly studied since it was introduced 
for social networks, being also applied in other contexts ranging from
physics to information retrieval [11], but as Freeman writes for social networks
[6], “there is certainly no unanimity on exactly what centrality is or on its conceptual 
foundations, and there is very little agreement on the proper procedure
for its measurement.” Over the years, a great variety of measures of centrality
have been proposed, most of them pointing to the problem of ﬁnding the most
“central” (regarding the notion and/or the measure) node in a network [18].

Most of classic work on centrality studies what we could call “individual”
centrality: what is the most central node in a network for a certain measure.
This idea can be extended to a group: Given an integer k > 1, what is the
most central set S of nodes, of size k? This is known as group centrality and
was introduced by Everett and Borgatti [5] for social networks in order to make
individual measures of centrality work with groups of elements.

In this paper we study the application of the idea of group centrality to
semantic networks. To the best of our knowledge, this link has not been explored
before. It can help to shed light on the idea of a “most signiﬁcative” group of
concepts in a semantic network, which in the age of big (semantic) data, seems
to be indispensable to make sense of notions like summary, table of contents,
compendium, etc. on huge semantic network datasets.

Our goal in this paper is to show the robustness of this application, develop its
properties, and present its opportunities, scope and possibilities and limitations.

We perform the study based on some classical centrality measures like degree,
 closeness and betweenness [6].1 We present in more detail random walk
centrality which is a measure based on the notion of hitting time (informally:
the expected length of a random walk). This idea has been used with success
in modeling semantic relatedness. The intuition is that the “jump” from one
concept to another is better modeled through random walks rather than using
deterministic movements. This approach has been explored in human memory
[2], lexical relations [8] and Wikipedia corpus [13].

hooves

frog

antlers

mammal

hair

deer

animal

dog

bat

blood

robin

bird

livingthing

antlers

deer

dog

chicken

hooves

hair

animal

frog

chicken

leaves

tree

mammal

bird

livingthing

blood

feathers

cottonwood

tree

bat

red

robin

red

rose

feathers

ﬂower

plant

color

daisy

green

cottonwood

leaves

color

plant

green

ﬂower

rose

daisy

{ livingthing } – { mammal }

{ red, livingthing } – {mammal, plant }

k = 1
k = 2
k = 3
k = 4 { plant, red, animal, livingthing } – { mammal, plant, bird, tree }

{ bird, plant, animal } – { mammal, plant, bird }

Fig. 1: Networks built by novices (left) and experts (right) over the same set of
25 concepts. In the table, for each k it is shown the most central set of k concepts
of the novice (left) and the expert (right) networks.

Let us consider the examples shown in Fig. 1. Schvaneveldt and social scientists 
[15] made a study of the process of construction of semantic networks.
They chose 25 natural concepts and devised a method to allow communities to
establish semantic relationships among them. The networks were built using two
diﬀerent communities: on the left by novices (psychology students) and on the
right by domain experts (biologists). What are the most central concepts here?
First, let us point that the four centrality measures mentioned previously, show
remarkable similarity in their outputs (these measures do not consider labels,
only the structure of the graph). On the novices’ network, the four measures

1 Degree centrality selects the node with higher degree; closeness indicates the node
that minimizes the distance from all other nodes in the network to it; Betweenness
selects the node over which pass the higher number of shortest paths between each
pair of nodes. We will deﬁne them formally in Section 3.

indicate “livingthing”. As for the biologists’ network they indicate “mammal”.2
Also, the four of them behaved remarkable similar when searching for central
groups of diﬀerent sizes, as shown in Fig. 1.

What do these examples tell? First of all, they give insights on the possible 
relevance and use that group centrality could have for semantic networks.
The contribution of this paper is a study of the assumptions that lie under this
optimistically/naive hypothesis, that is, an analysis of the strengths and opportunities 
and weaknesses and obstacles (known as SWOT analysis) of applying
this idea to semantic networks. (In the related work section we will go over the
work that has been done on ranking semantic networks, that touches some facets
of this problem).

The concrete contributions of this paper are:

1. Complexity analysis. We show that for the four previously mentioned centrality 
measures, the problem of ﬁnding a set with optimal centrality score
is NP-hard (and its associated decision problem is NP-complete).

2. We present a small-scale study of samples of semantic networks –because
of the hardness of the computations and the diﬃculties to get a human
evaluation of what is “central” in a huge network, a challenge by itself– to
show tendencies and illustrate the potential and robustness of the idea of
group centrality.

3. We develop the notion of random-walk centrality, and prove several theoretical 
properties for it. In particular, we show that testing random-walk group
centrality can be done in polynomial time.

The paper is organized as follows. In Section 2 we present the main arguments
of the SWOT analysis and related work on the subject. In Section 3 we show
the study of samples of some real semantic networks. In section 4 we include the
proofs and support for the theoretical arguments and in section 5 we present the
conclusions.

2 Group Centrality in Social Networks

Let us begin by stating formally the notions to be studied.

Basic graph notions. An undirected and simple graph is a pair G = (V, E) where
E ⊆ [V ]2, and [V ]2 is the set of all 2-elements subsets from V . The elements of
V are the vertices of G, and the ones from E are its edges. When necessary, we
will use the notation V (G) and E(G) for those sets. From now on, an element
{u, v} ∈ E will be denoted simply by uv.
A vertex u is said to be a neighbor of another vertex v, when uv ∈ E. Note
that the deﬁnition of E implies that v is also a neighbor of u. The set of neighbors
of v will be denoted by NG(v). The degree of v, dG(v) is the size of NG(v).

2 In the experiments, the biologists were asked separately to tell the most representative 
concept in the list, and they chose “mammal”.

A graph Pn =(cid:0){v0, v1, ..., vn},{v0v1, v1v2, ..., vn−1vn}(cid:1) with n ≥ 0, where all

vi are distinct is called a path, and the number of edges in it is its length. A cycle
is a special type of path such that v0 = vn. A path Pn in G, with n ≥ 1 such
that v0 = u and vn = v, is called a u-v path. Also G is said to be connected if
for all distinct u, v ∈ V a u-v path exists in G. A connected component of G is
a maximally connected subgraph H.
length of the shortest u-v path in G. For S ⊆ V , d(u, S) := minv∈S d(u, v).

The distance between u and v (cid:0)denoted by dG(u, v) or just d(u, v)(cid:1) is the

In what follows, all our graphs will considered to be simple, undirected and

connected.

Group Centrality The centrality measures that we study were designed to work
with single vertices [6] rather than groups of them, thus we use their natural
extension to sets as proposed in [5].
Let G = (V, E) be a graph and S ⊂ V . We denote by Cj(S) for j ∈ {d, c, bc}

the group centrality according to the centrality measure Cj.

Group degree centrality Cd(S) counts the number of vertices not in S that
are connected to some vertex in S. Multiple ties to the same vertex are counted
only once. Group closeness Cc(S) studies the value of the sum of the distances
from S to all vertices not in it. Note that for group degree, the higher the value
of Cd(S), the more central the set is. Whereas for group closeness the reciprocate
holds. Formally, they are deﬁned as follows:

Cd(S) =

|{v ∈ V \ S, N (v) ∩ S (cid:54)= φ}|

|V \ S|

,

Cc(S) =

v∈V \S d(v, S)
|V \ S|

.

(cid:80)

(cid:88)

u<v
u,v /∈S

Group betweenness centrality Cbc(S) indicates the proportion of geodesics

connecting pairs of vertices (that are not in S), that pass through S

Cbc(S) =

2BC(S)

|V \ S|(|V \ S| − 1)

, with BC(S) =

σuv(S)

σuv

,

where σuv is the total number of shortest paths from u to v and σuv(S) is the
number of those paths that pass through S. As for group degree, the higher the
value of Cbc, the better.

2.1 Random Walk Centrality

Random Walks The next deﬁnitions come from the work of Lov´asz in random
walk theory [10]. Let G = (V, E) be a graph such that |V | = n and |E| = m,
where n, m ∈ N. Formally, a random walk is a sequence of vertices obtained as
follows: it starts at a vertex v0, and if at the t-th step it is at a vertex vt = u, it
moves to a neighbor v of u with probability puv = 1/d(u).
Let S ⊆ V . The hitting time for a set H(u, S) is the expected number of
steps that a random walk starting at vertex u takes to reach some vertex in S
for the ﬁrst time. When S = {v} is a singleton, we will simply write H(u, v).

One of the issues that all three measures of group centrality (presented on
2.1) have in common, is that they only take into consideration shortest paths
between pairs of vertices. A semantic network represents relations between concepts,
 motivated by an speciﬁc context. Should that context change, new and
more direct connections may arise. In that regard, as much connections as possible 
should be taken into consideration, which is precisely what random walk
centrality attempts to do.

Deﬁnition 1 (Random Walk Centrality). The random walk centrality of
S ⊂ V is

(cid:80)

v∈V \S H(v, S)

|V \ S|

h(cid:46)(S) =

.

Essentially random walk centrality is a variation of closeness that takes into
consideration all ways to reach S from a vertex v, rather than only through the
shortest path. As for group closeness, the lower the value of h(cid:46)(S), the more
central the set is.

2.2 The arguments for the SWOT analysis

Let us state them in general terms the arguments of the SWOT analysis.

Obstacles: weaknesses and threats. First of all, the problem is computationally
hard; we prove that for all four measures considered it is NP-hard:

Theorem 2. The problem of ﬁnding an optimal solution S of size k, is an NPhard 
problem for each of the four group centrality measures studied in this paper
(that is, for C ∈ {Cd, Cc, Cbc, h(cid:46)}) .

This theorem points to the diﬃculties of applying the notion to big data (and
in this paper for experimenting with it) and gives a ﬁrst program of research on
this problem: to develop good approximate algorithms for it.

Second, there is a diﬃculty associated to experimenting with big data due to
the lack of good benchmarks and semantically “marked” data, that is, networks
for which we have a clear agreement on what are their most central concepts at
diﬀerent granularities (i.e. the size of an optimum candidate). This is a challenge
by itself (the scope of human physical abilities) and that is the reason why we
use small (human-scale) datasets in this paper.

Third, the quality of data. It turns out that the output of the centrality and
group centrality methods depends heavily on the “quality” of the network: in
other words, in how well it represents the semantics we are looking for. The examples 
we discussed on section 1 are prime cases of excellent semantic networks
(in fact, developed by psychologists based on systematic experimentation). Unfortunately,
 the semantic networks that we see in real life (for example RDF
data) are usually not as good. We will show through several sample datasets
that many of our current most popular semantic data representations (e.g. RDF
graphs) have a bias as they are mostly constructed in ad-hoc manners (union

of pieces of information scattered from diﬀerent sources; without the goal of
completeness, etc).3

Strengths and Opportunities. First, the notion of group centrality seems to be
worth exploring in the quest for developing summaries, small representations or
give insight about the meaning and contents of big semantic network datasets.
We show samples of diﬀerent types of networks, including stable RDF datasets
like DBpedia. The results were in all cases encouraging.

Second, the notion seems to be robust and become more stable as the size
of the optimum candidate k increases. There are indications from the sample
data we worked out and partial theoretical results (Proposition 7), that in the
long term, all measures behave in the same way as k converges to |V |/2 (that
is, there exists a set that is optimum for all four measures). From this we have
a second line of research: to develop concluding analytical and real data studies
that can support this conjecture.

Third, the notion of random walk shows in general less variance than other
measures, i.e. gives more “unique” solutions for k > 0. This virtue and the fact
that this measure systematically gives solutions that have elements in common
with those obtained through the use of other measures, are good signals of its
signiﬁcance. If we link this observation to the fact that its theoretical basis
(the notion of random walk) has been successfully used in semantic networks
(see Related Work section) there is good ground to make it a candidate for
systematic study at the theoretical and experimental level.

2.3 Related Work

Random Walks and Semantic Networks. The motivation to address random walk
centrality comes from works that have used the notion of random walks and related 
it with semantics. The work of Abbott et al. [2] compares the functioning
of the human mind when searching for memories with a random walk in a semantic 
network, as both present a similar behavior. They conclude that these
results can help clarify the possible mechanisms that could account for PageRank 
predicting the prominence of words in semantic memory. Yeh et al. [13]
use random walks to determine the semantic relatedness of two elements. In
particular, they use a personalized PageRank algorithm with a custom teleport
vector. They conclude that random walk combined with personalized PageRank
is a feasible and potentially fruitful mean of computing semantic relatedness for
words and texts. Hughes et al. [8] introduce a new measure of lexical relatedness
based on the divergence of the stationary distributions computed using random

3 We are well aware of the complexities of determining in a network of concepts which
one is more relevant. This relies on the underlying semantics of the notions involved,
on the domain from which they are taken from, on the goals that the people that
built the network had, and ﬁnally, on the quality of the network itself. Probably due
to all of the above, today we do not have standard, nor good benchmark data for
these tasks.

walks over graphs extracted from WordNet. All these works have been valuable
sources of inspiration for our method.

Relevance and quality ranking in the Semantic Web. As our motivation of
studying group centrality points to its use in the area of relevance in Semantic
data, below we discuss some related work made on it. Graves et al.[7] propose
a graph-theoretic measure called noc-order for ranking nodes in RDF graphs.
They base their ranking in a variation of closeness centrality, which measures
how costly is to reach all the nodes in the graph from one speciﬁc vertex. Note
that this idea does not use the concept of expected length (via random walks).
Zhang et al. [14] address the problem of summarizing RDF graphs using an
RDF sentence as the atomic unit. They focus in ranking the RDF sentences to
get a summary of the ontology and base their method in centrality measures
such as degree-, eigenvector-, or betweenness-centrality. Cheng et al. [3] address
the problem of ranking features of an entity in a semantic graph. Their method
relies on centrality notions enhanced with the information captured by labels.
The topics of these works is related to our work, though we focus in ﬁnding
central elements in the graph as a whole, while they do it in summarizing en-
tities/ontologies; also they do not present a conceptual notion of key elements,
rely only on experimental evaluations and do not address group centrality. Ding
et al. [4] address ranking of semantic objects on the Web. They concentrate on
documents, terms and RDF graphs as a whole. For terms, they use popularity
measures essentially based on use and population, weighted by some parameters
of the dataset (occurrences, etc.). Supekar et al. [12] address the problem of
quality of ontologies. They characterize ontological features to calculate a score
in order to determine the quality of the ontology. They characterize features like
how well the ontology reﬂects the domain of interest, correctness of the ontology
with respect to the domain of discourse, how easy the ontology is to understand
or the depth and width of the ontology. These last two papers deal with ranking
of ontologies as such, but do not address the problem of ﬁnding a set of relevant
concepts/nodes in RDF graphs, nor present any conceptual framework to deﬁne
them.

3 Group Centrality on Semantic Network Data

We are interested on knowing what the group centrality approach applied to
semantic networks has to oﬀer. In order to do this, we apply group degree,
closeness, betweenness, and random walk centrality notions to semantic networks
data and study their behavior. As was stated on section 2, we restrict our analysis
to small networks for two reasons: ﬁnding a set with optimal value for an speciﬁc
centrality measure is NP-hard; and small networks (human scale) allow to get
an insight about which concepts these group centralities select and how those
selections diﬀer from each other.

3.1 Datasets

Networks of natural concepts [15]. This is the dataset we presented in the
introduction. There we showed two networks built, respectively, by a group of
undergraduate psychology and by graduate biologist. Additionally, we show here
two other networks. A group of students from introductory psychology courses
were asked to rate the degree of relatedness of all pairwise combinations of 25
natural concepts. Using this information the authors constructed two networks,
according to two diﬀerent thresholds of concept relatedness.
Perception: semantic network of common sense [16]. This network stores
knowledge about what things look and feel like. It has about 9, 000 manually
annotated relations between 4, 000 mundane concepts, e.g, cat, house, snow, etc.
DBpedia Categories [1]. This dataset covers all the Wikipedia article categories 
and how they are related using the SKOS vocabulary. The network covers
1, 130, 342 categories with 2, 308, 585 relations between them.
Roget’s Thesaurus This is a widely used English-language thesaurus that
contains more than 15, 000 words. This network was been used in works studying
semantic similarity and sense disambiguation, among others.

For each of the last 3 networks we extracted samples of 40 nodes using
random-walk sampling according to Leskovec and Faloutsos [?] until 40 diﬀerent
nodes were visited and then we extracted the induced subgraph. Here we show
two such samples for each dataset.

{11}

closeness

degree
{11}
{4, 11}

{4, 11, 13} , {0, 11,

n
1
{2, 13} , {4, 11}
2
{0, 11, 13} , {4, 11,
3
4 {2, 4, 11, 13} , ... (23) {2, 4, 11, 13} , ... (5)]

{0, 11, 13}
{2, 4, 11, 13}
Table 1: Solutions for group centrality on Proximity Semantic Network 1.

{4, 11, 13}
{2, 4, 11, 13}

{11}
{4, 11}

{11}
{4, 11}

13} , ... (9)

13} , ... (4)

random-walk

betweenness

degree
{11}
{0, 11}

n
1
2
3
4 {2, 4, 13, 21} , ... (16)

closeness

{11}

{2, 11} , ... (2)
{2, 13, 21}
{2, 4, 13, 21}

{2, 13, 21} , ... (8)

{2, 13, 21}
{2, 4, 13, 21}
Table 2: Solutions for group centrality on Proximity Semantic Network 2.

{2, 11, 13, 21}

{2, 11, 13}

betweenness

{11}
{2, 11}

random-walk

{11}
{2, 11}

closeness

{6}

betweenness

{6} , {17}

degree
{0, 6}

{0, 6} , {10, 17}

{3, 13, 17} , {3, 10,

n
1
2
3
4 {0, 6, 10, 17} , ... (18) {0, 6, 10, 17} , ... (6)
Table 3: Solutions for group centrality on Proximity Semantic Network constructed 
by non-biologist students.

{3, 10, 17}
{0, 6, 10, 17}

{0, 13, 17}
{0, 6, 10, 17}

17}, ... (10)

{3, 10, 17}

{17}
{0, 6}

{6}
{0, 6}

random-walk

n
1
2
3 {3, 12, 19}, {3, 12, 21} {3, 12, 21}, {3, 12, 22}

closeness
{12}
{3, 12}

degree
{12}
{3, 12}

4

{3, 12, 19, 23}

{3, 12, 22}, {3, 12, 23}

{3, 12, 22}
{3, 12, 15, 19}, {3, 12, 15, 21} {3, 5, 12, 22}
Table 4: Solutions for group centrality on Proximity Semantic Network constructed 
by biologist students.

{3, 12, 19}, {3, 12, 21}

{3, 12, 19, 23}

{3, 12, 23}

betweenness

{12}
{3, 12}

random-walk

{12}
{3, 12}

degree
{4} , {20}
{4, 20}

n
1
2
{4, 18, 20}
3
4 {4, 6, 18, 20} , {0, 4, 6, 20} {0, 4, 6, 20} {4, 6, 7, 20} {4, 6, 18, 20}
Table 5: Solutions for group centrality on DBpedia sample 1.

closeness
{4, 20}
{0, 4, 20}

{20}
{4, 20}
{4, 7, 20}

{4, 6, 20} , {6, 7, 20}

random-walk

{4, 20}

betweenness

{4}

{4}

degree
{13}

n
1
2 {0, 13} , {1, 13} , ... (2)
3
4

{0, 1, 13} , ... (4)
{0, 1, 3, 13} , ... (1)

betweenness random-walk

closeness
{13}
{12, 13}
{1, 13}
{0, 1, 13}
{0, 1, 13}
{0, 1, 3, 13} {0, 1, 3, 13} {0, 1, 3, 13}

{13}
{0, 13}
{0, 1, 13}

{13}

Table 6: Solutions for group centrality on DBpedia sample 2.

degree
{25}
{14, 25}

n
1
2
3
4 {0, 4, 14, 25} , ... (10)

{4, 14, 25} , ... (5)

closeness

{14}

{5, 14} , {7, 13} , {7, 14} , {8, 13}

{4, 14, 25} , ... (4)
{1, 4, 14, 25} , ... (2)

betweenness

random-walk

{14}

{14, 25}
{4, 14, 25}

{13}
{6, 14}
{0, 6, 14}

{4, 12, 14, 25} {0, 4, 14, 25}

Table 7: Solutions for group centrality on Perception sample 1.

closeness

{29} , ... (1)

degree
{2, 29}

n
1
2
3
4 {2, 8, 29, 39} , ... (2) {2, 26, 29, 39} , ... (1) {2, 8, 10, 29} {2, 26, 29, 39}
Table 8: Solutions for group centrality on Perception sample 2.

{1}
{1, 2}
{1, 2, 8}

{0, 15, 32} , ... (7)

{2, 8, 29} , ... (3)

{29, 39}
{2, 8, 39}

{0, 15} , ... (1)

random-walk

betweenness

{29}

{15}

degree
{10}
{10, 25}

n
1
2
3
4 {10, 17, 25, 39} , ... (87) {10, 17, 25, 39} , ... (87) {10, 17, 25, 35} {10, 17, 25, 39}
Table 9: Solutions for group centrality on Roget’s Thesaurus sample 1.

closeness
{10, 25}

{10, 25, 39}

{10, 25, 35}

{10, 25, 39}

{10, 25, 39}

{25, 35}

{10, 25}

random-walk

betweenness

{35}

{35}

{10}

3.2 Discussion of results

For each semantic network, and each centrality measure, we computed the optimal 
solutions of size 1, 2, 3, and 4. The results are presented in Tables 1 to

degree
{11}
{11, 14}

{37}

{37}

n
1
2
3
4 {11, 14, 29, 37} , ... (440) {11, 14, 29, 37} , ... (440) {11, 14, 26, 37} {11, 14, 29, 37}
Table 10: Solutions for group centrality on Roget’s Thesaurus sample 2.

{11, 14, 29} , ... (6)

{1, 14, 29} , ... (6)

{37}

closeness
{14, 26}

{14, 26}

{11, 14, 29}

{26, 37}

{11, 26, 37}

betweenness

random-walk

10. It is important to note that in some cases there were multiple candidates for
optimal solution, but we only include at most two of them4 (if there are more
than two solutions, there is a mark “... (n)” where n is the number of extra
solutions) .

From the results obtained, we can advance two main observations:

Robustness of group centrality. It is noticeable that in the samples the optimal
solutions for all centrality measures become similar as the size k of the set
increases. Indeed, for groups of size 1, it can observed that there are networks
for which the solution is diﬀerent, depending on the centrality notion applied
(e.g. Table 13). On the other hand, for groups of size 4, it can be observed that
in most networks there is a common set of nodes selected by all four diﬀerent
measures used.
Non-ambiguity of group random-walk. As stated previously, for a given size k,
more than one optimal solution may exist for group centrality. Moreover, from
the results it can be inferred that as k increases, so does the number of optimal
candidates for certain particular measures. For example, Table 11 shows that
degree and closeness centrality have 24 (resp. 6) optimal solutions of size 4. There
is even an extreme case in Table 23, where degree and closeness have more than
400 diﬀerent optimal group solutions. In a small sample of 40 nodes this variance
does not help achieve the proposed goal. However, betweenness centrality seems
to behave in general better in this matter (there is some dispersion, e.g. Table
14, but it is small). Random-walk centrality, on the other hand, is consistently
better in this regard. In our sample selections, it is the only notion for which
there is systematically a unique optimal solution for any size, and for all the
networks.

4 Theoretical aspects of Group Centrality

As we saw in previous sections, group centrality oﬀers interesting new insights
on semantic networks. In this section we study the notion from a theoretical
point of view, focusing in the complexity of computing it.

4.1 Complexity of Group Centrality

Let us deﬁne formally the problem of computing the most central set of nodes
in a network under diﬀerent centrality measures.

4 The complete results, graphs and labels of the networks can be found in the appendix.


Deﬁnition 3 (Optimumset). Given a graph G = (V, E), a positive integer k
and a function C : P(V ) → R, deﬁne optimumset(G, k, C) as the problem of
ﬁnding a set S ⊂ V , of size k such that

S ∈ arg min
U⊂V ;|U|=k

C(S).

For a real number α > 0, optimumset(G, k, C, α) will denote the corresponding
decision problem: ﬁnd if there exists a set S ⊂ V , of size k such that C(S) = α.

Theorem 2. optimumset(G, k, C, α) is NP-complete for all four group centrality 
measures studied in this paper (that is, for C ∈ {Cd, Cc, Cbc, h(cid:46)}).

Proof. First note that it is not diﬃcult to prove that Cd, Cc and Cbc are all computable 
in polynomial time. Same for h(cid:46) but we will leave that demonstration
for later (see proposition 4). Thus the corresponding problems are all in NP.

Now, consider α = 1 and let Sd, Sc, Sbc and Sh(cid:46) be solutions of the corresponding 
decision problems.

Note that Cc(Sc) = 1 (resp. Cd(Sd) = 1) if and only if Sc (resp. Sd) is a
dominating set of cardinality k in G. That is, the problem of ﬁnding a dominating
set of cardinality k in G is polynomial time reducible to optimumset(G, k, α, Cc)
and optimumset(G, k, α, Cd).
Indeed, Cc(Sc) = 1 if and only if d(v, Sc) = 1,∀v /∈ Sc, which happens if
and only if every vertex v /∈ Sc has at least one neighbor on Sc (which is also a
necessary and suﬃcient condition for Cd(Sd) = 1). That is, Sc (and also Sd) is
a dominating set.

On the other hand, Cbc(Sbc) = 1 (resp. Ch(cid:46) (Sh(cid:46) ) = 1) if and only if Sbc
(resp. Sh(cid:46) ) is a vertex cover of G of size k. That is, the problem of ﬁnding a vertex
cover of G of size k is polynomial time reducible to optimumset(G, k, α, Ch(cid:46) )
and optimumset(G, k, α, Cbc).
Indeed, Cbc(Sbc) = 1 if and only if σuv(Sbc) = σuv,∀u, v /∈ Sbc with u < v,
which occurs if and only if for every pair ∀u, v /∈ Sbc with u < v, all shortest
paths that connect u and v have some vertex in Sbc. This is equivalent to having
N (v) ⊆ Sbc for all v ∈ Sbc. That is, Sbc is a vertex cover of G.
Finally, Ch(cid:46) (Sh(cid:46)) = 1 if and only if h(v, Sh(cid:46)) = 1,∀v /∈ Sh(cid:46) , but h(v, Sh(cid:46)) =
1 occurs if and only if N (v) ⊂ S,∀v /∈ Sh(cid:46) . That is, Sh(cid:46) is a vertex cover of G.(cid:117)(cid:116)

4.2 Properties of Random Walk Centrality

the transition probabilities for puv = ω(uv)/ω(u), with ω(u) =(cid:80)

Given a graph G = (V, E), we can generalize the notion of random walk to the
case where there is a weight function ω : E → R on the edges of G, by changing
w∈N (u) ω(uw)
(thus the case ω ≡ 1 gives the original deﬁnition of random walk).
Note that the sequence of random vertices (vt : t = 0, 1, ...) is a Markov
chain. Let Pt will denote the distribution of vt: Pt(v) = P(vt = v). The vertex
v0 may be ﬁxed, but may also be drawn from an initial distribution P0. This

easily proved that the distribution π(v) := ω(v)/(cid:80)

initial distribution is said to be stationary if P1 = P0 (which will imply that
Pt = P0 ∀t ≥ 0, because of the construction of the random walk). It can be
v∈V ω(w) is stationary for
every graph G and weight function ω. From now on π will be referred simply as
the stationary distribution (it is not diﬃcult to prove that this distribution is
unique, which makes this reference valid).
Proposition 4. Given G = (V, E), a weight function ω : E → R, a subset
S ⊂ V and v ∈ V , then h(v, S) and h(cid:46)(S) can be computed in polynomial time.

For the proof, ﬁrst we need the following result

Lemma 5. For a graph G = (V, E), a weight function ω and S ⊆ V deﬁne GS
as the following weighted graph:

V (GS) = (V \ S) ∪ {vS}
E(GS) = {e ∈ E/e ∩ S = φ} ∪ {e = uvS/u ∈ V, NG(u) ∩ S (cid:54)= φ}

and ωS : E(GS) −→ N deﬁned as: ωS(uv) = ω(uv) if uv ∩ S = ∅. Otherwise,

i.e. if v = vS, then ωS(uv) = (cid:80)

ω(uw).

Then for u ∈ V \ S we have that HG(u, S) = HGS (u, vS).

w∈NG(u)∩S

Proof. For a vertex w ∈ V \ S, let Tw,S and Tw,vS be two random variables
that count the number of steps that a random walk (on G and GS respectively)
starting at vertex w takes in order to reach some vertex of S and vS (respectively)
for the ﬁrst time. We will prove that Tu,S and Tu,vS have the same distribution.

Let us proceed by induction on the possible values of Tu,S and Tu,vS .
If n = 1 then

(cid:80)

P(Tu,S = 1) =

w∈NG(u)∩S
ω(u)

=

ωS(uvS)
ωS(u)

= P(Tu,vS = 1).

ω(uw)

Let n > 1 and assume that the property holds for n − 1. Then

ω(uw)
ω(u)

ω(uw)
ω(u)

P(Tw,S = n − 1)

P(Tw,vS = n − 1)

P(Tu,S = n) =

=

=

=

w∈V \S

(cid:88)
(cid:88)
(cid:88)
(cid:88)

w∈V \S

w∈V (GS )\vS

w∈V (GS )\vS

ω(uw)
ω(u)

(Tw,vS = n − 1)

ωS(uw)
ωS(u)

P(Tw,vS = n − 1) = P(Tu,vS = n).

Using this we have that HG(u, S) = E(Tu,S) = E(Tu,vS ) = HGS (u, vS).

(cid:117)(cid:116)

Proof (of proposition 5). Let P be the probability transition matrix associated
to a random walk on G, and the function ω. That is, Pij = ω(ij)
ω(i) . Deﬁne P∞ as

 .
π(1) π(2) . . . π(n)

...
...
π(1) π(2) . . . π(n)

. . .

...

P∞ =

where π is the stationary distribution. Deﬁne also Z := (I − (P − P∞))−1 and
let H be the hitting time matrix of G, that is, Hij = HG(i, j). Then it can be
proved as stated in [17] that for i, j ∈ V

HG(i, j) =

Zjj − Zij

π(j)

.

(1)

Therefore, by combining equation (1) and lemma 5 we can compute h(v, S)
(cid:117)(cid:116)

by inverting a suitable matrix Z, which can be made in polynomial time.

Lemma 6. There is a parameterized family of graphs for which Random walk
centrality behaves systematically diﬀerently than degree, closeness and betweenness 
group centrality.
Proof. Consider {Gm
n }, with n, m ∈ Z+ and n << m a family of graphs constructed 
as follows (see Fig. 2): for i ∈ {1, ..., m − 1}, K i
n is a subgraph of Gm
n
that is a n-clique and that is connected to v through an unique vertex ki
n. On
n of height 1 and size n with root tn
the other hand, T i
i the
only vertex in T i
n . Suppose we want to ﬁnd a set
n , m + 1, C) for C ∈ {Cd, Cc, Cbc, h(cid:46)}. It is not
S that solves optimumset(Gm
diﬃcult to prove that for group closeness and betweenness centrality the sets
n } are solutions. For group degree
SK = {v, k1
n, ..., km
n } for any 1 ≤ j ≤ m
centrality ¯SK = {tj
are possible solutions. However, one could argue the diﬀerence in connectivity
between sets SK and ST should be accounted for. Indeed, for random walk cen-
(cid:117)(cid:116)
trality, Sk (a more connected version of St) is the only solution.

n } and ST = {v, t1
n, k1

n that is neighbor of v in Gm

n } and ¯ST = {kj

n is a subtree of Gm

n, t1

n, ..., tm

n, ..., km

n, ..., tm

The following bound shows an interesting relation between random walk

centrality, degree and closeness
Proposition 7. There exists a real constant c > 0 such that for S ⊂ V

 (cid:88)

v∈V \S

 ≤ c|V \ S|3.

d(u, S)

d(v)

 (cid:88)

u∈V \S

h(cid:46)(S) ≤

1

|V \ S|

Proof. Deﬁne ∂(S) := {v ∈ S : ∃u ∈ V \ S, uv ∈ E}. Deﬁne the subgraph GS
of G as follows: V (GS) := (V \ S) ∪ ∂(S) and E(GS) := {uv ∈ E : u, v ∈
V (GS) ∧ u ∈ V \ S}.

First note that for any pair of vertices u ∈ V \ S and v ∈ ∂(S)

HG(u, S) = HGS (u, ∂(S)) ≤ HGS (u, v).

K 1
n

K 2
n

. . .

Km−1

n

Km
n

v

. . .

T 1
n

T 2
n

T m−1

n

T m
n

Fig. 2: Which set of size m + 1 is more central? SK or ST ?

The equality follows from the fact that the subgraph traversed and the target
hit in both expressions are the same, and the inequality holds because the target
of the expression on the right is more diﬃcult to hit, thus needs more walking
around.

Using this, we have that

(cid:80)

(cid:80)

h(cid:46)(S) =

u∈V \S HG(u, S)

|V \ S|

≤

u∈V \S HGS (u, vu)

|V \ S|

,

with each vu ∈ ∂(S) chosen such that dGS (u, vu) = dG(u, S).
pair of vertices u, v ∈ V (L), HL(u, v) ≤ 2|E(L)|dL(u, v), we have that

On the other hand, by using the known fact that for any graph L and any

(cid:80)
u∈V \S 2|E(GS)|dGS (u, vu)

(cid:80)

u∈V \S HGS (u, v)

|V \ S|

≤

≤

1

|V \ S|

|V \ S|

 (cid:88)

u∈V \S

dG(u, S)

 (cid:88)

 . (cid:117)(cid:116)

dG(v)

v∈V \S

5 Conclusions

We perform a SWOT analysis of the application of the notion of group centrality
to semantic networks, putting a particular focus on random walk group centrality.
 The challenges faced were mainly of two types. First, the ambiguity of the
very notion of the “most central” concept in a semantic network; and second,
the size of the networks.

In spite of these obstacles (that we discussed in the paper), we hope we
presented enough evidence that this topic deserves to be explored and researched.
We approached these obstacles by working with small samples and advancing
theoretical results. Besides stating the problem at the formal level, we proved
that the problem of computing group centrality is NP-hard, and showed that

random walk centrality comes up as a good candidate for capturing the notion
of centrality on semantic networks.

The above opens at least two lines of research on this topic: First, to ﬁnd good
approximation algorithms to compute group centrality particularly for random
walk centrality. Second, the task of building a good benchmark of big semantic 
networks together with their candidate sets to match the most important
concepts in the network.

References

1. DBpedia. http://www.dbpedia.org.
2. J. Abbott, J. Austerweil, T. Griﬃths. Human memory search as a random walk

in a semantic network. Adv. Neur. Inf. Proc. Syst. 25, 3050-58, 2012.

3. G. Cheng, Th. Tran, Y. Qu. RELIN: Relatedness and Informativeness-based Centrality 
for Entity Summarization. ISWC 2011, Springer 2011.

4. L. Ding, R. Pan, T. Finin, A. Joshi, Y. Peng, P. Kolari. Finding and ranking

knowledge on the semantic web. In ISWC 2005, 156–170. Springer, 2005.

5. M. G. Everett, S. P. Borgatti. The centrality of groups and classes. Journal of

Mathematical Sociology. 23(3): 181-201.

6. L. C. Freeman. Centrality in Social Networks. Conceptual Clariﬁcation. Social

Networks, 1 (1978/79) 215-239.

7. A. Graves, S. Adali. A method to rank nodes in an rdf graph. 7th. ISWC, 2008.
8. T. Hughes, D. Ramage. Lexical semantic relatedness with random graph walks.
2007 J. Conf. on Emp. Meth. in NLP and Comp. Nat. Lang. Learning, 581-589.
9. Jure Leskovec and Christos Faloutsos. Sampling from large graphs. Proc. 12th

ACM KDD ’06, pages 631–636.

10. L. Lov´asz Random Walks on Graphs; A Survey. Bolyai Soc., Math. Studies 2,

1993, pp. 1-46.

11. Mark EJ Newman. The structure and function of complex networks. SIAM review,

45(2):167–256, 2003.

12. K. Supekar, Ch. Patel, Y. Lee. Characterizing quality of knowledge on semantic

web. AAAI Florida AI Research Symp. (FLAIRS-2004), 17–19, 2004.

13. E. Yeh, D. Ramage, Ch. D. Manning, E. Agirre, A. Soroa. Wikiwalk: random walks
on wikipedia for semantic relatedness. 2009 Workshop on Graph-based Meth. for
NLP, 41–49, Stroudsburg, PA, USA, 2009.

14. X. Zhang, G. Cheng, Y. Qu. Ontology summarization based on rdf sentence graph.

16th Intl. Conf. on World Wide Web, 707–716. ACM, 2007.

15. R. Schvaneveldt, F. Durso, and D. Dearholt. Network structures in proximity data.

The psychology of learning and motivation, 1989, pages 249–284.

16. T. De Smedt, F. De Bleser, V. Van Asch, L. Nijs, W. Daelemans. Gravital: natural
language processing for computer graphics. Creativity and the Agile Mind: A MultiDisciplinary 
Study of a Multi-Faceted Phenomenon, 2013, pages 81

17. Howard Levinson An Eigenvalue Representation for Random Walk Hitting Times

and its Application to the Rook Graph.

18. S. Wasserman, K. Faust. Social Netowrk Analysis: Methods and Applications

(Structural Analysis in the Social Sciences). Cambridge Univ. Press, 1994.

A Appendix

Fig. 3: Proximity Semantic Network 1.

Proximity Semantic Network 1 Labels:

– 0.- animal
– 1.- antlers
– 2.- bat
– 3.- bird
– 4.- blood
– 5.- chicken
– 6.- color
– 7.- cottonwood
– 8.- daisy
– 9.- deer
– 10.- dog
– 11.- feathers
– 12.- ﬂower

– 13.- frog
– 14.- green
– 15.- hair
– 16.- hooves
– 17.- leaves
– 18.- livingthing
– 19.- mammal
– 20.- plant
– 21.- red
– 22.- robin
– 23.- rose
– 24.- tree

degree
{11}
{4, 11}

n
1
{2, 13} , {4, 11}
2
{0, 11, 13} , {4, 11,
3
4 {2, 4, 11, 13} , ... (23) {2, 4, 11, 13} , ... (5)]

{4, 11, 13} , {0, 11,

13} , ... (9)

13} , ... (4)

closeness

{11}

betweenness

{11}
{4, 11}

{4, 11, 13}
{2, 4, 11, 13}

random-walk

{11}
{4, 11}

{0, 11, 13}
{2, 4, 11, 13}

Table 11: Solutions for group centrality on Proximity Semantic Network 1.

Fig. 4: Proximity Semantic Network 2.

Proximity Semantic Network 2 Labels:

– 0.- animal
– 1.- antlers
– 2.- bat
– 3.- bird
– 4.- blood

– 5.- chicken
– 6.- color
– 7.- cottonwood
– 8.- daisy
– 9.- deer

– 10.- dog
– 11.- feathers
– 12.- ﬂower
– 13.- frog
– 14.- green
– 15.- hair
– 16.- hooves
– 17.- leaves

– 18.- livingthing
– 19.- mammal
– 20.- plant
– 21.- red
– 22.- robin
– 23.- rose
– 24.- tree

degree
{11}
{0, 11}

closeness

{11}

n
1
2
3
4 {2, 4, 13, 21} , ... (16)
Table 12: Solutions for group centrality on Proximity Semantic Network 2.

{2, 11} , ... (2)
{2, 13, 21}
{2, 4, 13, 21}

{2, 13, 21}
{2, 4, 13, 21}

{2, 11, 13}

{2, 11, 13, 21}

{2, 13, 21} , ... (8)

betweenness

{11}
{2, 11}

random-walk

{11}
{2, 11}

Fig. 5: Proximity network built by non-specialists.

Proximity network built by non-specialists labels:

– 0.- animal
– 1.- antlers
– 2.- bat
– 3.- bird
– 4.- blood
– 5.- chicken
– 6.- color
– 7.- cottonwood
– 8.- daisy
– 9.- deer
– 10.- dog
– 11.- feathers
– 12.- ﬂower

– 13.- frog
– 14.- green
– 15.- hair
– 16.- hooves
– 17.- leaves
– 18.- livingthing
– 19.- mammal
– 20.- plant
– 21.- red
– 22.- robin
– 23.- rose
– 24.- tree

closeness

{6}

{3, 13, 17} , {3, 10,

17}, ... (10)

{6} , {17}

degree
{0, 6}

n
1
2
3
4 {0, 6, 10, 17} , ... (18) {0, 6, 10, 17} , ... (6)
Table 13: Solutions for group centrality on Proximity Semantic Network constructed 
by non-biologist students.

{3, 10, 17}
{0, 6, 10, 17}

{0, 13, 17}
{0, 6, 10, 17}

{0, 6} , {10, 17}

{3, 10, 17}

betweenness

{17}
{0, 6}

random-walk

{6}
{0, 6}

Fig. 6: Proximity network built by biologists.

Proximity network built by biologists labels:

– 0.- animal
– 1.- antlers
– 2.- bat
– 3.- bird
– 4.- blood
– 5.- chicken
– 6.- color
– 7.- cottonwood
– 8.- daisy
– 9.- deer
– 10.- dog
– 11.- feathers
– 12.- ﬂower

– 13.- frog
– 14.- green
– 15.- hair
– 16.- hooves
– 17.- leaves
– 18.- livingthing
– 19.- mammal
– 20.- plant
– 21.- red
– 22.- robin
– 23.- rose
– 24.- tree

n
1
2
3 {3, 12, 19}, {3, 12, 21} {3, 12, 21}, {3, 12, 22}

closeness
{12}
{3, 12}

degree
{12}
{3, 12}

betweenness

{12}
{3, 12}

random-walk

{12}
{3, 12}

4

{3, 12, 19, 23}

{3, 12, 22}, {3, 12, 23}

{3, 12, 22}
{3, 12, 15, 19}, {3, 12, 15, 21} {3, 5, 12, 22}
Table 14: Solutions for group centrality on Proximity Semantic Network constructed 
by biologist students.

{3, 12, 19}, {3, 12, 21}

{3, 12, 19, 23}

{3, 12, 23}

Fig. 7: Sample 1 from Perception Network.

Sample 1 from Perception network labels:

– 0.- Akzidenz-Grotesk
– 1.- Christmas
– 2.- December
– 3.- Frutiger
– 4.- Helvetica
– 5.- Orient Express
– 6.- Trade Gothic
– 7.- amusement park
– 8.- carousel
– 9.- chameleon
– 10.- colorful
– 11.- consciousness
– 12.- deep
– 13.- diverse
– 14.- exotic
– 15.- grotesque
– 16.- heat
– 17.- hot
– 18.- interesting
– 19.- locomotive

– 20.- loud
– 21.- merry-go-round
– 22.- mill
– 23.- miracle
– 24.- mirror
– 25.- mysterious
– 26.- narrow
– 27.- repetitive
– 28.- roller coaster
– 29.- sans
– 30.- sans serif
– 31.- season
– 32.- sensational
– 33.- steam
– 34.- summer
– 35.- telescope
– 36.- turkey
– 37.- uniform
– 38.- waltz
– 39.- winter

degree
{25}
{14, 25}

n
1
2
3
4 {0, 4, 14, 25} , ... (10)

{4, 14, 25} , ... (5)

closeness

{14}

{5, 14} , {7, 13} , {7, 14} , {8, 13}

{4, 14, 25} , ... (4)
{1, 4, 14, 25} , ... (2)

{14}

{14, 25}
{4, 14, 25}

{13}
{6, 14}
{0, 6, 14}

betweenness

random-walk

{4, 12, 14, 25} {0, 4, 14, 25}

Table 15: Solutions for group centrality on Perception sample 1.

Fig. 8: Sample 2 from Perception Network.

Sample 2 from Perception network labels:

– 0.- New Age
– 1.- anger
– 2.- astrology
– 3.- baby
– 4.- bad
– 5.- calm
– 6.- cathedral
– 7.- child
– 8.- cirrus
– 9.- cold
– 10.- comedy
– 11.- competition
– 12.- dangerous
– 13.- dark
– 14.- deep
– 15.- evolution

– 16.- food
– 17.- fuzzy
– 18.- gene
– 19.- joke
– 20.- kid
– 21.- laugh
– 22.- light
– 23.- loiter
– 24.- negative
– 25.- old
– 26.- ominous
– 27.- park
– 28.- pillar
– 29.- playground
– 30.- reproduction
– 31.- safe

– 32.- shop
– 33.- silent
– 34.- sleep
– 35.- sly

– 36.- supermarket
– 37.- territory
– 38.- valley
– 39.- wicked

{29} , ... (1)

degree
{2, 29}

n
1
2
3
4 {2, 8, 29, 39} , ... (2) {2, 26, 29, 39} , ... (1) {2, 8, 10, 29} {2, 26, 29, 39}
Table 16: Solutions for group centrality on Perception sample 2.

{1}
{1, 2}
{1, 2, 8}

{0, 15, 32} , ... (7)

{2, 8, 29} , ... (3)

betweenness

random-walk

{29}

{29, 39}
{2, 8, 39}

closeness

{15}

{0, 15} , ... (1)

Sample 3 from Perception network labels:

– 0.- Duomo di Milano
– 1.- Middle Ages
– 2.- Milan Cathedral
– 3.- arch
– 4.- black
– 5.- boot
– 6.- calm
– 7.- cathedral
– 8.- dark
– 9.- grassland

– 10.- horse
– 11.- landscape
– 12.- marble
– 13.- mosque
– 14.- pillar
– 15.- prairie
– 16.- smooth
– 17.- soldier
– 18.- stallion
– 19.- veldt

Fig. 9: Sample 3 from Perception Network.

degree
{11}
{4, 11}

n
1
2
3 {0, 11, 13} , {4, 11, 13} , ... (9) {0, 11, 13} , {4, 11, 13} , ... (4) {4, 11, 13}
4

{2, 4, 11, 13} , ... (23)
Table 17: Solutions for group centrality on Perception sample 3.

{2, 4, 11, 13} , ... (5)

{2, 13} , {4, 11}

closeness

{11}

betweenness

random-walk

{11}
{4, 11}

{11}
{4, 11}

{0, 11, 13}
{2, 4, 11, 13} {2, 4, 11, 13}

Fig. 10: Sample 1 from DBpedia Categories network.

Sample 1 from DBpedia Categories network labels:

– 0.- 1996 ﬁlms
– 1.- 1990s ﬁlms
– 2.- 2000s in animation
– 3.- Decades in animation
– 4.- 1990s in animation
– 5.- 1910s in animation
– 6.- 1990s animated ﬁlms
– 7.- Years in animation
– 8.- 1964 anime
– 9.- 1965 anime
– 10.- 1969 anime
– 11.- 1990 anime
– 12.- 1990 in animation
– 13.- 1993 anime
– 14.- 1993 in animation
– 15.- 1994 anime
– 16.- 1994 in animation
– 17.- 1996 anime
– 18.- 1996 in animation
– 19.- 1998 in animation

– 20.- Anime ﬁlms by year
– 21.- 1990 anime ﬁlms
– 22.- 1990s anime ﬁlms
– 23.- 1996 anime ﬁlms
– 24.- 1981 anime ﬁlms
– 25.- 1960s anime ﬁlms
– 26.- 1998 animated ﬁlms
– 27.- 1996 animated ﬁlms
– 28.- 1994 animated ﬁlms
– 29.- 1993 animated ﬁlms
– 30.- 1990 animated ﬁlms
– 31.- 1965 anime ﬁlms
– 32.- 1969 anime ﬁlms
– 33.- 1945 anime ﬁlms
– 34.- 1945 anime
– 35.- 1990s American animated ﬁlms
– 36.- 1990s animated short ﬁlms
– 37.- Tamil ﬁlms of 1996
– 38.- 1964 anime ﬁlms
– 39.- 1990s computer-animated ﬁlms

degree
{4} , {20}
{4, 20}

n
1
2
{4, 18, 20}
3
4 {4, 6, 18, 20} , {0, 4, 6, 20} {0, 4, 6, 20} {4, 6, 7, 20} {4, 6, 18, 20}

closeness
{4, 20}
{0, 4, 20}

{20}
{4, 20}
{4, 7, 20}

{4, 6, 20} , {6, 7, 20}

{4}

betweenness

random-walk

{4}

{4, 20}

Table 18: Solutions for group centrality on DBpedia sample 1.

Fig. 11: Sample 2 from DBpedia Categories network.

Sample 2 from DBpedia Categories network labels:

– 0.- Wikipedia categories named after populated places in Italy
– 1.- Provinces of Italy
– 2.- Cities and towns in Emilia-Romagna
– 3.- Emilia-Romagna
– 4.- Parma
– 5.- Turin
– 6.- Cities and towns in Piedmont
– 7.- Communes of the Province of Parma
– 8.- Education in Italy by city
– 9.- Communes of the Province of Alessandria
– 10.- Geography of Emilia-Romagna
– 11.- Province of Pavia geography stubs
– 12.- Province of Pavia
– 13.- Bologna
– 14.- Communes of the Province of Bologna
– 15.- Buildings and structures in Bologna
– 16.- Province of Arezzo
– 17.- Province of Ancona
– 18.- Province of Lucca
– 19.- Visitor attractions in Bologna
– 20.- Tortona
– 21.- Novi Ligure
– 22.- Pavia
– 23.- Communes of the Province of Lucca
– 24.- Roman Catholic archbishops of Bologna
– 25.- History of Bologna
– 26.- Education in Turin
– 27.- Palaces in Bologna
– 28.- Education in Bologna
– 29.- University of Pavia
– 30.- Education in Emilia-Romagna
– 31.- Culture in Bologna
– 32.- Culture in Emilia-Romagna
– 33.- Buildings and structures in Pavia
– 34.- Buildings and structures in the Province of Pavia
– 35.- Companies by region of Italy
– 36.- Companies based in Bologna
– 37.- Companies based in Emilia-Romagna
– 38.- Education in Italy by region
– 39.- Media in Bologna

degree
{13}

n
1
2 {0, 13} , {1, 13} , ... (2)
3
4

{0, 1, 13} , ... (4)
{0, 1, 3, 13} , ... (1)

betweenness random-walk

{13}

closeness
{13}
{12, 13}
{1, 13}
{0, 1, 13}
{0, 1, 13}
{0, 1, 3, 13} {0, 1, 3, 13} {0, 1, 3, 13}

{13}
{0, 13}
{0, 1, 13}

Table 19: Solutions for group centrality on DBpedia sample 2.

Fig. 12: Sample 3 from DBpedia Categories network.

Sample 3 from DBpedia Categories network labels:

– 0.- Neognathae
– 1.- Birds by classiﬁcation
– 2.- Procellariiformes
– 3.- Phoenicopteriformes
– 4.- Pelecaniformes
– 5.- Hornbills
– 6.- Bucerotidae
– 7.- Ardeidae
– 8.- Egretta
– 9.- Aceros
– 10.- Anthracoceros

– 11.- Buceros
– 12.- Ceratogymna
– 13.- Ocyceros
– 14.- Penelopides
– 15.- Bucerotinae
– 16.- Tockus
– 17.- Phalacrocoracidae
– 18.- Fregatidae
– 19.- Falconiformes
– 20.- Accipitridae
– 21.- Oceanodroma

– 22.- Hydrobatinae
– 23.- Hydrobatidae
– 24.- Phalacrocorax
– 25.- Plotopteridae
– 26.- Bucerotiformes
– 27.- Limnofregata
– 28.- Fregata
– 29.- Oceanitinae
– 30.- Aviceda

– 31.- Dryotriorchis
– 32.- Erythrotriorchis
– 33.- Anorrhinus
– 34.- Fregetta
– 35.- Oceanites
– 36.- Botaurus
– 37.- Anhingidae
– 38.- Rhyticeros
– 39.- Pelecanus

degree
{5}
{4, 5}

n
1
2
3
4 {4, 5, 20, 23} {4, 5, 20, 23} {4, 5, 20, 23} {4, 5, 20, 23}

closeness
{1}
{1, 5}

betweenness

random-walk

{1}
{1, 5}

{4, 5, 23}

{1}
{4, 5}

{4, 5, 23}

{4, 5, 23}

{4, 5, 23}

Table 20: Solutions for group centrality on DBpedia sample 3.

Fig. 13: Sample 1 from Roget’s Thesaurus network.

Sample 1 from Roget’s Thesaurus network labels:

– 0.- Jack
– 1.- ardor
– 2.- chips
– 3.- cob
– 4.- cordiality
– 5.- dress down
– 6.- empressement
– 7.- fervency
– 8.- fervidness
– 9.- fervor
– 10.- ﬁre
– 11.- foment
– 12.- give a whipping
– 13.- go
– 14.- gusto
– 15.- heartiness
– 16.- hearty
– 17.- heat
– 18.- jacktar
– 19.- kale

– 20.- lather
– 21.- mariner
– 22.- navigator
– 23.- passionateness
– 24.- sailorman
– 25.- salt
– 26.- sea dog
– 27.- seafarer
– 28.- seafaring man
– 29.- spondulix
– 30.- sugar
– 31.- trim
– 32.- unction
– 33.- vehemence
– 34.- verve
– 35.- warm
– 36.- warm the blood
– 37.- warmth
– 38.- warmth of feeling
– 39.- whale

{10, 25, 39}

{35}

{10}

degree
{10}
{10, 25}

n
1
2
3
4 {10, 17, 25, 39} , ... (87) {10, 17, 25, 39} , ... (87) {10, 17, 25, 35} {10, 17, 25, 39}
Table 21: Solutions for group centrality on Roget’s Thesaurus sample 1.

closeness
{10, 25}

{25, 35}

{10, 25, 35}

{10, 25}

{10, 25, 39}

betweenness

random-walk

{35}

{10, 25, 39}

Fig. 14: Sample 2 from Roget’s Thesaurus network.

Sample 2 from Roget’s Thesaurus network labels:

– 0.- alliterate
– 1.- assonate
– 2.- bedpan
– 3.- blench
– 4.- blink
– 5.- chevy
– 6.- chuck
– 7.- chunk
– 8.- cracked
– 9.- cut
– 10.- dash
– 11.- dodge
– 12.- duck
– 13.- ﬁght shy of
– 14.- ﬂing
– 15.- hoar
– 16.- lay
– 17.- pelt
– 18.- poesy
– 19.- poetry

– 20.- pound
– 21.- pull back
– 22.- pun
– 23.- recoil
– 24.- reft
– 25.- rhyme
– 26.- rime
– 27.- scour
– 28.- shrink
– 29.- shy
– 30.- shy at
– 31.- side step
– 32.- slit
– 33.- song
– 34.- stab
– 35.- submarine
– 36.- swerve
– 37.- verse
– 38.- whack
– 39.- whop

degree
{29}
{26, 29}

n
1
2
3
4 {10, 12, 26, 32} , ... (4) {10, 12, 24, 26} , ... (4) {10, 12, 26, 29} {10, 12, 26, 29}

closeness
{26, 29}

{26, 29}

{10, 12, 26}

{9, 29}

{9, 26, 29}

betweenness

random-walk

{10, 12, 26}

{10, 12, 26}

{9}

{9}

{29}

Table 22: Solutions for group centrality on Roget’s Thesaurus sample 2.

Fig. 15: Sample 3 from Roget’s Thesaurus network.

Sample 3 from Roget’s Thesaurus network labels:

– 0.- ass
– 1.- bring about
– 2.- bring to eﬀect
– 3.- bring to pass
– 4.- burro
– 5.- chucklehead
– 6.- create
– 7.- cuddy
– 8.- dickey
– 9.- do
– 10.- doit
– 11.- donkey
– 12.- dub
– 13.- duﬀer
– 14.- eﬀect
– 15.- gawk
– 16.- gawkhammer
– 17.- gawky
– 18.- generate
– 19.- get

– 20.- lobster
– 21.- longear
– 22.- looby
– 23.- loon
– 24.- lubber
– 25.- lummox
– 26.- lump
– 27.- lunkhead
– 28.- majority
– 29.- mass
– 30.- moke
– 31.- most
– 32.- plurality
– 33.- preponderance
– 34.- preponderancy
– 35.- put up a tree
– 36.- slouch
– 37.- stick
– 38.- swab
– 39.- the greater number

degree
{11}
{11, 14}

{37}

{37}

n
1
2
3
4 {11, 14, 29, 37} , ... (440) {11, 14, 29, 37} , ... (440) {11, 14, 26, 37} {11, 14, 29, 37}
Table 23: Solutions for group centrality on Roget’s Thesaurus sample 3.

{11, 14, 29} , ... (6)

{1, 14, 29} , ... (6)

{37}

closeness
{14, 26}

{14, 26}

{11, 14, 29}

{26, 37}

{11, 26, 37}

betweenness

random-walk

