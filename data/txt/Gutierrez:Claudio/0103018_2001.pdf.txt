1
0
0
2

 
r
a

 

M
6
2

 
 
]
S
D
.
s
c
[
 
 

1
v
8
1
0
3
0
1
0
/
s
c
:
v
i
X
r
a

The existential theory of equations with

rational constraints in free groups is

PSPACE–complete

Volker Diekert1, Claudio Guti´errez2, Christian Hagenah1

1 Inst. f¨ur Informatik, Universit¨at Stuttgart,

Breitwiesenstr. 20-22, D-70565 Stuttgart

diekert@informatik.uni-stuttgart.de

christian@hagenah.de

2 Depto. de Ciencias de la Computaci´on, Universidad de Chile,

Blanco Encalada 2120, Santiago, Chile

cgutierr@dcc.uchile.cl

ACM Classiﬁcation F.2. Analysis of Algorithms and Problem Complexity,
 F.2.2. Computation on Discrete Structures, F.4. Mathematical Logic
and Formal Languages.
Subject Descriptor Equations in Free Groups.

Abstract

It is known that the existential theory of equations in free groups
is decidable. This is a famous result of Makanin. On the other hand
it has been shown that the scheme of his algorithm is not primitive
recursive. In this paper we present an algorithm that works in polynomial 
space, even in the more general setting where each variable
has a rational constraint, that is, the solution has to respect a speciﬁcation 
given by a regular word language. Our main result states
that the existential theory of equations in free groups with rational
constraints is PSPACE–complete. We obtain this result as a corollary
of the corresponding statement about free monoids with involution.

1

1

Introduction

Around the 1980’s a great progress was achieved on the algorithmic decidability 
of elementary theories of free monoids and groups. In 1977 Makanin
[17] proved that the existential theory of equations in free monoids is decidable 
by presenting an algorithm which solves the satisﬁability problem for
a single word equation with constants.
In 1983 he extended his result to
the more complicated framework in free groups [18]. In fact, using a result
by Merzlyakov [22] he also showed that the positive theory of equations in
free groups is decidable [19], and Razborov was able to give a description of
the whole solution set [28]. The algorithms of Makanin are very complex:
For word equations the running time was ﬁrst estimated by several towers
of exponentials and it took more than 20 years to lower it down to the best
known bound for Makanin’s original algorithm, which is to date EXPSPACE
[9]. For solving equations in free groups Ko´scielski and Pacholski [14] have
shown that the scheme proposed by Makanin is not primitive recursive.
In 1999 Plandowski invented another method for solving word equations and
he showed that the satisﬁability problem for word equations is in PSPACE,
[26]. One ingredient of his work is to use data compression to reduce the
exponential space to polynomial space. The importance of data compression
was ﬁrst recognized by Rytter and Plandowski when applying Lempel-Ziv
encodings to the minimal solution of a word equation [27]. Another important
notion is the deﬁnition of an ℓ-factorization of the solution being explained
below. Guti´errez extended Plandowski’s method to the case of free groups,
[10]. Thus, a non-primitive recursive scheme for solving equations in free
groups has been replaced by a polynomial space bounded algorithm. Hagenah
and Diekert worked independently in the same direction and using some ideas
of Guti´errez they obtained a result which includes the presence of rational
constraints. This appeared as extended abstract in [4] and also as a part of
the PhD-thesis of Hagenah [11].
The present paper is a journal version of [4, 10]. It shows that the existential
theory of equations in free groups with rational constraints is PSPACE–
complete. Rational constraints mean that a possible solution has to respect a
speciﬁcation which is given by a regular word language. The idea to consider
regular constraints for word equations goes back to Schulz [29] who also
pointed out the importance of this concept, see also [6, 8]. The PSPACE–
completeness for the case of word equations with regular constraints has been
stated by Rytter already, as cited in [26, Thm. 1].

2

Our proof reduces the case of equations with rational constraints in free
groups to the case equations with regular constraints in free monoids with
involution, which turns out to be the central object.
(Makanin uses the
notion of “paired alphabet”, but a main diﬀerence is that he considered
“non contractible” solutions only, whereas we deal with general solutions
and, in addition, we have constraints.) During our work we extend the
method of [26] such that it copes with the involution and the method of [10]
such that it copes with rational constraints. The ﬁrst step is a reduction
to the satisﬁability problem of a single equation with regular constraints in
a free monoid with involution.
In order to avoid an exponential blow-up,
we do not use a reduction as in [19], but a simpler one. In particular, we
can handle negations simply by positive rational constraints. In the second
step we show that the satisﬁability problem of a single equation with regular
constraints in a free monoid with involution is still in PSPACE. This part
is rather technical and we introduce several new notions like base-change,
projection, partial solution, and free interval. The careful handling of free
intervals is necessary because of the constraints. In some sense this is the
only additional diﬃculty which we will meet when dealing with constraints.
After these preparations we can follow Plandowski’s method. Throughout
we shall use many of the deep ideas which were presented in [26], and apply
them in a diﬀerent setting. Hence, as we cannot use Plandowski’s result
as a black box, we have to go through the whole construction again. As a
result our paper is (involuntarily) self-contained, up to standard knowledge
in combinatorics on words and linear Diophantine equations.

2 Free Groups and their Rational Subsets

Let Σ be a ﬁnite alphabet. By F (Σ) we denote the free group over Σ.
Elements of F (Σ) can be represented by words in (Σ ∪ Σ)∗, where Σ = { a |
a ∈ Σ }. We read a as a−1 in F (Σ) and we use the convention that a = a.
Hence the set Γ = Σ ∪ Σ is equipped with an involution
: Γ → Γ; the
involution is extended to Γ∗ by a1 · · · an = an · · · a1 for n ≥ 0 and ai ∈ Γ,
1 ≤ i ≤ n. The empty word as well as the unit element in other monoids is
denoted by 1. By ψ : Γ∗ → F (Σ) we denote the canonical homomorphism.
A word w ∈ Γ∗ is freely reduced , if it contains no factor of the form aa
with a ∈ Γ. The reduction of a word w ∈ Γ∗ can be computed by using the
Noetherian and conﬂuent rewriting system { aa → 1 | a ∈ Γ }. For w ∈ Γ∗ we

3

denote by bw the freely reduced word which denotes the same group element
in F (Σ) as w. Hence, ψ(u) = ψ(v) if and only if bu =bv in Γ∗.

The class of rational languages in F (Σ) is inductively deﬁned as follows:
Every ﬁnite subset of F (Σ) is rational. If P1, P2 ⊆ F (Σ) are rational, then
P1 ∪ P2, P1 · P2, and P ∗
1 are rational. Hence, P ⊆ F (Σ) is rational if and
only if P = ψ(P ′) for some regular language P ′ ⊆ Γ∗.1 In particular, we can
use a non-deterministic ﬁnite automata over Γ for specifying rational group
languages over F (Σ).
The following proposition is due to M. Benois [1], see also [2, Sect. III. 2].

Proposition 1 Let P ′ ⊆ Γ∗ be a regular language and P = ψ(P ′) ⊆ F (Σ).

Γ∗ | ψ(w) 6∈ P }. Hence, the complement of P is the rational group language

Then we eﬀectively ﬁnd a regular language eP ′ ⊆ Γ∗ such that eP ′ = { bw ∈
ψ(eP ′) and the family of rational group languages is an eﬀective Boolean algebra.


Proof.
(Sketch) Using the same state set (and some additional transitions
which are labeled with the empty word) we can construct (in polynomial
time) a ﬁnite automaton which accepts the following language

P ′′ = { v ∈ Γ∗ | ∃u ∈ P ′ : u

∗→ v }

∗→ v means that v is a descendant of u by the convergent rewriting
where u
system { aa → 1 | a ∈ Γ }. Then we complement P ′′ with respect to Γ∗; and
we build the intersection with the regular set of freely reduced words.
(cid:3)

3 The Existential Theory

In the following Ω denotes a ﬁnite set of variables (or unknowns) and we let
: Ω → Ω be an involution without ﬁxed points. Clearly, if X ∈ Ω has an

interpretation in F (Σ), then we read X as X −1 ∈ F (Σ).
The existential theory of equations with rational constraints in free groups is
inductively deﬁned as follows. Atomic formulae are either of the form W = 1,
where W ∈ (Γ ∪ Ω)∗ or of the form X ∈ P , where X is in Ω and P ⊆ F (Σ) is

1We follow the usual convention to call a rational subset of a free monoid regular. This
convention is due to Kleene’s Theorem stating that regular, rational, and recognizable
have the same meaning in free monoids. But in free groups these notions are diﬀerent and
we have to be more precise.

4

a rational language. A propositional formula is build up by atomic formulae
using negations, conjunctions and disjunctions. The existential theory refers
to closed existentially quantiﬁed propositional formulae which evaluate to
true over F (Σ).

Theorem 2 The following problem is PSPACE–complete.
INPUT: A closed existentially quantiﬁed propositional formula with rational
constraints in the free group F (Σ) for some ﬁnite alphabet Σ.
QUESTION: Does the formula evaluate to true over F (Σ)?

The PSPACE–hardness follows from a result of Kozen [15], since (due to
the constraints) the empty intersection problem of regular sets can easily be
encoded in the problem above. The same argument applies to Theorems 4
and 5 below and therefore the PSPACE–hardness is not discussed further in
the sequel: We have to show the inclusion in PSPACE, only.
The PSPACE algorithm for solving Theorem 2 will be described by a (highly)
non-deterministic procedure. We will make sure that if the input evaluates to
true, then at least one possible output is true. If it evaluates to false, then no
(positive) output is possible. By standard methods (Savitch’s Theorem) such
a procedure can be transformed into a polynomial space bounded deterministic 
decision procedure, see any textbook on complexity theory, e.g. [12, 23].
We start the procedure as follows. Using the rules of DeMorgan we may
assume that there are no negations at all, but the atomic formulae are now
of the either form: W = 1, W 6= 1, X ∈ P , X 6∈ P with W ∈ (Γ ∪ Ω)∗,
X ∈ Ω, and P ⊆ F (Σ) rational.2
The next step is to replace every formula W 6= 1 by

∃X : W X = 1 ∧ X 6∈ {1},

where X is a fresh variable, hence we can put ∃X to the front. Now we
eliminate all disjunctions. More precisely, every subformula of type A ∨ B
is non-deterministically replaced either by A or by B. At this stage the
propositional formula has become a conjunction of formulae of type W = 1,
X ∈ P , X 6∈ P with W ∈ (Γ ∪ Ω)∗, X ∈ Ω, and P ⊆ F (Σ) rational.
We may assume that |W | ≥ 3, since if 1 ≤ |W | < 3, then we may replace
W = 1 by W aa = 1 for some a ∈ Γ. For the following it is convenient to

2The reason that we keep X 6∈ P instead of X ∈ eP where eP = F (Σ) \ P is that the

complementation may involve an exponential blow-up of the state space; this has to be
avoided.

5

assume that |W | = 3 for all subformulae W = 1. This is also easy to achieve.
As long as there is a subformula x1 · · · xk = 1, xi ∈ Γ ∪ Ω for 1 ≤ i ≤ k and
k ≥ 4, we replace it by the conjunction

∃Y : x1x2Y = 1 ∧ Y x3 · · · xk = 1,

where Y is a fresh variable and ∃Y is put to the front, and then proceed
recursively. This ﬁnishes the ﬁrst phase. The output of this phase is a system
of atomic formulae of type W = 1, X ∈ P , X 6∈ P with W ∈ (Γ∪Ω)3, X ∈ Ω,
and P ⊆ F (Σ) rational.
At this point we switch to the existential theory of equations with regular
constraints in free monoids where these monoids have an involution. Recall
that X ∈ P (resp. X 6∈ P ) means in fact X ∈ ψ(P ′) (resp. X 6∈ ψ(P ′)) where
P ′ ⊆ Γ∗ is a regular word language speciﬁed by some ﬁnite non-deterministic
automaton. Using ψ-symbols we obtain an interpretation over (Γ∗, ) without
changing the truth value by replacing syntactically each subformula X ∈ P
(resp. X 6∈ P ) by ψ(X) ∈ ψ(P ′) (resp. ψ(X) 6∈ ψ(P ′)) and by replacing
each subformula W = 1 by ψ(W ) = 1.
We keep the interpretation over words, but we eliminate now all occurrences
of ψ again. We begin with the occurrences of ψ in the constraints. Let
P ′ ⊆ Γ∗ be regular being accepted by some ﬁnite automaton with state
set Q. As stated in the in the ﬁrst part of the proof of Proposition 1, we
construct a ﬁnite automaton, using the same state set, which accepts the
following language

P ′′ = { v ∈ Γ∗ | ∃u ∈ P ′ : u

∗→ v }.

In particular, ψ(P ′) = ψ(P ′′) and bP ⊆ P ′′ where bP = {bu ∈ Γ∗ | u ∈ P ′ }.

We replace all positive atomic subformulae of the form ψ(X) ∈ ψ(P ′) by
X ∈ P ′′. A simple reﬂection shows that the truth value has not changed
since we can think of X of being a freely reduced word. For a negative
formulae ψ(X) 6∈ ψ(P ′) we have to be a little more careful. Let N ⊆ Γ∗ be
the regular set of all freely reduced words. The language N is accepted by a
deterministic ﬁnite automaton with |Γ| + 1 states. We replace ψ(X) 6∈ ψ(P ′)
by

X 6∈ P ′′ ∧ X ∈ N,

where P ′′ is as above. Again the truth value did not change.

6

We now have to deal with the formulae ψ(xyz) = 1 where x, y, z ∈ Γ ∪ Ω.
Observe that the underlying propositional formula is satisﬁable over Γ∗ if
and only if it is satisﬁable in freely reduced words. The following lemma is
well-known. Its easy proof is left to the reader.

Lemma 3 Let u, v, w ∈ Γ∗ be freely reduced words. Then we have ψ(uvw) =
1 (i.e. uvw = 1 in F (Σ)) if and only if there are words P, Q, R ∈ Γ∗ such
that u = P Q, v = QR, and w = R P holds in Γ∗.

Based on this lemma we replace each atomic subformulae ψ(xyz) = 1 with
x, y, z ∈ Γ ∪ Ω by a conjunction

∃P ∃Q∃R : x = P Q ∧ y = QR ∧ z = R P ,

where P , Q, R are fresh variables and the existential block is put to the front.
The new existential formula has no occurrence of ψ anymore. The atomic
subformulae are of the form x = yz, X ∈ P , X 6∈ P , where x, y, z ∈ Γ ∪ Ω
and P ⊆ Γ∗ is regular. The size of the formula is linear in the size of the
original formula. Therefore Theorem 2 is a consequence of Theorem 4.

4 Free Monoids with Involution

: Γ → Γ and

As above, let Γ be an alphabet of constants and Ω be an alphabet of variables.
: Ω → Ω. The involution on Ω is
There are involutions
without ﬁxed points, but we explicitly allow ﬁxed points for the involution
on Γ. 3 The involution is extended to (Γ ∪ Ω)∗ by x1 · · · xn = xn · · · x1 for
n ≥ 0 and xi ∈ Γ ∪ Ω, 1 ≤ i ≤ n.
From now on, all monoids M under consideration are equipped with an
: M → M, i.e. we have 1 = 1 for the unit element, x = x, and
involution
xy = y x for all x, y ∈ M. A homomorphism between monoids M and M ′ is
therefore a mapping h : M → M ′ such that h(1) = 1, h(xy) = h(x)h(y), and
h(x) = h(x) for all x, y ∈ M. The pair (Γ∗, ) is called a free monoid with
involution. 4

3Fixed points for the involution on constants are needed in the proof later anyhow and

this more general setting leads to further applications, [5]

4Note that (Γ∗, ) is a free monoid which has an involution, but it is not a free object

in the category of monoids with involution, as soon as the involution has ﬁxed points.

7

The existential theory of equations with regular constraints in free monoids
with involution is based on atomic formulae of type U = V where U, V ∈
(Γ ∪ Ω)∗ and of type X ∈ P where X ∈ Ω and P ⊆ Γ∗ is a regular language
speciﬁed by some non-deterministic ﬁnite automaton. Again, a propositional
formula is build up by atomic formulae using negations, conjunctions and
disjunctions. The existential theory refers to closed existentially quantiﬁed
propositional formulae which evaluate to true over (Γ∗, ).
The following statement is the main result of the paper.

Theorem 4 The following problem is PSPACE–complete.
INPUT: A closed existentially quantiﬁed propositional formula with regular
constraints in a free monoid with involution over (Γ, ).
QUESTION: Does the formula evaluate to true over (Γ∗, )?

The proof of Theorem 4 is in a ﬁrst step (next section) a reduction to Theorem 
5. The proof of Theorem 5 will be the essential technical contribution.

5 From Regular Constraints to Boolean Matrices 
and a Single Equation

The ﬁrst part of the proof is very similar to what we have done above. By
DeMorgan we have no negations and all subformulae are of type U = V ,
U 6= V , X ∈ P , X 6∈ P , where U, V ∈ (Γ ∪ Ω)∗, X ∈ Ω, and P ⊆ Γ∗ is
regular.
Since we work over a free monoid Γ∗ it is easy to handle inequalities U 6= V
where U, V ∈ (Γ ∪ Ω)∗. We recall it under the assumption |Γ| ≥ 2: A
subformulae U 6= V is replaced by

∃X∃Y ∃Z : _a6=b

(U = V aX ∨ V = U aX ∨ (U = XaY ∧ V = XbZ)).

Making guesses we can eliminate all disjunctions and we obtain a propositional 
formula which is a single conjunction over subformulae of type U = V ,
X ∈ P , and X 6∈ P where U, V ∈ (Γ ∪ Ω)∗, X ∈ Ω, and P ⊆ Γ∗ is regular.
By another standard procedure we can replace a conjunction of word equations 
over (Γ ∪ Ω)∗ by a single word equation L = R with L, R ∈ (Γ ∪ Ω)+.
For example, we may choose a new letter a and then we can replace a system

8

L1 = R1, L2 = R2, . . . , Lk = Rk by L1aL2a · · · aLk = R1aR2a · · · aRk and a
list X ∈ Γ∗ for all X ∈ Ω; this works since a 6∈ Γ.
Therefore we may assume that our input is given by a single equation L = R
with L, R ∈ (Γ ∪ Ω)+ and by two lists (Xj ∈ Pj, 1 ≤ j ≤ m) and (Xj 6∈
Pj, m < j ≤ k) where Xj ∈ Ω and each regular language Pj ⊆ Γ∗ is speciﬁed
by some non-deterministic automaton Aj = (Qj, Γ, δj, Ij, Fj) where Qj is the
set of states, δj ⊆ Qj × Γ × Qj is the transition relation, Ij ⊆ Qj is the
subset of initial states, and Fj ⊆ Qj is the subset of ﬁnal states, 1 ≤ j ≤ k.
Of course, a variable X may occur several times in the list with diﬀerent
constraints, therefore we might have k greater than |Ω|. The question is
whether there is a solution.
A solution is a mapping σ : Ω → Γ∗ being extended to a homomorphism
σ : (Γ ∪ Ω)∗ → Γ∗ by leaving the letters from Γ invariant such that the
following conditions are satisﬁed:

σ(L) = σ(R),
σ(X) = σ(X)
σ(Xj) ∈ Pj
6∈ Pj
σ(Xj)

for X ∈ Ω,
for 1 ≤ j ≤ m,
for m < j ≤ k.

For the next steps it turns out to be more convenient to work within the
framework of Boolean matrices instead of ﬁnite automata: Let Q be the
disjoint union of the state spaces Qj, 1 ≤ j ≤ k. We may assume that

Q = {1, . . . , n}. Let δ =S1≤j≤k δj, then δ ⊆ Q × Γ × Q and with each a ∈ Γ

we can associate a Boolean n × n matrix g(a) ∈ Bn×n such that g(a)i,j =
“(i, a, j) ∈ δ” for 1 ≤ i, j ≤ n. Since our monoids should have an involution,
we shall in fact work with 2n × 2n matrices. Henceforth M ⊆ B2n×2n denotes
the following monoid with involution:

where

M = {(cid:18)A 0
0 B(cid:19) | A, B ∈ Bn×n },
0 B(cid:19) =(cid:18)BT
(cid:18)A 0

0 AT(cid:19)

0

and the operator T denotes the transposition. We deﬁne a homomorphism
h : Γ∗ → M by

h(a) =(cid:18)g(a)

0

0

g(a)T(cid:19) for a ∈ Γ,

9

where the mapping g : Γ → Bn×n is deﬁned as above. The homomorphism h
can be computed in polynomial time and it respects the involution. Now, for
each regular language Pj, 1 ≤ j ≤ k we compute vectors Ij, Fj ∈ B2n such
that for all w ∈ Γ∗ and 1 ≤ j ≤ k we have the equivalence:

w ∈ Pj ⇔ I T

j h(w)Fj = 1.

j ρ(X)Fj = 0, if m < j ≤ k).

Having done these computations we make a non-deterministic guess ρ(X) ∈
M for each variable X ∈ Ω. We verify ρ(X) = ρ(X) for all X ∈ Ω and
whenever there is a constraint of type X ∈ Pj for some 1 ≤ j ≤ m (or
X 6∈ Pj for some m < j ≤ k), then we verify I T
j ρ(X)Fj = 1, if 1 ≤ j ≤ m
(or I T
After these preliminaries, we introduce the formal deﬁnition of an equation
E with constraints: Let d, n ∈ N and let M ⊆ B2n×2n be the monoid with
involution deﬁned above. We consider an equation of length d over some
Γ and Ω with constraints in M being speciﬁed by a list E containing the
following items:

• The alphabet (Γ, ) with involution.

• The homomorphism h : Γ∗ → M which is speciﬁed by a mapping

h : Γ → M such that h(a) = h(a) for all a ∈ Γ.

• The alphabet (Ω, ) with involution without ﬁxed points.

• A mapping ρ : Ω → M such that ρ(X) = ρ(X) for all X ∈ Ω.

• The equation L = R where L, R ∈ (Γ ∪ Ω)+ and |LR| = d.

We will denote this list simply by

E = (Γ, h, Ω, ρ; L = R).

A convenient deﬁnition for the input size is given by n + d + log2(|Γ| + |Ω|).
This deﬁnition takes into account that there might be constants or variables
with constraints which are not present in the equation. Recall that n refers
to the dimension of the boolean matrices, and this parameter is part of the
input.
A solution of E is a mapping σ : Ω → Γ∗ (being extended to a homomorphism
σ : (Γ ∪ Ω)∗ → Γ∗ by leaving the letters from Γ invariant) such that the
following three conditions are satisﬁed:

σ(L) = σ(R),

10

σ(X) = σ(X) for all X ∈ Ω,
hσ(X) = ρ(X) for all X ∈ Ω.

By the reduction above, Theorem 4 is a consequence of the next statement
which says that the satisﬁability problem of equations with constraints can
be solved in polynomial space.

Theorem 5 The following problem is PSPACE–complete.
INPUT: An equation E0 with constraints E0 = (Γ0, h0, Ω0, ρ0; L0 = R0).
QUESTION: Is there a solution σ : Ω0 → Γ∗
0?

For the proof we need an explicit space bound. Therefore we ﬁx some polynomial 
p and and we allow working space p(n + d + log2(|Γ| + |Ω|). An
appropriate choice of the polynomial p can be calculated from the presentation 
below. What is important is that the notions of admissibility being used
in the next sections always refer to some ﬁxed polynomials. The following
lemma states that some basic operations, which we have to perform several
times can be done in PSPACE.

Lemma 6 The following two problems can be solved in polynomial space with
respect to the input size n + log(|Γ|).
INPUT: A matrix A ∈ M and a mapping h : Γ → M.
QUESTION: Is there some w ∈ Γ∗ such that h(w) = A?
INPUT: A matrix A ∈ M and a mapping h : Γ → M.
QUESTION: Is there some w ∈ Γ∗ such that h(w) = A and w = w?

Proof.
The ﬁrst question can be solved by guessing a word w letter by
letter and calculating h(w). The second question can be solved since w = w
implies w = uau for some u ∈ Γ∗ and a ∈ Γ ∪ {1} with a = a. Hence we can
guess u and a. During the guess we compute B = h(u) and then we verify
A = Bh(a)B.
(cid:3)
Here is a ﬁrst application of Lemma 6: Assume that an equation with constraints 
E = (Γ, h, Ω, ρ; L = R) contains in the speciﬁcation some variable
X which does not occur in LRLR, then the equation might be unsolvable,
simply because ρ(X) 6∈ h(Γ∗). However, by the lemma above we can test this
in PSPACE. If ρ(X) ∈ h(Γ∗), then we can safely cancel X and X. Thus,
we put this test in the preprocessing, and in the following we shall assume
that all variables occur somewhere in LRLR. In particular, we may assume
|Ω| ≤ 2|LR|.

11

6 The Exponent of Periodicity

A key step in proving Theorem 5 is to ﬁnd a bound on the exponent of
periodicity in a minimal solution. This idea is used in all known algorithms
for solving word equations in general, c.f., [17, 26].
Let w ∈ Γ∗ be a word. The exponent of periodicity exp(w) is deﬁned by

exp(w) = sup{ α ∈ N | ∃u, v, p ∈ Γ∗, p 6= 1 : w = upαv }.

We have exp(w) > 0 if and only if w is not the empty word. Let E =
(Γ, h, Ω, ρ, L = R) be an equation with constraints. The exponent of periodicity 
of E is also denoted by exp(E). It is deﬁned by

exp(E) = inf{{ exp(σ(L)) | σ is a solution of E } ∪ {∞} }.

By deﬁnitions we have exp(E) < ∞ if and only if E is solvable. Here we
show that the well-known result from word equations [13] transfers to the
situation here. The exponent of periodicity of a solvable equation can be
bounded by a singly exponential function. Thus, in the following sections we
shall assume that if E0 is solvable, then exp(E0) ∈ 2O(d+n log n). This is the
content of the next proposition.

Proposition 7 Let E = (Γ, h, Ω, ρ; L = R) be an equation with constraints
and let σ : Ω → Γ∗ be a solution. Then we ﬁnd eﬀectively a solution σ′ :
Ω → Γ∗ such that exp(σ′(L)) ∈ 2O(d+n log n).

The rest of this section is devoted to prove Proposition 7. Since it follows
standard lines, the proof can be skipped in a ﬁrst reading.
Proof. Let p ∈ A+ be a primitive word. In our setting the deﬁnition of the
p-stable normal form of a word w ∈ A∗ depends on the property whether or
not p is a factor of p2. So we distinguish two cases and in the following we
also write p−1 for denoting p. Then, for example, p−3 means the same as p3.
First case: We assume that p is not a factor of p2. The idea is to replace
each maximal factor of the form pα with α ≥ 2 by a sequence p, α − 2, p and
each maximal factor of the form pα with α ≥ 2 by a sequence p, −(α − 2), p.
This leads to the following notion:
The p-stable normal form (ﬁrst kind) of w ∈ A∗ is a shortest sequence (k is
minimal)

(u0, ε1α1, u1, . . . , εkαk, uk)

12

such that k ≥ 0, u0, ui ∈ A∗, εi ∈ {+1, −1}, αi ≥ 0 for 1 ≤ i ≤ k, and the
following conditions are satisﬁed:

• w = u0pε1α1u1 · · · pεkαkuk.

• k = 0 if and only if neither p2 nor p2 is a factor of w.

• If k ≥ 1, then:

u0 ∈ A∗pε1 \ A∗p±2A∗,
ui ∈ (A∗pεi+1 ∩ pεiA∗) \ A∗p±2A∗ for 1 ≤ i < k,
uk ∈ pεkA∗ \ A∗p±2A∗.

The p-stable normal form of w becomes

(uk, −εkαk, uk−1, . . . , −ε1α1, u0).

Example 8 Let p = aabaa with b 6= b and w = p4baap−1aabp−2. Then the
p-stable normal form of w is:

(aaab, 2, aabaabaa, −1, aabaabaa, 0, aabaa).

Second case: We assume that p is a factor of p2. Then we can write p = rs
with p = sr and r = r, s = s. We allow r = 1, hence the second case includes
the case p = p. In fact, if r = 1, then below we obtain the usual deﬁnition of
p-stable normal form. Moreover, by switching to some conjugated word of p
we could always assume that r ∈ {1, a} for some letter a being ﬁxed by the
involution, a = a, but this switch is not made here. The idea is to replace
each maximal factor of the form (rs)αr with α ≥ 2 by a sequence rs, α−2, sr.
In this notation α − 2 is representing the factor (rs)α−2r = pα−2r = rpα−2.
The p-stable normal form (second kind) of w ∈ A∗ is now a shortest sequence
(k is minimal)

(u0, α1, u1, . . . , αk, uk)

such that k ≥ 0, u0, ui ∈ A∗, αi ≥ 0 for 1 ≤ i ≤ k, and the following
conditions are satisﬁed:

• w = u0pα1ru1 · · · pαk ruk.

13

• k = 0 if and only if p2r is not a factor of w.

• If k ≥ 1, then:

u0 ∈ A∗rs \ (A∗p2rA∗ ∪ A∗rsrs),
ui ∈ (A∗rs ∩ srA∗) \ (srsrA∗ ∪ A∗p2rA∗ ∪ A∗rsrs) for 1 ≤ i < k,
uk ∈ srA∗ \ (A∗p2rA∗ ∪ srsrA∗).

Since rs = sr, the p-stable normal form of w becomes

(uk, αk, u1, . . . , α1, u0).

So, for the second kind no negative integers interfere.

Example 9 Let p = aab with b = b. Then r = aa and s = b. Let w =
ap4ap3a Then the p-stable normal form of w is:

(abaab, 2, baabaaab, 0, baaba).

In both cases we can write the p-stable normal form of w as a sequence

(u0, α1, u1, . . . , αk, uk)

where ui are words and αi are integers.
For every ﬁnite semigroup S there is a number c(S) such that for all s ∈ S
the element sc(S) is idempotent, i.e., sc(S) = s2c(S). It is clear that the number
c(M) for our monoid M ⊆ B2n×2n is the same as the number c(Bn×n). It is
well-known [21] that we can take c(Bn×n) = n! (it is however more convenient
to deﬁne c(M) = 3 for n = 1). Hence in the following c(M) = max{3, n!}.
For speciﬁc situations this might be an overestimation, but this choice guarantees 
h(uvc(M )w) = h(uv2c(M )w) for all u, v, w ∈ Γ∗ and all h : Γ∗ → M.
Now, let w, w′ ∈ Γ∗ be words such that the p-stable normal forms are identical
up to one position where for w appears an integer αi and for w′ appears an
i. We know h(w) = h(w′) whenever the following conditions are
integer α′
satisﬁed: αi · α′
i (mod c(M)).
Then we have h(w) = h(w′). This is the reason to change the syntax of the pstable 
normal form. Each non-zero integer α′ is written as α′ = ε(q + αc(M))
where ε, q, α are uniquely deﬁned by ε ∈ {+1, −1}, 0 ≤ q < c(M), and
α ≥ 0. For α′ = 0 we may choose ε = q = α = 0. We shall read α as a

i| ≥ c(M), and αi ≡ α′

i > 0, |αi| ≥ c(M), |α′

14

variable ranging over non-negative integers, but ε, q, and c(M) are viewed
as constants. In fact, if |α′| < c(M), then we best view α also as a constant
in order to avoid problems with the constraints.
Let u, v, and w be words such that uv = w holds. Write these words in their
p-stable normal forms:

u :
v :
w :

(u0, ε1(q1 + α1c(S)), u1, . . . , εk(qk + αkc(S)), uk),
(v0, ε′
(w0, ε′′

1(s1 + β1c(S)), v1, . . . , ε′
1(t1 + γ1c(S)), w1, . . . , ε′′

ℓ(sℓ + βℓc(S)), vℓ),
m(tm + γmc(S)), wm).

Since uv = w there are many identities. For example, for k, ℓ ≥ 2 we have
u0 = w0, vl = wm, q1 = t1, α1 = γ1, etc. What exactly happens depends only
on the p-stable normal form of the product ukv0. There are several cases,
which easily can be listed. We treat only one of them, which is in some sense
the worst case in order to produce a large exponent of periodicity. This
is the case where p = rs with r = r and s = s. Then it might be that
uk = srsr1 and v0 = r2srs with r1r2 = r (and r1 6= 1 6= r2). Hence we
have ukv0 = sp3 and k + ℓ = m + 1. It follows α1 = γ1, . . . , αk−1 = γk−1,
β2 = γk+1, . . . , βℓ = γm, and there is only one non-trivial identity:

qk + s1 + 4 + (αk + β1)c(S) = tk + γkc(S).

Since by assumption c(S) ≥ 3, the case ukv0 = sp3 leads to the identity:

γk = αk + β1 + c with c ∈ {0, 1, 2}.

k = α′

k + β′

1 ≥ 1, and γ′

k ≥ 1 such that still γ′

Assume now that αk ≥ 1 and β1 ≥ 1. If we replace αk, β1, and γk by some
α′
k ≥ 1, β′
1 + c, then we obtain
new words u′, v′, and w′ with the same images under h in M and still the
identity u′v′ = w′.
What follows then is completely analogous to what has been done in detail
in [13, 10, 11, 3]. Using the p-stable normal form we can associate with an
equation L = R of denotational length d together with its solution σ : Ω → Γ∗
some linear Diophantine system of d equations in at most 3d variables. The
variables range over natural numbers since zeros are substituted. (In fact the
number of variables can be reduced to be at most 2|Ω|). The parameters of
this system are such that maximal size of a minimal solution (with respect to
the component wise partial order of Nd) is in O(21.6d) with the same approach

15

as in [13]. This tight bound is based in turn on the work of [30]; a more
moderate bound 2O(d) (which is enough for our purposes) is easier to obtain,
see e.g. [3]. The maximal size of a minimal solution of the linear Diophantine
system has a backward translation to a bound on the exponent of periodicity.
For this translation we have to multiply with the factor c(M) ∈ 2O(n log n)
and to add c(M) + 1. Putting everything together we obtain the claim of the
proposition.
(cid:3)

7 Exponential Expressions

During the procedure which solves Theorem 5 various other equations with
constraints are considered but the monoid M will not change.
There will be not enough space to write down the equation L = R in plain
form, in general. In fact, there is a provable exponential lower bound for the
length |LR| in the worst case which we can meet during the procedure. In
order to overcome this diﬃculty Plandowski’s method uses data compression
for words in (Γ ∪ Ω)∗ in terms of exponential expressions.
Exponential expressions (their evaluation and their size) are inductively de-
ﬁned:

• Every word w ∈ Γ∗ denotes an exponential expression. The evaluation

eval(w) is equal to w, its size kwk is equal to the length |w|.

• Let e, e′ be exponential expressions. Then ee′ is an exponential expression.
 Its evaluation is the concatenation eval(ee′) = eval(e)eval(e′), its
size is kee′k = kek + ke′k.

• Let e be an exponential expression and k ∈ N. Then (e)k is an exponential 
expression. Its evaluation is eval((e)k) = (eval(e))k, its size is
k(e)kk = log(k) + kek where log(k) = max{1, ⌈log2(k)⌉}.

It is not diﬃcult to show that the length of eval(e) is at most exponential
in the size of e, a fact which is, strictly speaking, not needed for the proof
of Theorem 5. What we need however is the next lemma. Its proof can be
done easily by structural induction and it is omitted.

Lemma 10 Let u ∈ Γ∗ be a factor of a word w ∈ Γ∗. Assume that w can
be represented by some exponential expression of size p. Then we ﬁnd an
exponential expression of size at most p2 that represents u.

16

We say that an exponential expression e is admissible, if its size kek is
bounded by some ﬁxed polynomial in the input size of E0. The lemma above
states that if e is admissible, then we ﬁnd admissible exponential expressions
for all factors of eval(e). But now the admissibility is deﬁned with respect
to some polynomial which is the square of the original polynomial, so, in a
nested way, we can apply this procedure a constant number of times, only.
In our application the nested depth does not go beyond two.
The next lemma is straightforward since we allow a polynomial space bound
without any time restriction. Again, the proof is left to the reader.

Lemma 11 The following two problems can be solved in PSPACE.
INPUT: Exponential expressions e and e′.
QUESTION: Do we have eval(e) = eval(e′)?
INPUT: A mapping h : Γ → M and an exponential expression e.
OUTPUT: The matrix h(eval(e)) ∈ M.

Remark 12 The computation above can actually be performed in polynomial
time, but this is not evident for the ﬁrst question, see [24] for details.

Henceforth we allow that the part L = R of an equation with constraints may
also be given by a pair of exponential expressions (eL, eR) with eval(eL) = L
and eval(eR) = R. We say that E = (Γ, h, Ω, ρ; eL = eR) is admissible, if
eLeR is admissible, |Γ \ Γ0| has polynomial size, Ω ⊆ Ω0, and h(a) = h0(a)
for a ∈ Γ ∩ Γ0.
For two admissible equations with constraints E = (Γ, h, Ω, ρ; eL = eR) and
E′ = (Γ, h, Ω, ρ; e′
L = e′
L) and
R) as strings in (Γ ∪ Ω)∗. This means that they represent
eval(eR) = eval(e′
exactly the same equations.

R) we write E ≡ E′, if eval(eL) = eval(e′

8 Base Changes

In this section we ﬁx a mapping h : Γ → M which respects the involution.
Let (Γ′, ) be an alphabet with involution and let β : Γ′ → Γ∗ be some
mapping β such that β(a) = β(a) for all a ∈ Γ′. We deﬁne h′ : Γ′ → M such
that h′ = hβ. We also extend to a homomorphism β : (Γ′ ∪ Ω)∗ → (Γ ∪ Ω)∗
by leaving the variables invariant.

17

Let E′ = (Γ′, h′, Ω, ρ; L′ = R′). be an equation with constraints. The base
change β∗(E′) is deﬁned by

β∗(E′) = (Γ, h, Ω, ρ; β(L′) = β(R′)).

We also refer to β : Γ′ → Γ∗ as a base change and we say that β is admissible,
if |Γ′| has polynomial size and if β(a) can be represented by some admissible
exponential expression for all a ∈ Γ′.

Remark 13 If β : Γ′ → Γ∗ is an admissible base change and if L′ = R′ is
given by a pair of admissible exponential expressions, then we can represent
β∗(E′) by some admissible equation with constraints. A representation of
β∗(E′) is computable in polynomial time.

Lemma 14 Let E′ be an equation with constraints and β : Γ′ → Γ∗ be a
base change. If σ′ is a solution of E′, then σ = βσ′ is a solution of β∗(E′).

Proof. Clearly σ(X) = σ(X) and hσ(X) = hβσ′(X) = h′σ′(X) = ρ(X) for
all X ∈ Ω. Next by deﬁnition σ(a) = a for a ∈ Γ and β(X) = X for X ∈ Ω.
Hence σβ(a) = βσ′(a) for a ∈ Γ′ and therefore σβ = βσ′ : (Γ′ ∪ Ω)∗ → Γ∗.
This means σβ(L) = βσ′(L) = βσ′(R) = σβ(R) since σ′(L) = σ′(R).
(cid:3)
The lemma above leads to the ﬁrst rule.

Rule 1 If E is of the form β∗(E′) and if we are looking for a solution of E,
then it is enough to ﬁnd a solution for E′. Hence, during a non-deterministic
search we may replace E by E′.

Example 15 Consider the following equation E with constraints over Γ =
{a, b, c, ¯a, ¯b, ¯c}:

XX = Y ¯b¯c¯b¯a¯b¯c¯bY ZabcbY .

Let there be the constraints for X and Z saying X ∈ Γ300Γ∗ and Z ∈ ¯b¯c¯b¯aΓ∗.
Deﬁne Γ′ = {a, b, ¯a, ¯b} and a base change β : Γ′ → Γ∗ by β(a) = abcb and
β(b) = bcb. Then the equation E is of the form β∗(E′) where E′ is given by

XX = Y ¯a¯bY ZaY

18

and the new (and sharper) constraint for Z is simply Z ∈ ¯aΓ′∗, for X we
may sharpen the constraint to X ∈ Γ′100Γ′∗ According to Rule 1 it is enough
to solve E′. The eﬀect of the base change β is that the equation E′ is shorter
and the alphabet of constants becomes smaller, since the letter c is not used
anymore. Note also that the length restriction on X became smaller, too.
However this has a prize; in general, E = β∗(E′) might have a solution,
whereas E′ is unsolvable. As we will see later, our guess has been correct in
the sense that E′ still has a solution.

9 Projections

Let (Γ, ) and (Γ′, ) be alphabets with involution such that (Γ, ) ⊆ (Γ′, ).
A projection is a homomorphism π : Γ′∗ → Γ∗ such that both π(a) = a for
a ∈ Γ and π(a) = π(a) for all a ∈ Γ′. If h : Γ → M is given, then a projection
π deﬁnes also h′ : Γ′ → M by h′ = hπ.
Let E be an equation with constraints E = (Γ, h, Ω, ρ; L = R). Then we can
deﬁne an equation with constraints π∗(E) by

π∗(E) = (Γ′, hπ, Ω, ρ; L = R).

The diﬀerence between E and π∗(E) is only in the alphabets of constants
and in the mappings h and h′ = hπ. Note that every projection π : Γ′∗ → Γ∗
deﬁnes a base change π∗ such that π∗π∗(E) = E.

Lemma 16 Let E = (Γ, h, Ω, ρ; L = R) and E′ = (Γ′, h′, Ω, ρ; L = R) be
equations with constraints. Then the following two statements hold.

i) There is a projection π : Γ′∗ → Γ∗ such that π∗(E) = E′, if and only if
both h′(Γ′) ⊆ h(Γ∗) and for all a ∈ Γ′ with a = a there is some w ∈ Γ∗
with w = w such that h′(a) = h(w).

ii) If we have π∗(E) = E′ and if σ′ : Ω → Γ′∗ is a solution of E′, then we

eﬀectively ﬁnd a solution σ for E such that |σ(L)| ≤ 2|M||σ′(L)|.

Proof.
i) Clearly, the only-if condition is satisﬁed by the deﬁnition of a
projection since then h′ = hπ. For the converse, assume that h′(Γ′) ⊆ h(Γ∗)
and that a = a implies h′(a) ∈ h({w ∈ Γ∗ | w = w}). Then for each a ∈ Γ′ \Γ
we can choose a word wa ∈ Γ∗ such that h′(a) = h(wa). We can make the
choice such that wa = wa for all a ∈ Γ′ \ Γ. If a 6= a, then we can ﬁnd wa

19

such that |wa| < |M|, since we can take the shortest word wa ∈ Γ∗ such that
h(wa) = h′(a) ∈ M. For a = a we know that there is some word wa ∈ Γ∗ with
h′(a) = h(wa) and wa = wa. Hence we can write wa = vbv with b ∈ Γ ∪ {1}
and b = b. For b 6= 1 we can demand |wa| ≤ 2|M| − 1. For b = 1 we can
demand |wa| ≤ 2|M| − 2. Thus, we ﬁnd a projection π : Γ′∗ → Γ∗ such that
π∗(E) = E′ and moreover, |π(a)| < 2|M| for all a ∈ Γ′.
ii) Using the reasoning in the proof of i) we may assume that π : Γ′∗ → Γ∗
satisﬁes |π(a)| < 2|M| for all a ∈ Γ′. Since π deﬁnes a base change with
π∗(E′) = E, we know by Lemma 14 that σ = πσ′ is a solution of E. Clearly,
|σ(L)| = |πσ′(L)| ≤ 2|M||σ′(L)|.
(cid:3)

Remark 17 In the following we will meet the problem to decide whether
there is a projection π : Γ′∗ → Γ∗ such that π∗(E) = E′. We actually need
not too much space for this test. It is not necessary to write down π. We can
use the criterion in the lemma above and Lemma 6. Then we have to store
in the working space only some Boolean matrices of B2n×2n. In particular, if
n is a constant (or logarithmically bounded in the input size), then the test
∃π : π∗(E) = E′ can be done in polynomial time. However, if n becomes a
substantial part of the input size, then the test might be diﬃcult in the sense
that we might need the full power of PSPACE.

The lemma above leads now to the second rule.

Rule 2 If π is a projection and if we are looking for a solution of E, then
it is enough to ﬁnd a solution for π∗(E). Hence, during a non-deterministic
search we may replace E by π∗(E).

Example 18 Let us continue with the equation which has been obtained by
the transformation in Example 15. In order to simplify notations, we will
call E the equation XX = Y ¯a¯bY ZaY , and Γ = {a, b, ¯a, ¯b}.
Remember that the constraint on X demanded a rather long solution. Therefore 
we may reintroduce a letter c and put Γ′ = {a, b, c, ¯a, ¯b, ¯c}. Then we
may deﬁne a projection π : Γ′ → Γ∗ by, say, π(c) = b100. The equation
E′ = π∗(E) looks as above, but in E′ we may change the constraint for X.
We may sharpen the new constraint for X to be X ∈ Γ∗cΓ∗. Thus, the
solution for X might be very short now.

20

10 Partial Solutions

Let Ω′ ⊆ Ω be a subset of the variables which is closed under involution. We
assume that there is a mapping ρ′ : Ω′ → M with ρ′(x) = ρ′(x), but we do
not require that ρ′ is the restriction of ρ : Ω → M. Consider an equation
with constraints E = (Γ, h, Ω, ρ; L = R). A partial solution is a mapping
δ : Ω → Γ∗Ω′Γ∗ ∪ Γ∗ such that the following conditions are satisﬁed:

i) δ(X) ∈ Γ∗XΓ∗

for all X ∈ Ω′,

ii) δ(X) ∈ Γ∗

for all X ∈ Ω \ Ω′,

iii) δ(X) = δ(X)

for all X ∈ Ω.

The mapping δ is extended to a homomorphism δ : (Γ ∪ Ω)∗ → (Γ ∪ Ω′)∗ by
leaving the elements of Γ invariant. Let E′ = (Γ, h, Ω′, ρ′; L′ = R′) be another
equation with constraints (using the same Γ and h). We write E′ = δ∗(E),
if there exists some partial solution δ : Ω → Γ∗ΩΓ∗ ∪ Γ∗ such that the
following conditions hold: L′ = δ(L), R′ = δ(R), ρ(X) = h(u)ρ′(X)h(v) for
δ(X) = uXv, and ρ(X) = h(w) for δ(X) = w ∈ Γ∗.

Lemma 19 In the notation of above, let E′ = δ∗(E) for some partial solution
δ : Ω → Γ∗ΩΓ∗ ∪ Γ∗. If σ′ is a solution of E′, then σ = σ′δ is a solution of
E. Moreover, we have σ(L) = σ′(L′) and σ(R) = σ′(R′).

Proof. By deﬁnition, δ and σ′ are extended to homomorphisms δ : (Γ∪Ω)∗ →
(Γ ∪ Ω′)∗ and σ′ : (Γ ∪ Ω′)∗ → Γ∗ leaving the letters of Γ invariant. Since
E′ = δ∗(E) we have δ(L) = L′ and δ(R) = R′. Since σ′ is a solution, we have
σ(L) = σ′δ(L) = σ′(L′) = σ′(R′) = σ′δ(R) = σ(R) and σ leaves the letters of
Γ invariant. The solution σ′ satisﬁes hσ′(X) = ρ′(X) for all X ∈ Ω′. Hence,
if δ(X) = uXv, then ρ(X) = h(u)ρ′(X)h(v) = h(uσ′(X)v) = hσ′(uXv) =
hσ′δ(X) = hσ(X).
If δ(X) = w ∈ Γ∗, then σ(X) = σ′δ(X) = w and
ρ(X) = h(w), again by the deﬁnition of a partial solution.
(cid:3)

Lemma 20 The following problem can be solved in PSPACE.
INPUT: Two equations with constraints E = (Γ, h, Ω, ρ; eL = eR) and E′ =
(Γ, h, Ω′, ρ′; eL′ = eR′).
QUESTION: Is there some partial solution δ such that δ∗(E) ≡ E′?

21

Moreover, if δ∗(E) ≡ E′ is true, then there are admissible exponential expressions 
eu, ev for each X ∈ Ω′ and an admissible exponential expression
ew for each X ∈ Ω \ Ω′ such that

δ(X) = eval(eu)Xeval(ev)
δ(X) = eval(ew)

for X ∈ Ω′,
for X ∈ Ω \ Ω′.

Proof. Let L = eval(eL), R = eval(eR), L′ = eval(eL′), and R′ = eval(eR′).
The non-deterministic algorithm works as follows:
For each X ∈ Ω′ we guess admissible exponential expressions eu and ev with
eval(eu), eval(ev) ∈ Γ∗. We deﬁne an exponential expressions eX = euXev
and δ(X) = eval(eX). For each X ∈ Ω\Ω′ we guess an admissible exponential
eX with eval(eX ) ∈ Γ∗ and δ(X) = eval(eX ).
Next we verify whether or not δ∗(E) ≡ E′. During this test we have to
create an exponential expression fL (and fR, resp.) by replacing X in eL
(and eR, resp.) with the expression eX . This increases the size in the worst
case by a factor of max{||eX|| | X ∈ Ω}. The other tests whether ρ(X) =
h(u)ρ′(X)h(v) for δ(X) = uXv and ρ(X) = h(w) for δ(X) = w ∈ Γ∗ involve
admissible exponential expressions over Boolean matrices and can be done
in polynomial time.
The correctness of the algorithm follows from our general assumption that
all X ∈ Ω appear in LRLR. Therefore, if we have δ∗(E) ≡ E′, then δ(X) (or
δ(X)) appears necessarily as a factor in L′R′ = δ(LR). Hence δ(X) has an
exponential expression of polynomial size by Lemma 10. Therefore guesses
of eu, ev, and ew as above are possible without running out of space.
(cid:3)

Remark 21 Actually, the test for δ∗(E) ≡ E′ can be performed in nondeterministic 
polynomial time by Remark 12.

The lemma above leads to the third and last rule.

Rule 3 If δ is a partial solution and if we are looking for a solution of E, then
it is enough to ﬁnd a solution for δ∗(E). Hence, during a non-deterministic
search we may replace E by δ∗(E).

Remark 22 We can think of a partial solution δ : Ω → Γ∗Ω′Γ∗ ∪ Γ∗ in
the following sense. Assume we have an idea about σ(X) for some X ∈ Ω.
Then we might guess σ(X) entirely. In this case we can deﬁne δ(X) = σ(X)

22

and we have X 6∈ Ω′. For some other X we might guess only some preﬁx
u and some suﬃx v of σ(X). Then we deﬁne δ(X) = uXv and we have
to guess some ρ′(X) ∈ M such that ρ(x) : h(u)ρ′(X)h(v). If our guess was
correct, then such a matrix ρ′(X) ∈ M must exist. We have partially speciﬁed
the solution and applying Rule 3, we continue this process by replacing the
equation L = R by the new equation δ(L) = δ(R).

Example 23 We continue with our running example. After renaming, the
equation E is given by

XX = Y ¯a¯bY ZaY ,

and the alphabet of constant is given by Γ = {a, b, c, ¯a, ¯b, ¯c}. The constraints
are X ∈ Γ∗cΓ∗ and Z ∈ ¯a{a, b, ¯a, ¯b}∗.
We may guess the partial solution as follows: δ(X) = aX, δ(Y ) = Y , and
δ(Z) = ¯ab. The new equation δ∗(E) is

aXX¯a = Y ¯a¯bY ¯abaY .

The remaining constraint is that the solution for X has to use the letter c.
The process can continue, for example, we can apply Rule 1 again by deﬁning
another base change β(b) = ba to get the equation

aXX¯a = Y ¯bY ¯abY

over Γ = {a, b, c, ¯a, ¯b, ¯c}. Since the last equation has a solution (e.g., given
by σ(X) = bc¯c¯b¯babc and σ(Y ) = abc¯c¯b), the ﬁrst equation with constraints in
Example 15 has a solution too.

11 The Search Graph and Plandowski’s Algorithm


In the following we show that there is some ﬁxed polynomial (which can be
calculated from the presentation below) such that the high-level description
of Plandowski’s algorithm is as follows: On input E0 compute the maximal
space bound, given by the polynomial, to be used by the procedure. Then
apply non-deterministically Rules 1, 2, and 3 until an equation with a trivial
solution is found.

23

From the description above it follows that the speciﬁcation of the algorithm
just uses Rules 1, 2, 3. The algorithm is simple but it demands a good
heuristics to explore the search graph. The hard part is to prove that this
schema is correct; for this we have to be more precise.
The search graph is a directed graph: The nodes are admissible equations
with constraints. For two nodes E, E′, we deﬁne an arc E → E′, if there are
an admissible base change β, a projection π, and a partial solution δ such
that δ∗(π∗(E)) ≡ β∗(E′).

Lemma 24 The following problem can be decided in PSPACE.
INPUT: Admissible equations with constraints E and E′.
QUESTION: Is there an arc E → E′ in the search graph?
Proof. We ﬁrst guess some alphabet (Γ′′, ) of polynomial size together with
h′′ : Γ′′ → M. Then we guess some admissible base change β : Γ′ → Γ′′∗ such
that h′ = h′′β and we compute β∗(E′).
Next we guess some admissible equation with constraints E′′ which uses
Γ′′ and Ω. We check using Lemma 20 that there is some partial solution
δ : Ω → Γ′′∗Ω′Γ′′∗ ∪ Γ′′∗ such that δ∗(E′′) ≡ β∗(E′). (Note that every equation 
with constraints E′′ satisfying δ∗(E′′) ≡ β∗(E′) for some δ is admissible
by Lemma 10.) Finally we check using Remark 22 and that there is some
projection π : Γ′′ → Γ such that π∗(E) ≡ E′′. We obtain δ∗(π∗(E)) ≡ β∗(E′).
(cid:3)

Remark 25 Following Remarks 12 and 21 the problem in Lemma 24 can be
decided in non-deterministic polynomial time, if the monoid M is not part
of the input and viewed as a constant. If, as in our setting, M is part of
the input, then PSPACE is the best we can prove, because the test for the
projection becomes diﬃcult.

Plandowski’s algorithm works as follows:

begin

E := E0
while Ω 6= ∅ do

Guess an admissible equation E′ with constraints
Verify that E → E′ is an arc in the search graph
E := E′

endwhile
return “eval(eL) = eval(eR)”

end

24

By Rules 1–3 (Lemmata 14, 16 ii), and 19), if E → E′ is an arc in the search
graph and E′ is solvable, then E is solvable, too. Thus, if the algorithm
returns true, then E0 is solvable. The proof of Theorem 5 is therefore reduced
to the statement that if E0 is solvable, then the search graph contains a path
to some node without variables and the exponential expressions deﬁning the
equation evaluate to the same word. This existence proof is the hard part,
it covers the rest of the paper.

Remark 26 If E → E′ is due to some π : Γ′′∗ → Γ∗, δ : Ω → Γ′′∗Ω′Γ′′∗∪Γ′′∗,
and β : Γ′∗ → Γ′′∗, then a solution σ′ : Ω′ → Γ′∗ of E′ yields the solution σ =
π(βσ′)δ. Hence we may assume that the length of a solution has increased
by at most an exponential factor by Lemma 16 ii). Since we are going to
perform the search in a graph of at most exponential size, we get automatically
a doubly exponential upper bound for the length of a minimal solution by
backwards computation on such a path. This is still the best known upper
bound (although an singly exponential bound is conjectured), see [25].

12 Free Intervals

In this section we introduce the notion of free interval in order to cope with
long factors in the solution which are not related to any cut. If there were
no constraints, then these factors would not appear in a minimal solution.
In our setting we cannot avoid these factors.
For a word w ∈ Γ∗ we let {0, . . . , |w|} be the set of its positions. The
interpretation is that factors of w are between positions. To be more speciﬁc
let w = a1 · · · am, ai ∈ Γ for 1 ≤ i ≤ m. Then [α, β] with 0 ≤ α < β ≤ m
is called a positive interval and the factor w[α, β] is deﬁned by the word
w[α, β] = aα+1 · · · aβ.
It is convenient to have an involution on the set of intervals. Therefore [β, α]
is also called an interval (but it is never positive), and we deﬁne w[β, α] =
w[α, β]. We allow also α = β and we deﬁne w[α, α] to be the empty word.
For all 0 ≤ α, β ≤ m we let [α, β] = [β, α], then always w[α, β] = w[α, β].
Let us focus on the word w0 ∈ Γ∗
0 which in our notation is the solution w0 =
σ(L0) = σ(R0), where L0 = x1 · · · xg and R0 = xg+1 · · · xd, xi ∈ (Γ0 ∪ Ω0)
for 1 ≤ i ≤ d. We are going to deﬁne an equivalence relation ≈ on the set
of intervals of w0. For this we have to ﬁx some few more notations. We let
m0 = |w0| and for i ∈ {1, . . . , d} we deﬁne positions l(i) ∈ {0, . . . , m0 − 1}

25

and r(i) ∈ {1, . . . , m0} by the congruences

l(i) ≡ |σ(x1 · · · xi−1)| mod m0,
r(i) ≡ |σ(xi+1 · · · xd)| mod m0.

This means, the factor σ(xi) starts in w0 at the left position l(i) and it ends
at the right position r(i).
In particular, we have l(1) = l(g + 1) = 0 and
r(g) = r(d) = m0. The set of l and r positions is called the set of cuts.
Thus, the set of cuts is { l(i), r(i) | 1 ≤ i ≤ d }. There are at most d cuts.
These positions cut the word w0 in at most d − 1 factors. For convenience
we henceforth assume 2 ≤ g < d < m0 whenever necessary. We make also
the assumption that σ(xi) 6= 1 for all 1 ≤ i ≤ d. This assumption can be
realized e.g. by a ﬁrst step in Plandowski’s algorithm using a partial solution
δ which sends a variable X to the empty word, if σ(X) = 1 and sends X to
itself otherwise. Another choice to realize this assumption is by a guess in
some preprocessing.
We have σ(xi) = w0[l(i), r(i)] and σ(xi) = w0[r(i), l(i)] for 1 ≤ i ≤ d. By our
assumption, the interval [l(i), r(i)] is positive. Let us consider a pair (i, j)
such that i, j ∈ 1, . . . , d and xi = xj or xi = xj. For µ, ν ∈ {0, . . . , r(i)−l(i)}
we deﬁne a relation ∼ by:

[l(i) + µ, l(i) + ν] ∼ [l(j) + µ, l(j) + ν], if xi = xj,
[l(i) + µ, l(i) + ν] ∼ [r(j) − µ, r(j) − ν], if xi = xj.

Note that ∼ is a symmetric relation. Moreover, [α, β] ∼ [α′, β′] implies both
[β, α] ∼ [β′, α′] and w0[α, β] = w0[α′, β′]. By ≈ we denote the reﬂexive
and transitive closure of ∼. Then ≈ is an equivalence relation and again,
[α, β] ≈ [α′, β′] implies both [β, α] ≈ [β′, α′] and w0[α, β] = w0[α′, β′].
Next we deﬁne the notion of free interval . An interval [α, β] is called free,
if whenever [α, β] ≈ [α′, β′], then there is no cut γ′ with min{α′, β′} < γ′ <
max{α′, β′}. Clearly, the set of free intervals is closed under involution, i.e., if
[α, β] is free, then [β, α] is free, too. It is also clear that [α, β] is free whenever
|β − α| ≤ 1.

Example 27 The last equation in Example 23, namely

aXX¯a = Y ¯bY ¯abY ,

26

has a solution which yields the word

w0 =

0
| a

1
| bc¯c¯b

5
| ¯b

6
| abc

9
| ¯c¯b

11
| ¯a

12
| b

13
| bc¯c¯b

17
| ¯a

18
| .

The set of cuts is shown by the bars. The intervals [1, 5], [13, 17], and [6, 9]
are not free, since [1, 5] ≈ [17, 13] ≈ [7, 11] and [6, 9] ≈ [0, 3] and [7, 11], [0, 3]
contain cuts. There is only one equivalence class of free intervals of length
longer than 1 (up to involution), which is given by [1, 3] ∼ [17, 15] ∼ [7, 9] ∼
[11, 9] ∼ [5, 3] ∼ [13, 15].

The next lemma says that subintervals of free intervals are free again.

Lemma 28 Let [α, β] be a free interval and µ, ν such that min{α, β} ≤
µ, ν ≤ max{α, β}. Then the interval [µ, ν] is also free.

Proof. We may assume that α ≤ µ < ν ≤ β. By contradiction assume that
[µ, ν] is not free. Then there is some k ≥ 0 and some cut γ′ such that

[µ, ν] = [µ0, ν0] ∼ [µ1, ν1] ∼ · · · ∼ [µk, νk]

with min{µk, νk} < γ′ < max{µk, νk}. If k = 0, then we have a immediate
contradiction. For k ≥ 1 the relation [µ, ν] ∼ [µ1, ν1] is due to some pair
xi, xj with xi = xj or xi = xj. Since [α, β] contains no cut, we can use
the same pair to ﬁnd an interval [α1, β1] such that [α, β] ∼ [α1, β1] and
µ1, ν1 ∈ {min{α1, β1}, . . . , max{α1, β1}}. Using induction on k we see that
[α1, β1] cannot be free. A contradiction, because then [α, β] is not free. (cid:3)
Next we introduce the notion of implicit cut for non-free intervals. For our
purpose it is enough to deﬁne it for positive intervals. So, let 0 ≤ α < β ≤ m0
such that [α, β] is not free. A position γ with α < γ < β is called an implicit
cut of [α, β], if we meet the following situation. There is a cut γ′ and an
interval [α′, β′] such that

min{α′, β′} < γ′ < max{α′, β′},

[α, β] ≈ [α′, β′],
γ − α = |γ′ − α′|.

The following observation will be used throughout. If we have α ≤ µ < γ <
ν ≤ β and γ is an implicit cut of [α, β], then γ is also an implicit cut of [µ, ν].
In particular, neither [µ, ν] nor [ν, µ] is a free interval.5

5However, if γ is an implicit cut of [µ, ν], then it might happen that γ is no implicit

cut of [α, β], although [α, β] is certainly not free.

27

Lemma 29 Let 0 ≤ α ≤ α′ < β ≤ β′ ≤ m0 such that [α, β] and [α′, β′] are
free intervals. Then the interval [α, β′] is free, too.

Proof. Assume by contradiction that [α, β′] is not free. Then it contains an
implicit cut γ with α < γ < β′. By the observation above: If γ < β, then γ
is an implicit cut of [α, β] and [α, β] is not free. Otherwise, α′ < γ and α′, β′
is not free.
(cid:3)
We now consider the maximal elements. A free interval [α, β] is called maximal 
free, if there is no free interval [α′, β′] such that both α′ ≤ min{α, β} ≤
max{α, β} ≤ β′ and β′ − α′ > |β − α|. With this notion Lemma 29 states
that maximal free intervals do not overlap.

Lemma 30 Let [α, β] be a maximal free interval. Then there are intervals
[γ, δ] and [γ′, δ′] such that [α, β] ≈ [γ, δ] ≈ [γ′, δ′] and γ and δ′ are cuts.

Proof. We may assume that [α, β] is a positive interval, i.e., α < β. We show
the existence of [γ, δ] where [α, β] ≈ [γ, δ] and γ is a cut. The existence of
[γ′, δ′] where [α, β] ≈ [γ′, δ′] and δ′ is a cut follows by a symmetric argument.
If α = 0, then α itself is a cut and we can choose δ = β. Hence let 1 ≤ α
and consider the positive interval [α − 1, β]. This interval is not free, but the
only possible position for an implicit cut is α. Thus for some cut γ we have
[α − 1, β] ≈ [α′, β′] with min{α′, β′} < γ < max{α′, β′} and |γ − α′| = 1. A
simple reﬂection shows that we have [α − 1, α] ≈ [α′, γ] and [α, β] ≈ [γ, β′].
Hence we can choose δ = β′.
(cid:3)

Proposition 31 Let Γ be the set of words w ∈ Γ∗
imal free interval [α, β] with w = w0[α, β]. Then Γ is a subset of Γ+
at most 2d − 2. The set Γ is closed under involution.

0 such that there is a max0 
of size

Proof. Let [α, β] be maximal free. Then |β − α| ≥ 1 and [β, α] is maximal
free, too. Hence Γ ⊆ Γ+
0 and Γ is closed under involution. By Lemma 30 we
may assume that α is a cut. Say α < β. Then α 6= m0 and there is no other
maximal free interval [α, β′] with α < β′ because of Lemma 29. Hence there
are at most d − 1 such intervals [α, β]. Symmetrically, there are at most d − 1
maximal free intervals [α, β] where β < α and α is a cut.
(cid:3)
For a moment let Γ′
0 is the set deﬁned in Proposi0 
deﬁnes a natural projection π : Γ′
tion 31. The inclusion Γ′
0 and
a mapping h′
0 = h0π. Consider the equation with constraints
π∗(E), this is a node in the search graph, because the size of Γ is linear in d.

0 = Γ0 ∪ Γ where Γ ⊆ Γ+

0 ⊆ Γ+

0 → Γ∗

0 : Γ′

0 → M by h′

28

The reason to switch from Γ0 to Γ′
0 is that, due to the constraints, the word
w0 may have long free intervals, even in a minimal solution. Over Γ′
0 long
free intervals can be avoided. Formally, we replace w0 by a solution w′
0 where
w′
0 is based on a factorization of w0 in maximal
free intervals. There is a unique sequence 0 = α0 < α1 < · · · < αk = m0 such
that [αi−1, αi] is a maximal free interval for each 1 ≤ i ≤ k and

0 ∈ Γ∗. The deﬁnition of w′

w0 = w0[α0, αi] · · · w0[αk−1, αk].

0 and Γ′

0. Consider E′

Note that all cuts occur as some αp, therefore we can think of the factors
w0[αi−1, αi] as letters in Γ for 1 ≤ i ≤ k. Moreover, all constants which
0 ∈ Γ∗.
appear in L0R0 are elements of Γ. We replace w0 by the word w′
Then we can deﬁne σ : Ω → Γ∗ such that both σ(L0) = σ(R0) = w′
0 and
ρ0 = h′
0σ. In other terms, σ is a solution of π∗(E0). We have w0 = π(w′
0) and
exp(w′
0) ≤ exp(w0). The crucial point is that w′
0 has no long free intervals
anymore. With respect to w′
0 all maximal free intervals have length
exactly one.
In the next step we show that we can reduce the alphabet of constants to
be Γ. The inclusion of Γ in Γ′
0 deﬁnes an admissible base change β : Γ →
Γ′
0 = (Γ, h, Ω0, ρ0; L0 = R0) where h is the restriction of the
mapping h′
0). The search graph contains an
arc from E0 to E′
0, since we may choose δ to be the identity. The equation
with constraints E′
0 has a solution σ with σ(L0) = w′
0) ≤ exp(w0).
In order to avoid too many notations we identify E0 and E′
0, hence we also
assume w0 = w′
0. However, as a reminder that we have changed the alphabet
of constants (recall that some words became letters), we prefer to use the
notation Γ rather than Γ0. Thus, in what follows we shall make the following
assumptions:

0. Then we have π∗(E0) = β∗(E′

0 and exp(w′

E0 = (Γ, h, Ω0, ρ0; L0 = R0),
L0 = x1 · · · xg and g ≥ 2,
R0 = xg+1 · · · xd and d > g,
|Γ| ≤ 2d − 2,
|Ω0| ≤ 2d,

M ⊆ B2n×2n.

Moreover: All variables X occur in L0R0L0R0. There is a solution σ such
that w0 = σ(L0) = σ(R0) with σ(Xi) 6= 1 for 1 ≤ i ≤ d and ρ0 = hσ = h0σ.

29

We have |w0| = m0 and exp(w0) ∈ 2O(d+n log n). All maximal free intervals
have length exactly one, i.e., every positive interval [α, β] with β − α > 1
contains an implicit cut.
It is because of the last sentence that we have worked out the details about
free intervals. This diﬃculty is due to the constraints. Without them the
reasoning would have been much simpler. But the good news are that from
now on, the presence of constraints will not interfere very much.

Example 32 Following Example 27, we use the same equation aXX¯a =
Y ¯bY ¯abY and we consider the solution w0.
The new solution is deﬁned by replacing in w0 each factor bc by a new letter
d which represents a maximal free interval. The new w0 has the form

w0 =

0
| a

1
| d¯d

3
| ¯b

4
| ad

6
| ¯d

7
| ¯a

8
| b

9
| d¯d

11
| ¯a

12
| .

Now all maximal intervals have length one.

13 Critical Words and Blocks

In the following ℓ denotes an integer which varies between 1 and m0. For
each ℓ we deﬁne the set of critical words Cℓ by

Cℓ = { w0[γ − ℓ, γ + ℓ], w0[γ + ℓ, γ − ℓ] | γ is a cut and ℓ ≤ γ ≤ m0 − ℓ }.

We have 1 ≤ |Cℓ| ≤ 2d − 4 and Cℓ is closed under involution. Each word u ∈
Cℓ has length 2ℓ, it can be written in the form u = u1u2 with |u1| = |u2| = ℓ.
Then u1 (resp. u2) appears as a suﬃx, left of some cut and u2 (resp. u1)
appears as a preﬁx, right of the same cut.
A triple (u, w, v) ∈ ({1} ∪ Γℓ) × Γ+ × ({1} ∪ Γℓ) is called a block if ﬁrst,
up to a possible preﬁx or suﬃx no other factor of the word uwv is a critical
word, second, u 6= 1 if and only if a preﬁx of uwv of length 2ℓ belongs to
Cℓ, and third, v 6= 1 if and only if a suﬃx of uwv of length 2ℓ belongs
to Cℓ. The set of blocks is denoted by Bℓ.
It is viewed (as a possibly
inﬁnite) alphabet where the involution is deﬁned by (u, w, v) = (v, w, u).
We can deﬁne a homomorphism πℓ : B∗
ℓ → Γ∗ by πℓ(u, w, v) = w ∈ Γ+
being extended to a projection πℓ : (Bℓ ∪ Γ)∗ → Γ∗ by leaving Γ invariant.
We deﬁne hℓ
In the following we shall
consider ﬁnite subsets Γℓ ⊆ Bℓ ∪ Γ which are closed under involution. Then

: (Bℓ ∪ Γ) → M by hℓ = hπℓ.

30

ℓ → Γ∗ and hℓ : Γ∗

by πℓ : Γ∗
respective homomorphisms.
For every non-empty word w ∈ Γ+ we deﬁne its ℓ-factorization as follows.
We write

ℓ → M we understand the restrictions of the

Fℓ(w) = (u1, w1, v1) · · · (uk, wk, vk) ∈ B+

ℓ

such that w = w1 · · · wk and for 1 ≤ i ≤ k the following conditions are
satisﬁed:

• vi is a preﬁx of wi+1 · · · wk,

• vi = 1 if and only if i = k,

• ui is a suﬃx of w1 · · · wi−1,

• ui = 1 if and only if i = 1.

Note that the ℓ-factorization of a word w is unique. For k ≥ 2 we have |w1| ≥
ℓ and |wk| ≥ ℓ, but all other wi may be short. If no critical word appears as a
factor of w, then Fℓ(w) = (1, w, 1). In particular, this is the case for |w| < 2ℓ.
If we have w = puvq with |u| = |v| = ℓ and uv ∈ Cℓ, then there is a unique
i ∈ {1, . . . , k − 1} such that u = ui+1, v = vi, and pu = w1 · · · wi, vq =
wi+1 · · · wk. Thus, Fℓ(w) contains a factor (ui, wi, v)(u, wi+1, vi+1) where v is
a preﬁx of wi+1vi+1 and u is a suﬃx of uiwi. For example, the ℓ-factorization
of uv ∈ Cℓ with |u| = |v| = ℓ is

Fℓ(uv) = (1, u, v)(u, v, 1).

We deﬁne the head, body, and tail of a word w based on its ℓ-factorization

Fℓ(w) = (u1, w1, v1) · · · (uk, wk, vk)

in B∗

ℓ and Γ∗ as follows:

Headℓ(w) = (u1, w1, v1) ∈ Bℓ,
headℓ(w) = w1 ∈ Γ+,
Bodyℓ(w) = (u2, w2, v2) · · · (uk−1, wk−1, vk−1) ∈ B∗
ℓ ,
bodyℓ(w) = w2 · · · wk−1 ∈ Γ∗,
Tailℓ(w) = (uk, wk, vk) ∈ Bℓ,
tailℓ(w) = wk ∈ Γ+.

31

For k ≥ 2 (in particular, if bodyℓ(w) 6= 1) we have

Fℓ(w) = Headℓ(w)Bodyℓ(w)Tailℓ(w),
w = headℓ(w)bodyℓ(w)tailℓ(w).

Moreover, u2 is a suﬃx of w1 and vk−1 is a preﬁx of wk.
Assume bodyℓ(w) 6= 1 and let u, v ∈ Γ∗ be any words. Then we can view
w in the context uwv and Bodyℓ(w) appears as a proper factor in the ℓfactorization 
of uwv. More precisely, let

Fℓ(uwv) = (u1, w1, v1) · · · (uk, wk, vk).

Then there are unique 1 ≤ p < q ≤ k such that:

Fℓ(uwv) = (u1, w1, v1) · · · (up, wp, vp)Bodyℓ(w)(uq, wq, vq) · · · (uk, wk, vk),
w1 · · · wp = u headℓ(w),
wq · · · wk = tailℓ(w)v

Finally, we note that the above deﬁnitions are compatible with the involution.
We have Fℓ(w) = Fℓ(w), Headℓ(w) = Tailℓ(w), and Bodyℓ(w) = Bodyℓ(w).

14 The ℓ-Transformation

Our equation with constraints is E0 = (Γ, h, Ω0, ρ0; x1 · · · xg = xg+1 · · · xd).
We start with the ℓ-factorization of w0 = σ(x1 · · · xg) = σ(xg+1 · · · xd). Let

Fℓ(w0) = (u1, w1, v1) · · · (uk, wk, vk).

A sequence S = (up, wp, vp) · · · (uq, wq, vq) with 1 ≤ p ≤ q ≤ k is called
an ℓ-factor . We say that S is a cover of a positive interval [α, β], if both
|w1 · · · wp−1| ≤ α and |wq+1 · · · wk| ≤ m0 −β. Thus, w0[α, β] becomes a factor
of wp · · · wq. It is a minimal cover , if neither (up+1, wp+1, vp+1) · · · (uq, wq, vq)
nor (up, wp, vp) · · · (uq−1, wq−1, vq−1) is a cover of [α, β]. The minimal cover
exists and it is unique.
We let Ωℓ = { X ∈ Ω0 | bodyℓ(σ(X)) 6= 1 }, and we are going to deﬁne a new
left-hand side Lℓ ∈ (Bℓ ∪ Ωℓ)∗ and a new right-hand side Rℓ ∈ (Bℓ ∪ Ωℓ)∗.
For Lℓ we consider those 1 ≤ i ≤ g where bodyℓ(σ(xi)) 6= 1. Note that this
implies xi ∈ Ωℓ since ℓ ≥ 1 and then the body of a constant is always empty.

32

Recall the deﬁnition of l(i) and r(i), and deﬁne α = l(i) + |headℓ(σ(xi))|
and β = r(i) − |tailℓ(σ(xi))|. Then we have w0[α, β] = bodyℓ(σ(xi)). Next
consider the ℓ-factor Si = (up, wp, vp) · · · (uq, wq, vq) which is the minimal
cover of [α, β]. Then we have 1 < p ≤ q < k and wp · · · wq = w0[α, β] =
bodyℓ(σ(xi)). The deﬁnition of Si depends only on xi, but not on the choice
of the index i.
We replace the ℓ-factor Si in Fℓ(w0) by the variable xi. Having done this
for all 1 ≤ i ≤ g with bodyℓ(σ(xi)) 6= 1 we obtain the left-hand side Lℓ ∈
(Bℓ ∪ Ωℓ)∗ of the ℓ-transformation Eℓ. For Rℓ we proceed analogously by
replacing those ℓ-factors Si where bodyℓ(σ(xi)) 6= 1 and g + 1 ≤ i ≤ d.
For Eℓ we cannot use the alphabet Bℓ, because it might be too large or even
inﬁnite. Therefore we let Γℓ′ be the smallest subset of Bℓ which is closed
under involution and which satisﬁes LℓRℓ ∈ (Γℓ′ ∪ Ωℓ)∗. We let Γℓ = Γℓ′ ∪ Γ.
ℓ → Γ∗ and the mapping hℓ : Γℓ → M are deﬁned by
The projection πℓ : Γ∗
the restriction of πℓ : Bℓ → Γ∗, πℓ(u, w, v) = w and hℓ(u, w, v) = h(w) ∈ M
and by πℓ(a) = a and hℓ(a) = h(a) for a ∈ Γ.
Finally, we deﬁne the mapping ρℓ : Ωℓ → M by ρℓ(X) = h(bodyℓ(σ(X))).
This completes the deﬁnition of the ℓ-transformation:

Eℓ = (Γℓ, hℓ, Ωℓ, ρℓ; Lℓ = Rℓ).

Remark 33 One can verify that σℓ : Ωℓ → Γ∗
ℓ , σℓ(X) = ϕℓ(Bodyℓ(σ(X)))
deﬁnes a solution of Eℓ, where ϕℓ is the identity on Γℓ and πℓ on Bℓ \ Γℓ′.
Although, up to the trivial case ℓ = m0, we make no explicit use of this fact.

Example 34 We continue with our example aXX¯a = Y ¯bY ¯abY and the
solution σ which has been given by

w0 = | a | d¯d | ¯b | ad | ¯d | ¯a | b | d¯d | ¯a |,

where the bars show the cuts.
Up to involution, the set C1 is given by {ad, bd, ¯ab, d¯d} and C2 is given by
{d¯d¯ba, ¯d¯bad, ad¯d¯a, d¯d¯ab}. The 1-factorization of w0 can be obtained letter by
letter. The 2-factorization of w0 is given by the following sequence:

(1, ad¯d, ¯ba)(d¯d, ¯b, ad)(¯d¯b, ad, ¯d¯a)(ad, ¯d, ¯ab)(d¯d, ¯a, bd)(¯d¯a, b, d¯d)(¯ab, d¯da, 1).

Recall σ(X) = d¯d¯bad and σ(Y ) = ad¯d. Hence their 2-factorizations are
(1, ad¯d, ¯ba)(d¯d, ¯b, ad)(¯d¯b, ad, 1) and (1, ad¯d, 1), respectively.

33

By renaming letters, the 2-factorization of w0 becomes a¯bcdeb¯a and the equation 
E reduces to E2 : aXcdeX¯a = a¯bcdeb¯a since the body of σ(Y ) is empty.
The reader can check that the 3-factorization of w0 after renaming is the very
same word as the 2-factorization, but the 3-factorization of σ(X) is now one
letter, (1, d¯d¯bad, 1), so E3 becomes a trivial equation. Plandowski’s algorithm
will return true at this stage.

Remark 35 i) In the extreme case ℓ = m0, the ℓ-transformation becomes
trivial. Let a = (1, w0, 1). Then a = (1, w0, 1) and Γm0 = {a, a} ∪ Γ. Moreover,
 we have Lm0 = Rm0 = a, and hm0(a) = h(w0) ∈ M. Since Ωm0 = ∅,
the equation with constraints Em0 has trivially a solution. It is clear that Em0
is a node in the search graph, and if we reach Em0, then the algorithm will
return true.
ii) The other extreme case is ℓ = 1. The situation again is simple, but
the precise deﬁnition is technically more involved. Consider a block (u, w, v)
which appears in F1(w0). Then w = w0[α, β] for some β − α ≥ 1. We
cannot have β − α ≥ 2, because then [α, β] would have an implicit cut γ, but
w0[γ − 1, γ + 1] ∈ C1 and no critical word is a factor of w. An immediate
consequence is |Γ1| ≤ (|Γ| + 1)3 ∈ O(d3). Let X ∈ Ω0. Then Body1(σ(X)) 6=
1 if and only if |σ(X)| ≥ 3. Thus, for X ∈ Ω1 we have σ(X) = bcu = vde
with b, c, d, e ∈ Γ and u, v ∈ Γ+. It follows:

F1(σ(X)) = (1, b, c)(b, c, v2) · · · (u|v|+1, d, e)(d, e, 1).

1 as follows:

For example, for |v| = 1 this means b = u|v|+1, c = d, and v2 = e.
We can describe L1 ∈ Γ∗
For 1 ≤ i ≤ g let wi = σ(xi) and ai the last letter of σ(xi−1) if i > 1 and
a1 = 1. Let fi the ﬁrst letter of σ(xi+1) if i < g and fg = 1. Let bi the ﬁrst
letter of wi and ei the last letter of wi.
For |wi| = 1 we replace xi by the 1-factor (ai, bi, fi).
For |wi| = 2 we replace xi by the 1-factor (ai, bi, ei)(bi, ei, fi).
For |wi| ≥ 3 we let ci be the second letter of wi and di its second last. In this
case we replace xi by (ai, bi, ci)xi(di, ei, fi).
The deﬁnition of R1 is analogous. Thus, we obtain |L1R1| ≤ 3|L0R0| = 3d,
and E1 is admissible. We also see that there was an overestimation of the
size of |Γ1|. For each xi we need at most two constants together with their
involutions. Since Γ1 contains also Γ, we obtain |Γ1| ≤ 6d.

34

By the remark above, E1 and Em0 are admissible and hence nodes of the
search graph. The goal is to reach Em0 via E1 when starting with E0. For
the moment it is even not clear that the ℓ-transformations with 1 < ℓ < m0
belongs to the search graph. We prove this statement in the next section.

15 The ℓ-transformation Eℓ is admissible

Proposition 36 There is a polynomial p (of degree at most 4) such that
each Eℓ is admissible for all ℓ ≥ 1.

Proof.
It is enough to show that Lℓ and Rℓ can be represented by exponential
expressions of size O(d2(d+n log n)). Then Γℓ can have size at most O(d2(d+
n log n)) and the assertion follows. We will estimate the size of an exponential
expression for Lℓ, only.
We start again with the ℓ-transformation of

Fℓ(w0) = (u1, w1, v1) · · · (uk, wk, vk).

If k is small there is nothing to do since |Lℓ| ≤ |Fℓ(w0)|. An easy reﬂection
shows that |Lℓ| can become large, only if there is some 1 ≤ i ≤ g such
that headℓ(σ(xi)) or tailℓ(σ(xi)) is long. By symmetry we treat the case
headℓ(σ(xi)) only and we ﬁx some notation. We let 1 ≤ i ≤ g, α = l(i), and
β = α + |headℓ(σ(xi))|. Let

(up−1, wp−1, vp−1) · · · (uq+1, wq+1, vq+1)

be a minimal cover of [α, β]. We may assume that q − p is large. It is enough
to ﬁnd an exponential expression for the ℓ-factor

(up, wp, vp) · · · (uq, wq, vq)

having size in O(d(d + n log n)), because we want the whole expression to
have size in O(d2(d + n log n)).
Note that wp · · · wq is a proper factor of headℓ(σ(xi)). Hence no critical word
of Cℓ can appear as a factor inside wp · · · wq. This means there is some
p ≤ s ≤ q such that both |wp · · · ws−1| < ℓ and |ws+1 · · · wq| < ℓ. Indeed,
if |wp · · · wq−1| < ℓ, then we choose s = q. Otherwise we let p ≤ s ≤ q
be minimal such that |wp · · · ws| ≥ ℓ. Then |ws+1 · · · wq| ≥ ℓ is impossible
because us+1vs ∈ Cℓ would appear as a factor in wp · · · wq. We can write

(up, wp, vp) · · · (uq, wq, vq) = S1(us, ws, vs)S2;

35

and since (us, ws, vs) ∈ Γℓ is a letter, it is enough to ﬁnd exponential expressions 
for Si, i = 1, 2, of size O(d(d + n log n)) each. As a conclusion it is
enough to prove the following lemma.
(cid:3)
The statement of the next lemma is slightly more general as we need it above.
There we need the lemma for c = 1, but later we will apply the lemma with
values c ≤ 32d.

Lemma 37 Let c > 0 be a number and

S = (u1, w1, v1) · · · (uk, wk, vk) ∈ B∗

ℓ

be a sequence which appears as some ℓ-factor in Fℓ(w0). If we have k ≤ 3 or
|w2 · · · wk−1| ≤ cℓ, then we can represent the sequence by some exponential
expression of size O(cd(d + n log n)).

Proof. We show that there is an exponential expression of size O(d(d +
n log n)) under the assumption |w1 · · · wk| < ℓ. This is enough, because we
always can write S as a0S1a1 · · · Sc′ac′, where c′ ≤ c, the ai are letters, and
each Si satisﬁes the assumption. Note that the assumption implies u1 6=
1 6= vk and we may deﬁne uk+1 as the suﬃx of length ℓ of u1w1 · · · wk. For
1 ≤ i ≤ k let zi = ui+1vi. Then zi ∈ Cℓ is a critical word which appears
as a factor in z = u1w1w2 · · · wkvk. If the words zi, 1 ≤ i < k are pairwise
diﬀerent, then k − 1 ≤ |Cℓ| ∈ O(d) and we are done. Hence we may assume
that there are repetitions. Let j be the smallest index such that a critical
word is seen for the second time and let i < j be the ﬁrst appearance of zj.
This means for 1 ≤ i < j the words z1, · · · , zj−1 are pairwise diﬀerent and
zi = zj. Now, |w1 · · · wk| < ℓ and |zi| = 2ℓ, hence zi and zj overlap in z. We
can choose r maximal such that u1w1 · · · wi(wi+1 · · · wj)rvj is a preﬁx of the
word z. (Note that the last factor vj insures that the preﬁx ends with zj).
For some index s > j we can write

z = u1w1 · · · wi(wi+1 · · · wj)rws · · · wkvk.

We claim that zi 6∈ {zs, . . . , zk}. Indeed, let t be maximal such that zi = zt
and assume that j 6= t. Then both |wi+1 · · · wj| and |wj+1 · · · wt| are periods
of zi, but |wi+1 · · · wt| ≤ |z|. Hence by Fine and Wilf’s Theorem [16] we
obtain that the greatest common divisor of |wi+1 · · · wj| and |wj+1 · · · wt| is
a period, too. Due to the deﬁnition of an ℓ-factorization (zj was the ﬁrst

36

repetition) the length |wj+1 · · · wt| is therefore a multiple of |wi+1 · · · wj| and
we must have t = s − 1. This shows the claim. Moreover, we have

(u1, w1, v1) · · · (uk, wk, vk)

= (u1, w1, v1) · · · (ui, wi, vi)[(ui+1, wi+1, vi+1) · · · (uj, wj, vj)]r S′

where S′ = (us, ws, vs) · · · (uk, wk, vk) for s = i + 1 + r(j − i). We have
r ≤ exp(w0), hence r ∈ 2O(d+n log n). It follows that

(u1, w1, v1) · · · (ui, wi, vi)[(ui+1, wi+1, vi+1) · · · (uj, wj, vj)]r

is an exponential expression of size j+log(r) ∈ O(d+n log n). More precisely,

2O(d+n log n). By induction on the size of the set {z1, . . . , zk} we may assume
that S′ = (us, ws, vs) · · · (uk, wk, vk) has an exponential expression of size at

for some suitable constant ec its size is at most ec(d + n log n). The constant
ec depends only on the constant which is hidden when writing exp(w0) ∈
most |{zs, . . . , zk}|ec(d + n). Hence the exponential expression for S has size
ec(d + n log n) + |{zs, . . . , zk}|ec(d + n log n) ≤ |{z1, . . . , zk}|ec(d + n log n).

at most

Thus, the size is in O(d(d + n log n)).
(cid:3)
At this stage we know that all ℓ-transformations are admissible (with respect
to some suitable polynomial of degree 4). Thus E1, . . . , Em0 are nodes of the
search graph. Next we show that the search graph contains arcs E0 → E1
and Eℓ → Eℓ′ for 1 ≤ ℓ < ℓ′ ≤ 2ℓ. Hence the graph contains a path (of
logarithmic length in m0) from E0 to Em0. The non-deterministic procedure
is able to ﬁnd this path and on input E0 Plandowski’s algorithm gives the
correct answer.
In order to establish the existence of arcs from Eℓ to Eℓ′ for 0 ≤ ℓ < ℓ′ ≤
max{1, 2ℓ} we shall deﬁne intermediate equations Eℓ,ℓ′ such that there is an
admissible base change β, a projection π, and a partial solution δ with

δ∗(π∗(Eℓ)) ≡ Eℓ,ℓ′ ≡ β∗(Eℓ′).

16 The arc from E0 to E1

Recall the deﬁnition of E1 = (Γ1, h1, Ω1, ρ1; L1 = R1). The letters of Γ1 can
be written either as (a, b, c) or as b with a, c ∈ Γ ∪ {1} and b ∈ Γ. We deﬁne

37

a projection which is used here as a base change β : Γ1 → Γ by β(a, b, c) = b
and leaving the letters of Γ invariant. Clearly, h1 = hβ, and β deﬁnes an
admissible base change. Deﬁne E0,1 = β∗(E1). Then we have L0,1 = β(L1)
and R0,1 = β(R1) where β : (Γ1 ∪ Ω1)∗ → (Γ ∪ Ω1)∗ is the extension with
β(X) = X for all X ∈ Ω1. We have Γ0,1 = Γ
It is now obvious how to deﬁne the partial solution δ : Ω0 → ΓΩ1Γ ∪ Γ∗
If |σ(X)| ≤ 2, then we let δ(X) = σ(X). For
such that δ∗(E0) = E0,1.
|σ(X)| ≥ 3 we write σ(X) = aub with a, b ∈ Γ and u ∈ Γ+. Then we have
X ∈ Ω1 = Ω0,1 and we deﬁne δ(X) = aXb and ρ0,1(X) = h(u). For X ∈ Ω1
we have ρ1(X) = h(body1(σ(X))), hence ρ0,1 = ρ1, too. This shows that,
indeed, δ∗(E0) = β∗(E1). Formally, we can write this as δ∗(π∗(E0)) = β∗(E1),
where π is the identity. Hence there is an arc from E0 to E1.

17 The equations Eℓ,ℓ′ for 1 ≤ ℓ < ℓ′ ≤ 2ℓ
In this section we deﬁne for each 1 ≤ ℓ < ℓ′ ≤ 2ℓ an intermediate equation
with constraints

β∗(Eℓ′) = Eℓ,ℓ′ = (Γℓ,ℓ′, hℓ,ℓ′, Ωℓ′, ρℓ′; Lℓ,ℓ′ = Rℓ,ℓ′)

by some base change β : Γℓ′ → (Bℓ ∪ Γ)∗, then we show that β is admissible.
Recall Γ ⊆ Γℓ′ ⊆ Bℓ′ ∪ Γ. The base change β leaves the letters of Γ invariant.
Consider some (u, w, v) ∈ Γℓ′\Γ. It is enough to deﬁne β(u, w, v) or β(v, w, u).
Hence we may assume that (u, w, v) appears as a letter in the ℓ′-factorization
Fℓ′(w0). Therefore we ﬁnd a positive interval [α, β] such that w = w0[α, β]
and such that the following two conditions are satisﬁed:
1) We have u = 1 and α = 0 or |u| = ℓ′, α ≥ ℓ′, and u = w0[α − ℓ′, α].
2) We have v = 1 and β = m0 or |v| = ℓ′, β ≤ m0 − ℓ′, and v = w0[β, β + ℓ′].
Let (up, wp, vp) · · · (uq, wq, vq) be the ℓ-factor which is the minimal cover of
[α, β] with respect to the ℓ-factorization Fℓ(w0). Since ℓ ≤ ℓ′ we have
wp · · · wq = w. Moreover, the word up is a suﬃx of u and vq is a preﬁx
of v. We deﬁne

β(u, w, v) = (up, wp, vp) · · · (uq, wq, vq) ∈ B+
ℓ .

The deﬁnition does not depend on the choice of [α, β] as long as 0 ≤ α < β ≤
m0 and 1) and 2) are satisﬁed. We have β(u, w, v) = β(v, w, u) and hℓβ = hℓ′.
Now let Γℓ,ℓ′ ⊆ Bℓ ∪ Γ be the smallest subset such that β(Γℓ′) ⊆ Γ∗
ℓ,ℓ′. Then

38

Γℓ,ℓ′ contains Γ and it is closed under involution (since Γℓ′ has this property).
A crucial, but easy reﬂection shows that Γℓ ⊆ Γℓ,ℓ′. This will become essential
later.
We view β as a homomorphism β : Γ∗
ℓ,ℓ′ and deﬁne Eℓ,ℓ′ = β∗(Eℓ′).
Let us show that β deﬁnes an admissible base change. Since Eℓ′ is already
known to be admissible with respect to some polynomial of degree 4, it is
enough to ﬁnd some admissible exponential expression (again with respect
to some polynomial of degree 4) for the ℓ-factor

ℓ′ → Γ∗

β(u, w, v) = (up, wp, vp) · · · (uq, wq, vq)

where (u, w, v) ∈ Γℓ′ \ Γ. We use the same notations as above. Thus, for
some positive interval [α, β] we have wp · · · wq = w0[α, β], the word u is a
suﬃx of w0[0, α], and v is a preﬁx of w0[β, m0]. If q − p is small, there is
nothing to do. By Lemma 37 we may also assume that β − α > 32dℓ. We
are to deﬁne inductively a sequence of positions

α = α0 < α1 < · · · < αi < · · · < βi < · · · < β1 < β0 = β.

Each time we let Wi = w0[αi, βi]. Thus, W0 = wp · · · wq. Assume that
Wi = w0[αi, βi] is already deﬁned such that βi − αi ≥ 2. The interval [αi, βi]
is not free. Hence, there is some implicit cut γi with αi < γi < βi. The
word Wi is a factor of w, hence no factor of Wi belongs to the set of critical
words Cℓ′. This implies βi − γi < ℓ′ or γi − αi < ℓ′. If we have βi − γi < ℓ′
then we let αi+1 = αi and βi+1 = γi. In the other case we let αi+1 = γi and
βi+1 = βi. Thus Wi+1 is deﬁned such that Wi+1 is a proper factor of Wi with
|Wi| − |Wi+1| < ℓ′.
We need some additional book keeping. We deﬁne ri ∈ {l, r} by ri = r if
βi = βi+1 and ri = l otherwise (i.e., αi = αi+1). Furthermore the implicit cut
γi corresponds to some real cut γ′
i, β′
i]
or Wi = w0[β′
i] and
si = − otherwise (in particular, si = − implies Wi = w0[α′
i]). The triple
(γ′
i, ri, si) is denoted by γ(i). There are at most 4(d − 2) such triples and
γ(i) is deﬁned whenever Wi+1 is deﬁned. We stop the induction procedure
after the ﬁrst repetition. Thus we ﬁnd 0 ≤ i < j < 4d such that γ(i) = γ(j).
We obtain a sequence W0, W1, . . . , Wi, . . . , Wj where each word is a proper
factor of the preceding one. We have |W0| − |Wj| < 4dℓ′ ≤ 8dℓ and due to
|W0| > 32dℓ the sequence above really exists, moreover |Wj| > 8dℓ.
Next, we show that Wj has a non-trivial overlap with itself. We treat the
case γ(i) = γ(j) = (γ, r, +) only. The other three cases (γ, r, −), (γ, l, +),

i]. We deﬁne si ∈ {+, −} by si = + if Wi = w0[α′

i such that Wi = w0[α′
i, β′

i, α′

i and α′

i < γ′

i < β′

i, β′

39

and (γ, l, −) can be treated analogously. For some α′ < γ < β′ we have
Wi = w0[α′, β′] and Wi+1 = w0[γ, β′]. Thus, for some γ ≤ µ < ν ≤ β′ we have
Wj = w0[µ, ν] and we can assume that µ − γ < (j − i)ℓ′ ≤ 4dℓ′ − ℓ′ ≤ 8dℓ − ℓ′.
On the other hand we have γ(j) = (γ, r, +), too. Hence for some µ′ < γ < ν′
with γ − µ′ < ℓ′ we have Wj = w0[µ′, ν′], too. Therefore 0 < µ − µ′ < 8dℓ
and Wj has some non-trivial overlap. We can write Wj = W eW ′ such that
1 ≤ |W | < 8dℓ and W ′ is a preﬁx of W .
Putting everything together, we arrive in all cases at a factorization W0 =
U W eV with e ≤ exp(w0), 1 ≤ |W | < 8dℓ, and |U| + |V | < 16dℓ. However, we
have not ﬁnished yet. Recall that we are looking for an admissible exponential
expression for

β(u, w, v) = (up, wp, vp) · · · (uq, wq, vq).

Due to |W0| > ℓ we can choose r minimal, p < r ≤ q + 1, and s maximal
p − 1 ≤ s < q such that |wp · · · wr−1| > |U| + ℓ and |ws+1 · · · wq| > |V | + ℓ.
By Lemma 37 we may assume r < s and it is enough to ﬁnd an exponential
expression for

S = (ur, wr, vr) · · · (us, ws, vs).

Note that the word urwrwr+1 · · · wsvs is a factor of W e. Again, we may
assume that wrwr+1 · · · ws > 32dℓ. By switching to some conjugated word
W ′ if necessary, we may assume that urwrwr+1 · · · wsvs is a preﬁx of W e.
Moreover, by symmetry we may choose a positive interval [α, β] such that
w0[α, β] = urwrwr+1 · · · wsvs. Clearly, we have w0[i, j] = w0[i + |W |, j + |W |]
for all α ≤ i < j ≤ β − |W |. In particular, the critical word w0[α, α + 2ℓ]
appears as w0[α + |W |, α + |W | + 2ℓ] again. This means that there is some
r ≤ t < s such that |wr · · · wt| = |W |. More precisely, we can choose r ≤ t <
t′ ≤ s and a maximal e′ ≤ e such that

S =(cid:0)(ur, wr, vr) · · · (ut, wt, vt)(cid:1)e′

(ut′, wt′, vt′) · · · (us, ws, vs).

Since it holds e′ ≤ exp(w0), |wr · · · wt| = |W |, and |wt′ · · · ws| ≤ |W |, the
existence of an admissible exponential expression for β(u, w, v) follows. Hence
β is an admissible base change.

18 Passing from Eℓ to Eℓ,ℓ′ for 1 ≤ ℓ < ℓ′ ≤ 2ℓ

In the ﬁnal step we have to show that there exists some projection π :
ℓ,ℓ′ → Γ∗
Γ∗
ℓ,ℓ′ such that
δ∗(π∗(Eℓ)) ≡ Eℓ,ℓ′. We don’t have to care about admissibility anymore.

ℓ and some partial solution δ : Ωℓ → Γ∗

ℓ,ℓ′Ωℓ′Γ∗

ℓ,ℓ′ ∪ Γ∗

40

For the projection we have to consider a letter in Γℓ,ℓ′ \ Γℓ. Such a letter has
the form (u, w, v) ∈ Bℓ and we may deﬁne π(u, w, v) = w since Γ ⊆ Γℓ.
Clearly π((u, w, v)) = π(u, w, v) and hℓ,ℓ′(u, w, v) = hℓ′(u, w, v) = h(w) =
hℓ(π(u, w, v)) are veriﬁed. Thus π : Γ∗
ℓ deﬁnes a projection such that

ℓ,ℓ′ → Γ∗

π∗(Eℓ) = (Γℓ,ℓ′, hℓ,ℓ′, Ωℓ, ρℓ; Lℓ = Rℓ).

ℓ,ℓ′ ∪ Γ∗

ℓ,ℓ′Ωℓ′Γ∗

We have to deﬁne a partial solution δ : Ωℓ → Γ∗
ℓ,ℓ′ such that
δ(Lℓ) = β(Lℓ′) and δ(Rℓ) = β(Rℓ′). For this, we have to consider a variable
X ∈ Ω with bodyℓ(σ(X)) 6= 1. By symmetry, we may assume that X = xi
for some 1 ≤ i ≤ g. Hence σ(X) = w0[l(i), r(i)].
Let α = l(i) + |headℓ(σ(X))| and β = r(i) − |tailℓ(σ(X))|. Then l(i) + ℓ ≤
α < β ≤ r(i) − ℓ. Let (up, wp, vp) · · · (uq, wq, vq) be the minimal cover of [α, β]
with respect to the ℓ-factorization. We have wp · · · wq = bodyℓ(σ(X)).
For bodyℓ′(X) = 1 we have X ∈ Ωℓ \ Ωℓ′ and we deﬁne

δ(X) = (up, wp, vp) · · · (uq, wq, vq).

Then δ(X) ∈ B∗
ℓ and hℓδ(X) = ρℓ(X) since ρℓ(X) = h(bodyℓ(σ(X))). It is
also clear that the deﬁnition does not depend on the choice of i, and we have
δ(X) = δ(X).
Recall the deﬁnition of Lℓ′. Since bodyℓ′(σ(X)) = 1, there is a factor f1 · · · fr
ℓ′ and f1 · · · fr covers [α, β] with respect to the ℓ′-
of Lℓ′ which belongs to Γ∗
factorization Fℓ′(w0). It follows that δ(X) is a factor of β(f1 · · · fr), hence
δ(X) ∈ Γ∗
For bodyℓ′(X) 6= 1 we have X ∈ Ωℓ′ and we ﬁnd positions µ < ν such that
µ = l(i) + |headℓ′(σ(X))| and ν = r(i) − |tailℓ′(σ(X))|.
For some p ≤ r ≤ s ≤ q we have w0[α, µ] = wp · · · wr−1, w0[ν, β] =
ws+1 · · · wq, and bodyℓ′(σ(X)) = wr · · · ws. We deﬁne

ℓ,ℓ′ by deﬁnition of Γℓ,ℓ′.

δ(X) = (up, wp, vp) · · · (ur−1, wr−1, vr−1)X(us+1, ws+1, vs+1) · · · (uq, wq, vq).

As above, we can verify that δ(X) = U XV with U, V ∈ Γ∗
ℓ,ℓ′ such that
δ(X) = V X U and ρℓ(X) = hℓ,ℓ′(U)ρℓ′(X)hℓ,ℓ′(V ). Finally, δ(Lℓ) = Lℓ′ and
δ(Rℓ) = Rℓ′. Hence δ∗(π∗(Eℓ)) = β∗(Eℓ′). This proves Theorem 5.

19 Conclusion

In this paper we were dealing with the existential theory, only. For free groups
it is also known that the positive theory without constraints is decidable,

41

see [19]. Thus, one can allow also a mixture of existential and universal
quantiﬁers, if there are no negations at all. Since a negation can be replaced
with the help of an extra variable and some positive rational constraint,
one might be tempted to prove that the positive theory of equations with
rational constraints in free groups is decidable. But such a program must
fail: Indeed, by [20] and [7] it is known that the positive ∀∃3-theory of word
equations is unsolvable. Since Σ∗ is a rational subset of the free group F (Σ),
this theory can be encoded in the positive theory of equations with rational
constraints in free groups, and the later is undecidable, too. On the other
hand, a negation leads to a positive constraint of a very restricted type, so
the interesting question remains under which type of constraints the positive
theory becomes decidable.

Acknowledgments The research was partly supported by the German
Research Foundation Deutsche Forschungsgemeinschaft, DFG. In addition,
C. Guti´errez thanks Centro de Modelamiento Matem´atico, FONDAP Mate-
m´aticas Discretas, for ﬁnancial support.

References

[1] Mich`ele Benois. Parties rationelles du groupe libre. C. R. Acad. Sci.

Paris, S´er. A, 269:1188–1190, 1969.

[2] Jean Berstel. Transductions and context-free languages. Teubner Studi-

enb¨ucher, Stuttgart, 1979.

[3] Volker Diekert. Makanin’s Algorithm. In M. Lothaire, editor, Algebraic
Combinatorics on Words. Cambridge University Press, 2001. To appear.
A preliminary version is on the web:
http: //www-igm.univ-mlv.fr/ berstel/Lothaire/index.html.

[4] Volker Diekert, Claudio Guti´errez, and Christian Hagenah. The existential 
theory of equations with rational constraints in free groups is
PSPACE-complete. In A. Ferreira and H. Reichel, editors, Proc. of the
18th STACS (STACS’01), Dresden, Lecture Notes in Computer Science
2010, pages 170–182. Springer, 2001.

42

[5] Volker Diekert and Markus Lohrey. A note on the existential theory in
plain groups. International Journal of Algebra and Computation, 2001.
To appear.

[6] Volker Diekert, Yuri Matiyasevich, and Anca Muscholl. Solving word
equations modulo partial commutations. Theoretical Computer Science,
224:215–235, 1999. Special issue of LFCS’97.

[7] Valery G. Durnev. Undecidability of the positive ∀∃3-theory of a free
semi-group. Sib. Mat. Zh., 36(5):1067–1080, 1995. In Russian; English
translation: Sib. Math. J., 36 (5), 917–929, 1995.

[8] Yuri Gurevich and Andrei Voronkov. Monadic simultaneous rigid Euniﬁcation 
and related problems.
In P. Degano, R. Gorrieri, and
A. Marchetti-Spaccamela, editors, Proc. 24th ICALP (ICALP’97),
Bologna (Italy) 1997, number 1256 in Lecture Notes in Computer Science,
 pages 154–165. Springer, 1997.

[9] Claudio Guti´errez. Satisﬁability of word equations with constants is in
exponential space. In Proc. of the 39th Ann. Symp. on Foundations of
Computer Science, FOCS’98, pages 112–119, Los Alamitos, California,
1998. IEEE Computer Society Press.

[10] Claudio Guti´errez.

Satisﬁability of equations in free groups is in
In 32nd Ann. ACM Symp. on Theory of Computing

PSPACE.
(STOC’2000), pages 21–27. ACM Press, 2000.

[11] Christian Hagenah. Gleichungen mit regul¨aren Randbedingungen ¨uber
Institut f¨ur Informatik, Universit¨at

freien Gruppen. Ph.D.-thesis,
Stuttgart, 2000.

[12] John E. Hopcroft and Jeﬀrey D. Ullman.

Introduction to Automata

Theory, Languages, and Computation. Addison-Wesley, 1979.

[13] Antoni Ko´scielski and Leszek Pacholski. Complexity of Makanin’s algorithm.
 Journal of the Association for Computing Machinery, 43(4):670–
684, 1996. Preliminary version in Proc. of the 31st Annual IEEE Symposium 
on Foundations of Computer Science, Los Alamitos, 1990, 824–829.

[14] Antoni Ko´scielski and Leszek Pacholski. Makanin’s algorithm is not
primitive recursive. Theoretical Computer Science, 191:145–156, 1998.

43

[15] Dexter Kozen. Lower bounds for natural proof systems.

In Proc. of
the 18th Ann. Symp. on Foundations of Computer Science, FOCS 77,
pages 254–266, Providence, Rhode Island, 1977. IEEE Computer Society
Press.

[16] M. Lothaire. Combinatorics on Words, volume 17 of Encyclopaedia of
Mathematics and its Applications. Addison Wesley, 1983. Reprinted by
Cambridge University Press, 1997.

[17] Gennadi´ı Semyonovich Makanin. The problem of solvability of equations
in a free semigroup. Math. Sbornik, 103:147–236, 1977. English transl.
in Math. USSR Sbornik 32 (1977).

[18] Gennadi´ı Semyonovich Makanin. Equations in a free group. Izv. Akad.
Nauk SSR, Ser. Math. 46:1199–1273, 1983. English transl. in Math.
USSR Izv. 21 (1983).

[19] Gennadi´ı Semyonovich Makanin. Decidability of the universal and positive 
theories of a free group. Izv. Akad. Nauk SSSR, Ser. Mat. 48:735–
749, 1984. In Russian; English translation in: Math. USSR Izvestija,
25, 75–88, 1985.

[20] S. S. Marchenkov. Unsolvability of positive ∀∃-theory of a free semigroup.
 Sib. Mat. Zh., 23(1):196–198, 1982. In Russian.

[21] George Markowsky. Bounds on the index and period of a binary relation

on a ﬁnite set. Semigroup Forum, 13:253–259, 1977.

[22] Yuri I. Merzlyakov. Positive formulae over free groups. ALgebra i Logika,

5(4):25–42, 1966. English translation.

[23] Christos H. Papadimitriou. Computatational Complexity. Addison Wesley,
 1994.

[24] Wojciech Plandowski. Testing equivalence of morphisms on context-free
languages. In Jan van Leeuwen, editor, Algorithms—ESA ’94, Second
Annual European Symposium, volume 855 of Lecture Notes in Computer
Science, pages 460–470, Utrecht, The Netherlands, 1994. Springer.

[25] Wojciech Plandowski. Satisﬁability of word equations with constants
In Proceedings 31st Annual ACM Symposium on

is in NEXPTIME.
Theory of Computing, STOC’99, pages 721–725. ACM Press, 1999.

44

[26] Wojciech Plandowski. Satisﬁability of word equations with constants
is in PSPACE.
In Proc. of the 40th Ann. Symp. on Foundations of
Computer Science, FOCS 99, pages 495–500. IEEE Computer Society
Press, 1999.

[27] Wojciech Plandowski and Wojciech Rytter. Application of Lempel-Ziv
encodings to the solution of word equations. In Kim G. Larsen et al., editors,
 Proc. 25th ICALP (ICALP’98), Aalborg (Denmark) 1998, number
1443 in Lecture Notes in Computer Science, pages 731–742. Springer,
1998.

[28] Alexander A. Razborov. On systems of equations in a free group. Izv.
In Russian; English

Akad. Nauk SSSR, Ser. Mat. 48:779–832, 1984.
translation in: Math. USSR Izvestija, 25, 115–162, 1985.

[29] Klaus U. Schulz. Makanin’s algorithm for word equations — Two improvements 
and a generalization.
In Klaus U. Schulz, editor, Word
Equations and Related Topics, number 572 in Lecture Notes in Computer 
Science, pages 85–150. Springer, 1991.

[30] Joachim von zur Gathen and Malte Sieveking. A bound on solutions
of linear integer equalities and inequalities. Proc. Amer. Math. Soc.,
72(1):155–158, october 1978.

45

