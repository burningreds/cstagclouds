7
1
0
2

 
l
u
J
 

1
2

 
 
]
L
D
.
s
c
[
 
 

1
v
9
2
0
7
0

.

7
0
7
1
:
v
i
X
r
a

Data, Science and Society

(Notes for a talk at LEARN Conference, May 2017)∗

Version 1.0, May 4th, 2017

Claudio Gutierrez †

The foundations of experience (since we absolutely must get down to this) have been
non-existent or very weak; nor has a collection or store of particulars yet been sought or
made, able or in any way adequate, either in number, kind or certainty, to inform the
intellect. [...] Natural history contains nothing that has been researched in the proper
ways, nothing veriﬁed, nothing counted, nothing weighed, nothing measured.
Francis Bacon, 1620.1

The word “data” comes from the plural of the Latin word datum, meaning something given.2
The ﬁrst uses of the word in a scientiﬁc context can be traced back to the mid 17th century, where
the quote by Bacon suggests it arose as a means to try to conceptualize scientiﬁc research.
In
modern-day usage, the word refers to collections of measurements and factual information that
form a basis for research, reasoning, supporting evidence, etc.3 With the advent of computers in
the mid 20th century, a new meaning was added to this traditional scientiﬁc sense. It indicates
the basic abstract entities that the new machines deal with (“transmittable and storable computer
information”, 1946). Both, the scientiﬁc and computing senses remained conﬁned to rather technical
communities until recently. For example, the word “data” did not capture enough attention from
Raymond Williams to merit its inclusion in Keywords in 1983.4

The popularization of the word “data” in the news headlines, in magazine covers and in any report 
seeking to be considered scientiﬁcally grounded, is quite recent. Two metaphors are responsible
for this miracle: ﬁrst, the notion of “data deluge” and then the noun “big data”.

The notion of data deluge is powerful: an overﬂow of society and humans with data. Despite
its strong call to our daily experience, the notion is highly misleading. First, it suggests something
produced by an other, traditionally by a punishing God, or by natural powers external to our
command. Second, it makes current levels of data a catastrophe, surrounding data with a negative
connotation. In summary, it presents data as something that we can only react to, or even defend
ourselves from.

∗Data, Science, Society. Talk at LEARN Final Conference, Senate House, University of London, London, May

5th, 2017.

†Computer Science Department and Center for Web Semantics, Universidad de Chile, Chile.
1 F. Bacon, The New Organon. Edit. L. Jardine, M. Silverthorne. Cambridge Univ. Press, 2000. Aphorism

XCVIII, p. 80.

2 Despite that data is hlghly countable, in what follows, I will consider “data” as non-countable noun, like water,

love, information, oil. Hence I will treat it as singular.

3 D. Rosenberg. Data Before the Fact. American Historical Association, 2012.
4 R. Williams. Keywords, 1983.

1

The notion of big data is less deceiving. It avoids the explicit negative connotation and highlights
a main feature of the phenomenon: the size. People in computing coined the term in the early
nineties5, but the hype in bussiness (that popularized it) is more recent. In science and reaserch
the term began to be widely adopted only in the 21st century.6 My concern with this notion is
that it still represents the phenomenon as an external and unapproachable object. In fact, many
people speak (and think) of “big data” as an obscure and phantasmal object, far from the control
of mankind, haunting modern society. It is like a living nebula growing around us waiting to be
tamed and ready to run over us if not taken seriously.

1 Torrents of Data

Let us retain the essential fact: there is an enormous amount of data around. At this point one can
ask why this hype over data now. Historically, there have been several tsunamis overﬂowing the
semantic and symbolic capacities of the human. The adoption of writing and of media to preserve
it without doubt shook and transformed contemporary ways of approaching information. Later the
printing press must have produced a similar upheaval among the literate population. More recently,
magazines, newspapers and pervasive technology of printing, plus the radio and television media
overhelmed people with information. In the 1930’s, the Spanish thinker Jos´e Ortega y Gasset, in
a speech to the International Congress of Libraries and Bibliography, spoke of the “raging book”
and expressed his fears as follows:

“There are already too many books. Even when we drastically reduce the number
of subjects to which man must direct his attention, the quantity of books that he
must absorb is so enormous that it exceeds the limits of his time and his capacity of
assimilation.
[...] The culture which has liberated man from the primitive forest now
thrusts him anew into the midst of a forest of books no less inextricable and stiﬂing.”
And concluded: “Here then is the drama: the book is indispensable at this stage in
history, but the book is in danger because it has become a danger for man.”7

The book –according to Ortega– became a danger for man. We hear similar remarks about
data today. What is going on beneath the surface? The problem then and today can be stated,
paraphrasing a well known text about social change, as follows: At a certain stage of development,
the material forces of society began producing more symbolic material than the one existing social
relations can digest. From forms of development of the culture these relations turn into their fetters.
Then begins an era of information upheaval. The problem we face today is that capture, production
and digestion of data surpasses by far the social and human capabilities to manage and process it.
Let us develop this idea. The data that worries us are not billions of units of understandable
data (what scared Ortega), but the not understandability of the unit itself. Let us explain. There
are no unreadable books. Each of them was designed (written) to the be read by a human (even In
Search of the Lost Time). The obstacle Ortega was pointing to was the almost inﬁnite number of
books. The main problem was the quantity (and his solution functioned accordingly: to limit the

5 See https://bits.blogs.nytimes.com/2013/02/01/the-origins-of-big-data-an-etymological-detective-story/
6 Editorial of Nature special issue about Big data (2008) “Researchers need to adapt their institutions and practices

in response to torrents of new data”.

7 Jos´e Ortega y Gasset. Misi´on del Bibliotecario. 1935. The Mission of the Librarian. Translated by J. Lewis and

R. Carpenter. The Antioch Review, Vol. 21, No. 2, 1961, pp. 133-154.

2

human scale

beyond human






Byte B
Kilo KB
Mega MB ∼ 106
Giga GB ∼ 109 movies

∼ 100
a character
∼ 103 written text
image, music

Tera TB ∼ 1012 U.S. Congress Library
Peta PB ∼ 1015 Large data center
∼ 1018 All words ever spoken
Exa EB
Zetta ZB ∼ 1021 Amount of global data

Table 1: Data sizes and human-scale

production of books). On the other hand, the problem with data is that the unit itself (a dataset)
is not intelligible by a human, and worst, there are almost inﬁnitely many datasets. The problem
is both quality and quantity.

A metaphor from physics could help at this stage. Traditional mechanics is a human-scale
discipline in the sense that allows the direct participation of people. A bicycle is an artifact we can
understand, repair, transform almost entirely by ourselves. Chemistry ﬁrst and then atomic physics
crossed the barrier of the human manageable objects, a barrier beyond which the built-in senses of
the human do not help anymore. Today the advances in new technological media (computer power
and memory, networks, sensors, communication, etc.) has dramatically increased the capacity of
capturing data (sensors, telescopes, Web, etc.); of producing data (computers, games, media, LHC,
etc.); of storing data (memory, storage media, cloud, etc); of analyzing data (statistical techniques,
neural networks, (deep) learning, etc.). In one sentence: the limit we are surpassing today is that
of the human capabilities to understand and manipulate these vast new world of symbolic objects.8
Despite Ortega’s complaints, until very recently we humans could approach all symbolic objects
around us: texts, photographs, music, movies. But today this symbolic world is growing so fast
that escapes our “natural” human and societal capacities to handle it, and thus we feel that an
obscure and daunting, fundamentally unintelligible, (parallel) world is growing in front of our eyes.
Let us clarify. It is not that this symbolic world did not exist before. It existed and was vast, but
essentially volatile. The essential novelty is that it is being increasingly materialized in the form
of (digital) data and that ICT technologies have made us become aware of its vastness. (See some
numbers in table 1.) My hypothesis is that these objective and subjective phenomena have made
obsolete the conceptual models used to deal with the symbolic world. Among the main challenges
is the notion of scale.9

8 Hans Moravec. When will computer hardware match the human brain? Journal of Evolution and Technology.

1998. Vol. 1. http://www.jetpress.org/volume1/moravec.htm

9 Clark C. Gibson, Elinor Ostrom, T.K. Ahn. The concept of scale and the human dimensions of global change:

a survey. Ecological Economics 32 (2000) 217-239.

3

2 The Notion of Data

The audience –mostly of librarians, those rare intellectuals whose life’s objective is to contribute to
other people’s growth– is familiar with the subtle relations existing among the notions of knowledge
and information. Let us assume for now the widely accepted premise that data is in some sense the
starting point, constitutes a basic building block, of information and knowledge.10 On these lines,
let us present some facets and assumptions relevant for our understanding of this concept.

1. At the most basic and abstract level, data is a distinction, i.e., a sign of a lack of uniformity
in the world out there. As Luciano Floridi states: “As “fractures in the fabric of being” they can
only be posited as an external anchor of our information, [...] are never accessed or elaborated independently 
of a level of abstraction.”11 Caroline Haythornthwaite points to the same from another
point of view: “Datum is [the] smallest collectable unit associated with a phenomenon. Normally,
data occur in collections that are collected in order to monitor a process, assess a situation, and/or
otherwise gain a referent on a phenomenon.” 12 In summary, data is the most basic layer in the
symbolic world. Data has no meaning by itself, but is the source of meaning.

2. By data, we will mean materialized (digitally recorded) data, that is, data once it has been
frozen into material (digital) symbols. In this regard data, in the sense we are treating it in these
notes, is part of the “objective” world. Data is thus a material collection of symbols. This is
the spirit of the following entries in the deﬁnition of data: “information in numerical form that
can be digitally transmitted or processed” (Merriam-Webster) and “The quantities, characters, or
symbols on which operations are performed by a computer, which may be stored and transmitted
in the form of electrical signals and recorded on magnetic, optical, or mechanical recording media.”
(Oxford). In summary, despite its ontological ambiguity between the material and the intangible,
data is material.

3. The distinctions that deﬁne data assume an implicit context. This network of meanings is not
stated explicitly, that is, not speciﬁed in the data itself. This allows manifold interpretations of the
same data from diﬀerent points of view, to further explore new dimensions, etc. A good example
is a photograph. With high probability, the photographer took it with some agenda in mind. But
future generations could use it to “view” dimensions that were not present in the original focus
of the original photographer. Some contexts are usually explicitly included as metatada, that is,
additional data that give information or signal relations in the bulk data.13 In summary, data has
meaning, though not always explicit meaning. Although collected/constructed with some objective
in mind, it allows diverse interpretations and can support manifold thesis.

Data is the starting point for our discussion. Our task is not to clarify the ontological status of
data, but to understand its properties, its “mode of combination”, and hopefully to get a conceptual

10 Ch. Zins. Conceptual approaches for deﬁning data, information, and knowledge. Journal of the American

Society for Information Science and Technology 58, 2007. pp. 479-493

11

L. Floridi,

Semantic Conceptions

of

Information.

SEP,

version Wed

Jan

7,

2015.

https://plato.stanford.edu/entries/information-semantic/

12 C. Haythornthwaite. In Ch. Zinns, op. cit., p. 483.
13 It is important to distinguish the intrinsic metadata, what in the database ﬁeld is called a schema (describing
existing types, relationships among ﬁelds, etc.
in a dataset), from metadata describing whole datasets. Example
of the latter are the 15 classical properties of the Dublin Core Metadata Element Set (version 1.1): contributor,
coverage, creator, date, description, format, identiﬁer, language, publisher, relation, rights, source, subject, title, and
type.

4

model for it. That is, for us, as data guardians, curators, facilitators, data is just something given,
as the original Latin meaning. Our concern at this stage is not the possible semantics that could
be distilled from the data, but the data as “material” element. Using the counterpoint between
worlds of bits and atoms popularized by Nicholas Negroponte in Being Digital,14 we work in the
world of bits, a world as material as the one of atoms, but with strongly diﬀerent social signiﬁcance
as we will see.

Taking advantage of the bit-atom opposition, another metaphor could help shedding light on

the relations between these two worlds:

Data

Virtual World

=

Atoms

Material world

Pushing forward the association of ideas, the science of data would be the chemistry of the virtual
world. The sciences of information and knowledge work with this material, but at a diﬀerent level
of grouping and abstraction.

3 Research and scientiﬁc Data

The notion of research data, under terms like experience, facts, observation, evidence, etc. has a
long history. “Observation” in its scientiﬁc sense is mentioned by Aristotle; Bacon advocated its
relevance for research; and the awareness of the subleties of its connection to knowledge date from
the beginning of the 20th century.15 Nevertheless, is it only at the turn of the 21st century that
data began to be thought of as a driver of science. Turing award recipient Jim Gray wrote in 2007:

“Originally, there was just experimental science, and then there was theoretical science,
with Kepler’s Laws, Newton’s Laws of Motion, Maxwell’s equations, and so on. Then,
for many problems, the theoretical models grew too complicated to solve analytically,
and people had to start simulating. These simulations have carried us through much
of the last half of the last millennium. At this point, these simulations are generating
a whole lot of data, along with a huge increase in data from the experimental sciences.
[...] The world of science has changed, and there is no question about this.” 16

This change driven by the “material forces of society” is producing social changes, in particular

giving a prominent value to scientiﬁc data. The argument works as follows.

Since the industrial revolution there was awareness of the expanding role of (scientiﬁc) knowledge
in the economy, but only recently it has become a central player of it as the Organization for
Economic Co-operation and Development recognizes:

“The term “knowledge-based economy” results from a fuller recognition of the role of
knowledge and technology in economic growth. Knowledge, as embodied in human
beings (as “human capital”) and in technology, has always been central to economic development.
 But only over the last few years has its relative importance been recognised,

14 N. Negroponte. Being Digital. Vintage Books, New York, 1996.

15

James Bogen,

Theory

Science.
https://plato.stanford.edu/entries/science-theory-observation/

and Observation

in

SEP,

version Mar

28,

2017.

16 Jim Gray on eScience: A Transformed Scientiﬁc Method. (Based on the transcript of a talk given by Jim Gray
to the NRC-CSTB1 in Mountain View, CA, on January 11, 2007.) In: T. Hey, S. Tansley, K. Tolle. The Fourth
Paradigm. Data-Intensive Scientiﬁc Discovery. Microsoft Research, 2009.

5

just as that importance is growing. The OECD economies are more strongly dependent
on the production, distribution and use of knowledge than ever before.”17

From this statement and the premise stated above by Jim Gray (“science today is heavily based
on data”), the conclusion follows: data is the raw element of this new process of production. A
more allegorical version of this statement is: “data has become the new oil”.18

As data is playing an essential role in the economy, its production process is being under
pressure for eﬃciency. Thus division of labor is aﬀecting its cycle –data capture; data curation;
data analysis; data visualization– that traditionally was done by the same person or team. (Tycho
Brahe / Copernicus are an exception). The scientist and his collaborators designed the experiment
or the process of data collection (Von Humbolt, Darwin, Mendel, Pasteur, etc.).
In particular,
today there is an increasing tendency to separate the uses and the production/collection of data.
In this way data is acquiring a certain degree of autonomy.

Another relevant facet of scientiﬁc data is the old, but ongoing debate, about the epistemic status
of observation versus experimentation. The ﬁrst, rather direct and implicitly without touching,
without asking, the object. The second is a product of direct manipulation of the object to extract
what is needed. Bogen gives a good example: “To look at a berry on a vine and attend to its color
and shape would be to observe it. To extract its juice and apply reagents to test for the presence of
copper compounds would be to perform an experiment.”19 The distinction, if there is one, is subtle.
One can state it in computational terms as the question: Static or Dynamic data? Bulk Data or
API? 20 sThe discussion is relevant not only for how to collect or produce the data, but for how
to store it and how to deliver it to ﬁnal users. In fact we need both types of data. Today we can
“expose” live data, in the form of an API, through cameras, from sensors, etc. This is becoming an
increasingly relevant source of data. There is a growing interest in technologies devised to process
them lively, i.e., as a stream of data. Common examples are the value of currencies on the Web,
Weather channels, live news, etc.

Last, but not least, we should call the attention to the blurring diﬀerences between “scientiﬁc”
data and “common” data. Data comes in many forms and sources. One speaks of scientiﬁc data as
that collected systematically in the framework of a scientiﬁc endeavor. Today, there are huge data
companies outside what we would consider “scientiﬁc” projects or institutions, particularly at the
social level (which are among the most noisy and popular data). Tweets, identities and behavior of
users in social networks, social footprints of any kind, personal images and videos, etc., are among
the most valuable data. It is becoming everyday more diﬃcult to trace a clear divisory line between
scientiﬁc and, say, non-scientiﬁc data. At the end, all data is collected with some purpose in mind
(nobody would spend time, energy, resources to collect data that would not have some, although
far oﬀ, goal.)

17 OECD. The Knowledge-based economy. OECD, Paris 1996.
18 Seems that Clive Humby was the ﬁrst to coin this statement: “Data is just like crude.

It’s valuable,
but if unreﬁned it cannot really be used.
to create
a valuable entity that drives proﬁtable activity; so must data be broken down, analyzed for it to have value.”
http://ana.blogs.com/maestros/2006/11/data_is_the_new.html Since then, in Forbes, Fortune, Wired, etc. have
appeared articles with this idea in the title.

It has to be changed into gas, plastic, chemicals, etc.

19 J. Bogen. op. cit.
20 J. Tauberer. August 2014. https://opengovdata.io/2014/bulk-data-an-api/ (API: Application Programmer
Interface. For data, intuitively, an interface oriented to be used not by a human, but as source where applications
can be pluged to automatically interact with or retrieve data.)

6

4 The Social Character of Data

We have learned that data is everywhere; that data is relevant; that it is valuable. Not surprisingly
international organizations, governments, communities are devising ways to approach, and/or take
advantage of, this new resource.

As we saw, data is a resource that is essential to the development of scientiﬁc knowledge, and
as such, relevant to the understanding of us as humans, to the development of our societies, and to
satisfy personal human needs. On the other hand, as a “new oil”, that is, as an economic good, it
is under the tension of economic categories.

A naive approach would treat data in a similar way as knowledge, a resource that looks at ﬁrst
sight as non-excludable and non-sustractable, as Joseph Stiglitz, then Chief economist of the World
Bank, explained:

“A public good has two critical properties, non-rivalrous consumption–the consumption
of one individual does not detract from that of another–and non-excludability–it is difﬁcult 
if not impossible to exclude an individual from enjoying the good. [...] Knowledge
is a global public good requiring public support at the global level.”.21

We could change “knowledge” by data and obtain a program for data as a public good. It is,
in fact, the program of several government and international agencies. For example, the World
Bank’s focus is to make data accesible to particulars in order to “allow policymakers and advocacy
groups to make better-informed decisions and measure improvements more accurately.”22 Along
similar lines, OECD has a program for open access, deﬁned in its Principles of Access to Research
Data as follows: “Openness means access on equal terms for the international research community
at the lowest possible cost, preferably at no more than the marginal cost of dissemination. Open
access to research data from public funding should be easy, timely, user-friendly and preferably
Internet-based.” 23 From these principles follow the transparency and interoperability policies for
governments.

A good example of these initiatives in the scientiﬁc area is the U.S. National Science Foun-
dation’s open data policy, stating that “agencies must adopt a presumption in favor of openness
to the extent permitted by law and subject to privacy, conﬁdentiality, security, or other valid
restrictions.”24 They deﬁne open data as follows:

“Open data are publicly available data structured in a way to be fully accessible and
usable. This is important because data that is open, available, and accessible will help
spur innovation and inform how agencies should evolve their programs to better meet
the public’s needs.”

They state seven principles of consistency with open data, namely to be public, accessible, described,
reusable, complete, timely and managed post release.

A diﬀerent source for openness comes from the pressure of diverse communities known as “open
data” movement. Their notion of open data is essentially taken from the “open source” and “open

21 J. Stiglitz, Knowledge as a Global Public Good. In: Global Public Goods: International Cooperation in the

21st Century. 1998.

22 World Bank Open Data Initiative. World Bank, 4/30/2010. data.worldbank.org
23 OECD Principles and Guidelines for Access to Research Data from Public Funding. OECD, April 2007. p.15.
24 Open Data at NSF. https://www.nsf.gov/data/

7

access” communities. The “translation” of these notions to the world of data bears the same issues
and challenges (no less, no more) than in those ﬁelds. The Open Data Handbook 25 deﬁnes it as
follows: “Open data is data that can be freely used, re-used and redistributed by anyone –subject
only, at most, to the requirement to attribute and sharealike.” As we can see, there is here a more
ample conception than that of World Bank, OECD and international organizations and goverments,
whose openness agendas are triggered mainly by economic concerns.

5 Final Remarks: Beyond Access

Despite the advances these policies about access bring, relevant issues for data remain open.

Most approaches used to address the notion of “open data” implicitly associate it to knowledge 
and/or information, whose main threat is eﬀectively enclosure (in the form of patents and
copyright). A key assumption in this analysis is that the “good” under the threat of enclosure is
something ready to be consumed. Thus, the ultimate goal is access, that would allow people to
consume that good. This premise holds for simple data, as spreadsheets, transparency data, etc.,
but does not hold for most data today, namely “big” data. Access in this case is just a ﬁrst step
in the data cycle (collection; curation; analysis; visualization). The resources needed to store and
cure data, to analyze, and to ﬁnally visualize or use it, are tremendous. The challenge is the scale.
Here the framework of commons comes to the rescue. As Charlotte Hess and Elinor Ostrom

state,

“the essential questions for any commons analysis are inevitably about equity, eﬃciency
and sustainability. Equity refers to issues of just or equal appropriation from, and
contribution to, the maintenance of a resource. Eﬃciency deals with optimal production,
management and use of the resource. Sustainability looks at the oucomes over the long
term.”26

Due to the enormous sizes of data, to think about data as common implies including the
whole cycle of data as commons. Data is a resource shared (and produced) by groups of people.
On its intangible face, is clearly non-excludable and non-rivalrous: sharing it is almost eﬀortless;
consuming it does not substract the possibility of others to do the same. The problem comes when
we consider its material face. Here all the issues of a “material” commons surface, with its dilemmas
of commodiﬁcation or enclosure, pollution and degradation, and nonsustainability.

Data came to our societies to stay. And we already hear that data is the new oil. The allegory
can be expanded to include the history of oil on earth: It warns us of the possible conﬂicts that
the appropriation of this new resource could bring. It will depend on us, humans, to deﬁne what
we want from this new oil and how we can use it to improve our lives and societies.

The discussion we should open today is how we would like to manage and govern this new good,
including how it is generated, accessed, stored, curated, processed and delivered. The commons
approach oﬀers fresh insights to address these challenges.

25 What is Open Data? Open Data Handbook. http://opendatahandbook.org/guide/en/what-is-open-data/
26 Ch. Hess, E. Ostrom (Ed.) Understanding Knowledge as a Commons From Theory to Practice. MIT Press

2006. Introduction, p. 6.

8

