Indexed Dynamic Programming to boost
Edit Distance and LCSS Computation(cid:63)

Jérémy Barbay, Andrés Olivares

Departamento de Ciencias de la Computación, University of Chile, jeremy@barbay.cl, aolivare@dcc.uchile.cl

Abstract. There are eﬃcient dynamic programming solutions to the computation of the Edit Distance
from S ∈ [1..σ]n to T ∈ [1..σ]m, for many natural subsets of edit operations, typically in time within
O(nm) in the worst-case over strings of respective lengths n and m (which is likely to be optimal),
and in time within O(n+m) in some special cases (e.g. disjoint alphabets). We describe how indexing
the strings (in linear time), and using such an index to reﬁne the recurrence formulas underlying the
dynamic programs, yield faster algorithms in a variety of models, on a continuum of classes of instances
of intermediate diﬃculty between the worst and the best case, thus reﬁning the analysis beyond the
worst case analysis. As a side result, we describe similar properties for the computation of the Longest
Common Sub Sequence LCSS(S, T ) between S and T , since it is a particular case of Edit Distance, and we
discuss the application of similar algorithmic and analysis techniques for other dynamic programming
solutions. More formally, we propose a parameterized analysis of the computational complexity of the
Edit Distance for various set of operators and of the Longest Common Sub Sequence in function of the
area of the dynamic program matrix relevant to the computation.

1

Introduction

Given a set of edition operators on strings, a source string S ∈ [1..σ]n and a target string T ∈ [1..σ]m of
respective lengths n and m on the alphabet [1..σ], the Edit Distance is the minimum number of such operations 
required to transform the string S into the string T . Introduced in 1974 by Wagner and Fischer [16],
such computation is a fundamental problem in Computer Science, with a wide range of applications, from
text processing and information retrieval to computational biology. The typical edition distance between two
strings is deﬁned by the minimum number of insertions, deletions (in both cases, of a character at an
arbitrary position of S) and replacement (of one character of S by some other) needed to transform the
string S into T . Many generalizations have been deﬁned in the literature, including weighted costs for the edit
operations, and diﬀerent sets of edit operations – the standard set is {insertion, deletion, replacement}.
Each distinct set of correction operators yields a distinct correction distance on strings (see Figure 1
for a summary). For instance, Wagner and Fischer [16] showed that for the three following operations, the
insertion of a symbol at some arbitrary position, the deletion of a symbol at some arbitrary position, and
the replacement of a symbol at some arbitrary position, the Edit Distance can be computed in time within
O(nm) and space within O(n+m) using traditional dynamic programming techniques. As another variant of
interest, Wagner and Lowrance [17] introduced the Swap operator (S), which exchanges the positions of two
contiguous symbols. When considering only the Swap operator, one basically searches for the permutation
transforming the source string S into the target string T : some adaptive sorting technique yields a minor
improvement on the computation of the Swap Edit Distance (see appendix A). For two of the newly
deﬁned distances, the Insert Swap Edit Distance and the Delete Swap Edit Distance (equivalent
by symmetry), the best known algorithms take time exponential in the input size [4,5], which is likely to be
optimal [16]. The Edit Distance itself is linked to many other problems: for instance, given the two same
strings S ∈ [1..σ]n and T ∈ [1..σ]m, the computation of the Longest Common Sub-Sequence (LCSS) L
between S and T is equivalent to the computation of the Delete Insert Edit Distance d, as the symbols
deleted from S and inserted from T in order to “edit” S into T are exactly the same as the symbols deleted

(cid:63) Supported by project Fondecyt Regular no 1170366 from Conicyt.

8
1
0
2

 

n
u
J
 

2
1

 
 
]

R

I
.
s
c
[
 
 

1
v
7
7
2
4
0

.

6
0
8
1
:
v
i
X
r
a

Operators

Delete
Insert
Replace
Swap

(n, m)-Worst

Case Complexity

O(m) [15]
O(n) [15]
O(n) [15]
O(n2) [15]

Delete, Insert
Delete, Replace
= Insert, Replace

Delete, Swap
= Insert, Swap
Replace, Swap

Delete, Insert, Replace
Delete, Insert, Swap
Delete, Replace, Swap
Insert, Replace, Swap

Delete, Insert,
Replace, Swap

O(nm) [7]
O(nm) [18]
O(nm) [18]

NP-complete [16]
NP-complete [16]

O(nm) [18]
O(nm) [7]
O(nm) [16]
O(nm) [16]
O(nm) [16]
O(nm) [7]

Finer Results

Distance

Parikh vectors

O(n(1 + lg d/n) lg lg σ)
O(n(1 + lg d/n) lg n)

DNA

O(σ2nmγσ−1) [4]
O(σ2nmγσ−1) [4]

O(d2)
O(d2)
O(d2)

O(1.6181dm) [1]
O(1.6181dm) [1]

O(d2)
O(d2)
O(d2)
O(d2)
O(d2)
O(d2)

Fig. 1. Summary of results for various combinations of operators from the basic set {Insert, Delete,
Replace, Swap}. The column labeled “(n, m)-Worst Case Complexity” presents results in the worst case
over instances of ﬁxed sizes n and m, while the columns labeled “Finer Results” present results where the
analysis was reﬁned by various parameters: the distance d, the size σ of the alphabet, and some form of
imbalance γ = maxα∈[1..σ] min{nα, mα−nα} between the Parikh vectors of S and T . For brevity, the
only distance based on a single operator presented is the Swap Edit Distance, as the computation of
the others is always linear in the size of the input.

from S and T in order to produce L. Hence, the LCSS between S and T can be computed in time within
O(nm) and space within O(n+m) using traditional dynamic programming techniques.

Most of these computational complexities are likely to be optimal in the worst case over instances of
(Insert Edit Distance, Delete Edit
size (n, m): the algorithms computing the three basic distances
Distance and Replace Edit Distance)
in linear time are optimal as any algorithm must read the
whole strings; the Insert Swap Edit Distance and its symmetric the Delete Swap Edit Distance
are NP-hard to compute [18]; and in 2015 Backurs and Indyk [2] showed that the O(n2) upper bound for
the computation of the Delete Insert Replace Edit Distance is optimal unless the Strong Exponential
Time Hypothesis (SETH) is false.

More recently, Barbay and Pérez-Lantero [4,5], complementing Meister’s previous results [13] by the use
of an index supporting the operators rank and select on strings, described an algorithm computing this
distance in time within O(σ2nmγσ−1) in the worst case over instances where σ, n, m and γ are ﬁxed, where
γ = maxα∈[1..σ] min{nα, mα−nα} measures a form of imbalance between the frequency distributions of each
string.

Hypothesis: Given this situation, is it possible to take advantage of indexing techniques supporting rank
and select in order to speed up the computation of other edit distances? Can a similar analysis to
that of Barbay and Pérez-Lantero [4,5] be applied to other edit distances? Are there instances for which
the edit distance is easier to compute, and do such instances occur in real applications of the
computation of the edit distance?

Our Results: We answer all those questions positively, and describe general techniques to reﬁne the analysis of
dynamic programs beyond the traditional analysis in the worst case over input of ﬁxed size. More speciﬁcally,
we analyze the computational cost of four Edit Distances using various rank and select text indices, in
function of the Parikh vector [20] of the source S and target T strings. As a side result, this yields similar

2

properties for the computation of the Longest Common Sub Sequence LCSS(S, T ) between S and T , as
it can be deduced from the Delete Insert Edit Distance (LCSS(S, T ) = |S| + |T| − 2dDI (S, T )), and
deﬁnitions and techniques which can be applied to other dynamic programs. After deﬁning formally the notion
of Parikh’s vector and various index data structures supporting rank and select on strings in Section 2,
we describe the algorithms taking advantage of such techniques in Section 3: for the Longest Common
Sub Sequence and Delete-Insert Edit Distance (Section 3.1), the Delete Insert Replace Edit
Distance (Section 3.2), and ﬁnally for the Delete-Replace Edit Distance and its dual the InsertReplace 
Edit Distance (Section 3.3). We describe some preliminary experiments and their results, which
seem to indicate that those instances are not totally artiﬁcial and occur naturally in practical applications
in Section 4. We conclude in Section 5 with a discussion of other potential reﬁnement of the analysis, and
the extension of our results to other Edit Distances.

2 Preliminaries

Before describing our proposed algorithms to compute various Edit Distances, we describe formally in
Section 2.1 the notion of Parikh vector which is essential to our analysis technique; and in Section 2.2 two
key implementations of indices supporting the rank and select operators on strings.

2.1 Parikh vector
Given positive integers σ and n, a string S ∈ [1..σ]n, and the integers n1, . . . , nσ such that nα denotes the
number of occurrences of the letter α ∈ [1..σ] in the string S, the Parikh vector of S is deﬁned [20] as
p(S) = (n1, . . . , nσ).
Barbay and Pérez-Lantero [4] reﬁned the analysis of the Insert Swap Edit Distance from a string
S ∈ [1..σ]n to a string T ∈ [1..σ]m via a function of the Parikh vectors (n1, . . . , nσ) of S and (m1, . . . , mσ)
of T , the local imbalance γα = min{nα, mα − nα} for each symbol α ∈ [1..σ], projected to a global measure
of imbalance, γ = maxα∈[1..σ] γα. In the worst case among instances of ﬁxed Parikh vector, they describe
an algorithm to compute the Insert Swap Edit Distance in time within

dn + d2n ·

(cid:32) σ(cid:88)

O

(cid:33)

· (cid:89)

 ,

(mα − γα)

(γα + 1)

α=1

α∈[1..σ]

where [1..σ] = {α ∈ [1..σ] : γα > 0} if Πα∈[1..σ]γα = 0, and [1..σ] = [1..σ] \ {arg minα∈[1..σ] γα} otherwise.
This formula simpliﬁes to within O(σ2nmγσ−1) in the worst case over instances where σ, n, m and γ are
ﬁxed.

Such a vector is essential to the ﬁne analysis of dynamic programs for computing Edit Distances when
using operators whose running time depends on the number of occurrence of each symbol, such as for the
rank and select operators described in the next section.

2.2 Rank and Select in Strings
For every string X ∈ {S, L} and integer i ∈ [1..|X|], X[i] denotes the i-th symbol of X from left to right. For
every pair of integers i, j ∈ [1..|X|] such that i ≤ j, X[i..j] denotes the substring of X from the i-th symbol
to the j-th symbol, and for every pair of integers i, j ∈ [1..|X|] such that j < i, X[i..j] denotes the empty
string.
Given a symbol α ∈ [1..σ], an integer i ∈ [1..|X|] and an integer k > 0, the operation rank(X, i, α)
denotes the number of occurrences of the symbol α in the substring X[1..i], and the operation select(X, k, α)
denotes the value j ∈ [1..|X|] such that the k-th occurrence of α in X is precisely at position j, if j exists.
If j does not exist, then select(X, k, α) is null.

3

A simple way to support the rank and select operators in reasonably good time consists in, for each
symbol α ∈ [1..σ], listing all the occurrences of α in a sorted array (called a “Posting List” [21]): supporting
the select operator reduces to a simple access to the sorted array corresponding to the symbol α; while supporting 
the rank operator reduces to a Sorted Search in the same array, which can be simply implemented
by a Binary Search, or more eﬃciently in practice by a Doubling Search [6] in time within O(qα lg(nα/qα))
when supporting qα monotone queries in a posting list of size nα (for a given symbol α ∈ [1..σ].
Lemma 1. Given a string S ∈ [1..σ]n of Parikh vector (n1, . . . , nα), there exists an index using n + σ
machine words, which can be computed in time linear in the size n of S in order to support the operators
rank and select in time within O(qα lg(nα/qα)) in the comparison based decision tree model, when qα of
those queries concern the symbol α ∈ [1..σ].

Golynski et al. [10] described a more sophisticated (but asymptotically more eﬃcient) way to support
the rank and select operators in the RAM model, via a clever reduction to Y -Fast Trees on permutations
supporting the operators in time within O(lg lg σ). Barbay et al. [3] showed that it can be done on a
compressed representation of the text.
Lemma 2. Given a string S ∈ [1..σ]n, there exists an index using space within o(n lg σ) bits, which can be
computed in time linear in the size n of S in order to support the operators rank and select in time within
O(lg lg σ) in the RAM model.
We describe how to use those techniques to speed up the computation of various Edit Distances in the
following sections.

3 Adaptive Dynamic Programs

For each of the problems considered, we describe how to compute a subset of the values computed by classical
dynamic programs. We start with the computation of the Longest Common Sub Sequence (LCSS) and
the Delete Insert (DI) Edit Distance (Section 3.1) because it is the simplest; extend its results to the
computation of the Levenshtein Edit Distance (Section 3.2); and project those to the computation of
the Delete Replace (DR) Edit Distance and its symmetric Insert Replace (IR) Edit Distance
(Section 3.3).

3.1 LCSS and DI-Edit Distance
The Delete Insert Edit Distance is a classical problem in Stringology [7], if only as a variant of the
Longest Common Sub Sequence problem. It is classically computed using dynamic programming: we
describe the classical solution ﬁrst, which we then reﬁne in a simplistic way, as a pedagogical introduction
to a more sophisticated reﬁnement.

Classical solution: Given two strings S ∈ [1..σ]n and T ∈ [1..σ]m, we note dDI (n, m) the Delete Insert
Edit Distance from S to T . If the last symbols of S and T match, the edit distance is the same as the edit
distance between the preﬁxes of respective lengths n− 1 and m− 1 of S and T . Otherwise, the edit distance
is the minimum between the edit distance when inserting a copy of the last symbol of T in S (i.e. deleting
this symbol in T ) and the edit distance when deleting the mismatching symbol in T . More formally:



dDI (S[1..n], T [1..m]) =

n if m == 0;
m if n == 0;
dDI (S[1..n − 1], T [1..m − 1]) if S[n] == T [m]; and
otherwise.
1 + min

(cid:26) dDI (S[1..n − 1], T [1..m]),

(cid:27)

dDI (S[1..n], T [1..m − 1])

This recursive deﬁnition directly yields an algorithm to compute the Delete Insert Edit Distance
from S to T in time within O(nm) and space within O(n + m). We describe in the next section a technique
taking advantage of the discrepancies between the Parikh vectors of S and T .

4

A Pedagogical Example: Given two strings S ∈ [1..σ]n and T ∈ [1..σ]m, for each symbol α ∈ [1..σ], let’s
note nα and mα the number of occurrences of α respectively in S and T . Assembled in a vector, those form
the Parikh vectors (nα)α∈[1..σ] for S and (mα)α∈[1..σ] for T . Barbay and Pérez-Lantero [4] described an
algorithm to compute the Insert Swap Edit Distance which complexity is expressed in function of how
the Parikh vectors of S and T diﬀer. Likewise, we describe how those aﬀect the diﬃculty of computing
the Delete Insert Edit Distance from S to T .

Consider in Figure 2 the graphical representation MDI (S, T ) of the dynamic program computing the
Delete Insert Edit Distance from S to T , following the dynamic program described in the previous 
section. For general i ∈ [1..n] and j ∈ [1..m], the i-th value in the j-th row, a = MDI (S, T )[i, j] =
dDI (S[1..i], T [1..j]) is computed by taking the minimum between b = MDI (S, T )[i − 1, j] = dDI (S[1..i −
1], T [1..j]) and c = MDI (S, T )[i, j − 1] = dDI (S[1..i], T [1..j − 1]), the value directly on the left and directly
above it: a = min{b, c}.

1 i−1

i n

1

j−1

j

m

c↑
b ← a

S[i] (cid:54)= T [j]

1

j

m

1 i−1

i n

←
←
←
←
b ← a←
←
←
S[i] (cid:54)∈ T

1

j−1

j

m

1

i

n

c

↑ ↑ ↑ ↑ ↑ ↑ ↑

a

T [j] (cid:54)∈ S

Fig. 2. A graphical representation of the dynamic program computing the Delete Insert Edit Distance from S
to T in the general case (S[i] (cid:54)= T [j]) and in the particular case where the symbol at position i in S does not occur
in T (S[i] (cid:54)∈ T ), and where the symbol at position j in T does not occur in S (T [j] (cid:54)∈ S).

Consider a particular position i ∈ [1..n] in S such that the symbol S[i] at this position does not occur in
T (i.e. S[i] (cid:54)∈ T ): this symbol will be deleted in any edition of S into T , so that each value in the column i can
be obtained by merely duplicating the corresponding one in the column i−1. Similarly, consider a particular
position j ∈ [1..m] in T such that the symbol T [j] at this position does not occur in S (i.e. T [i] (cid:54)∈ S):
this symbol will be inserted in any edition of S into T , so that each value in the row j can be obtained by
merely duplicating the corresponding one in the row j−1. The duplication of such columns and row can be
simulated in constant time during the execution of the dynamic program, thus reducing the complexity to
within O(n(cid:48)m(cid:48) + n + m + σ) where n(cid:48) and m(cid:48) are the lengths of S and T once projected to the intersection
α,nα>0 mα. We show in the next section how to

of their eﬀective alphabets: n(cid:48) =(cid:80)

α,mα>0 nα and m(cid:48) =(cid:80)

further reﬁne this technique, in order to take advantage of rare symbols in each string.

Reﬁned Analysis: We described in the previous section how to take advantage of the fact that some
elements appear in one string, but not in the other. It is natural to wonder if a similar technique can take
advantage of cases where a symbol occurs many time in one string, but occurs only once in the other: at some
point, the dynamic program will reduce to the case described in the previous section. To be able to notice
when this happens, one would need to maintain dynamically the counters of occurrences of each symbol
during the execution of the dynamic program, or more simply pre-compute an index on S and T supporting
the operators rank and select on it.

5

Given the support for the rank and select operators on both S and T , we can reﬁne the dynamic

program to compute the distance dDI (n, m) as follows:





min

n if m == 0;
m if n == 0;
dDI (n − 1, m − 1) if S[n] == T [m];
1 + dDI (n − 1, m) if rank(S, T [m]) == 0;
1 + dDI (n, m − 1) if rank(T, S[n]) == 0;

1 + dDI (n − 1, m − 1),
n − select(S, T [m])
+ dDI (select(S, T [m], rank(S, T [m]) − 1) − 1, m − 1),
m − select(T, S[n])
+ dDI (n − 1, select(T, S[n], rank(T, S[n]) − 1)) − 1)

 otherwise.

The running time of the algorithm can then be expressed in function of the number of recursive calls,
the number of rank and select operations performed on the strings, in order to yield various running times
depending upon the solution used to support the rank and select operators.
Theorem 1. Given two strings S ∈ [1..σ]n and T ∈ [1..σ]m of respective Parikh vectors (na)a∈[1..σ] and
(ma)a∈[1..σ], the dynamic program above computes the Delete Insert Edit Distance from S to T and
the Longest Common Sub Sequence between S and T

1. through at most 4(cid:80)
2. within O((cid:80)
3. in time within O((cid:80)
4. in time within O((cid:80)

a∈[1..σ] nama) operations rank or select;

a∈[1..σ] nama recursive calls;
a∈[1..σ] nama × lg(maxa{na, ma}) × lg(nm)) in the comparison model; and
a∈[1..σ] nama × lg lg σ × lg(nm)) in the RAM memory model;

Proof. We prove point (1) by an amortization argument. Point (2) is a direct consequence of point (1), given
that each recursive call performs a ﬁnite number of calls to the rank and select operators. Point (3) is
a simple combination of Point (2) with the classical inverted posting list implementation [21] of an index
supporting the select operator in constant time and the rank operator via doubling search [6]; while point
(4) is a simple combination of Point (2) with the index described by Golynski et al. [10] to support the rank
and select operators.

Albeit quite simple, this results corresponds to real improvement in practice: see in Figure 3 how the
number of recursive calls is reduced by using such indexes. Moreover, such a reﬁnement of the analysis and
optimization of the computation can be applied to more than the Delete Insert Edit Distance: in the
next sections, we describe a similar one for computing the Levenshtein Distance (Section 3.2) and the
Delete Replace and Insert Replace Edit Distance (Section 3.3).

3.2 Levenshtein Distance, or DIR-Edit Distance
In information theory, linguistics and computer science, the Levenshtein distance is a string metric for
measuring the diﬀerence between two sequences [7]. It generalizes the Delete Insert Edit Distance
explored in the previous section by adding the Replace operator to the operators Delete and Insert (so
that it can be also called the Delete Insert Replace Edit Distance, or DIR for short). The recursion
traditionally used is a mere extension from the one described in the previous section:



dDIR(n, m) =

m if n == 0;
+∞ if n > m;
dDIR(n − 1, m − 1) if S[n] == T [m]; and
otherwise.
1 + min

(cid:26) dDIR(n, m − 1),

(cid:27)

dDIR(n − 1, m − 1)

6

The adaptive version is only a technical extension of the one for the Delete Insert Edit Distance:





min

n if m == 0;
m if n == 0;
dDIR(n − 1, m − 1) if S[n] == T [m];
1 + dDIR(n − 1, m − 1)
1 + dDIR(n − 1, m)
1 + dDIR(n, m − 1)

if rank(S, T [m]) == 0
and rank(T, S[n]) == 0 (REPLACE);

if rank(S, T [m]) == 0
but rank(T, S[n]) > 0 (DELETE);
if rank(T, S[n]) == 0
but rank(S, T [m]) > 0 (INSERT);

n − select(S, T [m])
+dDIR(select(S, T [m], rank(S, T [m]) − 1) − 1, m − 1) (DELETE) ,
m − select(T, S[n])
+dDIR(n − 1, select(T, S[n], rank(T, S[n]) − 1)) − 1) (INSERT) ,
1 + dDIR(n − 1, m − 1) (REPLACE)

 otherwise.

The reﬁned analysis yields similar results (we omit the proof for lack of space):

Theorem 2. Given two strings S ∈ [1..σ]n and T ∈ [1..σ]m of respective Parikh vectors (na)a∈[1..σ] and
(ma)a∈[1..σ], the dynamic program above computes the Levenshtein Edit Distance from S to T

1. through at most 4(cid:80)
2. within O((cid:80)
3. in time within O((cid:80)
4. in time within O((cid:80)

a∈[1..σ] nama) operations rank or select;

a∈[1..σ] nama recursive calls;
a∈[1..σ] nama × lg(maxa{na, ma}) × lg(nm)) in the comparison model; and
a∈[1..σ] nama × lg lg σ × lg(nm)) in the RAM memory model;

It is important to note that for two strings S and T , the computation of the Levenshtein Edit Distance
from S to T actually generates more recursive calls than the computation of the Delete Insert Edit
Distance from S to T , but that the analysis above does not capture this diﬀerence. In the following section,
we project this result to two equivalent edit distances, the Delete Replace and Insert Replace Edit
Distances, for which the dynamic program explores only half of the position in the dynamic program matrix
compared to the Levenshtein Edit Distance or Delete Insert Edit Distance.

3.3 DR-Edit Distance and IR-Edit Distance
Given a source string S ∈ [1..σ]n and a target string T ∈ [1..σ]m, the Delete Replace Edit Distance
from S to T and the Insert-Replace Edit Distance from T to S are the same, as the sequence of
Insert or Replace operations transforming S into T is the symmetric to the sequence of Delete or Replace
operations transforming T back into S.
As before, if the last symbols of S and T match, the edit distance is the same as the edit distance between
the preﬁxes of respective lengths n − 1 and m − 1 of S and T . Otherwise, the edit distance is the minimum
between the edit distance when inserting a copy of the last symbol of T in S (i.e. deleting this symbol in T )
and the edit distance when replacing the mismatching symbol in S by the corresponding one in T . More
formally:

dDR(S[1..n], T [1..m]) =

m if n == 0;
+∞ if n > m;
dDR(S[1..n − 1], T [1..m − 1]) if S[n] == T [m]; and
otherwise.
1 + min

(cid:26) dDR(S[1..n], T [1..m − 1]),

dDR(S[1..n − 1], T [1..m − 1])

(cid:27)



Using a few more optimizations than in Section 3.1, this recursive deﬁnition yields an algorithm to
compute the Insert Replace Edit Distance from S to T in time within O(m2) and space within O(m).

7

One can observe a few optimizations, such as that the edit distance can be computed in time linear in m as
soon as n is equal to m, as no further Insert operation can be performed.

As in the two previous sections, given the support for the rank and select operators on both S and
T , we can reﬁne the dynamic program to compute the Delete Replace Edit Distance dDR(n, m) as

follows: 

n if m == 0;
∞ if n < m;
dDR(n − 1, m − 1) if S[n] == T [m];
1 + dDR(n − 1, m) if rank(T, S[n]) == 0 (DELETE);
1 + dDR(n − 1, m − 1) if rank(S, T [n]) == 0 (REPLACE);

 n − select(S, T [m])

min

+dDR(select(S, T [m], rank(S, T [m]) − 1) − 1, m − 1) (DELETE),
1 + dDR(n − 1, m − 1) (REPLACE)

 otherwise.

The analysis from the two previous sections projects to a similar result.

Theorem 3. Given two strings S ∈ [1..σ]n and T ∈ [1..σ]m of respective Parikh vectors (na)a∈[1..σ] and
(ma)a∈[1..σ], the dynamic program above computes the Delete Replace Edit Distance from S to T

1. through at most 4(cid:80)
2. within O((cid:80)
3. in time within O((cid:80)
4. in time within O((cid:80)

a∈[1..σ] nama) operations rank or select;

a∈[1..σ] nama recursive calls;
a∈[1..σ] nama × lg(maxa{na, ma}) × lg(nm)) in the comparison model; and
a∈[1..σ] nama × lg lg σ × lg(nm)) in the RAM memory model;

Parameterizing the analysis of the computation of the Longest Common Sub Sequence, of the Levenshtein 
Edit Distance and of the Delete Replace or Insert Replace Edit Distance would be
only of moderate theoretical interest, if it did not correspond to some correspondingly “easy” instances in
practice. In the next section we describe some preliminary experimental results which seem to indicate the
existence of such “easy” instances in information retrieval.

4 Experiments

In order to test the practicality of the parameterization and algorithms described in the previous section,
we performed some preliminary experiments on some public data sets from the Gutemberg project [12].
We describe the data set and experimental setup in Section 4.1, and the preliminary results and their
interpretation in Section 4.2.

4.1 Data Sets

Started by Michael Hart in 1971 [19], the Gutemberg project gathers electronic copies of public domain
books, and as such is a publically available data set for testing algorithms on real text. We considered
each text as a sequence of words (hence considering as equivalent all the word separations, from blank
spaces to punctuations and line jumps), which results in large alphabets. Due to some problems with the
implementation, we could not run the algorithms for texts larger than 32kB (a memory issue with a library in
Python), so we extracted the ﬁrst 32kB of the texts “Romeo & Juliet” (English), “Romeo & Julia” (German),
“Hamlet” (German), and “Punch or the London chivalry vol 99” (English); the last text being a randomly
picked non Shakespeare text.

8

Fig. 3. Experimental results for the computation of the Delete Insert Edit Distance (and, by extention,
 of the computation of the Longest Common Sub Sequence) by the adaptive algorithm described
in Section 3.1.

Fig. 4. Experimental results for the computation of the Levenshtein Edit Distance by the adaptive
algorithm described in Section 3.2.

9

4.2 Experimental Results

Figures 3, 4 and 5 show the number of recursive calls from the main recursive function for four pairs of texts
for each algorithm described in Section 3: “Romeo & Juliet” (English) vs “Punch or the London chivalry vol
99” (English), “Romeo & Juliet” (English) vs “Romeo & Julia” (German), “Romeo & Juliet” (English) vs
“Hamlet” (English), and “Romeo & Julia” (German) vs “Hamlet” (German).

For the three types of Edit Distances and the four pairs of texts, the adaptive variants perform less
recursive calls. For the three types of Edit Distances, the diﬀerence in the number of recursive calls is
less between the two texts from the same author (i.e. “Romeo & Juliet” (English) vs “Hamlet” (English) and
“Romeo & Julia” (German) vs “Hamlet” (German)), because the vocabulary is the same, and is the most
between texts of distinct languages (i.e. “Romeo & Juliet” (English) vs “Romeo & Julia” (German)), because
the vocabulary (i.e. the alphabet) is mostly distinct. Still, for two texts in the same language, but from
distinct authors (i.e. “Romeo & Juliet” (English) vs “Punch or the London chivalry vol 99” (English)), the
diﬀerence is quite sensible.

Obviously, those experimental results are only preliminary, and a more thorough study is needed (and
underway), both with a larger data set and with a larger range of measures, from the running time with
various indexing data structures supporting the operators rank and select, to the number of entries of the
dynamic program matrix being eﬀectively computed. We discuss additional perspectives for future work in
the next section.

5 Discussion

We have shown how the computation of other Edit Distances than the Insert Swap and Delete Swap
Edit Distance is also sensitive to the Parikh vectors of the input. We discuss here various directions in
which these results can be extended, from the possibility of proving conditional lower bounds in the reﬁned
analysis model, to further reﬁnements of the analysis for these same Edit Distances, and to the analysis
of other dynamic programs.

Adaptive Conditional Lower Bounds: Backurs and Indyk [2] showed that the O(n2) upper bound for
the computation of the Delete Insert Replace Edit Distance is optimal unless the Strong Exponential
Time Hypothesis (SETH) is false, and since then the technique has been applied to various other related
problems. Should the reduction from the SETH to the Edit Distance computation be reﬁned as shown
here for the upper bound, it would speak in favor of the optimality of the analysis.

Other measures of diﬃculty: Abu-Khzam et al. [1] described an algorithm computing the Insert Swap
Edit Distance d from S to T in time within O(1.6181dm), which is polynomial in the size of the input
if exponential in the output size d. The output distance d itself can be as large as n, but such instances
are not necessarily diﬃcult: Barbay and Pérez-Lantero [4] showed that the gap vector between the Parikh
vectors separate the hard instances from the easy one, and we showed that the same applied to other Edit
Distances.

But still, among instances of ﬁxed input size, output distance, and imbalance between the Parikh
vectors, there are instances easier than others (e.g. the computation of the Insert Swap Edit Distance
on an instance where all the insertions are in the left part of S while all the swaps are in the right part of
S). A measure which to reﬁne the analysis would be the cost of encoding a certiﬁcate of the Edit Distance,
one which is easier to check than recomputing the distance itself.

Indexed Dynamic Programming: Our results are close in spirit to those in ﬁxed-parameter complexity,
but with an important diﬀerence, namely, trying to spot one or more parameters that explain what makes an
instance hard or easy. For the computation of the Insert Swap and Delete Swap Edit Distances, the
size of the alphabet d makes the diﬀerence between polynomial time and NP-hardness. However, Barbay and

10

Pérez-Lantero [4] showed that diﬀerent instances of the same size can exhibit radically diﬀerent costs-for a
given ﬁxed algorithm. The parameterized analysis captures in parameters the cause for such cost diﬀerences.
We described how the same logic applies to other types of Edit Distances, and it is likely that similar
situations happen with many other algorithms based on dynamic programming, such as the computation of
the Fréchet Distance [11], the Discrete Fréchet Distance [9] and the decision problem Orthogonal
Vector [8].

Acknowledgments: The author would like to thank Pablo Pérez-Lantero for introducing the problem of
computing the Edit Distance between strings; Felipe Lizama for a semester of very interesting discussions
about this approach; and an anonymous referee from the journal Transaction on Algorithms for his positive
feedback and encouragement. Funding: Jérémy Barbay is partially funded by the project Fondecyt Regular
no. 1170366 from Conicyt. Data and Material Availability: The source of this article, along with the
code and data used for the experiments described within, will be made publicly available upon publication
at the url https://github.com/FineGrainedAnalysis/EditDistances.

References

1. Abu-Khzam, F.N., Fernau, H., Langston, M.A., Lee-Cultura, S., Stege, U.: Charge and reduce: A ﬁxed-parameter

algorithm for string-to-string correction. Discrete Optimization (DO) 8(1), 41 – 49 (2011)

2. Backurs, A., Indyk, P.: Edit distance cannot be computed in strongly subquadratic time (unless SETH is false).

In: Proceedings of the annual ACM Symposium on Theory Of Computing (STOC) (2015)

3. Barbay, J., Claude, F., Gagie, T., Navarro, G., Nekrich, Y.: Eﬃcient fully-compressed sequence representations.

Algorithmica (ALGO) 69(1), 232–268 (2014)

4. Barbay, J., Pérez-Lantero, P.: Adaptive computation of the swap-insert correction distance. In: Proceedings of

the Annual Symposium on String Processing and Information Retrieval (SPIRE). pp. 21–32 (2015)

5. Barbay, J., Pérez-Lantero, P.: Adaptive computation of the swap-insert correction distance. In: ACM Transactions

on Algorithms (TALG) (2018), accepted on [2018-05-25 Fri], to appear.

6. Bentley, J.L., Yao, A.C.C.: An almost optimal algorithm for unbounded searching. Information Processing Letters

(IPL) 5(3), 82–87 (1976)

7. Bergroth, L., Hakonen, H., Raita, T.: A survey of longest common subsequence algorithms. In: Proceedings of

the 11th Symposium on String Processing and Information Retrieval (SPIRE). pp. 39–48 (2000)

8. Bringmann, K.: Why walking the dog takes time: Fréchet distance has no strongly subquadratic algorithms unless
SETH fails. In: Proceedings of the 2014 IEEE 55th Annual Symposium on Foundations of Computer Science. pp.
661–670. FOCS ’14, IEEE Computer Society, Washington, DC, USA (2014)

9. Eiter, T., Mannila, H.: Computing discrete Fréchet distance. Tech. rep., Christian Doppler Labor für Expertensyteme,
 Technische Universität Wien (1994)

10. Golynski, A., Munro, J.I., Rao, S.S.: Rank/select operations on large alphabets: A tool for text indexing. In:
Proceedings of the Seventeenth Annual ACM-SIAM Symposium on Discrete Algorithm (SODA). pp. 368–373.
SODA ’06, Society for Industrial and Applied Mathematics, Philadelphia, PA, USA (2006)

11. H., A., M., G.: Computing the Fréchet distance between two polygonal curves. International Journal of Computational 
Geometry and Applications (IJCGA) 5(1–2), 75–91 (1995)

12. Hart, M.: Gutemberg project. Online at https://www.gutenberg.org/ (last accessed on [2018-05-27 Sun])
13. Meister, D.: Using swaps and deletes to make strings match. Theoretical Computer Science (TCS) 562(0), 606 –

14. Moﬀat, A., Petersson, O.: An overview of adaptive sorting. Australian Computer Journal (ACJ) 24(2), 70–77

620 (2015)

(1992)

(2013)

173 (1974)

15. Spreen, T.D.: The Binary String-to-String Correction Problem. Master’s thesis, University of Victoria, Canada

16. Wagner, R.A., Fischer, M.J.: The string-to-string correction problem. Journal of the ACM (JACM) 21(1), 168–

17. Wagner, R.A., Lowrance, R.: An extension of the string-to-string correction problem. Journal of the ACM (JACM)

22(2), 177–183 (1975)

18. Wagner, R.A.: On the complexity of the extended string-to-string correction problem. In: Proceedings of the

annual ACM Symposium on Theory Of Computing (STOC). pp. 218–223. STOC ’75, ACM (1975)

11

19. Wikipedia: Project gutenberg. Online at https://en.wikipedia.org/wiki/Project_Gutenberg (last accessed

on [2018-05-27 Sun])

20. Wikipedia, Website.: Parikh’s theorem, last accessed on 2017-05-08.
21. Witten, I.H., Moﬀat, A., Bell, T.C.: Managing gigabytes : compressing and indexing documents and images.
The Morgan Kaufmann series in multimedia information and systems, San Francisco, Calif. Morgan Kaufmann
Publishers (1999)

12

APPENDIX

In this appendix, we brieﬂy discuss some minor topics, such as how the algorithm Local Insertion
Sort described and analyzed by Moﬀat and Petersson [14] combined with an index supporting the rank and
select operators potentially yields a faster computation of the Swap Edit Distance (Section A), or how
to combine techniques that take advantage of the Parikh vectors of the input strings with techniques that
take advantage of the output distance (Section B).

A Adaptive Computation of the Swap Edit Distance
Out of the 24 − 1 = 15 non trivial distances which can be obtained from the four operators Delete, Insert,
Replace and Swap, the Swap Edit Distance is the simplest for which no linear time algorithm is known:
Wagner and Lowrance [17] described a dynamic program to compute it in time within O(n2). As the swap
operator does not remove nor insert any symbol, the Swap Edit Distance between two strings of distinct
lengths, or of same lengths but with distinct Parikh vectors, is always inﬁnite. Such cases can be checked
in time within O(n + m + σ), and can be ignored as degenerated cases.
The computation of the Swap Edit Distance between two strings S, T ∈ [1..σ]n of same Parikh
vector (ni)i∈[1..σ] is quite similar to the problem of Sorting one string into the other via the exchange of
consecutive elements: the swap operator is merely reordering the symbols of the strings, and the combined
actions of all the swap operations can be summarized by a simple permutation over [1..n]. Consider the
shortest sequence of such swap operators transforming S into T , and π the corresponding permutation. The
number d of inversions in π, deﬁned as the number of pairs i, j ∈ [1..n] of positions i < j such that π[j] < π[j]
(i.e. the order of (i, j) is inverse to that of (π[i], π[j])), is exactly the number d of swaps required to “reorder”
S into T , i.e. the Swap Edit Distance d from S to T .

Moﬀat and Petersson [14] described two sorting algorithms adaptive to the number d of inversions of
the input. The ﬁrst one is the classical sorting algorithm Insertion Sort, which sorts an array A with d
inversions using only the operator swaps, using O(d) ⊆ O(n2) comparisons and swaps. The second one is
the adaptive algorithm Local Insertion Sort, based on a Finger Search Tree, which uses only O(n(1 +
lg(d/n))) comparisons and can be easily modiﬁed to count the number d of inversions, i.e. the Swap Edit
Distance d between an array A and its sorted version. We describe below how to take advantage of this
algorithm to compute the Swap Edit Distance between two arbitrary strings.

First, consider the one-to-one mapping between positions in S and positions in T :

Lemma 3. Given two strings S, T ∈ [1..σ]n of same Parikh vector, the i-th symbol α in S is mapped to
the i-th symbol α in T by the shortest sequence of Swap operations transforming S into T .

Proof. As Swap is the only operator available, no symbol is added or removed from S to obtain T , so that
there is a one to one mapping between each symbol of S and each symbol of T . Moreover, any sequence of
Swap operation matching the i-th symbol α in T to the j-th occurrence of α in T can be made shorter if
j (cid:54)= i.

Then, consider how to compute the Swap Edit Distance using such a mapping and an index supporting

the rank and select operators:
Theorem 4. Given two strings S, T ∈ [1..σ]n of same Parikh vector (ni)i∈[1..σ], there is an algorithm
computing the Swap Edit Distance from S to T via O(n(1 + lg(d/n)) rank and select operations.

Proof. Deﬁne the following process to decide if the symbols at positions i and j in S must be inversed
during the transformation of S into T minimizing the number of Swap operations: Let a = S[i] and b = S[j],
so that i and j are the rank(S, a, i)-th and rank(S, b, j)-th occurences of a and b in S, respectively. Let
i(cid:48) = select(T, a, rank(S, a, i)) and j(cid:48) = select(T, b, rank(S, b, j)) be the positions of the corresponding
occurences in T . The symbols will be inversed during the transformation of S into T if and only if (i, j) and
(i(cid:48), j(cid:48))’s orders are inversed.

13

“Sorting” S into T using the algorithm Local Insertion Sort described by Moﬀat and Petersson [14]
and the process described above to answer comparisons between π(i) and π(j) yields an algorithm computing
the Swap Edit Distance using within O(n(1 + lg(d/n)) rank and select operarions.

This yields as many solutions as there are data structures to support the rank and select operators,
each yielding a distinct computational tradeoﬀ on the previous lemma: we describe two. The ﬁrst one is
based on inverted posting lists [21] and an amortized analysis of doubling search algorithm [6]:
Corollary 1. Given two strings S, T ∈ [1..σ]n of same Parikh vector (ni)i∈[1..σ], there is an algorithm
computing the Swap Edit Distance from S to T in time within O(n(1 + lg(d/n)) lg(n)) ⊂ O(n2) in the
comparison based decision tree model.

Proof. A simple combination of Theorem 4 with the classical inverted posting list implementation [21] of an
index supporting the select operator in constant time and the rank operator via doubling search[6].

Note that it should be possible to reﬁne the analysis, as O(lg n) is a very crude upper bound on the
complexity of supporting rank or select, one should be able to express it in function of nα, and to amortize
it over all rank and select operations for each symbol α.

The second one is based on the more sophisticated succinct data structure described by Golynski et

al. [10]:
Corollary 2. Given two strings S, T ∈ [1..σ]n, there is an algorithm computing the Swap Edit Distance
from S to T in time within O(n(1 + lg(d/n)) lg lg σ).

Proof. A simple combination of Theorem 4 with the index described by Golynski et al. [10] to support the
rank and select operators.

Next, we discuss the minor topic of combining techniques that take advantage of the Parikh vectors of

the input strings with techniques that take advantage of the output distance.

B Distance Adaptive Computation for all Edit Distances

Abu-Khzam et al. [1] described an algorithm computing the Insert Swap Edit Distance d from S to T in
time within O(1.6181dm), which is adaptive to the distance d. We show here that a similar technique can be
applied to the other edit distances based on the operators Delete, Insert and Replace, so that to obtain
a complexity adaptive to the distance d being computed (Section B.1) and to combine this technique with
the ones described previously (Section B.2).

B.1 Distance Adaptive Computation
Lemma 4. For any edit distance based on a subset of the set of operators {Delete, Insert, Replace },
there is an algorithm which checks that the distance d from a source string S ∈ [1..σ]n to a target string
T ∈ [1..σ]m is smaller than a promise D ∈ [0.. max{n, m}] (i.e. if d ≤ D) in time within O(D min{n, m})
and space within O(n + m).
Theorem 5. For any edit distance based on a subset of the set of operators {Delete, Insert, Replace },
there is an algorithm which computes this distance d from a source string S ∈ [1..σ]n to a target string
T ∈ [1..σ]m in time within O(d min{n, m}) and space within O(n + m).

B.2 Combination with Other Adaptive Techniques
Corollary 3. There is an algorithm which computes this Delete Insert Edit Distance d from a source
string S ∈ [1..σ]n to a target string T ∈ [1..σ]m in time within O(d min{n(cid:48), m(cid:48)}) and space within O(n + m),

where n(cid:48) =(cid:80)

α,mα>0 nα and m(cid:48) =(cid:80)

α,nα>0 mα.

14

