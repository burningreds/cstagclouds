6
1
0
2

 

n
a
J
 

9
2

 
 
]
S
D
.
s
c
[
 
 

1
v
3
2
0
0
0

.

2
0
6
1
:
v
i
X
r
a

Optimal Preﬁx Free Codes

with Partial Sorting

Jérémy Barbay

Departmento de Ciencias de la Computación,

University of Chile,

jeremy@barbay.cl

(Full version)

Abstract. We describe an algorithm computing an optimal preﬁx free code for n unsorted positive weights in time
within O(n(1 + lg α)) ⊆ O(n lg n), where the alternation α ∈ [1..n − 1] measures the amount of sorting required
by the computation. This asymptotical complexity is within a constant factor of the optimal in the algebraic decision
tree computational model, in the worst case over all instances of size n and alternation α. Such results reﬁne the
state of the art complexity of Θ(n lg n) in the worst case over instances of size n in the same computational model, a
landmark in compression and coding since 1952, by the mere combination of van Leeuwen’s algorithm to compute
optimal preﬁx free codes from sorted weights (known since 1976), with Deferred Data Structures to partially sort a
multiset depending on the queries on it (known since 1988).

Keywords: Deferred Data Structure, Huffman, Median, Optimal Preﬁx Free Codes, van
Leeuwen.

1 Introduction
Given n positive weights W [1..n] coding1 for the frequencies nW [i]/Pn
of n messages2,
and a number D of output symbols, an OPTIMAL PREFIX FREE CODE [13] is a set of n code strings on
alphabet [1..D], of variable lengths L[1..n] and such that no string is preﬁx of another, and the average length
of a code is minimized (i.e. Pn
i=1 L[i]W [i] is minimal). The particularity of such codes is that even though
the code strings assigned to the messages can differ in lengths (assigning shorter ones to more frequent
messages yields compression to Pn
i=1 L[i]W [i] symbols), the preﬁx free property insures a non-ambiguous
decoding.

j=1 W [j]oi∈[1..n]

Such optimal codes, known since 1952 [13], are used in “all the mainstream compression formats” [8]
(e.g. PNG, JPEG, MP3, MPEG, GZIP and PKZIP). The concept is “one of the fundamental ideas that people
in computer science and data communications are using all the time” (Knuth [23]), and the code itself
is “one of the enduring techniques of data compression. It was used in the venerable PACK compression
program, authored by Szymanski in 1978, and remains no less popular today” (Moffat et al. [20] in 1997).

1.1 Previous works

Any preﬁx free code can be computed in linear time from a set of code lengths satisfying the Kraft inequality 
Pn
i=1 D−L[i] ≤ 1. The original description of the code by Huffman [13] yields a heap-based algorithm
1 We note [i..j] = {i, i + 1, . . . , j} the integer range from i to j, and A[i..j] = {A[i], A[i + 1], . . . , A[j]} the set of values of an

array A indexed within this range.

2 We use the terminology of messages for the input and symbols for the output, as introduced by Huffman [13], which should not
be confused with other terminologies found in the literature, of input symbols, letters or words for the input and output symbols
or bits in the binary case.

2performing O(n log n) algebraic operations, using the bijection between D-ary preﬁx free codes and D-ary
cardinal trees [11]. This complexity is asymptotically optimal for any constant value of D in the algebraic decision 
tree model, in the worst case over instances composed of n positive weights, as computing the optimal
binary preﬁx free code for the weights W [0, . . . , Dn] = {Dx1, . . . , Dx1, Dx2, . . . , Dx2, . . . , Dxn, . . . , Dxn}
is equivalent to sorting the positive integers {x1, . . . , xn} . We consider here only the binary case, where
D = 2. Not all instances require the same amount of work to compute an optimal code (see Table 1 for a
partial list of relevant results):

– When the weights are given in sorted order, van Leeuwen [16] showed that an optimal code can be

computed using within O(n) algebraic operations.

– When the weights consist of r ∈ [1..n] distinct values and are given in a sorted, compressed form, Moffat
and Turpin [21] showed how to compute an optimal code using within O(r(1 + log(n/r))) algebraic
operations, which is often sublinear in n.

– In the case where the weights are given unsorted, Belal et al. [5,6] described several families of instances
for which an optimal preﬁx free code can be computed in linear time, along with an algorithm claimed
to perform O(kn) algebraic operations, in the worst case over instances formed by n weights such that
there is an optimal binary preﬁx free code with k distinct code lengths3. This complexity was later
downgraded to O(16kn) in an extended version[4] of their article. Both results are better than the state
of the art when k is ﬁnite, but worse when k is larger than log n.

Ref. Note
[13] original
[16] Sorted Input
[19] Sorted Input

Time
O(n log n)
O(n)
O(n)
O(r(1 + log(n/r))) "efﬁcient" [21] Compressed Input/Output
O(log2k−1 n)
O(kn) claimed
O(16kn) proved
O(n(1 + log α))

[5] Sorted Input
[5] k distinct code lengths
[4] k distinct code lengths

Space
O(n)
O(n)
O(1)

O(n)
O(n)
O(n)

Year Name
1952 Huffman
1976 van Leeuwen
1995 Moffat and Katajainen
1998 Moffat and Turpin
2006 Belal and Elmasry
2006 Belal and Elmasry
2006 Belal and Elmasry
2016 Grouping-Docking-Mixing

[here] α = |S|EI ∈ [1..n − 1]

Table 1. A selection of results on the computational complexity of optimal preﬁx free codes. k is the number of distinct codelengths
produced. α = |S|EI ∈ [1..n − 1] is a difﬁculty measure, the number of alternation between External nodes and Internal nodes in
an execution of van Leeuwen [16]’s algorithm. Note that there can be various optimal codes for any given set of weights, each with
a distinct number of distinct code lengths k.

1.2 Contributions

In the context described above, various questions are left unanswered, from the conﬁrmation of the existence 
of an algorithm running in time O(16kn) or O(kn), to the existence of an algorithm taking advantage
of small values of both n and k, less trivial than running two algorithms in parallel and stopping both whenever 
one computes the answer. Given n positive integer weights, can we compute an optimal binary preﬁx
free code in time better than O(min{kn, n log n}) in the algebraic model? We answer in the afﬁrmative for
many classes of instances, identiﬁed by the alternation measure α deﬁned in Section 3.1:

3 Note that k is not uniquely deﬁned, as for a given set of weights there can exist several optimal preﬁx free codes varying in the

number of distinct code lengths used.

3
Theorem 1. Given n positive weights of alternation α ∈ [1..n − 1], there is an algorithm which computes
an optimal binary preﬁx free code using within O(n(1+ log α)) ⊆ O(n lg n) algebraic instructions, and
this complexity is asymptotically optimal among all algorithms in the algebraic decision tree computational
model in the worst case over instances of size n and alternation α.

Proof. We describe in Lemma 2 a deferred data structure which supports q queries of type rank, select
and partialSum in time within O(n(1 + lg q)), all within the algebraic computational model, and describe 
in Section 2.3 an algorithm using such a data structure to compute optimal preﬁx free codes given
an unsorted input. We show in Lemma 9 that any algorithm A in the algebraic computational model performs 
within Ω(n lg α) algebraic operations in the worst case over instances of size n and alternation α.
We show in Lemma 6 that the GDM algorithm, a variant of the van Leeuwen’s algorithm [16], modiﬁed to
use the deferred data structure from Lemma 2, performs q ∈ O(α(1 + lg n−1
α )) such queries, which yields
in Corollary 7 a complexity within O(n(1+ log α) + α(lg n)(lg n
α )), all within the algebraic computational
model. As α ∈ [1..n−1] and O(α(lg n)(lg n
α )) ⊆ O(n(1+ log α)) for this range (Lemma 8), the optimality
ensues.
⊓⊔

When α is at its maximal (i.e. α = n−1), this complexity matches the tight computational complexity
bound of Θ(n lg n) for algebraic algorithms in the worst case over all instances of size n. When α is substantially 
smaller than n (e.g. α ∈ O(lg n)), the GDM algorithm performs within o(n lg n) operations, down
to linear in n for ﬁnite values of α.

We discuss our solution in Section 2 in three parts: the intuition behind the general strategy in Section
2.1, the deferred data structure which maintains a partially sorted list of weights while supporting rank,
select and partialSum queries in Section 2.2, and the algorithm which uses those operators to compute
an optimal preﬁx free code in Section 2.3. Our main contribution consists in the analysis of the running time
of this solution, described in Section 3: the formal deﬁnition of the parameter of the analysis in Section 3.1,
the upper bound in Section 3.2 and the matching lower bound in Section 3.3. We conclude with a comparison
of our results with those from Belal et al. [5] in Section 4.

2 Solution

The solution that we describe is a combination of two results: some results about deferred data structures
for multisets, which support queries in a “lazy” way; and some results about optimal preﬁx free codes
themselves, about the relation between the computational cost of sorting a set of positive integers and the
computational cost of computing an optimal preﬁx free code for the corresponding frequency distribution.
We describe the general intuition of our solution in Section 2.1, the deferred data structure in Section 2.2,
and the algorithm in Section 2.3.

2.1 General Intuition

Observing that the algorithm suggested by Huffman [13] always creates the internal nodes in increasing
order of weight, van Leeuwen [16] described an algorithm to compute optimal preﬁx free codes in linear
time when the input (i.e. the weights of the external nodes) is given in sorted order.

A close look at the execution of van Leeuwen’s algorithm [16] reveals a sequence of sequential
searches for the insertion rank r of the weight of an internal node in the list of weights of external
nodes. Such sequential search could be replaced by a more efﬁcient search algorithm in order to reduce the
number of comparisons performed (e.g. a doubling search [7] would ﬁnd such a rank r in 2⌈log2 r⌉
comparisons).

4Example 1. Consider an instance of the optimal preﬁx free code problem formed by n sorted positive
weights W [1..n] such that the ﬁrst internal node created is bigger than the largest weight (i.e. W [1]+W [2] >
W [n]). On such an instance, van Leeuwen’s algorithm [16] starts by performing n − 2 comparisons in
the equivalent of a sequential search in W for W [1]+W [2]: a binary search would perform
⌈log2 n⌉ comparisons instead.

Of course, any algorithm must access (and sum) each weight at least once in order to compute an optimal
preﬁx free code for the input, so that reducing the number of comparisons does not reduce the running time
of van Leeuwen’s algorithm on a sorted input. Our claim is that in the case where the input is not sorted, the
computational cost of optimal preﬁx free codes on instances where van Leeuwen performs long sequential
searches can be greatly reduced. We deﬁne the “van Leeuwen signature” of an instance as a ﬁrst step to
characterize such instances:

Deﬁnition 1. Given an instance of the optimal preﬁx free code problem formed by n positive weights
W [1..n], its van Leeuwen signature S(W ) ∈ {E, I}2n−1 is a string of length 2n − 1 over the alphabet
{E, I} (where E stands for “External” and I for “Internal”) marking, at each step of the algorithm described 
by van Leeuwen [16], whether an external or internal node is chosen as the minimum (including the
last node returned by the algorithm, for simplicity).

Example 2. Given the sorted array W = 1 2 3 4 5 5 6 7 of length 8, its van Leeuwen signature is of length
15, starts with EE and ﬁnishes with I: S(W ) = EEEIEEEEIEIIIII.

The analysis described in Section 3 is based on the number of blocks formed only of E in the van

Leeuwen signature of the instance S. We can already show some basic properties of this measure:

Lemma 1. Given the van Leeuwen signature S of n unsorted positive weights W [1..n], |S|E = n; |S|I =
n − 1; |S| = 2n − 1; S starts with two E; S ﬁnishes with one I; |S|EI = |S|IE + 1; |S|EI ∈ [1..n − 1].

The three ﬁrst properties are simple consequences of basic properties on binary trees. S starts with two E as
the ﬁrst two nodes paired are always external. S ﬁnishes with one I as the last node returned is always (for
n > 1) an internal node. The two last properties are simple consequences of the fact that S is a binary string
starting with an E and ﬁnishing with an I.

Instances with very few blocks of E are easier to solve than instances with many such blocks. For
instance, an instance W of length n such that its signature S(W ) is composed of a single run of n Es
followed by a single run of n − 1 Is can be solved in linear time, and in particular without sorting the
weights: it is enough to assign the codelength l = ⌊log2 n⌋ to the n − 2l largest weights and the codelength
l + 1 to the 2l smallest weights. Separating those weights is a simple select operation, supported by the
data structures described in the following section.

2.2 Partial Sum Deferred Data Structure

Given a MULTISET W [1..n] on alphabet [1..σ] of size n, Karp et al. [15] deﬁned the ﬁrst deferred data
structure supporting for all x ∈ [1..σ] and r ∈ [1..n] queries such as rank(x), the number of elements
which are strictly smaller than x in W ; and select(r), the value of the r-th smallest value (counted
with multiplicity) in W . Their data structure supports q queries in time within O(n(1 + lg q)), all in the
comparison model. To achieve this results, it partially sorts its data in order to minimize the computational
cost of future queries, but avoids sorting all of the data if the queries don’t require it: the queries have

5
then become operators (they modify the data). Note that whereas the running time of each individual query
depends on the state of the data, the answer to each query is independent of the state of the data.

Karp et al.’s data structure [15] supports only rank and select queries in the comparison model,
whereas the computation of optimal preﬁx free codes requires to sum pairs of weights from the input, and the
algorithm that we propose in Section 2.3 requires to sum weights from a range in the input. Such requirement
can be reduced to partialSum queries. Whereas such queries have been deﬁned in the literature, we deﬁne
them here in a way that depends only on the content of the MULTISET (as opposed to a deﬁnition dpending
on the order in which it is given), so that it can be generalized to deferred data structures.

Deﬁnition 2. Given n unsorted positive weights W [1..n], a Partial Sum data structure supports the
following queries:

– rank(x), the number of elements which are strictly smaller than x in W ;
– select(r), the value of the r-th smallest value (counted with multiplicity) in W ;
– partialSum(r), the sum of the r smallest elements (counted with multiplicity) in W .

Example 3. Given the array A = 5 3 1 5 2 4 6 7 , rank(5) = 4, select(6) = 5, and partialSum(2) =
3.

We describe below how to extend Karp et al.’s deferred data structure [15], which supports rank and
select queries on MULTISETS, in order to add the support for partialSum queries, with an amortized
running time within a constant factor of the original asymptotic time. Note that the data structure is not
performing any more in the comparison model, but rather in the algebraic decision tree model, since it
performs algebraic operations (additions) on the elements of the MULTISET:

Lemma 2. Given n unsorted positive weights W [1..n], there is a PartialSum Deferred Data Structure
which supports q operations of type rank, select and partialSum in time within O(n(1 + lg q) +
q(1 + log n)), all within the algebraic decision tree computational model.

Karp et al. [15] described a deferred data structure which supports the rank and select queries (but
not partialSum queries). It is based on median computations and (2, 3)-trees, and performs q queries
on n values in time within O(n(1 + lg q) + q(1 + log n)), all within the algebraic computational model.
We describe below how to modify in a simple way their data structure so that to support partialSum
queries with asymptotically negligible additional cost. At the initialization of the data structure, compute
the n partial sums corresponding to the n positions of the unsorted array. After each median computation
and partitioning in a rank or select query, recompute the partial sums on the range of values newly
partitioned, adding only a constant factor to the cost of the query. When answering a partialSum query,
perform a select query and then return the value of the partial sum corresponding to the value by the
select query: the asymptotic complexity is within a constant factor of the one described by Karp et
al. [15].
⊓⊔
Barbay et al. [1] further improved Karp et al.’s result [15] with a simpler data structure (a single binary
array) and a ﬁner analysis taking into account the gaps between the position hit by the queries. Barbay et
al.’s results [1] can similarly be augmented in order to support partialSum queries while increasing the
computational complexity by only a constant factor. This result is not relevant to the analysis described in
Section 3.

Such a deferred data structure is sufﬁcient to simply execute van Leeuwen’s algorithm [16] on an unsorted 
array of positive integers, but would not result in an improvement in the computational complexity:

6van Leeuwen’s algorithm [16] is simply performing n select operations on the input, effectively sorting
the unsorted array.

We describe in the next section an algorithm which uses the deferred data structure described above to
batch the operations on the external nodes, and to defer the computation of the weights of some internal
nodes to later, so that for many instances the input is not completely sorted at the end of the execution,
which reduces the execution cost.

2.3 Algorithm “Grouping-Docking-Mixing” (GDM)

There are ﬁve main phases in the GDM algorithm: the Initialization, three phases (Grouping, Docking and
Mixing, hence the name “GDM” of the algorithm) inside a loop running until only internal nodes are left to
process, and the Conclusion:

– In the Initialization phase, initialize the Partial Sum deferred data structure with the input, and

initialize the ﬁrst internal node by pairing the two smallest weights of the input.

– In the Grouping phase, detect and group the weights smaller than the smallest internal node: this corresponds 
to a run of consecutive E in the van Leeuwen signature of the instance.

– In the Docking phase, pair the consecutive positions of those weights (as opposed to the weights themselves,
 which can be reordered by future operations) into internal nodes, and pair those internal nodes
until the weight of at least one such internal node becomes equal or larger than the smallest remaining
weight: this corresponds to a run of consecutive I in the van Leeuwen signature of the instance.

– In the Mixing phase, rank the smallest unpaired weight among the weights of the available internal

nodes: this corresponds to an occurrence of IE in the van Leeuwen signature of the instance.

– In the Conclusion phase, with i internal nodes left to process, assign codelength l = ⌊log2 i⌋ to the i − 2l
largest ones and codelength l+1 to the 2l smallest ones: this corresponds to the last run of consecutive I
in the van Leeuwen signature of the instance.

The algorithm and its complexity analysis distinguish two types of internal nodes: pure nodes, which
descendants were all paired during the same Grouping phase; and mixed nodes, which either is the ancestor
of such a mixed node, or pairs a pure internal node with an external node, or pairs two pure internal nodes
produced at distinct phases of the algorithm. The distinction is important as the algorithm computes the
weight of any mixed node at its creation (potentially generating several data structure operations), whereas
it defers the computation of the weight of some pure nodes to later.

Before describing each phase more in detail, it is important to observe the following invariant of the

algorithm:
Lemma 3. Given an instance of the optimal preﬁx free code problem formed by n > 1 positive weights
W [1..n], between each phase of the algorithm, all unpaired internal nodes have weight within a constant
factor of two (i.e. the maximal weight of an unpaired internal node is strictly smaller than the minimal
weight of an unpaired internal node).

We now proceed to describe each phase in more details:

Initialization: Initialize the Partial Sum deferred data structure; compute the weight currentMinInternal
of the ﬁrst internal node through the operation partialSum(2) (the sum of the two smallest weights);
create this ﬁrst internal node as a node of weight currentMinInternal and children 1 and 2 (the positions 
of the ﬁrst and second weights, in any order); compute the weight currentMinExternal of the
ﬁrst unpaired weight (i.e. the ﬁrst available external node) by the operation select(3); setup the variables
nbInternals = 1 and nbExternalProcessed = 2.

7
Grouping: Compute the position r of the ﬁrst unpaired weight which is larger than the smallest unpaired 
internal node, through the operation rank with parameter currentMinInternal; pair the ((r −
nbExternalProcessed) modulo 2) indices to form ⌊ r−nbExternalProcessed
⌋ pure internal nodes; if the
number r − nbExternalProcessed of unpaired weights smaller than the ﬁrst unpaired internal node is
odd, select the r-th weight through the operation select(r), compute the weight of the ﬁrst unpaired internal 
node, compare it with the next unpaired weight, to form one mixed node by combining the minimal of
the two with the extraneous weight.

2

Docking: Pair all internal nodes by batches (by Lemma 3, their weights are all within a factor of two, so
all internal nodes of a generation are processed before any internal node of the next generation); after each
batch, compare the weight of the largest such internal node (compute it through partialSum on its range
if it is a pure node, otherwise it is already computed) with the ﬁrst unpaired weight: if smaller, pair another
batch, and if larger, the phase is ﬁnished.

Mixing: Rank the smallest unpaired weight among the weights of the available internal nodes, by a doubling
search starting from the begining of the list of internal nodes. For each comparison, if the internal node’s
weight is not already known, compute it through a partialSum operation on the corresponding range (if
it is a mixed node, it is already known). If the number r of internal nodes of weight smaller than the unpaired
weight is odd, pair all but one, compute the weight of the last one and pair it with the unpaired weight. If r
is even, pair all of the r internal nodes of weight smaller than the unpaired weight, compare the weight of
the next unpaired internal node with the weight of the next unpaired external node, and pair the minimum
of the two with the ﬁrst unpaired weight. If there are some unpaired weights left, go back to the Grouping
phase, otherwise continue to the Conclusion phase.

Conclusion: There are only internal nodes left, and their weights are all within a factor of two from each
other. Pair the nodes two by two in batch as in the Docking phase, computing the weight of an internal node
only when the number of internal nodes of a batch is odd.

The combination of those phases forms the GDM algorithm, which computes an optimal preﬁx free code

given an unsorted sets of positive integers.

Lemma 4. The tree returned by the GDM algorithm describes an optimal preﬁx free code for its input.

In the next section, we analyze the number q of rank, select and partialSum queries performed by
the GDM algorithm, and deduce from it the complexity of the algorithm in term of algebraic operations.

3 Analysis

The GDM algorithm runs in time within O(n lg n) in the worst case over instances of size n (which is optimal
(if not a new result) in the algebraic decision tree model), but much faster on instances with few blocks of
consecutive Es in the van Leeuwen signature of the instance. We formalize this concept by deﬁning the
alternation α of the instance in Section 3.1. We then proceed in Section 3.2 to show upper bounds on the
number of queries and operations performed by the GDM algorithm in the worst case over instances of ﬁxed
size n and alternation α. We ﬁnish in Section 3.3 with a matching lower bound for the number of operations
performed.

83.1 Alternation α(W )

We suggested in Section 2.1 that the number of blocks of consecutive Es in the van Leeuwen signature of
an instance can be used to measure its difﬁculty. Indeed, some “easy” instances have few such blocks, and
the instance used to prove the Ω(n lg n) lower bound on computational complexity of optimal preﬁx free
codes in the algebraic decision tree model in the worst case over instances of size n has n−1 such blocks
(the maximum possible in an instance of size n). We formally deﬁne this measure as the “alternation” of the
instance (it measures how many times the van Leeuwen algorithm “alternates” from an external node to an
internal node) and denote it by the parameter α:

Deﬁnition 3. Given an instance of the optimal preﬁx free code problem formed by n positive weights
W [1..n], its alternation α(W ) ∈ [1..n − 1] is the number |S(W )|EI of occurrences of the substring “EI”
in its van Leeuwen signature S(W ).

Note that counting the number of blocks of consecutive Es is equivalent to counting the number of
blocks of consecutive Is: they are the same, because the van Leeuwen signature starts with two Es and ﬁnished 
with an I, and each new I-block ends an E-block and vice-versa. Also, the choice between measuring
the number of occurrences of “EI” or the number of occurrence of “IE” is arbitrary, as they are within a
term of 1 of each other: counting the number of occurrences of “EI” just gives a nicer range of [1..n − 1]
(as opposed to [0..n − 2]). This number is of particular interest as it measures the number of iteration of the
main loop in the GDM algorithm:

Lemma 5. Given an instance of the optimal preﬁx free code problem of alternation α, the GDM algorithm
performs α iterations of its main loop.

In the next section, we reﬁne this result to the number of data structure operations and algebraic operations 
performed by the GDM algorithm.

3.2 Upper Bound

In order to measure the number of queries performed by the GDM algorithm, we detail how many queries are
performed in each phase of the algorithm.

– The Initialization corresponds to a constant number of data structure operations: a select operation
to ﬁnd the third smallest weight, and a simple partialSum operation to sum the two smallest weights
of the input.

– Each Grouping phase corresponds to a constant number of data structure operations: a partialSum
operation to compute the weight of the smallest internal node if needed, and a rank operation to identify
the unpaired weights which are smaller or equal to this node.

– The number of operations performed by each Docking and Mixing phase is better analyzed together:
if there are i symbols in the I-block corresponding to this phase in the van Leeuwen signature, and if
the internal nodes are grouped on h levels before generating an internal node larger than the smallest
unpaired weights, the Docking phase corresponds to at most h partialSum operations, whereas the
Mixing phase corresponds to at most log2(i/2h) partialSum operations, which develops to log2(i) −
h, for a total of log2 i data structure operations.

– The Conclusion phase corresponds to a number of data structure operations logarithmic in the size of
the last block of Is in the Leeuwen’s signature of the instance: in the worst case, the weight of one pure
internal node is computed for each batch, through one single partialSum operation each time.

9
Lemma 5 and the concavity of the log yields the total number of data structure operations performed by

the GDM algorithm:
Lemma 6. Given an instance of the optimal preﬁx free code problem of alternation α, the GDM algorithm
performs within O(α(1 + lg n−1
α )) data structure operations on the deferred data structure given as input.
Proof. For i ∈ [1..α], let ni be the number of internal nodes at the beginning of the i-th Docking phase.
According to Lemma 5 and the analysis of the number of data structure operations performed in each phase,
the GDM algorithm performs in total within O(α + Pα
i=1 lg ni) data structure operations. Since there are at
most n − 1 internal nodes, by concavity of the logarithm this is within O(α + α lg n
α )). ⊓⊔
Combining this result with the complexity of the Partial Sum deferred data structure from Lemma 2

α ) = O(α(1 + lg n

directly yields the complexity of the GDM algorithm in algebraic operation (and running time):
Lemma 7. Given an instance of the optimal preﬁx free code problem of alternation α, the GDM algorithm
runs in time within O(n(1+ log α) + α(lg n)(lg n
Proof. Let q be the number of queries performed by the GDM algorithm. Lemma 6 implies that q ∈ O(α(1 +
lg n
α(lg n)(lg n

α )). Plunging this into the complexity of O(q lg n+n lg q) from Lemma 2 yields the complexity O(n(1+ log α)+

α )), all within the algebraic computational model.

α )).

⊓⊔

Some simple functional analysis further simpliﬁes the expression to our ﬁnal upper bound:

Lemma 8. Given two positive integers n > 0 and α ∈ [1..n − 1],

O(α(lg n)(lg

n
α

)) ⊆ O(n(1 + lg α))

Proof. Given two positive integers n > 0 and α ∈ [1..n − 1], α < n
yields α
result.

lg n and α
and α lg2 n > n lg α . Then, n/α < n implies α × lg n × lg n

lg α < n
lg2 n

lg α < n. A simple rewriting
α < n lg α, which yields the
⊓⊔

In the next section, we show that this complexity is indeed optimal in the algebraic decision tree model,

in the worst case over instances of ﬁxed size n and alternation α.

3.3 Lower Bound
A complexity within O(n(1 + lg α)) is exactly what one could expect, by analogy with the sorting of
MULTISETS: there are α groups of weights, so that the order within each groups does not matter much, but
the order between weights from different groups matter a lot. We prove a lower bound within Ω(n lg α) by
reduction to MULTISET sorting:
Lemma 9. Given the integers n ≤ 2 and α ∈ [1..n−1], for any algorithm A in the algebraic decision tree
computational model, there is a set W [1..n] of n positive weights of alternation α such that A performs
within Ω(n lg α) operations.
Proof. For any MULTISET A[1..n] = {x1, . . . , xn} of n values from an alphabet of α distinct values, deﬁne
the instance WA = {2x1, . . . , 2xn} of size n, so that computing an optimal preﬁx free code for W , sorted
by codelength, provides an ordering for A. W has alternation α: for any two distinct values x and y from A,
the van Leeuwen algorithm pairs all the weights of value 2x before pairing any weight of value 2y, so that
the van Leeuwen signature of WA has α blocks of consecutive Es. The lower bounds then results from the
classical lower bound on sorting MULTISETS in the comparison model in the worst case over MULTISETS
of size n with α distinct symbols.
⊓⊔

We compare our results to previous results in the next section.

104 Discussion

We described an algorithm computing an optimal preﬁx free code for n unsorted positive weights in time
within O(n(1+ lg α)) ⊆ O(n lg n), where the alternation α ∈ [1..n−1] roughly measures the amount
of sorting required by the computation, by combining van Leeuwen’s results about optimal preﬁx free
codes [16], known since 1976, with results about Karp et al.’s results about Deferred Data Structures [15],
known since 1988. The results described above yields many new questions, of which we discuss only a few
in the following sections.

We discuss in this section how those results relate to previous results on optimal preﬁx free codes (Section 
4.1), to other results on Deferred Data Structures obtained since 1988 (Section 4.2 and 4.3), to the lack
of practical applications of our results on optimal preﬁx free codes (Section 4.4), and about perspectives of
research on this topic (Section 4.5). We list in the appendix A.1 some interesting quotes about the importance
of optimal preﬁx free codes in general.

4.1 Relation to previous work on optimal preﬁx free codes

In 2006, Belal et al. [5], described a variant of Milidiú et al.’s algorithm [18,17] to compute optimal preﬁx
free codes, announcing that it performed O(kn) algebraic operations when the weights are not sorted, where
k is the number of distinct code lengths in some optimal preﬁx free code.

They describe an algorithm claimed to run in time O(16kn) when the weights are unsorted, and propose
to improve the complexity to O(kn) by partitioning the weights into smaller groups, each corresponding to
disjoint intervals of weights value4. The claimed complexity is asymptotically better than the one suggested
by Huffman when k ∈ o(log n), and they raise the question of whether there exists an algorithm running in
time O(n log k).

Like the GDM algorithm, the algorithm described by Belal et al. [5] for the unsorted case is based on
several computations of the median of the weights within a given interval, in particular, in order to select
the weights smaller than some well chosen value. The essential difference between both work is the use of
deferred data structure, which simpliﬁes both the algorithm and the analysis of its complexity.

4.2 Applicability of dynamic results on Deferred Data Structures

Karp et al. [15], when they deﬁned the ﬁrst Deferred Data Structures, supporting rank and select on
MULTISETS and other queries on CONVEX HULL, left as an open problem the support of dynamic operators
such as insert and delete: Ching et al. [9] quickly demonstrated how to add such support in good
amortized time.

The dynamic addition and deletion of elements in a deferred data structure (added by Ching et al. [9] to
Karp et al. [15]’s results) does not seem to have any application to the computation of optimal preﬁx free
codes: even if the list of weights was dynamic, further work is required to build a deferred data structure
supporting preﬁx free code queries.

4.3 Applicability of reﬁned results on Deferred Data Structures

Karp et al.’s analysis [15] of the complexity of the deferred data structure is in function of the total number q
of queries and operators, while Kaligosi et al. [14] analyzed the complexity of an ofﬂine version in function
of the size of the gaps between the positions of the queries. Barbay et al.[1] combined the three results

4 Those results were downgraded in the December 2010 update of their initial 2005 publication through Arxiv [4].

11
into a single deferred data structure for MULTISETS which supports the operators rank and select in
amortized time proportional to the entropy of the distribution of the sizes of the gaps between the positions
of the queries.

At ﬁrst view, one could hope to generalized the reﬁned entropy analysis (introduced by Kaligosi et
al. [14] and applied by Barbay et al.[1] to the online version) of MULTISETS deferred data structures supporting 
rank and select to the computational complexity of optimal preﬁx free codes: a complexity
proportional to the entropy of the distribution of codelengths in the output would nicely match the lower
bound of Ω(k(1 + H(n1, . . . , nh))) suggested by information theory, where the output contains ni codes
of length li, for some integer vector (l1, . . . , lh) of distinct codelengths and some integer h measuring the
number of distinct codelengths. Our current analysis does not yield such a result: the gap lengths between
queries in the list of weights are not as regular as (l1, . . . , lh).

4.4 Potential Practical Impact of our Results

The impact of our faster algorithm on the execution time of optimal preﬁx free code based techniques
should deﬁnitely be evaluated further. Yet, we expect it to be of little importance in most cases: compressing
a sequence S of |S| messages from an input alphabet of size n requires not only computing the code (in
time O(n) using our solution), but also computing the weights of the messages (in time |S|), and encoding
the sequence S itself using the computed code (in time O(|S|)). Improving the code computation time
will improve on the compression time only in cases where the size n of the input alphabet is very large
compared to the length |S| of the compressed sequence. One such application is the compression of texts
in natural language, where the input alphabet is composed of all the natural words [22]. Another potential
application is the boosting technique from Ferragina et al. [12], which divides the input sequence into very
short subsequence and computes a preﬁx free code for each subsequences on the input alphabet of the whole
sequence.

4.5 Perspectives

One could hope for an algorithm which complexity would match the lower bound of Ω(k(1+H(n1, . . . , nh)))
suggested by information theory, where the output contains ni codes of length li, for some integer vector
(l1, . . . , lh) of distinct codelengths and some integer h measuring the number of distinct codelengths. Our
current analysis does not yield such a result: the gap lengths between queries in the list of weights are not as
regular as (l1, . . . , lh), but a reﬁned analysis might. Minor improvements of our results could be brought by
studying the problem in external memory, where deferred data structures have also been developed [24,2],
or when the alphabet size is larger than two, as in the original article from Huffman [13].

Another promising line of research is given by variants of the original problem, such as OPTIMAL
BOUNDED LENGTH PREFIX FREE CODES, where the maximal length of each word of the preﬁx free code
must be less than or equal to a parameter l, while still minimizing the entropy of the code; or such as the
ORDER CONSTRAINED PREFIX FREE CODES, where the order of the words of the codes is constrained to
be the same as the order of the weights. Both problems have complexity O(n lg n) in the worst case over
instances of ﬁxed input size n, while having linear complexity when all the weights are within a factor of
two of each other, exactly as in the original problem.

A logical step would be to study, among the communication solutions using an optimal preﬁx free code
computed ofﬂine, which can now afford to compute a new optimal preﬁx free code more frequently and see
their compression performance improved by a faster preﬁx free code algorithm.

12 Another logical step would be to study, among the compression algorithms computing an optimal preﬁx
free code on each new instance (e.g. JPEG, MP3, MPEG), which ones get a better their time performance by
using a faster preﬁx free code algorithm.

A Appendix

A.1 Relevance of Preﬁx Free codes in General

Albeit 60 year old, Huffman’s result is still relevant nowadays. Optimal Preﬁx Free codes are used not
only for compressed encodings: they are also used in the construction of compressed data structures for
permutations [3], and using similar techniques for sorting faster multisets which contains subsequences of
consecutive positions already ordered [3].

In 1991, Gary Stix [25] stated that “Large networks of IBM computers use it. So do high-deﬁnition
television, modems and a popular electronic device that takes the brain work out of programming a videocassette 
recorder. All these digital wonders rely on the results of a 40-year-old term paper by a modest
Massachusetts Institute of Technology graduate student-a data compression scheme known as Huffman encoding 
(...) Products that use Huffman code might ﬁll a consumer electronics store. A recent entry on the
shop shelf is VCR Plus+, a device that automatically programs a VCR and is making its inventors wealthy.
(...) Instead of confronting the frustrating process of programming a VCR, the user simply types into the
small handheld device a numerical code that is printed in the television listings. When it is time to record,
the gadget beams its decoded instructions to the VCR and cable box with an infrared beam like those on
standard remote-control devices. This turns on the VCR, sets it (and the cable box) to the proper channel
and records for the designated time.”.

In 1995, Moffat and Katajainen [19], stated that: “The algorithm introduced by Huffman for devising
minimum-redundancy preﬁx free codes is well known and continues to enjoy widespread use in data compression 
programs. Huffman’s method is also a good illustration of the greedy paradigm of algorithm design
and, at the implementation level, provides a useful motivation for the priority queue abstract data type. For
these reasons Huffman’s algorithm enjoys a prominence enjoyed by only a relatively small number of
fundamental methods”.

In 1997, Moffat and Turpin [20] stated that those were “one of the enduring techniques of data compression.
 It was used in the venerable PACK compression program, authored by Szymanski in 1978, and
remains no less popular today”.

In 2010 Donald E. Knuth was quoted [23] as saying that: “Huffman code is one of the fundamental

ideas that people in computer science and data communications are using all the time”.

In 2010, the answer to the question “What are the real-world applications of Huffman coding?” on the
website Stacks Exchange [26] states that “Huffman is widely used in all the mainstream compression
formats that you might encounter - from GZIP, PKZIP (winzip etc) and BZIP2, to image formats such as
JPEG and PNG.”.

The Wikipedia website on Huffman coding states that “Huffman coding today is often used as a "back-
end" to some other compression method. DEFLATE (PKZIP’s algorithm) and multimedia codecs such as
JPEG and MP3 have a front-end model and quantization followed by Huffman coding.” [27].

Ironically, the pseudo-optimality of this algorithm seems to have become part of the folklore of the
area, as illustrated by a quote from Parker et al. [10] in 1999: “While there may be little hope of improving
on the O(n log n) complexity of the Huffman algorithm itself, there is still room for improvement in our
understanding of the algorithm.”.

References

13

1. J. Barbay, A. Gupta, S. Jo, S. S. Rao, and J. Sorenson. Theory and implementation of online multiselection algorithms. In

Proceedings of the Annual European Symposium on Algorithms (ESA), 2013.

2. J. Barbay, A. Gupta, S. S. Rao, and J. Sorenson. Dynamic online multiselection in internal and external memory. In Proceedings

of the International Workshop on Algorithms and Computation (WALCOM), 2014.

3. J. Barbay and G. Navarro. Compressed representations of permutations, and applications. In S. Albers and J.-Y. Marion, editors,
Proceedings of the International Symposium on Theoretical Aspects of Computer Science (STACS), volume 3 of LIPIcs, pages
111–122. Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik, Germany, 2009.

4. A. A. Belal and A. Elmasry. Distribution-sensitive construction of minimum-redundancy preﬁx codes. CoRR, abs/cs/0509015,

2005. Version of Tue, 21 Dec 2010 14:22:41 GMT, with downgraded results from the ones in the conference version [5].

5. A. A. Belal and A. Elmasry. Distribution-sensitive construction of minimum-redundancy preﬁx codes.

In B. Durand and
W. Thomas, editors, Proceedings of the International Symposium on Theoretical Aspects of Computer Science (STACS), volume
3884 of Lecture Notes in Computer Science, pages 92–103. Springer, 2006.

6. A. A. Belal and A. Elmasry. Veriﬁcation of minimum-redundancy preﬁx codes. IEEE Transactions on Information Theory

(TIT), 52(4):1399–1404, 2006.

7. J. L. Bentley and A. C.-C. Yao. An almost optimal algorithm for unbounded searching. Information Processing Letters (IPL),

5(3):82–87, 1976.

8. C. Chen, Y. Pai, and S. Ruan. Low power Huffman coding for high performance data transmission. In Hybrid Information

Technology, 2006. ICHIT’06. International Conference on, volume 1, pages 71–77. IEEE, 2006.

9. Y.-T. Ching, K. Mehlhorn, and M. H. Smid. Dynamic deferred data structuring.

Information Processing Letters (IPL),

35(1):37–40, June 1990.

10. P. R. DS Parker. Huffman codes submodular optimization. SIAM Journal of Computing (SJC), 1999.
11. S. Even and G. Even. Graph Algorithms, Second Edition. Cambridge University Press, 2012.
12. P. Ferragina, R. Giancarlo, G. Manzini, and M. Sciortino. Boosting textual compression in optimal linear time. Journal of the

ACM, 52(4):688–713, 2005.

13. D. A. Huffman. A method for the construction of minimum-redundancy codes. Proceedings of the Institute of Radio Engineers

(IRE), 40(9):1098–1101, September 1952.

14. K. Kaligosi, K. Mehlhorn, J. I. Munro, and P. Sanders. Towards optimal multiple selection. In Proceedings of the International

Conference on Automata, Languages, and Programming (ICALP), pages 103–114, 2005.

15. R. Karp, R. Motwani, and P. Raghavan. Deferred data structuring. SIAM Journal of Computing (SJC), 17(5):883–902, 1988.
16. J. V. Leeuwen. On the construction of Huffman trees. In Proceedings of the International Conference on Automata, Languages,

and Programming (ICALP), pages 382–410, Edinburgh University, 1976.

17. R. L. Milidiú, A. A. Pessoa, and E. S. Laber. A space-economical algorithm for minimum-redundancy coding. Technical

report, Departamento de Informática, PUC-RJ, Rio de, 1998.

18. R. L. Milidiú, A. A. Pessoa, and E. S. Laber. Three space-economical algorithms for calculating minimum-redundancy preﬁx

codes. IEEE Transactions on Information Theory (TIT), 47(6):2185–2198, Sept. 2001.

19. A. Moffat and J. Katajainen. In-place calculation of minimum-redundancy codes. In WADS, volume 955 of Lecture Notes in

Computer Science, pages 393–402. Springer, 1995.

20. A. Moffat and A. Turpin. On the implementation of minimum redundancy preﬁx codes. IEEE Transactions on Communications,
 45(10):1200–1207, 1997.

21. A. Moffat and A. Turpin. Efﬁcient construction of minimum-redundancy codes for large alphabets. IEEE Transactions on

Information Theory (TIT), pages 1650–1657, 1998.

22. E. Moura, G. Navarro, N. Ziviani, and R. Baeza-Yates. Fast and ﬂexible word searching on compressed text. ACM Transactions

on Information Systems (TOIS), 18(2):113–139, 2000.

23. M. N. Chandrasekaran. Discrete Mathematics. PHI Learning Pvt. Ltd., 2010.
24. J. F. Sibeyn. External selection. Journal of Algorithms (JALG), 58(2):104–117, 2006.
25. G. Stix. Proﬁle: David A. Huffman. Scientiﬁc American, pages 54–58, September 1991. http://www.huffmancoding.com/my-

uncle/scientiﬁc-american. Last accessed on 2012/06/29.

26. Website TCS Stack Exchange. “What are the real-world applications of huffman coding?”, February 2010. Last accessed

on 2012-10-25.

27. Website Wikipedia. Huffman coding. Last accessed on 2012-04-20.

