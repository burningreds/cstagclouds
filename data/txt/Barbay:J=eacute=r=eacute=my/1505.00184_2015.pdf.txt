Instance-Optimal Geometric Algorithms

PEYMAN AFSHANI, MADALGO, University of Aarhus
J´ER´EMY BARBAY, DCC, Universidad de Chile
TIMOTHY M. CHAN, University of Waterloo

A

1. INTRODUCTION

Instance optimality: our model(s). Standard worst-case analysis of algorithms has often been criticized as
overly pessimistic. As a remedy, some researchers have turned towards adaptive analysis where the execution
cost of algorithms is measured as a function of not just the input size but also other parameters that capture
in some ways the diﬃculty of the input instance. For example, for problems in computational geometry (the
primary domain of the present paper), parameters that have been considered in the past include the output
size (leading to so-called output-sensitive algorithms) [Kirkpatrick and Seidel 1986], the spread of an input
point set (the ratio of the maximum to the minimum pairwise distance) [Erickson 2005], various measures
of fatness of the input objects (e.g., ratio of circumradii to inradii) [Matouˇsek et al. 1994] or clutteredness
of a collection of objects [de Berg et al. 2002], the number of reﬂex angles in an input polygon, and so on.

The ultimate in adaptive algorithms is an instance-optimal algorithm, i.e., an algorithm A whose cost is
at most a constant factor from the cost of any other algorithm A′ running on the same input, for every
input instance. Unfortunately, for many problems, this requirement is too stringent. For example, consider
the 2-d convex hull problem, which has Θ(n log n) worst-case complexity in the algebraic computation tree
model: for every input sequence of n points, one can easily design an algorithm A′ (with its code depending
on the input sequence) that runs in O(n) time on that particular sequence, thus ruling out the existence of
an instance-optimal algorithm.1

To get a more useful deﬁnition, we suggest a variant of instance optimality where we ignore the order in

which the input elements are given, as formalized precisely below:

Deﬁnition 1.1. Consider a problem where the input consists of a sequence of n elements from a domain
D. Consider a class A of algorithms. A correct algorithm refers to an algorithm that outputs a correct answer
for every possible sequence of elements in D.
For a set S of n elements in D, let TA(S) denote the maximum running time of A on input σ over all
n! possible permutations σ of S. Let OPT(S) denote the minimum of TA′(S) over all correct algorithms
A′ ∈ A. If A ∈ A is a correct algorithm such that TA(S) ≤ O(1) · OPT(S) for every set S, then we say A is
instance-optimal in the order-oblivious setting.

For many problems, the output is a function of the input as a set rather than a sequence, and the above
deﬁnition is especially meaningful. In particular, for such problems, instance-optimal algorithms are automatically 
optimal output-sensitive algorithms; in fact, they are automatically optimal adaptive algorithms
with respect to any parameter that is independent of the input order, all at the same time! This property
is satisﬁed by simple parameters like the spread of an input point set S, or more complicated quantities like
the expected size fr(S) of the convex hull of a random sample of size r from S [Clarkson 1994].

For many algorithms (e.g., quickhull [Preparata and Shamos 1985], to name one), the running time is
not aﬀected so much by the order in which the input points are given but by the relative positions of the
input points. Combinatorial and computational geometers more often associate “bad examples” with bad
point sets rather than bad point sequences. All this supports the reasonableness and importance of the
order-oblivious form of instance optimality.

5
1
0
2

 

y
a
M
1

 

 
 
]

G
C
.
s
c
[
 
 

1
v
4
8
1
0
0

.

5
0
5
1
:
v
i
X
r
a

We can consider a still stronger variant of instance optimality:
Deﬁnition 1.2. For a set S of n elements in D, let T avg

σ over all n! possible permutations σ of S. Let OPTavg(S) denote the minimum of T avg

A (S) denote the average running time of A on input
A′ (S) over all correct

1The length of the program for A′ may depend on n in this example. If we relax the deﬁnition to permit the “constant factor”
to grow as a function of the program length of A′, then an instance-optimal algorithm A exists for many problems such as
sorting (or more generally problems that admit linear-time veriﬁcation). This follows from a trick attributed to Levin [Jones
1997], of enumerating and simulating all programs in parallel under an appropriate schedule. To say that algorithms obtained
this way are impractical, however, would be an understatement.

q2

q3

q1

q2

q3

q1

q1

q2

q3

q1

q2

q3

(a)

(b)

(c)

(d)

Fig. 1.
an “easier” point set for the 2-d maxima problem.

(a) A “harder” point set and (b) an “easier” point set for the 2-d upper hull problem. (c) A “harder” point set and (d)

algorithms A′ ∈ A. If A ∈ A is a correct algorithm such that TA(S) ≤ O(1) · OPTavg(S) for every set S,
then we say A is instance-optimal in the random-order setting.2

Note that an instance-optimal algorithm in the above sense is immediately also competitive against randomized 
(Las Vegas) algorithms A′, by the easy direction of Yao’s principle. The above deﬁnition has extra
appeal in computational geometry, as it is common to see the design of randomized algorithms where the
input elements are initially permuted in random order [Clarkson and Shor 1989].

Instance-optimality in the random-order setting also implies average-case optimality where we analyze
the expected running time under the assumption that the input elements are random and independently
chosen from a common given probability distribution. (To see this, just observe that the input sequence is
equally likely to be any permutation of S conditioned to the event that the set of n input elements equals any
ﬁxed set S.) An algorithm that is instance-optimal in the random-order setting can deal with all probability
distributions at the same time! Random-order instance optimality also remedies a common complaint about
average-case analysis, that it does not provide information about an algorithm’s performance on a speciﬁc
input.

Convex hull: our main result. After making the case for instance-optimal algorithms under our deﬁnitions,
the question remains: do such algorithms actually exist, or are they “too good to be true”? Speciﬁcally,
we turn to one of the most fundamental and well known problems in computational geometry—computing
the convex hull of a set of n points. Many O(n log n)-time algorithms in 2-d and 3-d have been proposed
since the 1970s [de Berg et al. 1997; Edelsbrunner 1987; Preparata and Shamos 1985], which are worstcase 
optimal under the algebraic computation tree model. Optimal output-sensitive algorithms can solve
the 2-d and 3-d problem in O(n log h) time, where h is the output size. The ﬁrst such output-sensitive
algorithm in 2-d was found by [Kirkpatrick and Seidel 1986] in the 1980s and was later simpliﬁed by [Chan
et al. 1997] and independently [Wenger 1997]; a diﬀerent, simple, optimal output-sensitive algorithm was
discovered by [Chan 1996b]. In 3-d, the ﬁrst optimal output-sensitive algorithm was obtained by [Clarkson
and Shor 1989] using randomization; another version was described by [Clarkson 1994]. The ﬁrst deterministic
optimal output-sensitive algorithm in 3-d was obtained by [Chazelle and Matouˇsek 1995] via derandomization;
the approach by [Chan 1996b] can also be extended to 3-d and yields a simpler optimal output-sensitive
algorithm. There are also average-case algorithms running in O(n) expected time for certain probability
distributions [Preparata and Shamos 1985], for example, when the points are independent and uniformly
distributed inside a circle or a constant-sized polygon in 2-d, or a ball or a constant-sized polyhedron in 3-d.
The convex hull problem is in some ways an ideal candidate to consider in our models. It is not diﬃcult
to think of examples of “easy” point sets and “hard” point sets (see Figure 1(a,b)). It is not diﬃcult to
think of diﬀerent heuristics for pruning nonextreme points, which may not necessarily improve worst-case
complexity but may help for many point sets encountered “in practice” (e.g., consider quickhull [Preparata
and Shamos 1985]). However, it is unclear whether there is a single pruning strategy that works best on all
point sets.

In this paper, we show that there are indeed instance-optimal algorithms for both the 2-d and 3-d convex
hull problem, in the order-oblivious or the stronger random-order setting. Our algorithms thus subsume all
the previous output-sensitive and average-case algorithms simultaneously, and are provably at least as good
asymptotically as any other algorithm for every point set, so long as input order is ignored.

2One can also consider other variations of the deﬁnition, e.g., relaxing the condition to T avg
expected running time over random permutations with analogous high-probability statements.

A (S) ≤ O(1)·OPTavg(S), or replacing

Techniques. We believe that our techniques—for both the upper-bound side (i.e., algorithms) and the

lower-bound side (i.e., proofs of their instance optimality)—are as interesting as our results.

On the upper-bound side, we ﬁnd that in the 2-d case, a new algorithm is not necessary: a version of
Kirkpatrick and Seidel’s output-sensitive algorithm [Kirkpatrick and Seidel 1986], or its simpliﬁcation by
[Chan et al. 1997], is instance-optimal in the order-oblivious and random-order setting. We view this as a
plus: these algorithms are simple and practical to implement [Bhattacharya and Sen 1997], and our analysis
sheds new light on their theoretical complexity. In particular, our result immediately implies that a version
of Kirkpatrick and Seidel’s algorithm runs in O(n) expected time for points uniformly distributed inside a
circle or a ﬁxed-size polygon—we were unaware of this fact before. (As another plus, our result provides a
positive answer to the question in the title of Kirkpatrick and Seidel’s paper, “The ultimate planar convex
hull algorithm?”)

In 3-d we propose a new algorithm, as none of the previous output-sensitive algorithms seems to be
instance-optimal. For example, known 3-d generalizations of the Kirkpatrick–Seidel algorithm have suboptimal 
O(n log2 h) running time [Chan et al. 1997; Edelsbrunner and Shi 1990], while a straightforward implementation 
of the algorithm by [Chan 1996b] fails to be instance-optimal even in 2-d. Our algorithm builds
on Chan’s technique [Chan 1996b] but requires additional ideas, notably the use of partition trees [Matouˇsek
1992].

The lower-bound side requires more innovation. We are aware of three existing techniques for proving worstcase 
Ω(n log n) (or output-sensitive Ω(n log h)) lower bounds in computational geometry: (i) informationtheoretic 
or counting arguments, (ii) topological arguments, from early work by [Yao 1981] to [Ben-Or 1983],
and (iii) Ramsey-theory-based arguments, by [Moran et al. 1985]. Ben-Or’s approach is perhaps the most
powerful and works in the general algebraic computation tree model, whereas Moran et al.’s approach works
in a decision tree model where all the test functions have a bounded number of arguments. For an arbitrary
input set S for the convex hull problem, the naive information-theoretic argument gives only an Ω(h log h)
lower bound on OPT(S). On the other hand, topological and Ramsey-theory approaches seem unable to give
any instance-speciﬁc lower bound (for example, modifying the topological approach is already nontrivial if
we just want a lower bound for some integer input set [Yao 1991], let alone for every input set, whereas the
Ramsey-theory approach considers only input elements that come from a cleverly designed subdomain).

We end up using a diﬀerent lower bound technique which is inspired by an adversary argument originally
used to prove time–space lower bounds for median ﬁnding [Chan 2010]. Note that this approach can lead to
another proof of the standard Ω(n log n) lower bounds for many geometric problems including the problem
of computing a convex hull; the proof is simple and works in any algebraic decision tree model where the
test functions have at most constant degree and have at most a constant number of arguments. We build on
the idea further and obtain an optimal lower bound for the convex hull problem for every input point set.
The assumed model is more restrictive: the class A of allowed algorithms consists of those under a decision
tree model in which the test functions are multilinear and have at most a constant number of arguments.
Fortunately, most standard primitive operations encountered in existing convex hull algorithms satisfy the
multilinearity condition (for example, the standard determinant test does). The ﬁnal proof interestingly
involves partition trees [Matouˇsek 1992], which are more typically used in algorithms (as in our new 3-d
algorithm) rather than in lower-bound proofs.

So, what is OPT(S), i.e., what parameter asymptotically captures the true diﬃculty of a point set S for
the convex hull problem? As it turns out, the bound has a simple expression (to be revealed in Section 3)
and shares similarity with entropy bounds found in average-case or expected-case analysis of geometric data
structures where query points come from a given probability distribution—these distribution-sensitive results
have been the subject of several previous pieces of work [Arya et al. 2007a; Arya et al. 2007b; Collette et al.
2012; Dujmovi´c et al. 2012; Iacono 2004]. However, lower bounds for distribution-sensitive data structures
cannot be applied to our problem because our problem is oﬀ-line (lower bounds for on-line query problems
usually assume that the query algorithms ﬁt a “classiﬁcation tree” framework, but an oﬀ-line algorithm may
compare a query point not only with points from the data set but also with other query points). Furthermore,
although in the oﬀ-line setting we can think of the query points as coming from a discrete point probability
distribution, this distribution is not known in advance.3 Lastly, distribution-sensitive data structures are
usually concerned with improving the query time, but not the total time that includes preprocessing.

3Self-improving algorithms [Ailon et al. 2011] also cope with the issue of how to deal with unknown input probability distributions,
 but are not directly comparable with our results, since in their setting each point can come from a diﬀerent distribution,
so input order matters.

Other results. The computation of the convex hull is just one problem for which we are able to obtain
instance optimality. We show that our techniques can lead to instance-optimal results for many other standard
problems in computational geometry, in the order-oblivious or random-order setting, including:

(a) maxima in 2-d and 3-d;
(b) reporting/counting intersection between horizontal and vertical line segments in 2-d;
(c) reporting/counting pairs of L∞-distance at most 1 between a red point set and a blue point set in 2-d;
(d) oﬀ-line orthogonal range reporting/counting in 2-d;
(e) oﬀ-line dominating reporting in 2-d and 3-d;
(f) oﬀ-line halfspace range reporting in 2-d and 3-d; and
(g) oﬀ-line point location in 2-d.

Optimal expected-case, entropy-based data structures for the on-line version of (g) are known [Arya et al.
2007b; Iacono 2004], but not for (e,f)—for example, [Dujmovi´c et al. 2012] only obtained results for 2-d
dominance counting, a special case of 2-d orthogonal range counting. Incidentally, as a consequence of our
ideas, we can also get new optimal expected-case data structures for on-line 2-d general orthogonal range
counting and 2-d and 3-d halfspace range reporting.

Related work. [Fagin et al. 2003] ﬁrst coined the term “instance optimality” (when studying the problem
of ﬁnding items with the k top aggregate scores in a database in a certain model), although some form of
the concept has appeared before. For example, the well known “dynamic optimality conjecture” is about
instance optimality concerning algorithms for manipulating binary search trees (see [Demaine et al. 2009]
for the latest in a series of papers). [Demaine et al. 2000] studied the problem of computing the union or
intersection of k sorted sets and gave instance-optimal results for any k for union, and for constant k for
intersection, in the comparison model; [Barbay and Chen 2008] extended their result to the computation
of convex hull of k convex polygons in 2-d for constant k. Another work about instance-optimal geometric
algorithms is by [Baran and Demaine 2005], who addressed an approximation problem about computing the
distance of a point to a curve under a certain black-box model. Other than these, there has not been much
work on instance optimality in computational geometry, especially concerning the classical problems under
conventional models.

The concept of instance optimality resembles competitive analysis of on-line algorithms. In fact, in the
on-line algorithms literature, our order-oblivious setting of instance optimality is related to what [Boyar
and Favrholdt 2007] called the relative worst order ratio, and our random-order setting is related to what
[Kenyon 1996] called the random order ratio. What makes instance optimality more intriguing is that we are
not bounding the objective function of an optimization problem, but rather the running time of an algorithm.

2. WARM-UP: 2-D MAXIMA
Before proving our main result on convex hull, we ﬁnd it useful to study a simpler problem: maxima in 2-d.
For two points p and q we say p dominates q if each coordinate of p is greater than that the corresponding
coordinate of q. Given a set S of n points in Rd, a point p is maximal if p ∈ S and p is not dominated by any
other point in S. For simplicity, we assume that the input is always nondegenerate throughout the paper (for
example, no two points share the same xor 
y-coordinate). The maxima problem is to report all maximal
points.

For an alternative formulation, we can deﬁne the orthant at a point p to be the region of all points that
are dominated by p. In 2-d, the boundary of the union of the orthants at all p ∈ S forms a staircase, and
the maxima problem is equivalent to computing the staircase of S.
This problem has a similar history as the convex hull problem: many worst-case O(n log n)-time algorithms
are known; an output-sensitive algorithm by [Kirkpatrick and Seidel 1985] runs in O(n log h) time for output
size h; and average-case algorithms with O(n) expected time have been analyzed for various probability
distributions [Bentley et al. 1990; Clarkson 1994; Preparata and Shamos 1985]. The problem is simpler than
computing the convex hull, in the sense that direct pairwise comparisons are suﬃcient. We therefore work
with the class A of algorithms in the comparison model where we can access the input points only through
comparisons of the coordinate of an input point with the corresponding coordinate of another input point.
The number of comparisons made by an algorithm yields a lower bound on the running time.

We deﬁne a measure H(S) to represent the diﬃculty of a point set S and prove that the optimal running
time OPT(S) is precisely Θ(n(H(S) + 1)) for the 2-d maxima problem in the order-oblivious and randomorder 
setting.

q1

q1

q1

q2

q2

q3

q3

q2

q3

Fig. 2. Three respectful partitions of an instance of the 2-d maxima problem. The two partitions on the left have entropy
12 log 12 + 7
1

4 ≈ 1.281. The partition Πvert on the right has higher entropy log 3 ≈ 1.585.

12 log 12

12 log 12

7 + 4

Deﬁnition 2.1. Consider a partition Π of the input set S into disjoint subsets S1, . . . , St. We say that
Π is respectful if each subset Sk is either a singleton or can be enclosed by an axis-aligned box Bk
whose interior is completely below the staircase of S. Deﬁne the entropy H(Π) of the partition Π to be
k=1(|Sk|/n) log(n/|Sk|). Deﬁne the structural entropy H(S) of the input set S to be the minimum of H(Π)
over all respectful partitions Π of S.

Pt

Remark 2.2. Alternatively, we could further insist in the deﬁnition that the bounding boxes Bi are
nonoverlapping and cover precisely the staircase of S. However, this will not matter, as it turns out that
the two deﬁnitions yield asymptotically the same quantity (this nonobvious fact is a byproduct of our lower
bound proof in Section 2.2).

Entropy-like expressions similar to H(Π) have appeared in the analysis of expected-case geometric data
structures for the case of a discrete point probability distribution, although our deﬁnition itself is nonprobabilistic.
 A measure proposed by [Sen and Gupta 1999] is identical to H(Πvert) where Πvert is a partition of S
obtained by dividing the point set S by h vertical lines at the h maximal points of S (see Figure 2(right) for
an illustration). Note that H(Πvert) is at most log h (see Figure 1(c)) but can be much smaller; in turn, H(S)
can be much smaller than H(Πvert) (see Figures 1(d) and 2). The complexity of the 1-d multiset sorting
problem [Munro and Spira 1976] also involves an entropy expression associated with one partition, but does
not require taking the minimum over multiple partitions.

2.1. Upper bound
We use a slight variant of Kirkpatrick and Seidel’s output-sensitive maxima algorithm [Kirkpatrick and Seidel
1985] (in their original algorithm, only points from Qℓ are pruned in line 4):

if |Q| = 1 then return Q

maxima2d(Q):
1.
2. divide Q into the left and right halves Qℓ and Qr by the median x-coordinate
3. discover the point q with the maximum y-coordinate in Qr
4. prune all points in Qℓ and Qr that are dominated by q
5.

return the concatenation of maxima2d(Qℓ) and maxima2d(Qr)

We call maxima2d(S) to start: Figure 3 illustrates the state of the algorithm after a single recursion level.
Kirkpatrick and Seidel showed that its running time is within O(n log h), and [Sen and Gupta 1999] improved
this upper bound to O(n(H(Πvert) + 1)). Improving this bound to O(n(H(Π) + 1)) for an arbitrary respectful
partition Π of S requires a bit more ﬁnesse:

Theorem 2.3. Algorithm maxima2d(S) runs in O(n(H(S) + 1)) time.
Proof. Consider the recursion tree of the algorithm and let Xj denote the sublist of all maximal points
of S discovered during the ﬁrst j recursion levels, in left-to-right order. Let S(j) be the subset of points of S
that survive recursion level j, i.e., that have not been pruned during levels 0, . . . , j of the recursion, and let

nj. Observe that

nj = |S(j)|. The running time is asymptotically bounded byP⌈log n⌉

j=0

(i)

there can be at most ⌈n/2j⌉ points of S(j) with x-coordinates between any two consecutive points in
Xj, and
(ii) all points of S that are strictly below the staircase of Xj have been pruned during levels 0, . . . , j of the

recursion.

median

q1

q2

q3

Fig. 3. Partial execution of maxima(S) after one
recursion level. In this example, after computing
the median x-coordinate, the algorithm found the
highest point q2 to the right of the median, and
pruned the 6 points dominated by it. Only 5 points
are left to recurse upon, 1 to the left and 4 to the
right.

Let Π be any respectful partition of S. Consider a subset Sk in Π. Let Bk be a box enclosing Sk whose
interior lies below the staircase of S. Fix a level j. Suppose that the upper-right corner of Bk has x-coordinate
between two consecutive points qi and qi+1 in Xj. By (ii), the only points in Bk that can survive level j
must have x-coordinates between qi and qi+1. Thus, by (i), the number of points in Sk that survive level j

is at most min(cid:8)|Sk|,⌈n/2j⌉(cid:9). Since the Sk’s cover the entire point set, with a double summation we have

min(cid:8)|Sk|,⌈n/2j⌉(cid:9)
min(cid:8)|Sk|,⌈n/2j⌉(cid:9)

⌈log n⌉Xj=0

nj ≤

⌈log n⌉Xj=0 Xk
⌈log n⌉Xj=0
= Xk
≤ Xk
≤ Xk

(|Sk|⌈log(n/|Sk|)⌉ + |Sk| + |Sk|/2 + |Sk|/4 + ··· + 1)

|Sk|(⌈log(n/|Sk|)⌉ + 2)

∈ O(n(H(Π) + 1)).

As Π can be any respectful partition of S, it can be in particular the one of minimum entropy, hence the
ﬁnal result.

2.2. Lower bound

For the lower-bound side, we ﬁrst provide an intuitive justiﬁcation for the bound Ω(n(H(S)+1)) and point out
the subtlety in obtaining a rigorous proof. Intuitively, to certify that we have a correct answer, the algorithm
must gather evidence for each point p eliminated why it is not a maximal point, by indicating at least one
witness point in S which dominates p. We can deﬁne a partition Π by placing points with a common witness
in the same subset. It is easy to see that this partition Π is respectful. The entropy bound nH(Π) roughly
represents the number of bits required to encode the partition Π, so in a vague sense, nH(S) represents the
length of the shortest “certiﬁcate” for S. Unfortunately, there could be many valid certiﬁcates for a given
input set S (due to possibly multiple choices of witnesses for each nonmaximal point). If hypothetically all
branches of an algorithm lead to a common partition Π, then a straightforward information-theoretic or
counting argument would indeed prove the lower bound. The problem is that each leaf of the decision tree
may give rise to a diﬀerent partition Π.

In Appendix A, we show that despite the aforementioned diﬃculty, it is possible to obtain a proof of
instance optimality via this approach, but the proof requires a more sophisticated counting argument, and
also works with a diﬀerent diﬃculty measure. Moreover, it is limited speciﬁcally to the 2-d maxima problem
and does not extend to 3-d maxima, let alone to nonorthogonal problems such as the convex hull problem.
In this subsection, we describe a diﬀerent proof, which generalizes to the other problems that we consider.
The proof is based on an interesting adversary argument. We show in Section 4 how to adapt the proof to
the random-order setting.

Theorem 2.4. OPT(S) ∈ Ω(n(H(S) + 1)) for the 2-d maxima problem in the comparison model.

q1

q1

q2

q2

q3

q3

Fig. 4. The diﬃculty of proving a lower bound by counting certiﬁcates: each elimination of a point p must be justiﬁed by
indicating a witness which dominates p. But there can be more than one such certiﬁcate for each instance, and some can be
encoded in less space than others. Here the same instance is partitioned on the left into 3 subsets of equal size 4, while it is
partitioned on the right into subsets of sizes 1, 7, 4 with lower entropy.

median

q1

q2

q3

Fig. 5. The beginning of the recursive partitioning of S by the k-d tree T , which will yield the ﬁnal partition Πkd-tree for the
adversarial lower bound for the 2-d maxima problem. The two bottom boxes are already leaves, while the two top boxes will
be divided further.

Proof. We prove that a speciﬁc respectful partition described below not only asymptotically achieves the
minimum entropy among all the respectful partitions, but also provides a lower bound for the running time
of any comparison-based algorithm that solves the 2-d maxima problem. The construction of the partition
is based on k-d trees [de Berg et al. 1997]. We deﬁne a tree T of axis-aligned boxes, generated top-down as
follows: The root stores the entire plane. For each node storing box B, if B is strictly below the staircase
of S, or if B contains just one point of S, then B is a leaf. Otherwise, if the node is at an odd (resp. even)
depth, divide B into two subboxes by the median x-coordinate (resp. y-coordinate) among the points of S
inside B. The two subboxes are the children of B (see Figure 5 for an illustration). Note that each box B at
depth j of T contains at least ⌊n/2j⌋ points of S, and consequently, the depth j is in Ω(log(n/|S ∩ B|)).
Our claimed partition, denoted by Πkd-tree, is one formed by the leaf boxes in this tree T (i.e., points
in the same leaf box are placed in the same subset). Clearly, Πkd-tree is respectful. We will prove that for
any correct algorithm A in the comparison model, there exists a permutation of S on which the algorithm
requires at least Ω(nH(Πkd-tree)) comparisons.
The adversary constructs a bad permutation for the input by simulating the algorithm A and resolving
each comparison so that the algorithm is forced to perform many others. During the simulation, we maintain
a box Bp in T for each point p. If Bp is a leaf box, the algorithm knows the exact location of p inside Bp.
But if Bp corresponds to an internal node, the only information the algorithm knows about p is that p lies
inside Bp. In other words, p can be assigned any point in Bp without aﬀecting the outcomes of the previous
comparisons made.

For each box B in T , let n(B) be the number of points p such that the box Bp is contained in B. We
maintain the invariant that n(B) ≤ |S ∩ B|. If n(B) = |S ∩ B|, we say that B is full. As soon as Bp becomes
a leaf box, we assign p to an arbitrary point in S ∩ Bp that has not been previously assigned (such a point
exists because of the invariant); we then call p a ﬁxed point.

Suppose that the algorithm A compares, say, the x-coordinates of two points p and q. The main case is

when neither Bp nor Bq is a leaf. The comparison is resolved in the following way:

(1) If Bp (resp. Bq) is at even depth, we arbitrarily reset Bp (resp. Bq) to one of its children that is not full.

Thus assume that Bp and Bq are both at odd depths.
Without loss of generality, suppose that the median x-coordinate of Bp is less than the median xq 
of Bq; if either B′
coordinate of Bq. We reset Bp to the left child B′
p
or B′
q allows us to deduce that
p has a smaller x-coordinate than q. Thus, the adversary declares to the algorithm that the x-coordinate
of p is smaller than that of q and continues with the rest of the simulation.

q is full, we go to step 2. Now, the knowledge that p and q lie in B′

p of Bp and Bq to the right child B′

p and B′

(2) An exceptional case occurs if B′

(resp. right) sibling B′′
1.

p of B′

p is full (or similarly B′

q is full). Here, we reset Bp instead to the left
p, but the comparison is not necessarily resolved yet, so we go back to step

p) + n(B′′

Note that in both steps the invariant is maintained. This is because Bp and B′

otherwise, we would have |S ∩ Bp| = |S ∩ B′
n(B′

p cannot be both full:
p ), but |S ∩ Bp| ≥ n(Bp) ≥
The above description can be easily modiﬁed in the case when Bp or Bq is a leaf box. If both Bp and
Bq are leaf boxes, then p and q are already ﬁxed and the comparison is already resolved. If (without loss of
generality) only Bp is a leaf, we follow step 1 except that now since p has been ﬁxed, we compare the actual
x-coordinate of p to the median x-coordinate of Bq, and reset only Bq.

p ) + 1 (the “+1” arises because at least one point, notably, p, has Bp as its box).

p| + |S ∩ B′′

p| = n(B′

p) + n(B′′

We now prove a lower bound on the number of comparisons, T , made by the algorithm A. Let D be the
sum of the depth of the boxes Bp in the tree T over all points p ∈ S at the end of the simulation of the
algorithm. We will lower-bound T in terms of D. Each time we reset a box to one of its children in step 1
or 2, D is incremented; we say that an ordinary (resp. exceptional ) increment occurs at the parent box if
this is done in step 1 (resp. step 2). Each comparison generates only O(1) ordinary increments. To account
for exceptional increments, we use a simple amortization argument: At each box B in T , the number of
ordinary increments has to reach at least ⌊|S ∩ B|/2⌋ ﬁrst, before any exceptional increments can occur,
and the number of exceptional increments is at most ⌈|S ∩ B|/2⌉. Thus, the total number of exceptional
increments can be upper-bounded by the total number of ordinary increments, which is in O(T ). It follows
that D ∈ O(T ), i.e., T ∈ Ω(D).
Thus, it remains to prove our lower bound for D. We ﬁrst argue that at the end of the simulation, Bq
must be a leaf box for every point q ∈ S. Suppose that this is not the case. After the end of the simulation,
we can do the following postprocessing: for every point p where Bp corresponds to an internal node, we reset
Bp to one of its nonfull children arbitrarily, and repeat. As a result, every Bp now becomes a leaf box, all
the input points have been assigned to points of S, and no two input points are assigned the same value, i.e.,
the input is ﬁxed to a permutation of S. The staircase of this input obviously coincides with the staircase
of S. Next, consider modifying this input slightly as follows. Suppose that Bq was not a leaf box before the
postprocessing. Then this box contained at least two points of S and was not completely underneath the
staircase of S. We can either move a nonmaximal point upward or a maximal point downward inside Bq,
and obtain a modiﬁed input that is consistent with the comparisons made but has a diﬀerent set of maximal
points. The algorithm would be incorrect on this modiﬁed input: a contradiction.

It follows that

T ∈ Ω(D)

⊆ Ω Xleaf B

|S ∩ B| log(n/|S ∩ B|)!

⊆ Ω(nH(Πkd-tree))
⊆ Ω(nH(S)).

Combined with the trivial Ω(n) lower bound, this establishes the Ω(n(H(S) + 1)) lower bound.

Remark 2.5. The above proof is inspired by an adversary argument described by [Chan 2010] for a 1-d
problem (the original proof maintains a dyadic interval for each input point, while the new proof maintains a

box from a hierarchical subdivision).4 The proof still holds for weaker versions of the problem, for example,
reporting just the number of maximal points (or the parity of the number). The lower-bound proof easily
extends to any constant dimension and can be easily modiﬁed to allow comparisons of diﬀerent coordinates
of any two points p = (x1, . . . , xd) and q = (x′
j, or even
xi < x′

j + a for any constant a. (For a still wider class of test functions, see the next section.)

1, . . . , x′

d), for example, testing whether xi < x′

3. CONVEX HULL
We now turn to our main result on 2-d and 3-d convex hull. It suﬃces to consider the problem of computing
the upper hull of an input point set S in Rd (d ∈ {2, 3}), since the lower hull can be computed by running
the upper hull algorithm on a reﬂection of S. (Up to constant factors, the optimal running time for convex
hull is equal to the maximum of the optimal running time for upper hull and the optimal running time for
lower hull, on every input.)

We work with the class A of algorithms in a multilinear decision tree model where we can access the input
points only through tests of the form f (p1, . . . , pc) > 0 for a multilinear function f , over a constant number
of input points p1, . . . , pc. We recall the following standard deﬁnition:

Deﬁnition 3.1. A function f : (Rd)c → R is multilinear if the restriction of f is a linear function 
from Rd to R when any c − 1 of the c arguments are ﬁxed. Equivalently, f is multilinear if
f ((x11, . . . , x1d), . . . , (xc1, . . . , xcd)) is a multivariate polynomial function in which each monomial has the
form xi1j1 ··· xikjk where i1, . . . , ik are all distinct (i.e., we cannot multiply coordinates from the same point).
Most of the 2-d and 3-d convex hull algorithms that we know ﬁt in this framework: it supports the standard
determinant test (for deciding whether p1 is above the line through p2, p3, or the plane through p2, p3, p4),
since the determinant is a multilinear function. For another example, in 2-d, comparison of the slope of
the line through p1, p2 with the slope of the line through p3, p4 reduces to testing the sign of the function
(y2 − y1)(x4 − x3) − (x2 − x1)(y4 − y3), which is clearly multilinear. We discuss in Section 5 the relevance
and limitations of the multilinear model.
We adopt the following modiﬁed deﬁnition of H(S) (as before, it does not matter whether we insist that

the simplices ∆k below are nonoverlapping for both the 2-d and 3-d problem):

Deﬁnition 3.2. A partition Π of S is respectful if each subset Sk in Π is either a singleton or can be enclosed
by a simplex ∆k whose interior is completely below the upper hull of S. Deﬁne the structural entropy H(S)

of S to be the minimum of H(Π) =Pk(|Sk|/n) log(n/|Sk|) over all respectful partitions Π of S.

3.1. Lower bound
The lower-bound proof for computing the convex hull builds on the corresponding lower-bound proof for
computing the maxima from Section 2.2 but is more involved, because a k-d tree construction no longer
suﬃces when addressing nonorthogonal problems. Instead, we use the known following lemma:

Lemma 3.3. For every set Q of n points in Rd and 1 ≤ r ≤ n for any constant d, we can partition Q
into r subsets Q1, . . . , Qr each of size Θ(n/r) and ﬁnd r convex polyhedral cells γ1, . . . , γr each with O(log r)
(or fewer) facets, such that Qi is contained in γi, and every hyperplane intersects at most O(r1−ε) cells.
Here, ε > 0 is a constant that depends only on d.

The above follows from the partition theorem of [Matouˇsek 1992], who obtained the best constant ε = 1/d;
in his construction, the cells γi are simplices (with O(1) facets) and may overlap, and subset sizes are
indeed upperand 
lower-bounded by Θ(n/r). (A version of the partition theorem by [Chan 2012] can avoid
overlapping cells, but does not guarantee an Ω(n/r) lower bound on the subset sizes.)

In 2-d or 3-d, a more elementary alternative construction follows from the 4-sectioning or 8-sectioning
theorem [Edelsbrunner 1987; Yao et al. 1989]: for every n-point set Q in R2, there exist 2 lines that divide
the plane into 4 regions each with n/4 points; for every n-point set Q in R3, there exist 3 planes that divide
space into 8 regions each with n/8 points. Since in R2 a line can intersect at most 3 of the 4 regions and
in R3 a plane can intersect at most 7 of the 8 regions, a simple recursive application of the theorem yields
ε = 1− log4 3 for d = 2 and ε = 1− log8 7 for d = 3. Each resulting cell γi is a convex polytope with O(log r)
facets, and the cells do not overlap.

We also need another fact, a geometric property about multilinear functions:

4There are also some indirect similarities to an adversary argument for sorting due to [Kahn and Kim 1995], as pointed out to
us by Jean Cardinal (personal communication, 2010).

Lemma 3.4.

If f : (Rd)c → R is multilinear and has a zero in γ1 × ··· × γc where each γi is a convex
polytope in Rd, then f has a zero (p1, . . . , pc) ∈ γ1 × ··· × γc such that all but at most one point pi is a
polytope’s vertex.

Proof. Let (p1, . . . , pc) ∈ γ1 × ··· × γc be a zero of f . Suppose that some pi does not lie on an edge
of γi. If we ﬁx the other c − 1 points, the equation f = 0 becomes a hyperplane, which intersects γi and
thus must intersect an edge of γi. We can move pi to such an intersection point. Repeating this process,
we may assume that every pi lies on an edge uivi of γi. Represent the line segment parametrically as
{(1 − ti)ui + tivi | 0 ≤ ti ≤ 1}.
Next, suppose that some two points pi and pj are not vertices. If we ﬁx the other c− 2 points and restrict
pi and pj to lie on uivi and ujvj respectively, the equation f = 0 becomes a multilinear function in two
parameters ti, tj ∈ [0, 1]. The equation has the form atitj + a′ti + a′′tj + a′′′ = 0 and is a hyperbola, which
intersects [0, 1]2 and must thus intersect the boundary of [0, 1]2. We can move pi and pj to correspond to
such a boundary intersection point. Then one of pi and pj is now a vertex. Repeating this process, we obtain
the lemma.

We are now ready for the main lower-bound proof:
Theorem 3.5. OPT(S) ∈ Ω(n(H(S) + 1)) for the upper hull problem in any constant dimension d in

the multilinear decision tree model.

Proof. We deﬁne a partition tree T as follows: Each node v stores a pair (Q(v), Γ(v)), where Q(v) is a
subset of S enclosed inside a convex polyhedral cell Γ(v). The root stores (S, Rd). If Γ(v) is strictly below
the upper hull of S, or if |Q(v)| drops below a constant, then v is a leaf. Otherwise, apply Lemma 3.3 with
r = b to partition Q(v) and obtain subsets Q1, . . . , Qb and cells γ1, . . . , γb. For the children v1, . . . , vb of v,
set Q(vi) = Qi and Γ(vi) = γi ∩ Γ(v). For a node v at depth j of the tree T we then have |Q(v)| ≥ n/Θ(b)j,
and consequently, the depth j is in Ω(logb(n/|Q(v)|)). Furthermore, since Γ(v) is the intersection of at most
j convex polyhedra with at most O(log b) facets each, it has size (j log b)O(1).
Let Πpart-tree be the partition formed by the subsets Q(v) at the leaves v in T . LeteΠpart-tree be a reﬁnement
of this partition obtained as follows: for each leaf v at depth j, we triangulate Γ(v) into (j log b)O(1) simplices
and subpartition Q(v) by placing points of Q(v) from the same simplex in the same subset; if |Q(v)| drops
below a constant, we subpartition Q(v) into singletons. Note that the subpartitioning of Q(v) causes the
entropy to increase5 by at most O((|Q(v)|/n) log(j log b)) ⊆ O((|Q(v)|/n) log log(n/|Q(v)|)) for any constant
b. The total increase in entropy is thus within O(H(Πpart-tree)). So H(eΠpart-tree) ∈ Θ(H(Πpart-tree)). Clearly,
eΠpart-tree is respectful.

The adversary constructs a bad permutation for the input points as follows. During the simulation, we
maintain a node vp in T for each point p. If vp is a leaf, the algorithm knows the exact location of p inside
Γ(vp). But if vp is an internal node, the only information the algorithm knows about p is that p lies inside
Γ(vp).

For each node v in T , let n(v) be the number of points p with vp in the subtree rooted at v. We maintain
the invariant that n(v) ≤ |Q(v)|. If n(v) = |Q(v)|, we say that v is full. As soon as vp becomes a leaf, we ﬁx
p to an arbitrary unassigned point in Q(vp) (such a point exists because of the invariant).
Suppose that the simulation encounters a test “f (p1, . . . , pc) > 0?”. The main case is when none of the

nodes vpi is a leaf.

(1) Consider a c-tuple (v′

p1 , . . . , v′

pc ) where v′

p1 )×···× γ(v′

pi is a child of vpi . We say that the tuple is bad if f has a zero in
γ(v′
pc), and good otherwise. We prove the existence of a good tuple by upper-bounding the
number of bad tuples: If we ﬁx all but one point pi, the restriction of f can have a zero in at most O(b1−ε)
pi ), by Lemma 3.3 and the multilinearity of f . There are O(bc−1 log b) choices of
cells of the form γ(v′
c− 1 vertices of the cells of the form γ(v′
pc ). By Lemma 3.4, it follows that the number of bad
tuples is at most O((bc−1 log b) · b1−ε) ⊆ o(bc). As the number of tuples is in Θ(bc), if b is a suﬃciently
large constant, then we can guarantee that some tuple (v′
pi for each
i = 1, . . . , c; if some v′
pi is full, we go to step 2. Since the tuple is good, the sign of f is determined and
the comparison is resolved.

pc ) is good. We reset vpi to v′

p1 ), . . . , γ(v′

p1 , . . . , v′

5If Pk

i=1 qi = q, then by concavity of the logarithm, Pk

i=1

qi
n log n
qi

= q

n Pk

i=1

qi
q log n
qi

≤ q

n log kn

q = q

n log n

q + q

n log k.

Fig. 6. Kirkpatrick and Seidel’s upper hull algorithm [Kirkpatrick and Seidel 1986] with an added pruning step. Line 5 prunes
the points in the shaded trapezoid. The added step in line 2 prunes points in the two shaded triangles.

(2) In the exceptional case when some v′

pi is full, we reset vpi instead to an arbitrary nonfull child, and go

back to step 1.

The above description can be easily modiﬁed in the case when some of the nodes vpi are leaves, i.e., when

some of the points pi are already ﬁxed (we just have to lower c by the number of ﬁxed points).

Let T be the number of tests made. Let D be the sum of the depth of vp over all points p ∈ S. The same
amortization argument as in the previous proof of Theorem 2.4 proves that T ∈ Ω(D). By an argument
similar to before, at the end of the simulation, vp must be a leaf for every p ∈ S. It follows that

|Q(v)| log(n/|Q(v)|)!

T ∈ Ω(D)

⊆ Ω Xleaf v
⊆ Ω(nH(eΠpart-tree))

⊆ Ω(nH(Πpart-tree))

⊆ Ω(nH(S)).

Combined with the trivial Ω(n) lower bound, this establishes the theorem.

The proof extends to weaker versions of the problem, for example, reporting the number of hull vertices

(or its parity).

3.2. Upper bound in 2-d
To establish a matching upper bound in 2-d, we use a version of the output-sensitive convex hull algorithm by
[Kirkpatrick and Seidel 1986] described below, where an extra pruning step is added in line 2. (This step is not
new and has appeared in both quickhull [Preparata and Shamos 1985] and the simpliﬁed output-sensitive
algorithm by [Chan et al. 1997]; see Figure 6 for illustration.)

hull2d(Q):
1.
2. prune all points from Q strictly below the line through the leftmost and

if |Q| = 2 then return Q
rightmost points of Q

3. divide Q into the left and right halves Qℓ and Qr by the median x-coordinate pm
4. discover points q, q′ that deﬁne the upper-hull edge qq′ intersecting the vertical

line at pm (in linear time)

5. prune all points from Qℓ and Qr that are strictly underneath the line segment qq′
6.

return the concatenation of hull2d(Qℓ) and hull2d(Qr)

Line 4 can be done in O(n) time (without knowing the upper hull beforehand) by applying a 2-d linear
programming algorithm in the dual [Preparata and Shamos 1985]. We call hull2d(S) to start. It is straightforward 
to show that the algorithm, even without line 2, runs in time O(n log h), or O(n(H(Πvert) + 1)) for
the speciﬁc partition Πvert of S obtained by placing points underneath the same upper-hull edge in the same
subset, as was done by [Sen and Gupta 1999]. To upper-bound the running time by O(n(H(Π) + 1)) for an
arbitrary respectful partition Π of S, we modify the proof in Theorem 2.3:

Theorem 3.6. Algorithm hull2d(S) runs in O(n(H(S) + 1)) time.
Proof. Like before, let Xj denote the sublist of all hull vertices discovered during the ﬁrst j levels of
the recursion, in left-to-right order. Let S(j) be the subset of points of S that survive recursion level j, and

nj = |S(j)|. The running time is asymptotically bounded byP⌈log n⌉

j=0

(i)

there can be at most ⌈n/2j⌉ points of S(j) with x-coordinates between any two consecutive vertices in
Xj, and
(ii) all points that are strictly below the upper hull of Xj have been pruned during levels 0, . . . , j of the

nj. Observe that

recursion.

Let Π be any respectful partition of S. Consider a subset Sk in Π. Let ∆k be a triangle enclosing Sk whose
interior lies below the upper hull of S. Fix a level j. If qi and qi+1 are two consecutive vertices in Xj such
that qiqi+1 does not intersect the boundary of ∆k (i.e., is above ∆k), then all points in ∆k with x-coordinates
between qk and qk+1 would have been pruned during the ﬁrst j levels by (ii). Since only O(1) edges qiqi+1
of the upper hull of Xj can intersect the boundary of ∆k, the number of points in Sk that survive level j is

at most min(cid:8)|Sk|, O(n/2j)(cid:9) by (i). We then have

⌈log n⌉Xj=0

nj ∈

⌈log n⌉Xj=0 Xk

min(cid:8)|Sk|, O(n/2j)(cid:9) ⊆ O(n(H(Π) + 1))

as before.

Remark 3.7. The same result holds for the simpliﬁed output-sensitive algorithm by [Chan et al. 1997],
which avoids the need to invoke a 2-d linear programming algorithm. (Chan et al.’s paper explicitly added
the pruning step in their algorithm description.) The only diﬀerence in the above analysis is that there can
be at most ⌈(3/4)jn⌉ points of S with x-coordinates between any two consecutive vertices in Xj.

3.3. Upper bound in 3-d
We next present an instance-optimal algorithm in 3-d that matches our lower bound. Unlike in 2-d, it
is unclear if any of the known algorithms can be modiﬁed for this purpose. For example, obtaining an
O(n(H(Πvert)+1)) upper bound is already nontrivial for the speciﬁc partition Πvert where points underneath
the same upper-hull facet are placed in the same subset. Informed by our lower-bound proof, we suggest an
algorithm that is also based on partition trees. We need the following subroutine:

Lemma 3.8. Given a set of n halfspaces in Rd for any constant d, we can answer a sequence of r linear
programming queries (ﬁnding the point that maximizes a query linear function over the intersection of the
halfspaces) in total time O(n log r + rO(1)).

The above lemma was obtained by Chan [Chan 1996b; 1996c] using a simple grouping trick (which was the
basis of his output-sensitive O(n log h)-time convex hull algorithm); the d ≥ 3 case required randomization. A
subsequent paper by [Chan 1996a] gave an alternative approach using a partition construction; this eliminated
randomization.

Our new upper hull algorithm can now be described as follows:

hull3d(Q):
1.
2.

for j = 0, 1, . . . ,⌊log(δ log n)⌋ do

partition Q by Lemma 3.3 to get rj = 22j
cells γ1, . . . , γrj
for each i = 1 to rj do

subsets Q1, . . . , Qrj and

3.
4.
5.

if γi is strictly below the upper hull of Q then prune all points in Qi from Q

compute the upper hull of the remaining set Q directly

j

Line 2 takes O(|Q| log rj + rO(1)
) time by known algorithms for Matouˇsek’s partition theorem [Matouˇsek
1992] (or alternatively recursive application of the 8-sectioning theorem). The test in line 4 reduces to deciding
whether each of the at most O(log rj ) vertices of the convex polyhedral cell γi is strictly below the upper hull
of Q. This can be done (without knowing the upper hull beforehand) by answering a 3-d linear programming
). As

query in dual space. Using Lemma 3.8, we can perform lines 3–4 collectively in time O(|Q| log rj + rO(1)

j

j

term is negligible, since its total over all iterations is sublinear in n by choosing a small

rj ≤ nδ, the rO(1)
constant δ. Line 5 is done by running any O(|Q| log |Q|)-time algorithm.
Theorem 3.9. Algorithm hull3d(S) runs in O(n(H(S) + 1)) time.
Proof. Let nj be the size of Q just after iteration j. The total running time is asymptotically bounded

byPj nj log rj+1. (This includes the cost of line 5, which is O(nj log nj) ∈ O(nj log rj+1) for the last index

j = ⌈log(δ log n)⌉.)
Let Π be any respectful partition of S. Consider a subset Sk in Π. Let ∆k be a simplex enclosing Sk
whose interior lies below the upper hull of S. Fix an iteration j. Consider the subsets Q1, . . . , Qrj and cells
γ1, . . . , γrj at this iteration. If a cell γi is completely inside ∆k, then all points inside γi are pruned. Since
O(r1−ε
) cells γi intersect the boundary of ∆k, the number of points in Sk that remain in Q after iteration

j

j is at most min(cid:8)|Sk|, O(r1−ε

j
double summation we have

j )(cid:9). The Sk’s cover the entire point set, so with a

Xj

· n/rj)(cid:9) = min(cid:8)|Sk|, O(n/rε
2ε2j(cid:17)o · 2j+1
2ε2j(cid:17)o · 2j+1
|Sk|2j +

minn|Sk|, O(cid:16) n
nj log rj+1 ≤ Xj Xk
minn|Sk|, O(cid:16) n
= Xk Xj
O
∈ Xk
∈ Xk

O (|Sk|(log(n/|Sk|) + 1))

j≤log((1/ε) log(n/|Sk|))+1

X

∈ O(n(H(Π) + 1)),

X

j>log((1/ε) log(n/|Sk|))+1

n

2ε2j−1

which yields the theorem.

Remark 3.10. Variants of the algorithm are possible. For example, instead of recomputing the partition
in line 3 at each iteration from scratch, another option is to build the partitions hierarchically as a tree.
Points are pruned as the tree is generated level by level.

One minor technicality is that the above description of the algorithm does not discuss the low-level test
functions involved. In Section 5 we explain how a modiﬁcation of the algorithm can indeed be implemented
in the multilinear model.

A similar approach works for the 3-d maxima problem in the comparison model. We just replace partition 
trees with k-d trees, and replace linear programming queries with queries to test whether a point lies
underneath the staircase, which can be done via an analog of Lemma 3.8.

4. EXTENSION TO THE RANDOM-ORDER SETTING
In this section, we describe how our lower-bound proofs in the order-oblivious setting can be adapted to
the random-order setting. We focus on the convex hull problem and describe how to modify the proof of
Theorem 3.5. We need a technical lemma ﬁrst:

Lemma 4.1. Suppose we place n random elements independently in t bins, where each element is placed
in the k-th bin with probability nk/n. Then the probability that the k-th bin contains exactly nk elements for
all k = 1, . . . , t is at least n−O(t).

Proof. The probability is exactly

n!

n(cid:1)n1 ···(cid:0) nt
n(cid:1)nt , which by Stirling’s formula is
Θ(√n1)(n1/e)n1 ··· Θ(√nt)(nt/e)nt (cid:16) n1
n(cid:17)n1

n1!···nt!(cid:0) n1

n(cid:17)nt
···(cid:16) nt

Θ(√n)(n/e)n

Θ(√n)t−1 ,

⊆

1

yielding the result.

We now present our lower-bound proof in the random-order setting. (The proof is loosely inspired by the

randomized “bit-revealing” argument by [Chan 2010].)

Theorem 4.2. OPTavg(S) ∈ Ω(n(H(S) + 1)) for the upper hull problem in any constant dimension d in

the multilinear decision tree model.

Proof. Fix a suﬃciently small constant δ > 0. Let T be as in the proof of Theorem 3.5, except that we

keep only the ﬁrst ⌊δ log n⌋ levels of the tree, i.e., when a node reaches depth ⌊δ log n⌋, it is made a leaf.
which each leaf cell is further triangulated and each subset corresponding to a cell of depth ⌊δ log n⌋ is further
subpartitioned into singletons. Note that each such subset has size Θ(nδ) and contributes Θ((nδ/n) log n)

Let Πpart-tree be the partition of S formed by the leaf cells in T . LeteΠpart-tree be a reﬁnement of Πpart-tree in
to both the entropy of Πpart-tree and eΠpart-tree. Thus, H(eΠpart-tree) ∈ Θ(H(Πpart-tree)). Clearly, eΠpart-tree is

The adversary proceeds diﬀerently. We do not explicitly maintain the invariant that no node v is full.
Whenever some vp ﬁrst becomes a leaf, we assign p to a random point among the points in Q(vp) that
has previously not been assigned. If all points in Q(vp) have in fact been assigned, we say that failure has
occurred.

respectful.

Suppose that the simulation encounters a test “f (p1, . . . , pc) > 0”. We do the following:

— We reset each vpi to one of its children at random, where each child v′
pi

is chosen with probability
pc ) is good (as deﬁned in the proof of

pi )|/|Q(vpi)| (which is in Θ(1/b)). If the tuple (v′

|Q(v′
Theorem 3.5), then the comparison is resolved. Otherwise, we repeat.
Since we have shown that the number of bad tuples is in o(bc), the probability that the test is not resolved
in one step is in o(bc) · Θ(1/b)c, which can be made less than 1/2 for a suﬃciently large constant b. The
number of iterations per comparison is thus upper-bounded by a geometrically distributed random variable
with mean O(1).

p1 , . . . , v′

Let T be the number of comparisons made. Let D be the sum of the depth of vp over all points p ∈ S at the
end of the simulation. Clearly, D is upper-bounded by the total number of iterations performed, which is at
most a sum of T independent geometrically distributed random variables with mean O(1). Let (∗) be the event
that D ≤ c0T for a suﬃciently large constant c0. By the Chernoﬀ bound, Pr[(∗)] ≥ 1 − 2−Ω(T ) ≥ 1 − 2−Ω(n).
By the same argument as before, at the end of the simulation, vp must be a leaf for every p ∈ S, assuming
that failure has not occurred.
Let (†) be the event that failure has not occurred. If (∗) and (†) are both true, then

|Q(v)| log(n/|Q(v)|)!

T ∈ Ω(D)

⊆ Ω Xleaf v
⊆ Ω(nH(eΠpart-tree))

⊆ Ω(nH(Πpart-tree))

⊆ Ω(nH(S)).

To analyze Pr[(†)], consider the leaf vp that a point p ends up with after the simulation (regardless of
whether failure has occurred). This is a random variable, which equals a ﬁxed leaf v with probability |Q(v)|/n.
Moreover, all these random variables are independent. Failure occurs if and only if for some leaf v, the number
of vp’s that equal v is diﬀerent from |Q(v)|. By Lemma 4.1, Pr[(†)] ≥ n−O(nδ), since there are O(nδ) leaves
in T . It follows that

Pr[not (∗) | (†)] ≤

Pr[not (∗)]

Pr(†)

2−Ω(n)
n−O(nδ) ⊆ 2−Ω(n).

∈

probability is exactlyQleaf v(cid:16) |Q(v)|

Finally, observe that Pr[(†) ∧ (the input equals σ)] is the same for all ﬁxed permutations σ of S (the
|Q(v)|! ). In other words, conditioned to (†), the input is a random
permutation of S, i.e., the adversary have not acted adversarily after all! It follows that T ∈ Ω(nH(S)) with
high probability for a random permutation of S. In particular, E[T ] ∈ Ω(nH(S)) for a random permutation
of S.
Remark 4.3. Applying the same ideas to the proof of Theorem 2.4 shows that OPTavg(S) ∈ Ω(n(H(S) +

n (cid:17)|Q(v)|

1)) for the maxima problem in the comparison model.

1

Fig. 7. An instance where nH(S) is no longer a lower bound if nonmultilinear tests are allowed. A circular disk covers all
nonmaximal points underneath the staircase. An algorithm tailored to this instance can identify the n − h points that are inside
the disk by O(n) nonmultilinear tests, then compute the staircase of the remaining h points in O(h log h) time, and verify that
the disk is underneath the staircase by O(h) additional nonmultilinear tests.

5. ON THE MULTILINEAR MODEL

We remark that if nonmultilinear test functions are allowed, then nH(S) may no longer a valid instanceoptimal 
lower bound under our deﬁnition of H(S). For example, one can design both an instance S of the
2-d maxima problem with h output points, having H(S) ∈ Ω(log h) (see Figure 7), and an algorithm A that
requires just O(n + h log h) operations on that instance using nonmultilinear tests. A similar example can
be constructed for the 2-d convex hull problem.

Nevertheless, many standard test functions commonly found in geometric algorithms are multilinear. For
example, in 3-d, the predicate above(p1, . . . , p4) which returns true if and only if p1 is above the plane
through p2, p3, p4 can be reduced to testing the sign of a multilinear function (a determinant).

To see the versatility of multilinear tests, consider the following extended deﬁnition: we say that a
function f : (Rd)c → Rd is quasi-multilinear if f (p1, . . . , pc) = (f1(p1, . . . , pc), . . . , fd(p1, . . . , pc)) where
fi = hi(p1, . . . , pc)/g(p1, . . . , pc) in which f1, . . . , fd, g : (Rd)c → R are multilinear functions. In 3-d, the
function plane(p1, p2, p3) which returns the dual of the plane through p1, p2, p3 is quasi-multilinear; similarly 
the function intersect(p1, p2, p3) which returns the intersection of the dual planes of p1, p2, p3 is
quasi-multilinear. This can be seen by expressing the answer as a ratio of determinants.

More generally, we have the following rules:

— if

— if

R3

→

fi

:

(R3)ci

then

function

the
{1, 2, 3},
intersect(f1(p11, . . . , p1c1), f2(p21, . . . , p2c2), f3(p31, . . . , p3c3)) are quasi-multilinear;
the

plane(f1(p11, . . . , p1c1), f2(p21, . . . , p2c2), f3(p31, . . . , p3c3))

then
above(f1(p11, . . . , p1c1), . . . , f4(p41, . . . , p4c4)) can be reduced to testing the sign of a multilinear function.


each i ∈ {1, . . . , 4},

is quasi-multilinear

(R3)ci → R3

function fi

:

∈
and

is

quasi-multilinear

for

each

i

for

By combining the above rules, more and more elaborate predicates can thus be reduced to testing the

signs of multilinear functions, such as in the following example:

above



p10,
p11,
p12,

plane(p7, p8, p9) !
intersect  plane(p1, p2, p3),

plane(p4, p5, p6),

.



However, we may run into problems if a point occurs more than once in the expression, as in the following
example:

p10,
p11,
p1,

plane(p7, p8, p9) !
intersect  plane(p1, p2, p3),

plane(p4, p5, p6),

.



above



Here, the expansion of the determinants may yield monomials of the wrong type. In most 2-d algorithms,
this kind of tests does not arise or can be trivially eliminated. Unfortunately, they can occasionally occur in
some 3-d algorithms, including our 3-d upper hull algorithm in Section 3.3. We now describe how to modify
our algorithm to avoid these problematic tests.

First, we consider the partition construction in Lemma 3.3. We choose the more elementary alternative
based on the 8-sectioning theorem: there exist 3 planes that divide space into 8 regions, each with n/8 points
of Q. By perturbing the 3 planes one by one, we can ensure that each of the 3 planes passes through 3
input points, and that the resulting 9 points are distinct, while changing the number of points of Q in each
region by ±O(1). A brute-force algorithm can ﬁnd 3 such planes in polynomial time. We can reduce the
construction time by using the standard tool called epsilon-approximations [Matouˇsek 2000]: we compute a
δ-approximation of Q in linear time for a suﬃciently small constant δ > 0, and then apply the polynomial
algorithm to the constant-sized δ-approximation. This only changes the fraction 1/8 by a small term ±O(δ).
It can be checked that known algorithms for epsilon-approximations [Matouˇsek 2000] require only multilinear
tests (it suﬃces to check the implementation of the so-called subsystem oracle, which only requires the above
predicate). We remove the 9 deﬁning points before recursively proceeding inside the 8 regions. As a result,
we can ensure that the facets in each convex polyhedral cell are all deﬁned by planes that pass through 3
input points, where no two planes share a common deﬁning point. A vertex v of a cell is an intersection of
3 such planes and is deﬁned by a set of 9 distinct input points, denoted def(v).

Next, we consider the proof of Lemma 3.8 for answering r linear programming queries. We choose the
alternative approach by [Chan 1996a] based on a partition construction, which we have from the previous
paragraph. This algorithm is based on a deterministic version of the sampling-based linear programming
algorithm by [Clarkson 1995]. The algorithm can also support up to r insertions and deletions of halfspaces
intermixed with the query sequence. The algorithm can be implemented with simple predicates such as
above.

Now, in the algorithm hull3d, we make one change: in line 5, we prune only when each vertex v of γi
lies strictly below the upper hull of Q − def(v) (instead of the upper hull of Q). In the dual, testing such a
vertex v reduces to a linear programming query after deletion of def(v) from Q, where the coeﬃcient vector
of the objective function is quasi-multilinear in def(v). Since def(v) has been deleted from Q, we avoid the
problem of test functions where some point appears more than once in the expression. It can be checked that
applying the algorithm for linear programming queries from the previous paragraph indeed requires only
multilinear tests now.

Since the pruning condition has been weakened, the analysis of hull3d needs to be changed. Recall that
the partition in line 3 is constructed by recursive application of the 8-sectioning theorem. At half the depth
of recursion, we obtain an intermediate partition of Q with O(√rj) subsets Q′
ℓ and corresponding cells γ′
ℓ
where each subset has O(n/√rj ) points, and every plane intersects at most O(√rj
1−ε) of these cells γ′
ℓ, for
ℓ, every plane intersects at most O(√rj
1−ε) of the cells γi of the
ε = 1 − log8 7. Furthermore, for a ﬁxed γ′
ﬁnal partition inside γ′
ℓ.

ℓ of the intermediate partition. We claim that if (i) γ′

In the second paragraph of the proof of Theorem 3.9, we do the analysis diﬀerently. Consider a cell γi of
the partition, which is contained in a cell γ′
ℓ is strictly
contained in ∆k and (ii) γi is strictly contained in γ′
ℓ, then all points inside γi are pruned. To see this, notice
that by (i), all points in Q′
ℓ are strictly below the upper hull of Q, and by (ii), the deﬁning points def(v) of
any vertex v of γi are in Q′
ℓ. Thus, the points in def(v) are strictly below the upper hull of Q, i.e., the upper
hull of Q − def(v) is the same as the upper hull of Q. As each vertex v of γi is strictly below the upper hull
of Q − def(v), all points inside γi are indeed pruned.
ℓ can intersect the boundary of ∆k. For each of the O(√rj ) cells γ′
ℓ
strictly contained in ∆k, at most O(√rj
ℓ can intersect the O(log rj ) boundary 
facets of γ′
ℓ. Hence, the number of points in Sk that remain in Q after iteration j is at most

1−ε log rj ) cells γi inside γ′

At most O(√rj

1−ε) cells γ′

min(cid:8)|Sk|, O(√rj

1−ε · n/√rj + √rj · (√rj

the proof is then the same, after readjusting ε by about a half.

1−ε log rj ) · n/rj)(cid:9) = minn|Sk|, O((n/rε/2

j

) log rj)o. The rest of

6. OTHER APPLICATIONS
We can apply our techniques to obtain instance-optimal algorithms for a number of geometric problems in
the order-oblivious and random-order setting:

(1) Oﬀ-line halfspace range reporting in 2-d and 3-d : given a set S of n points and halfspaces, report the
subset of points inside each halfspace. Algorithms with Θ(n log n+ K) running time [Chazelle et al. 1985;
Chan 2000; Afshani and Chan 2009; Chan and Tsakalidas 2015] are known for total output size K.

(2) Oﬀ-line dominance reporting in 2-d and 3-d : given a set S of n red/blue points, report the subset of red

points dominated by each blue point. The problem has similar complexity as (1).

(3) Orthogonal segment intersection in 2-d : given a set S of n horizontal/vertical line segments, report all
intersections between the horizontal and vertical segments, or count the number of such intersections.
The problem is known to have worst-case complexity Θ(n log n + K) in the reporting version for output
size K, and complexity Θ(n log n) in the counting version [de Berg et al. 1997; Preparata and Shamos
1985].

(4) Bichromatic L∞-close pairs in 2-d : given a set S of n red/blue points in 2-d, report all pairs (p, q) where
p is red, q is blue, and p and q have L∞-distance at most 1, or count the number of such pairs. Standard
techniques in computational geometry [de Berg et al. 1997; Preparata and Shamos 1985] yield algorithms
with the same complexity as in (3).

(5) Oﬀ-line orthogonal range searching in 2-d : given a set S of n points and axis-aligned rectangles, report
the subset of points inside each rectangle, or count the number of such points inside each rectangle. The
worst-case complexity is the same as in (3).

(6) Oﬀ-line point location in 2-d : given a set S of n points and a planar connected polygonal subdivision of
size O(n), report the face in the subdivision containing each point. Standard data structures [de Berg
et al. 1997; Preparata and Shamos 1985; Snoeyink 1997] imply a worst-case running time of Θ(n log n).

For each of the above problems, it is not diﬃcult to see that certain input sets are indeed “easier” than
others, for example, if the horizontal segments and the vertical segments respectively lie inside two bounding
boxes that are disjoint, then the orthogonal segment intersection problem can be solved in O(n) time.

Note that although some of the above problems may be reducible to others in terms of worst-case complexity,
 the reductions may not make sense in the instance-optimality setting. For example, an instance-optimal
algorithm for a problem does not imply an instance-optimal algorithm for a restriction of the problem in a
subdomain, because in the latter case, we are competing against algorithms that have to be correct only for
input from this subdomain.

6.1. A general framework for reporting problems

We describe our techniques for oﬀ-line reporting problems in a general framework. Let R ⊂ Rd × Rd′
be a
relation for some constant dimensions d and d′. We say that a red point p ∈ Rd and a blue point q ∈ Rd′
interact if (p, q) ∈ R. We consider the reporting problem: given a set S containing red points in Rd and blue
points in Rd′
of total size n, report all K interacting red/blue pairs of points in S. (By scanning the output
pairs, we can then collect the subset of all blue points that interact with each red point, in O(K) additional
time.)

We redeﬁne H(S) as follows:
Deﬁnition 6.1. Given a cell γ colored red (resp. blue), we say that γ is safe for S if every red (resp. blue)
point in γ interacts with exactly the same subset of blue (resp. red) points in S. We say that a partition Π
of S is respectful if each subset Sk in Π is a singleton, or a subset of red points enclosed by a safe red simplex
∆k for S, or a subset of blue points enclosed by a safe blue simplex ∆k for S. Deﬁne the structural entropy

H(S) of S to be the minimum of H(Π) =Pk(|Sk|/n) log(n/|Sk|) over all respectful partitions Π of S.
Theorem 6.2. OPT(S), OPTavg(S) ∈ Ω(n(H(S) + 1) + K) for the reporting problem in the multilinear

decision tree model.

Proof. This follows from a straightforward modiﬁcation of the proofs of Theorems 3.5 and 4.2. The
main diﬀerence is that we now keep two partition trees, one for the red (resp. blue) points in S, with cells
colored red (resp. blue). If a cell Γ(v) is safe for S, or if the number of red (resp. blue) points in Γ(v) drops

below a constant, then we make v a leaf in the red (resp. blue) partition tree. At the end, we argue that vp
must be a leaf for every red (resp. blue) point p ∈ S. Otherwise, the red (resp. blue) cell Γ(vp) contains at
least two red (resp. blue) points and is not safe, so we can move p to another point inside Γ(vp) and change
the answer. The algorithm would be incorrect on the modiﬁed input. (The Ω(K) term in the lower bound is
obvious.)

For the upper-bound side, we assume the availability of three oracles concerning R, where α is some

positive constant:

(A) A worst-case algorithm for the reporting problem that runs in O(n log n + K) time.
(B) A data structure with O(n log n) preprocessing time, such that we can report all κ blue (resp. red) points

in S interacting with a query red (resp. blue) point in O(n1−α + κ) time.

(C) A data structure with O(n log n) preprocessing time, such that we can test whether a query red or blue

convex polyhedral cell γ of size a is safe for S in O(an1−α) time.
Note that we can reduce to preprocessing time in (B) to O((n/m)· m log m) = O(n log m) while increasing
the query time to O((n/m)·m1−α+κ) for any given 1 ≤ m ≤ n. This follows from the grouping trick by [Chan
1996c]: namely, divide S into ⌈n/m⌉ subsets of size O(m) and build a data structure for each subset. By setting
m = r1/α, we can then answer r queries in total time O(n log m+r·(n/m)·m1−α+κ) ⊆ O(n log r+rO(1)+κ) for
total output size κ. Similarly, in (C), we can answer r queries in total time O(n log r + arO(1)). The grouping
trick is applicable because the query problems in (B) and (C) are decomposable, i.e., the answer of a query
for a union of subsets can be obtained from the answers of the queries for the subsets.

We now solve the reporting problem by a variant of the hull3d algorithm in Section 3.3:

report(Q):
1.
2.

for j = 0, 1, . . . ,⌊log(δ log n)⌋ do

partition the red points in Q by Lemma 3.3 to get rj = 22j
and red cells γ1, . . . , γrj
for each i = 1 to rj do

subsets Q1, . . . , Qrj

3.
4.
5.

6.
7.
8.
9.

if γi is safe for Q then

let Zi be the subset of blue points in Q that interact with
an arbitrary red point in Qi
output Qi × Zi
prune all red points in Qi from Q

redo lines 2–7 with “red” and “blue” reversed

solve the reporting problem for the remaining set Q directly

The test in line 4 for each convex polyhedral cell γi of size at most O(log rj) can be done by querying the
data structure in (C), and line 6 can be done by querying the data structure in (B); the cost of O(rj ) queries
is O(|Q| log rj + rO(1)

) plus the output size. Line 9 can be done by the algorithm in (A).

j

Theorem 6.3. Given oracles (A), (B), and (C), algorithm report(S) runs in O(n(H(S) + 1) + K) time.
Proof. The analysis is as in the proof of Theorem 3.9.

The partition construction in line 2 can be done in the multilinear model, as described in Section 5.
Whether the rest of the algorithm works in the multilinear model depends on the implementation of the
oracles.

For orthogonal-type problems dealing with axis-aligned objects, such as problems (2)–(5) in our list, we
can work instead in the comparison model. We just replace simplices with axis-aligned boxes in the deﬁnition
of H(S), replace convex polyhedral cells with axis-aligned boxes in oracle (C), and replace partition trees
with k-d trees in both the lower-bound proof and the algorithm.
We can immediately apply our framework to the reporting versions of problems (1)–(5), after checking the

oracle requirements for (B) and (C) in each case:

(1) Oﬀ-line halfspace range reporting in 2-d and 3-d : For the design of the needed data structures, it suﬃces
to consider just the lower halfspaces in the input. Color the given points red, and map the given lower
halfspaces to blue points by duality. The data structure problem in (B) is just halfspace range reporting.
The data structure problem in (C) is equivalent to testing whether any of the O(a) edges of a query convex

polyhedral cell intersects a given set of n hyperplanes (lines in 2-d or planes in 3-d). This reduces to
simplex range searching [Agarwal and Erickson 1999; Matouˇsek 1992] by duality; known results achieve
O(n log n) preprocessing time and close to O(an1−1/d) query time. It can be checked that the entire
algorithm is implementable in the multilinear model, at least using a simpler randomized algorithm for
(A) [Chan 2000].

(2) Oﬀ-line dominance reporting in 2-d and 3-d : The data structure problem in (B) is just dominance
reporting. The data structure problem in (C) is equivalent to testing whether all the corners of a query
box are dominated by the same number of points from a given n-point set. This reduces to orthogonal
range counting [Agarwal and Erickson 1999; de Berg et al. 1997; Preparata and Shamos 1985]; although
better data structures are known, k-d trees are suﬃcient for our purposes, with O(n log n) preprocessing
time and O(n1−1/d) query time. The entire algorithm works in the comparison model.

(3) Orthogonal segment intersection in 2-d : Map each each horizontal line segment (x, y)(x′, y) to a red
point (x, x′, y) ∈ R3 and each vertical line segment (ξ, η)(ξ, η′) to a blue point (ξ, η, η′) ∈ R3. Each
point in R3 is the image of a horizontal/vertical line segment. The data structure problem in (B) for red
queries corresponds to reporting all points from a given n-point set that lie in a query range of the form
{(ξ, η, η′) ∈ R3 : ((x ≤ ξ ≤ x′) ∨ (x′ ≤ ξ ≤ x)) ∧ ((η ≤ y ≤ η′) ∨ (η′ ≤ y ≤ η))} for some x, x′, y. This
reduces to 3-d orthogonal range reporting. The data structure problem in (C) for red queries corresponds
to testing whether a query box in R3 intersects any of the boundaries of n given ranges, where each
range is of the form {(x, x′, y) ∈ R3 : ((x ≤ ξ ≤ x′) ∨ (x′ ≤ ξ ≤ x)) ∧ ((η ≤ y ≤ η′) ∨ (η′ ≤ y ≤ η))} for
some ξ, η, η′. This is an instance of 3-d orthogonal intersection searching [Agarwal and Erickson 1999],
which reduces to orthogonal range searching in a higher dimension. Again k-d trees are suﬃcient for our
purposes. Blue queries are symmetric. The entire algorithm works in the comparison model.

j + a mentioned in Remark 2.5, which are allowed in the lower-bound proof.

(4) Bichromatic L∞-close pairs in 2-d : The problem in (B) corresponds to reporting all points of a given
point set that lie inside a query square of side length 2. This is an instance of orthogonal range reporting.
The problem in (C) corresponds to testing whether a query box intersects any of the edges of n given
squares of side length 2. This is an instance of orthogonal intersection searching. Note that here the
resulting algorithm requires a slight extension of the comparison model, to include tests of the form
xi ≤ x′
(5) Oﬀ-line orthogonal range reporting in 2-d : Color the given points red, and map each rectangle [ξ, ξ′] ×
[η, η′] to a blue point (ξ, ξ′, η, η′) ∈ R4. Every point in R4 is the image of a rectangle. The problem in (B)
for red queries corresponds to 2-d rectangle stabbing, i.e., reporting all rectangles, from a given set of n
rectangles, that contain a query point. The problem in (B) for blue queries corresponds to 2-d orthogonal
range reporting. The problem in (C) for red queries corresponds to deciding whether a query box in R2
intersects any of the edges of n given rectangles. The problem in (C) for blue queries corresponds to
deciding whether a query box in R4 intersects any of the boundaries of n given ranges, where each range
is of the form {(ξ, ξ′, η, η′) ∈ R4 : ((ξ ≤ x ≤ ξ′) ∨ (ξ′ ≤ x ≤ ξ)) ∧ ((η ≤ y ≤ η′) ∨ (η′ ≤ y ≤ η))} for some
x, y. All these data structure problems reduce to orthogonal range or intersection searching. Again the
algorithm works in the comparison model.

6.2. Counting problems
Our framework can also be applied to counting problems, where we simply want the total number of interacting 
red/blue pairs. We just change oracle (A) to a counting algorithm without the O(K) term, and oracle (B)
to counting data structures without the O(κ) term. In line 5 of the algorithm we compute |Z|, and in line 6 we
add |Qi|×|Z| to a global counter. The same lowerand 
upper-bound proofs yield an Θ(n(H(S) + 1)) bound.
The new oracle requirements are satisﬁed for (3) orthogonal segment intersection counting, (4) bichromatic
L∞-close pairs, and (5) oﬀ-line orthogonal range counting.

We can also modify the algorithm to return individual counts, i.e., compute the number of red points that
interact with each blue point and the number of blue points that interact with each red point. Here, we need
to not only strengthen oracle (A) to produce individual counts, but also modify oracle (B) to the following:

(B) A data structure with O(n log n) preprocessing time, forming a collection of canonical subsets of total
size O(n log n), such that we can express the subset of all blue (resp. red) points in S interacting with a
query red (resp. blue) point, as a union of O(n1−α) canonical subsets, in O(n1−α) time.

As before, the grouping trick can be used to reduce the preprocessing time and total size of the canonical
subsets. In line 4 of the algorithm we express Z as a union of canonical subsets. In line 5 we add |Z| to the
counter of each red point in Qi and add |Qi| to the counter of each canonical subset for Z. At the end of

the loop in lines 3–7, we make a pass over each canonical subset and add its counter value to the counters
of its blue points, before resetting the counter of the canonical subset. Line 8 is similar. The analysis of the
running time remains the same. The strengthened oracle requirements are satisﬁed for problems (3), (4),
and (5) by known orthogonal range searching results.

6.3. Detection problems?
We can also consider detection problems where we simply want to decide whether there exists an interacting
red/blue pair. Here, we redeﬁne H(S) by redeﬁning “safe”: a red (resp. blue) cell γ is now considered safe
for S if no red (resp. blue) point in γ interacts with any blue (resp. red) points in S. We change oracles (A)
and (B) to analogous detection algorithms and data structures, without the O(K) and O(κ) terms.

The proof of the upper bound O(n(H(S) + 1)) is the same, but unfortunately the proof of the lower bound
Ω(n(H(S) + 1)) only works for instances with a no answer: at the end, if vp is not a leaf for some red (resp.
blue) point p ∈ S, then Γ(vp) contains at least two red (resp. blue) points and is not safe, so we can move p
to some point inside Γ(vp) and change the answer from no to yes.
yes instances are problematic, but this is not a weakness of our technique but of the model: on every input
set S with a yes answer, OPT(S) is in fact O(n). To see this, consider an input set S for which there exists
an interacting pair (p, q). An algorithm that is “hardwired” with the ranks of p and q in S with respect to,
say, the x-sorted order of S can ﬁrst ﬁnd p and q from their ranks by linear-time selection, verify that p
and q interact in constant time, and return yes if true or run a brute-force algorithm otherwise. Then on
every permutation of this particular set S, the algorithm always takes linear time. Many problems admit
Ω(n log n) worst-case lower bounds even when restricted to yes instances, and for such problems, instance
optimality in the order-oblivious setting is therefore not possible on all instances.

6.4. Another general framework for oﬀ-line querying problems

We now study problems from another general framework. Let M be a mapping from points in Rd to “answers”
in some space for some constant d (the answer M(q) of a point q ∈ Rd may or may not have constant size
depending on the context). We consider the following oﬀ-line querying problem: given a set S of n points in
Rd, compute M(q) for every q ∈ S. Let K denote the total size of the answers.

We redeﬁne H(S) by redeﬁning “safe”:
Deﬁnition 6.4. We say that a cell γ is safe if every point q in γ has the same answer M(q). We say that
a partition Π of S is respectful if each subset Sk in Π is a singleton, or a subset of points enclosed by a safe

simplex ∆k. Deﬁne the structural entropy H(S) of S to be the minimum of H(Π) =Pk(|Sk|/n) log(n/|Sk|)
Theorem 6.5. OPT(S), OPTavg(S) ∈ Ω(n(H(S) + 1) + K) for the oﬀ-line querying problem in the

over all respectful partitions Π of S.

multilinear decision tree model.

Proof. This follows from a straightforward modiﬁcation of the proofs of Theorems 3.5 and 4.2. As before,
if a cell Γ(v) is safe, then we make v a leaf. At the end, we argue that vp must be a leaf for every p ∈ S.
Otherwise, Γ(vp) is not safe, so we can move p to another point inside Γ(vp) and change the answer. The
algorithm would be incorrect on the modiﬁed input.

The above lower bound holds even if we ignore the cost of preprocessing M. Furthermore, the test functions
For the upper-bound side, we assume that M has been preprocessed in an oracle data structure supporting

are only required to be multilinear with respect to S, not M.
the following types of queries:

(A) Given q ∈ Rd, we can compute M(q) in O(log m + κ) worst-case time for output size κ, where m is a
(C) Given a convex polyhedral cell γ of size a, we can test whether γ is safe in O(am1−α) time.

parameter describing the size of M.

The algorithm is simpler this time. Instead of using a 22j

progression, we can use a more straightforward
b-way recursion, for a suﬃciently large constant b (the resulting recursion tree mimics the tree T from the
lower-bound proof in Theorem 3.5, on purpose):

if |Q| drops below n/mδ then compute the answers directly and return
for i = 1 to b do

off-line-queries(Q, Γ), where Q ⊂ Γ:
1.
2. partition Q by Lemma 3.3 to get b subsets Q1, . . . , Qb and cells γ1, . . . , γb
3.
4.
5.
6.
7.

compute M(q) for an arbitrary point q ∈ γi ∩ Γ
output M(q) as the answer for all points in Qi

if γi ∩ Γ is safe then

else off-line-queries(Qi, γi ∩ Γ)

We call off-line-queries(S, Rd) to start. Line 1 takes O(|Q| log m+κ) time for output size κ by querying
the data structure for (A); note that each point in Q in this case has participated in Ω(log m) levels of
the recursion, and we can account for the ﬁrst term by charging each point unit cost for every level it
participates in. Line 2 takes O(|Q|) time for a constant b by known constructions of Matouˇsek’s partition
theorem [Matouˇsek 1992] (or alternatively recursive application of the 4or 
8-sectioning theorem in the 2-d
or 3-d case). The test in line 4 takes O(m1−α polylog m) time by querying the data structure for (C), since
the convex polyhedral cell γi ∩ Γ has at most O(log m) facets (and thus O(polylog m) size). As the tree has
O(mδ) nodes, the cost of line 4 is negligible, since its total over the entire recursion tree is sublinear in m by
choosing a suﬃciently small constant δ < α. Line 5 takes O(log m + κ) time for output size κ, by (A); the
O(log m) term is again negligible, since its total over the entire recursion tree is sublinear in m.

runs in O(n(H(S) + 1) + K) + o(m) time for total output size K.

Theorem 6.6. After M has been preprocessed for (A) and (C), algorithm off-line-queries(S, Rd)
the recursion. The total running time for the oﬀ-line problem is asymptotically bounded byPj nj, ignoring

Proof. Let nj be number of points in S that survive level j, i.e., participate in subsets Q at level j of

a o(m) extra term.

Let Π be any respectful partition of S. Consider a subset Sk in Π. Let ∆k be a safe simplex enclosing Sk.
Fix a level j. Let Qi’s and γi’s be the subsets Q and cells γ at level j. Each Qi has size at most n/Θ(b)j.
The number of γi’s that intersect the boundary of ∆k is at most O(b1−ε)j. Thus, the number of points in

Sk that survive level j is at most min(cid:8)|Sk|, O(b1−ε)j · n/Θ(b)j(cid:9). Since the Sk’s cover the entire point set,

with a double summation we have, for a suﬃciently large constant b,

Xj

nj ≤ Xj Xk
= Xk Xj
⊆ Xk

min(cid:8)|Sk|, n/Θ(b)εj(cid:9)
min(cid:8)|Sk|, n/Θ(b)εj(cid:9)

O (|Sk|(log(n/|Sk|) + 1))

= O(n(H(Π) + 1)),

which yields the theorem.

For orthogonal-type problems, we can work instead in the comparison model. We just replace simplices
with axis-aligned boxes in the deﬁnition of H(S), replace convex polyhedral cells with axis-aligned boxes in
oracle (C), and replace partition trees with k-d trees in both the lower-bound proof and the algorithm.

We can apply our framework to solve problem (6):

(6) Oﬀ-line point location in 2-d : For (A), data structures for planar point location with O(log m) worstcase 
query time are known, with O(m) preprocessing time and space [Kirkpatrick 1983; Chazelle 1991;
Snoeyink 1997]. The data structure problem in (C) is equivalent to testing whether any of the O(a) edges
of a query polygon intersect the given polygonal subdivision. This reduces to ray shooting (or segment
emptiness) queries in the subdivision, for which known results [Chazelle et al. 1994] achieve O(log m)
query time, with O(m) preprocessing time and space. For this problem, each answer has constant size, so
the O(K) and O(κ) terms can be omitted. The total running time is O(n(H(S)+1)), even if preprocessing
time is included, for a subdivision of size m = O(n). It can be checked that the entire algorithm works
in the multilinear model.

6.5. On-line querying problems and distribution-sensitive data structures
In the general framework of the preceding section, we can also consider the following on-line querying
problem: given a set S of n points in Rd, build a data structure so that we can compute M(q) for any query
point q ∈ Rd, while trying to minimize the average query cost over all q ∈ S.
Our oﬀ-line lower bound states that the total time required to answer queries for all n points in S is
Ω(n(H(S) + 1)). This immediately implies that the average query time over all q ∈ S must be Ω(H(S) + 1).
(In contrast, lower bounds for the on-line problem do not necessarily translate to lower bounds for the oﬀ-line
problem.)

On the other hand, our algorithm for oﬀ-line queries can be easily modiﬁed to give a data structure 
for on-line queries. We just build a data structure corresponding to the recursion tree generated by
off-line-queries(S, Rd), in addition to the data structure for (A) and (C). It can be shown that with such
a data structure, the average query time over all q ∈ S is O(H(S) + 1) (more details are given below).
We can extend the on-line querying problem to the setting where each point in S is weighted and the
goal is to bound the weighted average query time over the query points in S. Even more generally, we can
consider the setting where S is replaced by a (possibly continuous) probability distribution and the goal is
to bound the expected query time for a query point randomly chosen from S. We now provide more details
for the changes needed in this most general setting.

We ﬁrst redeﬁne H(S) for a probability distribution S.
Deﬁnition 6.7. We say that a cell γ is safe if every point q in γ has the same answer M(q). A partition Π of
Rd into regions is respectful if each region Sk of Π can be enclosed by a safe simplex ∆k. Deﬁne the structural

entropy H(S) of a probability distribution S to be the minimum of H(Π) =Pk µS(Sk) log(1/µS(Sk)) over

all respectful partition Π of Rd, where µS denotes the probability measure corresponding to S.

We need a continuous version of Lemma 3.3, which follows by straightforward modiﬁcation to the proof
of the partition theorem [Matouˇsek 1992] (or alternatively, recursive application of the 4or 
8-sectioning
theorem in the 2-d or 3-d case).

Lemma 6.8. For any probability measure in Rd and 1 ≤ r ≤ n for any constant d, we can partition Rd
into r (not necessarily convex or connected) polyhedral regions Q1, . . . , Qr each with measure Θ(1/r) and
with rO(1) (or fewer) facets, and ﬁnd r convex polyhedral cells γ1, . . . , γr each with O(log r) (or fewer) facets,
such that Qi is contained in γi, and every hyperplane intersects at most O(r1−ε) cells. Here, ε > 0 is a
constant that depends only on d.

The lower bound proof is easier for on-line problems, so we present the simpliﬁed proof in full below.
Because the input to a query algorithm is just a single point (unlike in the oﬀ-line setting where we could
perform a test that has more than one query points as arguments), multilinear tests can now be replaced
with linear tests.

Theorem 6.9. Any algorithm for the on-line querying problem requires Ω(H(S) + 1 + κ) expected query

time for output size κ, for any probability distribution S in the linear decision tree model.

Proof. We deﬁne a partition tree T as follows: Each node v stores a pair (Q(v), Γ(v)), where Q(v) is a
region enclosed inside a convex polyhedral cell Γ(v). The root stores (Rd, Rd). If µS(Q(v)) drops below 1/mδ
or Γ(v) is safe, then v is a leaf. Otherwise, apply Lemma 3.3 with r = b to the restriction of µS to Q(v), and
obtain regions Q1, . . . , Qb and cells γ1, . . . , γb. For the children v1, . . . , vb of v, set Q(vi) = Qi ∩ Q(v) and
Γ(vi) = γi ∩ Γ(v). For a node v at depth j of the tree T we then have µS(Q(v)) ≥ 1/Θ(b)j, and consequently,
the depth j is in Ω(logb(1/µS(Q(v)))).
Let Πpart-tree be the partition formed by the cells Γ(v) at the leaves v in T . Let eΠpart-tree be a reﬁnement
any constant b. So H(eΠpart-tree) ∈ Θ(H(Πpart-tree)). Clearly, eΠpart-tree is respectful.

of this partition after triangulating the leaf cells. Note that the subpartitioning of a leaf cell at depth j
causes the entropy to increase by at most O(µS(Q(v)) log(j log b)) ⊆ O(µS(Q(v)) log log(1/µS(Q(v)))) for
The adversary constructs a bad query point q as follows. During the simulation, we maintain a node vq in

T , where the only information the algorithm knows about q is that q lies inside Γ(vq).
the following:
— We reset vq to one of its children at random, where each child v′

Suppose that the simulation encounters a test to determine which side q lies inside a hyperplane h. We do

q is chosen with probability µS(Q′

q)/µS(Qq)

(which is in Θ(1/b)). If Γ′

q does not intersect h, then the comparison is resolved. Otherwise, we repeat.

The probability that the comparison is not resolved in a single step is at most O(b1−ε) · Θ(1/b), which can
be made less than 1/2 for a suﬃciently large constant b.
Let T be the number of tests made. Let D be the depth of vq at the end. For i ≤ T , let Ti = 1 and Di
be the number of steps needed to resolve the i-th test; then E[Di] ≤ 2. For i > T , let Ti = Di = 0. Since

E[2Ti − Di] ≥ 0 for all i, by linearity of expectation E[D] = E[Pi Di] ≤ 2 E[Pi Ti] = 2 E[T ].

At the end of the algorithm, vq must be a leaf. Thus,

E[T ] ≥ Ω(E[D])

µS(Q(v)) log(1/µS(Q(v)))!

⊆ Ω Xleaf v
⊆ Ω(H(eΠpart-tree))

⊆ Ω(H(Πpart-tree))

⊆ Ω(H(S)).

At the end, we can assign q to a random point in Q(vq) chosen from the distribution S. Then q satisﬁes
precisely the probability distribution S (in other words, the adversary have not acted adversarily after all).
Thus, E[T ] is the expected query time for a query point randomly chosen from the distribution S.

For the upper-bound side, below is the pseudocode for the preprocessing algorithm and the query algorithm,
which are derived from our previous algorithm off-line-queries; here, b is a suﬃciently large constant.
The query algorithm assumes an oracle data structure for (A) (oracle (C) is only needed in the preprocessing
algorithm).

if µS(Q) < 1/mδ then return

preprocess(Q, Γ):
1.
2. apply Lemma 6.8 to the restriction of µS to Q
to get b regions Q1, . . . , Qb and cells γ1, . . . , γb
for i = 1 to b do

3.
4.
5.

if γi ∩ Γ is safe then store M(q) for an arbitrary point q ∈ γi ∩ Γ
else preprocess(Qi ∩ Q, γi ∩ Γ)

on-line-query(q, Q, Γ):
1.
2.
3.
4.

if µS(Q) < 1/mδ then compute M(q) directly and return
locate the region Qi containing q
if γi ∩ Γ was marked as safe then look up the stored answer and return
else on-line-query(q, Qi ∩ Q, γi ∩ Γ)

The space of the tree generated by preprocess(Rd, Rd) is only O(mδ), and so the space for the data
structure for (A) dominates. We will not focus on the preprocessing time, which depends on the construction
time for Lemma 6.8, which in turn depends on the distribution S. The preprocessing can be done eﬃciently,
for example, for a discrete n-point distribution and for many other distributions.

In algorithm on-line-query, line 1 takes O(log m) time by (A); note that this case occurs only when the
query point q participates in Ω(log m) levels of the recursion, and we can account for the cost by charging
one unit to each level of the recursion. Line 2 takes O(1) time for a constant b. We can adapt the previous
proof of Theorem 6.6 for the rest of the query time analysis:

Proof. Let nj be 1 if the query point participates at level j of the recursion, and 0 otherwise. Then the

Theorem 6.10. After M has been preprocessed for (A) and preprocess(Rd, Rd) has been executed, algorithm 
on-line-query(q, Rd, Rd) runs in O(H(S) + 1 + κ) expected time for output size κ, for a query point
q randomly chosen from the distribution S.
query time is asymptotically bounded byPj nj.
Let Π be any respectful partition of S. Consider a region Sk in Π, enclosed in a safe simplex ∆k. Fix a level
j. Let Qi’s and Γi’s be the regions and cells at level j of the recursion tree. Each Qi satisﬁes µS(Qi) ≤ 1/Θ(b)j.
The number of Γi’s that intersect the boundary of ∆k is at most O(b1−ε)j. Thus, Pr[(nj = 1) ∧ (q ∈ Sk)] ≤

min{µS(Sk), O(b1−ε)j · 1/Θ(b)j} for a point q randomly chosen from the distribution S. Since the Sk’s cover
Rd, with a double summation we have, for a suﬃciently large constant b,

EXj

nj ≤ Xj Xk
= Xk Xj
⊆ Xk

= O(H(Π) + 1),

min{µS(Sk), 1/Θ(b)εj}

min{µS(Sk), 1/Θ(b)εj}

O (µS(Sk)(log(1/µS(Sk)) + 1))

which yields the theorem.

For orthogonal-type problems, we can again work in the comparison model, by replacing simplicial and

convex polyhedral cells with axis-aligned boxes.

We can apply our framework to on-line versions of several problems:

(6) On-line point location queries in 2-d : We immediately obtain optimal O(H(S) + 1) expected query cost,
with an O(m)-space data structure for a subdivision of size m, for any given distribution S. The query
algorithm works in the linear decision tree model. This on-line point location result is known before
[Arya et al. 2007a; Arya et al. 2007b; Collette et al. 2012; Iacono 2004] (some of these previous work
even optimize the constant factor in the query cost).

(1) On-line halfspace range reporting queries in 2-d and 3-d : Here, we map query lower halfspaces to points
by duality. For (A), data structures for 2-d and 3-d halfspace range reporting with O(log m + κ) worstcase 
time are known, with O(m) space [Chazelle et al. 1985; Afshani and Chan 2009]. We thus obtain
optimal O(H(S) + 1 + κ) expected query cost for output size κ, with an O(m)-space data structure for
a given 2-d or 3-d m-point set, for any given distribution S. The query algorithm works in the linear
decision tree model. This result is new.

(2) On-line dominance reporting queries in 2-d and 3-d : The story is similar to halfspace range reporting.

The query algorithm now works in the comparison model.

(4) On-line orthogonal range reporting/counting queries in 2-d : Here, we map query rectangles to points
in 4-d as in Section 6.1. For (A), data structures for 2-d orthogonal range reporting with O(log m + κ)
worst-case query time are known, with O(m logε m) space; and data structures for 2-d orthogonal range
counting with O(log m) worst-case query time are known, with O(m) space [Chazelle 1988]. For reporting,
we thus obtain optimal O(H(S)+1+κ) expected query cost for output size κ, with an O(m logε m)-space
data structure for a given 2-d m-point set, for any given distribution S; for counting, we get optimal
O(H(S) + 1) expected query cost with an O(m)-space data structure. The query algorithm works in the
comparison model. This result is apparently new, as it extends Dujmovi´c, Howat, and Morin’s result
on 2-d dominance counting [Dujmovi´c et al. 2012] and unintentionally answers one of their main open
problems (and at the same time improves their space bound from O(m log m) to O(m)).

Remark 6.11. Some months after the appearance of the conference version of the present paper, a similar

general technique for distribution-sensitive data structures was rediscovered by [Bose et al. 2010].

7. DISCUSSION
Although we have argued for the order-oblivious form of instance optimality, we are not denigrating adaptive
algorithms that exploit the order of the input. Indeed, for some geometric applications, the input order may
exhibit some sort of locality of reference which can speed up algorithms. There are various parameters that
one can deﬁne to address this issue, but it is unclear how a uniﬁed theory of instance optimality can be
developed for order-dependent algorithms.

We do not claim that the algorithms described here are the best in practice, because of possibly larger
constant factors (especially those that use Matouˇsek’s partition trees), although some variations of the ideas
might actually be useful. In some sense, our results can be interpreted as a theoretical explanation for why
heuristics based on bounding boxes and BSP trees perform so well (for example, see [Andrews et al. 1994]
on experimental results for the red/blue segment intersection problem), as many of our instance-optimal
algorithms prune input based on bounding boxes and spatial tree structures.

q0

q1

q2

p4 p5 p6

p1 p2 p3

p7 p8 p9

q3

q4

Fig. 8. Deﬁnition of F (S): In this instance, points p1, p2, p3 are
dominated by q1, q2, q3, and so |F (p1)| = |F (p2)| = |F (p3)| =
n = 12. Points p4, p5, p6, q2 are dominated only by q2, and so
|F (p4)| = |F (p5)| = |F (p6)| = |F (q2)| = 7. Similarly |F (p7)| =
|F (p8)| = |F (p9)| = |F (q3)| = 4 and |F (q1)| = 7. Thus, F (S) =
3 log 12

7 + 4 log 12
4 .

12 + 5 log 12

Note that specializations of our techniques to 1-d can also lead to order-oblivious instance-optimal results
for the multiset-sorting problem and the problem of computing the intersection of two (unsorted) sets.
Adaptive algorithms for similar 1-d problems (e.g., [Munro and Spira 1976]) were studied in diﬀerent settings
from ours.

Not all standard geometric problems admit nontrivial instance-optimal results in the order-oblivious setting.
 For example, computing the Voronoi diagram of n points or the trapezoidal decomposition of n disjoint
line segments, both having Θ(n) sizes, requires Ω(n log n) time for every point set by the naive informationtheoretic 
argument. Computing the (L∞-)closest pair for a monochromatic point set requires Ω(n log n) time
for every point set by our adversary lower-bound argument.

An open problem is to strengthen our lower bound proofs to allow for a more general class of test functions

beyond multilinear functions, for example, arbitrary ﬁxed-degree algebraic functions.

It remains to see how widely applicable the concept of instance optimality is. To inspire further work,
we mention the following geometric problems for which we currently are unable to obtain instance-optimal
results:

(a)

reporting all intersections between a set of disjoint red (nonorthogonal) line segments and a set of
disjoint blue line segments in 2-d;

(b) computing the L2or 
L∞-closest pair between a set of red points and a set of blue points in 2-d;
(c) computing the diameter or the width of a 2-d point set;
(d) computing the lower envelope of a set of (perhaps disjoint) line segments in 2-d.

Finally, we should mention that all our current results concern at most logarithmic-factor improvements.
Obtaining some form of instance-optimal results for problems with ω(n log n) worst-case complexity (for
example, oﬀ-line triangular range searching, 3SUM-hard problems, . . . ) would be even more fascinating.

APPENDIX

A. AN ALTERNATIVE PROOF FOR 2-D MAXIMA
In this appendix, we describe an alternative approach to the 2-d maxima problem, which uses a new deﬁnition
of a diﬃculty measure and a vastly diﬀerent lower-bound proof based on an interesting encoding argument.
This approach is more specialized and does not seem to work for 3-d maxima or other problems, but the
lower-bound proof has the advantage of being generalizable to nondeterministic algorithms.

We begin by deﬁning a measure of diﬃculty F (S) speciﬁc to the 2-d maxima problem, which is seemingly
diﬀerent from the structural entropy H(S) deﬁned previously. The new deﬁnition appears simpler in the
sense that we do not need to take the minimum over all partitions but measure the contribution of each
point directly, but as a byproduct of our analyses, F (S) is asymptotically equivalent to nH(S) (which is why
we do not give it a name).

Deﬁnition A.1. Given a point set S, let q1, . . . , qh denote the maximal points of S from left to right,
with q0 = (−∞,∞) and qh+1 = (∞,−∞). Given a point p ∈ S, let qi, . . . , qℓ be all the maximal points that
dominate p. Deﬁne F (p) to be the subset of all points in S in the slab (qi−1.x, qℓ+1.x) × R, where we use p.x

and p.y to denote the xand 
y-coordinates of p. Deﬁne F (S) =Pp∈S log(n/|F (p)|). (See Figure 8.)

For the upper-bound side, we use the same algorithm and an analysis similar to before:
Theorem A.2. Algorithm maxima2d(S) from Section 2.1 runs in O(F (S) + n) time.
Proof. We proceed as in the proof of Theorem 2.3, but a simpler argument replaces the second paragraph:

Fix a point p ∈ S. Let qi, . . . , qℓ be all the maximal points that dominate p. Fix a level j. If |F (p)| >(cid:4)n/2j(cid:5),

then by (i), that some maximal point from {qi, . . . , qℓ} must been discovered, and by (ii), this implies that
p does not survive level j. Thus, p can survive only for O(log(n/|F (p)|) + 1) levels. We can asymptotically

bound the running time byPj nj ∈ O(Pp(log(n/|F (p)|) + 1)) = O(F (S) + n).

For the lower-bound side, we ﬁrst consider a slightly strengthened problem which we call maxima with
witnesses: given a point set S, report (the indices of) all maximal points in left-to-right order, and for each
nonmaximal point p in S, report a maximal point (a witness) that dominates p.

Theorem A.3. OPT(S), OPTavg(S) ∈ Ω(F (S) + n) for the 2-d “maxima with witness” problem in the

comparison model.

Proof. The proof is a counting argument, which we express in terms of encoding schemes (see [Demaine
and L´opez-Ortiz 2003; Golynski 2009] for more sophisticated examples of counting arguments based on
encoding/decoding). We will describe a way to encode an arbitrary permutation σ of S, so that the length of
the encoding can be upper-bounded in terms of the running time of the given algorithm A on input σ. Since
the worst-case encoding length must be at least log(n!), the running time must be large for some permutation
σ. (All logarithms are in base 2.)

To describe the encoding scheme, we imagine that the permutation σ is initially unknown, and as we
proceed, we record bits of information about σ so that at the end, σ can be uniquely determined from these
bits. In the description below, we distinguish between an input point , as represented its index in the input
permutation σ (its actual location is not necessarily known), and an actual point in S (its coordinates are
known but its index in σ is not necessarily known). At any moment, if we know which input point corresponds
to an actual point p, we say (naturally) that p is known.

We ﬁrst simulate the algorithm on σ and record the outcomes of the comparisons made; this requires
TA(σ) bits (recall that TA(σ) denote the number of comparisons made by A on σ). Let M be the list of
maximal input points returned. For each input point qi, let W (qi) be the list of all nonmaximal input points
that have qi as witness. For each maximal actual point, we record its position in M , using h⌈log h⌉ bits in
total. Now all maximal points are known.
We process the nonmaximal actual points of S from left to right, and make them known as follows. To
process an actual point p, let qi, . . . , qj be all the maximal points that dominate p, which are all known.
Observe that p must be in W (qi) ∪ ··· ∪ W (qj). Let L be all the points that are left of p, which are all
known. We record the position of p in the list (W (qi)∪···∪ W (qj ))− L of input points (say, ordered by their
indices). This requires ⌈log(|(W (qi) ∪ ··· ∪ W (qj)) − L|⌉ bits. Observe that W (qi)∪···∪ W (qj) is contained
in (−∞, qj.x) × R. So, (W (qi) ∪ ··· ∪ W (qj)) − L is contained in the subset F (p) from our Deﬁnition A.1—a
lucky coincidence. Thus, the number of bits required is ⌈log |F (p)|⌉. Now p is known and we can continue
the process.
By our construction, any permutation σ of S can be uniquely decoded from its encoding, for any given

set S. The encoding has total length at most

TA(σ) + h log h +Xp

log |F (p)| + O(n) = TA(σ) + h log h + n log n − F (S) + O(n).

Taking the maximum over all permutations σ of S, we thus obtain log(n!) ≤ TA(S) + h log h + n log n −
F (S) + O(n), yielding TA(S) + n + h log h ∈ Ω(F (S)). Combined with the trivial lower bound Ω(n) and the
naive information-theoretic lower bound TA(S) ∈ Ω(h log h) (as the problem deﬁnition requires the output
to be in sorted order), this implies that TA(S) ∈ Ω(F (S) + n).
The proof works in the random-order setting as well: In any encoding scheme, at most a fraction 2−cn of
the n! permutations can have encoding length less than log(n!) − cn for any constant c. Thus, for a random
permutation σ, with high probability the encoding length is at least log(n!) − O(n), implying TA(σ) ∈
Ω(F (S) + n). In particular, T avg

A (S) ∈ Ω(F (S) + n).

Combining the above theorem with the following observation yields a complete proof of the Ω(F (S) + n)

lower bound:

Observation A.4. Any algorithm for the 2-d maxima problem in the comparison model can be made to

solve the 2-d “maxima with witnesses” problem without needing to make any additional comparisons.

Proof. Consider the partial order ≺x over S formed by the outcomes of the x-comparisons made by the
maxima algorithm A. Deﬁne the partial order ≺y similarly. Fix a nonmaximal point p. We argue that there
must be a point q ∈ S such that p ≺x q and p ≺y q. Suppose that every q ∈ S has p 6≺x q or p 6≺y q. Consider

modifying the point set as follows: First, increase the x-coordinates of p and all points in {q ∈ S : p ≺x q} by
a suﬃciently large common value; this does not aﬀect the outcomes of the comparisons made, and ensures
that all points q with p 6≺x q now have p.x > q.x. Similar, increase the y-coordinates of p and all points in
{q ∈ S : p ≺y q} by a suﬃciently large common value; all points q with p 6≺y q now have p.y > q.y. Then
every q ∈ S now has p.x > q.x or p.y > q.y, i.e., p is now maximal, and the algorithm would be incorrect on
the modiﬁed point set: a contradiction.
For every nonmaximal point p, we can thus ﬁnd a witness point q that dominates p, without making any
additional comparisons. One issue remains: the witness point may not be maximal. If not, we can change p’s
witness to the witness of the witness, and so on, until p’s witness is maximal.

Finally, note that we do not require the given algorithm A to report the maximal points in left-to-right
order. We argue that at the end we already know the x-order of the maximal points. Suppose that q 6≺x q′
for two consecutive maximal points q and q′. Consider modifying the point set as follows: Increase the xcoordinates 
of q and all points in {p ∈ S : q ≺x p} by a suﬃciently large common value; this does not
aﬀect the outcomes of the comparisons made, and ensures that we now have q.x > q′.x (while maintaining
q.y > q′.y). Then q′ is now nonmaximal, and the algorithm would be incorrect on the modiﬁed point set: a
contradiction.

Remark A.5. The proof can be modiﬁed for weaker versions of the problem, for example, reporting just

the number of maximal points (or its parity).

The proof does not appear to work for problems other than maxima in 2-d. One obvious issue is that
Observation A.4 only applies to comparison-based algorithms for orthogonal-type problems. Even more
critically, the proof of Theorem A.3 relies on a coincidence that is special to 2-d maxima.

Curiously, this lower-bound proof holds even for nondeterministic algorithms, i.e., algorithms that can
make guesses but must verify that the answer is correct; here we assume that each bit guessed costs unit
time. In the proof of Theorem A.3, we just record the guesses in the encoding. The previous proofs of instance
optimality by [Fagin et al. 2003] and [Demaine et al. 2000] all hold in nondeterministic settings. Perhaps this
strength of the proof prevents its applicability to other geometric problems, whereas our adversary-based
proofs more powerfully exploits the deterministic nature of the algorithms.

REFERENCES

Peyman Afshani and Timothy M. Chan. 2009. Optimal halfspace range reporting in three dimensions. In Proc. 20th ACM-SIAM

Symposium on Discrete Algorithms. 180–186.

Pankaj K. Agarwal and Jeﬀ Erickson. 1999. Geometric range searching and its relatives. In Advances in Discrete and Computational 
Geometry, B. Chazelle, J. E. Goodman, and R. Pollack (Eds.). Contemporary Mathematics, Vol. 223. American
Mathematical Society, Providence, RI, 1–56.

Nir Ailon, Bernard Chazelle, Kenneth L. Clarkson, Ding Liu, Wolfgang Mulzer, and C. Seshadhri. 2011. Self-improving algorithms.
 40 (2011), 350–375.

D. S. Andrews, J. Snoeyink, J. Boritz, T. Chan, G. Denham, J. Harrison, and C. Zhu. 1994. Further comparisons of algorithms

for geometric intersection problems. In Proc. 6th International Symposium on Spatial Data Handling. 709–724.

Sunil Arya, Theocharis Malamatos, and David M. Mount. 2007a. A simple entropy-based algorithm for planar point location.

ACM Transactions on Algorithms 3, Article 17 (2007).

Sunil Arya, Theocharis Malamatos, David M. Mount, and Ka Chun Wong. 2007b. Optimal expected-case planar point location.

SIAM J. Comput. 37 (2007), 584–610.

Ilya Baran and Erik D. Demaine. 2005. Optimal adaptive algorithms for ﬁnding the nearest and farthest point on a parametric

black-box curve. International Journal of Computational Geometry and Applications 15 (2005), 327–350.

J´er´emy Barbay and Eric Chen. 2008. Adaptive planar convex hull algorithm for a set of convex hulls. In Proc. 20th Canadian

Conference on Computational Geometry. 47–50.

Michael Ben-Or. 1983. Lower bounds for algebraic computation trees. In Proc. 15th ACM Symposium on Theory of Computing.

80–86.

Jon Louis Bentley, Kenneth L. Clarkson, and David B. Levine. 1990. Fast linear expected-time algorithms for computing

maxima and convex hulls. In Proc. 1st ACM-SIAM Symposium on Discrete Algorithms. 179–187.

Binay K. Bhattacharya and Sandeep Sen. 1997. On a simple, practical, optimal, output-sensitive randomized planar convex

hull algorithm. Journal of Algorithms 25 (1997), 177–193.

Prosenjit Bose, Luc Devroye, Karim Dou¨ıeb, Vida Dujmovi´c, James King, and Pat Morin. 2010. Odds-on trees. arXiv

abs/1002.1092 (2010).

Joan Boyar and Lene M. Favrholdt. 2007. The relative worst order ratio for online algorithms. ACM Transactions on Algorithms

3, Article 22 (2007).

Timothy M. Chan. 1996a. Fixed-dimensional linear programming queries made easy. In Proc. 12th ACM Symposium on Computational 
Geometry. 284–290.

Timothy M. Chan. 1996b. Optimal output-sensitive convex hull algorithms in two and three dimensions. Discrete and Computational 
Geometry 16 (1996), 361–368.

Timothy M. Chan. 1996c. Output-sensitive results on convex hulls, extreme points, and related problems. Discrete and Computational 
Geometry 16 (1996), 369–387.

Timothy M. Chan. 2000. Random sampling, halfspace range reporting, and construction of (≤ k)-levels in three dimensions.

SIAM J. Comput. 30 (2000), 561–575.

Timothy M. Chan. 2010. Comparison-based time–space lower bounds for selection. ACM Transactions on Algorithms 6, Article

26 (2010).

Timothy M. Chan. 2012. Optimal partition trees. Discrete and Computational Geometry 47 (2012), 661–690.
Timothy M. Chan, Jack Snoeyink, and Chee-Keng Yap. 1997. Primal dividing and dual pruning: Output-sensitive construction
of four-dimensional polytopes and three-dimensional Voronoi diagrams. Discrete and Computational Geometry 18 (1997),
433–454.

Timothy M. Chan and Konstantinos Tsakalidas. 2015. Optimal deterministic algorithms for 2-d and 3-d shallow cuttings. In

Proc. 31st Symposium on Computational Geometry. To appear.

Bernard Chazelle. 1988. A functional approach to data structures and its use in multidimensional searching. SIAM J. Comput.

17 (1988), 427–462.

Bernard Chazelle. 1991. Triangulating a simple polygon in linear time. Discrete and Computational Geometry 6 (1991), 485–524.
Bernard Chazelle, Herbert Edelsbrunner, Michelangelo Grigni, Leonidas J. Guibas, John Hershberger, Micha Sharir, and Jack

Snoeyink. 1994. Ray shooting in polygons using geodesic triangulations. Algorithmica 12 (1994), 54–68.

Bernard Chazelle, Leo J. Guibas, and D. T. Lee. 1985. The power of geometric duality. BIT 25 (1985), 76–90.
Bernard Chazelle and Jiˇr´ı Matouˇsek. 1995. Derandomizing an output-sensitive convex hull algorithm in three dimensions.

Computational Geometry: Theory and Applications 5 (1995), 27–32.

Kenneth L. Clarkson. 1994. More output-sensitive geometric algorithms. In Proc. 35th IEEE Symposium on Foundations of

Computer Science. 695–702.

Kenneth L. Clarkson. 1995. Las Vegas algorithms for linear and integer programming. J. ACM 42 (1995), 488–499.
Kenneth L. Clarkson and Peter W. Shor. 1989. Applications of random sampling in computational geometry, II. Discrete and

Computational Geometry 4 (1989), 387–421.

S´ebastien Collette, Vida Dujmovi´c, John Iacono, Stefan Langerman, and Pat Morin. 2012. Entropy, triangulation, and point

location in planar subdivisions. ACM Transactions on Algorithms 8, Article 29 (2012).

Mark de Berg, Matthew Katz, A. Frank van der Stappen, and Jules Vleugels. 2002. Realistic input models for geometric

algorithms. Algorithmica 34 (2002), 81–97.

Mark de Berg, Marc van Kreveld, Mark Overmars, and Otfried Schwarzkopf. 1997. Computational Geometry: Algorithms and

Applications. Springer-Verlag.

Erik D. Demaine, Dion Harmon, John Iacono, Daniel Kane, and Mihai Pˇatra¸scu. 2009. The geometry of binary search trees. In

Proc. 20th ACM-SIAM Symposium on Discrete Algorithms. 496–505.

Erik D. Demaine and Alejandro L´opez-Ortiz. 2003. A linear lower bound on index size for text retrieval. Journal of Algorithms

48 (2003), 2–15.

Erik D. Demaine, Alejandro L´opez-Ortiz, and J. Ian Munro. 2000. Adaptive set intersections, unions, and diﬀerences. In Proc.

11th ACM-SIAM Symposium on Discrete Algorithms. 743–752.

Vida Dujmovi´c, John Howat, and Pat Morin. 2012. Biased range trees. Algorithmica 62 (2012), 21–37.
Herbert Edelsbrunner. 1987. Algorithms in Combinatorial Geometry. Springer-Verlag.
Herbert Edelsbrunner and Weiping Shi. 1990. An O(n log2 h) time algorithm for the three-dimensional convex hull problem.

SIAM J. Comput. 20 (1990), 259–269.

Jeﬀ Erickson. 2005. Dense point sets have sparse Delaunay triangulations. Discrete and Computational Geometry 33 (2005),

83–115.

Ronald Fagin, Amnon Lotem, and Moni Naor. 2003. Optimal aggregation algorithms for middleware. J. Comput. System Sci.

66 (2003), 614–656.

Alexander Golynski. 2009. Cell probe lower bounds for succinct data structures. In Proc. 20th ACM-SIAM Symposium on

Discrete Algorithms. 625–634.

John Iacono. 2004. Expected asymptotically optimal planar point location. Computational Geometry: Theory and Applications

29 (2004), 19–22.

Neil D. Jones. 1997. Computability and Complexity: From a Programming Perspective. MIT Press.
Jeﬀ Kahn and Jeong Han Kim. 1995. Entropy and sorting. J. Comput. System Sci. 51 (1995), 390–399.
Claire Kenyon. 1996. Best-ﬁt bin-packing with random order. In Proc. 7th ACM-SIAM Symposium on Discrete Algorithms.

359–364.

David G. Kirkpatrick. 1983. Optimal search in planar subdivisions. SIAM J. Comput. 12 (1983), 28–35.
David G. Kirkpatrick and Raimund Seidel. 1985. Output-size sensitive algorithms for ﬁnding maximal vectors. In Proc. 1st

ACM Symposium on Computational Geometry. 89–96.

David G. Kirkpatrick and Raimund Seidel. 1986. The ultimate planar convex hull algorithm? SIAM J. Comput. 15 (1986),

287–299.

Jiˇr´ı Matouˇsek. 1992. Eﬃcient partition trees. Discrete and Computational Geometry 8 (1992), 315–334.

Jiˇr´ı Matouˇsek. 2000. Derandomization in computational geometry. In Handbook of Computational Geometry, J¨org-R¨udiger Sack

and Jorge Urrutia (Eds.). Elsevier Science Publishers B.V. North-Holland, Amsterdam, 559–595.

Jiˇr´ı Matouˇsek, Janos Pach, Micha Sharir, Shmuel Sifrony, and Emo Welzl. 1994. Fat triangles determine linearly many holes.

SIAM J. Comput. 23 (1994), 154–169.

Shlomo Moran, Marc Snir, and Udi Manber. 1985. Applications of Ramsey’s theorem to decision tree complexity. J. ACM 32

(1985), 938–949.

J. Ian Munro and Philip M. Spira. 1976. Sorting and Searching in Multisets. SIAM J. Comput. 5 (1976), 1–8.
Franco P. Preparata and Michael I. Shamos. 1985. Computational Geometry: An Introduction. Springer-Verlag.
Sandeep Sen and Neelima Gupta. 1999. Distribution-sensitive algorithms. Nordic Journal on Computing 6 (1999), 194–211.
J. Snoeyink. 1997. Point location. In Handbook of Discrete and Computational Geometry, Jacob E. Goodman and Joseph

O’Rourke (Eds.). CRC Press LLC, Boca Raton, FL, Chapter 30, 559–574.

Rephael Wenger. 1997. Randomized quickhull. Algorithmica 17 (1997), 322–329.
Andrew Chi-Chih Yao. 1981. A lower bound to ﬁnding convex hulls. J. ACM 28 (1981), 780–787.
Andrew Chi-Chih Yao. 1991. Lower bounds for algebraic computation trees with integer inputs. SIAM J. Comput. 20 (1991),

655–668.

F. Frances Yao, David P. Dobkin, Herbert Edelsbrunner, and Michael S. Paterson. 1989. Partitioning space for range queries.

SIAM J. Comput. 18 (1989), 371–384.

