Synergistic Solutions on MultiSets∗
Jérémy Barbay1, Carlos Ochoa2, and Srinivasa Rao Satti3

1 Departamento de Ciencias de la Computación, Universidad de Chile, Santiago,

2 Departamento de Ciencias de la Computación, Universidad de Chile, Santiago,

Chile
jeremy@barbay.cl

Chile
cochoa@dcc.uchile.cl

Seoul, South Korea
ssrao@cse.snu.ac.kr

3 Department of Computer Science and Engineering, Seoul National University,

Abstract

Karp et al. (1988) described Deferred Data Structures for Multisets as “lazy” data structures
which partially sort data to support online rank and select queries, with the minimum amount of
work in the worst case over instances of size n and number of queries q ﬁxed. Barbay et al. (2016)
reﬁned this approach to take advantage of the gaps between the positions hit by the queries (i.e.,
the structure in the queries). We develop new techniques in order to further reﬁne this approach
and take advantage all at once of the structure (i.e., the multiplicities of the elements), some
notions of local order (i.e., the number and sizes of runs) and global order (i.e., the number and
positions of existing pivots) in the input; and of the structure and order in the sequence of queries.
Our main result is a synergistic deferred data structure which outperforms all solutions in the
comparison model that take advantage of only a subset of these features. As intermediate results,
we describe two new synergistic sorting algorithms, which take advantage of some notions of
structure and order (local and global) in the input, improving upon previous results which take
advantage only of the structure (Munro and Spira 1979) or of the local order (Takaoka 1997) in
the input; and one new multiselection algorithm which takes advantage of not only the order and
structure in the input, but also of the structure in the queries.

1998 ACM Subject Classiﬁcation E.1 Data Structures, F.2.2 Nonnumerical Algorithms and
Problems, H.3.3 Information Search and Retrieval

Keywords and phrases deferred data structure, multivariate analysis, quick sort, select

Digital Object Identiﬁer 10.4230/LIPIcs.CPM.2017.31

Introduction

1
Consider a multiset M of size n. The multiplicity of an element x of M is the number mx of
occurrences of x in M. We call the distribution of the multiplicities of the elements in M the
input structure. As early as 1976, Munro and Spira [19] described a variant of the algorithm
MergeSort using counters, which optimally takes advantage of the input structure when
sorting a multiset M of n elements. Munro and Spira measure the “diﬃculty” of the instance
n log n
,

in terms of the “input structure” by the entropy function H(m1, . . . , mσ) =Pσ

mi

i=1

mi

∗ C. Ochoa is supported by CONICYT-PCHA/Doctorado Nacional/2013-63130161 (Chile). J. Barbay
is supported by the project Fondecyt Regular no 1170366 from Conicyt and the Millennium Nucleus
RC130003 “Information and Coordination in Networks” with code P10-024-F.

© Jérémy Barbay, Carlos Ochoa, and Srinivasa Rao Satti;
licensed under Creative Commons License CC-BY

28th Annual Symposium on Combinatorial Pattern Matching (CPM 2017).
Editors: Juha Kärkkäinen, Jakub Radoszewski, and Wojciech Rytter; Article No. 31; pp. 31:1–31:14

Leibniz International Proceedings in Informatics
Schloss Dagstuhl – Leibniz-Zentrum für Informatik, Dagstuhl Publishing, Germany

31:2

Synergistic Solutions on MultiSets

the σ distinct elements in M (such thatPσ

and r1, . . . , rρ are the sizes of the ρ runs in A (such thatPρ

where σ is the number of distinct elements in M and m1, . . . , mσ are the multiplicities of
i=1 mi = n), respectively. The time complexity of
the algorithm is within O(n(1 + H(m1, . . . , mσ))) ⊆ O(n(1+ log σ)) ⊆ O(n log n).
Any array A representing a multiset lists its element in some order, which we call the
input order and denote by a tuple. Maximal sorted subblocks in A are a local form of input
order and are called runs [16]. As early as 1973, Knuth [16] described a variant of the
algorithm MergeSort using a prepossessing step taking linear time to detect runs in the
array A. Takaoka [20] described a new sorting algorithm that optimally takes advantage of
the distribution of the sizes of the runs in the array A, which yields a time complexity within
O(n(1 + H(r1, . . . , rρ))) ⊆ O(n(1+ log ρ)) ⊆ O(n log n), where ρ is the number of runs in A
Given an element x of a multiset M and an integer j ∈ [1..n], the rank rank(x) of x is the
number of elements smaller than x in M, and selecting the j-th element in M corresponds
to computing the value select(j) of the j-th smallest element (counted with multiplicity) in
M. Those operations are central to the navigation of the Burrows-Wheeler transform [17] of
a text when searching for occurrences of a pattern in it. As early as 1961, Hoare [12] showed
how to support rank and select queries in average linear time, a result later improved
to worst case linear time by Blum et al. [7]. Twenty years later, Dobkin and Munro [10]
described a MultiSelection algorithm that supports several select queries and whose
running time is optimal in the worst case over all multisets of size n and all sets of q queries
hitting positions in the multisets separated by gaps (diﬀerences between consecutive select
queries in sorted order) of sizes g0, . . . , gq. Karp et al. [15] further extended Dobkin and
Munro’s result [10] to the online context, where the multiple rank and select queries arrive
one by one. They called their solution a Deferred Data Structure and describe it as
“lazy”, as it partially sorts data, performing the minimum amount of work necessary in the
worst case over all instances for a ﬁxed n and q. Barbay et al. [2] reﬁned this result by taking
advantage of the gaps between the positions hit by the queries (i.e., the query structure).

i=1 ri = n), respectively.

This suggests the following questions:

1. Is there a sorting algorithm for multisets which takes the best advantage of both its
input order and its input structure in a synergistic way, so that it performs as good as
previously known solutions on all instances, and much better on instances where it can
take advantage of both at the same time?

2. Is there a multiselection algorithm and/or a deferred data structure for answering rank
and select queries which takes the best advantage not only of both of those notions of
easiness in the input, but also of notions of easiness in the queries, such as the query
structure and the query order?
We answer both questions aﬃrmatively: In the context of Sorting, this improves
upon both algorithms from Munro and Spira [19] and Takaoka [20].
In the context of
MultiSelection and Deferred Data Structure for rank and select on Multisets,
this improves upon Barbay et al.’s results [2] by adding three new measures of diﬃculty
(input order, input structure and query order) to the single one previously considered (query
structure). Additionally, we correct the analysis of the Sorted Set Union algorithm by
Demaine et al. [9] (Section 2.2), and we deﬁne a simple yet new notion of “global” input
order (Section 2.4), not mentioned in previous surveys [11, 18] nor extensions [3].

We present our results incrementally, each building on the previous one, such that the
most complete and complex result is in Section 4. In Section 2 we describe how
to measure the interaction of the order (local and global) with the structure in the input,
and two new synergistic Sorting algorithms based on distinct paradigms (i.e., merging vs

J. Barbay, C. Ochoa, and S. R. Satti

31:3

splitting) which take advantage of both the input order and structure. We reﬁne the second
of those results in Section 3 with the analysis of a MultiSelection algorithm which takes
advantage of not only the order and structure in the input, but also of the query structure, in
the oﬄine setting. In Section 4 we analyze an online Deferred Data Structure taking
advantage of the order and structure in the input on one hand, and of the order and structure
in the queries on the other hand. We conclude with a discussion of our results in Section 5.

2

Sorting Algorithms

We review in Section 2.1 the algorithms MergeSort with Counters described by Munro
and Spira [19] and Minimal MergeSort described by Takaoka [20]: each takes advantage of
distinct features in the input. In Sections 2.2 and 2.3, we describe two synergistic Sorting
algorithms, which outperform both MergeSort with Counters and Minimal MergeSort by
taking advantage of both the order (local and global) and the structure in the input, in a
synergistic way.

2.1 Known Algorithms

The algorithm MergeSort with Counters described by Munro and Spira [19] is an adaptation 
of the traditional sorting algorithm MergeSort that optimally takes advantage of the
input structure when sorting a multiset M of size n. The algorithm divides M into two
parts of equal size, sorts both parts recursively, and then merges the two sorted lists. When
two elements of same value v are found, one is discarded and a counter holding the number
of occurrences of v is updated. Munro and Spira measure the “diﬃculty” of the instance
,
n log n
where σ is the number of distinct elements in M and m1, . . . , mσ are the multiplicities of
i=1 mi = n), respectively. The time complexity of
the algorithm is then within O(n(1 + H(m1, . . . , mσ))) ⊆ O(n(1+ log σ)) ⊆ O(n log n).

in terms of the input structure by the entropy function H(m1, . . . , mσ) =Pσ
the σ distinct elements in M (such thatPσ

mi

mi

i=1

The algorithm Minimal MergeSort described by Takaoka [20] optimally takes advantage
of the local input order, as measured by the decomposition into runs when sorting an array
A of size n. The main idea is to detect the runs ﬁrst and then merge them pairwise. The
runs are detected in linear time. Merging the two shortest runs at each step further reduces
the number of comparisons, making the running time of the merging process adaptive to the
entropy of the sequence formed by the sizes of the runs. If the array A is formed by ρ runs
i=1 ri = n), then the algorithm sorts
A in time within O(n(1 + H(r1, . . . , rρ))) ⊆ O(n(1+ log ρ)) ⊆ O(n log n).

and r1, . . . , rρ are the sizes of the ρ runs (such thatPρ

The algorithms MergeSort with Counters and Minimal MergeSort are incomparable,
in the sense that neither one performs always better than the other. Simple modiﬁcations
and combinations of these algorithms do not take full advantage of both the local input order
and the input structure (see the extended version [5] for detailed counter examples).

In the following sections we describe two sorting algorithms that take the best advantage
of both the order (local and global) and structure in the input all at once when sorting a
multiset. The ﬁrst one is a straightforward application of previous results, while the second
one prepares the ground for the MultiSelection algorithm (Section 3) and the Deferred
Data Structures (Section 4), which take advantage of the order (local and global) and
structure in the input and of the order and structure in the queries.

CPM 2017

31:4

Synergistic Solutions on MultiSets

Figure 1 An instance of the Sorted Set Union problem with ρ = 3 sorted sets. In each sorted
set A, the entry A[i] is represented by a point of x-coordinate A[i]. The sizes (gi)i∈[1..8] of the blocks
that form the sets are indicated. The sizes g4, g5 and g6 are 1 because they correspond to elements
of equal value and they determine the 4-th member of the partition π with value m4 equals 3. The
vertical bars separate the members of π.

2.2 “Kind-of-new” Sorting Algorithm DLM Sort

In 2000, Demaine et al. [9] described the algorithm DLM Union, an instance optimal algorithm
that computes the union of ρ sorted sets. The algorithm scans the sets from left to right
identifying blocks of consecutive elements in the sets that are also consecutive in the sorted
union (see Figure 1 for a graphical representation of such a decomposition on a particular
instance of the Sorted Set Union problem). In a minor way we reﬁne their analysis as
follows:

These blocks determine a partition π of the output into intervals such that any singleton
corresponds to a value that has multiplicity greater than 1 in the input, and each other
interval corresponds to a block as deﬁned above. Each member i of π has a value mi
associated with it: if the member i of π is a block, then mi is 1, otherwise, if the member i
of π is a singleton corresponding to a value of multiplicity q, then mi is q. If the instance is
formed by δ blocks of sizes g1, . . . , gδ such that these blocks determine a partition π of size χ
whose members have values m1, . . . , mχ, we express the time complexity of DLM Union as

(cid:1)). This time complexity is within a constant factor of

within Θ(Pδ

i=1 log gi +Pχ

the complexity of any other algorithm computing the union of these sorted sets (i.e., the
algorithm is instance optimal).

i=1 log(cid:0) ρ

mi

We adapt the DLM Union algorithm for sorting a multiset. The algorithm DLM Sort
detects the runs ﬁrst through a linear scan and then applies the algorithm DLM Union. After
that, transforming the output of the union algorithm to yield the sorted multiset takes only
linear time. The following corollary follows from our reﬁned analysis above:

(cid:73) Corollary 1. Given a multiset M of size n formed by ρ runs and δ blocks of sizes
g1, . . . , gδ such that these blocks determine a partition π of size χ of the output whose
i=1 log gi+

members have values m1, . . . , mχ, the algorithm DLM Sort performs within n+O(Pδ
i=1 log(cid:0) ρ
Pχ

(cid:1)) data comparisons. This number of comparisons is optimal in the worst case

over multisets of size n formed by ρ runs and δ blocks of sizes g1, . . . , gδ such that these blocks
determine a partition π of size χ of the output whose members have values m1, . . . , mχ.

mi

While the algorithm DLM Sort answers the Question 1 from Section 1, it does not yield
a MultiSelection algorithm nor a Deferred Data Structure answering Question
2. In the following section we describe another sorting algorithm that also optimally takes
advantage of the local order and structure in the input, but which is based on a distinct
paradigm, more suitable to such extensions.

g1g3g2g4g5g6g7g8ρJ. Barbay, C. Ochoa, and S. R. Satti

31:5

1: Compute the ρ runs of respective sizes (ri)i∈[1..ρ] in M such thatPρ

Algorithm 1 Quick Synergy Sort
Input: A multiset M of size n
Output: A sorted sequence of M
2: Compute the median µ of the middles of the runs, note j ∈ [1..ρ] the run containing µ;
3: Perform doubling searches for the value µ in all runs except the j-th, starting at both

i=1 ri = n;

4: Find the maximum max‘ (minimum minr) among the elements smaller (resp., greater)

ends of the runs in parallel;

than µ in all runs except the j-th;

5: Perform doubling searches for the values max‘ and minr in the j-th run, starting at the

6: Recurse on the elements smaller than or equal to max‘ and on the elements greater than

position of µ;

or equal to minr.

2.3 New Sorting Algorithm Quick Synergy Sort
Given a multiset M, the algorithm Quick Synergy Sort identiﬁes the runs in linear time
through a scanning process. It computes a pivot µ, which is the median of the set formed by
the middle elements of each run, and partitions each run by µ. This partitioning process
takes advantage of the fact that the elements in each run are already sorted. The insertion
ranks of the pivots in the runs are identiﬁed by doubling searches [6]. It then recurses on the
elements smaller than µ and on the elements greater than µ. (See Algorithm 1 for a more
formal description).
(cid:73) Deﬁnition 2 (Median of the middles). Given a multiset M formed by runs, the “median
of the middles” is the median element of the set formed by the middle elements of each run.
The number of data comparisons performed by the algorithm Quick Synergy Sort is
asymptotically the same as the number of data comparisons performed by the algorithm DLM
Sort described in the previous section. We divide the proof into two lemmas. We ﬁrst bound
the number of data comparisons performed by all the doubling searches of the algorithm
Quick Synergy Sort (i.e., steps 3 and 5 of the Algorithm 1).
(cid:73) Lemma 3. Let g1, . . . , gk be the sizes of the k blocks that form the r-th run. The overall
number of data comparisons performed by the doubling searches of the algorithm Quick
Synergy Sort to ﬁnd the values of the medians of the middles in the r-th run is within

O(Pk

i=1 log gi).

Proof. Every time the algorithm ﬁnds the insertion rank of one of the medians of the middles
in the r-th run, it partitions the run by a position separating two blocks. The doubling
search steps can be represented as a tree. Each node of the tree corresponds to a step. Each
internal node has two children, which correspond to the two subproblems into which the step
partitions the run. The cost of the step is less than four times the logarithm of the size of
the child subproblem with smaller size, because of the two doubling searches in parallel. The
leaves of the tree correspond to the blocks themselves.

We prove that at each step the total cost is bounded by eight times the sum of the
logarithms of the sizes of the leaf subproblems. This is done by induction over the number
of steps. If the number of steps is zero then there is no cost. For the inductive step, if the
number of steps increases by one, a new doubling search step is done and a leaf subproblem is
partitioned into two new subproblems. At this step, a leaf of the tree is transformed into an

CPM 2017

31:6

Synergistic Solutions on MultiSets

Figure 2 A multiset M formed by ρ runs. Each entry M[i] is represented by a point of xcoordinate 
M[i]. There is an element of multiplicity mv present in the last mv runs and the rest of
the runs are formed by only one block.

internal node and two new leaves are created. Let a and b such that a ≤ b be the sizes of the
new leaves created. The cost of this step is less than 4 log a. The cost of all the steps then
increases by 4 lg a, and hence the sum of the logarithms of the sizes of the leaves increases by
8(lg a + lg b) − 8 lg(a + b). But if a ≥ 4 and b ≥ a, then 2 lg(a + b) ≤ lg a + 2 lg b. The result
(cid:74)
follows.

mi

within O(Pχ
term log(cid:0) ρ

As shown in the following lemma, the overall number of data comparisons performed
during the computation of the medians of the middles (i.e., step 2 of the Algorithm 1) is
(see Section 2.2 for the deﬁnition of π) and ρ is the number of runs in M.

(cid:1)), where m1, . . . , mχ are the values of the member of the partition π
i=1 log(cid:0) ρ
(cid:1) comes. In this instance, there is a value v that has multiplicity mv > 1 in M

Consider the instance depicted in Figure 2 for an example illustrating from where the

and the rest of the values have multiplicity 1. The elements with value v are present at the
end of the last mv runs and the rest of the runs are formed by only one block. The elements
of the i-th run are greater than the elements of the (i + 1)-th run. During the computation of
the medians of the middles, the number of data comparisons that involve elements of value v

(cid:1)). The algorithm computes the median µ of the middles and partitions

is within O(log(cid:0) ρ

mv

mv

) = O(log(cid:0) ρ

(cid:1)), where log ρ

the runs by the value of µ. In the recursive call that involves elements of value v, the number
of runs is reduced by half. This is repeated until one occurrence of µ belongs to one of the
last mv runs. The number of data comparisons that involve elements of value v up to this
step is within O(mv log ρ
corresponds to the number of steps
mv
where µ does not belong to the last mv runs. The next recursive call will necessarily choose
one element of value v as the median of the middles.
(cid:73) Lemma 4. Let M be a multiset formed by ρ runs and δ blocks such that these blocks
determine a partition π of size χ of the output whose members have values m1, . . . , mχ.
Consider the steps that compute the medians of the middles and the steps that ﬁnd the
elements max‘ and minr in the algorithm Quick Synergy Sort, the overall number of data

comparisons performed during these steps is within O(Pχ

i=1 log(cid:0) ρ

(cid:1)).

mv

mv

mi

4 of the algorithm Quick Synergy Sort. We prove that T (π, ρ) ≤ Pχ

Proof. We prove this lemma by induction over the size χ of π and the number of runs
ρ. The number of data comparisons performed by one of these steps is linear in the
number of runs in the sub-instance (i.e., ignoring all the empty sets of this sub-instance).
Let T (π, ρ) be the overall number of data comparisons performed during the steps 2 and
− ρ.
Let µ be the ﬁrst median of the middles computed by the algorithm. Let ‘ and r be
the number of runs that are completely to the left and to the right of µ, respectively.
Let b be the number of runs that are split in the doubling searches for the value of µ

i=1 mi log ρ

mi

mvρJ. Barbay, C. Ochoa, and S. R. Satti

31:7

in all runs. Let π‘ and πr be the partitions determined by the blocks yielded to the
left and to the right of µ, respectively. Then, T (π, ρ) = T (π‘, ‘ + b) + T (πr, r + b) + ρ
because of the two recursive calls and the step that computes µ. By Induction Hypothesis,
− r − b. Hence,

T (π‘, ‘ + b) ≤Pχ‘
we need to prove that ‘ + r ≤Pχ‘
consequence ofPχ‘
i=1 mi ≥ ‘ + b,Pχr
the right of µ); and log(cid:0)1 + y

− ‘ − b and T (πr, r + b) ≤Pχr
i=1 mi log(cid:16)1 + r
(cid:17) +Pχr
(cid:1)x ≥ y for y ≤ x.

equal to the number of runs); ‘ ≤ r + b, r ≤ ‘ + b (at least ρ

i=1 mi ≥ r + b (the number of blocks is greater than or
2 runs are left to the left and to
(cid:74)

i=1 mi log(cid:16)1 + ‘

i=1 mi log r+b
r+b

(cid:17), but this is a

i=1 mi log ‘+b

‘+b

mi

x

mi

Consider the step that performs doubling searches for the values max‘ and minr in the
run that contains the median µ of the middles, this step results in the ﬁnding of the block g
that contains µ in at most 4 log |g| data comparisons, where |g| is the size of g. Combining
Lemma 3 and Lemma 4 yields an upper bound on the number of data comparisons performed
by the algorithm Quick Synergy Sort:
(cid:73) Theorem 5. Let M be a multiset of size n formed by ρ runs and δ blocks of sizes
g1, . . . , gδ such that these blocks determine a partition π of size χ of the output whose
members have values m1, . . . , mχ. The algorithm Quick Synergy Sort performs within

(cid:1)) data comparisons on M. This number of comparisons

i=1 log gi +Pχ

i=1 log(cid:0) ρ

n + O(Pδ

is optimal in the worst case over multisets of size n formed by ρ runs and δ blocks of sizes
g1, . . . , gδ such that these blocks determine a partition π of size χ of the output whose members
have values m1, . . . , mχ.

mi

We extend these results to take advantage of the global order of the multiset in a way

that can be combined with the notion of runs (local order).

2.4 Taking Advantage of Global Order
Given a multiset M, a pivot position is a position p in M such that all elements in previous
position are smaller than or equal to all elements at p or in the following positions. In 1962,
Iverson [13] described an improved version of BubbleSort [16] that identiﬁes such pivot
positions (as pair of consecutive elements that the algorithm have placed at their ﬁnal positions
and on which it does not make further comparisons). We show that detecting such positions
also yields an improved version of QuickSort in general, and of our QuickSort-inspired
solutions in particular. More formally:
(cid:73) Deﬁnition 6 (Pivot positions). Given a multiset M = (x1, . . . , xn) of size n, the “pivot
positions” are the positions p such that xa ≤ xb for all a, b such that a ∈ [1..p − 1] and
b ∈ [p..n].

Existing pivot positions in the input order of M divide the input into subsequences of
consecutive elements such that the range of positions of the elements at each subsequence
coincide with the range of positions of the same elements in the sorted sequence of M: the
more there are of such positions, the more “global” order there is in the input. Detecting
such positions takes only a linear number of comparisons by applying the ﬁrst phase of the
algorithm BubbleSort [16], which sequentially compares the elements, from left to right in a
ﬁrst phase and then from right to left in a second phase. The positions of the elements that
do not interchange their values during both executions are the pivot positions in M.

instances of sizes n0, . . . , nφ (such thatPφ

When there are φ such positions, they simply divide the input of size n into φ + 1 sub-
i=0 ni = n). Each sub-instance Ii for i ∈ [0..φ] then

CPM 2017

31:8

Synergistic Solutions on MultiSets

has its own number of runs ri and alphabet size σi, on which the synergistic solutions described
in this work can be applied, from mere Sorting (Section 2) to supporting MultiSelection
(Section 3) and the more sophisticated Deferred Data Structures (Section 4).
(cid:73) Corollary 7. Let M be a multiset of size n with φ pivot positions. Let n0, . . . , nφ be
integers such that the φ pivot positions divide M into φ + 1 sub-instances of sizes n0, . . . , nφ
i=0 ni = n). Let ρi and δi be such that each sub-instance Ii of size ni is formed
by ρi runs and δi blocks of sizes gi1, . . . , giδi such that these blocks determine a partition
πi of size χi of the output whose members have values mi1, . . . , miχi for i ∈ [0..φ]. There

(such thatPφ
exists an algorithm that performs within 3n+ O(Pφ
of sizes n0, . . . , nφ (such thatPφ

comparisons for sorting M. This number of comparisons is optimal in the worst case over
multisets of size n with φ pivot positions which divide the multiset into φ + 1 sub-instances
i=0 ni = n) and each sub-instance Ii of size ni is formed by
ρi runs and δi blocks of sizes gi1, . . . , giδi such that these blocks determine a partition πi of
size χi of the output whose members have values mi1, . . . , miχi for i ∈ [0..φ].

(cid:1)o) data

nPδi
j=1 log gij +Pχi

j=1 log(cid:0) ρi

mij

i=0

Next, we generalize the algorithm Quick Synergy Sort to an oﬄine multiselection
algorithm that partially sorts a multiset according to the set of select queries given as input.
This serves as a pedagogical introduction to the online Deferred Data Structures for
answering rank and select queries presented in Section 4.

3 MultiSelection Algorithm
Given a linearly ordered multiset M and a sequence of ranks r1, . . . , rq, a multiselection
algorithm must answer the queries select(r1), . . . , select(rq) in M, hence partially sorting
M. We describe a MultiSelection algorithm based on the sorting algorithm Quick
Synergy Sort introduced in Section 2.3. This algorithm is an intermediate result leading to
the Deferred Data Structure described in Section 4.
Given a multiset M and a set of q select queries, the algorithm Quick Synergy
MultiSelection follows the same ﬁrst steps as the algorithm Quick Synergy Sort. But
once it has computed the ranks of all elements in the block that contains the pivot µ, it
determines which select queries correspond to elements smaller than or equal to max‘ and
which ones correspond to elements greater than or equal to minr (see Algorithm 1 for the
deﬁnitions of max‘ and minr). It then recurses on both sides.

We extend the notion of blocks to the context of partial sorting. Next, we introduce the
deﬁnitions of pivot blocks and selection blocks (see Figure 3 for a graphical representation of
these deﬁnitions).
(cid:73) Deﬁnition 8 (Pivot Blocks). Given a multiset M formed by ρ runs and δ blocks. The
“pivot blocks” are the blocks of M that contain the pivots and the elements of value equals
to the pivots during the steps of the algorithm Quick Synergy MultiSelection.

In each run, between the pivot blocks and the insertion ranks of the pivots, there are
consecutive blocks that the algorithm Quick Synergy MultiSelection has not identiﬁed
as separated blocks, because no doubling searches occurred inside them.
(cid:73) Deﬁnition 9 (Selection Blocks). Given the i-th run, formed of various blocks, and q select
queries, the algorithm Quick Synergy MultiSelection computes ξ pivots in the process
of answering the q queries. During the doubling searches, the algorithm Quick Synergy
MultiSelection ﬁnds the insertion ranks of the ξ pivots inside the i-th run. These positions

J. Barbay, C. Ochoa, and S. R. Satti

31:9

Figure 3 An instance of the MultiSelection problem where the multiset M is formed by ρ = 5
runs. In each run R, the entry R[i] is represented by a point of x-coordinate R[i]. The dash lines
represent the answers of the two select queries. The solid vertical lines represent the positions
p1 and p2 of the ﬁrst two pivots computed by the Quick Synergy MultiSelection algorithm. The
pivot blocks corresponding to the pivots p1 and p2 are marked by contiguous open disks. The
algorithm divides the runs into selection blocks. s = 7 is the size of the second selection block, from
left to right, into which the third run is divided by the algorithm. m = 2 is the number of pivot
blocks of size 1 corresponding to the pivot p2.

determine a partition of size ξ + 1 of the i-th run where each element of the partition is
formed by consecutive blocks or is empty. We call the elements of this partition “selection
blocks”. The set of all selection blocks contains the set of all pivot blocks.

Using these deﬁnitions, we generalize the results proven in Section 2.3 to the more general

problem of MultiSelection.
(cid:73) Theorem 10. Given a multiset M of size n formed by ρ runs and δ blocks; and q oﬄine
select queries over M corresponding to elements of ranks r1, . . . , rq. Let ξ be the number of
pivots computed by the algorithm Quick Synergy MultiSelection in the process of answering 
the q queries. Let s1, . . . , sβ be the sizes of the β selection blocks determined by these ξ
pivots in all runs. Let m1, . . . , mλ be the numbers of pivot blocks corresponding to the values of
the λ pivots with multiplicity greater than 1, respectively. Let ρ0, . . . , ρξ be the sequence where
ρi is the number of runs that have elements with values between the pivots i and i+1 sorted by
ranks, for i ∈ [1..ξ]. The algorithm Quick Synergy MultiSelection answers the q select
queries performing within n + O

(cid:16)Pβ
i=1 log si + β log ρ −Pλ

i=1 mi log mi −Pξ

i=0 ρi log ρi

(cid:17) ⊆

O (n log n −Pq

i=0 ∆i log ∆i) data comparisons, where ∆i = ri+1 − ri, r0 = 0 and rq+1 = n.
Proof. The pivots computed by the algorithm Quick Synergy MultiSelection for answering 
the queries are a subset of the pivots computed by the algorithm Quick Synergy Sort
for sorting the whole multiset. Suppose that the selection blocks determined by every two
consecutive pivots form a multiset Mj such that for every pair of selection blocks in Mj
the elements of one are smaller than the elements of the other one. The algorithm Quick
Synergy Sort would perform within n + O
comparisons in this supposed instance (see the proof of Lemmas 3 and 4 analyzing the
algorithm Quick Synergy Sort for details). The number of comparisons needed to sort the
(cid:74)

(cid:16)Pβ
i=1 log si + β log ρ −Pλ

multisets Mj is within Θ(Pξ

i=0 ρi log ρi). The result follows.

(cid:17) data

i=1 mi log mi

The process of detecting the φ pre-existing pivot positions seen in Section 2.4 can be
applied as the ﬁrst step of the multiselection algorithm. The φ pivot positions divide the
input of size n into φ + 1 sub-instances of sizes n0, . . . , nφ. For each sub-instance Ii for
i ∈ [0..φ], the multiselection algorithm determines which select queries correspond to Ii

CPM 2017

p1p2sρm31:10

Synergistic Solutions on MultiSets

and applies then the steps of the algorithm Quick Synergy MultiSelection inside Ii in
order to answer these queries.

The Quick Synergy MultiSelection algorithm takes advantage of the number and sizes
of the runs (i.e., the local input order), the number and positions of the pre-existing pivot
positions (i.e., the global order), the multiplicities of the elements in the multiset (i.e., the
input structure) and the diﬀerences between consecutive select queries in sorted order (i.e.,
the query structure).

In the result above, the queries are given all at the same time (i.e., oﬄine). In the context
where they arrive one at the time (i.e., online), we deﬁne a Deferred Data Structure
for answering online rank and select queries, inspired by the algorithm Quick Synergy
MultiSelection.

Rank and Select Deferred Data Structures

4
We describe the Full-Synergistic Deferred Data Structure that answers a set of
rank and select queries, arriving one at the time, over a multiset M, progressively sorting
M. This deferred data structure is based in the Quick Synergy MultiSelection algorithm
described in the previous section. This data structure takes advantage of the order (local
and global) and structure in the input, and of the order and structure in the queries.

By “query order”, we mean to consider the “distances” between consecutive queries. To
take advantage of the query order, we introduce a data structure that ﬁnds the nearest
pivots to the left and to the right of a position p ∈ [1..n], while taking advantage of the
distance between the position of the last computed pivot and p, as measured by the number
of computed pivot blocks between the two positions. For that we use a ﬁnger search tree [8]
maintaining ﬁngers (i.e., pointers) to elements in the search tree and supporting eﬃcient
updates and searches in the vicinity of those. Brodal [8] described an implementation of
ﬁnger search trees that searches for an element x, starting the search at the element given by
the ﬁnger f in time within O(log d), where d is the distance between x and f in the set (i.e,
the diﬀerence between rank(x) and rank(f) in the set). This operation returns a ﬁnger to x
if x is contained in the set, otherwise a ﬁnger to the largest element smaller than x in the
set. This implementation supports the insertion of an element x immediately to the left or
to the right of a ﬁnger in worst-case constant time.

Given a multiset M of size n, the Full-Synergistic Deferred Data Structure
includes a ﬁnger search tree Fselect, in which it marks the elements in M that have been
computed as pivots when it answers the online queries. For each pivot p in Fselect, the data
structure stores pointers to the insertion ranks of p in each run, to the beginning and to the
end of the block g to which p belongs, and to the position of p inside g. This ﬁnger search
tree is also used to ﬁnd the two successive pivots between which the query ﬁts.

Once a pivot block g is computed, every element in g is a valid pivot for the rest of the
elements in M. In order to capture this idea, we modify the ﬁnger search tree Fselect so
that it contains the pivot blocks (i.e., a sequence of consecutive values) instead of singleton
pivots. This modiﬁcation allows the Full-Synergistic Deferred Data Structure to
answer select queries, taking advantage of the structure and order in the queries and of the
structure and order in the input. But in order to answer rank queries taking advantage of
the features in the queries and the input, the data structure needs another ﬁnger search tree
Frank. In Frank the data structure stores, for each block g identiﬁed, the value of one of the
elements in g, and pointers in M to the beginning and to the end of g, and in each run to
the position where the elements of g partition the run.

J. Barbay, C. Ochoa, and S. R. Satti

31:11

Figure 4 The state of the Full-Synergistic Deferred Data Structure on an instance where
the multiset M is formed by ρ = 5 runs. In each run, the entry M[i] is represented by a point
of x-coordinate M[i]. The dash lines represent the positions q1 and q2 of the answers of the ﬁrst
two queries. The solid vertical lines represent the positions p1, p2 and p3 of the ﬁrst three pivots
computed by the Full-Synergistic Deferred Data Structure. The pivot blocks corresponding
to the pivots p1, p2 and p3 are marked by contiguous open disks. d = 4 is the distance (i.e., the
number of computed pivot blocks) between the queries q1 and q2. If q1 is a rank query, then g = 4
is the size of the identiﬁed block that contains the answer of the query q1.

(cid:73) Theorem 11. Consider a multiset M of size n formed by ρ runs and δ blocks. Let γ and
r1, . . . , rq be the number of pivot blocks computed by the Full-Synergistic Deferred
Data Structure in the process of answering q online rank and select queries over M,
and the ranks of the elements corresponding to these queries, respectively. Let s1, . . . , sβ
be the sizes of the β selection blocks determined by the pivots in the γ blocks in all runs.
Let m1, . . . , mλ be the numbers of pivot blocks corresponding to the values of the λ pivots
with multiplicity greater than 1, respectively. Let ρ0, . . . , ργ be the sequence where ρi is
the number of runs that have elements with values between the elements in the blocks i
and i + 1 sorted by ranks, for i ∈ [1..γ]. Let d1, . . . , dq−1 be the sequence where dj is
the number of computed pivot blocks between the block that answers the (j − 1)-th query
and the one that answers the j-th query before starting the steps to answer the j-th query,
for j ∈ [2..q]. Let u and g1, . . . , gu be the number of rank queries and the sizes of the
computed and searched pivot blocks in the process of answering the u rank queries, respectively.
The Full-Synergistic Deferred Data Structure answers the q online queries by
i=1 log di+
i=0 ∆i log ∆i + q log n) data comparisons, where r0 = 0, rq+1 =

performing within n+O(Pβ
i=1 log gi) ⊆ O (n log n −Pq
Pu

i=0 ρi log ρi+Pq−1

i=1 log si+β log ρ−Pλ

i=1 mi log mi−Pγ

n, and ∆i = ri+1 − ri, for all i ∈ [1..n].
Proof. The algorithm answers a new select(i) query by searching in Fselect for the nearest
pivots to the left and right of the query position i. If i is contained in an element of Fselect,
then the block g that contains the element in the position i has already been computed. If i
is not contained in an element of Fselect, then the returned ﬁnger f points the nearest block
b to the left of i. The block that follows f in Fselect is the nearest block to the right of i.
It then applies the same steps as the algorithm Quick Synergy MultiSelection in order
to answer the query. Given f, the algorithm inserts in Fselect each pivot block computed
in the process of answering the query in constant time, and stores the respective pointers
to positions in M. In Frank the algorithm searches for the value of one of the elements in
b. Once the algorithm obtains the ﬁnger returned by this search, the algorithm inserts in
Frank the value of one of the elements of each pivot block in constant time, and stores the
respective pointers to positions in M (see Figure 4 for a graphical representation of some of
the parameters used in the analysis).

CPM 2017

p1p2ρp3q1q2dg31:12

Synergistic Solutions on MultiSets

The algorithm answers a new rank(x) query by ﬁnding the selection block sj in the
j-th run such that x is between the smallest and the greatest value of sj for all j ∈ [1..ρ].
For that the algorithm searches for the value x in Frank. The number of data comparisons
performed by this searching process is within O(log d), where d is the number of blocks in
Frank between the last inserted or searched block and the returned ﬁnger f. Given the ﬁnger
f, there are three possibilities for the rank r of x: (i) r is between the ranks of the elements
at the beginning and at the end of the block pointed by f, (ii) r is between the ranks of the
elements at the beginning and at the end of the block pointed by the ﬁnger following f, or
(iii) r is between the ranks of the elements in the selection blocks determined by f and the
ﬁnger following f. In the cases (i) and (ii), a binary search inside the block yields the answer
of the query. In the case (iii), the algorithm applies the same steps as the algorithm Quick
Synergy MultiSelection in order to compute the median µ of the middles, and partitions
the selection blocks by µ. The algorithm then decides to which side x belongs. Similar to
the algorithm for answering a select query, the data structure inserts in Fselect every block
(cid:74)
computed in the process of answering the rank query.

The process of detecting the φ pivot positions seen in Section 2.4 allows the FullSynergistic 
Deferred Data Structure to insert these pivots in Fselect and Frank. For
each pivot position p in Fselect and Frank, the data structure stores pointers to the end of
the runs detected on the left of p; to the beginning of the runs detected on the right of p;
and to the position of p in the multiset. This concludes the description of our synergistic
results. In the next section, we discuss how these results relate to various past results and
future work.

5 Discussion

Kaligosi et al.’s multiselection algorithm [14] and Barbay et al’s deferred data structure [2]
use the very same concept of runs as the one described in this work. The diﬀerence is, we
describe algorithms that detect the existing runs in the input in order to take advantage of
them, while the algorithms described by those previous works do not take into consideration
any pre-existing runs in the input, and rather build and maintain such runs as a strategy
to minimize the number of comparisons performed while partially sorting the multiset. We
leave the combination of both approaches as a topic for future work, which could probably
shave a constant factor oﬀ the number of comparisons performed by the Sorting and
MultiSelection algorithms and by the Deferred Data Structures supporting rank
and select queries on Multisets.

Barbay and Navarro [3] described how any Sorting algorithm taking advantage of
speciﬁcities in the input, directly implies a compressed encoding for permutations. By using
the similarity of the execution tree of the algorithm MergeSort with the Wavelet Tree data
structure, they described a compressed data structure for permutations taking advantage
of the local order, i.e., using space proportional to H(r1, . . . , rρ) and supporting direct
access (i.e. π()) and inverse access (i.e. π−1()) in worst time within O(1 + lg ρ) and
average time within O(1 + H(r1, . . . , rρ)). We leave as future work the extension of our work
into a compressed data structure for multisets taking advantage of both its structure and
(local and global) order.

Another perspective is to generalize the synergistic results to related problems in computational 
geometry: Karp et al. [15] deﬁned the ﬁrst deferred data structure not only to support
rank and select queries on multisets, but also to support online queries in a deferred way
on Point Membership in a Convex Hull in two dimensions and online Dominance

J. Barbay, C. Ochoa, and S. R. Satti

31:13

queries on sets of multi-dimensional vectors. Preliminary results [4] show that one can reﬁne
the results from Karp et al. [15] to take advantage of the blocks between queries (i.e., the
structure in the queries) as Barbay et al. [2] did for multisets; but also of the relative position
of the points (i.e., the structure in the input) as Afshani et al. [1] did for Convex Hulls
and Maxima; of the order in the points (i.e., the order in the input), as computing the
convex hull in two dimensions takes linear time if the points are sorted; and potentially of
the order in the queries.

1

2

3

4

5

6

References
Peyman Afshani, Jérémy Barbay, and Timothy M. Chan. Instance-optimal geometric algorithms.
 J. ACM, 64(1):3:1–3:38, March 2017. doi:10.1145/3046673.
Jérémy Barbay, Ankur Gupta, Srinivasa Rao Satti, and Jonathan Sorenson. Near-optimal
online multiselection in internal and external memory. J. Discrete Algorithms, 36:3–17,
2016. doi:10.1016/j.jda.2015.11.001.
Jérémy Barbay and Gonzalo Navarro. On compressing permutations and adaptive sorting.
Theor. Comput. Sci., 513:109–123, 2013. doi:10.1016/j.tcs.2013.10.019.
Jérémy Barbay and Carlos Ochoa. Synergistic computation of planar maxima and convex
hull, 2017. arXiv:1702.08545.
Jérémy Barbay, Carlos Ochoa, and Srinivasa Rao Satti. Synergistic sorting and deferred
data structures on multisets, August 2016. arXiv:1608.06666.
Jon Louis Bentley and Andrew Chi-Chih Yao. An almost optimal algorithm for unbounded
searching. Inf. Process. Lett., 5(3):82–87, 1976. doi:10.1016/0020-0190(76)90071-5.

7 Manuel Blum, Robert W. Floyd, Vaughan R. Pratt, Ronald L. Rivest, and Robert E.
Tarjan. Time bounds for selection. J. Comput. Syst. Sci., 7(4):448–461, 1973. doi:10.
1016/S0022-0000(73)80033-9.

8 Gerth S. Brodal. Finger search trees with constant insertion time. In Howard J. Karloﬀ, editor,
 Proceedings of the 9th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA
1998), pages 540–549. ACM/SIAM, 1998. URL: http://dl.acm.org/citation.cfm?id=
314613.314842.
Erik D. Demaine, Alejandro López-Ortiz, and J. Ian Munro. Adaptive set intersections,
unions, and diﬀerences. In David B. Shmoys, editor, Proceedings of the 11th Annual ACMSIAM 
Symposium on Discrete Algorithms (SODA 2000), pages 743–752. ACM/SIAM, 2000.
URL: http://dl.acm.org/citation.cfm?id=338219.338634.

9

10 David P. Dobkin and J. Ian Munro. Optimal time minimal space selection algorithms. J.

ACM, 28(3):454–461, 1981. doi:10.1145/322261.322264.

11 Vladimir Estivill-Castro and Derick Wood. A survey of adaptive sorting algorithms. ACM

Comput. Surv., 24(4):441–476, 1992. doi:10.1145/146370.146381.

12 Charles A. R. Hoare. Algorithm 65: Find. Commun. ACM, 4(7):321–322, 1961. doi:

10.1145/366622.366647.

13 Kenneth E. Iverson. A Programming Language. John Wiley & Sons, Inc., New York,
NY, USA, 1962. URL: http://www.softwarepreservation.org/projects/apl/Books/
APROGRAMMING%20LANGUAGE.

14 Kanela Kaligosi, Kurt Mehlhorn, J. Ian Munro, and Peter Sanders. Towards optimal
multiple selection. In Luís Caires, Giuseppe F. Italiano, Luís Monteiro, Catuscia Palamidessi,
 and Moti Yung, editors, Proceedings of the 32nd International Colloquium on Automata,
 Languages, and Programming (ICALP 2005), volume 3580 of LNCS, pages 103–114.
Springer, 2005. doi:10.1007/11523468_9.

15 Richard Karp, Rajeev Motwani, and Prabhakar Raghavan. Deferred data structuring.

SIAM J. Comput., 17(5):883–902, 1988. doi:10.1137/0217055.

CPM 2017

31:14

Synergistic Solutions on MultiSets

Edition). Addison-Wesley Professional, April 1998.

17 Veli Mäkinen and Gonzalo Navarro. Rank and select revisited and extended. Theor. Comput.
 Sci., 387(3):332–347, 2007. doi:10.1016/j.tcs.2007.07.013.

16 Donald E. Knuth. Art of Computer Programming, Volume 3: Sorting and Searching (2nd

19

18 Alistair Moﬀat and Ola Petersson. An overview of adaptive sorting. Aust. Comput. J.,
24(2):70–77, 1992. URL: http://50years.acs.org.au/__data/assets/pdf_file/0017/
111464/ACJ-V24-N02-199205.pdf.
J. Ian Munro and Philip M. Spira. Sorting and searching in multisets. SIAM J. Comput.,
5(1):1–8, 1976. doi:10.1137/0205001.

20 Tadao Takaoka. Partial solution and entropy. In Rastislav Královic and Damian Niwiński,
editors, Proceedings of the 34th International Symposium on Mathematical Foundations of
Computer Science (MFCS 2009), volume 5734 of LNCS, pages 700–711. Springer, 2009.
doi:10.1007/978-3-642-03816-7_59.

