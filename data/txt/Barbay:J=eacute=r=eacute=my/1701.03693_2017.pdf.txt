7
1
0
2

 

n
a
J
 

3
1

 
 
]

G
C
.
s
c
[
 
 

1
v
3
9
6
3
0

.

1
0
7
1
:
v
i
X
r
a

Multivariate analysis for

Maxima in High Dimensions

J´er´emy Barbay and Javiel Rojas

Departmento de Ciencias de la Computaci´on, Universidad de Chile, Chile

{jeremy, jrojas}@dcc.uchile.cl

Abstract We study the problem of computing the Maxima of a set of
n d-dimensional points. For dimensions 2 and 3, there are algorithms to
solve the problem with order-oblivious instance-optimal running time.
However, in higher dimensions there is still room for improvements. We
present an algorithm sensitive to the structural entropy of the input set,
which improves the running time, for large classes of instances, on the
best solution for Maxima to date for d ≥ 4.

Introduction

1
The problem of computing the Maxima of a set S of points was ﬁrst formulated
in 1974 by Kung et al. [9]: a point from S is called maximal
if none of the
remaining vectors in S dominates it in every component, and the Maxima of S
(denoted by M (S)) is the set of maximal points in S. For any constant dimension
d this problem can be solved naively in time within O(n2) by comparing every
possible pair of points. Kung et al. [9] proposed two diﬀerent algorithms to solve
the problem in two and three dimensions, respectively, running in time within
O(n log n). For higher dimensions, Kung et al. [9] presented a divide-and-conquer
algorithm running in time within O(n logd−2 n) for dimensions d > 3, and showed
a lower bound of within Ω(n log n) for this problem for any dimension d > 2.
In 1985, Kirkpatrick and Seidel [8] gave the ﬁrst output-size sensitive algorithm 
for this problem, running in time within O(n log h) in the plane, and
within O(n logd−2 h) for dimension d > 3, where h is the size of the Maxima.
In 2 and 3 dimensions, this result remained unbeaten for almost 25 years until
Afshani et al. [2] described an instance-optimal algorithm for this problem (i.e.,
an algorithm whose cost is at most a constant factor from the cost of any other
algorithm running on the same input, for every input instance).

Given this improvement in 2 and 3 dimensions, one natural question is
whether the same technique can be applied to higher dimensions in order to
improve upon Kirkpatrick and Seidel’s results [8] in dimension d ≥ 4. We show
that the upper bound can indeed be applied to higher dimensions, even though
the generalization of the order-oblivious instance-optimality result is still open
due to the lack of advanced lower bound techniques in high dimensions. We introduce 
some basic deﬁnitions in Section 2; and describe our generalization of
Afshani et al.’s upper bound [2] to high dimension in Section 3, before discussing
the potential for instance-optimality results in high dimension in Section 4.

2

2 Preliminaries

An algorithm is said to be instance-optimal
if its cost is at most a constant
factor from the cost of any other algorithm on the same input, for every input
instance. As such algorithm does not always exist, it is often useful to consider
a relaxation of this concept, order-oblivious instance-optimality: an algorithm is
said to be order-oblivious instance-optimal if it is instance-optimal on the worst
input order possible.

Afshani et al. [2] improved on the analysis of Kirkpatrick and Seidel [8] for the
computation of the Maxima in dimensions 2 and 3 reﬁning it to order-oblivious
instance-optimal algorithms. In 2 dimensions, they showed that a simple variant
of the output sensitive algorithm originally described by Kirkpatrick and Seidel
[8] is, surprisingly, order-oblivious instance-optimal. For the 3 dimensional case,
they described a completely new algorithm and a proof of its order-oblivious
instance-optimality.
To reﬁne the analysis, Afshani et al. [2] introduced the notion of structural
entropy of a set S of points, a measure of diﬃculty of the input instances of
Maxima, and described two algorithms sensitive to this measure. To deﬁne the
structural entropy of a point-set, Afshani et al. [2] ﬁrst introduced the concept
of respectful partition:

Deﬁnition 1 (Respectful Partition). A partition Π of a set S of points in
Rd into disjoint subsets S1, . . . ,St is said to be respectful if each subset Sk is
either a singleton or can be enclosed by a d-dimensional axis-aligned box Bk
whose interior is completely below the Maxima of S.

Intuitively, the entropy of a respectful partition is the minimum number of bits
required to encode it, and the structural entropy of a point-set is the entropy of
a respectful partition with minimal entropy. Formally, the structural entropy of
a point-set is deﬁned as follows:
Deﬁnition 2 (Structural Entropy). The entropy H(Π) of a partition Π of S
k=1 (|Sk|/n) log(n/|Sk|).
The structural entropy H(S) of the input set S is the minimum of H(Π) over
all respectful partitions Π of S.

into disjoint subsets S1, . . . ,St is deﬁned as the value(cid:80)t

Afshani et al. [2] showed that the Maxima of a point-set in dimensions 2 and 3,
can be computed in time within O(n(H(S) + 1)), and proved a matching lower
bound for any algorithm which ignores the input order.

In Section 3 we generalize to higher dimensions Afshani et al. [2]’s upper
bound on the computational complexity of Maxima in the worst case over instances 
of ﬁxed size and structural entropy. This yields an algorithm sensitive to
the structural entropy of the input set, hence improving on the solution described
by Kirkpatrick and Seidel [8] for d ≥ 4.

3 Multivariate analysis for the computation of Maxima

3

We present the algorithm DPC-Maxima (where DPC stands for “Divide, Prune,
and Conquer”) which computes the Maxima of a d-dimensional set S of points
with running time sensitive to the structural entropy of the input set.

Similar to Afshani et al.’s algorithm [2], DPC-Maxima performs several pruning 
steps removing from S points that are detected to be dominated, until the
remaining set becomes the Maxima of the original set. For this, the algorithm
repeatedly partitions S into an increasing number of subsets, and for each partition 
removes from S the subsets that are contained in a box completely below
the Maxima of S. The following is an outline of DPC-Maxima:

Algorithm 1 DPC-Maxima
Input: A set S of n points in Rd
Output: The Maxima of S
1: j ← 0, M ← {S}
2: while there is a set in M with more than one point do
3:
4:
5:
6:
7:

j ← j + 1, rj ← 22j
partition S into rj subsets S1, S2, . . . , Srj of size at most n/rj
for i = [1..rj] do

, M ← {}

Let Γi be the minimum axis-aligned enclosing rectangle of Si
if Γi is below the Maxima of S then remove all points in Si from S
else add Si to M

8: return the points remaining in S

Note that DPC-Maxima always terminates: due to the restriction on the size
of the subsets (in step 4), at most log log n iterations of the outer loop will
be performed for any set S of n points. Besides, the algorithm is correct: only
dominated points are removed, and in the last iteration of the outer loop every
subset is a singleton, so the points remaining in S after that iteration are precisely
those in the Maxima of S.

In the rest of the section we analyze the running time of DPC-Maxima. Two
main issues need to be addressed: how partitions are chosen in step 4, and how
the ﬁlter steps 5-7 are performed eﬃciently. Partitions need to be chosen with
care: if B is a box in an optimal respectful partition of S, the subsets considered
by the algorithm that contain points within B need to be small (compact) enough
so they fall completely inside B (and the subsets get pruned), but large enough
that only a small number of subsets is needed to prune all the points within B.
Besides, for a given partition, DPC-Maxima needs to check in small time whether
the minimum axis-aligned enclosing rectangle of each subset in the partition is
below the Maxima of S.

4

We address ﬁrst the selection of the partition. We show in the following
lemma that any point set S can be partitioned into subsets so that the faces of
any axis aligned box B intersects a small number of them. In particular, if B is
a box in a respectful partition of S, the lemma provides a bound on the number
of points in S ∩ B remaining after the ﬁltering in steps 5-7 of the algorithm
DPC-Maxima.
Lemma 1. Let S be a set of n points in Rd, and r be an integer in [1..n]. There
is a partition Π of S into r subsets such that each subset has at most n/r points,
and for any axis aligned d-dimensional box B:
i. B partially intersects (i.e. intersects but does not contain) at most O(r1−1/d)
ii. There are at most min{|S ∩ B|,O(n/r1/d)} points within B that belong to

subsets in Π

subsets in Π not fully contained within B

Besides, this partition can be found in time within O(|S| log r).

Proof. The partition Π can be obtained building a k-d tree [4], using the median
to split at each level, and recursing until the number of points within each cell
drops to n/r or less. Built like this, the k-d tree will have at most log r levels
and r leaf cells, each containing at most n/r points. For each leaf cell in the k-d
tree, the subset of points contained within it is added to Π. If needed, empty
subsets are added also to obtain the total r subsets. This way of proceeding
requires running time within O(|S|) for each level, for a total running time
within O(|S| log r).
To prove proposition (i), we use the fact that any axis aligned d-dimensional
box intersects at most O(r1−1/d) leaf cells in a k-d tree with O(r) leaf cells [4].
Since the subsets in Π fall within cells in a k-d tree with at most r leafs, the
result follows. Finally, (ii) derives from (i) and from the fact that every subset
has at most n/r points: the points within B belong, in the partition, to subsets
that are completely contained within B, or partially intersecting B; the latest
ones can be at most O(r1−1/d) because of (i), and contain at most n/r points
each, for a total number of points within O(n/r1/d), which completes the proof.(cid:117)(cid:116)

Now, we turn the attention to the ﬁltering process in steps 5-7 of the algorithm 
DPC-Maxima. For this, we introduce a data structure that answers dominance 
queries over a point set, and that beneﬁts from knowing a priori the
number of queries to perform. With it, the subsets in the partition that fall
within boxes completely below the Maxima can be detected in small time.
Lemma 2. Given a set S of n points in Rd, and an integer parameter r ∈ [1..n],
there is a data structure that answers r dominance queries over S (i.e. given a
query point q, detecting whether exists a point p ∈ S that dominates q) in total
time and space within O(n logd−2 r).

5

Proof. Afshani et al. [2] introduced a simple data structure that answers dominance 
queries over a point-set of size n in time within O(logd−2 n), with preprocessing 
time within O(n logd−2 n). Combining this data structure with a grouping 
trick (e.g. such as described by Chan [5]) yields the result in the lemma:
S is partitioned into (cid:100)n/r(cid:101) subsets S1, . . . , S(cid:100)n/r(cid:101) of size at most r, and each
Si is processed into a data structure to answer dominance queries over Si. The
r (r logd−2 r)) = O(n logd−2 r). To answer
total preprocessing time is within O( n
whether a point q is dominated by any of the points in S, the data structures
corresponding to each Si are queried, for i ∈ [1..(cid:100)n/r(cid:101)]: the ﬁnal answer is “yes”
only if q is dominated in any of the data structures. Thus, each query is answered
(cid:117)(cid:116)
in time within O( n
Note that, to detect whether a subset of S falls within a box that is completely
below the Maxima of S (as in step 7 of DPC-Maxima), it is enough to query
whether the corner with maximal coordinates (i.e. the “upper, right,. . . ” corner)
of the minimum axis-aligned rectangle of the subset is dominated by any point
in S. With this observation, and lemmas 1 and 2 we provide a ﬁrst bound for
the running time of DPC-Maxima:
Lemma 3. Let S be a set of points, and σj be the size of S right after iteration 
j of the algorithm DPC-Maxima. The running time of DPC-Maxima is within

r logd−2 r), for a total time within O(n logd−2 r).

O(cid:16)(cid:80)log log n

j=0

σj × 2j(d−2)(cid:17)

.

Proof. Let rj = 22j
(as deﬁned in step 3 of DPC-Maxima). Obtaining the partition 
in step 4 using Lemma 1 costs time within O(|S| log rj). Besides, the data
structure described in Lemma 2 allows to perform the ﬁltering steps in lines 5-7
collectively in total time within O(σj logd−2 rj) (using rj as the parameter in the
lemma). Applying the deﬁnition of rj yields a total time within O(σj × 2j(d−2)).
Since at most log log n iterations of the outer loop are performed, the result fol-
(cid:117)(cid:116)
lows.

).

nk

Finally, we show that DPC-Maxima runs in time sensitive to the structural entropy
of the input set in the following theorem:
Theorem 1. Let S be a set of n points in Rd, and let Π be any optimal respectful 
partition of S into subsets S1, . . . ,Sh of sizes n1, . . . , nh, respectively.
The algorithm DPC-Maxima computes the Maxima of S in time within O(n +

(cid:80)h
k=1 nk logd−2 n
running time of DPC-Maxima is within O(cid:16)(cid:80)log log n

Proof. Let σj be the size of S during iteration j. From Lemma 3 we have that the
. Now, consider
a subset Sk in Π, and let Bk be the minimum axis-aligned box enclosing Sk. By
Lemma 1.(ii) there are at most min{nk,O(n/22j /d)} points in Sk that remain
in S after iteration j (remember that rj = 22j
is used as the parameter in the
lemma). By summing over all the subsets in the partition, and plugging the
previous bound for the point remaining in each subset after the j-th iteration of
the outer loop, we have that the running time of the algorithm is within:

σj × 2j(d−2)(cid:17)

j=0

6

log log n(cid:88)

j=0

O(σj × 2j(d−2))

j=0

k=1

min{nk,O(n/22j /d)} × 2j(d−2)

⊆ log log n(cid:88)
h(cid:88)
log log n(cid:88)
h(cid:88)
min{nk,O(n/22j /d)} × 2j(d−2)
log(d log n
log log n(cid:88)
)(cid:88)
∈ h(cid:88)

nk × 2j(d−2) +

O

k=1

j=0

=

nk

k=1

j=0

j=1+log(d log n
nk

(reordering terms)

n
1

d 2j × 2j(d−2)

2

)



The left inner summation can be bounded by directly by O(nk logd−2 n
show that the right one is within O(nk), start by bounding 2j(d−2) by 2
and take the inﬁnite sum:

nk

) 1. To
2j−1

d

,

log log n(cid:88)

∞(cid:88)

× 2j(d−2) ≤

n
2j
d

2

n
2j−1

d

2

j=1+log(d log n
nk

)

j=1+log(d log n
nk

)

This settles the case when nk > n/2: when n is factored out, the remaining
series converges, and the total summation is within O(n) ⊆ O(nk). For nk ≤ n/2,
we substitute the variable of the sum by i = j − log(d log n

) − 1 to obtain:

∞(cid:88)

n
2j−1

d

2

n

22i log n/nk

=

j=1+log(d log n
nk

)

∞(cid:88)
∞(cid:88)

i=0

=

= nk
⊆ O(nk)

i=0

1

(n/nk)2i−1

≤ nk

(cid:17) ⊆ O(cid:16)

n +(cid:80)h

O(cid:16)(cid:80)h

∞(cid:88)
∞(cid:88)

i=0

i=0

n

2i

n/nk

1

(2)2i−1

(because n/nk ≥ 2)

(the serie converges)

nk

(cid:17)

Finally, replacing the bounds for the two inner summations yields the bound
(cid:117)(cid:116)

k=1 nk(1 + logd−2 n

k=1 nk logd−2 n

)

.

nk

nk

We prove next that, thanks to the concavity of the polylog function, the bound
in Theorem 1 is never asymptotically worse than O(n logd−2 h), and hence the
running time of DPC-Maxima is never worse than the running time of Kirkpatrick
and Seidel’s algorithm [8]:

i=0 2c×i = 2c+cm−1

2c−1 ∈ O(2cm) ⊆ O(logc m), for any constant c.

1 Use the identity(cid:80)m

7
Lemma 4. Let S be a set of n points in Rd, of Maxima of size h, and let Π be
any optimal respectful partition of S into subsets S1, . . . ,Sh of sizes n1, . . . , nh,
respectively. Then,

(cid:32) h(cid:88)

(cid:33)
⊆ O(cid:16)
k ak, b =(cid:80)
Proof. For any concave function ϕ, and a =(cid:80)
(cid:19)
(cid:18) b

nk logd−2 n
nk

(cid:18) bk

(cid:88)

states that

n logd−2 h

(cid:19)

O

k=1

(cid:17)

akϕ

ak

i

≤ aϕ

.

a

k bk, Gibbs’ inequality

Since f (x) = logd−2 x is a concave function 2, choosing ak = nk
k = [1..h] yields:

n , bk = 1 for

(cid:18) n

(cid:19)

nk

logd−2

nk
n

≤ logd−2 h.

h(cid:88)

k=1

Multiplying both sides of the inequality by n yields the result.

(cid:117)(cid:116)

The upper bound in Theorem 1 properly generalizes Afshani et al.’s result [2]
for dimensions 2 and 3 (d ∈ [2, 3]), but the lower bound proving is more tricky
to prove: we discuss in the next section.

4 Discussion

The bound of O(n +(cid:80)h

nk

k=1 nk logd−2 n

) in Theorem 1 is within Θ(n logd−2 h)
for instances where all the subsets in an optimal respectful partition have equal
size n/h (as illustrated in Figure 1.a). However, for instances with optimal unbalanced 
partitions (and hence with low structural entropy), the improvement
in running time of DPC-Maxima over Kirkpatrick and Seidel’s algorithm [8] can
be signiﬁcant. Consider, for example, an instance where all the points not in
the Maxima are dominated by a same unique point (as the one illustrated in
Figure 1.b), and let h ∈ O(n1−ε) be the size of the Maxima: Kirkpatrick and
Seidel’s algorithm [8] for this instance runs in time within O(n logd−2 n), while
DPC-Maxima runs in time within O(n), linear in the input size.
For the problem of Maxima in dimension d ≥ 3, the best computational com-
),
obtained by extending the bound given in 2009 by Afshani et al. [2] for 2 dimensions.
 For dimension d ≥ 4 there is a gap between this lower bound and the
running time of the algorithm DPC-Maxima (which increases with d). The generalization 
of the order-oblivious instance-optimality result for the d-dimensional
case for d ≥ 4 is still open. In general, there is a lack of advanced lower bound
2 f (x)(cid:48)(cid:48) = d(d − 1 − log x)(logd−2x)/x2 ≤ 0 for all x ≥ 2d−1

plexity lower bound known so far is Ω(n(H(S) + 1)) = Ω(n +(cid:80)h

k=1 nk log n
nk

8

REFERENCES

Figure 1: Two sets of n points and Maxima of size h with worst-case (a) and bestcase 
(b) structural entropies, respectively. In both cases Π = {B1, B2, . . . , Bh}
is an optimal respectful partition, in (a) all the boxes contain n/h points, while
in (b) B1 contains (n − h + 1) points, and B2, . . . , Bh are singletons.

techniques for problems handling high dimensional data. In this sense, a theory 
of ﬁne classes of problems (as partially done in the ﬁeld of Parameterized
Complexity [6]) could help to palliate this lack of techniques for lower bounds.
A closely related problem is Dominance Reporting, where for a set S of
n points in d-dimensional space, and a set of query points Q, one must report
the points q ∈ Q for which there is a point in S that dominates q. For the oﬄine
version of the problem, where the size of Q is known to be signiﬁcantly bigger
than the size of S, the fastest data structure known to date, in dimension d ≥ 3,
was presented in 2012 by Afshani et al. [1]. Karp et al. [7] considered an online
version, where the size of Q is not known in advance, for which they describe a
deferred data structure sensitive to the sizes of S and Q. One natural question
is whether the results by Karp et al. [7] for online Dominance Reporting can
be improved via a deferred data structure sensitive to the structural entropy of
the input set to query.

References

[1] Peyman Afshani, Lars Arge, and Kasper Green Larsen. “Higher-dimensional
orthogonal range reporting and rectangle stabbing in the pointer machine
model.” In: Symposuim on Computational Geometry 2012, SoCG ’12, Chapel
Hill, NC, USA, June 17-20, 2012. Ed. by Tamal K. Dey and Sue Whitesides.
ACM, 2012, pp. 323–332.

[2] Peyman Afshani, J´er´emy Barbay, and Timothy M. Chan. “Instance-Optimal
Geometric Algorithms.” In: 50th Annual IEEE Symposium on Foundations
of Computer Science, FOCS 2009, October 25-27, 2009, Atlanta, Georgia,
USA. IEEE Computer Society, 2009, pp. 129–138.

B1B2Bhx1x2B1B2Bhx1x2(a)(b)REFERENCES

9

[3] J´er´emy Barbay, Ankur Gupta, Srinivasa Rao Satti, and Jonathan Sorenson.
“Near-optimal online multiselection in internal and external memory.” In:
J. Discrete Algorithms 36 (2016), pp. 3–17.

[4] Jon Louis Bentley. “Multidimensional Binary Search Trees Used for Associative 
Searching.” In: Commun. ACM 18.9 (1975), pp. 509–517.

[5] Timothy M. Chan. “Output-Sensitive Results on Convex Hulls, Extreme

Points, and Related Problems.” In: vol. 16. 4. 1996, pp. 369–387.

[6] Rodney G. Downey and Michael R. Fellows. Parameterized Complexity.

Monographs in Computer Science. Springer, 1999.

[7] Richard M. Karp, Rajeev Motwani, and Prabhakar Raghavan. “Deferred

Data Structuring.” In: SIAM J. Comput. 17.5 (1988), pp. 883–902.

[8] David G. Kirkpatrick and Raimund Seidel. “Output-size sensitive algorithms 
for ﬁnding maximal vectors.” In: Proceedings of the First Annual
Symposium on Computational Geometry, Baltimore, Maryland, USA, June
5-7, 1985. Ed. by Joseph O’Rourke. ACM, 1985, pp. 89–96.

[9] H. T. Kung, Fabrizio Luccio, and Franco P. Preparata. “On Finding the

Maxima of a Set of Vectors.” In: J. ACM 22.4 (1975), pp. 469–476.

