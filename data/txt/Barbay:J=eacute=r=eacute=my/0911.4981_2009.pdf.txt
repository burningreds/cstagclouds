Eﬃcient Fully-Compressed Sequence Representations ∗
Gonzalo Navarro†

Francisco Claude‡

J´er´emy Barbay†

Travis Gagie§

2
1
0
2

 
r
p
A
2

 

 
 
]
S
D
.
s
c
[
 
 

4
v
1
8
9
4

.

1
1
9
0
:
v
i
X
r
a

Yakov Nekrich†

Abstract

We present a data structure that stores a sequence s[1..n] over alphabet [1..σ] in nH0(s) +
o(n)(H0(s)+1) bits, where H0(s) is the zero-order entropy of s. This structure supports the
queries access, rank and select, which are fundamental building blocks for many other compressed
data structures, in worst-case time O (lg lg σ) and average time O (lg H0(s)). The worst-case
complexity matches the best previous results, yet these had been achieved with data structures 
using nH0(s) + o(n lg σ) bits. On highly compressible sequences the o(n lg σ) bits of the
redundancy may be signiﬁcant compared to the the nH0(s) bits that encode the data. Our representation,
 instead, compresses the redundancy as well. Moreover, our average-case complexity
is unprecedented.

Our technique is based on partitioning the alphabet into characters of similar frequency.
The subsequence corresponding to each group can then be encoded using fast uncompressed
representations without harming the overall compression ratios, even in the redundancy.

The result also improves upon the best current compressed representations of several other
data structures. For example, we achieve (i) compressed redundancy, retaining the best time
complexities, for the smallest existing full-text self-indexes; (ii) compressed permutations π with
times for π() and π−1() improved to loglogarithmic; and (iii) the ﬁrst compressed representation
of dynamic collections of disjoint sets. We also point out various applications to inverted indexes,
suﬃx arrays, binary relations, and data compressors.

Our structure is practical on large alphabets. Our experiments show that, as predicted by
theory, it dominates the space/time tradeoﬀ map of all the sequence representations, both in
synthetic and application scenarios.

1

Introduction

A growing number of important applications require data representations that are space-eﬃcient
and at the same time support fast query operations.
In particular, suitable representations of
sequences supporting a small set of basic operations yield spaceand 
time-eﬃcient implementations
for many other data structures such as full-text indexes [32, 28, 22, 48], labeled trees [4, 3, 20],
binary relations [4, 2], permutations [6] and two-dimensional point sets [42, 11], to name a few.

Let s[1..n] be a sequence of characters belonging to alphabet [1..σ]. In this article we focus on

the following set of operations, which is suﬃcient for many applications:

Annual International Symposium on Algorithms and Computation (ISAAC), part II, pp. 215–326, 2010.

∗Funded in part by Fondecyt Project 1-110066, Chile. An early version of this article appeared in Proc. 21st
†Dept. of Computer Science, University of Chile. {jbarbay|gnavarro|yasha}@dcc.uchile.cl.
‡David R. Cheriton School of Computer Science, University of Waterloo, Canada. fclaude@cs.uwaterloo.ca.
§Dept. of Computer Science, Aalto University, Finland. travis.gagie@gmail.com.

1

Table 1: Best previous bounds and our new ones for data structures supporting access, rank and
select. The space bound of the form Hk(s) holds for any k = o(lgσ n), and those of the form
(1 + ) hold for any constant  > 0. On average lg σ becomes H0(s) in our time complexities (see
Corollary 3) and in row 1 [7, Thm. 5].

[29, Thm. 4]

[4, Lem. 4.1]

[33, Cor. 2]

space (bits)
nH0(s) + o(n)
nH0(s) + o(n lg σ)
nHk(s) + o(n lg σ)

[28, Thm. 2.2]

(1 + )n lg σ

O(cid:16)

1 + lg σ
lg lg n
O (lg lg σ)

O (1)
O (1)

Thm. 2

Thm. 2

Cor. 4

nH0(s) + o(n)(H0(s) + 1)
nH0(s) + o(n)(H0(s) + 1)

(1 + )nH0(s) + o(n)

O (lg lg σ)

O (1)
O (1)

access

rank

select

(cid:17) O(cid:16)

(cid:17) O(cid:16)

(cid:17)

1 + lg σ
lg lg n
O (1)

O (lg lg σ)

O (1)
O (1)

O (lg lg σ)

O (1)

1 + lg σ
lg lg n
O (lg lg σ)
O (lg lg σ)
O (lg lg σ)
O (lg lg σ)
O (lg lg σ)
O (lg lg σ)

s.access(i) returns the ith character of sequence s, which we denote s[i];

s.ranka(i) returns the number of occurrences of character a up to position i in s; and

s.selecta(i) returns the position of the ith occurrence of a in s.

order entropy space plus just o(n) bits of redundancy, and support queries in time O(cid:16)

Table 1 shows the best sequence representations and the complexities they achieve for the three
queries, where Hk(s) refers to the k-th order empirical entropy of s [43]. To implement the operations 
eﬃciently, the representations require some redundancy space on top of the nH0(s) or nHk(s)
bits needed to encode the data. For example, multiary wavelet trees (row 1) represent s within zero-
.
This is very attractive for relatively small alphabets, and even constant-time for polylog-sized ones.
For large σ, however, all the other representations in the table are exponentially faster, and some
even achieve high-order compression. However, their redundancy is higher, o(n lg σ) bits. While
this is still asymptotically negligible compared to the size of a plain representation of s, on highly
compressible sequences such redundancy is not always negligible compared to the space used to
encode the compressed data. This raises the challenge of retaining the eﬃcient support for the
queries while compressing the index redundancy as well.

1 + lg σ
lg lg n

(cid:17)

In this paper we solve this challenge in the case of zero-order entropy compression, that is,
the redundancy of our data structure is asymptotically negligible compared to the zero-order compressed 
text size (not only compared to the plain text size), plus o(n) bits. The worst-case time
our structure achieves is O (lg lg σ), which matches the best previous results for large σ. Moreover,
the average time is logarithmic on the entropy of the sequence, O (lg H0(s)), under reasonable assumptions 
on the query distribution. This average time complexity is also unprecedented: the only
previous entropy-adaptive time complexities we are aware of come from Huﬀman-shaped wavelet
query time with just

trees [32], which have recently been shown capable of achieving O(cid:16)

(cid:17)

1 +

H0(s)
lg lg n

o(n) bits of redundancy [7, Thm. 5].

2

Our technique is described in Section 3. It can be summarized as partitioning the alphabet
into sub-alphabets that group characters of similar frequency in s, storing in a multiary wavelet
tree [22] the sequence of sub-alphabet identiﬁers, and storing separate sequences for each subalphabet,
 containing the subsequence of s formed by the characters of that sub-alphabet. Golynski
et al.’s [28] or Grossi et al.’s [33] structures are used for these subsequences, depending on the
tradeoﬀ to be achieved. We show that it is suﬃcient to achieve compression in the multiary
wavelet tree, while beneﬁtting from fast operations on the representations of the subsequences.

The idea of alphabet partitioning is not new. It has been used in practical scenarios such as
fax encoding, JPEG and MPEG formats [51, 36], and in other image coding methods [50], with
the aim of speeding up decompression: only the (short) sub-alphabet identiﬁer is encoded with a
sophisticated (and slow) method, whereas the sub-alphabet characters are encoded with a simple
and fast encoder (even in plain form). Said [58] gave a more formal treatment to this concept,
and designed a dynamic programming algorithm to ﬁnd the optimal partitioning given the desired
number of sub-alphabets, that is, the one minimizing the redundancy with respect to the zero-order
entropy of the sequence. He proved that an optimal partitioning deﬁnes sub-alphabets according to
ranges of character frequencies, which reduces the cost of ﬁnding such partitioning to polynomial
time and space (more precisely, quadratic on the alphabet size).

Our contribution in this article is, on one hand, to show that a particular way to deﬁne the
sub-alphabets, according to a quantization of the logarithms of the inverse probabilities of the
characters, achieves o(H0(s) + 1) bits of redundancy per character of the sequence s. This value,
in particular, upper bounds the coding eﬃciency of Said’s optimal partitioning method. On the
other hand, we apply the idea to sequence data structures supporting operations access/rank/select,
achieving eﬃcient support of indexed operations on the sequence, not only fast decoding.

We also consider various extensions and applications of our main result. In Section 4 we show
how our result can be used to improve an existing text index that achieves k-th order entropy [22, 4],
so as to improve its redundancy and query times. In this way we achieve the ﬁrst self-index with
space bounded by nHk(s) + o(n)(Hk(s) + 1) bits, for any k = o(lgσ n), able to count and locate
pattern occurrences and extract any segment of s within the time complexities achieved by its
fastest predecessors. We also achieve new space/time tradeoﬀs for inverted indexes and binary
relations.
In Sections 5 and 6 we show how to apply our data structure to store a compressed
permutation and a compressed function, respectively, supporting direct and inverse applications
and in some cases improving upon previous results [6, 7, 47, 38]. We describe further applications
to text indexes and binary relations. In particular, an application of permutations, at the end of
Section 5, achieves for the ﬁrst time compressed redundancy to store function Ψ of text indexes
[35, 32, 56]. Section 7 shows how to maintain a dynamic collection of disjoint sets, while supporting
operations union and ﬁnd, in compressed form. This is, to the best of our knowledge, the ﬁrst result
of this kind.

2 Related work

Sampling. A basic attempt to provide rank and select functionality on a sequence s[1..n] over
alphabet [1..σ] is to store s in plain form and the values s.ranka(k·i) for all a ∈ [1..σ] and i ∈ [1..n/k],
where k ∈ [1..n] is a sampling parameter. This yields constant-time access, O (k/ lgσ n) time rank,
and O (k/ lgσ n + lg lg n) time select if we process Θ(lgσ n) characters of s in constant time using
universal tables, and organize the rank values for each character in predecessor data structures.

3

The total space is n lg σ + O ((n/k)σ lg n). For example, we can choose k = σ lg n to achieve total
space n lg σ +O (n) (that is, the data plus the redundancy space). Within this space we can achieve
time complexity O (σ lg σ) for rank and O (σ lg σ + lg lg n) for select.

Succinct indexes. The previous construction separates the sequence data from the “index”, that
is, the extra data structures to provide fast rank and select. There are much more sophisticated
representations for the sequence data that oﬀer constant-time access to Θ(lgσ n) consecutive characters 
of s (i.e., just as if s were stored in plain form), yet achieving nHk(s) + o(n lg σ) bits of space,
for any k = o(lgσ n) [57, 31, 23]. We recall that Hk(s) is the k-th order empirical entropy of s [43],
a lower bound to the space achieved by any statistical compressor that models the character probabilities 
using the context of their k preceding characters, so 0 ≤ Hk(s) ≤ Hk−1(s) ≤ H0(s) ≤ lg σ.
Combining such sequence representations with sophisticated indexes that require o(n lg σ) bits of
redundancy [4, 33] (i.e., they are “succinct”), we obtain results like row 3 of Table 1.
Bitmaps. A diﬀerent alternative is to maintain one bitmap ba[1..n] per character a ∈ [1..σ],
marking with a 1 the positions i where s[i] = a. Then s.ranka(i) = ba.rank1(i) and s.selecta(j) =
ba.select1(j). The bitmaps can be represented in compressed form using “fully indexable dictionar-
ies” (FIDs) [54], so that they operate in constant time and the total space is nH0(s) +O (n) + o(σn)
(and the
time is O (c)) for any constant c, which is acceptable only for polylog-sized alphabets, that is,
σ = O (polylog(n)). An alternative is to use weaker compressed bitmap representations [35, 49]
that can support select1 in constant time and rank1 in time O (lg n), and yield an overall space of
nH0(s) + O (n) bits. This can be considered as a succinct index over a given sequence representation,
 or we can note that we can actually solve s.access(i) by probing all the bitmaps ba.access(i).
Although this takes at least O (σ) time, it is a simple illustration of another concept: rather than
storing an independent index on top of the data, the data is represented in a way that provides
access, rank and select operations with reasonable eﬃciency.

bits. Even with space-optimal FIDs [52, 53], this space is nH0(s) + O (n) + O(cid:16) σn

(cid:17)

lgc n

By using FIDs [54] to represent those bitmaps, wavelet trees achieve nH0(s) + O(cid:16) n lg σ lg lg n

Wavelet trees. The wavelet tree [32] is a structure integrating data and index, that provides more
balanced time complexities. It is a balanced binary tree with one leaf per alphabet character, and
storing bitmaps in its internal nodes, where constant-time rank and select operations are supported.
bits
of space and support all three operations in time proportional to their height, O (lg σ). Multiary
wavelet trees [22] replace the bitmaps by sequences over sublogarithmic-sized alphabets [1..σ(cid:48)],
σ(cid:48) = O (lg n) for 0 <  < 1, in order to reduce that height. The FID technique is extended to
alphabets of those sizes while retaining constant times. Multiary wavelet trees obtain the same
space as the binary ones, but their time complexities are reduced by an O (lg lg n) factor. Indeed, if
σ is small enough, σ = O (polylog(n)), the tree height is a constant and so are all the query times.
Recently, the redundancy of multiary (and binary) wavelet trees has been reduced to just o(n) [29],
which yields the results in the ﬁrst row of Table 1.1

(cid:17)

lg n

1Because of these good results on polylog-sized alphabets, we focus on larger alphabets in this article, and therefore
do not distinguish between redundancies of the form o(n) lg σ and n o(lg σ), writing o(n lg σ) for all. See also Footnote
6 of Barbay et al. [4].

4

(cid:17)

Huﬀman-shaped wavelet trees. Another alternative to obtain zero-order compression is to
give Huﬀman shape to the wavelet tree [32]. This structure uses nH0(s) + o(nH0(s)) + O (n) bits
even if the internal nodes use a plain representation, using |b| + o(|b|) bits [13, 46], for their bitmaps
b. Limiting the height to O (lg σ) retains the worst-case times of the balanced version and also
the given space [7]. In order to reduce the time complexities by an O (lg lg n) factor, we can build
multiary wavelet trees over multiary Huﬀman trees [39]. This can be combined with the improved
representation for sequences over small alphabets [29] so as to retain the nH0(s) + o(n) bits of space
worst-case times of balanced multiary wavelet trees. The interesting aspect of
using Huﬀman-shaped trees is that, if the access queries distribute uniformly over the text positions,
and the character arguments a to ranka and selecta are chosen according to their frequency in s,
, the weighted leaf depth. This result [7,
Thm. 5] improves upon the multiary wavelet tree representation [29] in the average case. We note
that this result [7] involves O (σ lg n) extra bits of space redundancy, which is negligible only for
σ = o(n/ lg n).

and O(cid:16)
then the average time complexities are O(cid:16)

1 + lg σ
lg lg n

H0(s)
lg lg n

(cid:17)

1 +

(cid:17)

large enough. Yet, this representation requires again uncompressed space, n lg σ + O(cid:16) n lg σ

Reducing to permutations. A totally diﬀerent sequence representation [28] improves the times
to poly-loglogarithmic on σ, that is, exponentially faster than multiary wavelet trees when σ is
.2 It
cuts the sequence into chunks of length σ and represents each chunk using a permutation π (which
acts as an inverted index of the characters in the chunk). As both operations π() and π−1() are
needed, a representation [47] that stores the permutation within (1 + )σ lg σ bits and computes π()
in constant time and π−1() in time O (1/) is used. Depending on whether π or π−1 is represented
explicitly, constant time is achieved for select or for access. Using a constant value for  yields a
slightly larger representation that solves both access and select in constant time.
Later, the space of this representation was reduced to nH0(s) + o(n lg σ) bits while retaining
the time complexities of one of the variants (constant-time select) [4]. In turn, the variant oﬀering
constant-time access was superseded by the index of Grossi et al. [33], which achieves high-order
compression and also improves upon a slower alternative that takes the same space [4]. The best
current times are either constant or O (lg lg σ). We summarize them in rows 2 to 4.

lg lg σ

Our contribution, in rows 5 to 7 of Table 1, is to retain times loglogarithmic on σ, as in rows 2
to 4, while compressing the redundancy space. This is achieved only with space H0(s), not Hk(s).
We also achieve average times depending on H0(s) instead of lg σ.

3 Alphabet partitioning

Let s[1..n] be a sequence over eﬀective alphabet [1..σ].3 We represent s using an alphabet partitioning 
scheme. Our data structure has three components:

1. A character mapping m[1..σ] that separates the alphabet into sub-alphabets. That is, m is

personal communication).

2The representation actually compresses to the k-th order entropy of a diﬀerent sequence, not s (A. Golynski,
3By eﬀective we mean that every character appears in s, and thus σ ≤ n. In Section 3.3 we handle the case of

larger alphabets.

5

the sequence assigning to each character a ∈ [1..σ] the sub-alphabet

m[a] = (cid:100)lg(n/|s|a) lg n(cid:101),

where |s|a denotes the number of occurrences of character a in s; note that m[a] ≤(cid:6)lg2 n(cid:7)
sequence over(cid:2)1..(cid:6)lg2 n(cid:7)(cid:3) obtained from s by replacing each occurrence of a by m[a], namely

2. The sequence t[1..n] of the sub-alphabets assigned to each character in s. That is, t is the

for any a ∈ [1..σ].

t[i] = m[s[i]].

3. The subsequences s(cid:96)[1..σ(cid:96)] of characters of each sub-alphabet. For 0 ≤ (cid:96) ≤ (cid:100)lg2 n(cid:101), let
σ(cid:96) = |m|(cid:96), that is, the number of distinct characters of s replaced by (cid:96) in t. Then s(cid:96)[1..|t|(cid:96)] is
the sequence over [1..σ(cid:96)] deﬁned by

for all 1 ≤ i ≤ n such that t[i] = (cid:96).

s(cid:96)[t.rank(cid:96)(i)] = m.rank(cid:96)(s[i]),

Example 1 Let s = "alabar a la alabarda". Then n = 20 and |s|a = 9, |s|l = |s|(cid:48)
(cid:48) = 3,
|s|b = |s|r = 2, and |s|d = 1. Accordingly, we deﬁne the mapping as m[a] = 5, m[l] = m[(cid:48) (cid:48)] = 12,
m[b] = m[r] = 15, and m[d] = 19. As this is the eﬀective alphabet, and assuming that the order is
"’ ’,a,b,d,l,r", we have m = (12, 5, 15, 19, 12, 15). So the sequence of sub-alphabet identiﬁers
is t[1..20] = (5, 12, 5, 15, 5, 15, 12, 5, 12, 12, 5, 12, 5, 12, 5, 15, 5, 15, 19, 5), and the subsequences are
s5 = (1, 1, 1, 1, 1, 1, 1, 1, 1), s12 = (2, 1, 1, 2, 1, 2), s15 = (1, 2, 1, 2), and s19 = (1).

With these data structures we can implement the queries on s as follows:

s.access(i) = m.select(cid:96)(s(cid:96).access(t.rank(cid:96)(i))), where (cid:96) = t.access(i);

s.ranka(i) = s(cid:96).rankc(t.rank(cid:96)(i)), where (cid:96) = m.access(a) and c = m.rank(cid:96)(a);

s.selecta(i) = t.select(cid:96)(s(cid:96).selectc(i)) where (cid:96) = m.access(a) and c = m.rank(cid:96)(a).

Example 2 In the representation of Ex. 1, we solve s.access(6) by ﬁrst computing (cid:96) = t.access(6) =
15 and then m.select15(s15.access(t.rank15(6))) = m.select15(s15.access(2)) = m.select15(2) = r.
Similarly, to solve s.rankl(14) we compute (cid:96) = m.access(l) = 12 and c = m.rank12(l) = 2. Then
we return s12.rank2(t.rank12(14)) = s12.rank2(6) = 3. Finally, to solve s.selectr(2), we compute (cid:96) =
m.access(r) = 15 and c = m.rank15(r) = 2, and return t.select15(s15.select2(2)) = t.select15(4) = 18.

3.1 Space analysis

Recall that the zero-order entropy of s[1..n] is deﬁned as
|s|a
n

H0(s) =

(cid:88)

a∈[1..σ]

lg

n
|s|a

.

(1)

Recall also that, by convexity, nH0(s) ≥ (σ − 1) lg n + (n − σ + 1) lg
the key result for the space analysis.

n

n−σ+1 . The next lemma gives

6

Lemma 1 Let s, t, σ(cid:96) and s(cid:96) be as deﬁned above. Then nH0(t) +(cid:80)

(cid:96) |s(cid:96)| lg σ(cid:96) ∈ nH0(s) + o(n).

Proof: First notice that, for any character 1 ≤ (cid:96) ≤ (cid:100)lg2 n(cid:101) it holds that

(cid:88)

|s|c = |s(cid:96)| .

Now notice that, if m[a] = m[b] = (cid:96), then

c, (cid:96)=m[c]

therefore

and so

(cid:96) = (cid:100)lg(n/|s|a) lg n(cid:101) = (cid:100)lg(n/|s|b) lg n(cid:101),
lg(n/|s|b) − lg(n/|s|a) < 1/ lg n,

|s|a < 21/ lg n|s|b .

(2)

(3)

(4)

(cid:3)

Now, ﬁx a, call (cid:96) = m[a], and sum Eq. (3) over all those b such that m[b] = (cid:96). The second step
uses Eq. (2):

(cid:88)

(cid:88)

|s|a <

21/ lg n |s|b,

b, (cid:96)=m[b]

b, (cid:96)=m[b]

σ(cid:96) |s|a < 21/ lg n |s(cid:96)| ,

Since(cid:80)

a |s|a =(cid:80)

=

<

=

=

|s(cid:96)| lg σ(cid:96)

(cid:88)

(cid:88)

(cid:96)

|s(cid:96)| lg(n/|s(cid:96)|) +

σ(cid:96) < 21/ lg n |s(cid:96)|/|s|a .
(cid:96) |s(cid:96)| = n, we have, using Eq. (1), (2), and (4),
nH0(t) +
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)

|s|a lg(n/|s|a) +

|s|a lg(n/|s(cid:96)|) +

(cid:88)
(cid:88)

(cid:88)
(cid:88)

|s|a lg σ(cid:96)

(cid:96)

a, (cid:96)=m[a]

(cid:96)

a, (cid:96)=m[a]

(cid:96)

a, (cid:96)=m[a]

(cid:96)

a, (cid:96)=m[a]

(cid:96)

(cid:96)

a, (cid:96)=m[a]

|s|a lg(n/|s|a) + n/ lg n

a

∈ nH0(s) + o(n) .

(cid:16)

21/ lg n|s(cid:96)|/|s|a

(cid:17)

|s|a lg

|s|a/ lg n

In other words, if we represent t with H0(t) bits per character and each s(cid:96) with lg σ(cid:96) bits per
character, we achieve a good overall compression. Thus we can obtain a very compact representation
of a sequence s by storing a compact representation of t and storing each s(cid:96) as an “uncompressed”
sequence over an alphabet of size σ(cid:96).

7

3.2 Concrete representation

lg n

lg lg lg n

lg lg lg n

lg n

lg n

lg lg σ(cid:96)

(cid:17)

(cid:17)

(cid:17)

bits (the latter because σ(cid:96) > lg n). Thus in either case the space for

We represent t and m as multiary wavelet trees [22]; we represent each s(cid:96) as either a multiary
wavelet tree or an instance of Golynski et al.’s [28, Thm. 2.2] access/rank/select data structure,

bits and operates in constant time, because its alphabet size is polylogarithmic (i.e., (cid:100)lg2 n(cid:101)). If
bits4 and again

bits. Finally, since m is a sequence of length σ over
an alphabet of size (cid:100)lg2 n(cid:101), the wavelet tree for m takes O (σ lg lg n) bits and also operates in
constant time. Because of the convexity property we referred to in the beginning of this section,

depending on whether σ(cid:96) ≤ lg n or not. The wavelet tree for t uses at most nH0(t) + O(cid:16) n(lg lg n)2
(cid:17)
s(cid:96) is represented as a wavelet tree, it uses at most |s(cid:96)|H0(s(cid:96)) + O(cid:16)|s(cid:96)| lg σ(cid:96) lg lg n
(cid:17) ≤
operates in constant time because σ(cid:96) ≤ lg n; otherwise it uses at most |s(cid:96)| lg σ(cid:96) + O(cid:16)|s(cid:96)| lg σ(cid:96)
|s(cid:96)| lg σ(cid:96) + O(cid:16)|s(cid:96)| lg σ(cid:96)
s(cid:96) is bounded by |s(cid:96)| lg σ(cid:96) + O(cid:16)|s(cid:96)| lg σ(cid:96)
nH0(s) ≥ (σ − 1) lg n, the space for m is O(cid:16) n lg lg n
Therefore we have nH0(t) + o(n) bits for t, (cid:80)
where the o(n) term is O(cid:16)
space |s(cid:96)|Hk(s(cid:96)) + O(cid:16)|s(cid:96)| lg σ(cid:96)

Using the variant of Golynski et al.’s data structure [28, Thm. 4.2], that gives constant-time
select, and O (lg lg σ) time for rank and access, we obtain our ﬁrst result in Table 1 (row 4). To
obtain our second result (row 5), we use instead Grossi et al.’s result [33, Cor. 2], which gives
constant-time access, and O (lg lg σ) time for rank and select. We note that their structure takes

, yet we only need this to be at most |s(cid:96)| lg σ(cid:96) + O(cid:16)|s(cid:96)| lg σ(cid:96)

bits for the s(cid:96) sequences,
 and o(n)H0(s) bits for m. Using Lemma 1, this adds up to nH0(s) + o(n)H0(s) + o(n),

(cid:17) · H0(s).

(cid:16)

1 + O(cid:16)

(cid:96) |s(cid:96)| lg σ(cid:96)

(cid:17)(cid:17)

(cid:17)

.

(cid:17)

.

n

lg lg lg n

1

lg lg lg n

(cid:17)

lg lg σ(cid:96)

lg lg lg n

Theorem 2 We can store s[1..n] over eﬀective alphabet [1..σ] in nH0(s) + o(n)(H0(s) + 1) bits
and support access, rank and select queries in O (lg lg σ), O (lg lg σ), and O (1) time, respectively
(variant (i)); or in O (1), O (lg lg σ) and O (lg lg σ) time, respectively (variant (ii)).

We can reﬁne the time complexity by noticing that the only non-constant times are due to
operating on some sequence s(cid:96), where the alphabet is of size σ(cid:96) < 21/ lg n|s(cid:96)|/|s|a, where a is the
character in question, thus lg lg σ(cid:96) = O (lg lg(n/|s|a)). If we assume that the characters a used in
queries distribute with the same frequencies as in sequence s (e.g., access queries refer to randomly
= O (lg H0(s))

chosen positions in s), then the average query time becomes O(cid:16)(cid:80)

(cid:17)

|s|a
n lg lg n|s|a

a

by the log-sum inequality5.

Corollary 3 The O (lg lg σ) time complexities in Theorem 2 are also O (lg lg(n/|s|a)), where a
stands for s[i] in the access query, and for the character argument in the ranka and selecta queries. If

O (
in n.

√
4This is achieved by using block sizes of length lg n

, at the price of storing universal tables of size
n polylog(n)) = o(n) bits. Therefore all of our o(·) expressions involving n and other variables will be asymptotic
. Use ai = |s|i/n and bi = −ai lg ai

5Given σ pairs of numbers ai, bi > 0, it holds that(cid:80) ai lg ai

≥ ((cid:80) ai) lg

2 and not lg |s(cid:96)|

2

(cid:80) ai(cid:80) bi

bi

to obtain the result.

8

these characters a distribute on queries with the same frequencies as s, the average time complexity
for those operations is O (lg H0(s)).

Finally, to obtain our last result in Table 1 we use again Golynski et al.’s representation [28,
Thm. 4.2]. Given |s(cid:96)| lg σ(cid:96) extra space to store the inverse of a permutation inside chunks, it
answers select queries in time O (1) and access queries in time O (1/) (these two complexities can
be interchanged), and rank queries in time O (lg lg σ(cid:96)). While we initially considered 1/ = lg lg σ(cid:96)
to achieve the main result, using a constant  yields constant-time select and access simultaneously.

Corollary 4 We can store s[1..n] over eﬀective alphabet [1..σ] in (1 + )nH0(s) + o(n) bits, for any
constant  > 0, and support access, ranka and select queries in O (1/), O (lg lg min(σ, n/|s|a)), and
O (1) time, respectively (variant (i)); or in O (1), O (lg lg min(σ, n/|s|a)), and O (1/), respectively
(variant (ii)).

3.3 Handling arbitrary alphabets

In the most general case, s is a sequence over an alphabet Σ that is not an eﬀective alphabet,
and σ characters from Σ occur in s. Let Σ(cid:48) be the set of elements that occur in s; we can map
characters from Σ(cid:48) to elements of [1..σ] by replacing each a ∈ Σ(cid:48) with its rank in Σ(cid:48). All elements
of Σ(cid:48) are stored in the “indexed dictionary” (ID) data structure described by Raman et al. [54],
for any a ∈ Σ(cid:48) its rank in Σ(cid:48) can
so that the following queries are supported in constant time:
be found (for any a (cid:54)∈ Σ(cid:48) the answer is −1); and for any i ∈ [1..σ] the i-th smallest element in Σ(cid:48)
can be found. The ID structure uses σ lg(eµ/σ) + o(σ) + O (lg lg µ) bits of space, where e is the
base of the natural logarithm and µ is the maximal element in Σ(cid:48); the value of µ can be speciﬁed
with additional O (lg µ) bits. We replace every element in s by its rank in Σ(cid:48), and the resulting
sequence is stored using Theorem 2. Hence, in the general case the space usage is increased by
σ lg(eµ/σ) + o(σ) +O (lg µ) bits and the asymptotic time complexity of queries remains unchanged.
Since we are already spending O (σ lg lg n) bits in our data structure, this increases the given space
only by O (σ lg(µ/σ)).

3.4 Application to fast encode/decode
Given a sequence s to encode, we can build mapping m from its character frequencies |s|a, and
then encode each s[i] as the pair (m[s[i]], m.rankm[s[i]](s[i])). Lemma 1 (and some of the discussion
that follows in Section 3.2) shows that the overall output size is nH0(s) + o(n) bits if we represent
the sequence of pairs by partitioning it into three sequences: (1) the left part of the pairs in one
sequence, using Huﬀman coding on chunks (see next); (2) the right part of the pairs corresponding
to values where σ(cid:96) < lg n in a second sequence, using Huﬀman coding on chunks; (3) the remaining
right parts of the pairs, using plain encoding in (cid:100)lg σ(cid:96)(cid:101) bits (note σ(cid:96) = m.rank(cid:96)(σ)). The Huﬀman
lg n
coding on chunks groups
4 lg lg n characters, so that even in the case of the left parts, where the
alphabet is of size (cid:100)lg2 n(cid:101), the total length of a chunk is at most lg n
2 bits, and hence the Huﬀman
coding table occupies just O (

n lg n) bits. The redundancy on top of H0(s) adds up to O(cid:16) n lg lg n

√

(cid:17)

lg n

9

bits in sequences (1) and (2) (one bit of Huﬀman redundancy per chunk) and O(cid:16) n

lg lg n

(cid:17)

in sequence

(3) (one bit, coming from the ceil function, per lg σ(cid:96) > lg lg n encoded bits).

The overall encoding time is O (n). A pair ((cid:96), o) is decoded as s[i] = m.select(cid:96)(o), where after
reading (cid:96) we can compute σ(cid:96) to determine whether o is encoded in sequence (2) or (3). Thus
decoding also takes constant time if we can decode Huﬀman codes in constant time. This can be
achieved by using canonical codes and limiting the height of the tree [45, 24].
This construction gives an interesting space/time tradeoﬀ with respect to classical alternatives.
Using just Huﬀman coding yields O (n) encoding/decoding time, but only guarantees nH0(s)+O (n)
bits of space. Using arithmetic coding achieves nH0(s) + O (1) bits, but encoding/decoding is not
linear-time. The tradeoﬀ given by our encoding, nH0(s) + o(n) bits and linear-time decoding, is
indeed the reason why it is used in practice in various folklore applications, as mentioned in the
Introduction. In Section 8.2 we experimentally evaluate these ideas and show they are practical.
Next, we give more far-fetched applications of the rank/select capabilities of our structure, which
go much beyond the mere compression.

4 Applications to text indexing

Our main result can be readily carried over various types of indexes for text collections. These
include self-indexes for general texts, and positional and non-positional inverted indexes for natural
language text collections.

4.1 Self-indexes

A self-index represents a sequence and supports operations related to text searching on it. A
well-known self-index [22] achieves k-th order entropy space by partitioning the Burrows-Wheeler
transform [12] of the sequence and encoding each partition to its zero-order entropy. Those partitions 
must support queries access and rank. By using Theorem 2(i) to represent such partitions,
we achieve the following result, improving previous ones [22, 28, 4].

Theorem 5 Let s[1..n] be a sequence over eﬀective alphabet [1..σ]. Then we can represent s using
nHk(s) + o(n)(Hk(s) + 1) bits, for any k ≤ (δ lgσ n) − 1 and constant 0 < δ < 1, while supporting 
the following queries: (i) count the number of occurrences of a pattern p[1..m] in s, in time
O (m lg lg σ); (ii) locate any such occurrence in time O (lg n lg lg lg n lg lg σ); (iii) extract s[l, r] in
time O ((r − l) lg lg σ + lg n lg lg lg n lg lg σ).

Proof: To achieve nHk(s) space, the Burrows-Wheeler transformed text sbwt is partitioned into
r ≤ σk sequences s1 . . . sr [22]. Since k ≤ (δ lgσ n) − 1, it follows that σk+1 ≤ nδ. The space our
. Let
γ = (1 − δ)/2 (so 0 < δ + γ < 1 whenever 0 < δ < 1) and classify the sequences si according to
whether |si| < nγ (short sequences) or not (long sequences). The total space occupied by the short

Theorem 2(i) achieves using such a partition is (cid:80)
sequences can be bounded by r·O (nγ lg σ) = O(cid:0)nδ+γ(cid:1) = o(n) bits. In turn, the space occupied by
the long sequences can be bounded by(cid:80)

i |si|H0(si) + (H0(si) + 1) · O(cid:16)
(cid:17)·|si|H0(si)+ d |si|

lg lg lg n bits, for some constants

1 + c

lg lg lg n

i

(cid:16)

(cid:17)

|si|

lg lg lg |si|

10

(cid:16)

1 + c

(cid:17) · nHk(s) + dn
lower-order space term o(n) within O(cid:16)

lg lg lg n

.

c, d. An argument very similar to the one used by Ferragina et al. [22, Thm. 4.2] shows that these
lg lg lg n . Thus the space is nHk(s) + o(n)(Hk(s) + 1). Other
add up to
structures required by the alphabet partitioning technique [22] add o(n) more bits if σk+1 ≤ nδ.

lg lg lg n

The claimed time complexities stem from the rank and access times on the partitions. The
partitioning scheme [22] adds just constant time overheads. Finally, to achieve the claimed locating
and extracting times we sample one out of every lg n lg lg lg n text positions. This maintains our
(cid:3)

(cid:17)

n

In case [1..σ] is not the eﬀective alphabet we proceed as described in Section 3.3. Our main
improvement compared to Theorem 4.2 of Barbay et al. [4] is that we have compressed the redundancy 
from o(n lg σ) to o(n)(Hk(s) + 1). Our improved locating times, instead, just owe to the
denser sampling, which Barbay et al. could also use.

achieve even better space, nHk(s) + o(n) bits, and time complexities O(cid:16)

Note that, by using the zero-order representation of Golynski et al. [29, Thm. 4], we could
instead of

(cid:17)

1 + lg σ
lg lg n

O (lg lg σ).6 Such complexities are convenient for not so large alphabets.

4.2 Positional inverted indexes

These indexes retrieve the positions of any word in a text. They may store the text compressed
up to the zero-order entropy of the word sequence s[1..n], which allows direct access to any word.
In addition they store the list of the positions where each distinct word occurs. These lists can be
compressed up to a second zero-order entropy space [48], so the overall space is at least 2nH0(s). By
regarding s as a sequence over an alphabet [1..ν] (corresponding here to the vocabulary), Theorem 2
represents s within nH0(s)+o(n)(H0(s)+1) bits, which provides state-of-the-art compression ratios.
Variant (ii) supports constant-time access to any text word s[i], and access to the jth entry of the
list of any word a (s.selecta(j)) in time O (lg lg ν). These two time complexities are exchanged in
variant (i), or both can be made constant by spending nH0(s) redundancy for any constant  > 0
(using Corollary 4). The length of the inverted lists can be stored within O (ν lg n) bits (we also
need at least this space to store the sequence content of each word identiﬁer).

Apart from supporting this basic access to the list of each word, this representation easily
supports operations that are more complex to implement on explicit inverted lists [5]. For example,
we can ﬁnd the phrases formed by two words w1 and w2, that appear n1 and n2 times, by ﬁnding the
occurrences of one and verifying the other in the text, in time O (min(n1, n2) lg lg ν). Other more
sophisticated intersection algorithms [5] can be implemented by supporting operations such as “ﬁnd
the position in the list of w2 that follows the jth occurrence of word w1” (s.rankw2(s.selectw1(j))+1,
in time O (lg lg ν)) or “give the list of word w restricted to the range [x..y] in the collection”
(s.selectw(s.rankw(x − 1) + j), for j ≥ 1, until exceeding y, in time O (lg lg ν) plus O (1) per
retrieved occurrence). In Section 8.4 we evaluate this representation in practice.

6One can retain lg lg n in the denominator by using block sizes depending on n and not on |si|, as explained in

the footnote at the beginning of Section 3.2.

11

4.3 Binary relations and non-positional inverted indexes
Let R ⊆ L × O, where L = [1..λ] are called labels and O = [1..κ] are called objects, be a binary
relation consisting of n pairs. Barbay et al. [3] represent the relation as follows. Let li1 < li2 <
. . . < lik be the labels related to an object o ∈ O. Then we deﬁne sequence so = li1li2 . . . lik . The
representation for R is the concatenated sequence s = s1 · s2 · . . . · sκ, of length n, and the bitmap
b = 10|s1|10|s2| . . . 10|sκ|1, of length n + κ + 1.

This representation allows one to eﬃciently support various queries [3]:

table access : is l related to o?, s.rankl(b.rank0(b.select1(o + 1))) > s.rankl(b.rank0(b.select1(o)));

object select : the ith label related to an object o, s.access(b.rank0(b.select1(o) + i));
object nb : the number of labels an object o is related to, b.select1(o + 1) − b.select1(o) − 1;

object rank : the number of labels < l an object o is related to, carried out with a predecessor
search in s[b.rank0(b.select1(o))..b.rank0(b.select1(o + 1))], an area of length O (λ). The predecessor 
data structure requires o(n) bits as it is built over values sampled every lg2 λ positions,
and the query is completed with a binary search;

label select : the ith object related to a label l, b.rank1(b.select0(s.selectl(i)));

label nb : the number of objects a label l is related to, s.rankl(n).

It can also be solved like

object nb, using a bitmap similar to b that traverses the table label-wise;

label rank : the number of objects < o a label l is related to, s.rankl(b.rank0(b.select1(o))).

Bitmap b can be represented within O(cid:0)κ lg n

(cid:1) = o(n)+O (κ) bits and support all the operations

in constant time [54], and its label-wise variant needs o(n) + O (λ) bits. The rest of the space and
time complexities depend on how we represent s.

κ

Barbay et al. [3] used Golynski et al.’s representation for s [28], so they achieved n lg λ+o(n lg λ)
bits of space, and the times at rows 2 or 3 in Table 1 for the operations on s (later, Barbay et
al. [4] achieved nHk(s) + o(n lg λ) bits and slightly worse times). By instead representing s using
Theorem 2, we achieve compressed redundancy and slightly improve the times.
To summarize, we achieve nH0(s) + o(n)(H0(s) + 1) + O (κ + λ) bits, and solve label nb
and object nb in constant time, and table access and label rank in time O (lg lg λ). For
respectively, or O (lg lg λ), O (1) and O (lg lg λ), respectively. Corollary 4 yields a slightly larger
representation with improved times, and a multiary wavelet tree [29, Thm. 4] achieves less space
and diﬀerent times; we leave the details to the reader.

label select, object select and object rank we achieve times O (1), O (lg lg λ) and O(cid:0)(lg lg λ)2(cid:1),

between consecutive values, achieving overall space O(cid:16)(cid:80)
documents [60]. In our representation as a binary relation, it turns out that H0(s) =(cid:80)

A non-positional inverted index is a binary relation that associates each vocabulary word with
the documents where it appears. A typical representation of the lists encodes the diﬀerences
, where word v appears in nv
v nv lg n
,
nv
and thus the space achieved is comparable to the classical schemes. Within this space, however,
the representation oﬀers various interesting operations apart from accessing the ith element of a
list (using label select), including support for various list intersection algorithms; see Barbay et
al. [3, 4] for more details.

v nv lg n
nv

(cid:17)

12

5 Compressing permutations

n lg n
ni

(cid:80) ni

Barbay and Navarro [6] measured the compressibility of a permutation π in terms of the entropy
of the distribution of the lengths of runs of diﬀerent kinds. Let π be covered by ρ runs (using any
of the previous deﬁnitions of runs [40, 6, 44]) of lengths runs(π) = (cid:104)n1, . . . , nρ(cid:105). Then H(runs(π)) =
≤ lg ρ is called the entropy of the runs (and, because ni ≥ 1, it also holds nH(runs(π)) ≥
(ρ − 1) lg n). In their most recent variant [7] they were able to store π in 2nH(runs(π)) + o(n) +
O (ρ lg n) bits for runs consisting of interleaved sequences of increasing or decreasing values, and
nH(runs(π)) + o(n) +O (ρ lg n) bits for contiguous sequences of increasing or decreasing values (or,
alternatively, interleaved sequences of consecutive values). In all cases they can compute π() and
if the queries are uniformly

time, which on average drops to O(cid:16)

π−1() in O(cid:16) lg ρ

(cid:17)

(cid:17)

lg lg n

1 +

H(runs(π))

lg lg n

distributed in [1..n].
We now show how to use access/rank/select data structures to support the operations more eﬃciently 
while retaining compressed redundancy space. In general terms, we exchange their O (ρ lg n)
space term by o(n)H(runs(π)), and improve their times to O (lg lg ρ) in the worst case, and to
O (lg H(runs(π))) on average (again, this is an improvement only if ρ is not too small).

We ﬁrst consider interleaved sequences of increasing or decreasing values as ﬁrst deﬁned by
Levcopoulos and Petersson [40] for adaptive sorting, and later on for compression [6], and then give
improved results for more restricted classes of runs. In both cases we ﬁrst consider the application
of the permutation π() and its inverse, π−1(), and later show how to extend the support to the
iterated application of the permutation, πk(), extending and improving previous results [47].

Theorem 6 Let π be a permutation on n elements that consists of ρ interleaved increasing or
decreasing runs, of lengths runs(π). Suppose we have a data structure that stores a sequence s[1..n]
over eﬀective alphabet [1..ρ] within ψ(n, ρ,H0(s)) bits, supporting queries access, rank, and select
in time τ (n, σ). Then, given its run decomposition, we can store π in 2ψ(n, ρ,H(runs(π))) + ρ bits,
and perform π() and π−1() queries in time O (τ (n, σ)).

Proof: We ﬁrst replace all the elements of the rth run by r, for 1 ≤ r ≤ ρ. Let s be the resulting
sequence and let s(cid:48) be s permuted according to π, that is, s(cid:48)[π(i)] = s[i]. We store s and s(cid:48) using
the given sequence representation, and also store ρ bits indicating whether each run is increasing
or decreasing. Note that H0(s) = H0(s(cid:48)) = H(runs(π)), which gives the claimed space.
Therefore, if π(i) is part of an increasing run, then s(cid:48).ranks[i](π(i)) = s.ranks[i](i), so

Notice that an increasing run preserves the relative order of the elements of a subsequence.

π(i) = s(cid:48).selects[i]

(cid:0)s.ranks[i](i)(cid:1) .

If, instead, π(i) is part of a decreasing run, then s(cid:48).ranks[i](π(i)) = s.ranks[i](n) + 1− s.ranks[i](i), so

π(i) = s(cid:48).selects[i]

(cid:0)s.ranks[i](n) + 1 − s.ranks[i](i)(cid:1) .

A π−1() query is symmetric (exchange s and s(cid:48) in the formulas). Therefore we compute π() and
π−1 with O (1) calls to access, rank, and select on s or s(cid:48).
(cid:3)

13

Example 3 Let π = 1, 8 , 9, 3, 6 , 10, 5, 4 , 11, 7, 2 , 12 be formed by three runs (indicated by the
diﬀerent fonts). Then s = (1, 2, 3, 1, 2, 3, 1, 2, 3) and s(cid:48) = (1, 2, 1, 2, 1, 2, 1, 2, 3, 3, 3, 3).

By combining Theorem 6 with the representations in Theorem 2, we obtain a result that improves 
upon previous work [6, 7] in time complexity. Note that if the queried positions i are
uniformly distributed in [1..n], then all the access, rank, and select queries follow the same character 
distribution of the runs, and Corollary 3 applies. Note also that the ρ bits are contained in
o(n)H(runs(π)) because nH(runs(π)) ≥ (ρ − 1) lg n.

Corollary 7 Let π be a permutation on n elements that consists of ρ interleaved increasing or decreasing 
runs, of lengths runs(π). Then, given its run decomposition, we can store π in 2nH(runs(π))+
o(n)(H(runs(π)) + 1) bits and perform π() and π−1() queries in O (lg lg ρ) time. On uniformly distributed 
queries the average times are O (lg H(runs(π))).

The case where the runs are contiguous is handled within around half the space, as a simpliﬁcation 
of Theorem 6.

Corollary 8 Let π be a permutation on n elements that consists of ρ contiguous increasing or
decreasing runs, of lengths runs(π). Suppose we have a data structure that stores a sequence
s[1..n] over eﬀective alphabet [1..ρ] within ψ(n, ρ,H0(s)) bits, supporting queries access and rank in
time τar(n, ρ), and select in time τs(n, σ). Then, given its run decomposition, we can store π in
ψ(n, ρ,H(runs(π)) + ρ lg n
ρ +O (ρ) + o(n) bits of space, and perform π() queries in time O (τs(n, ρ))
and π−1() queries in time O (τar(n, ρ)).
Proof: We proceed as in Theorem 6, yet now sequence s is of the form s = 1n12n2 . . . ρnρ, and therefore 
it can be represented as a bitmap b = 10n1−110n2−1 . . . 10nρ−11. The required operations are
implemented as follows: s.access(i) = b.rank1(i), s.ranks[i](i) = i − b.select1(s[i]) + 1, s.ranks[i](n) =
b.select1(s[i] + 1)− b.select1(s[i]), and s.selecta(i) = b.select1(a) + i− 1. Those operations are solved
in constant time using a representation for b that takes (ρ + 1) lg(e(n + 1)/(ρ + 1)) + o(n) bits [54].
Added to the ρ bits that mark increasing or decreasing sequences, this gives the claimed space. The
claimed time complexities correspond to the operations on s(cid:48), as those in s take constant time. (cid:3)

Once again, by combining the corollary with representation (i) in Theorem 2, we obtain results
ρ bits are in o(n)(H(runs(π))+1) because they are
that improve upon previous work [6, 7]. The ρ lg n
o(n) as long as ρ = o(n), and otherwise they are O (ρ) = o(ρ lg n), and (ρ − 1) lg n ≤ nH(runs(π)).

Corollary 9 Let π be a permutation on n elements that consists of ρ contiguous increasing or decreasing 
runs, of lengths runs(π). Then, given its run decomposition, we can store π in nH(runs(π))+
o(n)(H(runs(π))+1) bits and perform π() queries in time O (1) and π−1() queries in time O (lg lg ρ)
(and O (lg H(runs(π))) on average for uniformly distributed queries).

If π is formed by interleaved but strictly incrementing (+1) or decrementing (−1) runs, then
π−1 is formed by contiguous runs, in the same number and length [6]. This gives an immediate
consequence of Corollary 8.

14

Corollary 10 Let π be a permutation on n elements that consists of ρ interleaved strict increasing
or decreasing runs, of lengths runs(π). Suppose we have a data structure that stores a sequence
s[1..n] over eﬀective alphabet [1..ρ] within ψ(n, ρ,H0(s)) bits, supporting queries access and rank in
time τar(n, ρ), and select in time τs(n, σ). Then, given its run decomposition, we can store π in
ψ(n, ρ,H(runs(π)))+ρ lg n
ρ +O (ρ)+o(n) bits of space, and perform π() queries in time O (τar(n, ρ))
and π−1() queries in time O (τs(n, σ)).

For example we can achieve the same space of Corollary 9, yet with the times for π and
π−1 reversed. Finally, if we consider runs for π that are both contiguous and incrementing or
decrementing, then so are the runs of π−1. Corollary 8 can be further simpliﬁed as both s and s(cid:48)
can be represented with bitmaps.

Corollary 11 Let π be a permutation on n elements that consists of ρ contiguous and strict increasing 
or decreasing runs, of lengths runs(π). Then, given its run decomposition, we can store π
in 2ρ lg n

ρ + O (ρ) + o(n) bits, and perform π() and π−1() in O (1) time.

We now show how to achieve exponentiation, πk(i) or π−k(i), within compressed space. Munro
et al. [47] reduced the problem of supporting exponentiation on a permutation π to the support
of the direct and inverse application of another permutation, related but with quite distinct runs
than π. Combining it with any of our results does yield compression, but one where the space
depends on the lengths of both the runs and cycles of π. The following construction, extending the
technique by Munro et al. [47], retains the compressibility in terms of the runs of π, which is more
natural. It builds an index that uses small additional space to support the exponentiation, thus
allowing the compression of the main data structure with any of our results.

Theorem 12 Suppose we have a representation of a permutation π on n elements that supports
queries π() in time τ + and queries π−1() in time τ−. Then for any t ≤ n, we can build a data
structure that takes O ((n/t) lg n) bits and, used in conjunction with operation π() or π−1(), supports
πk() and π−k() queries in O (t min(τ +, τ−)) time.

Proof: The key to computing i(cid:48) = πk(i) is to discover that i is in a cycle of length (cid:96) and to
assign it a position 0 ≤ j < (cid:96) within its cycle (note j is arbitrary, yet we must operate consistently
once it is assigned). Then πk(i) lies in the same cycle, at position j(cid:48) = (j + k mod (cid:96)), hence
πk(i) = πj(cid:48)−j(i) or πj(cid:48)+(cid:96)−j(i). Thus all we need is to ﬁnd out j and (cid:96), compute j(cid:48), and ﬁnally ﬁnd
the position i(cid:48) in π that corresponds to the j(cid:48)th element of the cycle.

We decompose π into its cycles and, for every cycle of length at least t, store the cycle’s length
(cid:96) and an array containing the position i in π of every tth element in the cycle. Those positions i
are called ‘marked’. We also store a binary sequence b[1..n], so that b[i] = 1 iﬀ i is marked. For
each marked element i we record to which cycle i belongs and the position j of i in its cycle.
To compute πk(i), we repeatedly apply π() at most t times until we either loop or ﬁnd a marked
element. In the ﬁrst case, we have found (cid:96), so we can assume j = 0, compute j(cid:48) < (cid:96) ≤ t, and apply
π() at most t more times to ﬁnd i(cid:48) = πj(cid:48)
(i) = πk(i) in the loop. If we reach a marked element,
instead, we have stored the cycle identiﬁer to which i belongs, as well as j and (cid:96). Then we compute
j(cid:48) and know that the previous marked position is j∗ = t · (cid:98)j(cid:48)/t(cid:99). The corresponding position i∗ is
found at cell j∗/t of the array of positions of marked elements, and we ﬁnally move from i∗ to i(cid:48)

15

by applying j(cid:48) − j∗ ≤ t times operation π(), i = πj(cid:48)−j∗
(i∗) = πk(i). A π−k query is similar (note
that it does not need to use π−1() as we can always move forward). Moreover, we can also proceed
using π−1() instead of π(), whichever is faster, to compute both πk() and π−k().
The space is O ((n/t) lg n) both for the samples and for a compressed representation of bitmap
b. Note that we only compute rank at the positions i such that b[i] = 1. Thus we can use the ID
structure [54], which uses O ((n/t) lg t) bits.
(cid:3)

5.1 Application to self-indexes

These results on permutations apply to a second family of self-indexes, which is based on the
representation of the so-called Ψ function [35, 32, 56]. Given the suﬃx array A[1..n] of sequence
s[1..n] over alphabet [1..σ], Ψ is deﬁned as Ψ(i) = A−1[(A[i] mod n) + 1]. Counting, locating,
and extracting is carried out through permutation Ψ, which replaces s and A. It is known [35]
that Ψ contains σ contiguous increasing runs so that H(runs(Ψ)) = H0(s), which allows for its
compression. Grossi et al. [32] represented Ψ within nHk(s)+O (n) bits, while supporting operation
Ψ() in constant time, or within nHk(s) + o(n lg σ) while supporting Ψ() in time O (lg σ). By using
Corollary 9, we can achieve the unprecedented space nH0(s) + o(n)(H0(s) + 1) and support Ψ() in
constant time. In addition we can support the inverse Ψ−1() in time O (lg lg σ). Having both Ψ()
and Ψ−1() allows for bidirectional indexes [55], which can for example display a snippet around
any occurrence found without the need for any extra space for sampling. Our construction of
Theorem 12 can be applied on top of any of those representations so as to support operation
Ψk(), which is useful for example to implement compressed suﬃx trees, yet the particularities of Ψ
allow for sublogarithmic-time solutions [32]. Note also that using Huﬀman-shaped wavelet trees to
represent the permutation [7] yields even less space, nH0(s) + o(n) + O (σ lg n) bits, and the time
complexities are relevant for not so large alphabets.

6 Compressing functions
Hreinsson, Krøyer and Pagh [38] recently showed how, given a domain X = {x1, x2, . . . , xn} ⊂ N
of numbers that ﬁt in a machine word, they can represent any f : X → [1..σ] in compressed
form and provide constant-time evaluation. Let us identify function f with the sequence of values
f [1..n] = f (x1)f (x2) . . . f (xn). Then their representation uses at most (1 + )nH0(f ) +O (n) + o(σ)
bits, for any constant  > 0. We note that this bound holds even when σ is much larger than n.

In the special case where X = [1..n] and σ = o(n), we can achieve constant-time evaluation
and a better space bound using our sequence representations. Moreover, we can support extra
functionality such as computing the pre-image of an element. A ﬁrst simple result is obtained by
representing f as a sequence.

Lemma 13 Let f : [1..n] → [1..σ] be a function. We can represent f using nH0(f ) + o(n)(H0(f ) +
1) + O (σ) bits and compute f (i) for any i ∈ [1..n] in O (1) time, and any element of f−1(a) for
any a ∈ [1..σ] in time O (lg lg σ), or vice versa. Using more space, (1 + )H0(f ) + o(n) bits for any

16

constant  > 0, we support both queries in constant time. The size |f−1(a)| is always computed in
O (1) time.
Proof: We represent sequence f [1..n] using Theorem 2 or Corollary 4, so f (i) = f.access(i) and
the jth element of f−1(a) is f.selecta(j). To compute |f−1(a)| in constant time we store a binary
sequence b = 10|f−1(1)|10|f−1(2)|1 . . . 10|f−1(σ)|1, so that |f−1(a)| = b.select1(a + 1)− b.select1(a)− 1.
is o(n) if σ = o(n), and otherwise it is O (σ). This extra space is also necessary because [1..σ] may
(cid:3)
not be the eﬀective alphabet of sequence f [1..n] (if f is not surjective).

The space is the one needed to represent s plus O(cid:0)σ lg n

(cid:1) bits to represent b using an ID [54]. This

σ

Another source of compressibility frequently arising in real-life functions is nondecreasing or
nonincreasing runs. Let us start by allowing interleaved runs. Note that in this case H(runs(f )) ≤
H0(f ), where equality is achieved if we form runs of equal values only.

Theorem 14 Let f : [1..n] → [1..σ] be a function such that sequence f [1..n] consists of ρ interleaved
non-increasing or non-decreasing runs. Then, given its run decomposition, we can represent f in
2nH(runs(f )) + o(n)(H(runs(f )) + 1) + O (σ) bits and compute f (i) for any i ∈ [1..n], and any
element in f−1(a) for any a ∈ [1..σ], in time O (lg lg ρ). The size |f−1(a)| is computed in O (1)
time.

Proof: We store function f as a combination of the permutation π that stably sorts the values

f (i), plus the binary sequence b of Lemma 13. Therefore, it holds
f (i) = b.rank1(b.select0(π−1(i))).

Similarly, the jth element of f−1(a) is

Since π−1 has the same runs as f (the runs in f can have equal values but those of π−1 cannot),
we can represent π−1 using Corollary 7 to obtain the claimed time and space complexities.
(cid:3)

π(b.rank0(b.select1(a)) + j).

Example 4 Let f [1..9] = (1, 3, 2, 5, 4, 9, 8, 9, 8). The odd positions form an increasing run (1, 2, 4, 8, 8)
and the even positions form (3, 5, 9, 9). The permutation π sorting the values is (1, 3, 2, 5, 4, 7, 9, 6, 8),
and its inverse is π−1 = (1, 3, 2, 5, 4, 8, 6, 9, 7). The bitmap b is 101010101011100100.

If we consider only contiguous runs in f , we obtain the following result by representing π−1

with Corollary 9. Note the entropy of contiguous runs is no longer upper bounded by H0(f ).

Corollary 15 Let f : [1..n] → [1..σ] be a function, where sequence f consists of ρ contiguous
non-increasing or non-decreasing runs. Then, given its run decomposition, we can represent f in
nH(runs(f )) + o(n)(H(runs(f )) + 1) + O (σ) bits, and compute any f (i) in O (1) time, as well as
retrieve any element in f−1(a) in time O (lg lg ρ). The size |f−1(a)| can be computed in O (1) time.
In all the above results we can use Huﬀman-shaped wavelet trees [7] to obtain an alternative

space/time tradeoﬀ. We leave the details to the reader.

17

6.1 Application to binary relations, revisited

number oi of objects associated with a label i, let us call it Hlab = H0(s) =(cid:80) oi
of the number li of labels associated with an object i, let us call it Hobj = H(runs(s)) =(cid:80) li

Recall Section 4.3, where we represent a binary relation in terms of a sequence s and a bitmap
b. By instead representing s as a function, we can capture another source of compressibility, and
achieve slightly diﬀerent time complexities. Note that H0(s) corresponds to the distribution of the
. On the other
hand, if we regard the contiguous increasing runs of s, the entropy corresponds to the distribution
n lg n
.
While Section 4.3 compresses B in terms of Hlab = H0(s), we can use Corollary 15 to achieve
li
nHobj + o(n)(Hobj + 1) + O (κ + λ) bits of space. Since f.access(i) = f (i) and f.selecta(j) is
the jth element of f−1(a), this representation solves label nb, object nb and object select in
constant time, and label select and object rank in time O (lg lg λ). Operations label rank
and table access require f.rank, which is not directly supported. The former can be solved in
time O (lg lg λ lg lg κ) as a predecessor search in π (storing absolute samples every lg2 κ positions),
and the latter in time O (lg lg λ) as the diﬀerence between two object rank queries. We can also
achieve nHobj + o(n) + O (κ + λ) bits using Huﬀman-shaped wavelet trees; we leave the details to
the reader.

n lg n
oi

7 Compressing dynamic collections of disjoint sets

Finally, we now give what is, to the best of our knowledge, the ﬁrst result about storing a compressed
collection of disjoint sets while supporting operations union and ﬁnd [59]. The key point in the next
theorem is that, as the sets in the collection C are merged, our space bound shrinks with the zeroorder 
entropy of the distribution of the function s that assigns elements to sets in C. We deﬁne

≤ lg |C|, where ni are the sizes of the sets, which add up to n.

H(C) =(cid:80) ni

n lg n
ni

Theorem 16 Let C be a collection of disjoint sets whose union is [1..n]. For any  > 0, we can
store C in (1 + )nH(C) + O (|C| lg n) + o(n) bits and perform any sequence of r union and ﬁnd
operations in O (rα(n) + (1/)n lg lg n) total time, where α(n) is the inverse Ackermann’s function.

Proof: We ﬁrst use Theorem 2 to store the sequence s[1..n] in which s[i] is the representative
of the set containing i. We then store the representatives in a standard disjoint-set data structure
D [59]. Since H0(s) = H(C), our data structures take nH(C) + o(n)(H(C) + 1) + O (|C| lg n) bits.
We can perform a query ﬁnd(i) on C by performing D.ﬁnd(s[i]), and perform a union(i, j) operation
on C by performing D.union(D.ﬁnd(s[i]), D.ﬁnd(s[j])).

As we only need access functionality on s, we use a simple variant of Theorem 2. We support
only rank and select on the multiary wavelet tree that represents sequence t, and store the s(cid:96)
subsequences as plain arrays. The mapping m is of length |C|, so it can easily be represented in
plain form to support constant-time operations, within O (|C| lg n) bits. This yields constant time
access, and therefore the cost of the r union and ﬁnd operations is O (rα(n)) [59].
For our data structure to shrink as we merge sets, we keep track of H(C) and, whenever it shrinks
by a factor of 1 + , we rebuild our entire data structure on the updated values s[i] ← ﬁnd(s[i]).
First, note that all those ﬁnd operations take O (n) time because of path-compression [59]: Only
the ﬁrst time one accesses a node v ∈ D it may occur that the representative is not directly v’s

18

parent. Thus the overall time can be split into O (n) time for the n instructions ﬁnd(s[i]) plus O (n)
for the n times a node v ∈ D is visited for the ﬁrst time.
Reconstructing the structure of Theorem 2 also takes O (n) time. The plain structures for
m and s(cid:96) are easily built in linear time, and so is the multiary wavelet tree supporting rank and
access [22], as it requires just tables of sampled counters.
Thus the overall cost of rebuilding is O ((1/)n lg lg n). This completes our time complexity.

Since H(C) is always less than lg n, we rebuild only O(cid:0)lg1+ lg n(cid:1) = O ((1/) lg lg n) times.

Finally, the space term o(n)H(C) is absorbed by H(C) by slightly adjusting , and this gives
(cid:3)

our ﬁnal space formula.

8 Experimental results

In this section we explore the performance of our structure in practice. We ﬁrst introduce, in
Section 8.1, a simpler and more practical alphabet partitioning scheme we call “dense”, which
experimentally performs better than the one we describe in Section 3, but on which we could not
prove useful space bounds. Next, in Section 8.2 we study the performance of both alphabet partitioning 
methods, as well as the optimal one [58], in terms of compression ratio and decompression
performance. Given the results of these experiments, we continue only with our dense partitioning
for the rest of the section.

In Section 8.3 we compare our new sequence representation with the state of the art, considering
the tradeoﬀ between space and time of operations rank, select, and access. Then, Sections 8.4, 8.5,
and 8.6 compare the same data structures on diﬀerent real-life applications of sequence representations.
 In the ﬁrst, the operations are used to emulate an inverted index on the compressed sequence
using (almost) no extra space. In the second, they are used to emulate self-indexes for text [48]. In
the third, they provide access to direct and reverse neighbors on graphs represented with adjacency
lists.
The machine used for the experiments has an Intel R(cid:13) Xeon R(cid:13) E5620 at 2.40GHz, 94GB of RAM.
We did not use multithreading in our implementations; times are measured using only one thread,
and in RAM. The operating system is Ubuntu 10.04, with kernel 2.6.32-33-server.x86 64. The code
was compiled using GNU/GCC version 4.4.3 with optimization ﬂags -O9.

Our code is available in Libcds version 1.0.10, downloadable from http://libcds.recoded.cl/.

8.1 Dense alphabet partitioning

Said [58] proved that an optimal assignment to sub-alphabets must group consecutive symbols once
sorted by frequency. A simple alternative to the partitioning scheme presented in Section 3, and
that follows this optimality principle, is to make mapping m group elements into consecutive chunks
of doubling size, that is, m[a] = (cid:98)lg r(a)(cid:99), where r(a) is the rank of a according to its frequency.
The rest of the scheme to deﬁne t[1..n] and the sequences s(cid:96)[1..σ(cid:96)] is as in Section 3. The classes are
in the range 0 ≤ (cid:96) ≤ (cid:98)lg σ(cid:99), and each element in s(cid:96) is encoded in (cid:96) bits. As we use all the available
bits of each symbol in sequences s(cid:96) (except possibly in the last one), we call this scheme dense.

19

(cid:80)
(cid:96) (cid:96)|s(cid:96)| = (cid:80)
frequency, it holds that r(a) ≤ n/|s|a and (cid:80)|s|a(cid:98)lg r(a)(cid:99) ≤ (cid:80)|s|a lg(n/|s|a) = nH0(s). Now
be lower bounded by nH0(t). Thus nH0(t) ≤ 2(cid:80)

We show that this scheme is not much worse than the one proposed in Section 3 (which will
be called sparse). First consider the total number of bits we use to encode the sequences s(cid:96),
a |s|a(cid:98)lg r(a)(cid:99). Since |s|a ≤ n/r(a) because the symbols are sorted by decreasing
consider the number of bits we use to encode t = (cid:98)lg r(s[1])(cid:99), . . . ,(cid:98)lg r(s[n]))(cid:99). We could store each
element (cid:98)lg r(s[i])(cid:99) of t in 2(cid:98)lg((cid:98)lg r(s[i])(cid:99) + 1)(cid:99)− 1 bits using γ-codes [60], and such encoding would
a |s|a lg lg(n/|s|a + 1) =
O(n(lg H0(s) + 1) = o(nH0(s)) + O(n) (recall Section 3.2). It follows that the total encoding length
is nH0(s) + O (n lg H0(s)) = nH0(s) + o(nH0(s)) + O (n) bits.

i lg lg(r(s[i]) + 1) ≤ 2(cid:80)

Apart from the pretty tight upper bound, it is not evident whether this scheme is more or less
eﬃcient than the sparse encoding. Certainly the dense scheme uses the least possible number of
classes (which could allow storing t in plain form using lg lg σ bits per symbol). On the other hand,
the sparse method uses in general more classes, which allows for smaller sub-alphabets using fewer
bits per symbol in sequences s(cid:96). As we will see in Section 8.2, the dense scheme uses less space
than the sparse one for t, but more for the sequences s(cid:96).

Example 5 Consider the same sequence s = "alabar a la alabarda" of Ex. 1. The dense partitioning 
will assign m[a] = 0, m[l] = m[(cid:48) (cid:48)] = 1, m[b] = m[r] = m[d] = 2. So the sequence of
sub-alphabet identiﬁers is t[1..20] = (0, 1, 0, 2, 0, 2, 1, 0, 1, 1, 0, 1, 0, 1, 0, 2, 0, 2, 2, 0), and the subsequences 
are s0 = (1, 1, 1, 1, 1, 1, 1, 1, 1), s1 = (2, 1, 1, 2, 1, 2), and s2 = (1, 3, 1, 3, 2).
This dense scheme uses 16 bits for the sequences s(cid:96), and the zero-order compressed t requires
nH0(t) = 30.79 bits. The overall compression is 2.34 bits per symbol. The sparse partitioning of
Ex. 1 used 10 bits in the sequences s(cid:96), and the zero-order compressed t required nH0(t) = 34.40
bits. The total gives 2.22 bits per symbol. In our real applications, the dense partitioning performs
better.

Note that the question of space optimality is elusive in this scenario. Since the encoding in t
plus that in the corresponding sequence s(cid:96) forms a unique code per symbol, the optimum is reached
when we choose one sub-alphabet per symbol, so that the sequences s(cid:96) require zero bits and all the
space is in nH0(t) = nH0(s). The alphabet partitioning always gives away some space, in exchange
for faster decompression (or, in our case, faster rank/select/access operations).

Said’s optimal partitioning [58] takes care of this problem by using a parameter k that is the
maximum number of sub-alphabets to use. We sort the alphabet by decreasing frequency and
call S(c, k) the total number of bits required to encode the symbols [c..σ] of the alphabet using
a partitioning into at most k sub-alphabets.
In general, we can make a sub-alphabet with the
symbols [c..c(cid:48)] and solve optimally the rest, but if k = 1 we are forced to choose c(cid:48) = σ. When we
can choose, the optimization formula is as follows:

(cid:18)

(cid:19)

S(c, k) = min
c≤c(cid:48)≤σ

f lg

n
f

+ f(cid:100)lg(c(cid:48) − c + 1)(cid:101) + S(c(cid:48) + 1, k − 1)

,

where f is the total frequency of symbols c-th to c(cid:48)-th in s. The ﬁrst term of the sum accounts
for the increase in tH0(s), the second for the size in bits of the new sequence s(cid:96), and the third for
the smaller subproblem, where it also holds S(σ + 1, k) = 0 for any k. This dynamic programming

algorithm requires O (kσ) space and O(cid:0)σ2(cid:1) time. We call this partitioning method optimal.

20

Collection
Simple English
Spanish

Articles Total words (n) Distinct words (σ) Entropy (H0(s))
11.60
100,000
1,590,453
11.37

766,968,140
511,173,618

664,194
3,210,671

Table 2: Main characteristics of the datasets used.

Example 6 The optimal partitioning using 3 classes just like the dense approach in Ex. 5 leaves
’a’ in its own class, then groups ’l’, ’ ’, ’b’ and ’r’ in a second class, and ﬁnally leaves ’d’

alone in a third class. The overall space nH0(t) +(cid:80)|s(cid:96)|(cid:100)lg σ(cid:96)(cid:101) is 2.23 bits per symbol, less than

the 2.34 reached by the dense partitioning. If, instead, we let it use four classes, it gives the same
solution as the sparse method in Ex. 1.

Finally, in Section 3 we represent the sequences s(cid:96) with small alphabets σ(cid:96) using wavelet trees
(just like t) instead of using the representation of Golynski et al. [28], which is used for large
σ(cid:96) > lg n. In theory, this is because Golynski et al.’s representation does not ensure sublinearity
on smaller alphabets when used inside our scheme. While this may appear to be a theoretical
issue, the implementation of such data structure (e.g., in Libcds) is indeed unattractive for small
alphabets. For this reason, we also avoid using it on the chunks where σ(cid:96) is small (in our case, the
ﬁrst ones). Note that using a wavelet tree for t and then another for the symbols in a sequence s(cid:96)
is equivalent to replacing the wavelet tree leaf corresponding to (cid:96) in t by the whole wavelet tree
of s(cid:96). The space used by such an arrangement is worse than the one obtained by building, from
scratch, a wavelet tree for t where the symbols t[i] = (cid:96) are actually replaced by the corresponding
symbol s[i].

In our dense representation we use a parameter (cid:96)min that controls the minimum (cid:96) value that
is represented outside of t. All the symbols that would belong to s(cid:96), for (cid:96) < (cid:96)min, are represented
directly in t. Note that, by default, since σ0 = 1, we have (cid:96)min = 1.

8.2 Compression performance

For all the experiments in Section 8, except Section 8.6, we used real datasets extracted from
Wikipedia. We considered two large collections, Simple English and Spanish, dated from 06/06/2011
and 03/02/2010, respectively. Both are regarded as sequences of words, not characters. These
collections contain several versions of each article. Simple English, in addition, uses a reduced
vocabulary. We collected a sample of 100,000 versions at random from all the documents of Simple
English, which makes a long and repetitive sequence over a small alphabet. For the Spanish collection,
 which features a much richer vocabulary, we took the oldest version of each article, which
yields a sequence of similar size, but with a much larger alphabet.

We generated a single sequence containing the word identiﬁers of all the articles concatenated,
obtained after stemming the collections using Porter for English and Snowball for Spanish. Table
2 shows some basic characteristics of the sequences obtained7.

We measured the compression ratio achieved by the three partitioning schemes, dense, sparse,
and optimal. For dense we did not include any individual symbols (other than the most frequent)
in sequence t, i.e., we let (cid:96)min = 1. For optimal we allow 1 + (cid:98)lg σ(cid:99) sub-alphabets, just like dense.

7The code for generating these sequences is available at https://github.com/fclaude/txtinvlists.

21

Figure 1: Space versus decompression time for basic and alphabet-partitioned schemes. The vertical
line marks the zero-order entropy of the sequences. AC can slightly break the entropy barrier on
Simple English because it is adaptive.

In all cases, the symbols in each s(cid:96) are represented using (cid:100)lg σ(cid:96)(cid:101) bits. The sequence of classes t,
instead, is represented in three diﬀerent forms: Plain uses a ﬁxed number of bits per symbol, (cid:100)lg (cid:96)(cid:101)
where (cid:96) is the maximum class; Huff uses Huﬀman coding of the symbols8, and AC uses Arithmetic
coding of the symbols9. The former encodings are faster, whereas the latter use less space.
In
addition we consider compressing the original sequences using Huﬀman (Huffman) and Arithmetic
coding (Arith).

As explained, the main interest in using alphabet partitioning in a compressor is to speed up
decompression without sacriﬁcing too much space. Figure 1 compares all these alternatives in
terms of space usage (percentage of the original sequence) and decompression time per symbol. It
can be seen that alphabet partitioning combined with AC compression of t wastes almost no space
due to the partitioning, and speeds up considerably the decompression of the bare AC compression.
However, bare Huffman also uses the same size and decompresses several times faster. Therefore,
alphabet partitioning combined with AC compression is not really interesting. The other extreme is
the combination with a Plain encoding of t. In the best combinations, this alphabet partitioning
wastes close to 10% of space, and in exchange decompresses around 30% faster than bare Huffman.
The intermediate combination, Huff, wastes less than 1% of space, while improving decompression
time by almost 25% over bare Huffman.

Another interesting comparison is that of partitioning methods.

In all cases, variant dense
performs better than sparse. The diﬀerence is larger when combined with Plain, where sparse
is penalized for the larger alphabet size of t, but still there is a small diﬀerence when combined
with AC, which shows that t has also (slightly) lower entropy in variant dense. Table 3 gives a
breakdown of the bits per symbol in t versus the sequences s(cid:96) in all the methods. It can be seen

8We use G. Navarro’s Huﬀman implementation; the code is available in Libcds.
9We use the code by J. Carpinelli, A. Moﬀat, R. Neal, W. Salamonsen, L. Stuiver, A. Turpin and I. Witten, available
at http://ww2.cs.mu.oz.au/∼alistair/arith coder/arith coder-3.tar.gz. We modiﬁed the decompressor to
read the whole stream before timing decompression.

22

l1112131415050100150200250300Decompression time − Simple EnglishBits per symbolTime in secslllStructurePlain−densePlain−sparsePlain−optimalHuff−denseHuff−sparseHuff−optimalAC−denseAC−sparseAC−optimalACHuffl1112131415050100150200250Decompression time − SpanishBits per symbolTime in secslllStructurePlain−densePlain−sparsePlain−optimalHuff−denseHuff−sparseHuff−optimalAC−denseAC−sparseAC−optimalACHuffCombination

Plain-dense
Plain-sparse
Plain-optimal
Huff-dense
Huff-sparse
Huff-optimal
AC-dense
AC-sparse
AC-optimal

Simple English
%
t
6.67
19.01
10.21
6.67
19.01
10.21
6.67
19.01
10.21

s(cid:96)
7.74
3.79
7.66
7.74
3.79
7.66
7.74
3.79
7.66

4.96
9.97
4.96
3.99
8.22
4.08
3.95
8.18
4.03

Spanish

s(cid:96)
7.72
4.08
7.61
7.72
4.08
7.61
7.72
4.08
7.61

%

11.70
32.91
16.88
11.70
32.91
16.88
11.70
32.91
16.88

t

4.76
9.79
4.76
4.13
8.01
4.24
4.10
7.99
4.20

Table 3: Breakdown, in bits per symbol, of the space used in sequence t versus the space used
in all the sequences s(cid:96), for the diﬀerent combinations. The third column in each collection is the
percentage of symbols that lie in sequences s(cid:96) with alphabet sizes σ(cid:96) = 1.

that sparse leaves much more information on sequence t than the alternatives, which makes it less
appealing since the operation of t is slower than that of the other sequences. However, this can
be counterweighted by the fact that sparse produces many more sequences with alphabet size 1,
which need no time for accessing. It is also conﬁrmed that dense leaves slightly less information on
t than optimal, and that the diﬀerence in space between the three alternatives is almost negligible
(unless we use Plain to encode t, which is not interesting).

Finally, let us consider how the partitioning method aﬀects decompression time, given an encoding 
method for t. For method AC, sparse is signiﬁcantly slower. This is explained by the t
component having many more bits, and the decompression time being dominated by the processing
of t by the (very slow) arithmetic decoder. For method Plain, instead, sparse is slightly faster,
despite the fact that it uses more space. Since now the reads on t and s(cid:96) take about the same time,
this diﬀerence is attributable to the fact that sparse leaves more symbols on sequences s(cid:96) with
alphabets of size 1, where only one read in t is needed to decode the symbol (see Table 3). For
Huff all the times are very similar, and very close to the fastest one. Therefore, for the rest of the
experiments we use the variant Huff with dense partitioning, which performs best in space/time.

8.3 Rank, select and access

We now consider the eﬃciency in the support for the operations rank, select, and access. We
compare our sequence representation with the state of the art, as implemented in Libcds v1.0.10,
a library of highly optimized implementations of compact data structures. As said, Libcds already
includes the implementation of our new structure.

We compare six data structures for representing sequences. Those based on wavelet trees are
obtained in Libcds by combining sequence representations (WaveletTreeNoptrs, WaveletTree)
with bitmap representations (BitSequenceRG, BitSequenceRRR) for the data on wavelet tree nodes.

• WTNPRG: Wavelet tree without pointers, obtained as WaveletTreeNoptrs+BitSequenceRG in
Libcds. This corresponds to the basic balanced wavelet tree structure [32], where all the

23

bitmaps of a level are concatenated [42]. The bitmaps are represented in plain form and their
operations are implemented using a one-level directory [30] (where rank is implemented in
time proportional to a sampling step and select uses a binary search on rank). The space is
n lg σ + o(n lg σ) and the times are O (lg σ). In practice the absence of pointers yields a larger
number of operations to navigate in the wavelet tree, and also select operation on bitmaps is
much costlier than rank. A space/time tradeoﬀ is obtained by varying the sampling step of
the bitmap rank directories.

• WTNPRRR: Wavelet tree without pointers with bitmap compression, obtained in Libcds as
WaveletTreeNoptrs+BitSequenceRRR. This is similar to WTNPRG, but the bitmaps are represented 
in compressed form using the FID technique [54] (select is also implemented with
binary search on rank). The space is nH0(s) + o(n lg σ) and the times are O (lg σ). In practice
the FID representation makes it considerably slower than the version with plain bitmaps, yet
select operation is less aﬀected. A space/time tradeoﬀ is obtained by varying the sampling
step of the bitmap rank directories.

• GMR: The representation proposed by Golysnki et al. [28], named SequenceGMR in Libcds.
The space is n lg σ + o(n lg σ), yet the lower-order term is sublinear on σ, not n. The time
is O (1) for select and O (lg lg σ) for rank and access, although on average rank is constanttime.
 A space/time tradeoﬀ, which in practice aﬀects only the time for access, is obtained by
varying the permutation sampling inside the chunks [28].

• WTRG: Wavelet tree with pointers and Huﬀman shape, obtained as WaveletTree+BitSequenceRG

in Libcds. The space is nH0(s) + O (n) + o(nH0(s)) + O (σ lg n). The time is O (lg σ), but
in our experiments it will be O (H0(s)) for access, since the positions are chosen at random
from the sequence and then we navigate less frequently to deeper Huﬀman leaves.

• WTRRR: Wavelet tree with pointers, obtained with WaveletTree+ BitSequenceRRR in Libcds.
The space is nH0(s) + o(nH0(s)) +O (σ lg n). The time is as in the previous structure, except
that in practice the FID representation is considerably slower.

• AP: Our new alphabet partitioned structure, named SequenceAlphPart in Libcds. We use
dense partitioning and include the 210 most frequent symbols directly in t, (cid:96)min = 10. Sequence
t is represented with a WTRG (since its alphabet is small and the pointers pose no signiﬁcant
overhead), and the sequences σ(cid:96) are represented with structures GMR. The space is nH0(s) +
o(nH0(s)), although the lower-order term is actually sublinear on σ (and only very slightly
on n). The times are as in GMR, although there is a small additive overhead due to the wavelet
tree on t. A space/time tradeoﬀ is obtained with the permutations sampling, just as in GMR.

Figure 2 shows the results obtained for both text collections, giving the average over 100,000
measures. The rank queries were generated by choosing a symbol from [1..σ] and a position from
[1..n], both uniformly at random. For select we chose the symbol a in the same way, and the other
argument uniformly at random in [1..|s|a]. Finally, for access we generated the position uniformly
at random in [1..n]. Note that the latter choice favors Huﬀman-shaped wavelet trees, on which we
descend to leaf a with probability |s|a/n, whereas for rank and select we descend to any leaf with
the same probability.
Let us ﬁrst analyze the case of Simple English, where the alphabet is smaller. Since σ is 1000
times smaller than n, the O (σ lg n) terms of Huﬀman-shaped wavelet trees are not signiﬁcant, and

24

Figure 2: Time for the three operations. The x axis starts at the entropy of the sequence.

25

lll152025024681012Rank Queries − Simple EnglishBits per symbolTime in microsecslStructureWTNPRGWTNPRRRGMRWTRGWTRRRAPlll15202530051015Rank Queries − SpanishBits per symbolTime in microsecslStructureWTNPRGWTNPRRRGMRWTRGWTRRRAPlll15202501020304050Select Queries − Simple EnglishBits per symbolTime in microsecslStructureWTNPRGWTNPRRRGMRWTRGWTRRRAPlll1520253001020304050Select Queries − SpanishBits per symbolTime in microsecslStructureWTNPRGWTNPRRRGMRWTRGWTRRRAPlll152025051015Access Queries − Simple EnglishBits per symbolTime in microsecslStructureWTNPRGWTNPRRRGMRWTRGWTRRRAPlll15202530051015Access Queries − SpanishBits per symbolTime in microsecslStructureWTNPRGWTNPRRRGMRWTRGWTRRRAPas a result the variant WTRRR reaches the least space, essentially nH0(s) + o(nH0(s)). It is followed
by three variants that use similar space: WTRG (which has an additional O (n)-bit overhead), AP
(whose o(nH0(s)) space term is higher than that of wavelet trees), and WTNPRRR (whose sublinear
space term is of the form o(n lg σ), that is, uncompressed). The remaining structures, WTNPRG and
GMR, are not compressed and use much more space.

In terms of time, structure AP is faster than all the others except GMR (which in exchange uses
much more space). The exception is on access queries, where as explained Huﬀman-shaped wavelet
trees, WTRG and WTRRR, are favored and reach the same performance of AP. In general, the rule is that
variants using plain bitmaps are faster than those using FID compression, and that variants using
pointers and Huﬀman shape are faster than those without pointers (as the latter need additional
operations to navigate the tree). These diﬀerences are smaller on select queries, where the binary
searches dominate most of the time spent.
The Spanish collection has a much larger alphabet: σ is only 100 times smaller than n. This
impacts on the O (σ lg n) bits used by the pointer-based wavelet trees, and as a result the space
of AP, nH0(s) + o(nH0(s)), is unparalleled. Variants WTRRR and WTRG use signiﬁcantly more space
and are followed, far away, by WTNPRRR, which has uncompressed redundancy. The uncompressed
variants WTNPRG and RG use signiﬁcantly more space. The times are basically as on Simple English.
This second collection illustrates more clearly that, for large alphabets, our structure AP sharply
dominates the whole space/time tradeoﬀ. It is only slightly slower than GMR in some cases, but in
exchange it uses half the space. From the wavelet trees, the most competitive alternative is WTRG,
but it always loses to AP. The situation is not too diﬀerent on smaller alphabets (as in Simple
English), except that variant WTRRR uses clearly less space, yet at the expense of doubling the
operation times of AP.

8.4 Intersecting inverted lists

An interesting application of rank/select operations on large alphabets was proposed by Clarke
et al. [14], and recently implemented by Arroyuelo et al. [1] using wavelet trees. The idea is to
represent the text collections as a sequence of word tokens (as done for Simple English and Spanish),
use a compressed and rank/select/access-capable sequence representation for them, and use those
operations to emulate an inverted index on the collection, without spending any extra space on
storing explicit inverted lists.
More precisely, given a collection of d documents T1, T2, . . . , Td, we concatenate them in C =
T1T2 . . . Td, and build an auxiliary bitmap b[1..|C|] where we mark the beginning of each document
with a 1. We can provide access to the text of any document in the collection via access operations
on sequence C (and select on b). In order to emulate the inverted list of a given term w, we just
need to list all the distinct documents where w occurs. This is achieved by iterating on procedure
nextDoc(C, w, p) of Algorithm 1 (called initially with p = 0 and then using the last p value returned).
Algorithm 1 also allows one to test whether a given document contains a term or not (p contains
w iﬀ p = nextDoc(C, w, p− 1)). Using this primitive we implemented Algorithm 2, which intersects
several lists (i.e., returns the documents where all the given terms appear) based on the algorithm
by Demaine et al. [18]. We tested this algorithm for both Simple English and Spanish collections,
searching for phrases extracted at random from the collection. We considered phrases of lengths
2 to 16. We averaged the results over 1,000 queries. As all the results were quite similar, we only
show the cases of 2 and 6 words. We tested the same structures as in Section 8.3.

26

input : C, w, p
output: next document after Tp that contains w
pos ← b.select1(p + 1);
cnt ← C.rankw(pos − 1);
return b.rank1(C.selectw(cnt + 1))
Algorithm 1: Function nextDoc(C, w, p), retrieves the next document after p containing w.
The x axis starts at the entropy of the sequence.

input : C, W = w1, w2, . . . , wk
output: documents that contain w1, . . . , wk
sort W by increasing number of occurrences in the collection;
res ← ∅;
p ← nextDoc (C, w1, 0);
while p is valid do

if w2, . . . , wk are contained in p (i.e., p = nextDoc (C, wj, p − 1) for 2 ≤ j ≤ k) then

Add p to res;
p ← nextDoc (C, w1, p)

end
else

Let wj be the ﬁrst word not contained in p;
p ← nextDoc (C, w1, nextDoc (C, wj, p − 1))

end

end
return res

Algorithm 2: Retrieving the documents where all w1, . . . , wk appear.

Figure 3 shows the results obtained by the diﬀerent structures. For space, of course, the results
are as before: AP is the best on Spanish and is outperformed by WTRRR on Simple English. With
respect to time, we observe that Huﬀman-shaped wavelet trees are favored compared to the random
rank and select queries of Section 8.3. The reason is that the queries in this application, at least in
the way we have generated them, do not distribute uniformly at random: the symbols for rank and
select are chosen according to their probability in the text, which favors Huﬀman-shaped trees. As
a result, structures WTRG perform similarly to AP in time, whereas WTRRR is less than twice as slow.

8.5 Self-indexes

A second application of the sequence operations on large alphabets was explored by Fari˜na et
al. [19]. The idea is to take a self-index [48] designed for text composed of characters, and apply
it to a word-tokenized text, in order to carry out word-level searches on natural language texts.
This requires less space and time than the character-based indexes and competes successfully with
word-addressing inverted indexes. One of the variants they explore is to build an FM-index [21, 22]
on words [15]. The FM-index represents the Burrows-Wheeler transform (BWT) [12] sbwt of s.
Using rank and access operations on sbwt the FM-index can, among other operations, count the
number of occurrences of a pattern p[1..k] (in our case, a phrase of k words) in s[1..n]. This requires

27

Figure 3: Results for intersection queries. The x axis starts at the entropy of the sequence.

O(k) applications of rank and access on sbwt. A self-index is also able to retrieve any passage of
the original sequence s.

We implemented the word-based FM-index with the same structures measured so far, plus a
new variant called APRRR. This is a version of AP where the bitmaps of the wavelet tree of t are
represented using FIDs [54]. The reason is that it was proved [41] that the wavelet tree of sbwt, if
the bitmaps are represented using Raman et al.’s FID [54], achieves space nHk(s) + o(n lg σ). Since
the wavelet tree t of sub-alphabets of sbwt is a coarsened version of that of sbwt, we expect it to
take advantage of Raman et al’s representation.

We extracted phrases at random text positions, of lengths 2 to 16, and counted their number
of occurrences using the FM-index. We averaged the results over 100,000 searches. As the results
are similar for all lengths, we show the results for lengths 2 and 8. Figure 4 shows the time/space
tradeoﬀ obtained.

Conﬁrming the theoretical results [41], the versions using compressed bitmaps require much less
space than the other alternatives. In particular, APRRR uses much less space than AP, especially on

28

lll1520251000200030004000Intersecting 2 lists − Simple EnglishBits per symbolTime in millisecslStructureWTNPRGWTNPRRRGMRWTRGWTRRRAPlll15202530100020003000400050006000Intersecting 2 lists − SpanishBits per symbolTime in millisecslStructureWTNPRGWTNPRRRGMRWTRGWTRRRAPlll152025200400600800Intersecting 6 lists − Simple EnglishBits per symbolTime in millisecslStructureWTNPRGWTNPRRRGMRWTRGWTRRRAPlll1520253050010001500Intersecting 6 lists − SpanishBits per symbolTime in millisecslStructureWTNPRGWTNPRRRGMRWTRGWTRRRAPFigure 4: Time for counting queries on word-based FM-indexes. The vertical line marks the zeroorder 
entropy of the sequences; remember that some schemes achieve high-order entropy spaces.

Simple English. In this text the least space is reached by WTRRR. On Spanish, instead, the O (σ lg n)
bits of Huﬀman-shaped wavelet trees become relevant and the least space is achieved by APRRR,
closely followed by AP and WTNPRRR. The space/time tradeoﬀ is dominated by APRRR and AP, the
two variants of our structure.

8.6 Navigating graphs

Finally, our last application scenario is the compact representation of graphs. Let G = (V, E) be a
directed graph. If we concatenate the adjacency lists of the nodes, the result is a sequence s[1..|E|]
over an alphabet of size |V |. If we add a bitmap b[1..|E|] that marks with a 1 the beginning of the
lists, it is very easy to retrieve the adjacency list of any node v ∈ V , that is, its neighbors, with one
select operation on b followed by one access operation on s per neighbor retrieved.10

10Note that this works well as long as each node points to at least one node. We solve this problem by keeping an

additional bitmap marking the nodes whose list is not empty.

29

lll05101520255101520Counting patterns 2 − Simple EnglishBits per symbolTime in microsecslStructureWTNPRGWTNPRRRGMRWTRGWTRRRAPAPRRRlll051015202530510152025Counting patterns 2 − SpanishBits per symbolTime in microsecslStructureWTNPRGWTNPRRRGMRWTRGWTRRRAPAPRRRlll0510152025406080100120140Counting patterns 8 − Simple EnglishBits per symbolTime in microsecslStructureWTNPRGWTNPRRRGMRWTRGWTRRRAPAPRRRlll051015202530406080100120140Counting patterns 8 − SpanishBits per symbolTime in microsecslStructureWTNPRGWTNPRRRGMRWTRGWTRRRAPAPRRRName
EU (EU-2005)
In (Indochina-2002)

Nodes
862,664
7,414,866

Edges

19,235,140
194,109,311

Plain adj. list (bits per edge)
20.81
23.73

Table 4: Description of the Web crawls considered.

It is not hard to reach this space with a classical graph representation. However, classical
representations do not allow one to retrieve eﬃciently the reverse neighbors of v, that is, the nodes
that point to it. The classical solution is to double the space to represent the transposed graph. Our
sequence representation, however, allows us to retrieve the reverse neighbors using select operations
on s, much as Algorithm 1 retrieves the documents where a term w appears: our “documents” are
the adjacency lists of the nodes, and the document identiﬁer is the node v ∈ V that points to
the desired node. Similarly, it is possible to determine whether a given node v points to a given
node v(cid:48), which is not an easy operation with classical adjacency lists. This idea has not only
been used in this simple form [15], but also in more sophisticated scenarios where it was combined
with grammar compression of the adjacency lists, or with other transformations, to compress Web
graphs and social networks [17, 16, 37].

For this experiment we used two crawls obtained from the well-known WebGraph project11. The
main characteristics of these crawls are shown in Table 4. Note that the alphabets are comparatively
much larger than on documents, just around 22–26 times smaller than the sequence length.

Figure 5 shows the results obtained. The nodes are sorted alphabetically by URL. A well-known
property of Web graphs [10] is that nodes tend to point to other nodes of the same domain. This
property turns into substrings of nearby symbols in the sequence, and this turns into runs of 0s or
1s in the bitmaps of the wavelet trees. This makes variants like WTNPRRR very competitive in space,
whereas APRRR does not beneﬁt so much. The reason is that the partitioning into classes reorders
the symbols, and the property is lost. Note that variant WTRRR does not perform well in space,
since the number of nodes is too large for a pointer-based tree to be advantageous. For the same
reason, even WTRG uses more space than GMR12. Overall, we note that our variants largely dominate
the space/time tradeoﬀ, except that WTNPRRR uses less space (but much more time).

9 Conclusions and future work

We have presented the ﬁrst zero-order compressed representation of sequences supporting queries
access, rank, and select in loglogarithmic time, so that the redundancy of the compressed representation 
is also compressed. That is, our space for sequence s[1..n] over alphabet [1..σ] is
nH0(s) + o(n)(H0(s) + 1) instead of the usual nH0(s) + o(n lg σ) bits. This is very important in
many practical applications where the data is so highly compressible that a redundancy of o(n lg σ)
bits would dominate the overall space. While there exist representations using even nH0(s) + o(n)
bits, ours is the ﬁrst one supporting the operations in time O (lg lg σ) while breaking the o(n lg σ)

11http://law.dsi.unimi.it
12Note that WTRRR is almost 50% larger than WTRG. This is because the former is a more complex structure and
requires a larger (constant) number of pointers to be represented. Multiplying by the σ nodes of the Huﬀman-shaped
wavelet tree makes a signiﬁcant diﬀerence when the alphabet is so large.

30

Figure 5: Performance on Web graphs, to retrieve direct and reverse neighbors. The vertical line
marks the bits per edge required by a plain adjacency list representation.

redundancy barrier. Moreover, our time complexities are adaptive to the compressibility of the sequence,
 reaching average times O (lg H0(s)) under reasonable assumptions. We have given various
byproducts of the result, where the compressed-redundancy property carries over representations of
text indexes, permutations, functions, binary relations, and so on. It is likely that still other data
structures can beneﬁt from our compressed-redundancy representation. Finally, we have shown
experimentally that our representation is highly practical, on large alphabets, both in synthetic
and real-life application scenarios.

On the other hand, various interesting challenges on sequence representations remain open:
1. Use nHk(s) + o(n)(Hk(s) + 1) bits of space, rather than nHk(s) + o(n lg σ) [4, 33] or our
nH0(s) + o(n)(H0(s) + 1) bits, while still supporting the queries access, rank, and select
eﬃciently.

2. Remove the o(nH0(s)) term from the redundancy while retaining loglogarithmic query times.

31

lll020406080050100150Direct neighbors time − EUSpace in bits per edgeTime in microsec/querylStructureWTNPRGWTNPRRRGMRWTRGWTRRRAPAPRRRlll020406080050100150Direct neighbors time − IndochinaSpace in bits per edgeTime in microsec/querylStructureWTNPRGWTNPRRRGMRWTRGWTRRRAPAPRRRlll0204060800100200300400Reverse neighbors time − EUSpace in bits per edgeTime in microsec/querylStructureWTNPRGWTNPRRRGMRWTRGWTRRRAPAPRRRlll0204060800200400600800Reverse neighbors time − IndochinaSpace in bits per edgeTime in microsec/querylStructureWTNPRGWTNPRRRGMRWTRGWTRRRAPAPRRRGolynski et al. [29] have achieved nH0(s) + o(n) bits of space, but the time complexities are

exponentially higher on large alphabets, O(cid:16)
quences. Our o(n) redundancy is indeed O(cid:16)
obtained O(cid:16) n

(cid:17)

lgc n

3. Lower the o(n) redundancy term, which may be not negligible on highly compressible se-
,
is more attractive, at least for small alphabets. Moreover, for the binary case, P˘atra¸scu [52]
for any constant c, and this is likely to carry over multiary wavelet trees.

. That of Golynski et al. [29], o

lg lg lg n

lg n

n

1 + lg σ
lg lg n

.

(cid:17)

(cid:17)

(cid:16) n lg σ

(cid:17)

After the publication of the conference version of this paper, Belazzougui and Navarro [8]
achieved a diﬀerent tradeoﬀ for one of our byproducts (Theorem 5). By spending O (n) further
bits, they completely removed the terms dependent on σ in all time complexities, achieving O (m),
O (lg n) and O (r − l + lg n) times for counting, locating and extracting, respectively. Their technique 
is based in monotone minimum perfect hash functions (mmphfs), which can also be used to
improve some of our results on permutations, for example obtaining constant time for query π(i)
in Theorem 6 and thus improving all the derived results13.

a RAM machine of word size w, which holds for any space of the form O(cid:0)nwO(1)(cid:1), and achieved

This is just one example of how lively current research is on this fundamental problem. Another
example is the large amount of recent work attempting to close the gap between lower and upper
bounds when taking into account compression, time and redundancy [27, 25, 29, 52, 34, 53, 26, 33].
Very recently, Belazzougui and Navarro [9] proved a lower bound of Ω(lg lg σ
lg w ) for operation rank on
this time within O (n lg σ) bits of space. Then, making use of the results we present in this paper,
they reduced the space to nH0(s) + o(nH0(s)) + o(n) bits. This is just one example of how our
technique can be easily used to move from linear-space to compressed-redundancy-space sequence
representations.

Acknowledgments. We thank Djamal Belazzougui for helpful comments on a draft of this paper,
and Meg Gagie for righting our grammar.

References

[1] D. Arroyuelo, S. Gonz´alez, and M. Oyarz´un. Compressed self-indices supporting conjunctive
queries on document collections. In Proc. 17th International Symposium on String Processing
and Information Retrieval (SPIRE), pages 43–54, 2010.

[2] J. Barbay, F. Claude, and G. Navarro. Compact rich-functional binary relation representations.
In Proc. 9th Latin American Symposium on Theoretical Informatics (LATIN), LNCS 6034,
pages 170–183, 2010.

[3] J. Barbay, A. Golynski, J. I. Munro, and S. S. Rao. Adaptive searching in succinctly encoded
binary relations and tree-structured documents. Theoretical Computer Science, 387(3):284–
297, 2007.

13Djamal Belazzougui, personal communication.

32

[4] J. Barbay, M. He, J. I. Munro, and S. S. Rao. Succinct indexes for strings, binary relations
and multi-labeled trees. In Proc. 18th Annual ACM-SIAM Symposium on Discrete Algorithms
(SODA), pages 680–689, 2007.

[5] J. Barbay, A. L´opez-Ortiz, T. Lu, and A. Salinger. An experimental investigation of set intersection 
algorithms for text searching. ACM Journal of Experimental Algorithmics, 14(3):article
7, 2009.

[6] J. Barbay and G. Navarro. Compressed representations of permutations, and applications. In
Proc. 26th Symposium on Theoretical Aspects of Computer Science (STACS), pages 111–122,
2009.

[7] J. Barbay and G. Navarro. On compressing permutations and adaptive sorting. CoRR,

1108.4408v1, 2011.

[8] D. Belazzougui and G. Navarro. Alphabet-independent compressed text indexing. In Proc.

19th Annual European Symposium on Algorithms (ESA), LNCS 6942, pages 748–759, 2011.

[9] D. Belazzougui and G. Navarro. New lower and upper bounds for representing sequences.

CoRR, 1111.2621, 2011.

[10] P. Boldi and S. Vigna. The WebGraph framework I: compression techniques. In Proc. 13th

World Wide Web Conference (WWW), pages 595–602, 2004.

[11] N. Brisaboa, M. Luaces, G. Navarro, and D. Seco. A new point access method based on
wavelet trees. In Proc. 3rd International Workshop on Semantic and Conceptual Issues in GIS
(SeCoGIS), LNCS 5833, pages 297–306, 2009.

[12] M. Burrows and D. Wheeler. A block sorting lossless data compression algorithm. Technical

Report 124, Digital Equipment Corporation, 1994.

[13] D. Clark. Compact Pat Trees. PhD thesis, University of Waterloo, Canada, 1996.

[14] C. Clarke, G. Cormack, and E. Tudhope. Relevance ranking for one to three term queries.
In Proc. 5th International Conference on Computer-Assisted Information Retrieval (RIAO),
pages 388–401, 1997.

[15] F. Claude and G. Navarro. Practical rank/select queries over arbitrary sequences. In Proc.
15th International Symposium on String Processing and Information Retrieval (SPIRE), pages
176–187, 2008.

[16] F. Claude and G. Navarro. Extended compact web graph representations.

In T. Elomaa,
H. Mannila, and P. Orponen, editors, Algorithms and Applications (Ukkonen Festschrift),
LNCS 6060, pages 77–91. Springer, 2010.

[17] F. Claude and G. Navarro. Fast and compact web graph representations. ACM Transactions

on the Web (TWEB), 4(4):article 16, 2010.

[18] E. Demaine, A. L´opez-Ortiz, and J.I. Munro. Adaptive set intersections, unions, and diﬀerences.
 In Proc. 11th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages
743–752, 2000.

33

[19] A. Fari˜na, N. Brisaboa, G. Navarro, F. Claude, A. Places, and E. Rodr´ıguez. Word-based
self-indexes for natural language text. ACM Transactions on Information Systems (TOIS),
30(1), 2012. To appear.

[20] P. Ferragina, F. Luccio, G. Manzini, and S. Muthukrishnan. Compressing and indexing labeled

trees, with applications. Journal of the ACM, 57(1), 2009.

[21] P. Ferragina and G. Manzini. Indexing compressed texts. Journal of the ACM, 52(4):552–581,

2005.

[22] P. Ferragina, G. Manzini, V. M¨akinen, and G. Navarro. Compressed representations of sequences 
and full-text indexes. ACM Transactions on Algorithms, 3(2), 2007.

[23] P. Ferragina and R. Venturini. A simple storage scheme for strings achieving entropy bounds.

Theoretical Computer Science, 372(1):115–121, 2007.

[24] T. Gagie and Y. Nekrich. Worst-case optimal adaptive preﬁx coding. In Proc. 11th International 
Symposium on Algorithms and Data Structures (WADS), LNCS 5664, pages 315–326,
2009.

[25] A. Golynski. Optimal lower bounds for rank and select indexes. Theoretical Computer Science,

387(3):348–359, 2007.

[26] A. Golynski. Cell probe lower bounds for succinct data structures.

In Proc. 20th Annual

ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 625–634, 2009.

[27] A. Golynski, R. Grossi, A. Gupta, R. Raman, and S. Srinivasa Rao. On the size of succinct
indices. In Proc. 15th Annual European Symposium on Algorithms (ESA), LNCS 4698, pages
371–382, 2007.

[28] A. Golynski, J. I. Munro, and S. S. Rao. Rank/select operations on large alphabets: a tool for
text indexing. In Proc. 17th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA),
pages 368–373, 2006.

[29] A. Golynski, R. Raman, and S. Rao. On the redundancy of succinct data structures. In Proc.
11th Scandinavian Workshop on Algorithm Theory (SWAT), LNCS 5124, pages 148–159, 2008.

[30] R. Gonz´alez, Sz. Grabowski, V. M¨akinen, and G. Navarro. Practical implementation of rank
and select queries. In Proc. 4th Workshop on Eﬃcient and Experimental Algorithms (WEA),
pages 27–38, 2005. Posters.

[31] R. Gonz´alez and G. Navarro. Statistical encoding of succinct data structures. In Proc. 17th

Annual Symposium on Combinatorial Pattern Matching (CPM), pages 294–305, 2006.

[32] R. Grossi, A. Gupta, and J. Vitter. High-order entropy-compressed text indexes. In Proc. 14th

Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 841–850, 2003.

[33] R. Grossi, A. Orlandi, and R. Raman. Optimal trade-oﬀs for succinct string indexes. In Proc.
37th International Colloquim on Automata, Languages and Programming (ICALP), pages 678–
689, 2010.

34

[34] R. Grossi, A. Orlandi, R. Raman, and S. Srinivasa Rao. More haste, less waste: Lowering the
redundancy in fully indexable dictionaries. In Proc. 26th Symposium on Theoretical Aspects
of Computer Science (STACS), pages 517–528, 2009.

[35] R. Grossi and J. Vitter. Compressed suﬃx arrays and suﬃx trees with applications to text

indexing and string matching. SIAM Journal on Computing, 35(2):378–407, 2006.

[36] B. Haskel, A. Puri, and A. Netravali. Digital Video: an Introduction to MPEG-2. Chapman

& Hall, 1997.

[37] C. Hern´andez and G. Navarro. Compression of web and social graphs supporting neighbor
and community queries. In Proc. 5th ACM Workshop on Social Network Mining and Analysis
(SNA-KDD). ACM, 2011.

[38] J. B. Hreinsson, M. Krøyer, and R. Pagh. Storing a compressed function with constant time

access. In Proc. 17th European Symposium on Algorithms (ESA), pages 730–741, 2009.

[39] D. Huﬀman. A method for the construction of minimum-redundancy codes. Proceedings of

the I.R.E., 40(9):1090–1101, 1952.

[40] C. Levcopoulos and O. Petersson. Sorting shuﬄed monotone sequences.

Information and

Computation, 112(1):37–50, 1994.

[41] V. M¨akinen and G. Navarro.

Implicit compression boosting with applications to selfindexing.
 In Proc. 14th International Symposium on String Processing and Information Retrieval 
(SPIRE), LNCS 4726, pages 214–226, 2007.

[42] V. M¨akinen and G. Navarro. Rank and select revisited and extended. Theoretical Computer

Science, 387(3):332–347, 2007.

[43] G. Manzini. An analysis of the Burrows-Wheeler transform. Journal of the ACM, 48(3):407–

430, 2001.

[44] K. Mehlhorn. Sorting presorted ﬁles. In Proc. 4th GI-Conference on Theoretical Computer

Science, LNCS 67, pages 199–212, 1979.

[45] A. Moﬀat and A. Turpin. On the implementation of minimum-redundancy preﬁx codes. IEEE

Transactions on Communications, 45(10):1200–1207, 1997.

[46] I. Munro. Tables.

In Proc. 16th Conference on Foundations of Software Technology and

Theoretical Computer Science (FSTTCS), LNCS v. 1180, pages 37–42, 1996.

[47] J. I. Munro, R. Raman, V. Raman, and S. S. Rao. Succinct representations of permutations.
In Proc. 30th International Colloquium on Algorithms, Languages and Programming (ICALP),
pages 345–356, 2003.

[48] G. Navarro and V. M¨akinen. Compressed full-text indexes. ACM Computing Surveys, 39(1):article 
2, 2007.

[49] D. Okanohara and K. Sadakane. Practical entropy-compressed rank/select dictionary. In Proc.

10th Workshop on Algorithm Engineering and Experiments (ALENEX), pages 60–70, 2007.

35

[50] W. Pearlman, A. Islam, N. Nagaraj, and A. Said. Eﬃcient, low-complexity image coding with
IEEE Transactions on Circuits and Systems for

a set-partitioning embedded block coder.
Video Technology, 14(11):1219–1235, 2004.

[51] W. Pennebaker and J. Mitchell. JPEG: Still Image Data Compression Standard. Von Nostrand

Reinhold, 1992.

[52] M. P˘atra¸scu. Succincter. In Proc. 49th Annual IEEE Symposium on Foundations of Computer

Science (FOCS), pages 305–313, 2008.

[53] M. P˘atra¸scu. A lower bound for succinct rank queries. CoRR, abs/0907.1103, 2009.

[54] R. Raman, V. Raman, and S. Rao. Succinct indexable dictionaries with applications to encoding 
k-ary trees and multisets. In Proc. 13th Annual ACM-SIAM Symposium on Discrete
Algorithms (SODA), pages 233–242, 2002.

[55] L. Russo, G. Navarro, A. Oliveira, and P. Morales. Approximate string matching with compressed 
indexes. Algorithms, 2(3):1105–1136, 2009.

[56] K. Sadakane. New text indexing functionalities of the compressed suﬃx arrays. Journal of

Algorithms, 48(2):294–313, 2003.

[57] K. Sadakane and R. Grossi. Squeezing succinct data structures into entropy bounds. In Proc.
17th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1230–1239, 2006.

[58] A. Said. Eﬃcient alphabet partitioning algorithms for low-complexity entropy coding. In Proc.

15th Data Compression Conference (DCC), pages 193–202, 2005.

[59] R. E. Tarjan and J. van Leeuwen. Worst-case analysis of set union algorithms. Journal of the

ACM, 31(2):245–281, 1984.

[60] I. Witten, A. Moﬀat, and T. Bell. Managing Gigabytes. Morgan Kaufmann Publishers, 2nd

edition, 1999.

36

