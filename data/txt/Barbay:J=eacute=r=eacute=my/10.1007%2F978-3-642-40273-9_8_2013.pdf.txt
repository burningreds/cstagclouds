From Time to Space:

Fast Algorithms That Yield

Small and Fast Data Structures

Jérémy Barbay

Departamento de Ciencias de la Computación (DCC),

Universidad de Chile, Santiago, Chile

jbarbay@dcc.uchile.cl

Abstract. In many cases, the relation between encoding space and execution 
time translates into combinatorial lower bounds on the computational 
complexity of algorithms in the comparison or external memory
models. We describe a few cases which illustrate this relation in a distinct 
direction, where fast algorithms inspire compressed encodings or
data structures. In particular, we describe the relation between searching 
in an ordered array and encoding integers; merging sets and encoding
a sequence of symbols; and sorting and compressing permutations.

Keywords: Adaptive (Analysis of) Algorithms, Compressed Data
Structures, Permutation, Set Union, Sorting, Unbounded Search.

1 Introduction

The worst-case analysis of algorithmic constructs is the theoretical equivalent of
a grumpy and bitter fellow who always predicts the worst outcome possible for
any actions you might take, however far-fetched the prediction.

Consider for instance the case of data compression. In a modern digital camera,
 one does not assume that each picture of width w and height h measured
in pixels of b bytes will require h × w × b bytes of storage space. Rather, the
design assumes that the pictures taken have some regularities (e.g. many pixels
of similar colors grouped together), which permit to encode it in less space. Only
a truly random set of pixels will require h × w × b bytes of space, and those are
usually not meaningful in the usage of a camera.

Similar to compression techniques, some opportunistic algorithms perform
faster than the worst case on some instances. As a simple example, consider the
sequential search algorithm on a sorted array of n elements, which performs n
comparisons in the worst case but much less in other cases. More sophisticated
search algorithms [9] uses O(log n) comparisons in the worst case, yet much less
in many particular cases. The study and comparison of the performance of such
algorithms requires ﬁner analysis techniques than the worst case among instances
of ﬁxed sizes. An analysis technique reinvented several times (under names such
as parameterized complexity, output-sensitive complexity, adaptive (analysis of)

A. Brodnik et al. (Eds.): Munro Festschrift, LNCS 8066, pp. 97–111, 2013.
c(cid:2) Springer-Verlag Berlin Heidelberg 2013

98

J. Barbay

algorithms, distribution-sensitive algorithms and others) is to study the worstcase 
performance among ﬁner classes of instances, deﬁned not only by a bound
on their size but also by bounds on some deﬁned measure of their diﬃculty.

This concept of opportunism, in common between encodings and algorithms,
has yielded some dual analysis, where any ﬁne-analysis of an encoding scheme
implies a similar analysis of an algorithm, and vice-versa. For instance, already
in 1976 Bentley and Yao [9] described adaptive algorithms inspired by Elias
codes [13] for searching in a (potentially unbounded) sorted array. More recently 
in 2009, Laber and Avila [1] showed that any comparison-based merging
algorithm can be adapted into a compression scheme for bit vectors. In particular,
 they discovered a relation between previous independent results on the
distinct problems of comparison-based merging and compression schemes for
bit vectors: the Binary Merging algorithm proposed by Hwang and Lin [24]
is closely related to a runlength-based coder using Rice coding [35], while
Recursive Merging is closely related to a runlength-based coder using the
Binary Interpolative Coder. The very same year, Barbay and Navarro [7],
observing that any comparison-based sorting algorithm yields an encoding for
permutations, exploited the similarity between the Wavelet Tree [22], a data
structure used for compression, and the execution of an adaptive variant of Merge
Sort, to develop and analyze various compressed data structures for permutations 
and adaptive sorting algorithms. In this case, the construction algorithm
of each data structure is in fact an adaptive algorithm sorting the permutation,
in a number of comparisons roughly the same as the number of bits used by the
data structure. Given those results, the following questions naturally arise:
1. All comparison-based adaptive sorting algorithms yield compressed encodings.
 One can wonder if all comparison-based adaptive sorting algorithm,
and not only those based on MergeSort, can inspire compressed data structures,
 and if a similar relationship exists for all comparison-based algorithms
on other problems than sorting. Which adaptive algorithms yield compressed 
data structures?

2. In the cases of permutations [7], techniques from data compression inspired
improvements on the design of algorithms and the analysis of their performance.
 Are there cases where such relations between compressed
data structures and adaptive algorithms are bijective?

3. Ignoring the relation between comparison-based merging algorithms and
compressed encodings of bit vectors led to the independent discovery of similar 
techniques [1]. Could a better understanding of such relations
simplify future research?

This survey aims to be a ﬁrst step toward answering those questions, by reviewing
some adaptive techniques (Section 2), some related data structures (Section 3),
and various relations between them in the comparison model, such as between
sorted search algorithms and encodings for sequences of integers (Section 4.1),
merging algorithms and string data structures (Section 4.2), and sorting algorithms 
and permutation data structures (Section 4.3). We conclude with a
selection of open problems in Section 5.

Fast Algorithms That Yield Small and Fast Data Structures

99

2 Adaptive Analysis

2.1 Sorted Search
A regular implementation of binary search returns the insertion rank r (deﬁned
as r ∈ [1..n] such that r = 1 and x ≤ A[1] or r > 1 and A[r − 1] < x ≤ A[r])
of an element x in a sorted array A of n elements in (cid:4)lg n(cid:5) + 1 ∈ O(log n)
comparisons1 in the worst case and in (cid:6)lg n(cid:7) + 1 ∈ O(log n) comparisons in
the best case. The performance of sequential search is much more variable: the
algorithm will perform min(r, n) + 1 comparisons (between 2 and n + 1), which
corresponds to a worst-case complexity of O(n) comparisons. An interesting (if
minor) fact is that, whenever the insertion rank of x is less than (cid:4)lg n(cid:5) + 1, linear
search outperforms binary search.

In 1976, inspired by Elias’ codes [13] for sequences of integers, Bentley and
Yao [9] described a family of algorithms for unbounded search in a (potentially
inﬁnite) sorted array. Among those, the doubling search algorithm [9] returns
the insertion rank after 1 + 2(cid:6)lg r(cid:7) comparisons. Hence, doubling search outpern,
 and never
forms binary search whenever the insertion rank of x is less than
performs worse than twice the complexity of the binary search. It is widely used
in practice, whereas its asymptotic worst-case complexity is the same as binary
search, both optimal in the comparison model: the traditional worst-case analysis
for a ﬁxed value of n fails to distinguish the performance of those algorithms.

√

2.2 Union of Sorted Sets
A problem where the output size can vary but is not a good measure of diﬃculty,
 is the description of the sorted union of sorted sets: given k sorted sets,
describe their union. On the one hand, the sorted union of A = {0, 1, 2, 3, 4}
and B = {5, 6, 7, 8, 9} is easier to describe (all values from A in their original
order, followed by all values from B, in their original order) than the union of
C = {0, 2, 4, 6, 8} and D = {1, 3, 5, 7, 9}. On the other hand, a deterministic
algorithm must ﬁnd this description, which can take much more time than to
output it when computing the union of many sets at once.

Carlsson et al. [11] deﬁned the adaptive algorithm Adaptmerge to compute
the union of two sorted sets in adaptive time. This can be used to compute the
union of k sets by merging them two by two, but Demaine et al. [12] proposed
an algorithm whose complexity depends on the minimal encoding size C of a
certiﬁcate, a set of comparisons required to check the correctness of the output
of the algorithm (yielding worst-case complexity Θ(C)) over instances of ﬁxed
size n and certiﬁcate-encoding-size C). An alternative approach is to consider the
non-deterministic complexity [6], the number of steps δ performed by a non deterministic 
algorithm to solve the instance, or equivalently the minimal number
of comparisons of a certiﬁcate (yielding worst-case complexity Θ(δk log(n/δk))
over instances of ﬁxed size n and certiﬁcate-comparison-size δ).
1 We note lg n= log2 n, lg(k)(n)= the logarithm iterated k times, and log n when the

base do not matter, such as in asymptotic notations.

100

J. Barbay

2.3 Sorting
Sorting an array A of numbers is a basic problem, where the size of the output
of an instance is always equal to its input size. Still, some instances are easier
than others to sort (e.g. a sorted array, which can be checked/sorted in linear
time). Instead of the output size, one can consider the disorder in an array as a
measure of the diﬃculty of sorting this array [10, 28].

There are many ways to measure this disorder: one can consider the number 
of exchanges required to sort an array; the number of adjacent exchanges
required; the number of pairs (i, j) such that A[i] > A[j], but there are many
others [33]. For each disorder measure, the logarithm of the number of instances
with a ﬁxed size and disorder forms a natural lower bound to the worst-case
complexity of any sorting algorithm in the comparison model, as a correct algorithm 
must at least be able to distinguish all instances. As a consequence, there
could be as many optimal algorithms as there are diﬃculty measures. Instead,
one can reduce diﬃculty measures between themselves, which yields a hierarchy
of disorder measures [29].

An algorithm is adaptive with respect to a given measure of presortedness M
if its running time is a function of both the measure M and the size n of the
input list X = (cid:9)x1, . . . , xn(cid:10), being linear for small values of M and at most
polylogarithmic in n for larger ones. Many measures have been deﬁned [29], we
list here only a few relevant ones:
– nSRuns, the number of Strict Runs, subsequences of consecutive positions in
the input with a gap between successive values exactly 1, from beginning to
end (e.g. (1, 2, 6, 7, 8, 9, 3, 4, 5) is composed of nSRuns = 3 strict runs);

– nRuns, the number of Runs, subsequences of consecutive positions in the
input with a positive gap between successive values, from beginning to end
(e.g. (1, 2, 6, 7, 8, 9, 3, 4, 5) is composed of nRuns = 2 runs);

– nSSUS, the number of Strict Shuﬄed Up Sequences, subsequences in the input
with a gap between successive values exactly 1, from beginning to end (e.g.
(1, 5, 2, 6, 3, 8, 4, 9, 7) is composed of nSSUS = 4 strict shuﬄed up sequences);
– nSUS, the number of Shuﬄed Up Sequences, subsequences in the input
with a positive gap between successive values, from beginning to end (e.g.
(1, 5, 2, 6, 3, 8, 4, 9, 7) is composed of nSUS = 2 shuﬄed up sequences);

– nSMS, the number of Shuﬄed Monotone Sequences, subsequences in the input
with a positive gap between successive values, from beginning to end or from
end to beginning (e.g. (1, 9, 2, 8, 3, 7, 4, 6, 5) is composed of nSMS = 2 shuﬄed
monotone sequences);
– nInv, the number nInv(X) = |{(i, j) : i < j ∧ xi > xj}| of inversions (i.e.,
pairs in wrong order) in the input (e.g. (2, 3, 4, 5, 6, 7, 8, 9, 1) has nInv = 8
inversions); and

– nRem, the minimum number of elements that must be removed from the input
in order to obtain a sorted subsequence (e.g. (2, 3, 4, 5, 6, 7, 8, 9, 1) needs only
nRem = 1 removal to be sorted).

Moﬀat and Petersson [33] proposed a framework to compare those measures of
presortedness, based on the cost function CM (|X|, k) representing the minimum

Fast Algorithms That Yield Small and Fast Data Structures

101

Reg

Hist

Loc

nSMS

Block

Osc

Enc

nRem

Inv≡DS

Exc ≡ Ham
Max≡Par

nSUS

nRuns

nSRuns

H(vSMS)

H(vSUS)

H(vLRM)

H(vRuns)

H(vSRuns)

Fig. 1. Partial order on some measures of disorder for adaptive sorting, completed
from Moﬀat and Petersson’s survey [29] in 1992. A measure A dominates a measure B
(A → B or A ⊇ B) if superior asymptotically. Round lined boxes signal the measures
for which a compressed data structure supporting π() and π−1() in sublinear time is
known [3, 4, 7], round dotted boxes signal the results that can be inferred, and bold
round boxes signal the additional compressed data structure presented in this article
(see Section 4.3).

is

superior

⊇ M2, M1
O(CM2 (|X|, M2(X)));

number of comparisons to sort a list X such that M (X) = k, where M is a
measure of presortedness. Given two measures of disorder M1 and M2:
– M1
– M1 ⊃ M2, M1 is strictly superior to M2 if M1 ⊇ M2 and M2 (cid:2) M1;
– M1 ≡ M2, M1 and M2 are equivalent if M1 ⊇ M2 and M2 ⊇ M1;
– M1 and M2 are independent if M1 (cid:3) M2 and M2 (cid:3) M1.
This organization yields a partial on those measures and the corresponding algorithms,
 such as the one given in Figure 1.

if CM1 (|X|, M1(X)) =

to M2

3 Encodings and Data Structures

3.1 Integers

In 1975, Elias introduced the concept of universal preﬁx-free codeword set [13],
such that given any countable set M of messages and any probability distribution
P on M, the codewords in order of increasing length yield a code set of average
cost within a constant factor of the source entropy, for any alphabet B of size
|B| ≥ 2. In the binary case (|B| = 2), he discussed the properties of existing
representation of integers such as unary (code α) and binary (code ˆβ), and
(cid:3), δ,
presented several new techniques of binary representations of integers (γ, γ
and ω), showing that the δ and ω codes are asymptotically optimal universal.

The γ code of an integer i is constructed by inserting after each of the bit
of the binary representation ˆβ(i) of i a single bit of the unary representation
α(| ˆβ(i)|) of the length | ˆβ(j)| of ˆβ(j). Dropping the ﬁrst bit (always equal to 1)
of ˆβ(i) yields a code of length 1+2(cid:6)lg i(cid:7) for any positive integer i > 0. Reordering
(i) = α(| ˆβ(i)|). ˆβ(i) with the same
the bits of γ(i) yields a variant of the code, γ

(cid:3)

102

J. Barbay

length which “is easier for people to read” [13] and is usually referred to as the
(cid:3) codes are universal but neither is asymptotically
“Gamma Code”. Both γ and γ
optimal, as the length of the codes stays at a factor of 2 of the optimal value of
(cid:6)lg i(cid:7) suggested by information theory. The δ and ω codes below reduce this.
The δ code of an integer i is constructed in a similar way to the γ code,
only encoding the length | ˆβ(i)| of the binary representation ˆβ(i) of i through
the γ code γ(| ˆβ(i)|) instead of the unary code α(| ˆβ(i)|), i.e. replacing α(| ˆβ(i)|)
by γ(| ˆβ(i)|). The δ code δ(i) of an integer i has length |δ(i)| = 1 + (cid:6)lg i(cid:7) +
2(cid:6)lg(1 + (cid:6)lg i(cid:7))(cid:7). This code is, this time, asymptotically optimal as the ratio
1 + 1+2(cid:4)lg(1+(cid:4)lg i(cid:5))(cid:5)
of its length to the information theory lower bound tends to
1 for large values of i. The same improvement techniques can be applied again
in order to improve the convergence rate of this ratio.

The ω code is constructed by concatenating several groups of bits, the rightmost 
group being the binary representation ˆβ(i) of i, and each other group being
the binary encoding ˆβ(l − 1) of the length l less one of the following group, the
process halting on the left with a group of length 2. Elias points out that there
is a code performing even better than the ω code, but only for very large integer
values (“much larger than Eddington’s estimate of the number of protons and
electrons in the universe”).

lg i

3.2 Sets and Bit Vectors
Jacobson introduced the concept of succinct data structures encoding Sets and
bit vectors [25] within space close to the information lower bounds while supporting 
eﬃciently some basic operators on it, as a constructing block for other
data-structures, such as tree structures and planar graphs. Given a bit vector
B[1, . . . , n] (potentially representing a set S ⊂ [1..n] such that α ∈ S if and
only if B[α] = 1), a bit α ∈ {0, 1}, a position x ∈ [1..n] = {1, . . . , n} and an
integer r ∈ {1, . . . , n}, the operator bin_rank(B, α, x) returns the number of
occurrences of α in B[1..x], and the operator bin_select(B, α, r) returns the
position of the r-th label α in B, or n if none.

lg n + O( n

An index of n lg lg n

log n ) additional bits [19] support those operators in
constant time in the Θ(log n)-word RAM model on a bit vector of length n. This
space is asymptotically negligible (i.e. it is within o(n)) compared to the n bits
required to encode the bit vector itself, in the worst case over all bit vectors of n
bits, and is optimal up to asymptotically negligible terms among the encodings
keeping the index separated from the encoding of the binary string [19]. As the
index is separated from the encoding of the binary string, the result holds even
(cid:3)(cid:5)
if the binary string has exactly v bits set to one and is compressed to (cid:4)lg
bits, as long as the encoding supports in constant time the access to a machine
word of the string [34].

(cid:2)
n
v

3.3 Permutations and Functions
Another basic building block for various data structures is the representation of a
permutation of the integers {1, . . . , n}, denoted by [1..n]. The basic operators on

Fast Algorithms That Yield Small and Fast Data Structures

103

a permutation are the image of a number i through the permutation, through its
inverse or through πk(), the k-th power of it (i.e. π() iteratively applied k times
starting at i, where k can be any integer so that π

−1() is the inverse of π()).

A straightforward array of n words encodes a permutation π and supports the
application of the operator π() in constant time. An additional index composed
of n/t shortcuts [18] cutting the largest cycles of π in loops of less than t elements
−1() in at most t word-accesses. Using such
supports the inverse permutation π
an encoding for the permutation mapping a permutation π to one of its cyclic
representations, one can also support the application of the operator πk(), the k
times iterated application of operator π(), in at most t word-accesses (i.e. in time
independent of k), with the same space constraints [31]. Those results extend
to functions on ﬁnite sets [31] by a simple combination with the tree encodings
from Jacobson [25].

3.4 Strings

Another basic abstract data type is the string, composed of n characters taken
from an alphabet of arbitrary size σ (as opposed to binary for the bit vector).
The basic operations on a string are to access it, and to search and count the
occurrences of a pattern, such as a simple character from [1..σ] in the simplest 
case [22]. Formally, for any character α ∈ [1..σ], position x ∈ [1..n] in the
string and positive integer r, those operations are performed via the operators
string_access(x), the x-th character of the string; string_rank(α, x), the
number of occurrences of α before position x; and string_select(α, r), the
position of the r-th α-occurrence.

(cid:2)
(cid:3)
lg σ + o(log σ)

Golynski et al. [21] showed how to encode a string of length n over the alphabet 
[1..σ] via n/σ permutations over [1..σ] and a few bit vectors of length n; and
how to support the string operators using the operators on those permutations.
Choosing a value of t = lg σ in the encoding of the permutation from Munro et
al. [31] yields an encoding using n
bits in order to support the
operators in at most O(lg lg σ) word accesses. Observing that the encoding of permutations 
already separates the data from the index, Barbay et al. [5] properly
separated the data and the index of strings, yielding a succinct index using the
same space and supporting the operators in O(lg lg σ lg lg lg σ(f (n, σ) + lg lg σ))
word accesses, if each word of the data can be accessed in f (n, σ) word accesses.
The space used by the resulting data-structures is optimal up to asymptotically
negligible terms, among all possible succinct indexes [20] of ﬁxed alphabet size.
A large body of work has further compressed strings to within entropy limits,
culminating (or close) with full independence on the alphabet size from both
the redundancy space of the compressed indexes and the time of support of the
operators [8].

104

J. Barbay

4 Fast Algorithms That Yield Compression Schemes

4.1 From Unbounded Search to Integer Compression

Bentley and Yao [9] mentioned that each comparison-based unbounded search
algorithm A implies a corresponding encoding for integers, by memorizing the bit
result of each comparison performed by A: simulating A’s execution with those
bits will yield the same position in the array, hence those bits code the position
of the searched element. Their search algorithms are clearly inspired from Elias’
codes [13] yet Bentley and Yao do not give explicitly the correspondence between
the codes generated by their unbounded search algorithms and the codes from
Elias. We remedy this in the following table:

Search Algorithm

binary

sequential

B1 [9]
B2 [9]
U [9]

cmps and bits

(cid:6)lg(cid:7)n + 1
(cid:6)p(cid:7) + 1
2(cid:6)lg p(cid:7) + 1

2(cid:6)lg lg p(cid:7) + (cid:6)lg p(cid:7) + 1
p(cid:7) + (cid:6)lg

(cid:6)lg(i) p(cid:7) + (cid:6)lg

p(cid:5)

∗

(cid:4)lg

Encoding Scheme

binary
unary
(cid:3) [13]
γ
δ [13]
ω [13]

∗

p(cid:7) + 1

(cid:4)

i

Each of those codes is readily extendable to a compressed data structure for
integers supporting algebraic operations such as the sum, diﬀerence, and product 
in time linear in the sum of the sizes of the compressed encodings of the
, δ and ω even support, by
operands and results. The most advanced codes γ, γ
construction, the extraction of the integer part of the logarithm in base two, in
time linear in the sum of the sizes of the compressed encodings of the operands.

(cid:3)

4.2 From Merging Algorithms to Set and String Compression

Consider k sorted arrays of integers A1, . . . , Ak. A k-Merging Algorithm computes 
the sorted union A = A1 ∪ . . . ∪ Ak of those k arrays, with no repetitions.
Ávila and Laber observed that any comparison-based merging algorithm
yields an encoding for strings [1]. The transformation is simple: given a string S
of n symbols from alphabet [1..k] and a comparison-based merging algorithm M,
deﬁne the k sorted arrays A1, . . . , Ak such that for each value i ∈ [1..k], Ai is the
set of positions in S of symbol i. Running algorithm M on input (A1, . . . , Ak)
yields the elements from [1..n] in sorted order. Let C be the bit string formed by
the sequence of results of each comparison performed by M. Since the execution
of M on (A1, . . . , Ak) can be simulated via C without any access to (A1, . . . , Ak),
the bit vector C encodes the string S, potentially in less than n(cid:4)lg k(cid:5) bits.

Many merging algorithms perform sublinearly in the size of the input on
particular instances: this means that some strings of n symbols from an alphabet 
[1..σ] of size σ are encoded in less than n(cid:4)lg σ(cid:5) bits, i.e. that the encoding
implied by the merging algorithm is compressing the string. Ávila and Laber focused 
on binary merging algorithms and the compression schemes they implied
for bit vectors and sets (i.e. binary sources). They observed that Hwang and

Fast Algorithms That Yield Small and Fast Data Structures

105

Lin’s binary merging algorithm [24] yields an encoding equivalent to using Rice
coding [35] in a runlength-based encoder of the string; and that the Recursive
Merging algorithm [2] yields an encoding equivalent to Moﬀat and Stuiver’s
Binary Interpolative coder [30]. Furthermore, they note that at least one
merging algorithm [15] yields a new encoding scheme (probabilistic in this case)
for bit vectors, which was not considered before!

Merging Algorithm
Hwang and Lin [24]

Recursive Merging [2]

Probabilistic Binary [15] (m+n) lg( m

cmps and bits
n lg(1 + m
n )
n lg(1 + m
n )

Source Encoding Scheme
Rice coding+runlength [35]
Binary Interpolative [30]
m+n )+0.2355 Randomized Rice Code [1]

4.3 From Sorting Algorithms to Permutations Data Structures

Barbay and Navarro [7] observed that each adaptive sorting algorithm in the
comparison model also describes an encoding of the permutation π that it sorts,
so that it can be used to compress permutations from speciﬁc classes to less than
the information-theoretic lower bound of lg(n!) ∈ n log n − n
2 + Θ(1)
bits. Furthermore they used the similarity of the execution of the Merge Sort
algorithm with a wavelet tree [22], to support the application of the operator π()
−1() in time logarithmic in the disorder of the permutation π (as
and its inverse π
measured by nRuns, nSRuns, nSUS, nSSUS or nSMS) in the worst case. We describe
below their results and some additional on additional preorder measures.

ln 2 + log(n)

H(vRuns)-Adaptive Sorting and Compression: The simplest way to partition 
a permutation into sorted chunks is to divide it into runs of consecutive
positions forming already sorted blocks, in n − 1 comparisons. For example, the
permutation (8 , 9 , 1, 4, 5, 6, 7, 2, 3) contains nRuns = 3 ascending runs, of lengths
forming the vector vRuns = (cid:9)2, 5, 2(cid:10).

Using a simple partition of the permutation into runs, merging those via
a wavelet tree sorts the permutation and yields a data structure compressing
a permutation to nH(vRuns) + O(nRuns log n) + o(n) bits in time O(n(1 +
H(vRuns))), which is worst-case optimal in the comparison model. Further-
−1() in sublinear time
more, this data structure supports the operators π() and π
O(1 + log nRuns), with the average supporting time O(1 +H(vRuns)) decreasing
with the entropy of the partition of the permutation into runs [7].

Strict-Runs-Adaptive Sorting and Compression: A two-level partition
of the permutation yields further compression [7]. The ﬁrst level partitions the
permutation into strict ascending runs (maximal ranges of positions satisfying
π(i + k) = π(i) + k). The second level partitions the heads (ﬁrst position) of
those strict runs into conventional ascending runs.
For example, the permutation π = (8, 9, 1, 4, 5, 6, 7, 2, 3) has nSRuns = 4 strict
runs of lengths forming the vector vSRuns = (cid:9)2, 1, 4, 2(cid:10). The run heads are

J. Barbay

106
(cid:9)8, 1, 4, 2(cid:10), which form 3 monotone runs, of lengths forming the vector vHRuns =
(cid:9)1, 2, 1(cid:10). The number of strict runs can be anywhere between nRuns and n: for instance 
the permutation (6 , 7 , 8 , 9 , 10 , 1, 2, 3, 4, 5) contains nSRuns = nRuns = 2
strict runs while the permutation (1 , 3 , 5 , 7 , 9 , 2, 4, 6, 8, 10) contains nSRuns =
10 strict runs, each of length 1, and 2 runs, each of length 5.

H(vSUS)-Adaptive Sorting and Compression: The preorder measures seen
so far have considered runs which group contiguous positions in π: this does not
need to be always the case. A permutation π over [1..n] can be decomposed in n
comparisons into a minimal number nSUS of Shuﬄed Up Sequences, deﬁned as a
set of, not necessarily consecutive, subsequences of increasing numbers that have
to be removed from π in order to reduce it to the empty sequence [26]. Then those
subsequences can be merged using the same techniques as above, which yields
a new adaptive sorting algorithm and a new compressed data structure [7]. For
example, the permutation (1 , 6, 2 , 7, 3 , 8, 4 , 9, 5 , 10) contains nSUS = 2 shuﬄed
up sequences of lengths forming the vector vSUS = (cid:9)5, 5(cid:10), but nRuns = 5 runs,
all of length 2.

Note that it is a bit more complicated to partition a permutation π over
[1..n] into a minimal number nSMS of Shuﬄed Monotone Sequences, sequences
of not necessarily consecutive subsequences of increasing or decreasing numbers:
an optimal partition is N P -hard to compute [27].

H(vLRM)-Adaptive Sorting and Compression: LRM-Trees partition a sequence 
of values into consecutive sorted blocks, and express the relative position
of the ﬁrst element of each block within a previous block. They were introduced
under this name as an internal tool for basic navigational operations in ordinal
trees [36] and, under the name “2d-Min Heaps”, to index integer arrays in order
to support range minimum queries on them [16]. Such a tree can be computed
in 2(n − 1) comparisons within the array and overall linear time, through an
algorithm similar to that of Cartesian Trees [17].

The interest of LRM trees in the context of adaptive sorting and permutation 
compression is that the values are increasing in each root-to-leaf branch:
they form a partition of the array into sub-sequences of increasing values. Barbay 
et al. [4] described how to compute the partition of the LRM-tree of minimal
size-vector entropy, which yields a sorting algorithm asymptotically better than
H(vRuns)-adaptive sorting, and better in practice than H(vSUS)-adaptive sorting;
 as well as a smaller compressed data structure.

nRem-Adaptive Sorting and Compression: The preorder measures described
above are all variants of MergeSort, exploiting the similarity of its execution
with a wavelet tree: they are all situated on the same “branch” of the graph from
Figure 1 representing the measures of preorder and their relation.

The preorder measure nRem counts how many elements must be removed from
a permutation so that what remains is already sorted. Its exact value is n minus

Fast Algorithms That Yield Small and Fast Data Structures

107

the length of the Longest Increasing Subsequence, which can be computed in
time n lg n, but in order to adaptively sort in time faster than this, nRem can be
approximated within a factor of 2 in n comparisons by an algorithm very similar
to the one building a LRM-tree, which returns a partition of π into one part of
2nRem unsorted elements, and n − 2nRem elements in increasing order. Sorting
those 2nRem unsorted elements using any n-worst-case optimal comparison-based
algorithm (ideally, one of the adaptive algorithms described above), and merging
its result with the n − 2nRem elements found to be already in increasing order,
yields an adaptive sorting algorithm that performs 2n + 2nRem lg(n/nRem + 1)
comparisons [14, 29]. Similarly, partitioning π into those two parts by a bit vector
of n bits; representing the order of the 2nRem elements in a wavelet tree (using
any of the data structures described above) and representing the merging of both
into n bits yields a compressed data structure using 2n + 2nRem lg(n/nRem) +
−1() in sublinear time, within
o(n) bits and supporting the operators π() and π
O(log(nRem + 1) + 1).

nInv-Adaptive Sorting and Compression: The preorder measure nInv
counts the number of pairs (i, j) of positions 1 ≤ i < j ≤ n in a permutation π
over [1..n] such that π(i) > π(j). Its value is exactly the number of comparisons
performed by the algorithm Insertion Sort, between n and n2 for a permutation 
over [1..n]. A variant of Insertion Sort, named Local Insertion Sort,
sorts π in n(1 + lg(nInv/n)) comparisons [14, 29].

As before, the bit vector B listing the binary results of the comparisons performed 
by Local Insertion Sort on a permutation π identiﬁes exactly π, because 
B is suﬃcient to simulate the execution of Local Insertion Sort on π
without access to it. This yields an encoding of π into n(1 + (cid:4)lg(nInv/n)(cid:5)) bits,
which is smaller than n(cid:4)lg n(cid:5) bits for permutations such that nInv ∈ o(n2). Yet
−1()) on
it is not clear how to support the operator π() (yet even its inverse π
such an encoding without reading all the n(1 + lg(nInv/n)) bits of B: the bits
deciding of a single value can be spread in the whole encoding.
But reordering those bits does yield a compressed data structure supporting
the operator π() in constant time, by simply encoding the n values (π(i)−i)i∈[1..n]
(cid:3) code from Elias [13], and indexing the positions of the beginning
using the γ
of each code by a compressed bit vector. Following the execution of Linear
Insertion Sort algorithm over a permutation π over [1..n] presenting nInv
inversions, the number of swaps of the i-th element π(i) required to reach its
ﬁnal position in the sorted list is π(i) = i + g+
i (π) =
|{j ∈ [1..n] : j > i and π(j) < π(i)}| is the number of swaps to the right; and
i (π) = |{j ∈ [1..n] : j < i and π(j) > π(i)}| is the number of swaps to the
−
g
−
left. By deﬁnition of nInv, g+ and g
i (π) = nInv, and
by property of the γ
i (π)
(cid:4)
−
n
(or g
i (π)) is Gap(g+
n ,
= n lg nInv
by concavity of the logarithm. Since π(i)− i = g+
−
i (π), the data structure
uses n+Gap((π(i)−i)i∈[1..n])+o(n) ⊂ n+2n lg nInv
n )+o(n)
bits.

i=1 g+
i (π) =
i (π) ≤ n lg
i (π)− g
n +o(n) = n(1+2 lg nInv

−
i (π), where g+

i (π) − g

n
i=1 g

−,

(cid:3) code, the number of bits used to store the values of g+
i (π))i∈[1..n] =

i=1 lg g+

(cid:5) (cid:2)n

+
i (π)

i=1 g
n

(cid:6)

(cid:4)

n

(cid:4)

108

J. Barbay

Note that the compressed data structure described so far supports the operator 
π() in constant time, which is faster than the compressed data structure
−1() (other than in at least linear time,
described above, but not the operator π
−1 has the
by reconstructing π). By deﬁnition of nInv, the inverse permutation π
−1 uses the same
same number nInv of inversions than π: the data structure for π
−1 as
space as for π. Hence encoding both the permutations π and its inverse π
described above yields a data structure using space within 2n(1+2 lg nInv
n )+o(n)
−1() in constant time. This space is less
which supports both operators π() and π
4 ], but of course in the worst
than n log n + o(n) for instance where nInv[0..n
case where nInv is close to n2, this space is getting close to 8n log n + 2n + o(n),
a solution four times as costly as merely encoding in a raw form both π and its
inverse π

−1.

5

Other Adaptive Sorting and Compression: As we observed in Section 2.3,
Moﬀat and Petersson [29] list many other measures of preorder and adaptive
sorting techniques. Each measure explored above yields a compressed data struc-
−1() in sublinear time.
ture for permutation supporting the operators π() and π
Figure 1 shows the relation between those measures, and the table below shows
the relation between a selection of adaptive sorting algorithms and some permutation 
data structure (omitting both the o() order terms in the space and the
support time for the operators for sake of space).

Sorting Algorithm
Natural MergeSort [23]
BN-MergeSort [7]
H(vSUS)-Sort [7]
H(SMS)-Sort [3]
Rem-Sort (here)
Local Ins Sort (here)

cmps=bits

n(1+ lg nRuns)
n(1+H(vRuns))
2nH(vSUS)
2nH(vSMS)

2n + 2nRem lg(n/nRem)
2n(1 + 2 lg(nInv/n))

Permutation Data Structure
Runs [7]
Huﬀman Runs [7]
Huﬀman SUS [7]
Huﬀman SMS [7]
Rem-encoding (here)
Inv-encoding (here)

Note that all comparison-based adaptive sorting algorithms yield a compression 
scheme for permutations, but not all yield one for which we know how to
support useful operators (such as π() and π

−1()) in sublinear time.

5 Selected Open Problems

From Compression Schemes to Compressed Data Structures: In most
current applications, compressed data is useless if it needs to be totally decompressed 
in order to access a small part of it. We saw how to support some
operators on compressed data inspired from adaptive algorithms in the cases of
permutations, integers and strings, but there are many other operators to study,
from the iterated operator πk() on a compressed permutation π, non-algebraic
operators on compressed integers, patern matching operators on compressed
strings, etc...

Fast Algorithms That Yield Small and Fast Data Structures

109

From Compression Schemes to Adaptive Algorithms: Bentley and Yao
[9] were already asking in 1976 if there are cases where such relations between
compressed data structures and adaptive algorithms are bijective. Such a question 
comes naturally and applies to many other Abstract Data Types than integers 
or permutations, as both compression schemes and adaptive algorithms
take advantage of forms of regularity in the instances considered. If a systematic
transformation generating a distinct adaptive algorithm from each distinct compression 
scheme might not exist, at least one should be able to deﬁne a subclass
of compression schemes which are in bijection with adaptive algorithms.

Other Compressed Data Structures for Permutations: Each adaptive
sorting algorithm in the comparison model yields a compression scheme for permutations,
 but the encoding thus deﬁned does not necessarily support the simple
application of the permutation to a single element without decompressing the
whole permutation, nor the application of the inverse permutation. Figure 1
represents the preorder measures for which opportunistic sorting algorithms are
known, and in round boxes the ones for which compressed data structures for per-
−1()) are known.
mutations (supporting in sublinear time the operators π() and π
Are there compressed data structures for permutations, supporting the
−1() in sublinear time and using space proportional to
operators π() and π
the other preorder measures? What about other useful operators on permutations,
 such as πk()?

Sorting and Encoding Multisets: Munro and Spira [32] showed how to sort
multisets through MergeSort, Insertion Sort and Heap Sort, adapting them
with counters to sort in time O(n(1+H((cid:9)m1, . . . , mr(cid:10)))) where mi is the number
of occurrences of i in the multiset (note this is totally diﬀerent from our results,
that depend on the distribution of the lengths of monotone runs). It seems easy to
combine both approaches (e.g. on Merge Sort in a single algorithm using both
runs and counters), yet quite hard to analyze the complexity of the resulting
algorithm. The diﬃculty measure must depend not only on both the entropy of
the partition into runs and the entropy of the repartition of the values of the
elements, but also on their interaction.

References

[1] Ávila, B.T., Laber, E.S.: Merge source coding. In: Proceedings of IEEE International 
Symposium on Information Theory (ISIT), pp. 214–218. IEEE (2009)

[2] Baeza-Yates, R.: A fast set intersection algorithm for sorted sequences. In: Sahinalp,
 S.C., Muthukrishnan, S.M., Dogrusoz, U. (eds.) CPM 2004. LNCS, vol. 3109,
pp. 400–408. Springer, Heidelberg (2004)

[3] Barbay, J., Claude, F., Gagie, T., Navarro, G., Nekrich, Y.: Eﬃcient fullycompressed 
sequence representations. Algorithmica (to appear, 2013)

[4] Barbay, J., Fischer, J., Navarro, G.: LRM-trees: Compressed indices, adaptive
sorting, and compressed permutations. Elsevier Theoretical Computer Science
(TCS) 459, 26–41 (2012)

110

J. Barbay

[5] Barbay, J., He, M., Munro, J.I., Satti, S.R.: Succinct indexes for strings, binary
relations and multilabeled trees. ACM Transactions on Algorithms 7(4), 52 (2011)
[6] Barbay, J., Kenyon, C.: Deterministic algorithm for the t-threshold set problem.
In: Proceedings of the 14th International Symposium Algorithms and Computation 
(ISAAC), pp. 575–584 (2003)

[7] Barbay, J., Navarro, G.: Compressed representations of permutations, and applications.
 In: 26th International Symposium on Theoretical Aspects of Computer
Science (STACS 2009), vol. 3, pp. 111–122 (2009)

[8] Belazzougui, D., Navarro, G.: Alphabet-independent compressed text indexing.

ACM Transactions on Algorithms (TALG) (to appear, 2013)

[9] Bentley, J.L., Yao, A.C.-C.: An almost optimal algorithm for unbounded searching.

Information Processing Letters 5(3), 82–87 (1976)

[10] Burge, W.H.: Sorting, trees, and measures of order. Information and Control 1(3),

181–197 (1958)

[11] Carlsson, S., Levcopoulos, C., Petersson, O.: Sublinear merging and natural mergesort.
 Algorithmica 9(6), 629–648 (1993)

[12] Demaine, E.D., López-Ortiz, A., Munro, J.I.: Adaptive set intersections, unions,
and diﬀerences. In: Proceedings of the 11th ACM-SIAM Symposium on Discrete
Algorithms (SODA), pp. 743–752 (2000)

[13] Elias, P.: Universal codeword sets and representations of the integers. IEEE Transactions 
on Information Theory 21(2), 194–203 (1975)

[14] Estivill-Castro, V., Wood, D.: A survey of adaptive sorting algorithms. ACM Computing 
Surveys 24(4), 441–476 (1992)

[15] Fernandez de la Vega, W., Kannan, S., Santha, M.: Two probabilistic results on
merging. In: Asano, T., Imai, H., Ibaraki, T., Nishizeki, T. (eds.) SIGAL 1990.
LNCS, vol. 450, pp. 118–127. Springer, Heidelberg (1990)

[16] Fischer, J.: Optimal succinctness for range minimum queries. In: López-Ortiz, A.

(ed.) LATIN 2010. LNCS, vol. 6034, pp. 158–169. Springer, Heidelberg (2010)

[17] Gabow, H.N., Bentley, J.L., Tarjan, R.E.: Scaling and related techniques for geometry 
problems. In: Proc. STOC, pp. 135–143. ACM Press (1984)

[18] Gennaro, R., Trevisan, L.: Lower bounds on the eﬃciency of generic cryptographic
constructions. In: IEEE Symposium on Foundations of Computer Science, pp.
305–313 (2000)

[19] Golynski, A.: Optimal lower bounds for rank and select indexes. In: Bugliesi, M.,
Preneel, B., Sassone, V., Wegener, I. (eds.) ICALP 2006. LNCS, vol. 4051, pp.
370–381. Springer, Heidelberg (2006)

[20] Golynski, A.: Upper and Lower Bounds for Text Indexing Data Structures. PhD

thesis. University of Waterloo (2007)

[21] Golynski, A., Munro, J.I., Rao, S.S.: Rank/select operations on large alphabets: a
tool for text indexing. In: Proceedings of the 17th Annual ACM-SIAM Symposium
on Discrete Algorithms (SODA), pp. 368–373. ACM (2006)

[22] Grossi, R., Gupta, A., Vitter, J.S.: High-order entropy-compressed text indexes.
In: Proceedings of the 14th Annual ACM-SIAM Symposium on Discrete Algorithms 
(SODA), pp. 841–850. ACM (2003)

[23] Harris, J.D.: Sorting unsorted and partially sorted lists using the natural merge

sort. Software: Practice and Experience 11(12), 1339–1340 (1981)

[24] Hwang, F.K., Lin, S.: A simple algorithm for merging two disjoint linearly-ordered

sets. SIAM J. Comput. 1(1), 31–39 (1972)

[25] Jacobson, G.: Space-eﬃcient static trees and graphs. In: Proceedings of the 30th
IEEE Symposium on Foundations of Computer Science (FOCS), pp. 549–554
(1989)

Fast Algorithms That Yield Small and Fast Data Structures

111

[26] Levcopoulos, C., Petersson, O.: Sorting shuﬄed monotone sequences. In: Gilbert,
J.R., Karlsson, R. (eds.) SWAT 1990. LNCS, vol. 447, pp. 181–191. Springer,
Heidelberg (1990)

[27] Levcopoulos, C., Petersson, O.: Sorting shuﬄed monotone sequences. Inf. Comput.
 112(1), 37–50 (1994)

[28] Mannila, H.: Measures of presortedness and optimal sorting algorithms. IEEE

Trans. Computers 34(4), 318–325 (1985)

[29] Moﬀat, A., Petersson, O.: An overview of adaptive sorting. Australian Computer

[30] Moﬀat, A., Stuiver, L.: Binary interpolative coding for eﬀective index compression.

Journal 24(2), 70–77 (1992)

Inf. Retr. 3(1), 25–47 (2000)

[31] Munro, J.I., Raman, R., Raman, V., Rao, S.S.: Succinct representations of permutations 
and functions. Theor. Comput. Sci. 438, 74–88 (2012)

[32] Munro, J.I., Spira, P.M.: Sorting and searching in multisets. SIAM J. Com-

[33] Petersson, O., Moﬀat, A.: A framework for adaptive sorting. Discrete Applied

put. 5(1), 1–8 (1976)

Mathematics 59, 153–179 (1995)

[34] Raman, R., Raman, V., Satti, S.R.: Succinct indexable dictionaries with applications 
to encoding k-ary trees, preﬁx sums and multisets. ACM Transactions on
Algorithms 3(4) (2007)

[35] Rice, R.F., Plaunt, J.R.: Adaptive variable-length coding for eﬃcient compression

of spacecraft television data. IEEE Trans. Commun. COM-19, 889–897 (1971)

[36] Sadakane, K., Navarro, G.: Fully-functional succinct trees. In: Proc. SODA,

pp. 134–149. ACM/SIAM (2010)

