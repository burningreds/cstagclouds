LRM-Trees: Compressed Indices, Adaptive

Sorting, and Compressed Permutations(cid:2)

J´er´emy Barbay1, Johannes Fischer2, and Gonzalo Navarro1

1 Department of Computer Science, University of Chile

{jbarbay,gnavarro}@dcc.uchile.cl

2 Computer Science Department, Karlsruhe University

johannes.fischer@kit.edu

Abstract. LRM-Trees are an elegant way to partition a sequence of
values into sorted consecutive blocks, and to express the relative position
of the ﬁrst element of each block within a previous block. They were used
to encode ordinal trees and to index integer arrays in order to support
range minimum queries on them. We describe how they yield many other
convenient results in a variety of areas: compressed succinct indices for
range minimum queries on partially sorted arrays; a new adaptive sorting
algorithm; and a compressed succinct data structure for permutations
supporting direct and inverse application in time inversely proportional
to the permutation’s compressibility.

1 Introduction

Introduced by Fischer [9] as an indexing data structure which supports Range
Minimum Queries (RMQs) in constant time with no access to the main data,
and by Sadakane and Navarro [26] to support navigation operators on ordinal
trees, Left-to-Right-Minima Trees (LRM-Trees) are an elegant way to partition
a sequence of values into sorted consecutive blocks, and to express the relative
position of the ﬁrst element of each block within a previous block.

We describe how the use of LRM-Trees and variants yields many other convenient 
results in the design of data structures and algorithms:

1. We deﬁne three compressed succinct indices supporting RMQs, which use
less space when the indexed array is partially sorted, improving in those cases
on the 2n + o(n) bits usual space [9], and on other techniques of compression
for RMQs such as taking advantage of repetitions in the input [10].

2. Based on LRM-Trees, we deﬁne a new measure of presortedness for permutations.
 It combines some of the advantages of two well-known measures,
runs and shuﬄed up-sequences: the new measure is computable in linear
time (like the former), but considers sorted sub-sequences (instead of only
contiguous sub-arrays) in the input (similar, yet distinct, to the latter).

(cid:2) First and third author partially funded by Fondecyt grant 1-110066, Chile; second

author supported by a DFG grant (German Research Foundation).

R. Giancarlo and G. Manzini (Eds.): CPM 2011, LNCS 6661, pp. 285–298, 2011.
c(cid:2) Springer-Verlag Berlin Heidelberg 2011

286

J. Barbay, J. Fischer, and G. Navarro

3. Based on this measure, we propose a new sorting algorithm and its adaptive
analysis, asymptotically superior to sorting algorithms based on runs [2], and
on many instances faster than sorting algorithms based on subsequences [19].
4. We design a compressed succinct data structure for permutations based on
this measure, which supports the access operator and its inverse in time inversely 
proportional to the permutation’s presortedness, improving on previous 
similar results [2].

All our results are in the word RAM model, where it is assumed that we
can do arithmetic and logical operations on w-bit wide words in O(1) time, and
w = Ω(lg n). In our algorithms and data structures, we distinguish between the
work performed in the input (often called “data complexity” in the literature)
and the accesses to the internal data structures (“index complexity”). This is
important in cases where the input is large and cannot be stored in main memory,
whereas the index is potentially small enough to be kept in fast main memory.
For instance, in the context of compressed indexes like our RMQ structures,
given a ﬁxed limited amount of local memory, this additional precision permits
identifying the instances whose compressed index ﬁts in it while the main data
does not. On these instances, between two data structures that support operators
with the same total asymptotic complexity but distinct index complexity, the
one with the lowest index complexity is more desirable.

2 Previous Work and Concepts

2.1 Left-to-Right-Minima Trees

LRM-Trees partition a sequence of values into sorted consecutive blocks, and
express the relative position of the ﬁrst element of each block within a previous
block. They were introduced under this name as an internal tool for basic navigational 
operations in ordinal trees [26], and, under the name “2d-Min Heaps,”
to index integer arrays in order to support range minimum queries on them [9].
Let A[1, n] be an integer array. For technical reasons, we deﬁne A[0] = −∞

as the “artiﬁcial” overall minimum of the array.
Deﬁnition 1 (Fischer [9]; Sadakane and Navarro [26]). For 1 ≤ i ≤ n,
let psvA(i) = max{j ∈ [0..i − 1]
: A[j] < A[i]} denote the previous smaller
value of position i. The Left-to-Right-Minima Tree (LRM-Tree) TA of A is an
ordered labeled tree with n + 1 vertices each labeled uniquely from {0, . . . , n}. For
1 ≤ i ≤ n, psvA(i) is the parent node of i. The children of each node are ordered
in increasing order from left to right.

See Fig. 1 for an example of LRM-Trees. Fischer [9] gave a (complicated) lineartime 
construction algorithm with advantages that are not relevant for this paper.
The following lemma shows a simpler way to construct the LRM-Tree in at most
2(n−1) comparisons within the array and overall linear time, which will be used
in Thms. 4 and 5.

LRM-Trees

287

i

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
A[i] 15 8 13 7 11 16 1 10 9 14 2 12 3 6 5 4

−∞

15(1)

8(2)

13(3)

7(4)

11(5)

1(7)

16(6)

10(8)

9(9)

2(11)

14(10)

12(12)

3(13)

6(14) 5(15) 4(16)

Fig. 1. An example of an array and its LRM-Tree

Lemma 1. Given an array A[1, n] of totally ordered objects, there is an algorithm 
computing its LRM-Tree in at most 2(n − 1) comparisons within A and
O(n) total time.
Proof. The computation of the LRM-Tree corresponds to a simple scan over
the input array, starting at A[0] = −∞, building down iteratively the current
rightmost branch of the tree with increasing elements of the sequence until an
element x smaller than its predecessor is encountered. At this point one climbs
the rightmost branch up to the ﬁrst node v holding a value smaller than x, and
starts a new branch with a rightmost child of v of value x. As the root of the
tree has value A[0] = −∞ (smaller than all elements), the algorithm always
terminates.
The construction algorithm performs at most 2(n − 1) comparisons: the ﬁrst
two elements A[0] and A[1] can be inserted without any comparison as a simple
path of two nodes (so A[1] will be charged only once). For the remaining elements,
we charge the last comparison performed during the insertion of an element x to
the node of value x itself, and all previous comparisons to the elements already
in the LRM-Tree. Thus, each element (apart from A[1] and A[n]) is charged at
most twice: once when it is inserted into the tree, and once when scanning it
while searching for a smaller value on the rightmost branch. As in the latter case
all scanned elements are removed from the rightmost path, this second charging
occurs at most once for each element. Finally, the last element A[n] is charged
only once, as it will never be scanned: hence the total number of comparisons of
2n− 2 = 2(n − 1). Since the number of comparisons within the array dominates
the number of other operations, the overall time is also in O(n).
(cid:5)(cid:6)

288

J. Barbay, J. Fischer, and G. Navarro

2.2 Range Minimum Queries

We consider the following queries on a static array A[1, n] (parameters i and j
with 1 ≤ i ≤ j ≤ n):
Deﬁnition 2 (Range Minimum Queries). rmqA(i, j) = position of a minimum 
in A[i, j].

RMQs have a wide range of applications for various data structures and algorithms,
 including text indexing [11], pattern matching [7], and more elaborate
kinds of range queries [6].

For two given nodes i and j in a tree T , let lcaT (i, j) denote their Lowest
Common Ancestor (LCA), that is, the deepest node that is an ancestor of both
i and j. Now let TA be the LRM-Tree of A. For arbitrary nodes i and j in TA,
1 ≤ i < j ≤ n, let (cid:3) = lcaTA(i, j). Then if (cid:3) = i, rmqA(i, j) is i, otherwise,
rmqA(i, j) is given by the child of (cid:3) that is on the path from (cid:3) to j [9].

Since there are succinct data structures supporting the LCA operator [9, 17].
in succinctly encoded trees in constant time, this yields a succinct index (which
we improve with Thms. 1 and 3).

Lemma 2 (Fischer [9]). Given an array A[1, n] of totally ordered objects, there
is a succinct index using 2n + o(n) bits and supporting RMQs in zero accesses
to A and O(1) accesses to the index. This index can be built in O(n) time.

2.3 Adaptive Sorting and Compression of Permutations

Sorting a permutation in the comparison model requires Θ(n lg n) comparisons
in the worst case over permutations of n elements. Yet, better results can be
achieved for some parameterized classes of permutations. For a ﬁxed permutation
π, Knuth [18] considered Runs (contiguous ascending subsequences), counted by
: 1 ≤ i < n, πi+1 < πi}|; Levcopoulos and Petersson [19]
|Runs| = 1 + |{i
introduced Shuﬄed Up-Sequences and its generalization Shuﬄed Monotone Sequences,
 respectively counted by |SUS| = min{k : π is covered by k increasing
subsequences}, and |SMS| = min{k : π is covered by k monotone subsequences}.
Barbay and Navarro [2] introduced strict variants of some of those concepts,
namely Strict Runs and Strict Shuﬄed Up-Sequences, where sorted subsequences
are composed of consecutive integers (e.g., (2, 3, 4, 1, 5 , 6 , 7 , 8 ) has two runs but
three strict runs), counted by |SRuns| and |SSUS|, respectively. For any of those
ﬁve measures of disorder X, there is a variant of the merge-sort algorithm which
sorts a permutation π, of size n and of measure of presortedness X, in time
O(n(1 + lg X)), which is within a constant factor of optimal in the worst case
among instances of ﬁxed size n and ﬁxed values of X (this is not necessarily true
for other measures of disorder).

As the merging cost induced by a subsequence is increasing with its length,
the sorting time of a permutation can be improved by rebalancing the merging
tree [2]. The complexity can then be expressed more precisely as a function of
the entropy of the relative sizes of the sorted subsequences identiﬁed, where

LRM-Trees

289

the entropy H(Seq) of a sequence Seq = (cid:7)n1, n2, . . . , nr(cid:8) of r positive integers
adding up to n is deﬁned as H(Seq) =
ni . This entropy satisﬁes
(r − 1) lg n ≤ nH(Seq) ≤ n lg r by concavity of the logarithm, a formula which
we will use later.

ni
n lg n

(cid:2)r

i=1

Barbay and Navarro [2] observed that each adaptive sorting algorithm in the
comparison model also describes an encoding of the permutation π that it sorts,
so that it can be used to compress permutations from speciﬁc classes to less than
the information-theoretic lower bound of n lg n bits. Furthermore they used the
similarity of the execution of the merge-sort algorithm with a Wavelet Tree [14],
to support the application of π() and its inverse π−1() in time logarithmic in the
disorder of the permutation π (as measured by |Runs|, |SRuns|, |SUS|, |SSUS| or
|SMS|) in the worst case. We summarize their technique in Lemma 3 below, in a
way independent of the partition chosen for the permutation, and focusing only
on the merging part of the sorting.

Lemma 3 (Barbay and Navarro [2]). Given a partition of an array π of
n totally ordered objects into |Seq| sorted subsequences of respective lengths
Seq = (cid:7)n1, n2, . . . , n|Seq|(cid:8), these subsequences can be merged with n(1 + H(Seq))
comparisons on π and O(n(1+H(Seq))) total running time. This merging can be
encoded using at most (1 + H(Seq))(n + o(n)) + O(|Seq| lg n) bits so that it supports 
the computation of π(i) and π−1(i) in time O(1+lg |Seq|) in the worst case
∀i ∈ [1..n], and in time O(1 + H(Seq)) on average when i is chosen uniformly
at random in [1..n].

3 Compressed Succinct Indexes for Range Minima

We now explain how to improve on the result of Lemma 2 for permutations
that are partially ordered. We consider only the case where the input A is a
permutation of [1..n]: if this is not the case, we can sort the elements in A by
rank, considering earlier occurrences of equal elements as smaller. Our ﬁrst and
simplest compressed data structure for RMQs uses an amount of space which is
a function of |SRuns|, the number of strict runs in π. Beside its simplicity, its
interest resides in that it uses a total space within o(n) bits on permutations
where |SRuns| ∈ o(n), and introduces techniques which we will use in Thms. 2
and 3.

Theorem 1. Given an array A[1, n] of totally ordered objects, composed of
(cid:4)(cid:11) +
|SRuns| strict runs, there is a compressed succinct index using (cid:10)lg
2|SRuns| + o(n) bits which supports RMQs in zero accesses to A and O(1) accesses 
to the index.

n|SRuns|

(cid:3)

Proof. We mark the beginnings of the runs in A with a 1 in a bit-vector B[1, n],
and represent B with the compressed succinct data structure from Raman et
(cid:4)(cid:11) + o(n) bits. Further, we deﬁne A(cid:3) as the (conceptual)
al. [24], using (cid:10)lg
array consisting of the heads of A’s runs (A(cid:3)[i] = A[select1(B, i)]). We build the
LRM-Tree from Lemma 2 on A(cid:3) using 2|SRuns|(1 + o(1)) bits. To answer a query

(cid:3)
n|SRuns|

290

J. Barbay, J. Fischer, and G. Navarro

rmqA(i, j), we compute x = rank1(B, i) and y = rank1(B, j), then compute
m(cid:3) = rmqA(cid:2)(x, y) as the minimum of the heads of those runs that overlap the
query interval, and map it back to its position in A by m = select1(B, m(cid:3)). Then
if m < i, we return i as the ﬁnal answer to rmqA(i, j), otherwise we return m.
The correctness of this algorithm follows from the fact that only i and the heads
that are contained in the query interval can be the range minimum. Because the
runs are strict, the former occurs if and only if the head of the run containing i
(cid:5)(cid:6)
is smaller than all other heads in the query range.

The same idea as in Thm. 1 applied to more general runs yields another compressed 
succinct index for RMQs, potentially smaller but this time requiring to
compare two elements from the input to answer RMQs.

(cid:3)

Theorem 2. Given an array A[1, n] of totally ordered objects, composed of
(cid:4)(cid:11)+o(n)
|Runs| runs, there is a compressed succinct index using 2|Runs|+(cid:10)lg
bits and supporting RMQs in 1 comparison within A and O(1) accesses to the
index.
(cid:3)
(cid:4)(cid:11)
Proof. We build the same data structures as in Thm. 1, using 2|Runs|+(cid:10)lg
n|Runs|
+o(n) bits. To answer a query rmqA(i, j), we compute x = rank1(B, i) and y =
rank1(B, j). If x = y, return i. Otherwise, compute m(cid:3) = rmqA(cid:2)(x + 1, y), and
map it back to its position in A by m = select1(B, m(cid:3)). The ﬁnal answer is i if
(cid:5)(cid:6)
A[i] < A[m], and m otherwise.

n|Runs|

To achieve a compressed succinct index which never accesses the array and
whose space usage is a function of |Runs|, we need more space and a more heavy
machinery, as shown next. The main idea is that a permutation with few runs
results in a compressible LRM-Tree, where many nodes have out-degree 1.

Theorem 3. Given an array A[1, n] of totally ordered objects, composed of
|Runs| runs, there is a compressed succinct index using 2|Runs| lg n + o(n) bits,
and supporting RMQs in zero accesses to A and O(1) accesses to the index.
Proof. We build the LRM-Tree TA from Sect. 2.1 directly on A, and then
compress it with the tree representation of Jansson et al. [17].
To see that this results in the claimed space, let nk denote the number of nodes
in TA with out-degree k ≥ 0. Let (i1, j1), . . . , (i|Runs|, j|Runs|) be an encoding of
the runs in A as (start, end), and look at a pair (ix, jx). We have psvA(k) = k−1
for all k ∈ [ix + 1..jx], and so the nodes in [ix..jx] form a path in TA, possibly
interrupted by branches stemming from heads iy of other runs y > x with
psvA(iy) ∈ [ix..jx − 1]. Hence n0 = |Runs|, and n1 ≥ n − |Runs| − (|Runs| − 1) >
n − 2|Runs|, as in the worst case the values psvA(iy) for iy ∈ {i2, i3, . . . , i|Runs|}
are all diﬀerent.
As an illustrative example, look again at the tree in Fig. 1. It has n0 = 9
leaves, corresponding to the runs (cid:7)15(cid:8), (cid:7)8, 13(cid:8), (cid:7)7, 11, 16(cid:8), (cid:7)1, 10(cid:8), (cid:7)9, 14(cid:8), (cid:7)2, 12(cid:8),
(cid:7)3, 6(cid:8), (cid:7)5(cid:8), and (cid:7)4(cid:8) in A. The ﬁrst four runs have a PSV of A[0] = −∞ for their
corresponding head elements, the next two head-PSVs point to A[7] = 1, the

LRM-Trees

291

next one to A[11] = 2, and the last two to A[13] = 3. Hence, the heads of the
runs “destroy” exactly four of the n − n0 + 1 potential degree-1 nodes in the
tree, so n1 = n − n0 − 4 + 1 = 16 − 9 − 3 = 4.
Now TA, with degree-distribution n0, . . . , nn−1, is compressed into nH∗(TA)+

(cid:5)

O

n(lg lg n)

lg n

(cid:6)

2

bits [17], where

nH∗

(TA) = lg

(cid:7)

1
n

(cid:7)

n

(cid:8)(cid:8)

n0, n1, . . . , nn−1

is the so-called tree entropy [17] of TA. This representation supports all navigational 
operations in TA in constant time, and in particular those required for
Lemma 2. A rough inequality yields a bound on the number of possible such
LRM-Trees:

n

n0, n1, . . . , nn−1

=

n!

n0!n1! . . . nn−1!

≤

≤ n!
n1!

n!

(n − 2|Runs|)!

≤ n2|Runs| ,

(cid:7)

(cid:8)

from which one easily bounds the space usage of the compressed succinct index:
= (2|Runs| − 1) lg n < 2|Runs| lg n .

(cid:5)
n2|Runs|−1

(TA) ≤ lg

n2|Runs|

nH∗

= lg

(cid:8)

(cid:6)

(cid:7)

1
n

Adding the space required to index the structure of Jansson et al. [17] yields the
(cid:5)(cid:6)
claimed space bound.

4 Sorting Permutations

Barbay and Navarro [2] showed how to use the decomposition of a permutation
π into |Runs| ascending consecutive runs of respective lengths Runs to sort adaptively 
to their entropy H(Runs). Those runs entirely partition the LRM-Tree of
π: one can easily draw the partition corresponding to the runs considered by
Barbay and Navarro [2] by iteratively tagging the leftmost maximal untagged
leaf-to-root path of the LRM-Tree. For instance, the permutation of Figure 1 has
nine runs ((cid:7)15(cid:8), (cid:7)8, 13(cid:8), (cid:7)7, 11, 16(cid:8), (cid:7)1, 10(cid:8), (cid:7)9, 14(cid:8), (cid:7)2, 12(cid:8), (cid:7)3, 6(cid:8), (cid:7)5(cid:8), and (cid:7)4(cid:8)), of
respective sizes given by the vector < 1, 2, 3, 2, 2, 2, 2, 1, 1 >.

But any partition of the LRM-Tree into branches (such that the values traversed 
by the path are increasing) can be used to sort π, and a partition of
smaller entropy yields a faster merging phase. To continue with the previous
example, the nodes of the LRM-Tree of Figure 1 can be partitioned diﬀerently,
so that the vector formed by the sizes of the increasing subsequences it describes 
has lower entropy. One such partition would be (cid:7)15(cid:8), (cid:7)8, 13(cid:8), (cid:7)7, 11, 16(cid:8),
(cid:7)1, 2, 3, 4(cid:8), (cid:7)10(cid:8), (cid:7)9, 14(cid:8), (cid:7)12(cid:8), (cid:7)6(cid:8), and (cid:7)5(cid:8), of respective sizes given by the vector
< 1, 2, 3, 4, 1, 2, 1, 1, 1 >.
Deﬁnition 3 (LRM-Partition). An LRM-Partition P of an LRM-Tree T for
an array A is a partition of the nodes of T into |LRM| down-paths, i.e. paths
starting at some branching node of the tree, and ending at a leaf. The entropy of

292

J. Barbay, J. Fischer, and G. Navarro

P is H(P ) = H(r1, . . . , r|LRM|), where r1, . . . , r|LRM| are the lengths of the downpaths 
in P . P is optimal if its entropy is minimal among all the LRM-partitions
of T . The entropy of this optimal partition is the LRM-entropy of the LRM-Tree
T and, by extension, the LRM-entropy of the array A.
Note that since there are exactly |Runs| leaves in the LRM-Tree, there will always
be |Runs| down-paths in the LRM-partition; hence |LRM| = |Runs|. We ﬁrst
deﬁne a particular LRM-partition and prove that its entropy is minimal. Then
we show how it can be computed in linear time.
Deﬁnition 4 (Left-Most Spinal LRM-Partition). Given an LRM-Tree T ,
the left-most spinal chord of T is the leftmost path among the longest root-toleaf 
paths in T ; and the left-most spinal LRM-partition is deﬁned recursively
as follows. Removing the left-most spinal chord of T leaves a forest of shallower
trees, which are partitioned recursively. The left-most spinal partition is obtained
by concatenating all resulting LRM-partitions in arbitrary order. LRM denotes
the vector formed by the |LRM| lengths of the subsequences in the partition.
For instance, the left-most spinal LRM-partition of the LRM-tree given in Figure 
1 is quite easy to build: the ﬁrst left-most spinal chord is −∞, 1, 2, 3, 6, which
removal leaves a forest of simple branches. The resulting partition is (cid:7)15(cid:8), (cid:7)8, 13(cid:8),
(cid:7)7, 11, 16(cid:8), (cid:7)1, 2, 3, 6(cid:8), (cid:7)10(cid:8), (cid:7)9, 14(cid:8), (cid:7)12(cid:8), (cid:7)5(cid:8), and (cid:7)4(cid:8), of respective sizes given by
the vector < 1, 2, 3, 4, 1, 2, 1, 1, 1 >.

The LRM-partition, by successively extracting increasing subsequences of
maximal length, actually yields a partition of minimal entropy, as shown in
the following lemma.

Lemma 4. The entropy of the left-most spinal LRM-partition is minimal among
all LRM-partitions.
Proof. Given an LRM-Tree T , consider the leftmost leaf L0 among the leaves
of maximal depth in T . We prove that there is always an optimal LRM-partition
which contains the down-path (−∞, L0). Applying this property recursively in
the trees produced by removing the nodes of (−∞, L0) from T yields the optimality 
of the leftmost LRM-partition.

R

M

N0

N1

L0

L1

Fig. 2. Consider an arbitrary LRM-partition P and the
down-path (N0, L0) in P ﬁnishing at L0. If N0 (cid:3)= −∞ (that
is, N0 is not the root), then consider the parent M of N0 and
the down-path (R, L1) which contains M and ﬁnishes at a
leaf L1. Call N1 the child of M on the path to L1.

Consider an arbitrary LRM-partition P and the nodes R, M, N0, N1 and L1
as described in Figure 2. Call r the number of nodes in (R, M), d0 the number
of nodes in (N0, L0), and d1 the number of nodes in (N1, L1). Note that d1 ≤ d0

LRM-Trees

293

i=1

1 = r + d0 and n(cid:3)

because L0 is one of the deepest leaves. Thus the LRM-partition P has a downpath 
(N0, L0) of length d0 and another (R, L1) of length r + d1. We build a new
LRM-partition P (cid:3) by switching some parts of the down-paths, so that one goes
from R to L0 and the other from N1 to L1, with new down-path lengths r + d0
and d1, respectively.
Let (cid:7)n1, n2, . . . , n|LRM|(cid:8) be the down-path lengths in P , such that H(P ) =
H(n1, n2, . . . , n|LRM|) = n lg n − (cid:2)|LRM|
ni lg ni. Without loss of generality (the
entropy is invariant to the order of the parameters), assume that n1 = d0 and
n2 = r + d1 are the down-paths we have considered: they are replaced in P (cid:3)
by down-paths of length n(cid:3)
2 = d1. The variation in entropy
is [(r + d1) lg(r + d1) + d0 lg d0] − [(r + d0) lg(r + d0) + d1 lg d1], which can be
rewritten as f(d1) − f(d0) with f(x) = (r + x) lg(r + x) − x lg x. Since the
function f(x) = (r + x) lg(r + x)− x lg x has positive derivative and d1 ≤ d0, the
diﬀerence is non-positive (and strictly negative if d1 < d0, which would imply
that P was not optimal). Iterating this argument until the path of the LRMpartition 
containing L0 is rooted in −∞ yields an LRM-partition of entropy no
larger than that of the LRM-partition P , and one which contains the down-path
(−∞, L0).
Applying this argument to an optimal LRM-partition demonstrates that there
is always an LRM-partition which contains the down-path (−∞, L0). This, in
turn, applied recursively to the subtrees obtained by removing the nodes from
the path (−∞, L0) from T , shows the minimality of the entropy of the left-most
(cid:5)(cid:6)
spinal LRM-partition.

While the deﬁnition of the left-most spinal LRM-partition is constructive, building 
this partition in linear time requires some sophistication, described in the
following lemma:
Lemma 5. Given an LRM-Tree T , there is an algorithm which computes its
left-most spinal LRM-partition in linear overall time (without accessing the original 
array).
Proof. Given an LRM-Tree T (and potentially no access to the array from
which it originated), we ﬁrst set up an array D containing the depths of the
nodes in T , listed in preorder. We then index D for range maximum queries in
linear time using Lemma 2. Since D contains only internal data, the number of
accesses to it matters only to the running time of the algorithm (they are distinct 
from accesses to the array at the construction of T ). Now the deepest node
in T can be found by a range maximum query over the whole array, supported
in constant time. From this node, we follow the path to the root, and save the
corresponding nodes as the ﬁrst subsequence. This divides A into disconnected
subsequences, which can be processed recursively using the same algorithm, as
the nodes in any subtree of T form an interval in D. We do so until all elements
in A have been assigned to a subsequence. Note that, in the recursive steps,
the numbers in D are not anymore the depths of the corresponding nodes in the

294

J. Barbay, J. Fischer, and G. Navarro

remaining subtrees. Yet, as all depths listed in D diﬀer by the same oﬀset from
their depths in any connected subtree, this does not aﬀect the result of the range
(cid:5)(cid:6)
maximum queries.

Note that the left-most spinal LRM-partition is not much more expensive to
compute than the partition into ascending consecutive runs [2]: at most 2(n− 1)
comparisons between elements of the array for the LRM-partition instead of n−1
for the Runs-Partition. Note also that H(LRM) ≤ H(Runs), since the partition
of π into consecutive ascending runs is just one LRM-partition among many.
The concept of LRM-partitions yields a new adaptive sorting algorithm:
Theorem 4. Let π be a permutation of size n, and of LRM-Entropy H(LRM).
The LRM-Sorting algorithm sorts π in a total of at most n(3 + H(LRM)) −
2 comparisons between elements of π and in total running time of O(n(1 +
H(LRM))).
Proof. Obtaining the left-most optimal LRM-partition P composed of runs of
respective lengths LRM through Lemma 5 uses at most 2(n − 1) comparisons
between elements of π and O(n) total running time. Now sorting π is just a
matter of applying Lemma 3: it merges the subsequences of P in n(1 +H(LRM))
additional comparisons between elements of π and O(|LRM| lg |LRM|) additional
internal operations. The sum of those complexities yields n(3 + H(LRM)) − 2
data comparisons, and since |LRM| lg |LRM| < nH(LRM)+lg |LRM| by concavity
of the logarithm, the total time complexity is in O(n(1 + H(LRM))).
(cid:5)(cid:6)
On instances where H(LRM) = H(Runs), LRM-Sorting can actually perform
n − 1 more comparisons than Runs-Sorting, due to the cost of the construction
of the LRM-Tree. Yet, the entropy of the LRM-partition is never larger than the
entropy of the Runs partition (H(LRM) ≤ H(Runs)), which ensures that LRM-
sorting’s asymptotical performance is never worse than Runs-sorting’s performance 
[2]. Furthermore, LRM-Sorting is arbitrarily faster than Runs-Sorting on
permutations with few consecutive inversions, as the lower entropy of the LRMpartition 
more than compensates for the additional cost of computing the LRMTree.
 For instance, for n > 2 odd and π = 1, 3, 2, 5, 4, . . . , 2i + 1, 2i, . . . , n, n − 1,
|Runs| = |LRM| = n/2, Runs = (cid:7)2, . . . , 2(cid:8) and LRM = (cid:7)n/2 + 1, 1, . . . , 1(cid:8), so that
the entropy of LRM is arbitrarily smaller than the one of Runs.
When H(LRM) is much larger than H(SUS), the merging of the LRM-partition
can actually require many more comparisons than the merging of the SUS partition 
produced by Levcopoulos and Petersson’s algorithm [19]. For instance, for
n > 2 even and π = 1 , n/2+1, 2 , n/2+2, . . . , n/2 , n, |LRM| = |Runs| = n/2 and
H(LRM) = lg n
Yet, the high cost of computing the SUS partition (up to n(1 + H(SUS))
additional comparisons within the array, as opposed to only 2(n−1) for the LRM-
partition) means that on instances where H(LRM) ∈ [H(SUS), 2H(SUS) − 1],
LRM-Sorting actually performs fewer comparisons within the array than SUSSorting 
(if only potentially half, given that H(SUS) ≤ H(LRM)). Consider for
instance, for n > 2 multiple of 3, π = 1, 2, n, 3, 4, n − 1, 5, 6, n − 2, . . . 2n/3 + 1:

2 , whereas |SUS| = 2 and H(SUS) = lg 2.

LRM-Trees

295

there LRM = SUS = (cid:7)2n/3 + 1, 1, . . . , 1(cid:8), so that LRM and SUS have the same
entropy, and LRM-sorting outperforms SUS-sorting. A similar reasoning applies
to the comparison of the worst-case performances of LRM-Sorting and SMSSorting.


Another major advantage of LRM-Sorting over SUS and SMS sorting is that
the optimal partition can be computed in linear time, whereas no such linear
time algorithm is known to compute the partition of minimal entropy of π into
Shuﬄed Up-Sequences or Shuﬄed Monotone Sequences; the notation H(SUS) is
deﬁned only as the entropy of the partition of π produced by Levcopoulos and
Petersson’s algorithm [19], which only promises the smallest number of Shuﬄed
Up-Sequences [2].

LRM-Sorting generally improves on both Runs-Sorting and SUS-Sorting in
the number of comparisons performed within the input array. As mentioned in
the Introduction, this is important in cases where the internal data structures
used by the algorithm do ﬁt in main memory, but not the input itself. Furthermore,
 we show in the next section that this diﬀerence in performance implies an
even more meaningful diﬀerence in the size of the compressed data structures
for permutations corresponding to those sorting algorithms.

5 Compressing Permutations

As shown by Barbay and Navarro [2], sorting opportunistically in the comparison
model yields a compression scheme for permutations, and with some more work
a compressed succinct data structure supporting the direct and inverse operators
in time logarithmic on the disorder of the permutation. We show that the sorting
algorithm of Thm. 4 corresponds to a compressed succinct data structure for
permutations which supports the direct and reverse operators in time logarithmic
on its LRM-Entropy (deﬁned in the previous section), while often using less
space than previous solutions. The essential component of our solution is a data
structure for encoding an LRM-partition P . In order to apply Lemma 3, our
data structure must eﬃciently support two operators:
– the operator map(i) indicates, for each position i ∈ [1..n] in the input permutation 
π, the corresponding subsequence s of P , and the relative position
p of i in this subsequence;
– the operator unmap(s, p) is the reverse of map(): given a subsequence s ∈
[1..|LRM|] of P and a position p ∈ [1..ns] in s, it indicates the corresponding
position i in π.
We obviously cannot aﬀord to rewrite the numbers of π in the order described
by the partition, which would use n lg n bits. A naive solution would be to
encode this partition as a string S over alphabet [1..|LRM|], using a succinct data
structure supporting the access, rank and select operators on it. This solution is
not suitable as it would require at the very least nH(Runs) bits only to encode the
LRM-partition, making this encoding worse than the |Runs| compressed succinct
data structure [2]. We describe a more complex data structure which uses less
space, and which supports the desired operators in constant time.

296

J. Barbay, J. Fischer, and G. Navarro

Lemma 6. Let P be an LRM-partition consisting of |LRM| subsequences of respective 
lengths given by the vector LRM, summing to n. There is a succinct data
structure using 2|LRM| lg n + O(|LRM|) + o(n) bits which supports the operators
map and unmap on P in constant time (without accessing the original array).

Proof. The main idea of the data structure is that the subsequences of an LRMpartition 
P for a permutation π are not as general as, say, the subsequences of a
partition into |SUS| up-sequences. For each pair of subsequences (u, v), either the
positions of u and v belong to disjoint intervals of π, or the values corresponding
to u (resp. v) all fall between two values from v (resp. u).
As such, the subsequences in P can be organized into a forest of ordinal trees,
where (1) the internal nodes of the trees correspond to the |LRM| subsequences
of P , organized so that the node u is the parent of the node v if the positions
of the subsequence corresponding to v are contained between two positions of
the subsequence corresponding to u, (2) the children of a node are ordered in
the same order as their corresponding subsequences in the permutation, and
(3) the leaves of the trees correspond to the n positions in π, children of the
internal node u corresponding to the subsequence they belong to.
For instance in Figure 3, the permutation π = (4, 5, 9, 6, 8, 1, 3, 7, 2) has the
LRM-partition (cid:7)4, 5, 6, 8(cid:8),(cid:7)9(cid:8),(cid:7)1, 3, 7(cid:8),(cid:7)2(cid:8), whose encoding can be visualized by
the expression (45(9)68)(137)(2) and encoded by the balanced parenthesis expression 
(()()(())()())(()()())(()) (note that this is a forest, not a tree, hence the
excess of ’(’s versus ’)’s is going to zero several times inside the expression).

(cid:4)4, 5, 6, 8(cid:5)

(cid:4)1, 3, 7(cid:5)

(cid:4)2(cid:5)

(cid:4)9(cid:5)

Fig. 3. Given a permutation π = (4, 5, 9, 6, 8, 1, 3, 7, 2),
its LRM-partition
(cid:4)4, 5, 6, 8(cid:5), (cid:4)9(cid:5),(cid:4)1, 3, 7(cid:5),(cid:4)2(cid:5) can be visualized by the expression (45(9)68)(137)(2) and
encoded as a forest

Given a position i ∈ [1..n] in π, the corresponding subsequence s of P is
simply obtained by ﬁnding the parent of the i-th leaf, and returning its preorder
rank among internal nodes. The relative position p of i in this subsequence is
given by the number of its left siblings which are leaves. Conversely, given the
rank s ∈ [1..|LRM|] of a subsequence in P and a position p ∈ [1..ns] in this
subsequence, the corresponding position i in π is computed by ﬁnding the s-th
internal node in preorder, selecting its p-th child which is a leaf, and computing
the preorder rank of this node among all the leaves of the tree.

We represent such a forest using the structure of Jansson et al. [17] by adding
a fake root node to the forest. The only operation it does not support is counting
the number of leaf siblings to the left of a node, and ﬁnding the p-th leaf child of
a node. Jansson et al.’s structure [17] encodes a DFUDS representation [4] of the
tree, where each node with d children is represented as d opening parentheses

LRM-Trees

297

followed by a closing parenthesis: “(··· ()”. Thus we set up an additional bitmap,
of the same length and aligned to the parentheses string of Jansson et al.’s
structure, where we mark with a one each opening parenthesis that corresponds
to an internal node (the remaining parentheses, opening or closing, are set to
zero). Then the operations are easily carried out using rank and select on this
bitmap and the one from Jansson et al.’s structure.
Since the forest has n leaves and |LRM| internal nodes, Jansson et al.’s struc-
(cid:3)
≤
ture [17] takes space H∗+o(n) bits, where H∗ = lg
n,n1,...,nn−1
(n + |LRM|)|LRM|(cid:4)
(cid:3)
= |LRM| lg(n + |LRM|) = |LRM| lg n + O(|LRM|). On the
lg
other hand, the bitmap that we added is of length 2(n + |LRM|) ≤ 4n and has
exactly |LRM| 1s, and thus a compressed representation [24] requires |LRM| lg n+
O(|LRM|) + o(n) additional bits.
(cid:5)(cid:6)
Given the data structure for LRM-partitions from Lemma 6, and applying the
merging data structure from Lemma 3 immediately yields a compressed succinct
data structure for permutations. Note that the index and the data are interwoven
in a single data structure (i.e., this encoding is not a succinct index [1]), so
we express the complexity of its operators as a single measure (as opposed to
previous ones, for which we distinguished data and index complexity).

(cid:4) ≤ lg (n+|LRM|)!

n+|LRM|

n!

Theorem 5. Let π be a permutation of size n, such that it has an optimal LRMpartition 
of size |LRM| and entropy H(LRM). There is a compressed succinct
data structure using (1 + H(LRM))(n + o(n)) + O(|LRM| lg n) bits, supporting
the computation of π(i) and π−1(i) in time O(1 + lg |LRM|) in the worst case
∀i ∈ [1..n], and in time O(1+H(LRM)) on average when i is chosen uniformly at
random in [1..n]. It can be computed in at most n(3 +H(LRM))− 2 comparisons
in π and total running time of O(n(1 + H(LRM))).
Proof. Lemma 6 yields a data structure for an optimal LRM-partition of π
using 2|LRM| lg n + O(|LRM|) + o(n) bits, and supports the map and unmap
operators in constant time. The merging data structure from Lemma 3 requires
(1 + H(LRM))(n + o(n)) + O(|LRM| lg n) bits, and supports the operators π()
and π−1() in the time described, through the additional calls to the operators
(cid:5)(cid:6)
map() and unmap(). The latter space is asymptotically dominant.

References

1. Barbay, J., He, M., Munro, J.I., Rao, S.S.: Succinct indexes for strings, binary relations,
 and multi-labeled trees. In: Proc. SODA, pp. 680–689. ACM/SIAM (2007)
2. Barbay, J., Navarro, G.: Compressed representations of permutations, and applications.
 In: Proc. STACS, pp. 111–122. IBFI Schloss Dagstuhl (2009)

3. Bender, M.A., Farach-Colton, M., Pemmasani, G., Skiena, S., Sumazin, P.: Lowest
common ancestors in trees and directed acyclic graphs. J. Algorithms 57(2), 75–94
(2005)

4. Benoit, D., Demaine, E.D., Munro, J.I., Raman, R., Raman, V., Rao, S.S.: Representing 
trees of higher degree. Algorithmica 43(4), 275–292 (2005)

298

J. Barbay, J. Fischer, and G. Navarro

5. Brodal, G.S., Davoodi, P., Rao, S.S.: On space eﬃcient two dimensional range
minimum data structures. In: de Berg, M., Meyer, U. (eds.) ESA 2010. LNCS,
vol. 6347, pp. 171–182. Springer, Heidelberg (2010)

6. Chen, K.-Y., Chao, K.-M.: On the range maximum-sum segment query problem.
In: Fleischer, R., Trippen, G. (eds.) ISAAC 2004. LNCS, vol. 3341, pp. 294–305.
Springer, Heidelberg (2004)

7. Crochemore, M., Iliopoulos, C.S., Kubica, M., Rahman, M.S., Walen, T.: Improved
algorithms for the range next value problem and applications. In: Proc. STACS,
pp. 205–216. IBFI Schloss Dagstuhl (2008)

8. Daskalakis, C., Karp, R.M., Mossel, E., Riesenfeld, S., Verbin, E.: Sorting and

selection in posets. In: Proc. SODA, pp. 392–401. ACM/SIAM (2009)

9. Fischer, J.: Optimal succinctness for range minimum queries. In: L´opez-Ortiz, A.

(ed.) LATIN 2010. LNCS, vol. 6034, pp. 158–169. Springer, Heidelberg (2010)

10. Fischer, J., Heun, V., St¨uhler, H.M.: Practical entropy bounded schemes for O(1)-
range minimum queries. In: Proc. DCC, pp. 272–281. IEEE Press, Los Alamitos
(2008)

11. Fischer, J., M¨akinen, V., Navarro, G.: Faster entropy-bounded compressed suﬃx

trees. Theor. Comput. Sci. 410(51), 5354–5364 (2009)

12. G´al, A., Miltersen, P.B.: The cell probe complexity of succinct data structures.

Theor. Comput. Sci. 379(3), 405–417 (2007)

13. Golynski, A.: Optimal lower bounds for rank and select indexes. Theor. Comput.

Sci. 387(3), 348–359 (2007)

14. Grossi, R., Gupta, A., Vitter, J.S.: High-order entropy-compressed text indexes.

In: Proc. SODA, pp. 841–850. ACM/SIAM (2003)

15. Huﬀman, D.: A method for the construction of minimum-redundancy codes. Proceedings 
of the I.R.E. 40, 1090–1101 (1952)

16. Jacobson, G.: Space-eﬃcient static trees and graphs. In: Proc. FOCS, pp. 549–554.

IEEE Computer Society, Los Alamitos (1989)

17. Jansson, J., Sadakane, K., Sung, W.-K.: Ultra-succinct representation of ordered

trees. In: Proc. SODA, pp. 575–584. ACM/SIAM (2007)

18. Knuth, D.E.: Art of Computer Programming, 2nd edn. Sorting and Searching,

vol. 3. Addison-Wesley Professional, Reading (1998)

19. Levcopoulos, C., Petersson, O.: Sorting shuﬄed monotone sequences. Inf. Comput.
 112(1), 37–50 (1994)

20. M¨akinen, V., Navarro, G.: Implicit compression boosting with applications to selfindexing.
 In: Ziviani, N., Baeza-Yates, R. (eds.) SPIRE 2007. LNCS, vol. 4726, pp.
229–241. Springer, Heidelberg (2007)

21. Munro, J.I., Raman, R., Raman, V., Rao, S.S.: Succinct representations of permutations.
 In: Baeten, J.C.M., Lenstra, J.K., Parrow, J., Woeginger, G.J. (eds.)
ICALP 2003. LNCS, vol. 2719, pp. 345–356. Springer, Heidelberg (2003)

22. Navarro, G., M¨akinen, V.: Compressed full-text indexes. ACM Computing Surveys 
39(1), Article No. 2 (2007)

23. Pˇatra¸scu, M.: Succincter. In: Proc. FOCS, pp. 305–313. IEEE Computer Society,

Los Alamitos (2008)

24. Raman, R., Raman, V., Rao, S.S.: Succinct indexable dictionaries with applications
to encoding k-ary trees and multisets. ACM Transactions on Algorithms 3(4),
Art. 43 (2007)

25. Sadakane, K., Grossi, R.: Squeezing succinct data structures into entropy bounds.

In: Proc. SODA, pp. 1230–1239. ACM/SIAM (2006)

26. Sadakane, K., Navarro, G.: Fully-functional succinct trees. In: Proc. SODA, pp.

134–149. ACM/SIAM (2010)

