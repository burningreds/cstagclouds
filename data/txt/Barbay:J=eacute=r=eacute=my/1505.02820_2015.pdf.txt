Reﬁning the Analysis of

Divide and Conquer:

How and When

J´er´emy Barbay1, Carlos Ochoa1(cid:63), and Pablo P´erez-Lantero2

1 Departamento de Ciencias de la Computaci´on, Universidad de Chile, Chile

2 Escuela de Ingenier´ıa Civil Inform´atica, Universidad de Valpara´ıso, Chile.

jeremy@barbay.cl, cochoa@dcc.uchile.cl.

pablo.perez@uv.cl.

5
1
0
2

 

p
e
S
5
2

 

 
 
]
S
D
.
s
c
[
 
 

3
v
0
2
8
2
0

.

5
0
5
1
:
v
i
X
r
a

summing to n, where the entropy function H(n1, . . . , nk) = (cid:80)k

Abstract. Divide-and-conquer is a central paradigm for the design of algorithms, through which some
fundamental computational problems, such as sorting arrays and computing convex hulls, are solved
in optimal time within Θ(n log n) in the worst case over instances of size n. A ﬁner analysis of those
problems yields complexities within O(n(1 + H(n1, . . . , nk))) ⊆ O(n(1+ log k)) ⊆ O(n log n) in the
worst case over all instances of size n composed of k “easy” fragments of respective sizes n1, . . . , nk
measures the “diﬃculty”
of the instance. We consider whether such reﬁned analysis can be applied to other algorithms based
on divide-and-conquer, such as polynomial multiplication, input-order adaptive computation of convex
hulls in 2D and 3D, and computation of Delaunay triangulations.

ni
n log n
ni

i=1

Keywords: Adaptive Analysis, Convex Hull, Delaunay Triangulation, Divide and Conquer,
Voronoi Diagrams.

1

Introduction

The divide-and-conquer paradigm is used to solve central computational problems such as Sorting arrays 
[10], computing Convex Hulls [9], Delaunay Triangulations and Voronoi Diagrams [6] of
points in the plane and in higher dimensions, Matrix Multiplication [19], Polynomial Multiplication 
[16], Half-Plane Intersection [15], among others. For some of these problems, this paradigm yields
an optimal running time within Θ(n log n) in the worst case over instances of n elements.
An adaptive analysis of slight variants of some of these algorithms yields improved running times on large
classes of instances. Those results can be reﬁned [1,2,14] up to complexities within O(n(1+H(n1, . . . , nk))) ⊆
O(n(1+ log k)) ⊆ O(n log n) in the worst case over all instances of size n composed of k “easy” fragments
ni
n log n
ni
measures the “diﬃculty” of the instance. We describe here two examples for the problem of Sorting arrays
(see Section 2 for more).

of respective sizes n1, . . . , nk summing to n, where the entropy function H(n1, . . . , nk) = (cid:80)k

i=1

of distinct elements in S and m1, . . . , mσ (such that(cid:80)σ

– Munro and Spira [14] showed that the algorithm MergeSort can be adapted to sort a multiset S of n
elements in time within O(n(1 +H(m1, . . . , mσ))) ⊆ O(n(1+ log σ)) ⊆ O(n log n), where σ is the number
i=1 mi = n) are the ρ multiplicities of the distinct

elements in S, respectively.

– Taking advantage of the input order, Knuth [10] considered sequences formed by runs i.e., contiguous
increasing subsequences, and described an algorithm sorting such sequences in time within O(n(1 +
log ρ)) ⊆ O(n lg n), where ρ is the number of runs in the sequence. Barbay and Navarro [2] improved the
analysis of this algorithm in time within O(n(1 + H(r1, . . . , rρ))) ⊆ O(n(1+ log ρ)) ⊆ O(n log n), where

r1, . . . , rρ (such that(cid:80)ρ

i=1 ri = n) are the sizes of these runs.

(cid:63) Corresponding author.

2 Similar analysis techniques have been applied to some other problems, though only partially. Following an
example for the problem of computing the Convex Hull in the plane (see Section 3.2 for more): Levcopoulos
et al. [11] described an adaptive algorithm for computing the Convex Hull of a polygonal chain. The
algorithm takes advantage of the minimum number κ of simple subchains into which the polygonal chain can
be partitioned. They showed that the time complexity of the algorithm is within O(n(1+log κ)) ⊆ O(n log n).

Hypothesis. Which similar reﬁnements can be applied to which divide-and-conquer algorithms, if any, and
to what depth?

Our Results. In Section 2, we list and classify previous reﬁned analyses between those that are Structure
Based and those Input-Order Based. In Section 3.1, we describe a reﬁned analysis of the principal step in the
algorithm for Polynomial Multiplication using the Fast Fourier Transformation. In Sections 3.2 and
3.3, we describe two distinct reﬁned analyses for problems in computational geometry, which yield various
optimal input-order adaptive results on the computation of Convex Hulls, Delaunay Triangulations,
and Voronoi Diagrams in the plane. In Section 3.2, we reﬁne the analysis of Levcopoulos et al. [11]’s
algorithm for computing the Convex Hull of polygonal chains in time within O(n(1 + H(n1, . . . , nκ))) ⊆
O(n(1+ log κ))) ⊆ O(n log n), where n1, . . . , nκ are the lengths of the subchains of a partition of a polygonal
chain of n points into the minimum number κ of simple subchains. In Section 3.3, we describe a reﬁned
analysis of the computation of Voronoi Diagrams and Delaunay Triangulations for sequences S
formed by n points, which yields a time complexity within O(n(1 + H(v1, . . . , vµ))) ⊆ O(n(1+ log µ)) ⊆
O(n log n), where v1, . . . , vµ are the sizes of the minimum number µ of monotone histograms in which S
can be cut, with respect to two ﬁxed orthogonal directions and show, as a corollary, an upper bound for
computing the Convex Hull of a sequence S formed by n points in time within O(n(1 +H(v1, . . . , vµ))) ⊆
O(n(1+ log µ)) ⊆ O(n log n), where v1, . . . , vµ are the sizes of the minimum number µ of monotone histograms
in which S can be cut, with respect to two ﬁxed orthogonal directions. In Section 4, we describe some more
diﬃcult applications of such reﬁned analyses. In Section 5, we discuss the possibility of designing algorithms
which analyses combine Structure Based and Input-Order Based results synergically as opposed to running
them in parallel.

2 Classiﬁcation of Results

We review here some results on the reﬁned analysis of algorithms for Sorting arrays and for computing
planar Convex Hulls, Delaunay Triangulations and Voronoi Diagrams. We classify the various
reﬁned analysis between those focusing on the structure of the instance (Section 2.1) versus those focusing
on the order in which the input is given (Section 2.2). There does not seem to be any example where both
strategies are mixed: we discuss the potential for those in Section 5.

2.1 Structure Based Results

By “Structure Based Results” we mean algorithms taking advantage of the structure of the instance, for
example, taking advantage of the frequencies of the values in a multiset or of the relative positions of the
points in a set. Such results are known for Sorting multisets and for computing Convex Hulls.

MergeSort is a divide-and-conquer Sorting algorithm in the comparison model. This algorithm relies

that (cid:80)σ
O(n(1+ log σ)) ⊆ O(n log n) for Sorting a multiset, where H(m1, . . . , mσ) =(cid:80)σ

on a linear time merge process, that combines two ordered sequences into a single ordered sequence.
Concerning the problem of Sorting, Munro and Spira [14] considered the task of Sorting a multiset
S = {x1, . . . , xn} of n real numbers with σ distinct values, of multiplicities m1, . . . , mσ, respectively, so
i=1 mi = n. They showed that adding counters to various classical algorithms (among which the
divide-and-conquer based algorithm MergeSort) yields a time complexity within O(n(1+H(m1, . . . , mσ))) ⊆
measures the
entropy of the distribution of the multiplicities (cid:104)m1, . . . , mσ(cid:105). This result takes advantage of the frequencies
of the values i.e., the structure of the instance.

mi
n log n
mi

i=1

3
Given a set P of n points, the Convex Hull of P is the smallest convex set containing P (see Figure 1).
Considering the problem of computing the Convex Hull, Kirkpatrick and Seidel [9] described an algorithm
to compute the Convex Hull of a set of n planar points in time within O(n(1 + log h)) ⊆ O(n log n), where
h is the number of vertices in the Convex Hull. The algorithm relies on a variation of the divide-andconquer 
paradigm, which they call the “Marriage-Before-Conquest” principle. For computing the upper hull,
the algorithm ﬁnds a vertical line that divides the input point set into two approximately equal-size parts
in liner time. Next, it determines the edge of the upper hull that intersects this line in linear time. It then
eliminates the points that lie underneath this edge and ﬁnally applies the same procedure to the two sets of
the remaining points on the left and right side of the vertical line. A similar algorithm computes the lower
hull. Afshani et al. [1] reﬁned the complexity analysis of this algorithm to within O(n(1 +H(n1, . . . , nh))) ⊆
O(n(1+ log h)) ⊆ O(n log n), where n1, . . . , nh are the sizes of a partition of the input, such that every element
of the partition is a singleton or can be enclosed by a triangle whose interior is completely below the upper
hull of the set, and H(n1, . . . , nh) has the minimum possible value (minimum entropy of the distribution of
the points into a certiﬁcate of the instance). This result takes advantage of the positions of the points i.e.,
the structure of the instance.

2.2

Input-Order Based Results

By “Input-Order Based Results” we mean algorithms taking advantage of the order of the input, for example,
taking advantage of the order of the values in a sequence of numbers or of the order in which the points
are given in a polygonal chain. We describe only a sampling of such results on the problem of Sorting
permutations (see the survey from Moﬀat and Petersson [13] for more), revisit some results on the computation 
of Convex Hulls in the plane and 3D space, and show that those results in the plane are actually
only “Input-Order Based”. Those results for computing Convex Hulls in 3D space show that no algorithm 
can take advantage of the position of the points i.e., structure based, in order to compute Delaunay
triangulations in the plane.

Concerning the problem of Sorting, Knuth [10] described an adaptive sorting algorithm that takes
advantage of permutations formed by sorted blocks called runs, that is, subsequences of consecutive positions
in the input with a positive gap between successive values, from beginning to end. He showed that the time
complexity of this algorithm is within O(n(1+ log ρ)) ⊆ O(n lg n), where ρ is the number of runs in the
permutation (e.g. (1,2,6,7,8,9, 3, 4, 5) is composed of 2 such sorted blocks (1, 2, 6, 7, 8, 9) and (3, 4, 5)). Barbay
and Navarro [2] reﬁned the algorithm described by Knuth [10]. They included in the analysis not only the
number of runs but also their sizes, The main idea is to detect the runs ﬁrst and then merge them pairwise,
using a mergesort-like step. The detection of ascending runs can be done in linear time by a scanning process
identifying the positions i in π such that π(i) > π(i + 1). Merging the two shortest runs at each step further
reduces the number of comparisons, making the running time of the merging process adaptive to the entropy
of the sequence of the lengths of the runs. The merging process is then represented by a tree with the shape
of a Huﬀman [7] tree, built from the distribution of the runs sizes. They extend this result to mix ascending
and descending runs, showing that, if the permutation π is formed by ρ runs of sizes given by the vector
(cid:104)r1, . . . , rρ(cid:105), then π can be sorted in time within O(n(1 +H(r1, . . . , rρ))) ⊆ O(n(1+ log ρ)) ⊆ O(n log n). This
result takes advantage of the order of the values in the input i.e., the input order.

Considering the computation of the Convex Hull in the plane, Levcopoulos et al. [11] described a divide-
and-conquer algorithm for computing the Convex Hull of a polygonal chain. The algorithm is based in the
fact that the Convex Hull of a simple chain can be computed in linear time, and that deciding whether a
given chain is simple can be done in linear time. They measured the complexity of this algorithm in terms
of the minimum number of simple subchains κ into which the chain can be cut. They showed that the
time complexity of this algorithm is within O(n(1+ log κ)) ⊆ O(n log n). We improve the analysis of this
algorithm including not only the minimum number of simple subchain into which the polygonal chain can
be partitioned but also their sizes (see Section 3.2). This result takes advantage of the order in which the
points are given i.e., the input order.

Given a set P of n planar points, a triangulation of P is a subdivision of the convex hull of P into
triangles with vertex set the set P . Concerning the computation of the Convex Hull in 3D, a related

4

Fig. 1. A point set P . a) The convex hull of P , b) the Delaunay triangulation of P , and c) the Delaunay triangulation
and the Voronoi diagram of P .

concept is that of the Delaunay Triangulation DT (P ) of a point set P in the plane, a triangulation
where for every edge e there exists a disk C with the following properties: (i) the endpoints of edge e are on
the boundary of C, and (ii) no other point of P is in the interior of C: it is named after Boris Delaunay for
his work on this topic in 1934. An equivalent deﬁnition is such that no point in P is inside the circumcircle
of any triangle of DT (P ). Computing the Delaunay Triangulation is equivalent to computing its dual,
called the Voronoi Diagram: each one can be constructed from the other in linear time [15] (see Figure 1).
Computing the Delaunay Triangulation of a set of points in two dimensions reduces to computing the
Convex Hull in three dimensions of the projections of those points on an hyperbolic plane. The projection
of P onto the unit elliptic paraboloid z = x2 + y2 yields a point set P (cid:48). The Convex Hull CH(P (cid:48)) of
P (cid:48) contains every point of the set. The downward-facing facet of CH(P (cid:48)) are those whose normal vectors
have a negative z-value. Projecting the edges of downward-facing facet in CH(P (cid:48)) onto the plane yields the
Delaunay triangulation of P . By showing tight bounds for input-order oblivious i.e., structure based,
algorithms for computing Convex Hulls in three dimensions, Afshani et al. [1] indirectly proved that no
planar Delaunay Triangulation algorithm can take advantage of the position of the points.

Theorem 1 (Afshani et al. [1]). Consider a set of n points in the plane. For any algorithm A computing
the Delaunay triangulation in the algebraic decision tree model, A performs in time within Ω(n log n) on
average on a random order of the points. This implies that there is an order of those points for which A
performs in time within Ω(n log n).

We describe in Sections 3 and 4 some algorithms taking advantage of the order of the input to compute
Delaunay Triangulations and Voronoi Diagrams, among other desirable objects in computational
geometry.

3 Reﬁned Analysis: Three Examples

We show in this section how to reﬁne the analysis of the principal step in the algorithm for multiplying
polynomials using the Fast Fourier Transformation [16], how to reﬁne the analysis of the algorithm from
Levcopoulos et al. [11] for the decomposition of a polygonal chain into simple sequences, and how to extend
this analysis to the computation of Voronoi diagrams and Delaunay triangulations for another
measure of diﬃculty based on monotone histograms.

3.1 Polynomial Multiplication: Adaptivity to Zero-coeﬃcients

Given two polynomials A = (a0, . . . , an−1) and B = (b0, . . . , bn−1) described by their coeﬃcients, the polynomial 
multiplication problem is to compute the coeﬃcients of the polynomial C = A · B. The approach

a)b)c)5
to multiplying polynomials using the Fast Fourier Transformation [16] can be divided into three steps: (i)
evaluate the polynomials A and B in 2n values (the (2n)th roots of the unity); (ii) evaluate C in these 2n
values by multiplying the evaluations of A and B; and (iii) obtain the coeﬃcients of C by interpolation
using the values computed in the step (ii). The steps (i) and (iii) are accomplished by a divide-and-conquer
algorithm for polynomial evaluation. We reﬁne the analysis of the divide-and-conquer polynomial evaluation
algorithm to take advantage of the number of zero-coeﬃcients and of their relative positions in the vector of
coeﬃcients that describes the polynomial.

Given a polynomial A, the polynomial evaluation algorithm deﬁnes two polynomials, Aeven and Aodd,
that consist of the even-indexed and odd-indexed coeﬃcients of A, respectively. Hence, A(x) = Aeven(x2) +
x× Aodd(x2). If x is one of the (2n)th roots of the unity, then x2 is one of the nth roots of the unity. In order
to evaluate the polynomial A on each of the (2n)th roots of the unity, the recursive procedure divides A into
Aeven and Aodd, evaluates Aeven and Aodd in the nth roots of the unity, and once these values are computed,
evaluate A on each of the (2n)th roots of the unity using the formula A(x) = Aeven(x2) + x × Aodd(x2).
The time complexity T (n) of this algorithm follows the recurrence T (n) ≤ T ( n
2 ) + O(n), which yields a time
complexity within O(n log n). If at one step of the recursion call all the coeﬃcients are zero, the algorithm
ﬁnishes the computation at this branch (see Figure 2).

Fig. 2. Vector formed by the coeﬃcients of the polynomial A(x) = 4 + 3x3 + 2x4 + x7 and the recursion tree of the
divide-and-conquer evaluation algorithm. At each step, the algorithm divides the coeﬃcients of A into Aeven and
Aodd.

Given a polynomial A = (a0, . . . , an−1), we deﬁne the equivalence relation E between the positions of the
zero-coeﬃcients in A: two positions of zeros p and q are equivalent if and only if (i) there exists k ∈ Z+ such
that p ≡ q (mod 2k); (ii) all the positions r such that r ≡ p (mod 2k) are zeros; and (iii) there exist a position
t such that t ≡ p (mod 2k−1) and the value in t is diﬀerent from zero. The idea of this equivalence relation
is to group the positions in classes such that the positions in the same class form a vector where all the
coeﬃcients are zero in a node of the recursion tree. Let ζ and η be the number of zeros in the vector formed
by the coeﬃcients of A and the number of equivalence classes deﬁned by E, respectively. Let (cid:104)n1, . . . , nη(cid:105)
i=1 ni = ζ. The following
theorem sets the reﬁned analysis in function of ζ and the vector (cid:104)n1, . . . , nη(cid:105).
Theorem 2. The complexity of the evaluation algorithm for polynomials of n coeﬃcients, number of zerocoeﬃcients 
ζ and vector (cid:104)n1, . . . , nη(cid:105) formed by the sizes of the equivalence classes deﬁned by E is within

be the vector formed by the sizes of the equivalence classes deﬁned by E. Then,(cid:80)η
O((n − ζ) log n +(cid:80)η

i=1 ni log n
ni

) ⊆ O(n log n).

In the following sections, we apply similar techniques to obtain optimal reﬁned analysis for input-order

adaptive algorithms computing Convex Hulls and Delaunay triangulations in the plane.

3.2 Computing Convex Hulls: Adaptivity to Simple Subchains

A polygonal chain is a curve speciﬁed by a sequence of points p1, p2, . . . , pn. The curve itself consists of the
line segments connecting the pairs of consecutive points. A polygonal chain C is simple if any two edges of
C that are not adjacent are disjoint, or if the intersection point is a vertex of C; and any two adjacent edges

(4,0,0,3,2,0,0,1)(4,0,2,0)(0,3,0,1)(0,0)(4,2)(4)(2)(3,1)(0,0)(3)(1)6share only their common vertex. Melkman [12] described an algorithm that computes the Convex Hull
of a simple polygonal chain in linear time, and Chazelle [3] described an algorithm for testing whether a
polygonal chain is simple in linear time.

Levcopoulos et al. [11] combined these results to yield an adaptive divide-and-conquer algorithm for
computing the Convex Hull of polygonal chains. The algorithm tests if the chain C is simple, using
Chazelle [3]’s algorithm: if the chain C is simple, the algorithm computes the Convex Hull of C in
linear time, using Melkman [12]’s algorithm. Otherwise, if C is not simple, the algorithm cuts C into the
subsequences C(cid:48) and C(cid:48)(cid:48), whose sizes diﬀer at most in one; recurses on each of them; and merges the
resulting Convex Hulls using Preparata and Shamos’s algorithm [15]. They measured the complexity
of this algorithm in terms of the minimum number of simple subchains κ into which the chain C can be
cut. Let t(n, κ) be the worst-case time complexity taken by this algorithm for an input chain of n vertices
that can be cut into κ simple subchains. They showed that t(n, κ) satisﬁes the following recursion relation:
t(n, κ) ≤ t((cid:100) n
O(n log n). In the sequel, this algorithm will be named as Test-And-Divide.

2(cid:99), κ2), κ1+κ2 ≤ κ+1. The solution to this recursion gives t(n, κ) ∈ O(n(1+ log κ)) ⊆

2(cid:101), κ1)+t((cid:98) n

The Test-And-Divide algorithm partitions the input chain into simple subchains. If it was possible to
partition the input chain into the minimum number κ of simple subchains in linear time, then the same
approach described by Barbay and Navarro [2] could be applied to obtain a reﬁned analysis in function of
O(n(1 + H(n1, . . . , nκ))) ⊆ O(n(1+ log κ)) ⊆ O(n log n). But, as far as we know, there does not exist any
linear time algorithm to accomplish this task.

In the recursion tree of the execution of the Test-And-Divide algorithm on input C of n points, every
node represents a subchain of C. The cost of every node is linear in the size of the subchain that it represents.
The simplicity test and the merge process are both linear in the number of points in the subchain. When
this subchain is simple the node that represents this subchain is a leaf. Every time this algorithm discovers
that the polygonal chain is simple, it executes a number of operations linear in the size of the chain and the
corresponding node in the recursion tree becomes a leaf.

Width Analysis: A Warm-up. Consider for illustration the particular case of a polygonal chain C of
n = 2m planar points such that C can be partitioned into the minimum number of simple subchains κ = m
of lengths 21, 21, 22, 23 . . . , 2m−1, respectively. Every time the algorithm cuts the current chain in half, the
right subchain is simple and the recursive call is made only in the left subchain. Hence, the recursion tree
of the algorithm on input C has only two nodes per level, one of which is a leaf (see Figure 3). The overall
running time of the algorithm on C is then within O(n).

Fig. 3. The recursion tree of A on C. Each node represents a recursive call. Noted in each node is the asymptotic
complexities of the simplicity test and the merging process on the subchain that it represents.

Deﬁnition 1 (Width). The width ω of the recursion tree in the execution of the Test-And-Divide algorithm 
on input C is the maximum number of nodes at any level.

O(n)O(n2)O(n2)O(n4)O(n8)O(n4)O(1)O(1)7
Levcopoulos et al. [11] analyzed the complexity of this algorithm in the worst case over instances of ﬁxed
size n and κ. The following lemma gives an alternate analysis in the worst case over instances of ﬁxed size
n and width ω.

Lemma 1. Let ω be the width of the recursion tree in the execution of the Test-And-Divide algorithm on
input C of n planar points. The complexity of this algorithm on input C is within O(ωn).

Reﬁned Analysis. Let (cid:104)(cid:96)1, . . . , (cid:96)m(cid:105) be the vector formed by the sizes of the subchains represented by the
i=1 (cid:96)i = n). The
number of operations “saved” by the algorithm every time it discovers a leaf of size (cid:96)i is within Ω((cid:96)i log (cid:96)i)
(the cost of the subtree of the perfect binary tree rooted in a node of size (cid:96)i) minus O((cid:96)i) (the operations in
the leaf are not saved). The time complexity T (C) of this algorithm on input C is within O(n log n) (the cost
i=1 (cid:96)i log (cid:96)i − (cid:96)i) (the number of operations saved by the algorithm).
i=1((cid:96)i log (cid:96)i − (cid:96)i)) = O(n(1 + H((cid:96)1, . . . , (cid:96)m))) ⊆ O(ωn) ∩ O(n log m) ⊆ O(n log n).

m leaves of the recursion tree of the Test-And-Divide algorithm on input C (such that(cid:80)m
of the perfect binary tree) minus Ω((cid:80)m
So, T (C) ⊆ O(n log n −(cid:80)m
the recursion tree in the execution of the algorithm A on input C of n =(cid:80)m

width and the number of leaves in the recursion tree.
Lemma 2. Let (cid:104)(cid:96)1, . . . , (cid:96)m(cid:105) be the vector formed by the sizes of the subchains represented by the m leaves of
1 (cid:96)i planar points. Let ω be the
maximum width of the recursion tree. The time complexity of A on C is within O(n(1 + H((cid:96)1, . . . , (cid:96)m))) ⊆
O(ωn) ∩ O(n log m) ⊆ O(n log n).

The following lemma summarizes this ﬁner analysis of the Test-And-Divide algorithm in function of the

Is there a relationship between the vector (cid:104)(cid:96)1, . . . , (cid:96)m(cid:105) formed by the sizes of the subchains represented
by the leaves of the recursion tree in the execution of the Test-And-Divide algorithm from on input C and
the vector (cid:104)n1, . . . , nκ(cid:105) formed by the sizes of a partition of C into κ simple subchains?

For a given polygonal chain, there can be several partitions into simple subchains of minimum size κ
for it. The Levcopoulos et al.’s analysis is in the worst case over instances for n and κ ﬁxed. We describe
below a reﬁned analysis which takes into account the relative imbalance between the sizes of the subchains.
The idea behind the reﬁnement is to bound the number of operations that the algorithm executes for every
simple subchain. This analysis makes it possible to identify families of instances where the complexity of the
algorithm is linear even though the number of simple subchains into which the chain is split is logarithmic.
Theorem 3. Let (cid:104)n1, . . . , nκ(cid:105) be the vector formed by the sizes of the subchains of any partition Π of the
chain C into the minimum number κ of simple subchains. The time complexity of the Test-And-Divide
algorithm on input C is within O(n(1 + H(n1, . . . , nκ))) ⊆ O(n(1+ log κ)) ⊆ O(n log n), which is worst-case
optimal in the comparison model over instances of n points that can be partitioned into κ simple subchains
of sizes (cid:104)n1, . . . , nκ(cid:105).
Proof. Fix the subchain ci of size ni in Π. In the worst case, the algorithm considers the ni points of ci for
the simplicity test, and the merging process in all the levels of the recursion tree from the ﬁrst level to the
(cid:101) + 1, because the sizes of the subchains in these levels are greater than ni. In the next level, one
level (cid:100)log n
of the nodes (cid:96) of the recursion tree ﬁts completely inside ci and therefore it becomes a leaf. Hence, at least
ni
4 points from ci are dismissed for the following iterations. The remaining points of ci are in the left or the
right ends of subchains represented by nodes in the same level of (cid:96) in the recursion tree. In all of the following
levels, the number of operations of the algorithm involving points from ci can be bounded by the size of the
subchains in those levels. So, the sum of the number of these operations in these levels is within O(ni). As
a result, the number of operations of the algorithm involving points from ci is within O(ni log n
+ ni). In
) = O(n(1 + H(n1, . . . , nκ))) ⊆
ni
O(n(1+ log κ)) ⊆ O(n log n).
We prove the optimality of this complexity by giving a tight lower bound. Barbay and Navarro [2] showed
a lower bound of Ω(n(1 +H(r1, . . . , rρ))) in the comparison model for Sorting a sequence of n numbers, in
the worst case over instances covered by ρ runs (increasing or decreasing) of lengths r1, . . . , rρ, respectively,
summing to n. The Sorting problem can be reduced in linear time to the problem of computing the

total, the time complexity of the algorithm is within O(n +(cid:80)κ

i=1 ni log n
ni

ni

8Convex Hulls of a chain of n planar points that can be cut into ρ simple subchains of lengths r1, . . . , rρ,
respectively. For each real number r, this is done by producing a point with (x, y)-coordinates (r, r2). The
ρ runs (alternating increasing and decreasing) are transformed into ρ simple subchains of the same lengths.
The sorted sequence of the numbers can be obtained from the Convex Hull of the points in linear time. (cid:117)(cid:116)

3.3 Computing Delaunay Triangulations: Adaptivity to Monotone Histograms

We propose a new input-order based algorithm for computing the Delaunay triangulation and the
Voronoi diagram of a set of n planar points. A monotone histogram is a sequence of points sorted with
respect to two orthogonal directions. The algorithm takes advantage of the minimal number µ of monotone
histograms and their sizes into which a polygonal chain can be cut. A monotone histogram is also a simple
polygonal chain. Therefore, an algorithm for computing Convex Hulls adaptive to the decomposition of
the input into monotone histograms is obtained as a corollary of Theorem 3. We extend the reﬁned analysis
to the computation of Delaunay triangulations and Voronoi diagrams adaptive to the decomposition
of the input into monotone histograms.

Djidjev and Lingas [4] described an algorithm which, given a monotone histogram, computes the Voronoi
diagram (and hence the Delaunay triangulation) of the input sequence in linear time. This algorithm
suggests a way to partition the input into subsequences such that the Delaunay triangulation of each
subsequence can be computed in linear time in its length. The partitioning algorithm cuts the sequence into
µ monotone histograms with respect to two orthogonal directions d1 and d2. The ﬁrst two points of the
subsequence determine the ordering deﬁned by the combination of ascending and descending with respect
to d1 and d2. Given a sequence of points and two orthogonal directions, it is possible to test whether the
sequence is a monotone histograms with respect to these two directions in linear time.

Binary Merge of Voronoi Diagrams. Kirkpatrick [8] described a linear time algorithm for the merging
of two arbitrary Voronoi diagrams. Given the Voronoi diagrams of two disjoint point sets P and Q,
the algorithm ﬁnds the Voronoi diagram of P ∪ Q in time within O(|P| + |Q|). The plane is partitioned
into points closer to P , points closer to Q, and points equidistant from P and Q. The points equidistant
from P and Q are deﬁned as the contour separating P and Q. The contour is composed of straight line
segments: it is formed from the edges of the Voronoi diagram of P ∪ Q that separates the points in P
from the points in Q. Inside the region of points closer to P (resp. Q) the Voronoi diagram of P ∪ Q and
the Voronoi diagram of P (resp. Q) are identical. Thus, the merging of two Voronoi diagrams can be
seen as the process of cutting the Voronoi diagrams of P and Q along the contour.

This leads to a divide-and-conquer algorithm for constructing the Voronoi diagram of a set of n points
and hence for computing the Delaunay triangulation in time within O(n log n). This time complexity
of O(n log n) is asymptotically optimal in the comparison model in the worst case over instances composed
of n points. Shamos and Hoey [17] showed that the construction of any triangulation over n points requires
Ω(n log n), as Sorting can be reduced to computing the triangulation of n + 1 points, which yields an
asymptotic computational lower bound of Ω(n log n) in the worst case over sets of n planar points, in the
comparison model.

Multiary Merge. Given µ Delaunay triangulations of sizes v1, . . . , vµ, respectively, to be merged,
we make a sequence of binary merges, reducing at each step the number of Delaunay triangulations
to be merged by 1. The merging process can be represented by a binary tree where the internal nodes are
the merged Delaunay triangulations, and the leaves are the original µ Delaunay triangulations.
Merging the two Delaunay triangulations of minimum sizes at each step, similar as the Huﬀman code
algorithm [7], further improves the merging process, which takes advantage of the potential disequilibrium
in the distribution of the points between the µ Delaunay triangulations. We can apply the Huﬀman [7]
algorithm to the vector (cid:104)v1, . . . , vµ(cid:105), thus obtaining a Huﬀman-shaped tree representing the merging process.

ming to n =(cid:80)µ

Lemma 3 (Multiary Merge). Given µ Delaunay triangulations of respective sizes (cid:104)v1, . . . , vµ(cid:105) sum9

i=1 vi, there is an algorithm computing the Delaunay triangulation of the n points in
time within O(n(1 + H(v1, . . . , vµ))) ⊆ O(n(1 + log µ)) ⊆ O(n log n).
Proof. The algorithm follows the same steps as the algorithm suggested by Huﬀman [7] on a set of ρ messages
of probabilities {ri/n}i∈[1..ρ]:
1. Initialize a heap H with the ρ Voronoi diagrams, indexed by their size;
2. While H contains more than one Voronoi diagram

– extract the two smallest Voronoi diagrams from H, of respective sizes n1 and n2,
– merge them into a Voronoi diagram T of size n1 + n2, and
– insert T in H.
This algorithm executes in time within O(n(1 +H(v1, . . . , vµ))). The merging process is then represented
by a tree with the shape of a Huﬀman [7] tree. Consider the i-th Voronoi diagram of the input ∀i ∈ [1..µ]:
let ci be the binary string describing the path leading from the root to the corresponding leaf, and li the
length of this path. The sum of the computational costs of the binary merges is the sum of the sizes of the
Voronoi diagram computed. The i-th Voronoi diagram contributes a cost within O(vi) to li levels,
i=1 livi). Consider the binary tree where the µ initial Voronoi diagrams are
leaves and the µ − 1 computed Voronoi diagrams are internal nodes:
– The set of binary strings {c1, . . . , cµ} is a preﬁx free code, i.e. no code is preﬁx of another root-to-leaf

which has a sum within O((cid:80)µ
– The lengths of those codes minimize(cid:80)µ
(v1, . . . , vµ), i.e.(cid:80)µ

path, simply because they are paths in a tree.

i=1 liri as a property of Huﬀman [7] codes.

By the optimality of Huﬀman codes, this complexity is within a linear term of the entropy of the distribution
i=1 liri ∈ O(n(1 + H(v1, . . . , vµ))). This yields the ﬁnal time complexity, within O(n(1 +
(cid:117)(cid:116)
H(v1, . . . , vµ))).
The combination of the partitioning algorithm and the merging process yields an optimal algorithm

computing these structures adaptive to the decomposition of the input into monotone histograms.

Theorem 4. Let d1 and d2 be two perpendicular directions. Let S be a sequence of n planar points. Let µ and
(cid:104)v1, . . . , vµ(cid:105) be the minimum number of monotone histograms with respect to d1 and d2 and the sizes of these
monotone histograms, respectively, in which S can be cut. The Delaunay triangulation and the Voronoi
diagram of S can be computed in time within O(n(1+H(v1, . . . , vµ))) ⊆ O(n(1+ log µ)) ⊆ O(n log n), which
is worst-case optimal in the comparison model over instances of n points that can be partitioned into monotone
histograms of sizes (cid:104)v1, . . . , vµ(cid:105).
Proof. The combination of the partitioning algorithm and the merging process yields an algorithm computing
these structures within this time. In order to provide a lower bound, we use again the result of Ω(n(1 +
H(r1, . . . , rρ))) for Sorting a sequence of n numbers, in the worst case over instances covered by ρ runs
of lengths r1, . . . , rρ summing n in the comparison model, demonstrated by Barbay and Navarro [2]. This
problem can be reduced in linear time to the problem of computing the Delaunay triangulation of
a sequence of n planar points covered by ρ monotone histograms of lengths r1, . . . , rρ with respect to the
coordinates axes. The ρ runs are transformed into ρ monotone histograms of the same lengths, using points
on the parabola, in linear time. The sorted sequence of the numbers can be obtained from the Delaunay
(cid:117)(cid:116)
triangulation of the points in linear time.
Such examples of complete reﬁnements are not the rule: in the following section we describe some examples

where such reﬁnements are problematic or impossible.

4 Partial Reﬁnements

We describe a new partitioning algorithm for a sequence of points in Sections 4.1 to 4.3 such that the
Delaunay triangulation of each subsequence can be computed in linear time in its length, and discuss
alternate partitions in Section 4.4. Each diﬀerent partitioning algorithm, in combination with the merging
process described in Section 3.3, yields a diﬀerent algorithm adaptive to the input order.

104.1

Incremental Construction

Many Computational Geometry algorithms use tests known as the orientation and incircle tests [18]. The
orientation test determines whether a point lies to the left of, to the right of, or on a line or plane deﬁned
by other points. The incircle test determines whether a point lies inside, outside, or on a circle deﬁned by
other points.

Green and Sibson [5] proposed the ﬁrst incremental algorithm for computing the Delaunay triangulation 
of a point set which ﬁnds the triangle containing each new point, and updates the diagram by
correcting the edges violating the circumcircle condition —an operation named ﬂipping. The algorithm adds
points to the structure one by one. For each point, it performs two basic steps:

1. The algorithm ﬁnds the triangle containing the new point using the structure as a guide to the relative
position of the points. A greedy approach for locating the point is to start at an edge in the structure
and to walk across adjacent edges in the direction of the new point until the correct triangle is found.
Orientation tests [18] are performed on each edge of such a path to see whether the new point lies on
the correct side of that edge. We call the operations involved in this walk navigation operations.

2. It then updates the structure adding three new edges from the point inserted to the vertices of the
triangle containing the point and ﬂips all invalid edges resulting from the insertion. Note that ﬂipping
an edge can make another edge invalid, but that each edge is ﬂipped at most once, so each insertion can
trigger at most a linear number of ﬂips.

The time complexity of this incremental algorithm for computing the Delaunay triangulation is within
O(dn) ⊆ O(n2), where d ∈ [1..n] is an upper bound on the amount of operations required to insert each point
and to correct the Delaunay triangulation for the instance being considered. We show the problems
that arise when we try to adapt this algorithm in order to obtain a new one whose time complexity is within
O(n log d) ⊆ O(n log n).

4.2 Adaptivity to D-linear Runs

Consider an incremental algorithm G computing the Delaunay triangulation of n planar points in time
within O(n) in the best case; and an algorithm M merging two Delaunay triangulations of sizes summing
to n in time within O(n) in the worst case. Given a sequence S of n distinct planar points and a constant k, the
following naive algorithm computes its Delaunay triangulation in time within O(n) in the best case, in
time within O(n log n) in the worst case, and in time within O(n(1+H(r1, . . . , rρ))) ⊆ O(n log ρ) ⊆ O(n log n)
i=1 ri) measure the diﬃculty of

in the general case, where ρ ∈ [1..n] and r1, . . . , rρ (such that n = (cid:80)ρ

partitioning the instance into “easy” subinstances:

1. Run the incremental algorithm G on S until, for the i-th point p, it performs either more than k operations
to locate the point on the triangulation, or more than k ﬂip operations on it.
2. Store the Delaunay triangulation of the i − 1 ﬁrst points, and restart the greedy incremental
algorithm G on the sequence formed by p and its n− i successors in the input sequence S, until no points
are left in S.

3. Let ρ ∈ [1..n] and r1, . . . , rρ (such that n = (cid:80)ρ

i=1 ri) be the number of Delaunay triangulations
computed in this way and the sizes of these Delaunay triangulations, respectively. Merge the ρ Delaunay 
triangulations, in overall time within O(n(1+H(r1, . . . , rρ))) ⊆ O(n(1+ log ρ)) ⊆ O(n log n).

Greedy Partitioning. The algorithm described above suggests a way to partition the input into subsequences 
such that the Delaunay triangulation of each subsequence can be computed in linear time in
its length.

We use the algorithm described to deﬁne “easy” sequences of points for the computation of the Delaunay

triangulation:

11

Fig. 4. The sequences A, B and A(cid:48).

Deﬁnition 2 (D-linear). Consider a sequence S of n distinct planar points. Given an integer value k > 0, S
is k-D-linear (for “Delaunay linear”) if G performs at most k location and ﬂip operations for each point while
computing the Delaunay triangulation of S. If S is k-D-linear and k ∈ O(1) is a constant independent
of n, we say that S is D-linear.

Such a simple deﬁnition yields a simple partitioning of the input sequence into subsequences of consecutive

positions such that the Delaunay triangulation of each subsequence can be computed in linear time.

Deﬁnition 3 (D-linear Run). A D-linear run in a sequence S of points is a D-linear subsequence formed
by consecutive points in S.

4.3 Non Optimality of D-linear Runs Partitioning

We describe a family of sequences S, such that for all positive integer n, there is a sequence Sn of n
points in S where the greedy partitioning algorithm yields three D-linear runs when Sn can be optimally
partitioned into just two D-linear runs.
The Delaunay triangulation of m ≥ 2 points on the parabola y = x2 is such that the leftmost point
is adjacent to all the other m − 1 points, and the left-to-right order is a D-linear run. The proof of the
following lemma is based on this fact.
Lemma 4. For all positive integers n, there exists a sequence of n planar points where the greedy partitioning
algorithm yields three D-linear runs, whereas the optimal partition of this sequence has only two D-linear
runs.

Proof. Let k be the threshold used in the deﬁnition of D-linear sequences. Let A be a set of m > k points in
the parabola y = x2, denoted p1, . . . , pm from left to right. Let C be the circumcircle of the triangle p1, p2, p3.
Let u, v, w be three points such that the triangle ∆uvw with vertex set {u, v, w} is small enough with respect
to the convex hull of A, and located inside C. Let A(cid:48) = {p(cid:48)1, . . . , p(cid:48)m} be a scaled copy of A, located inside
∆uvw. Let B be the k + 3 vertex set of k + 1 decreasing area and disjoint triangles such as every two
consecutive triangles share an edge (see Fig. 4). Suppose that B is ordered such that the sequence of points
is a D-linear run. The points u, v and w are the last three points of B. The triangle formed by the ﬁrst three
points of B contains the point p1 of A. Let P = A∪A(cid:48)∪B be ordered as (cid:104)B, p1, . . . , pm, p(cid:48)1, . . . , p(cid:48)m(cid:105). Applying
the greedy partitioning algorithm to P gives the three D-linear runs (cid:104)B(cid:105), (cid:104)p1, . . . , pm(cid:105), and (cid:104)p(cid:48)1, . . . , p(cid:48)m(cid:105) since:
(1) adding p1 to the Delaunay triangulation of B requires traversing k triangles to locate p1; and (2)
adding p(cid:48)1 to the Delaunay triangulation of A requires creating m triangles (i.e. p(cid:48)1 is adjacent to every
point in A). However, (cid:104)B \ {u, v, w}(cid:105) is a D-linear run and (cid:104)u, v, w, p1, . . . , pm, p(cid:48)1, . . . , p(cid:48)m(cid:105) is another one.
(cid:117)(cid:116)
Note that the triangle ∆uvw blocks the points in A(cid:48) of being adjacent to any point in A.
This family of sequences S can be generalized to show a gap of Θ(n) between the number of D-linear
runs yielded by the greedy partitioning algorithm in Sn and the minimum number of D-linear runs in which
Sn can be partitioned.

b1b2b3b4b5p1uwvA0Ap1p2p3p4p5p6B12Lemma 5. For all positive integer n, there exists a sequence of n planar points where the greedy partitioning
algorithm yields ρ ∈ Θ(n) D-linear runs, and the optimal partition of this sequence has only 2 D-linear runs.
Proof. Let A be a set of m > k consecutive points in the parabola y = x2 denoted p1, . . . , pm from left to
right. Let C be the circumcircle of the triangle p1, p2, p3. Let u, v, w be three points such that the triangle
∆uvw with vertex set {u, v, w} is small enough with respect to the convex hull of A, and located inside C.
Let A(cid:48) be a D-linear run sequence of l points, of increasing y-coordinate and decreasing x-coordinate, inside C
denoted p(cid:48)1, . . . , p(cid:48)l. During the incremental construction of the Delaunay triangulation of the sequence
AA(cid:48) the ﬁrst point of A(cid:48) forces a ﬂipping of m edges in A (i.e. m new edges are created connecting p(cid:48)1 with
all points in A). Every next point of A(cid:48) forces a ﬂipping of the m edges connecting points in A with points
in A(cid:48). Let q(cid:48)1 and q(cid:48)2 be two consecutive points from the sequence A(cid:48) and let q1, . . . , qk+1 be k + 1 consecutive
points from A. The sequence Q = (cid:104)q(cid:48)1, q1, . . . , qk+1, q(cid:48)2(cid:105) is not a D-linear run because q(cid:48)2 forces a ﬂipping of the
k + 1 edges connecting q(cid:48)1 with all the points in q1, q2, . . . , qk+1. Let B be the (k + 3) vertex sequence of k + 1
decreasing area and disjoint triangles so that every two consecutive triangles share an edge. The points u, v
and w are the last three points of B. The triangle formed by the ﬁrst three points of B contains inside the
point p1 of A. B is ordered such that the sequence of vertices is a D-linear run. Let P = A∪A(cid:48)∪B be ordered
as (cid:104)B, p1, . . . , pk+1, p(cid:48)1, pk+2, . . . , p2k+3, p(cid:48)2, . . . , pm(cid:105), that is, ﬁrst the points of B, then alternate k + 1 points
of A with 1 point of A(cid:48), resulting subsequences similar to Q. The greedy partition algorithm divides P into
k+1 )+1 D-linear runs: (cid:104)B(cid:105), (cid:104)p1, . . . , pk+1(cid:105) and every subsequence starting with 1 point from A(cid:48) followed
min(l, m
by k + 1 points from A. Since: (i) adding p1 to the Delaunay triangulation of B requires traversing k
triangles to locate p1; (ii) adding p(cid:48)1 to the Delaunay triangulation of p1 . . . pk+1 requires creating k
triangles (i.e. p(cid:48)1 is adjacent to every point in p1 . . . pk+1); and (iii) adding p(cid:48)i+1 to (cid:104)p(cid:48)i, pik+j, . . . , p(i+1)k+j+1(cid:105)
produces a sequence similar to Q, and hence is not a D-linear run. However, (cid:104)B \{u, v, w}(cid:105) is a D-linear run
and (cid:104)u, v, w, p1, . . . , pk, p(cid:48)1, pk+1, pk+2, . . . , p2k, p(cid:48)2, . . . , pm(cid:105) is another one. Note that the triangle ∆uvw blocks
k+1 = n−k+3
the points in A(cid:48) of being adjacent to any vertex in A. It is possible to build a sequence l = m
(1 point in A(cid:48) for each sequence of k + 1 points in A) such that the number of D-linear runs yielded by the
(cid:117)(cid:116)
greedy partitioning algorithm is n−k+3

k+2 + 1 and the optimal partition has only 2 D-linear runs.

k+2

4.4 Other Partitioning Schemes

Since the greedy partitioning algorithm does not yield an optimal partition, we explore more sophisticated
partition techniques and show that they suﬀer similar problems.

We adapt the Levcopoulos et al. [11] partition technique to cut a sequence of planar points into D-linear
runs. The test and divide partitioning is another partition technique based on the incremental algorithm G for
the computation of the Delaunay triangulation (seen in Sect. 4.1). Given a sequence S = (cid:104)p1, . . . , pn(cid:105)
of n planar points, the test and divide partitioning ﬁrst tests whether the sequence S is a D-linear run.
In such a case, it identiﬁes the sequence S as a D-linear run. If not, it partitions the sequence S into

S(cid:48) = (cid:104)p1, . . . , p(cid:98)n/2(cid:99)
(cid:105) and S(cid:48)(cid:48) = (cid:104)p(cid:98)n/2(cid:99)+1, . . . , pn(cid:105) and recursively the same procedure is applied to S(cid:48) and
S(cid:48)(cid:48). While the greedy partitioning has a linear time complexity, this partitioning has a time complexity
within O(n log ρ) ⊆ O(n log n) where ρ is the number of D-linear runs identiﬁed by the partitioning process.
The following lemma shows that the test and divide partitioning suﬀers the same problems as the greedy

partitioning.

Lemma 6. For all positive integer n, there exists a sequence of n planar points where the test and divide
partitioning yields ρ ∈ Θ(n) D-linear runs and the optimal partition of this sequence has only 2 D-linear
runs.

Proof. Consider the sequence S used in the proof of Lemma 5 where the number of points in B is changed to
(cid:98)n/2(cid:99) instead of k + 3 then the test and divide partitioning will cut S in the last point of B. B is a D-linear
(cid:117)(cid:116)
run, but the bisection partitioning will cut the rest of S in

n

2(k+2) D-linear runs.

13

The greedy partitioning algorithm adds the point pi to the current run whether the number of location
and update operations of the incremental algorithm G in pi is at most a threshold k. But it is possible that
in some points the number r of operations would be much lower than k. The amortized greedy partitioning
algorithm makes use of the k − r remaining operations in the subsequent points, similar to the accounting
method for amortized analysis. The number k − r of remaining operations are credited to be used later, so
that the point pi will be added to the current run if the number of navigation and update operations of G
in pi is less than k plus this credit.

We use this algorithm to give another deﬁnition of “easy” sequences of points for the computation of the

Delaunay triangulation:

Deﬁnition 4 (Amortized D-linear). Consider a sequence S = (cid:104)p1, . . . , pn(cid:105) of n distinct planar points.
Given an integer value k > 0, S is Amortized k-D-linear if for each point pi ∈ S, the incremental algorithm
G performs at most k ∗ i navigation and ﬂip operations while computing the Delaunay triangulation of
the sequence p1, . . . , pi. If S is Amortized k-D-linear and k ∈ O(1) is a constant independent of n, we say
that S is Amortized D-linear.

Such deﬁnition yields a partitioning of the input sequence into subsequence of consecutive positions:

Deﬁnition 5 (Amortized D-linear Run). An Amortized D-linear run in a sequence S of points is an
Amortized D-linear subsequence formed by consecutive points in S.

We deﬁne the optimal partition into Amortized D-Linear runs as the partition with the minimum number

of Amortized D-linear runs.

Lemma 7. For all positive integer n, there exists a sequence of n planar points where the amortized greedy
partitioning yields ρ ∈ Θ(n) Amortized D-linear runs and the optimal partition has only 2 Amortized D-linear
runs.

Proof. The construction of the sequence follows the same scheme of previous constructions (see the proof
of Lemma 5). The sequence B is such that the incremental algorithm G performs k navigation and ﬂip
operations in almost every point in B, so that B is still a run but the credit is almost zero at the end of B.
Again, the last 3 points of B form a triangle that blocks the points in A(cid:48) of being adjacent to any vertex in
A. The sequence S alternates points in A with points in A(cid:48) such that each new point in A(cid:48) is adjacent to
every point in A, thus when the numbers of points in A is large enough to exceed the credit, the Amortized
D-linear run is cut. This partition algorithm yields close to n/k2 Amortized D-linear runs. This sequence can
be partitioned optimally in just 2 Amortized D-linear runs. Note that the triangle ∆uvw blocks the points
(cid:117)(cid:116)
in A(cid:48) of being adjacent to any point in A.

Amortized Test and Divide Partitioning. The amortized test and divide partitioning is a combination
of the test and divide partitioning and the amortized greedy partitioning. Given a sequence S = (cid:104)p1, . . . , pn(cid:105)
of n planar points we ﬁrst test whether the sequence S is Amortized D-linear. In such a case, we identify the
sequence S as an Amortized D-linear run. If not, we partition the sequence S into S(cid:48) = (cid:104)p1, . . . , p(cid:98)n/2(cid:99)
(cid:105) and
S(cid:48)(cid:48) = (cid:104)p(cid:98)n/2(cid:99)+1, . . . , pn(cid:105) and recursively the same procedure is applied to S(cid:48) and S(cid:48)(cid:48).
Lemma 8. For all positive integer n, there exists a sequence of n planar points where the amortized test and
divide partitioning yields ρ ∈ Θ(n) Amortized D-linear runs and the optimal partition has only 2 Amortized
D-linear runs.

14Proof. The construction of the sequence follows the same scheme of previous constructions (see the proof
of Lemma 5). The sequence B is such that the incremental algorithm G performs k navigation and ﬂip
operations in almost every point in B, so that B is still a run but the credit is almost zero at the end of B.
Again, the last 3 points of B form a triangle that blocks the points in A(cid:48) of being adjacent to any vertex in
A. The sequence S alternates points in A with points in A(cid:48) such that each new point in A(cid:48) is adjacent to
every point in A, thus when the numbers of points in A is large enough to exceed the credit, the Amortized
D-linear run is cut. This partition algorithm yields close to n/k2 Amortized D-linear runs. This sequence can
be partitioned optimally in just 2 Amortized D-linear runs. Note that the triangle ∆uvw blocks the points
(cid:117)(cid:116)
in A(cid:48) of being adjacent to any point in A.

5 Discussion

where H(n1, . . . , nk) =(cid:80)k

We describe one technique to reﬁne the analysis of the divide-and-conquer polynomial evaluation algorithm,
and two techniques to reﬁne the analysis of divide-and-conquer input-order adaptive algorithms for computing
the Convex Hull, Voronoi Diagram and Delaunay Triangulation of n planar points in time within
O(n(1 + H(n1, . . . , nk))) ⊆ O(n(1+ log k)) ⊆ O(n log n) for some parameters n1, . . . , nk summing to n,
measures the “diﬃculty” of the instance. We show the optimality of
the algorithms for computing Convex Hulls, Voronoi Diagrams and Delaunay Triangulations by
providing lower bounds that match the reﬁned analyses. The later results improve on those from Levcopoulos
et al. [11] in that this analysis takes advantage of the sizes of the simple polygonal chains into which the
input polygonal chain can be partitioned.

ni
n log n
ni

i=1

There is still work to be done in the directions initiated by our work. One of these techniques is based
on the partitioning of a sequence of n planar points into subsequences, whose corresponding Voronoi diagrams 
and Delaunay triangulations can be computed quickly, and a technique to eﬃciently merge
those structures into a single one, inspired by Huﬀman [7] codes. Each combination of partition and merging
techniques yields adaptive algorithms to the input order for computing Voronoi Diagrams and Delaunay 
Triangulations. Those results show that the Delaunay Triangulation can be computed in time
within o(n log n) on some classes of instances, yet we described various partitioning algorithms where the
time complexity achieved is sub-optimal. The next step is to ﬁnd better partitioning algorithms taking full
advantage of the input order.

We classify the various reﬁned analysis between those focusing on the structure of the instance versus
those focusing on the order in which the input is given. We have been analyzing an algorithm for the Union
Set problem that can be adapted for solving the Sorting problem taking advantage of the structure of the
instance and the order of the input in synergy. We will continue studying potential synergistic solutions to the
Sorting problem and their extension to the Convex Hull and Delaunay triangulation computation.

References

1. Afshani, P., Barbay, J., Chan, T.: Instance-optimal geometric algorithms. In: Proceedings 50th IEEE Symposium

on Foundations of Computer Science (FOCS) (2009)

2. Barbay, J., Navarro, G.: On compressing permutations and adaptive sorting. Theoretical Computer Science (TCS)

513, 109–123 (2013)

3. Chazelle, B.: Triangulating a simple polygon in linear time. Discrete & Computational Geometry (DCG) 6(5),

485–524 (Aug 1991)

4. Djidjev, H., Lingas, A.: On computing voronoi diagrams for sorted point sets. International Journal of Computational 
Geometry & Applications (IJCGA) 5(3), 327–337 (1995)

5. Green, P.J., Sibson, R.: Computing dirichlet tessellations in the plane. The Computer Journal (TCJ) 21(2),

168–173 (1978)

6. Guibas, L., Stolﬁ, J.: Primitives for the manipulation of general subdivisions and the computation of voronoi.

ACM Trans. Graph. (TOG) 4(2), 74–123 (Apr 1985)

7. Huﬀman, D.A.: A method for the construction of minimum-redundancy codes. Proceedings of the Institute of

Radio Engineers (IRE) 40(9), 1098–1101 (September 1952)

15
8. Kirkpatrick, D.G.: Eﬃcient computation of continuous skeletons. In: Proceedings of the 20th Annual Symposium
on Foundations of Computer Science (FOCS). pp. 18–27. IEEE Computer Society, Washington, DC, USA (1979)
9. Kirkpatrick, D.G., Seidel, R.: The ultimate planar convex hull algorithm? SIAM Journal on Computing

(SICOMP) 15(1), 287–299 (1986)

10. Knuth, D.E.: The Art of Computer Programming, Vol 3, chap. Sorting and Searching, Section 5.3. Addison-Wesley

(1973)

11. Levcopoulos, C., Lingas, A., Mitchell, J.S.B.: Adaptive algorithms for constructing convex hulls and triangulations
of polygonal chains. In: Proceedings of the 8th Scandinavian Workshop on Algorithm Theory (SWAT). pp. 80–89.
Springer-Verlag, London, UK (2002)

12. Melkman, A.A.: On-line construction of the convex hull of a simple polyline. Information Processing Letters

(IPL) 25(1), 11–12 (Apr 1987)

13. Moﬀat, A., Petersson, O.: An overview of adaptive sorting. Australian Computer Journal 24(2), 70–77 (1992)
14. Munro, J.I., Spira, P.M.: Sorting and searching in multisets. SIAM Journal on Computing (SICOMP) 5(1), 1–8

(1976)

15. Preparata, F.P., Shamos, M.I.: Computational Geometry: An Introduction. Springer-Verlag (1985)
16. Press, W.H., Flannery, B.P., Teukolsky, S.A., Vetterling, W.T.: Numerical Recipes in C: The Art of Scientiﬁc

Computing. Cambridge University Press, New York, NY, USA (1988)

17. Shamos, M., Hoey, D.: Closest point problems. In: Proceedings of the 16th Annual IEEE Symposium on Foundations 
of Computer Science (FOCS). pp. 151–162 (1975)

18. Shewchuk, J.R.: Adaptive Precision Floating-Point Arithmetic and Fast Robust Geometric Predicates. Discrete

& Computational Geometry 18(3), 305–363 (Oct 1997)

19. Strassen, V.: Gaussian elimination is not optimal. Numerische Mathematik 13(4), 354–356 (1969)

