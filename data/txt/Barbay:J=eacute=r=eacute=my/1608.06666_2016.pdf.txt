6
1
0
2

 

p
e
S
0
3

 

 
 
]
S
D
.
s
c
[
 
 

2
v
6
6
6
6
0

.

8
0
6
1
:
v
i
X
r
a

Synergistic Sorting, MultiSelection and Deferred Data Structures

on MultiSets

J´er´emy Barbay1, Carlos Ochoa1, and Srinivasa Rao Satti2

1 Departamento de Ciencias de la Computaci´on, Universidad de Chile, Chile

2 Department of Computer Science and Engineering, Seoul National University, South Korea

jeremy@barbay.cl, cochoa@dcc.uchile.cl

ssrao@cse.snu.ac.kr

Abstract. Karp et al. (1988) described Deferred Data Structures for Multisets as “lazy” data structures 
which partially sort data to support online rank and select queries, with the minimum amount
of work in the worst case over instances of size n and number of queries q ﬁxed (i.e., the query size).
Barbay et al. (2016) reﬁned this approach to take advantage of the gaps between the positions hit by
the queries (i.e., the structure in the queries). We develop new techniques in order to further reﬁne this
approach and to take advantage all at once of the structure (i.e., the multiplicities of the elements),
the local order (i.e., the number and sizes of runs) and the global order (i.e., the number and positions
of existing pivots) in the input; and of the structure and order in the sequence of queries. Our main
result is a synergistic deferred data structure which performs much better on large classes of instances,
while performing always asymptotically as good as previous solutions. As intermediate results, we describe 
two new synergistic sorting algorithms, which take advantage of the structure and order (local
and global) in the input, improving upon previous results which take advantage only of the structure
(Munro and Spira 1979) or of the local order (Takaoka 1997) in the input; and one new multiselection
algorithm which takes advantage of not only the order and structure in the input, but also of the
structure in the queries. We described two compressed data structures to represent a multiset taking
advantage of both the local order and structure, while supporting the operators rank and select on
the multiset.

Keywords: Deferred Data Structure, Divide and Conquer, Fine Grained Analysis, Quick Select,
Quick Sort, Compressed Data Structure, Rank, Select.

Introduction

the entropy function H(m1, . . . , mσ) = (cid:80)σ
and m1, . . . , mσ are the multiplicities of the σ distinct elements in M (such that(cid:80)σ

1
Consider a multiset M of size n (e.g., M = {1, 2, 3, 3, 4, 5, 6, 7, 8, 9} of size n = 10). The multiplicity of
an element x of M is the number mx of occurrences of x in M (e.g., m3 = 2). We call the distribution
of the multiplicities of the elements in M the input structure , and denote it by a set of pairs (x, mx)
(e.g., {(1, 1), (2, 1), (3, 2), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1)} in M). As early as 1976, Munro and
Spira [25] described a variant of the algorithm MergeSort using counters, which optimally takes advantage
of the input structure (i.e., the multiplicities of the distinct elements) when sorting a multiset M of n
elements. Munro and Spira measure the “diﬃculty” of the instance in terms of the “input structure” by
. The time complexity of the algorithm is within
O(n(1 + H(m1, . . . , mσ))) ⊆ O(n(1+ log σ)) ⊆ O(n log n), where σ is the number of distinct elements in M
i=1 mi = n), respectively.
Any array A representing a multiset lists its element in some order, which we call the input order and
denote by a tuple (e.g., A = (2, 3, 1, 3, 7, 8, 9, 4, 5, 6)). Maximal sorted subblocks in A are a local form of input
order and are called runs [21] (e.g., {(2, 3), (1, 3, 7, 8, 9), (4, 5, 6)} in A). As early as 1973, Knuth [21] described
a variant of the algorithm MergeSort using a prepossessing step taking linear time to detect runs in the
array A , which he named Natural MergeSort. Mannila [22] reﬁned the analysis of the Natural MergeSort
algorithm to yield a time complexity for sorting an array A of size n in time within O(n(1+log ρ)) ⊆ O(n lg n),
where ρ is the number of runs in the A. Takaoka [28] described a new sorting algorithm that optimally takes

mi
n log n
mi

i=1

are the sizes of the ρ runs in A (such that (cid:80)ρ

2advantage of the distribution of the sizes of the runs in the array A, which yields a time complexity within
O(n(1 + H(r1, . . . , rρ))) ⊆ O(n(1+ log ρ)) ⊆ O(n log n), where ρ is the number of runs in A and r1, . . . , rρ
i=1 ri = n), respectively. It is worth noting that in 1997,
Takaoka [27] ﬁrst described this algorithm in a technical report.
Given an element x of a multiset M and an integer j ∈ [1..n] (e.g., M = {1, 2, 3, 3, 4, 5, 6, 7, 8, 9}, x = 3
and j = 4) , the rank rank(x) of x is the number of elements smaller than x in M, (e.g., rank(3) = 2) and
selecting the j-th element in M corresponds to computing the value select(j) of the j-th smallest element
(counted with multiplicity) in M (e.g., select(4) = 3). As early as 1961, Hoare [17] showed how to support
rank and select queries in average linear time, a result later improved to worst case linear time by Blum et
al. [6]. Twenty years later, Dobkin and Munro [10] described a MultiSelection algorithm that supports
several select queries in parallel and whose running time is optimal in the worst case over all multisets
of size n and all sets of q queries hitting positions in the multisets separated by gaps (diﬀerences between
consecutive select queries in sorted order) of sizes g0, . . . , gq. Kaligosi et al. [19] later described a variant of
this algorithm which number of comparisons performed is within a negligible additional term of the optimal.
In the online context where the queries arrive one at a time, Karp et al. [20] further extended Dobkin and
Munro’s result [10]. Karp et al. called their solution a Deferred Data Structure and describe it as
“lazy”, as it partially sorts data, performing the minimum amount of work necessary in the worst case over
all instances for a ﬁxed n and q. Barbay et al. [2] reﬁned this result by taking advantage of the gaps between
the positions hit by the queries (i.e., the query structure). This suggests the following questions:

1. Is there a sorting algorithm for multisets which takes the best advantage of both its input
order and its input structure in a synergistic way, so that it performs as good as previously
known solutions on all instances, and much better on instances where it can take advantage
of both at the same time?

2. Is there a multiselection algorithm and a deferred data structure for answering rank and
select queries which takes the best advantage not only of both of those notions of easiness
in the input, but additionally also of notions of easiness in the queries, such as the query
structure and the query order ?

We answer both questions aﬃrmatively: In the context of Sorting, this improves upon both algorithms
from Munro and Spira [25] and Takaoka [28]. In the context of MultiSelection and Deferred Data
Structure for rank and select on Multisets, this improves upon Barbay et al.’s results [2] by adding
3 new measures of diﬃculty (input order, input structure and query order) to the single one previously
considered (query structure). Even though the techniques used by our algorithms are known, the techniques
used to reﬁne the analysis of these algorithms to show that they improve the state of the art are complex.
Additionally, we correct the analysis of the Sorted Set Union algorithm by Demaine et al. [9] (Section 2.3),
and we deﬁne a simple yet new notion of “global” input order (Section 2.5), formed by the number of
preexisting pivot positions in the input (e.g. (3, 2, 1, 6, 5, 4) has one pre-existing pivot position in the middle),
not mentioned in previous surveys [11, 23] nor extensions [3].

We present our results incrementally, each building on the previous one, such that the most complete and
complex result is in Section 4. In Section 2 we describe how to measure the interaction of the order (local
and global) with the structure in the input, and two new synergistic Sorting algorithms based on distinct
paradigms (i.e., merging vs splitting) which take advantage of both the input order and structure in order to
sort the multiset in less time than traditional solutions based on one of those, at most. We reﬁne the second
of those results in Section 3 with the analysis of a MultiSelection algorithm which takes advantage of not
only the order and structure in the input, but also of the query structure, in the oﬄine setting. In Section 4
we analyze an online Deferred Data Structure taking advantage of the order and structure in the input
on one hand, and of the order and structure in the queries on the other hand, in a synergistic way. As an
additional result, we describe in Section 5 two compressed data structures to represent a multiset taking
advantage of both the input order and structure, while supporting the operators rank and select on the
multiset. We conclude with a discussion of our results in Section 6.

2 Sorting Algorithms

3

We review in Section 2.1 the algorithms MergeSort with Counters described by Munro and Spira [25]
and Minimal MergeSort described by Takaoka [28], each takes advantage of distinct features in the input.
In Section 2.2 we show that the algorithm MergeSort with Counters is incomparable with the algorithm
Minimal MergeSort, in the sense that none performs always better than the other (by describing families
of instances where MergeSort with Counters performs better than Minimal MergeSort on one hand, and
families of instances where the second one performs better than the ﬁrst one on the other hand), and some
simple modiﬁcations and combinations of these algorithms, which still do not take full advantage of both
the order (local and global) and structure in the input. In Sections 2.3 and 2.4, we describe two synergistic
Sorting algorithms, which never perform worse than the algorithms presented in Section 2.1 and Section 2.2,
and perform much better on some large classes of instances by taking advantage of both the order (local and
global) and the structure in the input, in a synergistic way.

2.1 Previously Known Input-structure and Local Input-order Algorithms

The algorithm MergeSort with Counters described by Munro and Spira [25] is an adaptation of the traditional 
sorting algorithm MergeSort that optimally takes advantage of the structure in the input when sorting
a multiset M of size n. The algorithm divides M into two equal size parts, sorts both parts recursively, and
then merges the two sorted lists. When two elements of same value v are found, one is thrown away and a
counter holding the number of occurrences of v is updated. Munro and Spira measure the “diﬃculty” of the
. The
time complexity of the algorithm is within O(n(1 + H(m1, . . . , mσ))) ⊆ O(n(1+ log σ)) ⊆ O(n log n), where
σ is the number of distinct elements in M and m1, . . . , mσ are the multiplicities of the σ distinct elements

instance in terms of the “input structure” by the entropy function H(m1, . . . , mσ) =(cid:80)σ
in M (such that(cid:80)σ

mi
n log n
mi

i=1

i=1 mi = n), respectively.

The algorithm Minimal MergeSort described by Takaoka [28] optimally takes advantage of the local order
in the input, as measured by the decomposition into runs when sorting an array A of size n. The main idea
is to detect the runs ﬁrst and then merge them pairwise, using a MergeSort-like step. The runs are detected
in linear time by a scanning process identifying the positions i in A such that A[i] > A[i + 1]. Merging the
two shortest runs at each step further reduces the number of comparisons, making the running time of the
merging process adaptive to the entropy of the sequence of the sizes of the runs. The merging process is then
represented by a tree with the shape of a Huﬀman [18] tree, built from the distribution of the sizes of the
i=1 ri = n),
then the algorithm sorts A in time within O(n(1 + H(r1, . . . , rρ))) ⊆ O(n(1+ log ρ)) ⊆ O(n log n).

runs. If the array A is formed by ρ runs and r1, . . . , rρ are the sizes of the ρ runs (such that(cid:80)ρ

2.2 Comparison Between Sorting Algorithms
Example 1: (cid:104)1, 2, 1, 2, . . . , 1, 2(cid:105)

On this family of instances, the algorithm Minimal MergeSort detects n

2 runs that merges two at a
time. The time complexity of Minimal MergeSort is within Θ(n log n). On the other hand, the algorithm
MergeSort with Counters recognizes the multiplicity of the elements with values 1 and 2. In every merging
step, the resulting set is always {1, 2}. Therefore, the number of elements is reduced by half at each step,
which yields a complexity linear in the size of the instance for sorting such instances.
Example 2: (cid:104)1, 2, . . . , n(cid:105)

On this family of instances, the algorithm Minimal MergeSort detects in time linear in the size n of the
input that the sequence is already sorted, and in turn it ﬁnishes. On the other hand, the running time of the
algorithm MergeSort with Counters is within Θ(n log n) because this algorithm would perform at least as
many operations as MergeSort.

4 The algorithm Parallel Minimal Counter MergeSort runs both algorithms in parallel, when one of
these algorithms manages to sort the sequence, the algorithm Parallel Minimal Counter MergeSort returns 
the sorted sequence and then ﬁnishes. The time complexity of this algorithm in any instance is twice
the minimum of the complexities of Minimal MergeSort and MergeSort with Counters for this instance.
The algorithm Parallel Minimal Counter MergeSort needs to duplicate the input in order to run both
algorithms in parallel.

Combining the ideas of identifying and merging runs from Takaoka [28] with the use of counters by Munro
and Spira [25], we describe the Small vs Small Sort algorithm to sort a multiset. It identiﬁes the runs
using the same linear scanning as Minimal MergeSort and associates counters to the elements in the same
way that MergeSort with Counters does. Once the runs are identiﬁed, this algorithm initializes a heap
with them ordered by sizes. At each merging step the two shorter runs are selected for merging and both are
removed from the heap. The pair is merged and the resulting run is inserted into the heap. The process is
repeated until only one run is left and the sorted sequence is known. The Small vs Small Sort algorithm
is adaptive to the sizes of the resulting runs in the merging process.

The time complexity of Small vs Small Sort for all instances is within a constant factor of the time
complexity of Parallel Minimal Counter MergeSort; but the next example shows that there are families
of instances where Small vs Small Sort performs better than a constant factor of the time complexity of
Parallel Minimal Counter MergeSort.
Example 3: (cid:104)1, 2, . . . , σ, 1, 2, . . . , σ . . . , 1, 2, . . . , σ(cid:105)

In this family of instances, there are ρ runs, each of size σ. The complexity of MergeSort with Counters
for this instance is within Θ(ρσ log σ), while the complexity of Minimal MergeSort for this instance is within
Θ(ρσ log ρ). On the other hand, the complexity Small vs Small Sort for this instance is better, within
Θ(ρσ): at each level of the binary tree representing the merging order, the sum of the sizes of the runs is
halved.

Even though Small vs Small Sort is adaptive to the sizes of the resulting runs, it does not take advantage 
of the fact that there may exist a pair of runs that can be merged very quickly, but it needs linear
time in the sum of the sizes to merge one of them when paired with another run of the same size. We show
this disadvantage in the Example 4 at the end of the next section.

In the following sections we describe two sorting algorithms that take the best advantage of both the
order (local and global) and structure in the input all at once when sorting a multiset. The ﬁrst one is a
straightforward application of previous results, while the second one prepares the ground for the MultiSelection 
algorithm (Section 3) and the Deferred Data Structures (Section 4), which take advantage
of the order (local and global) and structure in the data and of the order and structure in the queries.

2.3 “Kind-of-new” Sorting Algorithm DLM Sort

In 2000, Demaine et al. [9] described the algorithm DLM Union, an instance optimal algorithm that computes
the union of ρ sorted sets. It inserts the smallest element of each set in a heap. At each step, it deletes
from the heap all the elements whose values are equal to the minimum value of the heap. If more than one
element is deleted, it knows the multiplicity of this value in the union of the sets. It then adds to the heap
the elements following the elements of minimum value of each set that contained the minimum value. If
there is only one minimum element, it extracts from the heap the second minimum and executes a doubling
search [5] in the set where the minimum belongs for the value of the second minimum. Once it ﬁnds the
insertion rank r of the second minimum (i.e., number of elements smaller than the second minimum in the
set that contains the minimum), it also knows that the multiplicity of all elements whose positions are before
r in the set that contain the minimum are 1 in the union of the ρ sets. The process is repeated until all
elements are discarded.

The time complexity of the DLM Union algorithm is measured in terms of the number and sizes of blocks
of consecutive elements in the sets that are also consecutive in the sorted union (see Figure 1 for a graphical
representation of such a decomposition on a particular instance of the Sorted Set Union problem). The

5
sizes of these blocks are referred to as gaps in the analysis of the algorithm. These blocks induce a partition
π of the output into intervals such that any singleton corresponds to a value that has multiplicity greater
than 1 in the input, and each other interval corresponds to a block as deﬁned above. Each member i of π has
a value mi associated with it: if the member i of π is a block, then mi is 1, otherwise, if the member i of π is
a singleton corresponding to a value of multiplicity q, then mi is q. Let χ be the size of π. If the instance is
formed by δ blocks of sizes g1, . . . , gδ such that these blocks induce a partition π whose members have values

m1, . . . , mχ, we express the time complexity of DLM Union as within Θ((cid:80)δ

i=1 log gi +(cid:80)χ

i=1 log(cid:0) ρ

(cid:1)). This

time complexity is within a constant factor of the complexity of any other algorithm computing the union
of these sorted sets (i.e., the algorithm is instance optimal).

mi

Fig. 1: An instance of the Sorted Set Union
problem with ρ = 3 sorted sets. In each set,
the entry A[i] is represented by a point of xcoordinate 
A[i]. The blocks that form the sets
are noted. The blocks g4, g5 and g6 are of size
1 because they correspond to elements of equal
value and they induce the 4-th member of the
partition π with value m4 equals 3. The vertical
bars separate the members of π.

We adapt the DLM Union algorithm for sorting a multiset. The algorithm DLM Sort detects the runs ﬁrst
through a linear scan and then applies the algorithm DLM Union. After that, transforming the output of the
union algorithm to yield the sorted multiset takes only linear time. The following corollary follows from our
reﬁned analysis above:
Corollary 1. Given a multiset M of size n formed by ρ runs and δ blocks of sizes g1, . . . , gδ such that these
blocks induce a partition π of size χ of the output whose members have values m1, . . . , mχ, the algorithm DLM

(cid:1)) data comparisons. This number of comparisons is

Sort performs within O(n +(cid:80)δ

i=1 log gi +(cid:80)χ

i=1 log(cid:0) ρ

optimal in the worst case over multisets of size n formed by ρ runs and δ blocks of sizes g1, . . . , gδ such that
these blocks induce a partition π of size χ of the output whose members have values m1, . . . , mχ.

mi

There are families of instances where the algorithm DLM Sort behaves signiﬁcantly better than the

algorithm Small vs Small Sort. Consider for instance the following example:

(cid:69)

Example 4:

ρ n, . . . , 1, . . . , 1
ρ n

(cid:68) ρ−1
ρ n + 1, . . . , n, ρ−2
ρ n + 1, . . . , ρ−1
(cid:17)

(cid:16)

In this family of instances, there are ρ runs of size n

ρ each. The runs are pairwise disjoint and the
elements of each run are consecutive in the sorted set. The time complexity of the algorithm Small vs
Small Sort in this instances is within Θ(n log ρ), while the time complexity of the algorithm DLM Sort is

within Θ

n + ρ log ρ + ρ log n
ρ

= Θ(n + ρ log n) (which is better than Θ(n log ρ) for ρ ∈ o(n)).

While the algorithm DLM Sort answers the Question 1 from Section 1, it does not yield a MultiSelection 
algorithm nor a Deferred Data Structure answering Question 2. In the following section we
describe another sorting algorithm that also optimally takes advantage of the local order and structure in
the input, but which is based on a distinct paradigm, more suitable to such extensions.

2.4 New Sorting Algorithm Quick Synergy Sort
Given a multiset M, the algorithm Quick Synergy Sort identiﬁes the runs in linear time through a scanning
process. As indicated by its name, the algorithm is directly inspired by the QuickSort [16] algorithm. It
computes a pivot µ, which is the median of the set formed by the middle elements of each run, and partitions
each run by the value of µ. This partitioning process takes advantage of the fact that the elements in each
run are already sorted. It then recurses on the elements smaller than µ and on the elements greater than µ.
(See Algorithm 1 for a more formal description).

g1g3g2g4g5g6g7g8ρ6Deﬁnition 1 (Median of the middles). Given a multiset M formed by runs, the “ median of the middles”
is the median element of the set formed by the middle elements of each run.

1: Compute the ρ runs of respective sizes (ri)i∈[1..ρ] in M such that(cid:80)ρ

Algorithm 1 Quick Synergy Sort
Input: A multiset M of size n
Output: A sorted sequence of M
2: Compute the median µ of the middles of the runs, note j ∈ [1..ρ] the run containing µ;
3: Perform doubling searches for the value µ in all runs except the j-th, starting at both ends of the runs in parallel;

i=1 ri = n;

4: Find the maximum max(cid:96) (minimum minr) among the elements smaller (resp., greater) than µ in all runs except

the j-th;

5: Perform doubling searches for the values max(cid:96) and minr in the j-th run, starting at the position of µ;
6: Recurse on the elements smaller than o equal to max(cid:96) and on the elements greater than or equal to minr.

The number of data comparisons performed by the algorithm Quick Synergy Sort is asymptotically the
same as the number of data comparisons performed by the algorithm DLM Sort described in the previous
section. We divide the proof into two lemmas. We ﬁrst bound the overall number of data comparisons
performed by all the doubling searches of the algorithm Quick Synergy Sort.

Lemma 1. Let g1, . . . , gk be the sizes of the k blocks that form the r-th run. The overall number of data
comparisons performed by the doubling searches of the algorithm Quick Synergy Sort to ﬁnd the values of

the medians of the middles in the r-th run is within O((cid:80)k

i=1 log gi).

Proof. Every time the algorithm ﬁnds the insertion rank of one of the medians of the middles in the r-th
run, it partitions the run by a position separating two blocks. The doubling search steps can be represented
as a tree (see Figure 2 for a tree representation of a particular instance). Each node of the tree corresponds
to a step. Each internal node has two children, which correspond to the two subproblems into which the step
partitions the run. The cost of the step is less than four times the logarithm of the size of the child subproblem
with smaller size, because of the two doubling searches in parallel. The leaves of the tree correspond to the
blocks themselves.

We prove that at each step, the total cost is bounded by eight times the sum of the logarithms of the
sizes of the leaf subproblems. This is done by induction over the number of steps. If the number of steps
is zero then there is no cost. For the inductive step, if the number of steps increases by one, then a new
doubling search step is done and a leaf subproblem is partitioned into two new subproblems. At this step, a
leaf of the tree is transformed into an internal node and two new leaves are created. Let a and b such that
a ≤ b be the sizes of the new leaves created. The cost of this step is less than 4 log a. The cost of all the
steps then increases by 4 lg a, and hence the sum of the logarithms of the sizes of the leaves increases by
8(lg a + lg b) − 8 lg(a + b). But if a ≥ 4 and b ≥ a, then 2 lg(a + b) ≤ lg a + 2 lg b. The result follows.
(cid:117)(cid:116)

Fig. 2: The tree that represents the doubling
search steps for a run composed of four blocks of
respective sizes 2, 4, 2, 8. The size of the subproblem 
is noted in each node. At each subproblem,
the cost of the step is the logarithm of the size of
the subproblem of the solid child.

The step that computes the median µ of the middles of ρ runs and the step that ﬁnds the maximum
max(cid:96) (minimum minr) among the elements smaller (resp., greater) than µ of ρ runs performs linear in ρ data

88162642mi

i=1 log(cid:0) ρ

these steps is within O((cid:80)χ
Consider the instance depicted in Figure 3 for an example illustrating where the term log(cid:0) ρ
of value v is within O(log(cid:0) ρ

7
comparisons. As shown in the following lemma, the overall number of data comparisons performed during
(see Section 2.3 for the deﬁnition of π) and ρ is the number of runs in M.
from. In this instance, there is a value v that has multiplicity mv > 1 in M and the rest of the values have
multiplicity 1. The elements with value v are present at the end of the last mv runs and the rest of the runs
are formed by only one block. The elements of the i-th run are greater than the elements of the (i+1)-th run.
During the computation of the medians of the middles, the number of data comparisons that involve elements

(cid:1)), where m1, . . . , mχ are the values of the member of the partition π
(cid:1) comes
(cid:1)). The algorithm computes the median µ of the middles and partitions the
(cid:1)), where

runs by the value of µ. In the recursive call that involves elements of value v, the number of runs is reduced
by half. This is repeated until one occurrence of µ belongs to one of the last mv runs. The number of data
comparisons that involve elements of value v up to this step is within O(mv log ρ
mv
log ρ
mv
call will necessarily choose one element of value v as the median of the middles.

corresponds to the number of steps where µ does not belong to the last mv runs. The next recursive
Fig. 3: A multiset M formed by ρ runs. Each entry 
M[i] is represented by a point of x-coordinate
M[i]. There is an element of multiplicity mv
present in the last mv runs and the rest of the
runs are formed by only one block.

) = O(log(cid:0) ρ

mv

mv

mv

Lemma 2. Let M be a multiset formed by ρ runs and δ blocks such that these blocks induce a partition π
of the output of size χ whose members have values m1, . . . , mχ. Consider the steps that compute the medians
of the middles and the steps that ﬁnd the elements max(cid:96) and minr in the algorithm Quick Synergy Sort,

the overall number of data comparisons performed during these steps is within O((cid:80)χ

i=1 log(cid:0) ρ

(cid:1)).

mi

Proof. We prove this lemma by induction over the size of π and ρ. The number of data comparisons performed
by one of these steps is linear in the number of runs in the sub-instance (i.e., ignoring all the empty sets
of this sub-instance). Let T (π, ρ) be the overall number of data comparisons performed during the steps
that compute the medians of the middles and during the steps that ﬁnd the elements max(cid:96) and minr in the
− ρ. Let µ be the ﬁrst median of
the middles computed by the algorithm. Let (cid:96) and r be the number of runs that are completely to the left
and to the right of µ, respectively. Let b be the number of runs that are split in the doubling searches for the
value µ in all runs. Let π(cid:96) and πr be the partitions induced by the blocks yielded to the left and to the right
of µ, respectively. Then, T (π, ρ) = T (π(cid:96), (cid:96) + b) + T (πr, r + b) + ρ because of the two recursive calls and the
− (cid:96) − b and T (δr, r + b) ≤
,

algorithm Quick Synergy Sort. We prove that T (π, ρ) ≤(cid:80)χ
step that computes µ. By Induction Hypothesis, T (π(cid:96), (cid:96) + b) ≤(cid:80)χ(cid:96)
(cid:80)χr
− r − b. Hence, we need to prove that (cid:96) + r ≤(cid:80)χ(cid:96)
but this is a consequence of(cid:80)χ(cid:96)
log(cid:0)1 + y

(cid:17)
i=1 mi ≥ r + b (the number of blocks is greater than or equal
2 runs are left to the left and to the right of µ); and
(cid:117)(cid:116)

to the number of runs); (cid:96) ≤ r + b, r ≤ (cid:96) + b (at least ρ

i=1 mi ≥ (cid:96) + b,(cid:80)χr

(cid:1)x ≥ y for y ≤ x.

+(cid:80)χr

i=1 mi log r+b
mi

i=1 mi log (cid:96)+b
mi

i=1 mi log ρ
mi

i=1 mi log

1 + (cid:96)
r+b

1 + r
(cid:96)+b

(cid:16)

x

(cid:17)

(cid:16)

i=1 mi

Consider the step that performs doubling searches for the values max(cid:96) and minr in the run that contains
the median µ of the middles, this step results in the ﬁnding of the block g that contains µ in at most 4 log |g|
data comparisons, where |g| is the size of g. Combining Lemma 1 and Lemma 2 yields an upper bound on
the number of data comparisons performed by the algorithm Quick Synergy Sort:
Theorem 1. Let M be a multiset of size n formed by ρ runs and δ blocks of sizes g1, . . . , gδ such that these
blocks induce a partition π of the output of size χ whose members have values m1, . . . , mχ. The algorithm

Quick Synergy Sort performs within O(n +(cid:80)δ

(cid:1)) data comparisons on M. This

i=1 log gi +(cid:80)χ

i=1 log(cid:0) ρ

number of comparisons is optimal in the worst case over multisets of size n formed by ρ runs and δ blocks of
sizes g1, . . . , gδ such that these blocks induce a partition π of size χ of the output whose members have values
m1, . . . , mχ.

mi

mvρ8 We extend these results to take advantage of the global order of the multiset in a way that can be
combined with the notion of runs (local order).

2.5 Taking Advantage of Global Order
Given a multiset M, a pivot position is a position p in M such that all elements in previous position are
smaller than or equal to all elements at p or in the following positions. Formally:
Deﬁnition 2 (Pivot positions). Given a multiset M = (x1, . . . , xn) of size n, the pivot positions are the
positions p such that xa ≤ xb for all a, b such that a ∈ [1..p − 1] and b ∈ [p..n].

Existing pivot positions in the input order of M divide the input into subsequences of consecutive elements
such that the range of positions of the elements at each subsequence coincide with the range of positions
of the same elements in the sorted sequence of M: the more there are of such positions, the more “global”
order there is in the input. Detecting such positions takes only a linear number of comparisons.
Lemma 3. Given a multiset M of size n with φ pivot positions p1, . . . , pφ, the φ pivot positions can be
detected within a linear number of comparisons.

Proof. The bubble-up step of the algorithm BubbleSort [21] sequentially compares the elements in positions
i − 1 and i of M, for i from 2 to n. If M[i − 1] > M[i], then the elements interchange their values. As
consequence of this step the elements with large values tend to move to the right. In an execution of a
bubble-up step in M, the elements that do not interchange their values are those elements whose values are
greater than or equal to all the elements on their left. The bubble-down step is similar to the bubble-up
step, but it scans the sequence from right to left, interchanging the elements in positions i − 1 and i if
M[i − 1] > M[i]. In an execution of a bubble-down step in M, the elements that do not interchange their
values are those elements whose values are smaller than or equal to all the elements on their right. Hence,
the positions of the elements that do not interchange their values during the executions of both bubble-up
and bubble-down steps are the pivot positions in M.
(cid:117)(cid:116)

n0, . . . , nφ (such that(cid:80)φ

When there are φ such positions, they simply divide the input of size n into φ + 1 sub-instances of sizes

i=0 ni = n). Each sub-instance Ii for i ∈ [0..φ] then has its own number of runs ri
and alphabet size σi, on which the synergistic solutions described in this work can be applied, from mere
Sorting (Section 2) to supporting MultiSelection (Section 3) and the more sophisticated Deferred
Data Structures (Section 4).
Corollary 2. Let M be a multiset of size n with φ pivot positions. The φ pivot positions divide M into
i=0 ni = n). Each sub-instance Ii of size ni is formed
by ρi runs and δi blocks of sizes gi1, . . . , giδi such that these blocks induce a partition πi of the output of size
χi whose members have values mi1, . . . , miχi for i ∈ [0..φ]. There exists an algorithm that performs within
) data comparisons for sorting M. This number of comparisons
is optimal in the worst case over multisets of size n with φ pivot positions which divide the multiset into
i=0 ni = n) and each sub-instance Ii of size ni is formed
by ρi runs and δi blocks of sizes gi1, . . . , giδi such that these blocks induce a partition πi of the output of size
χi whose members have values mi1, . . . , miχi for i ∈ [0..φ].

φ + 1 sub-instances of sizes n0, . . . , nφ (such that (cid:80)φ
O(n +(cid:80)φ
φ + 1 sub-instances of sizes n0, . . . , nφ (such that(cid:80)φ

(cid:1)(cid:111)

(cid:110)(cid:80)δi
j=1 log gij +(cid:80)χi

j=1 log(cid:0) ρ

mij

i=0

Next, we generalize the algorithm Quick Synergy Sort to an oﬄine multiselection algorithm that partially 
sorts a multiset according to the set of select queries given as input. This serves as a pedagogical
introduction to the online Deferred Data Structures for answering rank and select queries presented
in Section 4.

For simplicity, in Section 3 and Section 4 we ﬁrst describe results ignoring existing pivot positions, and

then present the complete result as corollary.

3 MultiSelection Algorithm
Given a linearly ordered multiset M and a sequence of ranks r1, . . . , rq, a multiselection algorithm must
answer the queries select(r1), . . . , select(rq) in M, hence partially sorting M. We describe a MultiSelection 
algorithm based on the sorting algorithm Quick Synergy Sort introduced in Section 2.4. This
algorithm is an intermediate result leading to the two Deferred Data Structures described in Section 4.
Given a multiset M and a set of q select queries, the algorithm Quick Synergy MultiSelection
follows the same ﬁrst steps as the algorithm Quick Synergy Sort. But once it has computed the ranks
of all elements in the block that contains the pivot µ, it determines which select queries correspond to
elements smaller than or equal to max(cid:96) and which ones correspond to elements greater than or equal to minr
(see Algorithm 1 for the deﬁnitions of max(cid:96) and minr). It then recurses on both sides. See Algorithm 2 for
a formal description of the algorithm Quick Synergy MultiSelection.

9

1: Compute the ρ runs of respective sizes (ri)i∈[1..ρ] in M such that(cid:80)ρ

Algorithm 2 Quick Synergy MultiSelection
Input: A multiset M and a set Q of q oﬄine select queries
Output: The q selected elements
2: Compute the median µ of the middles of the ρ runs, note j ∈ [1..ρ] the run containing µ;
3: Perform doubling searches for the value µ in all runs except the j-th, starting at both ends of the runs in parallel;

i=1 ri = n;

4: Find the maximum max(cid:96) (minimum minr) among the elements smaller (resp., greater) than µ in all runs except

the j-th;

5: Perform doubling searches for the values max(cid:96) and minr in the j-th run, starting at the position of µ;
6: Compute the set of queries Q(cid:96) that go to the left of max(cid:96) and the set of queries Qr that go to the right of minr;
7: Recurse on the elements smaller than or equal to max(cid:96) and on the elements greater than or equal to minr with

the set of queries Q(cid:96) and Qr, respectively.

We extend the notion of blocks to the context of partial sorting. The idea is to consider consecutive
blocks, which have not been identiﬁed by the Quick Synergy MultiSelection algorithm, as a single block.
We next introduce the deﬁnitions of pivot blocks and selection blocks.
Deﬁnition 3 (Pivot Blocks). Given a multiset M formed by ρ runs and δ blocks. The “ pivot blocks” are
the blocks of M that contain the pivots and the elements of value equals to the pivots during the steps of the
algorithm Quick Synergy MultiSelection.

In each run, between the pivot blocks and the insertion ranks of the pivots, there are consecutive blocks
that the algorithm Quick Synergy MultiSelection has not identiﬁed as separated blocks, because no
doubling searches occurred inside them.

Deﬁnition 4 (Selection Blocks). Given the i-th run, formed of various blocks, and q select queries,
the algorithm Quick Synergy MultiSelection computes ξ pivots in the process of answering the q queries.
During the doubling searches, the algorithm Quick Synergy MultiSelection ﬁnds the insertion ranks of
the ξ pivots inside the i-th run. These positions determine a partition of size ξ + 1 of the i-th run where each
element of the partition is formed by consecutive blocks or is empty. We call the elements of this partition
“ selection blocks”. The set of all selection blocks include the set of all pivot blocks.

Using these deﬁnitions, we generalize the results proven in Section 2.4 to the more general problem of

MultiSelection.
Theorem 2. Given a multiset M of size n formed by ρ runs and δ blocks; and q oﬄine select queries
over M corresponding to elements of ranks r1, . . . , rq. The algorithm Quick Synergy MultiSelection
computes ξ pivots in the process of answering the q queries. Let s1, . . . , sβ be the sizes of the β selection

10blocks determined by these ξ pivots in all runs. Let m1, . . . , mλ be the numbers of pivot blocks among this
selection blocks corresponding to the values of the λ pivots with multiplicity greater than 1, respectively.
Let ρ0, . . . , ρξ be the sequence where ρi is the number of runs that have elements with values between the
pivots i and i + 1 sorted by ranks, for i ∈ [1..ξ]. The algorithm Quick Synergy MultiSelection answers
the q select queries performing within O

i=1 log si + β log ρ −(cid:80)λ

i=1 mi log mi −(cid:80)ξ

n +(cid:80)β

(cid:17) ⊆

i=0 ρi log ρi

(cid:16)

i=0 ∆i log ∆i) data comparisons, where ∆i = ri+1 − ri, r0 = 0 and rq+1 = n.

O (n log n −(cid:80)q
algorithm perform within O((cid:80)β

Proof. The doubling searches that ﬁnd the insertion ranks of the pivots during the overall execution of the
i=1 log si) data comparisons . At each run, a constant factor of the sum of
the logarithm of the sizes of the selection blocks bounds the number of data comparisons performed by these
doubling searches (see the proof of Lemma 1 analyzing the algorithm Quick Synergy Sort for details).

The pivots computed by the algorithm Quick Synergy MultiSelection for answering the queries are a
subset of the pivots computed by the algorithm Quick Synergy Sort for sorting the whole multiset. Suppose
that the selection blocks determined by every two consecutive pivots form a multiset Mj such that for every
(cid:17)
pair of selection blocks in Mj the elements of one are smaller than the elements of the other one. Consider
the steps that compute the medians of the middles in the algorithm Quick Synergy Sort, the number of
data comparisons performed by these steps would be within O

details). The number of comparisons needed to sort the multisets Mj is within Θ((cid:80)ξ

in this supposed instance (see the proof of Lemmas 2 analyzing the algorithm Quick Synergy Sort for
i=0 ρi log ρi). The result
(cid:117)(cid:116)

i=1 log si + β log ρ −(cid:80)λ

n +(cid:80)β

i=1 mi log mi

(cid:16)

follows.

φ + 1 sub-instances of sizes n0, . . . , nφ (such that (cid:80)φ

The process of detecting the φ pre-existing pivot positions seen in Section 2.5 can be applied as the ﬁrst
step of the multiselection algorithm. The φ pivot positions divide the input of size n into φ + 1 sub-instances
of sizes n0, . . . , nφ. For each sub-instance Ii for i ∈ [0..φ], the multiselection algorithm determines which
select queries correspond to Ii and applies then the steps of the Algorithm 2 inside Ii in order to answer
these queries.
Corollary 3. Let M be a multiset of size n with φ pivot positions. The φ pivot positions divide M into
i=0 ni = n). Let q be the number of oﬄine select
queries over M, such that qi queries correspond to the sub-instance Ii, for i ∈ [0..φ]. In each sub-instance
Ii of size ni formed by ρi runs, the algorithm Quick Synergy MultiSelection selects ξi pivots when
it answers the qi queries. These ξi pivots determine βi selection blocks of sizes si1, . . . , siβi
inside Ii.
Let mi1, . . . , miλi be the numbers of pivot blocks among this selection blocks corresponding to the values 
of the λi pivots with multiplicity greater than 1, respectively. Let ρi0, . . . , ρiξi be the sequence where
ρij is the number of runs that have elements with values between the pivots ij and i(j + 1) sorted by
ranks, for j ∈ [1..ξi]. There is an algorithm that answers the q oﬄine select queries performing within

(cid:110)(cid:80)βi
j=1 log sij + βi log ρi −(cid:80)λi

j=1 mij log mij −(cid:80)ξi

j=0 ρij log ρij

) data comparisons.

O(n +(cid:80)φ

i=0

(cid:111)

In the result above, the queries are given all at the same time (i.e., oﬄine). In the context where they
arrive one at the time (i.e., online), we deﬁne two Deferred Data Structures for answering online rank
and select queries, both inspired by the algorithm Quick Synergy MultiSelection.

4 Rank and Select Deferred Data Structures

We describe two Deferred Data Structures that answer a set of rank and select queries arriving one
at the time over a multiset M, progressively sorting M. Both data structures take advantage of the order
(local and global) and structure in the input, and of the structure in the queries. The ﬁrst data structure
is in the RAM model of computation, at the cost of not taking advantage of the order in which the queries
are given. The second data structure is in the comparison model (a more constrained model) but does take
advantage of the query order.

4.1 Taking Advantage of Order and Structure in the Input, but only of Structure in the

11

Queries

Given a multiset M of size n, the RAM Deferred Data Structure is composed of a bitvector A of size
n, in which we mark the elements in M that have been computed as pivots by the algorithm when it answers
the online queries; a dynamic predecessor and successor structure B over the bitvector A, which allows us to
ﬁnd the two successive pivots between which the query ﬁts; and for each pivot p found, the data structure
stores pointers to the insertion ranks of p in each run, to the beginning and end of the block g to which p
belongs, and to the position of p inside g. The dynamic predecessor and successor structure B requires the
RAM model of computation in order to answer predecessor and successor queries in time within o(log n) [4].
Theorem 3. Consider a multiset M of size n formed by ρ runs and δ blocks. The RAM Deferred Data
Structure computes ξ pivots in the process of answering q online rank and select queries over M. Let
s1, . . . , sβ be the sizes of the β selection blocks determined by these ξ pivots in all runs. Let m1, . . . , mλ
be the numbers of pivot blocks among this selection blocks corresponding to the values of the λ pivots with
multiplicity greater than 1, respectively. Let ρ0, . . . , ρξ be the sequence where ρi is the number of runs that
have elements with values between the pivots i and i + 1 sorted by ranks, for i ∈ [1..ξ]. Let u and g1, . . . , gu
be the number of rank queries and the sizes of the identiﬁed and searched blocks in the process of answering
the u rank queries, respectively. The RAM Deferred Data Structure answers these q online rank and
i=0 ρi log ρi + ξ log log n +

select queries in time within O(n +(cid:80)β
u log n log log n +(cid:80)u

i=1 log si + β log ρ −(cid:80)λ

i=1 mi log mi −(cid:80)ξ

i=1 log gi).

Proof. The algorithm answers a new select(i) query by accessing in A the query position i. If A[i] is 1,
then the element e has been computed as pivot, and hence the algorithm answers the query in constant
time by following the position of e inside the block at which e belongs. If A[i] is 0, then the algorithm ﬁnds
the nearest pivots to its left and right using the predecessor and successor structure, B. If the position i
is inside a block to which one of the two nearest pivots belong, then the algorithm answers the query and
in turn ﬁnishes. If not, it then applies the same steps as the algorithm Quick Synergy MultiSelection in
order to answer the query; it updates the bitvector A and the dynamic predecessor and successor structure
B whenever a new pivot is computed; and for each pivot p computed, the structure stores the pointers to
the insertion ranks of p in each run, to the beginning and end of the block g to which p belongs, and to the
position of p inside g.
The algorithm answers a new rank(x) query by ﬁnding the selection block sj in the j-th run such that x is
between the smallest and the greatest value of sj for all j ∈ [1..ρ]. For that, the algorithm performs a sort of
parallel binary searches for the value x at each run taking advantage of the pivots that have been computed
by the algorithm. The algorithm accesses the position n
2 has
been computed as pivot. Following the pointer to the block g to which e belongs, the algorithm decides if x
is to the right, to the left or inside g by performing a constant number of data comparisons. In the last case,
a binary search for the value x inside g yields the answer of the query. If A[ n
2 ] is 0, then the algorithm ﬁnds
2 using the predecessor and successor structure, B.
the nearest pivots to the left and right of the position n
Following the pointers to the blocks that contain these pivots the algorithm decides if x is inside one of these
blocks, to the right of the rightmost block, to the left of the leftmost block, or between these two blocks.
In the last case, the algorithm applies the same steps as the algorithm Quick Synergy MultiSelection in
order to compute the median µ of the middles and partitions the selection blocks by µ. The algorithm then
decides to which side x belongs. These steps identify several new pivots, and in consequence several new
(cid:117)(cid:116)
blocks in the structure.
The RAM Deferred Data Structure includes the pivot positions (seen in Section 2.5) as a natural
extension of the algorithm. The φ pivot positions are marked in the bitvector A. For each pivot position p,
the structure stores pointers to the end of the runs detected on the left of p; to the beginning of the runs
detected on the right of p; and to the position of p in the multiset.
Corollary 4. Let M be a multiset of size n with φ pivot positions. The φ pivot positions divide M into φ+1
i=0 ni = n). Let q be the number of online rank and select

sub-instances of sizes n0, . . . , nφ (such that (cid:80)φ

2 ] is 1, then the element e of rank n

2 in A. If A[ n

12queries over M, such that qi queries correspond to the sub-instance Ii, for i ∈ [0..φ]. In each sub-instance
Ii of size ni formed by ρi runs, the RAM Deferred Data Structure selects ξi pivots in the process of
answering the qi online rank and select queries over Ii. Let si1, si2, . . . , siβi be the sizes of the βi selection
blocks determined by the ξi pivots in all runs of Ii. Let m1i, . . . , miλi be the numbers of pivot blocks among this
selection blocks corresponding to the values of the λi pivots with multiplicity greater than 1, respectively. Let
ρi0, . . . , ρiξi be the sequence where ρij is the number of runs that have elements with values between the pivots
ij and i(j + 1) sorted by ranks, for j ∈ [1..ξi]. Let ui and gi1, . . . , giui be the number of rank queries and
(cid:80)φ
the sizes of the identiﬁed and searched blocks in the process of answering the ui rank queries over Ii, respectively.
 There exists an algorithm that answers these q online rank and select queries in time within O(n +

j=0 ρij log ρij + ξi log log ni + u log ni log log ni +(cid:80)ui

j=1 log gij

(cid:110)
βi log ρi −(cid:80)λi

j=1 mij log mij −(cid:80)ξi

(cid:111)

).

i=0

The RAM Deferred Data Structure takes advantage of the structure in the queries and of the
structure and order (local and global) in the input. Changing the order in the rank and select queries does
not aﬀect the time complexity of the RAM Deferred Data Structure. Once the structure identiﬁes the
nearest pivots to the left and right of the query positions, the steps of the algorithms are the same as in the
oﬄine case (Section 3). We next describe a deferred data structure taking advantage of the structure and
order in the queries and of the structure and order (local and global) in the input data.

4.2 Taking Advantage of the Order and Structure in both the Input and the Queries

To take advantage of the order in the queries, we introduce a data structure that ﬁnds the nearest pivots
to the left and to the right of a position p ∈ [1..n], while taking advantage of the distance between the
position of the last computed pivot and p. This distance is measured in the number of computed pivots
between the two positions. For that we use a ﬁnger search tree [15] which is a search tree maintaining ﬁngers
(i.e., pointers) to elements in the search tree. Finger search trees support eﬃcient updates and searches in
the vicinity of the ﬁngers. Brodal [7] described an implementation of ﬁnger search trees that searches for
an element x, starting the search at the element given by the ﬁnger f in time within O(log d), where d is
the distance between x and f in the set (i.e, the diﬀerence between rank(x) and rank(f ) in the set). This
operation returns a ﬁnger to x if x is contained in the set, otherwise a ﬁnger to the largest element smaller
than x in the set. This implementation supports the insertion of an element x immediately to the left or to
the right of a ﬁnger in worst-case constant time.
In the description of the RAM Deferred Data Structure from Theorem 3, we substitute the dynamic
predecessor and successor structure B by a ﬁnger search tree Fselect, as described by Brodal [7]. Once a
block g is identiﬁed, every element in g is a valid pivot for the rest of the elements in M. In order to capture
this idea, we modify the structure Fselect so that it contains blocks (i.e., a sequence of consecutive values)
instead of singleton pivots. Each element in Fselect points in M to the beginning and the end of the block g
that it represents and in each run to the position where the elements of g partition the run. This modiﬁcation
allows the structure to answer select queries, taking advantage of the structure and order in the queries
and of the structure and order of the input data. But in order to answer rank queries taking advantage of
the features in the queries and the input data, the structure needs another ﬁnger search tree Frank. In Frank
the structure stores for each block g identiﬁed, the value of one of the elements in g, and pointers in M to
the beginning and the end of g and in each run to the position where the elements of g partition the run.
We name this structure Full-Synergistic Deferred Data Structure.
Theorem 4. Consider a multiset M of size n formed by ρ runs and δ blocks. The Full-Synergistic
Deferred Data Structure identiﬁes γ blocks in the process of answering q online rank and select
queries over M. The q queries correspond to elements of ranks r1, . . . , rq. Let s1, . . . , sβ be the sizes of the β
selection blocks determined by the γ blocks in all runs. Let m1, . . . , mλ be the numbers of pivot blocks among
this selection blocks corresponding to the values of the λ pivots with multiplicity greater than 1, respectively.
Let ρ0, . . . , ρξ be the sequence where ρi is the number of runs that have elements with values between the pivots
i and i+1 sorted by ranks, for i ∈ [1..ξ]. Let d1, . . . , dq−1 be the sequence where dj is the number of identiﬁed
blocks between the block that answers the j−1-th query and the one that answers the j-th query before starting

the steps to answer the j-th query, for j ∈ [2..q]. Let u and g1, . . . , gu be the number of rank queries and
the sizes of the identiﬁed and searched blocks in the process of answering the u rank queries, respectively.
The Full-Synergistic Deferred Data Structure answers the q online rank and select queries
i=1 log gi) ⊆

i=1 log si +β log ρ−(cid:80)λ

i=1 mi log mi−(cid:80)ξ

performing within O(n+(cid:80)β
O (n log n −(cid:80)q

i=0 ρi log ρi +(cid:80)q−1

i=1 log di +(cid:80)u

13

i=0 ∆i log ∆i + q log n) data comparisons, where ∆i = ri+1 − ri, r0 = 0 and rq+1 = n.

Proof. The steps for answering a new select(i) query are the same as the above description except when
the algorithm searches for the nearest pivots to the left and right of the query position i. In this case, the
algorithm searches for the position i in Fselect. If i is contained in an element of Fselect, then the block g
that contains the element in the position i has already been identiﬁed. If i is not contained in an element
of Fselect, then the returned ﬁnger f points the nearest block b to the left of i. The block that follows f in
Fselect is the nearest block to the right of i. Given f , the algorithm inserts in Fselect each block identiﬁed in
the process of answering the query in constant time and stores the respective pointers to positions in M. In
Frank the algorithm searches for the value of one of the elements in g or b. Once the algorithm obtains the
ﬁnger returned by this search, the algorithm inserts in Frank the value of one of the elements of each block
identiﬁed in constant time and stores the respective pointers to positions in M.
The algorithm answers a new rank(x) query by ﬁnding the selection block sj in the j-th run such that
x is between the smallest and the greatest value of sj for all j ∈ [1..ρ] , similar to the steps of the RAM
Deferred Data Structure for answering the query . For that the algorithm searches for the value x
in Frank. The number of data comparisons performed by this searching process is within O(log d), where d
is the number of blocks in Frank between the last inserted or searched block and returned ﬁnger f . Given
the ﬁnger f , there are three possibilities for the rank r of x: (i) r is between the ranks of the elements at
the beginning and the end of the block pointed by f , (ii) r is between the ranks of the elements at the
beginning and the end of the block pointed by the ﬁnger following f , or (iii) r is between the ranks of the
elements in the selection blocks determined by f and the ﬁnger following f . In the cases (i) and (ii), a binary
search inside the block yields the answer of the query. In case (iii), the algorithm applies the same steps
as the algorithm Quick Synergy MultiSelection in order to compute the median µ of the middles and
partitions the selection blocks by µ. The algorithm then decides to which side x belongs. These doubling
searches identify two new blocks in the structure, the block that contains the greatest element smaller than
or equal to x in M and the block that contains the smallest element greater than x in M. Once compute
rank(x), the algorithm searches for this value in Fselect. It inserts then in Fselect the block that contains
the greatest element smaller than or equal to x and the block that contains the smallest element greater
(cid:117)(cid:116)
than x.
The process of detecting the φ pivot positions seen in Section 2.5 allows the Full-Synergistic Deferred 
Data Structure to insert these pivots in Fselect and Frank. For each pivot position p in Fselect
and Frank, the structure stores pointers to the end of the runs detected on the left of p; to the beginning of
the runs detected on the right of p; and to the position of p in the multiset.
Corollary 5. Let M be a multiset of size n with φ pivot positions. The φ pivot positions divide M into
i=0 ni = n). Let q be the number of online rank and
select queries over M, such that qi queries correspond to the sub-instance Ii, for i ∈ [0..φ]. In each
sub-instance Ii of size ni formed by ρi runs, the Full-Synergistic Deferred Data Structure identiﬁes 
γi blocks in the process of answering qi online rank and select queries over Ii. Let si1, si2, . . . , siβi
be the sizes of the βi selection blocks determined by the γi blocks in all runs of Ii. Let m1i, . . . , miλi
be the numbers of pivot blocks among this selection blocks corresponding to the values of the λi pivots
with multiplicity greater than 1, respectively. Let ρi0, . . . , ρiξi be the sequence where ρij is the number of
runs that have elements with values between the pivots ij and i(j + 1) sorted by ranks, for j ∈ [1..ξi].
Let di1, di2, . . . , diqi−1 be the sequence where dij is the number of identiﬁed blocks between the block that
answers the ij − 1-th query and the one that answers the ij-th query before starting the steps for answering 
the ij-th query, for j ∈ [2..qi]. Let ui and gi1, . . . , giui be the number of rank queries and the
sizes of the identiﬁed and searched blocks in the process of answering the ui rank queries over Ii, respectively.
 There exists an algorithm that answers the q online rank and select queries performing within

φ + 1 sub-instances of size n0, . . . , nφ (such that (cid:80)φ

O(n +(cid:80)φ

14

i=0

(cid:110)(cid:80)βi
j=1 log sij + βi log ρi −(cid:80)λi

j=1 mij log mij −(cid:80)ξi

j=0 ρij log ρij +(cid:80)qi−1

j=1 log dij +(cid:80)ui

j=1 log gij

(cid:111)

)

data comparisons.
The Full-Synergistic Deferred Data Structure has two advantages over the RAM Deferred Data
Structure: (i) it is in the pointer machine model of computation, which is less powerful than the RAM
model; and (ii) it takes advantage of the structure and order in the queries and of the structure and order
(local and global) in the input, when the RAM Deferred Data Structure does not take advantage of
the order in the queries. Next, we present two compressed data structures, taking advantage of the block
representation of a multiset M while supporting the operators rank and select on M.

5 Compressed Data Structures for Rank and Select
We describe two compressed representations of a multiset M of size n formed by ρ runs and δ blocks
while supporting the operators rank and select on it. The ﬁrst compressed data structure represents M in
δ log ρ+3n+o(δ log ρ+n) bits and supports each rank query in constant time and each select query in time
within O(log log ρ). The second compressed data structure represents M in δ log δ + 2n + O(δ log log δ) + o(n)
bits and supports each select query in constant time and each rank query in time within O

Given a bitvector V, rank1(V, j) ﬁnds the number of occurrences of bit 1 in V[0..j], and select1(V, i)
ﬁnds the position of the i-th occurrence of bit 1 in V. Given a sequence S from an alphabet of size ρ, rank(S,
c, j) ﬁnds the number of occurrences of character c in S[0..j]; select(S, c, i) ﬁnds the position of the i-th
occurrence of character c in S; and access(S, j) returns the character at position j in S.

(cid:16) log δ

(cid:17)

log log δ

.

5.1 Rank-aware Compressed Data Structure for Rank and Select

The Rank-aware Compressed Data Structure supports rank in constant time and select in time
within O(log log ρ) using δ log ρ + 3n + o(δ log ρ + n) bits. It contains three structures A, B and C representing
bitvectors of size n supporting for V ∈ {A,B,C}, rank1(V, j) and select1(V, i) in constant time using
n + o(n) bits each [8]. It contains a data structure S representing a sequence of length δ from an alphabet
of size ρ supporting rank(S, c, j) in time within O(log log ρ), access(S, j) in time within O(log log ρ), and
select(S, c, i) in constant time, using δ log ρ + o(δ log ρ) bits [14]. Given the blocks g1, . . . , gδ in sorted
order, A contains the information of the lengths of these blocks in this order in a bitvector of length n with
a 1 marking the position where each block starts. B contains the information of the lengths of the blocks
similar to A but with the blocks maintaining the original order, such that all blocks belonging to the same
run are consecutive. C contains the information of the length of the runs in a bitvector of length n with a 1
marking the position where each run starts. The structure S contains the run to which g belongs for each
block g in sorted order.
Theorem 5. Let M be a multiset formed by ρ runs and δ blocks. The Rank-aware Compressed Data
Structure represents M in δ log ρ + 3n + o(δ log ρ + n) bits supporting each rank query in constant time
and each select query in time within O(log log ρ).
Proof. To answer a query rank(M, x), the following operations are executed: rank1(C, i) returns the run
r that contains x in constant time, where i is the position of x in the original order of M; select1(C, r)
returns the position q where r starts in the original order of M in constant time; rank1(B, i) − rank1(B,
q − 1) returns the position p inside of r of the block g that contains x in constant time; select(S, r, p)
returns the position j of g in sorted order in constant time; and select1(A, j) returns the rank of the ﬁrst
element in g in constant time.
For answering a query select(M, i), the following operations are executed: rank1(A, i) returns the
position j of the block g in sorted order that contains the selected element in constant time; access(S, j)
returns the run r that contains the selected element in time within O(log log ρ); rank(S, r, j) returns the
position p of g inside r in time within O(log log ρ); and select1(B, p + rank1(B, select1(C, r))) returns
(cid:117)(cid:116)
the position where g starts in the original order of M in constant time.

15
We describe next a compressed data structure that represents a multiset, taking advantage of the block
representation of it, but unlike Rank-aware Compressed Data Structure, the structure supports
select in constant time.

5.2 Select-aware Compressed Data Structure for Rank and Select

(cid:16) log δ

log log δ

(cid:17)

log log δ

(cid:17)

(cid:16) log δ

The Select-aware Compressed Data Structure supports select in constant time and rank in time
using δ log δ + 2n + O(δ log log δ) + o(n) bits. It contains the same two structures A and
within O
B described above and a structure representing a permutation π of the numbers [1..δ] supporting the direct
operator π() in constant time and the inverse operator π−1() in time within O
using δ log δ +
O(δ log log δ) bits [24]. Given the blocks g1, . . . , gδ in sorted order, π(i) returns the position j of the block gi
in the original order of M and π−1(j) = i if the position of the block gi is j in the original order of M.
Theorem 6. Let M be a multiset formed by ρ runs and δ blocks. The Select-aware Compressed Data
Structure represents M in δ log δ +2n+O(δ log log δ)+o(n) bits supporting each select query in constant
time and each rank query in time within O
Proof. To answer a query select(M, i), the following operations are executed: rank1(A, i) returns the
position j of the block gj in sorted order that contains the selected element in constant time; π(j) returns
the position p of gj in the original order of M in constant time; and select1(B, p) returns the position
where gj starts in M in constant time.
To answer a query rank(M, x), the following operations are executed: rank1(B, i) returns the position
j of the block g that contains x in constant time, where i is the position of x in the original order of M;
; and select1(A, p) returns the
π−1(j) returns the position p of g in sorted order in time within O
(cid:117)(cid:116)
rank of the ﬁrst element of g in constant time.

(cid:16) log δ

(cid:16) log δ

(cid:17)

.

(cid:17)

log log δ

log log δ

This concludes the description of our synergistic results. In the next section we discuss how these results
relate to various past results and future work.

6 Discussion

In the context of deferred data structure, the concept of runs was introduced previously in [2, 19], but for a
diﬀerent purpose than the reﬁned analysis of the complexity presented in this work: we clarify the diﬀerence
and the research perspectives that it suggests in Section 6.1, and other perspectives for future research in
Section 6.2. At a higher cognition level, we discuss the importance of categorizing techniques of multivariate
analysis of algorithms in Section 6.3.

6.1 Comparison with previous work

Kaligosi et al.’s multiselection algorithm [19] and Barbay et al’s deferred data structure [2] use the very same
concept of runs as the one described in this work. The diﬀerence is that whereas we describe algorithms
which detect the existing runs in the input in order to take advantage of them, the algorithms described by
those previous works do not take into consideration any pre-existing runs in the input (assuming that there
are none) and rather build and maintain such runs as a strategy to minimize the number of comparisons
performed while partially sorting the multiset. We leave the combination of both approaches as a topic
for future work which could probably shave a constant factor oﬀ the number of comparisons performed by
the Sorting and MultiSelection algorithms and by the Deferred Data Structures supporting rank
and select on Multisets. Johnson and Frederickson [13] described an algorithm answering a single select
i=1 log ri). Using their algorithm on
pre-existing runs outperforms the Deferred Data Structures described in Section 4, when there is a single

query in a set of sorted arrays of sizes r1, r2, . . . , rρ, in time within O((cid:80)ρ

16query. Yet it is not clear how to generalize their algorithm into a deferred data structure in order to support
more than one query. The diﬀerence is somehow negligible as the cost of such a query is anyway dominated
by the cost (n − 1 comparisons) of partitioning the input into runs. We leave the generalization of Johnson
and Frederickson’s algorithm into a deferred data structure which optimally supports more than one query
as an open problem. We describe additional perspectives for future research in the next section.

6.2 Perspectives for future research

One question to tackle is to see how frequent are “easy” instances in concrete applications, in terms of input
order and structure, and in terms of query order and structure; and how much advantage can be taken of
them.

Barbay and Navarro [3] described how sorting algorithms in the comparison model directly imply encodings 
for permutations, and in particular how sorting algorithms taking advantage of speciﬁcities of the
input imply compressed encodings of such permutations. By using the similarity of the execution tree of the
algorithm MergeSort with the well known Wavelet Tree data structure, they described a compressed data
structure for permutations taking advantage of local order, i.e., using space proportional to H(r1, . . . , rρ)
and supporting direct access (i.e. π()) and inverse access (i.e. π−1()) in worst time within O(1 + lg ρ)
and average time within O(1 + H(r1, . . . , rρ)). We leave the deﬁnition of a compressed data structure for
multisets taking additional advantage of its structure and global order as future work.

Another perspective is to generalize the synergistic results to related problems in computational geometry:
Karp et al. [20] deﬁned the ﬁrst deferred data structure not only to support rank and select queries on
multisets, but also to support online queries in a deferred way on Convex Hull in two dimensions and
online Maxima queries on sets of multi-dimensional vectors. One could reﬁne the results from Karp et
al. [20], expressed in function of the number of queries, to take into account the blocks between each queries
(i.e., the structure in the queries) as Barbay et al. [2] did for multisets; but also for the relative position of
the points (i.e., the structure in the data) as Afshani et al. [1] did for Convex Hulls and Maxima; the order
in the points (i.e., the order in the data), as computing the convex hull in two dimension takes linear time
if the points are sorted; and potentially the order in the queries.

6.3

Importance of the Parameterization of Structure and Order

The computational complexity of most problems is studied in the worst case over instances of ﬁxed size n,
for n asymptotically tending to inﬁnity. This approach was reﬁned for NP-diﬃcult problems under the term
“parameterized complexity” [12], for polynomial problems under the term “Adaptive Algorithms” [11, 23],
and more simply for data encodings under the term of “Data Compression” [3], for a wide range of problems
and data types. Such a variety of results has motivated various tentative to classify them, in the context
of NP-hard problems with a theory of Fixed Parameter Tractability [12], and in the context of sorting
in the comparison model with a theory of reduction between parameters [26]. We introduced two other
perspectives from which to classify algorithms and data structures. Through the study of the sorting of
multisets according to the potential “easiness” in both the order and the values in the multiset, we aimed
to introduce a way to classify reﬁned techniques of complexity analysis between the ones considering the
input order and the ones considering the structure in the input; and to show an example of the diﬃculty of
combining both into a single hybrid algorithmic technique. Through the study of the online support of rank
and select queries on multisets according to the potential “easiness” in both the order and the values in the
queries themselves (in addition to the potential easiness in the data being queried), we aimed to introduce
such categorizations. We predict that such analysis techniques will take on more importance in the future,
along with the growth of the block between practical cases and the worst case over instances of ﬁxed sizes.
Furthermore, we conjecture that synergistic techniques taking advantage of more than one “easiness” aspect
will be of practical importance if the block between theoretical analysis and practice is to ever be reduced.

References

17

1. Afshani, P., Barbay, J., Chan, T.M.: Instance-optimal geometric algorithms. In: Proceedings of the Annual IEEE

Symposium on Foundations of Computer Science (FOCS). pp. 129–138. IEEE Computer Society (2009)

2. Barbay, J., Gupta, A., Satti, S.R., Sorenson, J.: Near-optimal online multiselection in internal and external

memory. Journal of Discrete Algorithms (JDA) 36, 3–17 (2016)

3. Barbay, J., Navarro, G.: On compressing permutations and adaptive sorting. Theoretical Computer Science (TCS)

513, 109–123 (2013)

4. Beame, P., Fich, F.E.: Optimal bounds for the predecessor problem and related problems. Journal of Computer

and System Sciences (JCSS) 65(1), 38 – 72 (2002)

5. Bentley, J.L., Yao, A.C.C.: An almost optimal algorithm for unbounded searching. Information Processing Letters

(IPL) 5(3), 82–87 (1976)

6. Blum, M., Floyd, R.W., Pratt, V.R., Rivest, R.L., Tarjan, R.E.: Time bounds for selection. Journal of Computational 
System Science (JCSS) 7(4), 448–461 (1973)

7. Brodal, G.S.: Finger search trees with constant insertion time. In: Proceedings of the ninth annual ACM-SIAM
symposium on Discrete algorithms (SODA). pp. 540–549. Society for Industrial and Applied Mathematics (1998)

8. Clark, D.R.: Compact Pat Trees. Ph.D. thesis, University of Waterloo (1996)
9. Demaine, E.D., L´opez-Ortiz, A., Munro, J.I.: Adaptive set intersections, unions, and diﬀerences. In: Proceedings

of the 11th ACM-SIAM Symposium on Discrete Algorithms (SODA). pp. 743–752 (2000)

10. Dobkin, D.P., Munro, J.I.: Optimal time minimal space selection algorithms. Journal of the ACM (JACM) 28(3),

454–461 (1981)

11. Estivill-Castro, V., Wood, D.: A survey of adaptive sorting algorithms. ACM Computing Surveys (ACMCS)

24(4), 441–476 (1992)

12. Flum, J., Grohe, M.: Parameterized Complexity Theory (Texts in Theoretical Computer Science. An EATCS

Series). Springer-Verlag New York, Inc., Secaucus, NJ, USA (2006)

13. Frederickson, G.N., Johnson, D.B.: Generalized selection and ranking. In: Proceedings of the 12th Annual ACM
Symposium on Theory of Computing (STOC), April 28-30, 1980, Los Angeles, California, USA. pp. 420–428
(1980)

14. Golynski, A., Munro, J.I., Rao, S.S.: Rank/select operations on large alphabets: A tool for text indexing. In:
Proceedings of the Seventeenth Annual ACM-SIAM Symposium on Discrete Algorithm (SODA). pp. 368–373.
SODA ’06, Society for Industrial and Applied Mathematics, Philadelphia, PA, USA (2006)

15. Guibas, L.J., McCreight, E.M., Plass, M.F., Roberts, J.R.: A new representation for linear lists. In: Proceedings
of the ninth annual ACM symposium on Theory of computing (STOC). pp. 49–60. ACM Press, New York, NY,
USA (1977)

16. Hoare, C.A.R.: Algorithm 64: Quicksort. Communication of the ACM (CACM) 4(7), 321 (1961)
17. Hoare, C.A.R.: Algorithm 65: Find. Communication of the ACM (CACM) 4(7), 321–322 (1961)
18. Huﬀman, D.A.: A method for the construction of minimum-redundancy codes. Proceedings of the Institute of

Radio Engineers (IRE) 40(9), 1098–1101 (September 1952)

19. Kaligosi, K., Mehlhorn, K., Munro, J.I., Sanders, P.: Towards optimal multiple selection. In: Proceedings of the

International Conference on Automata, Languages, and Programming (ICALP). pp. 103–114 (2005)

20. Karp, R.M., Motwani, R., Raghavan, P.: Deferred data structuring. SIAM Journal on Computing (SICOMP)

17(5), 883–902 (1988)

21. Knuth, D.E.: The Art of Computer Programming, Vol 3, chap. Sorting and Searching, Section 5.3. Addison-Wesley

(1973)

22. Mannila, H.: Measures of presortedness and optimal sorting algorithms. IEEE Trans. Computers 34(4), 318–325

(1985)

23. Moﬀat, A., Petersson, O.: An overview of adaptive sorting. Australian Computer Journal (ACJ) 24(2), 70–77

(1992)

24. Munro, J.I., Raman, R., Raman, V., S., S.R.: Succinct representations of permutations and functions. Theoretical

Computer Science (TCS) 438, 74 – 88 (2012)

25. Munro, J.I., Spira, P.M.: Sorting and searching in multisets. SIAM Journal on Computing (SICOMP) 5(1), 1–8

(1976)

26. Petersson, O., Moﬀat, A.: A framework for adaptive sorting. Discrete Applied Mathematics (DAM) 59, 153–179

(1995)

27. Takaoka,

T.:

University
http://ir.canterbury.ac.nz/handle/10092/9676, last accessed [2016-08-23 Tue]

mergesort.

Minimal

Tech.

rep.,

of

Canterbury

(1997),

1828. Takaoka, T.: Partial solution and entropy. In: Kr´aloviˇc, R., Niwi´nski, D. (eds.) Mathematical Foundations of
Computer Science (MFCS) 2009: 34th International Symposium, Novy Smokovec, High Tatras, Slovakia, August
24-28, 2009. Proceedings. pp. 700–711. Springer Berlin Heidelberg, Berlin, Heidelberg (2009)

